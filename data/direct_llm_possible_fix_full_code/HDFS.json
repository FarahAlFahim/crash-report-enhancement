[
    {
        "filename": "HDFS-4558.json",
        "creation_time": "2013-03-06T11:10:08.000+0000",
        "bug_report": {
            "Title": "Start Balancer Failed with NullPointerException",
            "Description": "The balancer fails to start due to a NullPointerException (NPE) occurring in the BlockPlacementPolicy.getInstance method. This issue arises when the balancer attempts to check the replication policy compatibility during its initialization. The NPE indicates that the configuration object passed to the BlockPlacementPolicy.getInstance method is likely null or improperly initialized.",
            "StackTrace": [
                "2013-03-06 00:19:55,174 ERROR org.apache.hadoop.hdfs.server.balancer.Balancer: java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicy.getInstance(BlockPlacementPolicy.java:165)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.checkReplicationPolicyCompatibility(Balancer.java:799)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.<init>(Balancer.java:808)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:831)"
            ],
            "RootCause": "The root cause of the NullPointerException is likely due to a null Configuration object being passed to the BlockPlacementPolicy.getInstance method, which is expected to return a valid BlockPlacementPolicy instance.",
            "StepsToReproduce": [
                "Attempt to start the balancer without a properly initialized Configuration object."
            ],
            "ExpectedBehavior": "The balancer should start successfully and check the replication policy compatibility without throwing an exception.",
            "ObservedBehavior": "The balancer fails to start and throws a NullPointerException, preventing it from checking the replication policy compatibility.",
            "Suggestions": "Ensure that a valid Configuration object is passed to the BlockPlacementPolicy.getInstance method. This may involve checking the initialization of the Configuration object before it is used in the balancer.",
            "problem_location": {
                "files": [
                    "Balancer.java",
                    "BlockPlacementPolicy.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.balancer.Balancer",
                    "org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicy"
                ],
                "methods": [
                    "Balancer.checkReplicationPolicyCompatibility",
                    "BlockPlacementPolicy.getInstance"
                ]
            },
            "possible_fix": "Modify the Balancer class to ensure that a valid Configuration object is created and passed to the BlockPlacementPolicy.getInstance method. For example, check if the Configuration object is null before calling the method and initialize it if necessary."
        },
        "possible_fix_code": {
            "Balancer.checkReplicationPolicyCompatibility": "  private static void checkReplicationPolicyCompatibility(Configuration conf\n      ) throws UnsupportedActionException {\n    if (conf == null) {\n      throw new IllegalArgumentException(\"Configuration object cannot be null\");\n    }\n    if (!(BlockPlacementPolicy.getInstance(conf, null, null) instanceof \n        BlockPlacementPolicyDefault)) {\n      throw new UnsupportedActionException(\n          \"Balancer without BlockPlacementPolicyDefault\");\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-13039.json",
        "creation_time": "2018-01-19T18:52:12.000+0000",
        "bug_report": {
            "Title": "StripedBlockReader#createBlockReader leaks socket on IOException",
            "Description": "The StripedBlockReader#createBlockReader method is leaking sockets, leading to an excessive number of connections in the CLOSE_WAIT state. This issue arises when an IOException occurs, preventing the DataNode from properly closing socket connections. As a result, the DataNode reaches the limit of open files, causing it to fail to accept new connections and log warnings about too many open files.",
            "StackTrace": [
                "java.io.IOException: Too many open files",
                "at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)",
                "at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422)",
                "at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250)",
                "at io.netty.channel.socket.nio.NioServerSocketChannel.doReadMessages(NioServerSocketChannel.java:135)",
                "at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:75)",
                "at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:563)",
                "at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:504)",
                "at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:418)",
                "at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:390)",
                "at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:742)",
                "at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:145)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The root cause of the issue is that the sockets are not being closed properly in the event of an IOException, leading to resource leaks and eventually exhausting the available file descriptors.",
            "StepsToReproduce": [
                "Run the EC on a cluster with a high load.",
                "Trigger an IOException during the execution of the StripedBlockReader#createBlockReader method.",
                "Monitor the number of open connections using lsof or similar tools."
            ],
            "ExpectedBehavior": "The DataNode should handle IOException gracefully by closing any open sockets and freeing resources, allowing it to continue accepting new connections.",
            "ObservedBehavior": "The DataNode accumulates millions of CLOSE_WAIT connections, eventually reaching the limit of open files and failing to accept new connections.",
            "Suggestions": "Implement proper exception handling in the StripedBlockReader#createBlockReader method to ensure that sockets are closed in the event of an IOException. Consider using try-with-resources or finally blocks to guarantee resource cleanup.",
            "problem_location": {
                "files": [
                    "StripedBlockReader.java"
                ],
                "classes": [
                    "com.example.StripedBlockReader"
                ],
                "methods": [
                    "createBlockReader"
                ]
            },
            "possible_fix": "In the createBlockReader method, wrap socket creation and usage in a try-with-resources statement or ensure that sockets are closed in a finally block to prevent leaks. Example:\n\ntry (Socket socket = new Socket()) {\n    // socket operations\n} catch (IOException e) {\n    // handle exception\n} // socket will be closed automatically"
        },
        "possible_fix_code": {
            "createBlockReader": "public void createBlockReader() {\n    Socket socket = null;\n    try {\n        socket = new Socket(); // Assuming socket is created here\n        // Perform operations with the socket\n    } catch (IOException e) {\n        // Handle the IOException appropriately\n        System.err.println(\"IOException occurred: \" + e.getMessage());\n    } finally {\n        if (socket != null) {\n            try {\n                socket.close();\n            } catch (IOException e) {\n                System.err.println(\"Failed to close socket: \" + e.getMessage());\n            }\n        }\n    }\n}"
        }
    },
    {
        "filename": "HDFS-13023.json",
        "creation_time": "2018-01-16T20:27:39.000+0000",
        "bug_report": {
            "Title": "Journal Sync does not work on a secure cluster",
            "Description": "The journal synchronization process fails due to an authorization issue when attempting to sync with a journal node in a secure cluster environment. The error indicates that the user is not authorized for the protocol interface, which is restricted to a specific user with Kerberos authentication. This issue arises during the execution of the syncWithJournalAtIndex method in the JournalNodeSyncer class.",
            "StackTrace": [
                "2018-01-10 01:15:40,517 INFO server.JournalNodeSyncer (JournalNodeSyncer.java:syncWithJournalAtIndex(235)) - Syncing Journal /0.0.0.0:8485 with xxx, journal id: mycluster",
                "2018-01-10 01:15:40,583 ERROR server.JournalNodeSyncer (JournalNodeSyncer.java:syncWithJournalAtIndex(259)) - Could not sync with Journal at xxx/xxx:8485",
                "com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User nn/xxx (auth:PROXY) via jn/xxx (auth:KERBEROS) is not authorized for protocol interface org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol: this service is only accessible by nn/xxx@EXAMPLE.COM",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:242)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)",
                "at com.sun.proxy.$Proxy16.getEditLogManifest(Unknown Source)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncWithJournalAtIndex(JournalNodeSyncer.java:254)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncJournals(JournalNodeSyncer.java:230)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.lambda$startSyncJournalsDaemon$0(JournalNodeSyncer.java:190)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The root cause of the issue is an authorization failure due to the user 'nn/xxx' not having the necessary permissions to access the QJournalProtocol interface. This is likely due to misconfigured Kerberos authentication settings or user permissions.",
            "StepsToReproduce": [
                "1. Set up a secure cluster with Kerberos authentication.",
                "2. Attempt to sync the journal using the JournalNodeSyncer.",
                "3. Observe the error in the logs indicating authorization failure."
            ],
            "ExpectedBehavior": "The journal synchronization should succeed without authorization errors, allowing the journal nodes to sync properly.",
            "ObservedBehavior": "The journal synchronization fails with an authorization exception, preventing the sync from completing.",
            "Suggestions": "Verify the Kerberos configuration and ensure that the user 'nn/xxx' has the appropriate permissions to access the QJournalProtocol interface. Adjust the security settings or user roles as necessary.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer",
                    "org.apache.hadoop.ipc.Client"
                ],
                "methods": [
                    "JournalNodeSyncer.syncWithJournalAtIndex",
                    "Client.call",
                    "Client.getRpcResponse"
                ]
            },
            "possible_fix": "Ensure that the user 'nn/xxx' is properly configured in the Kerberos system and has the necessary permissions to access the QJournalProtocol. This may involve updating the Hadoop security configuration files to include the correct user roles and permissions."
        },
        "possible_fix_code": {
            "syncWithJournalAtIndex": "  private void syncWithJournalAtIndex(int index) {\n    LOG.info(\"Syncing Journal \" + jn.getBoundIpcAddress().getAddress() + \":\" +\n        jn.getBoundIpcAddress().getPort() + \" with \" +\n        otherJNProxies.get(index) + \", journal id: \" + jid);\n    final QJournalProtocolPB jnProxy = otherJNProxies.get(index).jnProxy;\n    if (jnProxy == null) {\n      LOG.error(\"JournalNode Proxy not found.\");\n      return;\n    }\n\n    List<RemoteEditLog> thisJournalEditLogs;\n    try {\n      thisJournalEditLogs = journal.getEditLogManifest(0, false).getLogs();\n    } catch (IOException e) {\n      LOG.error(\"Exception in getting local edit log manifest\", e);\n      return;\n    }\n\n    GetEditLogManifestResponseProto editLogManifest;\n    try {\n      editLogManifest = jnProxy.getEditLogManifest(null,\n          GetEditLogManifestRequestProto.newBuilder().setJid(jidProto)\n              .setSinceTxId(0)\n              .setInProgressOk(false).build());\n    } catch (ServiceException e) {\n      LOG.error(\"Could not sync with Journal at \" +\n          otherJNProxies.get(index), e);\n      return;\n    }\n\n    // Check user authorization before proceeding\n    if (!isUserAuthorized()) {\n      LOG.error(\"User is not authorized to access the QJournalProtocol.\");\n      return;\n    }\n\n    getMissingLogSegments(thisJournalEditLogs, editLogManifest,\n        otherJNProxies.get(index));\n  }\n\n  private boolean isUserAuthorized() {\n    // Implement the logic to check if the user has the necessary permissions\n    // This is a placeholder for the actual authorization check\n    return true; // Replace with actual authorization logic\n  }"
        }
    },
    {
        "filename": "HDFS-3157.json",
        "creation_time": "2012-03-28T14:35:53.000+0000",
        "bug_report": {
            "Title": "Error in deleting block keeps occurring from DN even after block report and directory scanning",
            "Description": "In a Hadoop cluster setup with one NameNode (NN) and three DataNodes (DN1, DN2, DN3), an issue arises when attempting to delete a block from a DataNode (DN1) after writing a file ('a.txt') with sync but not closing it. The replication factor is set to 2, meaning that the blocks are replicated to another DataNode. After the deletion command is issued, the DataNode logs an error indicating that the block information cannot be found in the volume map, leading to an IOException. This issue is compounded by the mismatch in generation stamps between the reported RBW (replicating block write) replica and the COMPLETE block in the block map.",
            "StackTrace": [
                "2012-02-29 13:54:13,126 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block blk_2903555284838653156_1003. BlockInfo not found in volumeMap.",
                "2012-02-29 13:54:13,126 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Error processing datanode Command",
                "java.io.IOException: Error in deleting blocks.",
                "at org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate(FSDataset.java:2061)",
                "at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:581)",
                "at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:545)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand(BPServiceActor.java:690)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:522)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:662)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The root cause of the issue is that the block information for the block being deleted is not found in the volume map, which is likely due to a mismatch in the generation stamp between the reported RBW replica and the COMPLETE block in the block map. This leads to the DataNode being unable to locate the block for deletion, resulting in an IOException.",
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with 1 NN and 3 DNs (DN1, DN2, DN3) with a replication factor of 2.",
                "2. Write a file 'a.txt' with sync but do not close it.",
                "3. Manually delete the blocks from DN1.",
                "4. Close the file 'a.txt'.",
                "5. Observe the logs for errors related to block deletion."
            ],
            "ExpectedBehavior": "The block should be successfully deleted from the DataNode without any errors, and the block information should be correctly updated in the volume map.",
            "ObservedBehavior": "An error occurs indicating that the block information cannot be found in the volume map, leading to an IOException when attempting to delete the block.",
            "Suggestions": "Review the logic in the FSDataset.invalidate method to ensure that the block information is correctly maintained in the volume map. Additionally, investigate the handling of generation stamps to prevent mismatches between reported and actual block states.",
            "problem_location": {
                "files": [
                    "FSDataset.java",
                    "BPOfferService.java",
                    "BPServiceActor.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.FSDataset",
                    "org.apache.hadoop.hdfs.server.datanode.BPOfferService",
                    "org.apache.hadoop.hdfs.server.datanode.BPServiceActor"
                ],
                "methods": [
                    "FSDataset.invalidate",
                    "BPOfferService.processCommandFromActive",
                    "BPOfferService.processCommandFromActor",
                    "BPServiceActor.processCommand"
                ]
            },
            "possible_fix": "In the FSDataset.invalidate method, ensure that the block information is correctly checked and updated in the volume map before attempting to delete the block. Additionally, implement checks to handle cases where the generation stamp does not match, possibly by logging a more informative error message or handling the deletion process differently."
        },
        "possible_fix_code": {
            "FSDataset.invalidate": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error = false;\n    for (int i = 0; i < invalidBlks.length; i++) {\n      File f = null;\n      final FSVolume v;\n      synchronized (this) {\n        f = getFile(bpid, invalidBlks[i].getBlockId());\n        ReplicaInfo dinfo = volumeMap.get(bpid, invalidBlks[i]);\n        if (dinfo == null || \n            dinfo.getGenerationStamp() != invalidBlks[i].getGenerationStamp()) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                           + invalidBlks[i] + \n                           \". BlockInfo not found in volumeMap. Generation stamp mismatch.\");\n          error = true;\n          continue;\n        }\n        v = (FSVolume)dinfo.getVolume();\n        if (f == null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Block not found in blockMap.\" +\n                            ((v == null) ? \" \" : \" Block found in volumeMap.\"));\n          error = true;\n          continue;\n        }\n        if (v == null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". No volume for this block.\" +\n                            \" Block found in blockMap. \" + f + \".\");\n          error = true;\n          continue;\n        }\n        File parent = f.getParentFile();\n        if (parent == null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Parent not found for file \" + f + \".\");\n          error = true;\n          continue;\n        }\n        ReplicaState replicaState = dinfo.getState();\n        if (replicaState == ReplicaState.FINALIZED || \n            (replicaState == ReplicaState.RUR && \n                ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() == \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      File metaFile = DatanodeUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp());\n\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f, metaFile,\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-4850.json",
        "creation_time": "2013-05-25T00:18:15.000+0000",
        "bug_report": {
            "Title": "NegativeArraySizeException in OfflineImageViewer with empty files or snapshots",
            "Description": "The OfflineImageViewer fails to process fsimages that include empty files or snapshots, resulting in a NegativeArraySizeException. This issue occurs when the fsimage is generated after creating an empty file, leading to unexpected behavior during the image loading process.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.NegativeArraySizeException",
                "at org.apache.hadoop.io.Text.readString(Text.java:458)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processPermission(ImageLoaderCurrent.java:370)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINode(ImageLoaderCurrent.java:671)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processChildren(ImageLoaderCurrent.java:557)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:464)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:470)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:470)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processLocalNameINodesWithSnapshot(ImageLoaderCurrent.java:444)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINodes(ImageLoaderCurrent.java:398)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.loadImage(ImageLoaderCurrent.java:199)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.go(OfflineImageViewer.java:136)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.main(OfflineImageViewer.java:260)"
            ],
            "RootCause": "The NegativeArraySizeException is triggered when the method readString attempts to read a string with a negative length, which can occur if the number of blocks for an empty file is incorrectly processed, leading to an invalid array size.",
            "StepsToReproduce": [
                "Deploy hadoop-trunk HDFS.",
                "Create a directory /user/schu.",
                "Force a checkpoint and fetch the fsimage.",
                "Run OfflineImageViewer on the fsimage.",
                "Create an empty file /user/schu/testFile1.",
                "Force another checkpoint and fetch the new fsimage.",
                "Run OfflineImageViewer on the new fsimage."
            ],
            "ExpectedBehavior": "The OfflineImageViewer should successfully process the fsimage and include the empty file in the output without errors.",
            "ObservedBehavior": "The OfflineImageViewer throws a NegativeArraySizeException and fails to process the fsimage containing the empty file.",
            "Suggestions": "Review the logic in the processINode and processPermission methods to ensure that empty files are handled correctly. Implement checks to prevent negative lengths when reading strings.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/Text.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/ImageLoaderCurrent.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewer.java"
                ],
                "classes": [
                    "org.apache.hadoop.io.Text",
                    "org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent",
                    "org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer"
                ],
                "methods": [
                    "Text.readString",
                    "ImageLoaderCurrent.processPermission",
                    "ImageLoaderCurrent.processINode",
                    "ImageLoaderCurrent.processChildren",
                    "ImageLoaderCurrent.processDirectoryWithSnapshot",
                    "ImageLoaderCurrent.loadImage",
                    "OfflineImageViewer.go"
                ]
            },
            "possible_fix": "In the method 'readString' of the Text class, add validation to ensure that the length read from the input stream is non-negative before creating the byte array. Additionally, in 'processINode', ensure that the handling of empty files correctly sets the number of blocks to zero and does not lead to negative values."
        },
        "possible_fix_code": {
            "Text.readString": "  public static String readString(DataInput in, int maxLength)\n      throws IOException {\n    int length = WritableUtils.readVIntInRange(in, 0, maxLength);\n    if (length < 0) { // Check for negative length\n      throw new IOException(\"Negative length encountered while reading string.\");\n    }\n    byte [] bytes = new byte[length];\n    in.readFully(bytes, 0, length);\n    return decode(bytes);\n  }",
            "ImageLoaderCurrent.processINode": "  private void processINode(DataInputStream in, ImageVisitor v,\n      boolean skipBlocks, String parentName, boolean isSnapshotCopy)\n      throws IOException {\n    boolean supportSnapshot = \n        LayoutVersion.supports(Feature.SNAPSHOT, imageVersion);\n    boolean supportInodeId = \n        LayoutVersion.supports(Feature.ADD_INODE_ID, imageVersion);\n    \n    v.visitEnclosingElement(ImageElement.INODE);\n    String pathName = FSImageSerialization.readString(in);\n    if (parentName != null) {  // local name\n      pathName = \"/\" + pathName;\n      if (!\"/\".equals(parentName)) { // children of non-root directory\n        pathName = parentName + pathName;\n      }\n    }\n\n    long inodeId = INodeId.GRANDFATHER_INODE_ID;\n    v.visit(ImageElement.INODE_PATH, pathName);\n    if (supportInodeId) {\n      inodeId = in.readLong();\n      v.visit(ImageElement.INODE_ID, inodeId);\n    }\n    v.visit(ImageElement.REPLICATION, in.readShort());\n    v.visit(ImageElement.MODIFICATION_TIME, formatDate(in.readLong()));\n    if(LayoutVersion.supports(Feature.FILE_ACCESS_TIME, imageVersion))\n      v.visit(ImageElement.ACCESS_TIME, formatDate(in.readLong()));\n    v.visit(ImageElement.BLOCK_SIZE, in.readLong());\n    int numBlocks = in.readInt();\n\n    if (numBlocks < 0) { // Handle empty files\n      numBlocks = 0; // Set to zero for empty files\n    }\n\n    processBlocks(in, v, numBlocks, skipBlocks);\n    \n    if (numBlocks > 0) { // File\n      if (supportSnapshot) {\n        // process file diffs\n        processFileDiffList(in, v, parentName);\n        if (isSnapshotCopy) {\n          boolean underConstruction = in.readBoolean();\n          if (underConstruction) {\n            v.visit(ImageElement.CLIENT_NAME,\n                FSImageSerialization.readString(in));\n            v.visit(ImageElement.CLIENT_MACHINE,\n                FSImageSerialization.readString(in));\n          }\n        }\n      }\n    } else if (numBlocks == -1) { // Directory\n      if (supportSnapshot && supportInodeId) {\n        dirNodeMap.put(inodeId, pathName);\n      }\n      v.visit(ImageElement.NS_QUOTA, numBlocks == -1 ? in.readLong() : -1);\n      if (LayoutVersion.supports(Feature.DISKSPACE_QUOTA, imageVersion))\n        v.visit(ImageElement.DS_QUOTA, numBlocks == -1 ? in.readLong() : -1);\n      if (supportSnapshot) {\n        boolean snapshottable = in.readBoolean();\n        if (!snapshottable) {\n          boolean withSnapshot = in.readBoolean();\n          v.visit(ImageElement.IS_WITHSNAPSHOT_DIR, Boolean.toString(withSnapshot));\n        } else {\n          v.visit(ImageElement.IS_SNAPSHOTTABLE_DIR, Boolean.toString(snapshottable));\n        }\n      }\n    } else if (numBlocks == -2) {\n      v.visit(ImageElement.SYMLINK, Text.readString(in));\n    } else if (numBlocks == -3) { // reference node\n      final boolean isWithName = in.readBoolean();\n      int snapshotId = in.readInt();\n      if (isWithName) {\n        v.visit(ImageElement.SNAPSHOT_LAST_SNAPSHOT_ID, snapshotId);\n      } else {\n        v.visit(ImageElement.SNAPSHOT_DST_SNAPSHOT_ID, snapshotId);\n      }\n      \n      final boolean firstReferred = in.readBoolean();\n      if (firstReferred) {\n        v.visitEnclosingElement(ImageElement.SNAPSHOT_REF_INODE);\n        processINode(in, v, skipBlocks, parentName, isSnapshotCopy);\n        v.leaveEnclosingElement();  // referred inode    \n      } else {\n        v.visit(ImageElement.SNAPSHOT_REF_INODE_ID, in.readLong());\n      }\n    }\n\n    processPermission(in, v);\n    v.leaveEnclosingElement(); // INode\n  }"
        }
    },
    {
        "filename": "HDFS-3415.json",
        "creation_time": "2012-05-13T13:44:43.000+0000",
        "bug_report": {
            "Title": "NameNode startup fails due to NullPointerException when storage directories have different layout versions",
            "Description": "When starting the NameNode with multiple storage directories, if one of the directories has a different layout version than the others, a NullPointerException occurs. This happens because the system attempts to access a storage file that does not exist or is not properly initialized, leading to the failure of the NameNode startup process.",
            "StackTrace": [
                "2012-05-13 19:01:41,483 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.NNStorage.getStorageFile(NNStorage.java:686)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditsInStorageDir(FSImagePreTransactionalStorageInspector.java:243)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getLatestEditsFiles(FSImagePreTransactionalStorageInspector.java:261)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditLogStreams(FSImagePreTransactionalStorageInspector.java:276)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:596)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:247)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:498)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:390)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:368)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:402)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:564)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:545)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1093)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1151)"
            ],
            "RootCause": "The root cause of the issue is that the method `getStorageFile` in `NNStorage` is called with a storage directory that has an incompatible layout version, resulting in a NullPointerException when the expected edits file does not exist.",
            "StepsToReproduce": [
                "1. Configure three storage directories for the NameNode.",
                "2. Write 10 files to the NameNode.",
                "3. Manually edit the version file of one of the storage directories to set the layout version to 123, which is different from the default layout version (-40).",
                "4. Stop the NameNode.",
                "5. Start the NameNode again."
            ],
            "ExpectedBehavior": "The NameNode should start successfully and load the storage directories without any exceptions, regardless of the layout versions.",
            "ObservedBehavior": "The NameNode fails to start and throws a NullPointerException due to an attempt to access a non-existent storage file.",
            "Suggestions": "Ensure that all storage directories have the same layout version before starting the NameNode. Implement checks to validate the layout versions of the storage directories during the startup process.",
            "problem_location": {
                "files": [
                    "NNStorage.java",
                    "FSImagePreTransactionalStorageInspector.java",
                    "FSImage.java",
                    "FSNamesystem.java",
                    "NameNode.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.NNStorage",
                    "org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector",
                    "org.apache.hadoop.hdfs.server.namenode.FSImage",
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem",
                    "org.apache.hadoop.hdfs.server.namenode.NameNode"
                ],
                "methods": [
                    "NNStorage.getStorageFile",
                    "FSImagePreTransactionalStorageInspector.getEditsInStorageDir",
                    "FSImage.loadFSImage",
                    "FSNamesystem.loadFSImage",
                    "NameNode.initialize"
                ]
            },
            "possible_fix": "Add a validation step in the `initialize` method of `NameNode` to check if all storage directories have the same layout version before proceeding with loading the file system image. If a mismatch is detected, log an error and prevent the NameNode from starting."
        },
        "possible_fix_code": {
            "NameNode.initialize": "  protected void initialize(Configuration conf) throws IOException {\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    loadNamesystem(conf);\n\n    // New validation step to check layout versions\n    validateStorageDirectoriesLayoutVersion();\n\n    rpcServer = createRpcServer(conf);\n    \n    try {\n      validateConfigurationSettings(conf);\n    } catch (IOException e) {\n      LOG.fatal(e.toString());\n      throw e;\n    }\n\n    startCommonServices(conf);\n  }\n\n  private void validateStorageDirectoriesLayoutVersion() throws IOException {\n    int expectedLayoutVersion = storage.getLayoutVersion();\n    for (StorageDirectory sd : storage.dirIterator()) {\n      int layoutVersion = NNStorage.getLayoutVersion(sd);\n      if (layoutVersion != expectedLayoutVersion) {\n        LOG.error(\"Incompatible layout version found in storage directory: \" + sd.getRoot() + \". Expected: \" + expectedLayoutVersion + \", Found: \" + layoutVersion);\n        throw new IOException(\"Storage directories have incompatible layout versions. Please ensure all directories are consistent.\");\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-2245.json",
        "creation_time": "2011-08-10T22:55:33.000+0000",
        "bug_report": {
            "Title": "BlockManager.chooseTarget(..) throws NPE",
            "Description": "The method BlockManager.chooseTarget is throwing a NullPointerException (NPE) when attempting to allocate a new block for a file. This occurs during the addBlock operation, specifically when the system tries to choose target DataNodes for replication. The NPE indicates that a required object is null, which is likely due to the client node being null or the targets array being empty.",
            "StackTrace": [
                "java.io.IOException: java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1225)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1455)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:786)"
            ],
            "RootCause": "The root cause of the NullPointerException is likely due to the clientNode being null when passed to the BlockManager.chooseTarget method, which can happen if the pending file does not have a valid client node associated with it.",
            "StepsToReproduce": [
                "1. Attempt to add a block to a file without a valid client node.",
                "2. Ensure that the file is in a state that requires a new block to be allocated.",
                "3. Observe the exception thrown during the addBlock operation."
            ],
            "ExpectedBehavior": "The system should successfully allocate a new block to the specified file and return a LocatedBlock object without throwing an exception.",
            "ObservedBehavior": "The system throws a NullPointerException when trying to choose target DataNodes for the new block allocation.",
            "Suggestions": "Ensure that the client node is properly initialized and associated with the pending file before calling chooseTarget. Additionally, add null checks to handle cases where the client node may not be set.",
            "problem_location": {
                "files": [
                    "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                    "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                    "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager",
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem",
                    "org.apache.hadoop.hdfs.server.namenode.NameNode"
                ],
                "methods": [
                    "BlockManager.chooseTarget",
                    "FSNamesystem.getAdditionalBlock",
                    "NameNode.addBlock"
                ]
            },
            "possible_fix": "In the FSNamesystem.getAdditionalBlock method, ensure that the clientNode is properly set before calling blockManager.chooseTarget. For example, add a check to verify that pendingFile.getClientNode() does not return null, and handle the case appropriately, possibly by throwing a more descriptive exception or providing a default value."
        },
        "possible_fix_code": {
            "FSNamesystem.getAdditionalBlock": "  LocatedBlock getAdditionalBlock(String src,\n                                         String clientName,\n                                         ExtendedBlock previous,\n                                         HashMap<Node, Node> excludedNodes\n                                         ) \n      throws LeaseExpiredException, NotReplicatedYetException,\n      QuotaExceededException, SafeModeException, UnresolvedLinkException,\n      IOException {\n    checkBlock(previous);\n    long fileLength, blockSize;\n    int replication;\n    DatanodeDescriptor clientNode = null;\n    Block newBlock = null;\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\n          \"BLOCK* NameSystem.getAdditionalBlock: file \"\n          +src+\" for \"+clientName);\n    }\n\n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot add block to \" + src, safeMode);\n      }\n\n      // have we exceeded the configured limit of fs objects.\n      checkFsObjectLimit();\n\n      INodeFileUnderConstruction pendingFile  = checkLease(src, clientName);\n\n      // commit the last block and complete it if it has minimum replicas\n      blockManager.commitOrCompleteLastBlock(pendingFile, ExtendedBlock\n          .getLocalBlock(previous));\n\n      //\n      // If we fail this, bad things happen!\n      //\n      if (!checkFileProgress(pendingFile, false)) {\n        throw new NotReplicatedYetException(\"Not replicated yet:\" + src);\n      }\n      fileLength = pendingFile.computeContentSummary().getLength();\n      blockSize = pendingFile.getPreferredBlockSize();\n      clientNode = pendingFile.getClientNode();\n      replication = (int)pendingFile.getReplication();\n\n      // Check if clientNode is null and handle it\n      if (clientNode == null) {\n        throw new IOException(\"Client node is null for file: \" + src);\n      }\n    } finally {\n      writeUnlock();\n    }\n\n    // choose targets for the new block to be allocated.\n    final DatanodeDescriptor targets[] = blockManager.chooseTarget(\n        src, replication, clientNode, excludedNodes, blockSize);\n\n    // Allocate a new block and record it in the INode. \n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot add block to \" + src, safeMode);\n      }\n      INode[] pathINodes = dir.getExistingPathINodes(src);\n      int inodesLen = pathINodes.length;\n      checkLease(src, clientName, pathINodes[inodesLen-1]);\n      INodeFileUnderConstruction pendingFile  = (INodeFileUnderConstruction) \n                                                pathINodes[inodesLen - 1];\n                                                           \n      if (!checkFileProgress(pendingFile, false)) {\n        throw new NotReplicatedYetException(\"Not replicated yet:\" + src);\n      }\n\n      // allocate new block record block locations in INode.\n      newBlock = allocateBlock(src, pathINodes, targets);\n      \n      for (DatanodeDescriptor dn : targets) {\n        dn.incBlocksScheduled();\n      }      \n    } finally {\n      writeUnlock();\n    }\n\n    // Create next block\n    LocatedBlock b = new LocatedBlock(getExtendedBlock(newBlock), targets, fileLength);\n    blockManager.setBlockToken(b, BlockTokenSecretManager.AccessMode.WRITE);\n    return b;\n  }"
        }
    },
    {
        "filename": "HDFS-10320.json",
        "creation_time": "2016-04-20T23:02:09.000+0000",
        "bug_report": {
            "Title": "Rack failures may result in NN termination",
            "Description": "The system encounters a critical failure when there are rack failures that leave only one rack available. The method `BlockPlacementPolicyDefault#chooseRandom` attempts to select a datanode from the available racks. If the only remaining rack is excluded due to the failure, it results in an `InvalidTopologyException`, which propagates up to the `BlockManager`'s `ReplicationMonitor` thread, ultimately causing the NameNode (NN) to terminate unexpectedly.",
            "StackTrace": [
                "org.apache.hadoop.net.NetworkTopology$InvalidTopologyException: Failed to find datanode (scope=\"\" excludedScope=\"/rack_a5\").",
                "at org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:729)",
                "at org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:694)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:635)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:580)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:348)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:214)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:111)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.chooseTargets(BlockManager.java:3746)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.access$200(BlockManager.java:3711)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1400)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1306)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3682)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3634)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that when there is only one rack available and it is excluded from the selection process, the `chooseRandom` method fails to find a valid datanode, leading to an `InvalidTopologyException`.",
            "StepsToReproduce": [
                "Simulate a rack failure that leaves only one rack available.",
                "Ensure that the remaining rack is excluded from the datanode selection process.",
                "Attempt to perform operations that require datanode selection, such as block replication."
            ],
            "ExpectedBehavior": "The system should handle rack failures gracefully without terminating the NameNode. It should either retry the selection process or log an appropriate error without crashing.",
            "ObservedBehavior": "The NameNode terminates unexpectedly due to an unhandled `InvalidTopologyException` when it cannot find a valid datanode.",
            "Suggestions": "Implement error handling in the `chooseRandom` method to manage scenarios where no valid datanodes are available. Consider adding fallback mechanisms to retry selection or to log errors without crashing the NameNode.",
            "problem_location": {
                "files": [
                    "NetworkTopology.java",
                    "BlockPlacementPolicyDefault.java",
                    "BlockManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.net.NetworkTopology",
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault",
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager"
                ],
                "methods": [
                    "NetworkTopology.chooseRandom",
                    "BlockPlacementPolicyDefault.chooseRandom",
                    "BlockPlacementPolicyDefault.chooseRemoteRack",
                    "BlockManager.computeDatanodeWork"
                ]
            },
            "possible_fix": "In the `chooseRandom` method of `NetworkTopology`, add a check to handle the case where no datanodes are available after exclusions. Instead of throwing an `InvalidTopologyException`, log a warning and return a fallback value or null. This will prevent the NameNode from terminating unexpectedly."
        },
        "possible_fix_code": {
            "NetworkTopology.chooseRandom": "  private Node chooseRandom(String scope, String excludedScope){\n    if (excludedScope != null) {\n      if (scope.startsWith(excludedScope)) {\n        return null;\n      }\n      if (!excludedScope.startsWith(scope)) {\n        excludedScope = null;\n      }\n    }\n    Node node = getNode(scope);\n    if (!(node instanceof InnerNode)) {\n      return node;\n    }\n    InnerNode innerNode = (InnerNode)node;\n    int numOfDatanodes = innerNode.getNumOfLeaves();\n    if (excludedScope == null) {\n      node = null;\n    } else {\n      node = getNode(excludedScope);\n      if (!(node instanceof InnerNode)) {\n        numOfDatanodes -= 1;\n      } else {\n        numOfDatanodes -= ((InnerNode)node).getNumOfLeaves();\n      }\n    }\n    if (numOfDatanodes == 0) {\n      // Log a warning instead of throwing an exception\n      LOG.warn(\"Failed to find datanode (scope=\\\"\" + String.valueOf(scope) + \"\\\" excludedScope=\\\"\" + String.valueOf(excludedScope) + \"\\\"). Returning null.\");\n      return null; // Return null instead of throwing an exception\n    }\n    int leaveIndex = r.nextInt(numOfDatanodes);\n    return innerNode.getLeaf(leaveIndex, node);\n  }"
        }
    },
    {
        "filename": "HDFS-4201.json",
        "creation_time": "2012-11-16T21:28:57.000+0000",
        "bug_report": {
            "Title": "NPE in BPServiceActor#sendHeartBeat",
            "Description": "A NullPointerException (NPE) occurs in the sendHeartBeat method of the BPServiceActor class. The issue arises when either the datanode (dn) or the filesystem dataset (dn.getFSDataset()) is null, which can happen due to misconfiguration or failure in the local directory setup. This leads to an attempt to access methods on a null object, resulting in the NPE.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:434)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:520)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:673)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "RootCause": "The root cause of the NPE is that the datanode (dn) or its filesystem dataset (dn.getFSDataset()) is null when the sendHeartBeat method is called. This can occur if the datanode is not properly initialized or if there is a failure in the configuration.",
            "StepsToReproduce": [
                "Start the datanode service with an incorrect or incomplete configuration.",
                "Ensure that the filesystem dataset is not initialized or is set to null.",
                "Monitor the logs for the occurrence of the NPE during the heartbeat process."
            ],
            "ExpectedBehavior": "The system should successfully send a heartbeat without throwing a NullPointerException, even if the datanode or its dataset is not fully initialized.",
            "ObservedBehavior": "The system throws a NullPointerException when attempting to send a heartbeat due to null references in the sendHeartBeat method.",
            "Suggestions": "Ensure that the datanode and its filesystem dataset are properly initialized before calling sendHeartBeat. Implement null checks before accessing methods on these objects.",
            "problem_location": {
                "files": [
                    "BPServiceActor.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.BPServiceActor"
                ],
                "methods": [
                    "BPServiceActor.sendHeartBeat",
                    "BPServiceActor.offerService",
                    "BPServiceActor.run"
                ]
            },
            "possible_fix": "Add null checks in the sendHeartBeat method to ensure that dn and dn.getFSDataset() are not null before accessing their methods. For example:\n\n```java\nif (dn == null || dn.getFSDataset() == null) {\n    throw new IOException(\"Datanode or its filesystem dataset is not initialized.\");\n}\n```\nThis will prevent the NPE and allow for better error handling."
        },
        "possible_fix_code": {
            "sendHeartBeat": "  HeartbeatResponse sendHeartBeat() throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat from service actor: \" + this);\n    }\n    // Check for null datanode or filesystem dataset\n    if (dn == null || dn.getFSDataset() == null) {\n        throw new IOException(\"Datanode or its filesystem dataset is not initialized.\");\n    }\n    // reports number of failed volumes\n    StorageReport[] report = { new StorageReport(bpRegistration.getStorageID(),\n        false,\n        dn.getFSDataset().getCapacity(),\n        dn.getFSDataset().getDfsUsed(),\n        dn.getFSDataset().getRemaining(),\n        dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId())) };\n    return bpNamenode.sendHeartbeat(bpRegistration, report,\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        dn.getFSDataset().getNumFailedVolumes());\n  }"
        }
    },
    {
        "filename": "HDFS-6904.json",
        "creation_time": "2014-08-21T10:17:21.000+0000",
        "bug_report": {
            "Title": "YARN unable to renew delegation token fetched via webhdfs due to incorrect service port",
            "Description": "YARN fails to renew delegation tokens obtained through the WebHDFS REST API because the service port used for the token renewal is incorrectly set to the RPC port instead of the WebHDFS port. This issue arises when a user creates a delegation token using the WebHDFS API and submits it to YARN, which then attempts to renew the token but encounters an IOException due to the mismatch in expected service ports.",
            "StackTrace": [
                "2014-08-19 03:12:54,733 WARN  security.DelegationTokenRenewer (DelegationTokenRenewer.java:handleDTRenewerAppSubmitEvent(661)) - Unable to add the application to the delegation token renewer.",
                "java.io.IOException: Failed to renew token: Kind: WEBHDFS delegation, Service: NameNodeIP:8020, Ident: (WEBHDFS delegation token 2222 for hrt_qa)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:394)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.access$5(DelegationTokenRenewer.java:357)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.handleDTRenewerAppSubmitEvent(DelegationTokenRenewer.java:657)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.run(DelegationTokenRenewer.java:638)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.io.IOException: Unexpected HTTP response: code=-1 != 200, op=RENEWDELEGATIONTOKEN, message=null",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:331)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$200(WebHdfsFileSystem.java:90)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:598)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:448)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:477)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:473)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.renewDelegationToken(WebHdfsFileSystem.java:1318)",
                "at org.apache.hadoop.hdfs.web.TokenAspect$TokenManager.renew(TokenAspect.java:73)",
                "at org.apache.hadoop.security.token.Token.renew(Token.java:377)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:477)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:1)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.renewToken(DelegationTokenRenewer.java:473)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:392)"
            ],
            "RootCause": "The root cause of the issue is that the delegation token generated by the NameNode is of kind WebHDFS, but the service port used for renewing the token is incorrectly set to the RPC port (8020) instead of the WebHDFS port (50070).",
            "StepsToReproduce": [
                "1. Use the WebHDFS REST API to create a delegation token.",
                "2. Submit an application to YARN using the created delegation token.",
                "3. Observe the logs for the failure during the token renewal process."
            ],
            "ExpectedBehavior": "YARN should successfully renew the delegation token using the correct WebHDFS service port.",
            "ObservedBehavior": "YARN fails to renew the delegation token and logs an IOException indicating an unexpected HTTP response.",
            "Suggestions": "Ensure that the service port for WebHDFS tokens is correctly configured in the YARN settings. Modify the token renewal logic to use the appropriate WebHDFS port instead of the RPC port.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                    "hadoop-yarn-project/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.web.WebHdfsFileSystem",
                    "org.apache.hadoop.security.UserGroupInformation",
                    "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"
                ],
                "methods": [
                    "WebHdfsFileSystem.renewDelegationToken",
                    "UserGroupInformation.doAs",
                    "DelegationTokenRenewer.handleAppSubmitEvent"
                ]
            },
            "possible_fix": "Modify the method that generates the delegation token to ensure that it sets the correct service port for WebHDFS tokens. This may involve updating the configuration settings for YARN to recognize the WebHDFS service port."
        },
        "possible_fix_code": {
            "WebHdfsFileSystem.renewDelegationToken": "  public synchronized long renewDelegationToken(final Token<?> token) throws IOException {\n    final HttpOpParam.Op op = PutOpParam.Op.RENEWDELEGATIONTOKEN;\n    // Ensure we are using the correct WebHDFS port instead of the RPC port\n    Configuration conf = getConf();\n    String webHdfsPort = conf.get(\"dfs.webhdfs.port\", \"50070\");\n    String url = String.format(\"http://%s:%s/webhdfs/v1/renewDelegationToken?token=%s&op=RENEWDELEGATIONTOKEN\", getNameNodeAddress(), webHdfsPort, token.encodeToUrlString());\n    return new FsPathResponseRunner<Long>(op, url, null) {\n      @Override\n      Long decodeResponse(Map<?,?> json) throws IOException {\n        return (Long) json.get(\"long\");\n      }\n    }.run();\n  }"
        }
    },
    {
        "filename": "HDFS-13721.json",
        "creation_time": "2018-07-05T20:11:57.000+0000",
        "bug_report": {
            "Title": "NPE in DataNode due to uninitialized DiskBalancer",
            "Description": "A NullPointerException (NPE) occurs in the DataNode when attempting to retrieve the DiskBalancer status via JMX. This issue arises because the DiskBalancer object is not properly initialized before it is accessed, leading to a failure when the method getDiskBalancerStatus is called. The exception is thrown during the DataNode startup process, indicating that the DiskBalancer's state is not set up correctly.",
            "StackTrace": [
                "javax.management.RuntimeMBeanException: java.lang.NullPointerException",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:839)",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrowMaybeMBeanException(DefaultMBeanServerInterceptor.java:852)",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:651)",
                "at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)",
                "at org.apache.hadoop.jmx.JMXJsonServlet.writeAttribute(JMXJsonServlet.java:338)",
                "at org.apache.hadoop.jmx.JMXJsonServlet.listBeans(JMXJsonServlet.java:316)",
                "at org.apache.hadoop.jmx.JMXJsonServlet.doGet(JMXJsonServlet.java:210)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)",
                "at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)",
                "at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)",
                "at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)",
                "at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)",
                "at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
                "at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)",
                "at org.eclipse.jetty.server.Server.handle(Server.java:534)",
                "at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)",
                "at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)",
                "at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)",
                "at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)",
                "at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)",
                "at java.lang.Thread.run(Thread.java:748)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.getDiskBalancerStatus(DataNode.java:3146)"
            ],
            "RootCause": "The root cause of the issue is that the DiskBalancer object is not initialized before it is accessed in the getDiskBalancerStatus method of the DataNode class. This leads to a NullPointerException when the method attempts to call queryWorkStatus on a null DiskBalancer.",
            "StepsToReproduce": [
                "Start the DataNode service.",
                "Attempt to access the DiskBalancer status via JMX."
            ],
            "ExpectedBehavior": "The system should return the DiskBalancer status without throwing an exception, even if the status is empty or indicates that no balancing is currently occurring.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to access the DiskBalancer status, causing the JMX request to fail.",
            "Suggestions": "Ensure that the DiskBalancer object is properly initialized before it is accessed in the getDiskBalancerStatus method. Consider adding a check to return a default value or an appropriate exception if the DiskBalancer is not initialized.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/jmx/JMXJsonServlet.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.DataNode",
                    "org.apache.hadoop.jmx.JMXJsonServlet"
                ],
                "methods": [
                    "DataNode.getDiskBalancerStatus",
                    "JMXJsonServlet.doGet"
                ]
            },
            "possible_fix": "In the DataNode class, modify the getDiskBalancerStatus method to check if the DiskBalancer is initialized before calling queryWorkStatus. If it is not initialized, return an appropriate default value or throw an IOException instead of a NullPointerException."
        },
        "possible_fix_code": {
            "DataNode.getDiskBalancerStatus": "  public String getDiskBalancerStatus() {\n    if (this.diskBalancer == null) {\n      LOG.warn(\"DiskBalancer is not initialized.\");\n      return \"DiskBalancer not initialized\";\n    }\n    try {\n      return this.diskBalancer.queryWorkStatus().toJsonString();\n    } catch (IOException ex) {\n      LOG.debug(\"Reading diskbalancer Status failed. ex:{},\", ex);\n      return \"\";\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-7180.json",
        "creation_time": "2014-10-02T03:07:04.000+0000",
        "bug_report": {
            "Title": "NFSv3 gateway frequently gets stuck due to GC",
            "Description": "The NFSv3 gateway is experiencing frequent hangs, particularly after prolonged operation (approximately one day) and significant data uploads (hundreds of GBs). The issue manifests as the NFS daemon becoming unresponsive, with repeated log entries indicating that the server is not responding. This behavior occurs despite the HDFS operations functioning normally, suggesting a potential issue with the NFS layer rather than the underlying HDFS. The logs indicate that the NFS daemon is unable to process requests, leading to a situation where commands like 'ls' and 'df -hT' hang indefinitely.",
            "StackTrace": [
                "java.io.IOException: Bad response ERROR for block BP-1960069741-10.0.3.170-1410430543652:blk_1074363564_623643 from datanode 10.0.3.176:50010",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:828)"
            ],
            "RootCause": "The root cause appears to be related to the NFSv3 daemon's inability to handle requests properly, potentially due to garbage collection (GC) issues or network problems with the datanode at 10.0.3.176, which is reported as 'bad' in the logs. The NFS daemon may be getting overwhelmed by requests or unable to recover from errors in communication with the HDFS datanodes.",
            "StepsToReproduce": [
                "1. Start the NFSv3 gateway on a node in the Hadoop cluster.",
                "2. Mount the NFS on the same node.",
                "3. Upload several hundreds of GBs of data using rsync.",
                "4. Monitor the NFS daemon and check for responsiveness after approximately one day."
            ],
            "ExpectedBehavior": "The NFSv3 gateway should remain responsive and able to handle requests without hanging, even under heavy load.",
            "ObservedBehavior": "The NFSv3 daemon becomes unresponsive, with repeated log messages indicating that the server is not responding, and commands like 'ls' and 'df -hT' hang indefinitely.",
            "Suggestions": "Consider reviewing the garbage collection settings for the JVM running the NFS daemon. Additionally, investigate the health of the datanode at 10.0.3.176 to ensure it is functioning correctly. Implementing monitoring for NFS performance and potential network issues may also help identify the root cause.",
            "problem_location": {
                "files": [
                    "NFSv3Gateway.java",
                    "DFSOutputStream.java"
                ],
                "classes": [
                    "org.apache.hadoop.nfs.nfs3.NFSv3Gateway",
                    "org.apache.hadoop.hdfs.DFSOutputStream"
                ],
                "methods": [
                    "NFSv3Gateway.handleRequest",
                    "DFSOutputStream.DataStreamer.ResponseProcessor.run"
                ]
            },
            "possible_fix": "Review and optimize the garbage collection settings for the NFSv3 daemon. Ensure that the datanode at 10.0.3.176 is healthy and consider implementing retry logic or timeout handling in the NFS request processing to prevent hangs."
        },
        "possible_fix_code": {
            "NFSv3Gateway.handleRequest": "// Assuming this method exists and is responsible for handling requests\npublic void handleRequest(Request request) {\n    try {\n        // Process the request\n        // Implement retry logic or timeout handling here\n        processRequest(request);\n    } catch (IOException e) {\n        // Log the error and handle it gracefully\n        log.error(\"Error processing request: \" + e.getMessage());\n        // Optionally implement a retry mechanism\n        retryRequest(request);\n    }\n}\n\nprivate void retryRequest(Request request) {\n    int retries = 3;\n    while (retries > 0) {\n        try {\n            // Attempt to process the request again\n            processRequest(request);\n            return; // Exit if successful\n        } catch (IOException e) {\n            log.warn(\"Retrying request due to error: \" + e.getMessage());\n            retries--;\n            if (retries == 0) {\n                log.error(\"Failed to process request after retries\");\n            }\n        }\n    }\n}\n\nprivate void processRequest(Request request) throws IOException {\n    // Actual request processing logic goes here\n}"
        }
    },
    {
        "filename": "HDFS-11377.json",
        "creation_time": "2017-01-26T23:40:53.000+0000",
        "bug_report": {
            "Title": "Balancer hung due to no available mover threads",
            "Description": "The balancer process hangs when attempting to balance a large cluster with over 3000 Datanodes. The issue arises when there are no available mover threads, causing the Dispatcher to wait indefinitely for move completion. The stack trace indicates that the main thread is in a TIMED_WAITING state, repeatedly calling sleep while waiting for the completion of block moves. The logs show multiple warnings about the unavailability of mover threads, which leads to the condition where the Datanode's pending queue is not empty, causing the balancer to hang.",
            "StackTrace": [
                "\"main\" #1 prio=5 os_prio=0 tid=0x00007ff6cc014800 nid=0x6b2c waiting on condition [0x00007ff6d1bad000]",
                "java.lang.Thread.State: TIMED_WAITING (sleeping)",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher.waitForMoveCompletion(Dispatcher.java:1043)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchBlockMoves(Dispatcher.java:1017)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchAndCheckContinue(Dispatcher.java:981)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.runOneIteration(Balancer.java:611)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:663)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer$Cli.run(Balancer.java:776)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:905)"
            ],
            "RootCause": "The root cause of the issue is that when there are no mover threads available, the method DDatanode.isPendingQEmpty() returns false, leading to the balancer hanging indefinitely while waiting for move completion.",
            "StepsToReproduce": [
                "1. Set up a large cluster with more than 3000 Datanodes.",
                "2. Initiate the balancer process.",
                "3. Monitor the logs for warnings about 'No mover threads available'.",
                "4. Observe that the balancer hangs indefinitely."
            ],
            "ExpectedBehavior": "The balancer should complete its operation without hanging, even when there are no mover threads available.",
            "ObservedBehavior": "The balancer hangs indefinitely, waiting for move completion due to the absence of available mover threads.",
            "Suggestions": "Consider increasing the number of mover threads available for the balancer or implementing a timeout mechanism to prevent indefinite waiting.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.balancer.Dispatcher",
                    "org.apache.hadoop.hdfs.server.balancer.Balancer"
                ],
                "methods": [
                    "Dispatcher.waitForMoveCompletion",
                    "Dispatcher.dispatchBlockMoves",
                    "Balancer.runOneIteration"
                ]
            },
            "possible_fix": "To resolve this issue, increase the number of mover threads in the balancer configuration. Additionally, implement a timeout in the waitForMoveCompletion method to break the loop if no progress is made after a certain period."
        },
        "possible_fix_code": {
            "Dispatcher.waitForMoveCompletion": "  public static boolean waitForMoveCompletion(\n      Iterable<? extends StorageGroup> targets) {\n    boolean hasFailure = false;\n    long timeout = 60000; // 60 seconds timeout\n    long startTime = System.currentTimeMillis();\n    for (;;) {\n      boolean empty = true;\n      for (StorageGroup t : targets) {\n        if (!t.getDDatanode().isPendingQEmpty()) {\n          empty = false;\n          break;\n        } else {\n          hasFailure |= t.getDDatanode().hasFailure;\n        }\n      }\n      if (empty) {\n        return hasFailure; // all pending queues are empty\n      }\n      if (System.currentTimeMillis() - startTime > timeout) {\n        LOG.warn(\"Timeout reached while waiting for move completion.\");\n        return hasFailure; // return hasFailure even if timeout occurs\n      }\n      try {\n        Thread.sleep(1000);\n      } catch (InterruptedException ignored) {\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-6753.json",
        "creation_time": "2014-07-25T11:36:43.000+0000",
        "bug_report": {
            "Title": "Initialize checkDisk when DirectoryScanner not able to get files list for scanning",
            "Description": "The DataNode fails to shut down when all configured volumes are marked as failed due to permission issues or when the disk is full. This occurs because the DirectoryScanner is unable to list files in the specified directory, leading to an IOException. The expected behavior is that the DataNode should shut down when it detects that all volumes are failed, but this is not happening under certain conditions.",
            "StackTrace": [
                "2014-07-21 14:10:52,814 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: XX1.XX1.XX1.XX1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /XX2.XX2.XX2.XX2:10106 dst: /XX1.XX1.XX1.XX1:50010",
                "org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=4096 B) is less than the block size (=134217728 B).",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:60)",
                "2014-07-21 14:13:00,180 WARN org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Exception occurred while compiling report:",
                "java.io.IOException: Invalid directory or I/O error occurred for dir: /mnt/tmp_Datanode/current/BP-1384489961-XX2.XX2.XX2.XX2-845784615183/current/finalized",
                "at org.apache.hadoop.fs.FileUtil.listFiles(FileUtil.java:1164)",
                "at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.compileReport(DirectoryScanner.java:596)"
            ],
            "RootCause": "The DataNode does not properly handle the case where all volumes are marked as failed due to permission issues or full disks. The DirectoryScanner fails to list files, resulting in an IOException, but this does not trigger a shutdown of the DataNode.",
            "StepsToReproduce": [
                "1. Set the permissions of /mnt/tmp_Datanode to root.",
                "2. Fill the /mnt/tmp_Datanode disk to capacity.",
                "3. Attempt to perform write operations on the DataNode."
            ],
            "ExpectedBehavior": "The DataNode should shut down when it detects that all configured volumes are failed.",
            "ObservedBehavior": "The DataNode does not shut down even when all configured volumes are marked as failed due to permission issues or when the disk is full.",
            "Suggestions": "Implement a check in the DataNode to ensure that it shuts down when all volumes are marked as failed, especially when an IOException occurs during directory scanning.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy.java",
                    "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileUtil.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy",
                    "org.apache.hadoop.fs.FileUtil",
                    "org.apache.hadoop.hdfs.server.datanode.DirectoryScanner"
                ],
                "methods": [
                    "RoundRobinVolumeChoosingPolicy.chooseVolume",
                    "FileUtil.listFiles",
                    "DirectoryScanner$ReportCompiler.compileReport"
                ]
            },
            "possible_fix": "In the RoundRobinVolumeChoosingPolicy.chooseVolume method, ensure that the DataNode checks for the state of all volumes and triggers a shutdown if all are marked as failed. Additionally, handle IOException in the DirectoryScanner to ensure that it properly reports failure and triggers a shutdown."
        },
        "possible_fix_code": {
            "RoundRobinVolumeChoosingPolicy.chooseVolume": "  public synchronized V chooseVolume(final List<V> volumes, final long blockSize\n      ) throws IOException {\n    if(volumes.size() < 1) {\n      throw new DiskOutOfSpaceException(\"No more available volumes\");\n    }\n    \n    // since volumes could've been removed because of the failure\n    // make sure we are not out of bounds\n    if(curVolume >= volumes.size()) {\n      curVolume = 0;\n    }\n    \n    int startVolume = curVolume;\n    long maxAvailable = 0;\n    \n    while (true) {\n      final V volume = volumes.get(curVolume);\n      curVolume = (curVolume + 1) % volumes.size();\n      long availableVolumeSize = volume.getAvailable();\n      if (availableVolumeSize > blockSize) { return volume; }\n      \n      if (availableVolumeSize > maxAvailable) {\n        maxAvailable = availableVolumeSize;\n      }\n      \n      if (curVolume == startVolume) {\n        // Check if all volumes are marked as failed\n        if (maxAvailable < blockSize) {\n          // Trigger shutdown of DataNode\n          throw new DiskOutOfSpaceException(\"All volumes are marked as failed. DataNode should shut down.\");\n        }\n        throw new DiskOutOfSpaceException(\"Out of space: \n            + \"The volume with the most available space (=\" + maxAvailable\n            + \" B) is less than the block size (=\" + blockSize + \" B).\");\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-3443.json",
        "creation_time": "2012-05-18T11:30:50.000+0000",
        "bug_report": {
            "Title": "Fix NPE when namenode transition to active during startup by adding checkNNStartup() in NameNodeRpcServer",
            "Description": "During the startup of the NameNode, if the NameNode transitions to active while the edit log tailer is being initialized, a NullPointerException (NPE) is thrown. This occurs because the edit log is not properly initialized before the catchupDuringFailover() method is called. The issue arises in the startActiveServices() method of the FSNamesystem class, where the edit log is expected to be open for writing but is not, leading to the NPE when attempting to catch up with the old active's edits.",
            "StackTrace": [
                "2012-05-18 16:51:27,586 WARN org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call org.apache.hadoop.ha.HAServiceProtocol.transitionToActive from XX.XX.XX.55:58004: error: java.lang.NullPointerException",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:602)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1287)",
                "\tat org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)",
                "\tat org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:63)",
                "\tat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1219)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:978)",
                "\tat org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)"
            ],
            "RootCause": "The root cause of the NPE is that the edit log is not initialized properly before the catchupDuringFailover() method is called in the startActiveServices() method. This leads to a situation where the edit log is expected to be available but is null, resulting in a NullPointerException.",
            "StepsToReproduce": [
                "Start the NameNode in standby mode.",
                "Trigger the transition to active state while the edit log tailer is being initialized."
            ],
            "ExpectedBehavior": "The NameNode should transition to active state without throwing a NullPointerException, and the edit log should be properly initialized before any operations are performed on it.",
            "ObservedBehavior": "A NullPointerException is thrown when the NameNode attempts to catch up with the old active's edits during the transition to active state.",
            "Suggestions": "Add a check to ensure that the edit log is properly initialized before calling catchupDuringFailover(). This can be done by adding a call to checkNNStartup() to verify the state of the NameNode and the edit log.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem",
                    "org.apache.hadoop.hdfs.server.namenode.NameNode"
                ],
                "methods": [
                    "FSNamesystem.startActiveServices",
                    "NameNode.transitionToActive"
                ]
            },
            "possible_fix": "In the startActiveServices() method, before calling editLogTailer.catchupDuringFailover(), add a check to ensure that the edit log is initialized. For example:\n\nif (editLog == null) {\n    throw new IOException(\"Edit log is not initialized\");\n}\n\nThis will prevent the NPE from occurring."
        },
        "possible_fix_code": {
            "FSNamesystem.startActiveServices": "  void startActiveServices() throws IOException {\n    LOG.info(\"Starting services required for active state\");\n    writeLock();\n    try {\n      FSEditLog editLog = dir.fsImage.getEditLog();\n      \n      if (editLog == null) {\n        throw new IOException(\"Edit log is not initialized\");\n      }\n\n      if (!editLog.isOpenForWrite()) {\n        // During startup, we're already open for write during initialization.\n        editLog.initJournalsForWrite();\n        // May need to recover\n        editLog.recoverUnclosedStreams();\n        \n        LOG.info(\"Catching up to latest edits from old active before \" +\n            \"taking over writer role in edits logs.\");\n        editLogTailer.catchupDuringFailover();\n        \n        LOG.info(\"Reprocessing replication and invalidation queues...\");\n        blockManager.getDatanodeManager().markAllDatanodesStale();\n        blockManager.clearQueues();\n        blockManager.processAllPendingDNMessages();\n        blockManager.processMisReplicatedBlocks();\n        \n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"NameNode metadata after re-processing \" +\n              \"replication and invalidation queues during failover:\\n\" +\n              metaSaveAsString());\n        }\n        \n        long nextTxId = dir.fsImage.getLastAppliedTxId() + 1;\n        LOG.info(\"Will take over writing edit logs at txnid \" + \n            nextTxId);\n        editLog.setNextTxId(nextTxId);\n\n        dir.fsImage.editLog.openForWrite();\n      }\n      if (haEnabled) {\n        // Renew all of the leases before becoming active.\n        // This is because, while we were in standby mode,\n        // the leases weren't getting renewed on this NN.\n        // Give them all a fresh start here.\n        leaseManager.renewAllLeases();\n      }\n      leaseManager.startMonitor();\n      startSecretManagerIfNecessary();\n    } finally {\n      writeUnlock();\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-11479.json",
        "creation_time": "2017-03-01T11:08:07.000+0000",
        "bug_report": {
            "Title": "Socket re-use address option should be used in SimpleUdpServer",
            "Description": "The NFS gateway restart can fail due to a bind error in the SimpleUdpServer. This occurs when the server attempts to bind to a UDP port that is still in the TIME_WAIT state, leading to a BindException. To resolve this, the socket option for reusing the address should be enabled in the SimpleUdpServer to allow binding to the port even when it is in the TIME_WAIT state.",
            "StackTrace": [
                "2017-02-28 04:19:53,495 FATAL mount.MountdBase (MountdBase.java:startUDPServer(66)) - Failed to start the UDP server.",
                "org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:4242",
                "at org.jboss.netty.bootstrap.ConnectionlessBootstrap.bind(ConnectionlessBootstrap.java:204)",
                "at org.apache.hadoop.oncrpc.SimpleUdpServer.run(SimpleUdpServer.java:68)",
                "at org.apache.hadoop.mount.MountdBase.startUDPServer(MountdBase.java:64)",
                "at org.apache.hadoop.mount.MountdBase.start(MountdBase.java:97)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:71)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)",
                "Caused by: java.net.BindException: Address already in use",
                "at sun.nio.ch.Net.bind0(Native Method)",
                "at sun.nio.ch.Net.bind(Net.java:433)",
                "at sun.nio.ch.DatagramChannelImpl.bind(DatagramChannelImpl.java:691)",
                "at sun.nio.ch.DatagramSocketAdaptor.bind(DatagramSocketAdaptor.java:91)",
                "at org.jboss.netty.channel.socket.nio.NioDatagramPipelineSink.bind(NioDatagramPipelineSink.java:129)",
                "at org.jboss.netty.channel.socket.nio.NioDatagramPipelineSink.eventSunk(NioDatagramPipelineSink.java:77)",
                "at org.jboss.netty.channel.Channels.bind(Channels.java:561)",
                "at org.jboss.netty.channel.AbstractChannel.bind(AbstractChannel.java:189)",
                "at org.jboss.netty.bootstrap.ConnectionlessBootstrap.bind(ConnectionlessBootstrap.java:198)"
            ],
            "RootCause": "The root cause of the issue is that the SimpleUdpServer is attempting to bind to a UDP port that is already in use, specifically when it is in the TIME_WAIT state. This results in a BindException.",
            "StepsToReproduce": [
                "Restart the NFS gateway service.",
                "Observe the logs for the bind error related to the UDP server."
            ],
            "ExpectedBehavior": "The UDP server should start successfully without any bind errors, even if the port was recently used.",
            "ObservedBehavior": "The UDP server fails to start due to a BindException indicating that the address is already in use.",
            "Suggestions": "Implement the socket option for reusing the address in the SimpleUdpServer to allow binding to the port in the TIME_WAIT state.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/SimpleUdpServer.java",
                    "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/Nfs3.java",
                    "hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/mount/MountdBase.java"
                ],
                "classes": [
                    "org.apache.hadoop.oncrpc.SimpleUdpServer",
                    "org.apache.hadoop.mount.MountdBase",
                    "org.apache.hadoop.hdfs.nfs.nfs3.Nfs3"
                ],
                "methods": [
                    "SimpleUdpServer.run",
                    "MountdBase.startUDPServer",
                    "Nfs3.startServiceInternal"
                ]
            },
            "possible_fix": "In the SimpleUdpServer class, modify the run method to include the option for reusing the address when binding the socket. This can be done by adding the following line before the bind call:\n\nserver.setOption('reuseAddress', true);"
        },
        "possible_fix_code": {
            "SimpleUdpServer.run": "  public void run() {\n    // Configure the client.\n    DatagramChannelFactory f = new NioDatagramChannelFactory(\n        Executors.newCachedThreadPool(), workerCount);\n\n    server = new ConnectionlessBootstrap(f);\n    server.setPipeline(Channels.pipeline(RpcUtil.STAGE_RPC_MESSAGE_PARSER,\n        rpcProgram, RpcUtil.STAGE_RPC_UDP_RESPONSE));\n\n    server.setOption(\"broadcast\", \"false\");\n    server.setOption(\"sendBufferSize\", SEND_BUFFER_SIZE);\n    server.setOption(\"receiveBufferSize\", RECEIVE_BUFFER_SIZE);\n\n    // Set the option to reuse the address\n    server.setOption(\"reuseAddress\", true);\n\n    // Listen to the UDP port\n    ch = server.bind(new InetSocketAddress(port));\n    InetSocketAddress socketAddr = (InetSocketAddress) ch.getLocalAddress();\n    boundPort = socketAddr.getPort();\n\n    LOG.info(\"Started listening to UDP requests at port \" + boundPort + \" for \"\n        + rpcProgram + \" with workerCount \" + workerCount);\n  }"
        }
    },
    {
        "filename": "HDFS-8055.json",
        "creation_time": "2015-04-02T23:21:55.000+0000",
        "bug_report": {
            "Title": "NullPointerException when topology script is missing.",
            "Description": "The NameNode encounters a NullPointerException when attempting to retrieve block locations for files, specifically when the topology script is not present. This issue arises during the execution of the `getBlockLocations` method in the `NameNodeRpcServer`, which subsequently calls `sortLocatedBlocks` in the `DatanodeManager`. The absence of the topology script leads to a failure in resolving the block locations, resulting in a NullPointerException. Improving the validation logic to check for the existence of the topology script before proceeding with block location retrieval could prevent this error.",
            "StackTrace": [
                "org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedBlocks(DatanodeManager.java:359)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1789)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)"
            ],
            "RootCause": "The root cause of the NullPointerException is the failure to handle the case where the topology script is missing, leading to null values being passed to methods that expect valid objects.",
            "StepsToReproduce": [
                "1. Ensure that the topology script is not present in the expected location.",
                "2. Attempt to access the NameNode and request block locations for a file.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The system should validate the presence of the topology script and provide a meaningful error message instead of throwing a NullPointerException.",
            "ObservedBehavior": "The system throws a NullPointerException when attempting to retrieve block locations due to the missing topology script.",
            "Suggestions": "Implement a check for the existence of the topology script before attempting to retrieve block locations. If the script is missing, return a user-friendly error message indicating the issue.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager",
                    "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager",
                    "org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer"
                ],
                "methods": [
                    "DatanodeManager.sortLocatedBlocks",
                    "FSNamesystem.getBlockLocations",
                    "NameNodeRpcServer.getBlockLocations"
                ]
            },
            "possible_fix": "Add a validation check in the `getBlockLocations` method of the `NameNodeRpcServer` class to ensure that the topology script is present. If it is missing, throw a custom exception or return an error response indicating the absence of the topology script."
        },
        "possible_fix_code": {
            "NameNodeRpcServer.getBlockLocations": "  public LocatedBlocks getBlockLocations(String src, \n                                          long offset, \n                                          long length) \n      throws IOException {\n    checkNNStartup();\n    metrics.incrGetBlockLocations();\n    // Check if the topology script is present\n    if (!isTopologyScriptPresent()) {\n        throw new IOException(\"Topology script is missing. Please ensure it is present before requesting block locations.\");\n    }\n    return namesystem.getBlockLocations(getClientMachine(), \n                                        src, offset, length);\n  }\n\n  private boolean isTopologyScriptPresent() {\n      // Implement the logic to check for the presence of the topology script\n      // This is a placeholder for the actual implementation\n      // For example, check a specific path or configuration\n      return new File(\"/path/to/topology/script\").exists();\n  }"
        }
    },
    {
        "filename": "HDFS-6533.json",
        "creation_time": "2014-06-14T16:15:02.000+0000",
        "bug_report": {
            "Title": "TestBPOfferService#testBasicFunctionality test fails intermittently",
            "Description": "The test 'testBasicFunctionality' in the 'TestBPOfferService' class is failing intermittently during automated builds, while it passes consistently when run locally. The failure is due to the expected interaction with the mock object 'datanodeProtocolClientSideTranslatorPB.registerDatanode' not occurring, leading to a verification error from Mockito. This suggests that the test may be dependent on certain conditions that are not consistently met in the CI environment.",
            "StackTrace": [
                "org.mockito.exceptions.verification.WantedButNotInvoked: Wanted but not invoked: datanodeProtocolClientSideTranslatorPB.registerDatanode(<any>);",
                "-> at org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBasicFunctionality(TestBPOfferService.java:175)",
                "Actually, there were zero interactions with this mock."
            ],
            "RootCause": "The test is failing because the method 'registerDatanode' on the mock 'datanodeProtocolClientSideTranslatorPB' is not being called during the execution of 'testBasicFunctionality'. This could be due to the test setup not correctly simulating the conditions under which this method should be invoked.",
            "StepsToReproduce": [
                "Run the test 'TestBPOfferService.testBasicFunctionality' in a CI environment.",
                "Observe the failure related to the mock interaction."
            ],
            "ExpectedBehavior": "The method 'registerDatanode' should be invoked at least once during the execution of 'testBasicFunctionality'.",
            "ObservedBehavior": "The test fails with a verification error indicating that 'registerDatanode' was never called.",
            "Suggestions": "Review the test setup to ensure that all necessary conditions for invoking 'registerDatanode' are met. Consider adding logging to track the flow of execution and verify that the expected interactions occur.",
            "problem_location": {
                "files": [
                    "TestBPOfferService.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.TestBPOfferService"
                ],
                "methods": [
                    "testBasicFunctionality"
                ]
            },
            "possible_fix": "Ensure that the mock 'datanodeProtocolClientSideTranslatorPB' is correctly set up and that the conditions leading to the invocation of 'registerDatanode' are met. This may involve adjusting the test's initial state or the inputs provided to the method under test."
        },
        "possible_fix_code": {
            "testBasicFunctionality": "public void testBasicFunctionality() {\n    // Arrange\n    DatanodeProtocolClientSideTranslatorPB datanodeProtocolClientSideTranslatorPB = Mockito.mock(DatanodeProtocolClientSideTranslatorPB.class);\n    // Set up the necessary conditions for the test\n    // For example, if there are specific inputs or states that need to be set\n    // Ensure that the mock is used in the context of the test\n\n    // Act\n    // Call the method that should trigger the registerDatanode call\n    // For example:\n    // someService.methodThatShouldInvokeRegisterDatanode(datanodeProtocolClientSideTranslatorPB);\n\n    // Assert\n    Mockito.verify(datanodeProtocolClientSideTranslatorPB, Mockito.times(1)).registerDatanode(Mockito.any());\n}"
        }
    },
    {
        "filename": "HDFS-10609.json",
        "creation_time": "2016-07-11T17:14:39.000+0000",
        "bug_report": {
            "Title": "Uncaught InvalidEncryptionKeyException during pipeline recovery may abort downstream applications",
            "Description": "During normal operations, the system is designed to handle InvalidEncryptionKeyException gracefully by retrying the connection after fetching a new encryption key. However, when this exception occurs during pipeline recovery, it is not caught properly, leading to an uncaught exception that propagates to downstream applications like SOLR, causing them to abort their operations. This behavior deviates from the expected error handling mechanism in the system.",
            "StackTrace": [
                "2016-07-06 12:12:51,992 ERROR org.apache.solr.update.HdfsTransactionLog: Exception closing tlog.",
                "org.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=557709482) doesn't exist. Current key: 1350592619",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:1308)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1272)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1433)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1147)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:632)"
            ],
            "RootCause": "The root cause of the issue is that the InvalidEncryptionKeyException is not being caught during the pipeline recovery process, which leads to an uncaught exception that propagates to downstream applications.",
            "StepsToReproduce": [
                "1. Start a pipeline with a valid encryption key.",
                "2. Simulate a scenario where the encryption key becomes invalid during pipeline recovery.",
                "3. Observe the behavior of downstream applications like SOLR."
            ],
            "ExpectedBehavior": "The system should catch the InvalidEncryptionKeyException during pipeline recovery, fetch a new encryption key, and retry the operation without propagating the exception to downstream applications.",
            "ObservedBehavior": "The InvalidEncryptionKeyException is thrown during pipeline recovery and is not caught, causing downstream applications like SOLR to abort their operations.",
            "Suggestions": "Implement proper exception handling for InvalidEncryptionKeyException during pipeline recovery to ensure that it is caught and retried, similar to the handling in createBlockOutputStream().",
            "problem_location": {
                "files": [
                    "SaslDataTransferClient.java",
                    "DataTransferSaslUtil.java",
                    "DFSOutputStream.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient",
                    "org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil",
                    "org.apache.hadoop.hdfs.DFSOutputStream"
                ],
                "methods": [
                    "SaslDataTransferClient.doSaslHandshake",
                    "SaslDataTransferClient.getEncryptedStreams",
                    "DFSOutputStream$DataStreamer.processDatanodeError"
                ]
            },
            "possible_fix": "In the method DFSOutputStream$DataStreamer.processDatanodeError, add a catch block for InvalidEncryptionKeyException to handle it appropriately, similar to the logic in createBlockOutputStream(). This will ensure that the exception is contained and retried without affecting downstream applications."
        },
        "possible_fix_code": {
            "DFSOutputStream$DataStreamer.processDatanodeError": "  public void processDatanodeError(DatanodeID datanodeId, IOException e) {\n    if (e instanceof InvalidEncryptionKeyException) {\n        LOG.warn(\"Invalid encryption key encountered. Attempting to fetch a new key and retry.\", e);\n        try {\n            // Logic to fetch a new encryption key\n            fetchNewEncryptionKey();\n            // Retry the operation after fetching the new key\n            retryOperation();\n        } catch (Exception retryException) {\n            LOG.error(\"Failed to recover from InvalidEncryptionKeyException\", retryException);\n            // Handle the failure to recover, possibly rethrow or log\n        }\n    } else {\n        // Handle other types of exceptions\n        LOG.error(\"Encountered an error with datanode: \" + datanodeId, e);\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-2310.json",
        "creation_time": "2011-09-05T15:31:00.000+0000",
        "bug_report": {
            "Title": "TestBackupNode fails since HADOOP-7524 went in.",
            "Description": "The TestBackupNode is failing due to an unregistered JournalProtocol with the server, leading to an IOException indicating an unknown protocol. This issue arises when the server attempts to start a log segment but cannot recognize the JournalProtocol, which is essential for the operation of the NameNode in Hadoop's architecture.",
            "StackTrace": [
                "2011-09-05 10:44:36,811 INFO  ipc.Server (Server.java:run(1497)) - IPC Server handler 0 on 60758, call: startLogSegment(NamenodeRegistration(localhost:60754, role=NameNode), 3), rpc version=2, client version=1, methodsFingerPrint=-852377201 from 127.0.0.1:60760, error:",
                "java.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.hdfs.server.protocol.JournalProtocol",
                "at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)",
                "2011-09-05 10:44:36,813 ERROR namenode.FSEditLog (FSEditLog.java:mapJournalsAndReportErrors(1048)) - Error starting log segment 3 (journal JournalAndStream(mgr=org.apache.hadoop.hdfs.server.namenode.BackupJournalManager@2876b359, stream=null))",
                "java.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.hdfs.server.protocol.JournalProtocol",
                "at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)"
            ],
            "RootCause": "The root cause of the issue is that the JournalProtocol is not registered with the server, which prevents the server from recognizing the protocol when attempting to start a log segment.",
            "StepsToReproduce": [
                "Start the Hadoop NameNode.",
                "Run the TestBackupNode test.",
                "Observe the logs for the IOException related to the unknown JournalProtocol."
            ],
            "ExpectedBehavior": "The TestBackupNode should successfully start the log segment without any protocol errors.",
            "ObservedBehavior": "The TestBackupNode fails with an IOException indicating an unknown protocol: org.apache.hadoop.hdfs.server.protocol.JournalProtocol.",
            "Suggestions": "Ensure that the JournalProtocol is properly registered with the server before attempting to start log segments. Review the changes made in HADOOP-7524 to identify any modifications that may have affected the registration process.",
            "problem_location": {
                "files": [
                    "Server.java",
                    "FSEditLog.java",
                    "UserGroupInformation.java"
                ],
                "classes": [
                    "org.apache.hadoop.ipc.Server",
                    "org.apache.hadoop.hdfs.server.namenode.FSEditLog",
                    "org.apache.hadoop.security.UserGroupInformation"
                ],
                "methods": [
                    "Server.run",
                    "FSEditLog.mapJournalsAndReportErrors",
                    "UserGroupInformation.doAs"
                ]
            },
            "possible_fix": "To fix the issue, ensure that the JournalProtocol is registered correctly in the server's initialization code. This may involve reviewing the changes introduced in HADOOP-7524 and ensuring that the protocol registration logic is executed as expected."
        },
        "possible_fix_code": {
            "Server.run": "  public void run() {\n    // Ensure that the JournalProtocol is registered before starting the server\n    registerProtocols();\n    try {\n      // Existing server run logic\n      while (true) {\n        // Handle incoming requests\n      }\n    } catch (IOException e) {\n      LOG.error(\"Error in IPC Server\", e);\n    }\n  }\n\n  private void registerProtocols() {\n    // Register the JournalProtocol with the server\n    ProtocolMetaData.registerProtocol(JournalProtocol.class);\n  }"
        }
    },
    {
        "filename": "HDFS-3385.json",
        "creation_time": "2012-05-08T12:45:31.000+0000",
        "bug_report": {
            "Title": "ClassCastException when trying to append a file",
            "Description": "A ClassCastException occurs when attempting to append to a file in HDFS. The exception indicates that an object of type BlockInfo is being incorrectly cast to BlockInfoUnderConstruction. This issue arises during the lease recovery process when the system tries to handle a file that is under construction but is not recognized as such due to a type mismatch.",
            "StackTrace": [
                "2012-05-08 18:13:40,506 WARN  util.KerberosName (KerberosName.java:<clinit>(87)) - Kerberos krb5 configuration not found, setting default realm to empty",
                "Exception in thread \"main\" java.lang.ClassCastException: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo cannot be cast to org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:1787)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1584)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:1824)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:425)",
                "at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1150)",
                "at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1189)",
                "at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1177)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:221)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:1)",
                "at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:981)",
                "at org.apache.hadoop.hdfs.server.datanode.DeleteMe.main(DeleteMe.java:26)"
            ],
            "RootCause": "The root cause of the ClassCastException is that the code is attempting to cast an instance of BlockInfo to BlockInfoUnderConstruction in the recoverLeaseInternal method, which is not valid if the file is not actually under construction.",
            "StepsToReproduce": [
                "1. Attempt to append to a file in HDFS that is currently under construction.",
                "2. Ensure that the file's state is not properly recognized as under construction.",
                "3. Observe the ClassCastException in the logs."
            ],
            "ExpectedBehavior": "The system should successfully append to the file without throwing a ClassCastException, handling the lease recovery correctly.",
            "ObservedBehavior": "A ClassCastException is thrown, indicating that the system is trying to cast BlockInfo to BlockInfoUnderConstruction incorrectly.",
            "Suggestions": "Review the logic in the recoverLeaseInternal method to ensure that the type of the file inode is checked before casting. Implement proper handling for files that are not under construction.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem"
                ],
                "methods": [
                    "recoverLeaseInternal",
                    "startFileInternal",
                    "appendFile"
                ]
            },
            "possible_fix": "In the recoverLeaseInternal method, add a check to ensure that the fileInode is indeed an instance of INodeFileUnderConstruction before casting it to prevent the ClassCastException. For example:\n\nif (fileInode instanceof INodeFileUnderConstruction) {\n    INodeFileUnderConstruction pendingFile = (INodeFileUnderConstruction) fileInode;\n    // existing logic\n} else {\n    throw new IOException(\"File is not under construction\");\n}"
        },
        "possible_fix_code": {
            "recoverLeaseInternal": "  private void recoverLeaseInternal(INode fileInode, \n      String src, String holder, String clientMachine, boolean force)\n      throws IOException {\n    assert hasWriteLock();\n    if (fileInode != null) {\n      if (fileInode.isUnderConstruction()) {\n        INodeFileUnderConstruction pendingFile = (INodeFileUnderConstruction) fileInode;\n        //\n        // If the file is under construction , then it must be in our\n        // leases. Find the appropriate lease record.\n        //\n        Lease lease = leaseManager.getLease(holder);\n        //\n        // We found the lease for this file. And surprisingly the original\n        // holder is trying to recreate this file. This should never occur.\n        //\n        if (!force && lease != null) {\n          Lease leaseFile = leaseManager.getLeaseByPath(src);\n          if ((leaseFile != null && leaseFile.equals(lease)) ||\n              lease.getHolder().equals(holder)) { \n            throw new AlreadyBeingCreatedException(\n              \"failed to create file \" + src + \" for \" + holder +\n              \" on client \" + clientMachine + \n              \" because current leaseholder is trying to recreate file.\");\n          }\n        }\n        //\n        // Find the original holder.\n        //\n        lease = leaseManager.getLease(pendingFile.getClientName());\n        if (lease == null) {\n          throw new AlreadyBeingCreatedException(\n            \"failed to create file \" + src + \" for \" + holder +\n            \" on client \" + clientMachine + \n            \" because pendingCreates is non-null but no leases found.\");\n        }\n        if (force) {\n          // close now: no need to wait for soft lease expiration and \n          // close only the file src\n          LOG.info(\"recoverLease: recover lease \" + lease + \", src=\" + src +\n            \" from client \" + pendingFile.getClientName());\n          internalReleaseLease(lease, src, holder);\n        } else {\n          assert lease.getHolder().equals(pendingFile.getClientName()) :\n            \"Current lease holder \" + lease.getHolder() +\n            \" does not match file creator \" + pendingFile.getClientName();\n          //\n          // If the original holder has not renewed in the last SOFTLIMIT \n          // period, then start lease recovery.\n          //\n          if (lease.expiredSoftLimit()) {\n            LOG.info(\"startFile: recover lease \" + lease + \", src=\" + src +\n                \" from client \" + pendingFile.getClientName());\n            boolean isClosed = internalReleaseLease(lease, src, null);\n            if(!isClosed)\n              throw new RecoveryInProgressException(\n                  \"Failed to close file \" + src +\n                  \". Lease recovery is in progress. Try again later.\");\n          } else {\n            BlockInfoUnderConstruction lastBlock=pendingFile.getLastBlock();\n            if(lastBlock != null && lastBlock.getBlockUCState() ==\n              BlockUCState.UNDER_RECOVERY) {\n              throw new RecoveryInProgressException(\n                \"Recovery in progress, file [\" + src + \"], \"+\n                \"lease owner [\" + lease.getHolder() + \"]\");\n              } else {\n                throw new AlreadyBeingCreatedException(\n                  \"Failed to create file [\" + src + \"] for [\" + holder +\n                  \"] on client [\" + clientMachine +\n                  \"], because this file is already being created by [\" +\n                  pendingFile.getClientName() + \"] on [\" +\n                  pendingFile.getClientMachine() + \"]\");\n                }\n             }\n          }\n      } else {\n        throw new IOException(\"File is not under construction\");\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-4006.json",
        "creation_time": "2012-10-04T22:05:14.000+0000",
        "bug_report": {
            "Title": "TestCheckpoint#testSecondaryHasVeryOutOfDateImage occasionally fails due to unexpected exit",
            "Description": "The test 'TestCheckpoint#testSecondaryHasVeryOutOfDateImage' intermittently fails due to a NullPointerException (NPE) occurring during the checkpointing process. This issue arises when the background checkpointing initiated by the SecondaryNameNode conflicts with explicit checkpoints triggered by the test itself. The stack trace indicates that the NPE occurs in the 'doCheckpoint' method of the SecondaryNameNode class, suggesting that there may be uninitialized or improperly handled variables during the checkpointing process.",
            "StackTrace": [
                "2012-09-16 01:55:05,901 FATAL hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1355)) - Test resulted in an unexpected exit",
                "org.apache.hadoop.util.ExitUtil$ExitException: Fatal exception with message null",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:480)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:331)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2.run(SecondaryNameNode.java:298)",
                "at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:452)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:294)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is a NullPointerException in the 'doCheckpoint' method of the SecondaryNameNode class, likely due to uninitialized variables or improper handling of the checkpointing state when multiple checkpoints are triggered simultaneously.",
            "StepsToReproduce": [
                "Run the test 'TestCheckpoint#testSecondaryHasVeryOutOfDateImage' multiple times.",
                "Observe the test results for intermittent failures."
            ],
            "ExpectedBehavior": "The test should complete successfully without any unexpected exits or exceptions.",
            "ObservedBehavior": "The test occasionally fails with a NullPointerException, leading to an unexpected exit of the MiniDFSCluster.",
            "Suggestions": "Review the 'doCheckpoint' method to ensure all necessary variables are properly initialized and that concurrent checkpointing scenarios are handled gracefully. Consider adding synchronization or checks to prevent conflicts between background and explicit checkpoints.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode",
                    "org.apache.hadoop.security.SecurityUtil"
                ],
                "methods": [
                    "SecondaryNameNode.doCheckpoint",
                    "SecondaryNameNode.doWork",
                    "SecurityUtil.doAsLoginUserOrFatal"
                ]
            },
            "possible_fix": "In the 'doCheckpoint' method, ensure that all necessary objects are initialized before use. Implement checks to handle cases where the checkpointing state may be inconsistent due to concurrent operations. For example, add null checks for 'checkpointImage' and 'dstStorage' before accessing their methods."
        },
        "possible_fix_code": {
            "doCheckpoint": "  boolean doCheckpoint() throws IOException {\n    checkpointImage.ensureCurrentDirExists();\n    NNStorage dstStorage = checkpointImage.getStorage();\n\n    // Check for null to prevent NullPointerException\n    if (checkpointImage == null || dstStorage == null) {\n        LOG.error(\"Checkpoint image or destination storage is null.\");\n        return false;\n    }\n\n    // Tell the namenode to start logging transactions in a new edit file\n    // Returns a token that would be used to upload the merged image.\n    CheckpointSignature sig = namenode.rollEditLog();\n\n    if ((checkpointImage.getNamespaceID() == 0) ||\n        (sig.isSameCluster(checkpointImage) &&\n         !sig.storageVersionMatches(checkpointImage.getStorage()))) {\n      // if we're a fresh 2NN, or if we're on the same cluster and our storage\n      // needs an upgrade, just take the storage info from the server.\n      dstStorage.setStorageInfo(sig);\n      dstStorage.setClusterID(sig.getClusterID());\n      dstStorage.setBlockPoolID(sig.getBlockpoolID());\n    }\n    sig.validateStorageInfo(checkpointImage);\n\n    // error simulation code for junit test\n    CheckpointFaultInjector.getInstance().afterSecondaryCallsRollEditLog();\n\n    RemoteEditLogManifest manifest =\n      namenode.getEditLogManifest(sig.mostRecentCheckpointTxId + 1);\n\n    boolean loadImage = downloadCheckpointFiles(\n        fsName, checkpointImage, sig, manifest);   // Fetch fsimage and edits\n    doMerge(sig, manifest, loadImage, checkpointImage, namesystem);\n\n    //\n    // Upload the new image into the NameNode. Then tell the Namenode\n    // to make this new uploaded image as the most current image.\n    //\n    long txid = checkpointImage.getLastAppliedTxId();\n    TransferFsImage.uploadImageFromStorage(fsName, getImageListenAddress(),\n        dstStorage, txid);\n\n    // error simulation code for junit test\n    CheckpointFaultInjector.getInstance().afterSecondaryUploadsNewImage();\n\n    LOG.warn(\"Checkpoint done. New Image Size: \" \n             + dstStorage.getFsImageName(txid).length());\n\n    return loadImage;\n  }"
        }
    },
    {
        "filename": "HDFS-6715.json",
        "creation_time": "2014-07-21T21:26:25.000+0000",
        "bug_report": {
            "Title": "WebHDFS Fails to Failover During Namenode Startup",
            "Description": "During high availability (HA) testing of MapReduce jobs using WebHDFS, it was observed that the system does not properly handle failover when the Namenode is in startup mode. This results in a java.io.IOException, causing the job to fail without a proper recovery mechanism. The logs indicate that the container is killed by the ApplicationMaster due to the Namenode being unavailable, leading to a non-zero exit code.",
            "StackTrace": [
                "2014-04-17 05:08:06,346 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1397710493213_0001_r_000008_0: Container killed by the ApplicationMaster.",
                "Container killed on request. Exit code is 143",
                "Container exited with a non-zero exit code 143",
                "2014-04-17 05:08:10,205 ERROR [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Could not commit job",
                "java.io.IOException: Namenode is in startup mode",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:525)"
            ],
            "RootCause": "The root cause of the issue is that the WebHDFS client does not handle the Namenode's startup state correctly, leading to an IOException when attempting to access HDFS resources during this state.",
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with high availability enabled.",
                "2. Start the Namenode and then shut it down to simulate a failover scenario.",
                "3. Attempt to run a MapReduce job that interacts with WebHDFS while the Namenode is in startup mode.",
                "4. Observe the logs for errors related to the Namenode being unavailable."
            ],
            "ExpectedBehavior": "The system should automatically failover to the standby Namenode and allow the MapReduce job to continue processing without encountering an IOException.",
            "ObservedBehavior": "The MapReduce job fails with a java.io.IOException indicating that the Namenode is in startup mode, and the job cannot commit due to the unavailability of the Namenode.",
            "Suggestions": "Implement a retry mechanism in the WebHDFS client to handle the Namenode startup state gracefully. Additionally, ensure that the failover logic is robust enough to switch to the standby Namenode when the active Namenode is unavailable.",
            "problem_location": {
                "files": [
                    "TaskAttemptImpl.java",
                    "CommitterEventHandler.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl",
                    "org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler"
                ],
                "methods": [
                    "TaskAttemptImpl.getDiagnostics",
                    "CommitterEventHandler.commitJob"
                ]
            },
            "possible_fix": "Modify the WebHDFS client to include a retry mechanism when encountering a Namenode in startup mode. For example, in the TaskAttemptImpl class, add logic to check the Namenode state and retry the operation after a brief wait if the Namenode is not ready."
        },
        "possible_fix_code": {
            "TaskAttemptImpl.getDiagnostics": "public String getDiagnostics() {\n    StringBuilder diagnostics = new StringBuilder();\n    try {\n        // Attempt to get diagnostics from the Namenode\n        // Add retry mechanism for Namenode startup state\n        int retryCount = 0;\n        while (retryCount < 5) {\n            try {\n                // Simulate getting diagnostics from Namenode\n                // This is where the actual logic to get diagnostics would go\n                // For example: diagnostics.append(namenode.getDiagnostics());\n                return diagnostics.toString();\n            } catch (IOException e) {\n                if (e.getMessage().contains(\"Namenode is in startup mode\")) {\n                    // Wait for a brief period before retrying\n                    Thread.sleep(2000); // wait for 2 seconds\n                    retryCount++;\n                } else {\n                    throw e; // rethrow if it's not a startup issue\n                }\n            }\n        }\n        diagnostics.append(\"Failed to get diagnostics after retries.\");\n    } catch (InterruptedException ie) {\n        Thread.currentThread().interrupt(); // restore interrupted status\n    }\n    return diagnostics.toString();\n}"
        }
    },
    {
        "filename": "HDFS-2392.json",
        "creation_time": "2011-09-30T20:29:09.000+0000",
        "bug_report": {
            "Title": "Distcp operation fails with IOException",
            "Description": "The DistCp operation to copy a file from HDFS using the hftp protocol is failing with an IOException. The logs indicate that the job is unable to copy any files, resulting in a failure status for all map tasks. The source path is valid, but the destination path does not exist, which is likely causing the failure.",
            "StackTrace": [
                "java.io.IOException: Copied: 0 Skipped: 0 Failed: 1",
                "at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "RootCause": "The destination path '/user/hadoopqa/out3' does not exist, which is causing the DistCp job to fail when attempting to copy files.",
            "StepsToReproduce": [
                "Run the command: hadoop distcp hftp://<NN Hostname>:50070/user/hadoopqa/input1/part-00000 /user/hadoopqa/out3",
                "Check the logs for any IOException related to file copying."
            ],
            "ExpectedBehavior": "The DistCp command should successfully copy the specified file from the source to the destination path.",
            "ObservedBehavior": "The DistCp command fails with an IOException, indicating that no files were copied due to the destination path not existing.",
            "Suggestions": "Ensure that the destination path '/user/hadoopqa/out3' exists before running the DistCp command. If it does not exist, create the directory or specify a valid existing destination.",
            "problem_location": {
                "files": [
                    "DistCp.java",
                    "JobClient.java"
                ],
                "classes": [
                    "org.apache.hadoop.tools.DistCp",
                    "org.apache.hadoop.mapred.JobClient"
                ],
                "methods": [
                    "DistCp.copy",
                    "JobClient.runJob"
                ]
            },
            "possible_fix": "Before executing the DistCp command, check if the destination directory exists. If it does not, create it using HDFS commands (e.g., hadoop fs -mkdir /user/hadoopqa/out3) or modify the command to point to an existing directory."
        },
        "possible_fix_code": {
            "DistCp.copy": "public void copy() throws IOException {\n    // Check if the destination path exists\n    Path destPath = new Path(destinationPath);\n    FileSystem fs = destPath.getFileSystem(getConf());\n    if (!fs.exists(destPath)) {\n        // Create the destination directory if it does not exist\n        fs.mkdirs(destPath);\n    }\n    // Proceed with the copy operation\n    // ... (existing copy logic here)\n}"
        }
    },
    {
        "filename": "HDFS-11472.json",
        "creation_time": "2017-02-28T18:39:13.000+0000",
        "bug_report": {
            "Title": "Fix inconsistent replica size after a data pipeline failure",
            "Description": "A bug was identified where the on-disk length of a replica is less than the acknowledged length, violating assumptions in the recovery code. This inconsistency arises when exceptions occur during the execution of the BlockReceiver#receivePacket method, leading to the in-memory replica size not being updated while the data is still written to disk. This results in discrepancies between the expected and actual replica sizes, causing recovery operations to fail.",
            "StackTrace": [
                "2017-01-08 01:41:03,532 WARN org.apache.hadoop.hdfs.server.protocol.InterDatanodeProtocol: Failed to obtain replica info for block (=BP-947993742-10.204.0.136-1362248978912:blk_2526438952_1101394519586) from datanode (=DatanodeInfoWithStorage[10.204.138.17:1004,null,null])",
                "java.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: getBytesOnDisk() < getVisibleLength(), rip=ReplicaBeingWritten, blk_2526438952_1101394519586, RBW",
                "getNumBytes() = 27530",
                "getBytesOnDisk() = 27006",
                "getVisibleLength()= 27268",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2284)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2260)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.initReplicaRecovery(DataNode.java:2566)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.callInitReplicaRecovery(DataNode.java:2577)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2645)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.access$400(DataNode.java:245)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode$5.run(DataNode.java:2551)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that exceptions thrown during the execution of BlockReceiver#receivePacket prevent the in-memory replica size from being updated, while the data is still written to disk. This leads to a situation where the on-disk size is less than the acknowledged size, causing recovery operations to fail.",
            "StepsToReproduce": [
                "1. Initiate a data write operation to the HDFS.",
                "2. Simulate a failure during the write operation (e.g., by interrupting the thread).",
                "3. Observe the replica sizes in the HDFS.",
                "4. Attempt to recover the block and check for discrepancies in the replica sizes."
            ],
            "ExpectedBehavior": "The on-disk length of the replica should always match or exceed the acknowledged length, ensuring that recovery operations can proceed without errors.",
            "ObservedBehavior": "The on-disk length of the replica is less than the acknowledged length, leading to exceptions during recovery operations.",
            "Suggestions": "Improve the replica recovery code to handle cases where the on-disk size is less than the acknowledged size. Ensure that the in-memory checksum is updated accordingly during write operations, even when exceptions occur.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.BlockReceiver",
                    "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl",
                    "org.apache.hadoop.hdfs.server.datanode.DataNode"
                ],
                "methods": [
                    "BlockReceiver.receivePacket",
                    "FsDatasetImpl.initReplicaRecovery",
                    "DataNode.initReplicaRecovery"
                ]
            },
            "possible_fix": "Modify the BlockReceiver#receivePacket method to ensure that the in-memory replica size is updated correctly even when exceptions occur. Additionally, enhance the FsDatasetImpl.initReplicaRecovery method to handle cases where the on-disk size is less than the acknowledged size, ensuring that the recovery process can proceed without errors."
        },
        "possible_fix_code": {
            "BlockReceiver.receivePacket": "  private int receivePacket() throws IOException {\n    // read the next packet\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader header = packetReceiver.getHeader();\n    if (LOG.isDebugEnabled()){\n      LOG.debug(\"Receiving one packet for block \" + block +\n                \": \" + header);\n    }\n\n    // Sanity check the header\n    if (header.getOffsetInBlock() > replicaInfo.getNumBytes()) {\n      throw new IOException(\"Received an out-of-sequence packet for \" + block + \n          \"from \" + inAddr + \" at offset \" + header.getOffsetInBlock() +\n          \". Expecting packet starting at \" + replicaInfo.getNumBytes());\n    }\n    if (header.getDataLen() < 0) {\n      throw new IOException(\"Got wrong length during writeBlock(\" + block + \n                            \") from \" + inAddr + \" at offset \" + \n                            header.getOffsetInBlock() + \": \" +\n                            header.getDataLen()); \n    }\n\n    long offsetInBlock = header.getOffsetInBlock();\n    long seqno = header.getSeqno();\n    boolean lastPacketInBlock = header.isLastPacketInBlock();\n    final int len = header.getDataLen();\n    boolean syncBlock = header.getSyncBlock();\n\n    // avoid double sync'ing on close\n    if (syncBlock && lastPacketInBlock) {\n      this.syncOnClose = false;\n    }\n\n    // update received bytes\n    final long firstByteInBlock = offsetInBlock;\n    offsetInBlock += len;\n    if (replicaInfo.getNumBytes() < offsetInBlock) {\n      replicaInfo.setNumBytes(offsetInBlock);\n    }\n\n    // put in queue for pending acks, unless sync was requested\n    if (responder != null && !syncBlock && !shouldVerifyChecksum()) {\n      ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n          lastPacketInBlock, offsetInBlock, Status.SUCCESS);\n    }\n\n    // Drop heartbeat for testing.\n    if (seqno < 0 && len == 0 &&\n        DataNodeFaultInjector.get().dropHeartbeatPacket()) {\n      return 0;\n    }\n\n    //First write the packet to the mirror:\n    if (mirrorOut != null && !mirrorError) {\n      try {\n        long begin = Time.monotonicNow();\n        // For testing. Normally no-op.\n        DataNodeFaultInjector.get().stopSendingPacketDownstream(mirrorAddr);\n        packetReceiver.mirrorPacketTo(mirrorOut);\n        mirrorOut.flush();\n        long now = Time.monotonicNow();\n        setLastSentTime(now);\n        long duration = now - begin;\n        DataNodeFaultInjector.get().logDelaySendingPacketDownstream(\n            mirrorAddr,\n            duration);\n        trackSendPacketToLastNodeInPipeline(duration);\n        if (duration > datanodeSlowLogThresholdMs) {\n          LOG.warn(\"Slow BlockReceiver write packet to mirror took \" + duration\n              + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms)\");\n        }\n      } catch (IOException e) {\n        handleMirrorOutError(e);\n      }\n    }\n\n    ByteBuffer dataBuf = packetReceiver.getDataSlice();\n    ByteBuffer checksumBuf = packetReceiver.getChecksumSlice();\n\n    if (lastPacketInBlock || len == 0) {\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"Receiving an empty packet or the end of the block \" + block);\n      }\n      // sync block if requested\n      if (syncBlock) {\n        flushOrSync(true);\n      }\n    } else {\n      final int checksumLen = diskChecksum.getChecksumSize(len);\n      final int checksumReceivedLen = checksumBuf.capacity();\n\n      if (checksumReceivedLen > 0 && checksumReceivedLen != checksumLen) {\n        throw new IOException(\"Invalid checksum length: received length is \"\n            + checksumReceivedLen + \" but expected length is \" + checksumLen);\n      }\n\n      if (checksumReceivedLen > 0 && shouldVerifyChecksum()) {\n        try {\n          verifyChunks(dataBuf, checksumBuf);\n        } catch (IOException ioe) {\n          // checksum error detected locally. there is no reason to continue.\n          if (responder != null) {\n            try {\n              ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n                  lastPacketInBlock, offsetInBlock,\n                  Status.ERROR_CHECKSUM);\n              // Wait until the responder sends back the response\n              // and interrupt this thread.\n              Thread.sleep(3000);\n            } catch (InterruptedException e) { }\n          }\n          throw new IOException(\"Terminating due to a checksum error.\" + ioe);\n        }\n\n        if (needsChecksumTranslation) {\n          // overwrite the checksums in the packet buffer with the\n          // appropriate polynomial for the disk storage.\n          translateChunks(dataBuf, checksumBuf);\n        }\n      }\n\n      if (checksumReceivedLen == 0 && !streams.isTransientStorage()) {\n        // checksum is missing, need to calculate it\n        checksumBuf = ByteBuffer.allocate(checksumLen);\n        diskChecksum.calculateChunkedSums(dataBuf, checksumBuf);\n      }\n\n      // by this point, the data in the buffer uses the disk checksum\n\n      final boolean shouldNotWriteChecksum = checksumReceivedLen == 0\n          && streams.isTransientStorage();\n      try {\n        long onDiskLen = replicaInfo.getBytesOnDisk();\n        if (onDiskLen < offsetInBlock) {\n          // Normally the beginning of an incoming packet is aligned with the\n          // existing data on disk. If the beginning packet data offset is not\n          // checksum chunk aligned, the end of packet will not go beyond the\n          // next chunk boundary.\n          // When a failure-recovery is involved, the client state and the\n          // datanode state may not exactly agree. I.e. the client may\n          // resend part of data that is already on disk. Correct number of\n          // bytes should be skipped when writing the data and checksum\n          // buffers out to disk.\n          long partialChunkSizeOnDisk = onDiskLen % bytesPerChecksum;\n          long lastChunkBoundary = onDiskLen - partialChunkSizeOnDisk;\n          boolean alignedOnDisk = partialChunkSizeOnDisk == 0;\n          boolean alignedInPacket = firstByteInBlock % bytesPerChecksum == 0;\n\n          // If the end of the on-disk data is not chunk-aligned, the last\n          // checksum needs to be overwritten.\n          boolean overwriteLastCrc = !alignedOnDisk && !shouldNotWriteChecksum;\n          // If the starting offset of the packet data is at the last chunk\n          // boundary of the data on disk, the partial checksum recalculation\n          // can be skipped and the checksum supplied by the client can be used\n          // instead. This reduces disk reads and cpu load.\n          boolean doCrcRecalc = overwriteLastCrc &&\n              (lastChunkBoundary != firstByteInBlock);\n\n          // If this is a partial chunk, then verify that this is the only\n          // chunk in the packet. If the starting offset is not chunk\n          // aligned, the packet should terminate at or before the next\n          // chunk boundary.\n          if (!alignedInPacket && len > bytesPerChecksum) {\n            throw new IOException(\"Unexpected packet data length for \"\n                +  block + \" from \" + inAddr + \": a partial chunk must be \"\n                + \" sent in an individual packet (data length = \" + len\n                +  \" > bytesPerChecksum = \" + bytesPerChecksum + \")\");\n          }\n\n          // If the last portion of the block file is not a full chunk,\n          // then read in pre-existing partial data chunk and recalculate\n          // the checksum so that the checksum calculation can continue\n          // from the right state. If the client provided the checksum for\n          // the whole chunk, this is not necessary.\n          Checksum partialCrc = null;\n          if (doCrcRecalc) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"receivePacket for \" + block \n                  + \": previous write did not end at the chunk boundary.\"\n                  + \" onDiskLen=\" + onDiskLen);\n            }\n            long offsetInChecksum = BlockMetadataHeader.getHeaderSize() +\n                onDiskLen / bytesPerChecksum * checksumSize;\n            partialCrc = computePartialChunkCrc(onDiskLen, offsetInChecksum);\n          }\n\n          // The data buffer position where write will begin. If the packet\n          // data and on-disk data have no overlap, this will not be at the\n          // beginning of the buffer.\n          int startByteToDisk = (int)(onDiskLen - firstByteInBlock) \n              + dataBuf.arrayOffset() + dataBuf.position();\n\n          // Actual number of data bytes to write.\n          int numBytesToDisk = (int)(offsetInBlock - onDiskLen);\n\n          // Write data to disk.\n          long begin = Time.monotonicNow();\n          streams.writeDataToDisk(dataBuf.array(),\n              startByteToDisk, numBytesToDisk);\n          long duration = Time.monotonicNow() - begin;\n\n          if (duration > maxWriteToDiskMs) {\n            maxWriteToDiskMs = duration;\n          }\n\n          final byte[] lastCrc;\n          if (shouldNotWriteChecksum) {\n            lastCrc = null;\n          } else {\n            int skip = 0;\n            byte[] crcBytes = null;\n\n            // First, prepare to overwrite the partial crc at the end.\n            if (overwriteLastCrc) { // not chunk-aligned on disk\n              // prepare to overwrite last checksum\n              adjustCrcFilePosition();\n            }\n\n            // The CRC was recalculated for the last partial chunk. Update the\n            // CRC by reading the rest of the chunk, then write it out.\n            if (doCrcRecalc) {\n              // Calculate new crc for this chunk.\n              int bytesToReadForRecalc =\n                  (int)(bytesPerChecksum - partialChunkSizeOnDisk);\n              if (numBytesToDisk < bytesToReadForRecalc) {\n                bytesToReadForRecalc = numBytesToDisk;\n              }\n\n              partialCrc.update(dataBuf.array(), startByteToDisk,\n                  bytesToReadForRecalc);\n              byte[] buf = FSOutputSummer.convertToByteStream(partialCrc,\n                  checksumSize);\n              crcBytes = copyLastChunkChecksum(buf, checksumSize, buf.length);\n              checksumOut.write(buf);\n              if(LOG.isDebugEnabled()) {\n                LOG.debug(\"Writing out partial crc for data len \" + len +\n                    \", skip=\" + skip);\n              }\n              skip++; //  For the partial chunk that was just read.\n            }\n\n            // Determine how many checksums need to be skipped up to the last\n            // boundary. The checksum after the boundary was already counted\n            // above. Only count the number of checksums skipped up to the\n            // boundary here.\n            long skippedDataBytes = lastChunkBoundary - firstByteInBlock;\n\n            if (skippedDataBytes > 0) {\n              skip += (int)(skippedDataBytes / bytesPerChecksum) +\n                  ((skippedDataBytes % bytesPerChecksum == 0) ? 0 : 1);\n            }\n            skip *= checksumSize; // Convert to number of bytes\n\n            // write the rest of checksum\n            final int offset = checksumBuf.arrayOffset() +\n                checksumBuf.position() + skip;\n            final int end = offset + checksumLen - skip;\n            // If offset >= end, there is no more checksum to write.\n            // I.e. a partial chunk checksum rewrite happened and there is no\n            // more to write after that.\n            if (offset >= end && doCrcRecalc) {\n              lastCrc = crcBytes;\n            } else {\n              final int remainingBytes = checksumLen - skip;\n              lastCrc = copyLastChunkChecksum(checksumBuf.array(),\n                  checksumSize, end);\n              checksumOut.write(checksumBuf.array(), offset, remainingBytes);\n            }\n          }\n\n          // flush entire packet, sync if requested\n          flushOrSync(syncBlock);\n\n          replicaInfo.setLastChecksumAndDataLen(offsetInBlock, lastCrc);\n\n          datanode.metrics.incrBytesWritten(len);\n          datanode.metrics.incrTotalWriteTime(duration);\n\n          manageWriterOsCache(offsetInBlock);\n        }\n      } catch (IOException iex) {\n        // Volume error check moved to FileIoProvider\n        throw iex;\n      }\n    }\n\n    // if sync was requested, put in queue for pending acks here\n    // (after the fsync finished)\n    if (responder != null && (syncBlock || shouldVerifyChecksum())) {\n      ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n          lastPacketInBlock, offsetInBlock, Status.SUCCESS);\n    }\n\n    /*\n     * Send in-progress responses for the replaceBlock() calls back to caller to\n     * avoid timeouts due to balancer throttling. HDFS-6247\n     */\n    if (isReplaceBlock\n        && (Time.monotonicNow() - lastResponseTime > responseInterval)) {\n      BlockOpResponseProto.Builder response = BlockOpResponseProto.newBuilder()\n          .setStatus(Status.IN_PROGRESS);\n      response.build().writeDelimitedTo(replyOut);\n      replyOut.flush();\n    }\n\n    if (throttler != null) { // throttle I/O\n      throttler.throttle(len);\n    }\n\n    return lastPacketInBlock ? -1 : len;\n  }"
        }
    },
    {
        "filename": "HDFS-10760.json",
        "creation_time": "2016-08-12T08:29:24.000+0000",
        "bug_report": {
            "Title": "DataXceiver#run() should not log InvalidToken exception as an error",
            "Description": "The DataXceiver#run() method currently logs the InvalidToken exception as an error when a client attempts to access a block with an expired token. This behavior is misleading since the InvalidToken exception is already logged as a warning in the checkAccess() method. The logging in run() should be adjusted to avoid redundancy and to reflect the actual nature of the exception.",
            "StackTrace": [
                "org.apache.hadoop.security.token.SecretManager$InvalidToken: Block token with block_token_identifier (expiryDate=1470850746803, keyId=-2093956963, userId=hbase, blockPoolId=BP-641703426-10.17.1.2-1468517918886, blockId=1077120201, access modes=[READ]) is expired.",
                "at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:280)",
                "at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:301)",
                "at org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager.checkAccess(BlockPoolTokenSecretManager.java:97)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess(DataXceiver.java:1236)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:481)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:242)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the InvalidToken exception is being logged as an error in the DataXceiver#run() method, despite it being a non-critical issue that has already been logged as a warning in the checkAccess() method.",
            "StepsToReproduce": [
                "1. Start the DataNode service.",
                "2. Attempt to read a block using an expired token from a client.",
                "3. Observe the logs for the DataXceiver."
            ],
            "ExpectedBehavior": "The DataXceiver should log the InvalidToken exception as a warning only, without logging it as an error in the run() method.",
            "ObservedBehavior": "The DataXceiver logs the InvalidToken exception as an error, leading to confusion in the logs.",
            "Suggestions": "Modify the DataXceiver#run() method to catch the InvalidToken exception and avoid logging it as an error. This will prevent redundancy and improve log clarity.",
            "problem_location": {
                "files": [
                    "DataXceiver.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.DataXceiver"
                ],
                "methods": [
                    "DataXceiver.run",
                    "DataXceiver.checkAccess"
                ]
            },
            "possible_fix": "In the DataXceiver#run() method, catch the InvalidToken exception and do not log it as an error. Instead, allow the warning logged in checkAccess() to suffice. Example code change:\n\ntry {\n    // existing code\n} catch (InvalidToken e) {\n    // Do not log as error\n    // Optionally log a debug message if needed\n}"
        },
        "possible_fix_code": {
            "DataXceiver.run": "  public void run() {\n    int opsProcessed = 0;\n    Op op = null;\n\n    try {\n      synchronized(this) {\n        xceiver = Thread.currentThread();\n      }\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input = socketIn;\n      try {\n        IOStreamPair saslStreams = datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input = new BufferedInputStream(saslStreams.in,\n            smallBufferSize);\n        socketOut = saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at \" + peer.getRemoteAddressString() + \n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\");\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed != 0) {\n            assert dnConf.socketKeepaliveTimeout > 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op = readOp();\n        } catch (InterruptedIOException ignored) {\n          break;\n        } catch (EOFException | ClosedChannelException e) {\n          LOG.debug(\"Cached {} closing after {} ops.  \" +\n              \"This message is usually benign.\", peer, opsProcessed);\n          break;\n        } catch (IOException err) {\n          incrDatanodeNetworkErrors();\n          throw err;\n        }\n\n        if (opsProcessed != 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime = monotonicNow();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer != null) &&\n          (!peer.isClosed() && dnConf.socketKeepaliveTimeout > 0));\n    } catch (Throwable t) {\n      String s = datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op == null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op == Op.WRITE_BLOCK && t instanceof ReplicaAlreadyExistsException) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op == Op.READ_BLOCK && t instanceof SocketTimeoutException) {\n        String s1 =\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 += \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);\n        }\n      } else if (t instanceof InvalidToken) {\n        // Do not log InvalidToken as an error\n        LOG.warn(\"Invalid token encountered: \" + t.getMessage());\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer != null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-13635.json",
        "creation_time": "2018-05-29T19:54:50.000+0000",
        "bug_report": {
            "Title": "Incorrect message when block is not found",
            "Description": "When a client attempts to check the visible length of a block that does not exist on the DataNode, the system incorrectly throws a 'Cannot append to a non-existent replica' message. This occurs because the method getReplicaVisibleLength() is being used inappropriately for this context. The expected behavior should simply indicate that the block is not found, rather than implying an append operation.",
            "StackTrace": [
                "2018-05-29 09:23:41,966 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getReplicaVisibleLength from 10.0.0.14:53217 Call#38334117 Retry#0",
                "org.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Cannot append to a non-existent replica BP-725378529-10.236.236.8-1410027444173:13276792346",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo(FsDatasetImpl.java:792)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaVisibleLength(FsDatasetImpl.java:2588)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.getReplicaVisibleLength(DataNode.java:2756)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolServerSideTranslatorPB.getReplicaVisibleLength(ClientDatanodeProtocolServerSideTranslatorPB.java:107)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ClientDatanodeProtocolService$2.callBlockingMethod(ClientDatanodeProtocolProtos.java:17873)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2217)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2213)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2211)"
            ],
            "RootCause": "The root cause of the issue is that the method getReplicaVisibleLength() is incorrectly handling the case where a block does not exist, leading to an inappropriate error message being thrown. Specifically, the getReplicaInfo() method throws a ReplicaNotFoundException, which is not being handled correctly in the context of checking visible length.",
            "StepsToReproduce": [
                "1. Open a file that references a block that does not exist on the DataNode.",
                "2. Observe the error message returned when the system attempts to check the block's visible length."
            ],
            "ExpectedBehavior": "The system should return a message indicating that the block is not found, without implying that an append operation is being attempted.",
            "ObservedBehavior": "The system throws an error message stating 'Cannot append to a non-existent replica', which is misleading.",
            "Suggestions": "Modify the error handling in the getReplicaVisibleLength() method to return a more appropriate message when a block is not found, rather than suggesting an append operation.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientDatanodeProtocolServerSideTranslatorPB.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolServerSideTranslatorPB",
                    "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl",
                    "org.apache.hadoop.hdfs.server.datanode.DataNode"
                ],
                "methods": [
                    "ClientDatanodeProtocolServerSideTranslatorPB.getReplicaVisibleLength",
                    "FsDatasetImpl.getReplicaVisibleLength",
                    "DataNode.getReplicaVisibleLength"
                ]
            },
            "possible_fix": "In the method FsDatasetImpl.getReplicaVisibleLength(), modify the catch block for the ReplicaNotFoundException to return a more user-friendly message indicating that the block is not found, instead of allowing the IOException to propagate with the misleading append message."
        },
        "possible_fix_code": {
            "FsDatasetImpl.getReplicaVisibleLength": "  public long getReplicaVisibleLength(final ExtendedBlock block)\n  throws IOException {\n    try (AutoCloseableLock lock = datasetLock.acquire()) {\n      final Replica replica;\n      try {\n        replica = getReplicaInfo(block.getBlockPoolId(), block.getBlockId());\n      } catch (ReplicaNotFoundException e) {\n        throw new IOException(\"Block not found: \" + block.getBlockId(), e);\n      }\n      if (replica.getGenerationStamp() < block.getGenerationStamp()) {\n        throw new IOException(\n            \"replica.getGenerationStamp() < block.getGenerationStamp(), block=\"\n                + block + \", replica=\" + replica);\n      }\n      return replica.getVisibleLength();\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-11608.json",
        "creation_time": "2017-03-31T17:37:20.000+0000",
        "bug_report": {
            "Title": "HDFS write crashed with block size greater than 2 GB",
            "Description": "The HDFS client crashes when attempting to write files larger than 2 GB using a block size greater than 2 GB. This results in an OutOfMemoryError on the client side and an IOException on the DataNode side, specifically indicating an incorrect packet payload size. The issue arises due to the handling of packet sizes exceeding the maximum integer value in Java, which is 2,147,483,647 bytes (2 GB). When the payload size exceeds this limit, it leads to memory allocation issues and subsequent exceptions.",
            "StackTrace": [
                "2017-03-30 16:34:33,828 ERROR datanode.DataNode (DataXceiver.java:run(278)) - c6401.ambari.apache.org:50010:DataXceiver error processing WRITE_BLOCK operation src: /192.168.64.101:47167 dst: /192.168.64.101:50010",
                "java.io.IOException: Incorrect value for packet payload size: 2147483128",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:159)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:502)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:898)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:806)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is the handling of packet payload sizes that exceed the maximum integer value in Java (2 GB). When attempting to write a block larger than this limit, the system fails to allocate the required memory, leading to an OutOfMemoryError and IOException.",
            "StepsToReproduce": [
                "1. Set up an HDFS cluster.",
                "2. Attempt to write a file larger than 2 GB using a block size greater than 2 GB.",
                "3. Observe the OutOfMemoryError on the client and IOException on the DataNode."
            ],
            "ExpectedBehavior": "The HDFS client should successfully write files larger than 2 GB without crashing, regardless of the block size, as long as the system has sufficient resources.",
            "ObservedBehavior": "The HDFS client crashes with an OutOfMemoryError, and the DataNode throws an IOException indicating an incorrect packet payload size.",
            "Suggestions": "Consider implementing checks to ensure that the packet payload size does not exceed the maximum integer value. Additionally, review the handling of large files and block sizes in the HDFS architecture to prevent such crashes.",
            "problem_location": {
                "files": [
                    "DataXceiver.java",
                    "BlockReceiver.java",
                    "PacketReceiver.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.DataXceiver",
                    "org.apache.hadoop.hdfs.server.datanode.BlockReceiver",
                    "org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver"
                ],
                "methods": [
                    "DataXceiver.run",
                    "BlockReceiver.receivePacket",
                    "PacketReceiver.doRead"
                ]
            },
            "possible_fix": "Implement a check in the PacketReceiver.doRead method to validate the payload length before processing. If the payload length exceeds Integer.MAX_VALUE, throw an appropriate exception or handle it gracefully to prevent memory allocation issues."
        },
        "possible_fix_code": {
            "PacketReceiver.doRead": "  private void doRead(ReadableByteChannel ch, InputStream in)\n      throws IOException {\n    Preconditions.checkState(curHeader == null || !curHeader.isLastPacketInBlock());\n\n    curPacketBuf.clear();\n    curPacketBuf.limit(PacketHeader.PKT_LENGTHS_LEN);\n    doReadFully(ch, in, curPacketBuf);\n    curPacketBuf.flip();\n    int payloadLen = curPacketBuf.getInt();\n\n    if (payloadLen < Ints.BYTES) {\n      throw new IOException(\"Invalid payload length \" + payloadLen);\n    }\n    int dataPlusChecksumLen = payloadLen - Ints.BYTES;\n    int headerLen = curPacketBuf.getShort();\n    if (headerLen < 0) {\n      throw new IOException(\"Invalid header length \" + headerLen);\n    }\n\n    LOG.trace(\"readNextPacket: dataPlusChecksumLen={}, headerLen={}\", dataPlusChecksumLen, headerLen);\n\n    int totalLen = payloadLen + headerLen;\n    if (totalLen < 0 || totalLen > MAX_PACKET_SIZE) {\n      throw new IOException(\"Incorrect value for packet payload size: \" + payloadLen);\n    }\n\n    // New check to prevent OutOfMemoryError for large payloads\n    if (payloadLen > Integer.MAX_VALUE) {\n      throw new IOException(\"Payload length exceeds maximum allowed size: \" + payloadLen);\n    }\n\n    reallocPacketBuf(PacketHeader.PKT_LENGTHS_LEN + dataPlusChecksumLen + headerLen);\n    curPacketBuf.clear();\n    curPacketBuf.position(PacketHeader.PKT_LENGTHS_LEN);\n    curPacketBuf.limit(PacketHeader.PKT_LENGTHS_LEN + dataPlusChecksumLen + headerLen);\n    doReadFully(ch, in, curPacketBuf);\n    curPacketBuf.flip();\n    curPacketBuf.position(PacketHeader.PKT_LENGTHS_LEN);\n\n    byte[] headerBuf = new byte[headerLen];\n    curPacketBuf.get(headerBuf);\n    if (curHeader == null) {\n      curHeader = new PacketHeader();\n    }\n    curHeader.setFieldsFromData(payloadLen, headerBuf);\n\n    int checksumLen = dataPlusChecksumLen - curHeader.getDataLen();\n    if (checksumLen < 0) {\n      throw new IOException(\"Invalid packet: data length in packet header exceeds data length received. dataPlusChecksumLen=\" + dataPlusChecksumLen + \" header: \" + curHeader);\n    }\n\n    reslicePacket(headerLen, checksumLen, curHeader.getDataLen());\n  }"
        }
    },
    {
        "filename": "HDFS-12638.json",
        "creation_time": "2017-10-11T11:29:04.000+0000",
        "bug_report": {
            "Title": "Delete copy-on-truncate block along with the original block, when deleting a file being truncated",
            "Description": "The system encounters a NullPointerException (NPE) in the ReplicationMonitor thread when attempting to delete a file that is being truncated. The root of the issue lies in the BlockCollection being null when creating ReplicationWork, which leads to the failure in choosing targets for replication. This situation arises due to the removal of the null check for BlockCollection in a previous change (HDFS-9754).",
            "StackTrace": [
                "2017-10-11 16:29:06,161 ERROR [ReplicationMonitor] org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor thread received Runtime exception.",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:55)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1532)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1491)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3792)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3744)",
                "at java.lang.Thread.run(Thread.java:834)"
            ],
            "RootCause": "The BlockCollection passed to ReplicationWork is null, which is causing the NullPointerException when the system attempts to choose targets for replication.",
            "StepsToReproduce": [
                "1. Create a file in HDFS.",
                "2. Start truncating the file.",
                "3. Attempt to delete the file while it is being truncated."
            ],
            "ExpectedBehavior": "The system should successfully delete the file and its associated blocks without throwing a NullPointerException.",
            "ObservedBehavior": "The system throws a NullPointerException in the ReplicationMonitor thread, causing the NameNode to exit unexpectedly.",
            "Suggestions": "Reintroduce a null check for BlockCollection in the ReplicationWork class to prevent the NPE. Additionally, ensure that the BlockCollection is properly initialized before being passed to ReplicationWork.",
            "problem_location": {
                "files": [
                    "ReplicationWork.java",
                    "BlockManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork",
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager"
                ],
                "methods": [
                    "ReplicationWork.chooseTargets",
                    "BlockManager.computeDatanodeWork"
                ]
            },
            "possible_fix": "In ReplicationWork.java, add a null check for BlockCollection before it is used in the chooseTargets method. For example:\n\nif (getBc() == null) {\n    throw new IllegalStateException(\"BlockCollection cannot be null\");\n}\n\nThis will prevent the NPE and ensure that the system behaves correctly when deleting files."
        },
        "possible_fix_code": {
            "ReplicationWork.chooseTargets": "  void chooseTargets(BlockPlacementPolicy blockplacement,\n      BlockStoragePolicySuite storagePolicySuite,\n      Set<Node> excludedNodes) {\n    assert getSrcNodes().length > 0\n        : \"At least 1 source node should have been selected\";\n    if (getBc() == null) {\n        throw new IllegalStateException(\"BlockCollection cannot be null\");\n    }\n    try {\n      DatanodeStorageInfo[] chosenTargets = blockplacement.chooseTarget(\n          getBc().getName(), getAdditionalReplRequired(), getSrcNodes()[0],\n          getLiveReplicaStorages(), false, excludedNodes,\n          getBlock().getNumBytes(),\n          storagePolicySuite.getPolicy(getBc().getStoragePolicyID()),\n          null);\n      setTargets(chosenTargets);\n    } finally {\n      getSrcNodes()[0].decrementPendingReplicationWithoutTargets();\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-12383.json",
        "creation_time": "2017-08-31T20:43:50.000+0000",
        "bug_report": {
            "Title": "Re-encryption updater should handle canceled tasks better",
            "Description": "The re-encryption updater is currently exiting unexpectedly when a task is canceled, leading to subsequent tasks not being executed. This behavior is observed when the updater encounters a CancellationException, which is not handled properly, causing the updater thread to terminate. The logs indicate that the updater cancels a task but does not continue processing other tasks, resulting in a failure to execute further re-encryption operations.",
            "StackTrace": [
                "java.util.concurrent.CancellationException",
                "at java.util.concurrent.FutureTask.report(FutureTask.java:121)",
                "at java.util.concurrent.FutureTask.get(FutureTask.java:192)",
                "at org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.takeAndProcessTasks(ReencryptionUpdater.java:404)",
                "at org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.run(ReencryptionUpdater.java:250)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the re-encryption updater does not properly handle the CancellationException when a task is canceled. Instead of continuing to process remaining tasks, the updater thread exits.",
            "StepsToReproduce": [
                "Submit a re-encryption task to the updater.",
                "Cancel the task while it is being processed.",
                "Observe the behavior of the updater thread in the logs."
            ],
            "ExpectedBehavior": "The re-encryption updater should continue processing remaining tasks even if one task is canceled, logging the cancellation appropriately without exiting.",
            "ObservedBehavior": "The re-encryption updater exits when a task is canceled, preventing any further tasks from being executed.",
            "Suggestions": "Modify the takeAndProcessTasks method to handle CancellationException gracefully, allowing the updater to continue processing other tasks instead of terminating.",
            "problem_location": {
                "files": [
                    "ReencryptionUpdater.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater"
                ],
                "methods": [
                    "ReencryptionUpdater.takeAndProcessTasks",
                    "ReencryptionUpdater.run"
                ]
            },
            "possible_fix": "In the takeAndProcessTasks method, add a check for the CancellationException and log the cancellation without exiting the thread. For example, modify the catch block for Throwable to handle CancellationException specifically and allow the loop to continue."
        },
        "possible_fix_code": {
            "ReencryptionUpdater.takeAndProcessTasks": "  private void takeAndProcessTasks() throws Exception {\n    final Future<ReencryptionTask> completed = batchService.take();\n    throttle();\n    checkPauseForTesting();\n    ReencryptionTask task = null;\n    try {\n      task = completed.get();\n    } catch (CancellationException ce) {\n      LOG.debug(\"Skipped canceled re-encryption task, continuing to next task.\");\n      return; // Exit this method but continue the loop in run()\n    }\n\n    if (completed.isCancelled()) {\n      LOG.debug(\"Skipped canceled re-encryption task for zone {}, last: {}\",\n          task.zoneId, task.lastFile);\n    }\n\n    boolean shouldRetry;\n    do {\n      dir.getFSNamesystem().writeLock();\n      try {\n        throttleTimerLocked.start();\n        processTask(task);\n        shouldRetry = false;\n      } catch (RetriableException | SafeModeException re) {\n        // Keep retrying until succeed.\n        LOG.info(\"Exception when processing re-encryption task for zone {}, \"\n                + \"retrying...\", task.zoneId, re);\n        shouldRetry = true;\n        Thread.sleep(faultRetryInterval);\n      } catch (IOException ioe) {\n        LOG.warn(\"Failure processing re-encryption task for zone {}\",\n            task.zoneId, ioe);\n        ++task.numFailures;\n        task.processed = true;\n        shouldRetry = false;\n      } finally {\n        dir.getFSNamesystem().writeUnlock(\"reencryptUpdater\");\n        throttleTimerLocked.stop();\n      }\n      // logSync regardless, to prevent edit log buffer overflow triggering\n      // logSync inside FSN writelock.\n      dir.getEditLog().logSync();\n    } while (shouldRetry);\n  }"
        }
    },
    {
        "filename": "HDFS-5322.json",
        "creation_time": "2013-10-07T22:52:54.000+0000",
        "bug_report": {
            "Title": "HDFS delegation token not found in cache errors seen on secure HA clusters",
            "Description": "During high availability (HA) tests, errors related to missing HDFS delegation tokens in the cache have been observed, leading to job failures. The error indicates that the system is unable to locate the specified HDFS delegation token in the cache, which is critical for authenticating and authorizing access to HDFS resources. This issue may arise due to improper token management or expiration of tokens before they are utilized.",
            "StackTrace": [
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1347)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1300)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)",
                "at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source)"
            ],
            "RootCause": "The root cause of the issue appears to be the failure to retrieve the HDFS delegation token from the cache, which may be due to token expiration or improper caching mechanisms in the secure HA cluster setup.",
            "StepsToReproduce": [
                "Set up a secure HA cluster with HDFS.",
                "Run a job that requires HDFS delegation tokens.",
                "Monitor the logs for errors related to missing delegation tokens."
            ],
            "ExpectedBehavior": "The system should successfully retrieve the HDFS delegation token from the cache, allowing jobs to run without encountering token-related errors.",
            "ObservedBehavior": "Jobs fail with errors indicating that the HDFS delegation token cannot be found in the cache, resulting in RemoteException.",
            "Suggestions": "Investigate the token caching mechanism to ensure tokens are properly stored and retrieved. Consider implementing a refresh mechanism for tokens to prevent expiration issues.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java"
                ],
                "classes": [
                    "org.apache.hadoop.ipc.Client",
                    "org.apache.hadoop.security.UserGroupInformation"
                ],
                "methods": [
                    "Client.call",
                    "UserGroupInformation.doAs"
                ]
            },
            "possible_fix": "Review the implementation of the token caching logic in the Client class. Ensure that tokens are not prematurely expired and are correctly stored in the cache. Additionally, consider adding logging to track token lifecycle events."
        },
        "possible_fix_code": {
            "UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      LOG.error(\"PriviledgedActionException as:\" + this + \" cause:\" + cause);\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    } finally {\n      // Add logic to refresh or validate the token cache here\n      refreshTokenCache();\n    }\n  }",
            "Client.call": "  public Writable call(RPC.RpcKind rpcKind, Writable rpcRequest,\n      ConnectionId remoteId, int serviceClass) throws IOException {\n    final Call call = createCall(rpcKind, rpcRequest);\n    Connection connection = getConnection(remoteId, call, serviceClass);\n    try {\n      connection.sendRpcRequest(call);                 // send the rpc request\n    } catch (RejectedExecutionException e) {\n      throw new IOException(\"connection has been closed\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      LOG.warn(\"interrupted waiting to send rpc request to server\", e);\n      throw new IOException(e);\n    }\n\n    boolean interrupted = false;\n    synchronized (call) {\n      while (!call.done) {\n        try {\n          call.wait();                           // wait for the result\n        } catch (InterruptedException ie) {\n          // save the fact that we were interrupted\n          interrupted = true;\n        }\n      }\n\n      if (interrupted) {\n        // set the interrupt flag now that we are done waiting\n        Thread.currentThread().interrupt();\n      }\n\n      if (call.error != null) {\n        if (call.error instanceof RemoteException) {\n          call.error.fillInStackTrace();\n          throw call.error;\n        } else { // local exception\n          InetSocketAddress address = connection.getRemoteAddress();\n          throw NetUtils.wrapException(address.getHostName(),\n                  address.getPort(),\n                  NetUtils.getHostname(),\n                  0,\n                  call.error);\n        }\n      } else {\n        return call.getRpcResponse();\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-11741.json",
        "creation_time": "2017-05-02T17:57:25.000+0000",
        "bug_report": {
            "Title": "Long running balancer may fail due to expired DataEncryptionKey",
            "Description": "The long running balancer fails to move blocks due to the KeyManager returning an expired DataEncryptionKey. This results in an InvalidEncryptionKeyException being thrown, indicating that the required block key does not exist. The issue arises because the KeyManager does not update the DataEncryptionKey in sync with the block keys, leading to failures in data transfer operations after the expiration of the keys.",
            "StackTrace": [
                "2017-04-30 05:03:58,661 WARN  [pool-1464-thread-10] balancer.Dispatcher (Dispatcher.java:dispatch(325)) - Failed to move blk_1067352712_3913241 with size=546650 from 10.0.0.134:50010:DISK to 10.0.0.98:50010:DISK through 10.0.0.134",
                "org.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=1005215027) doesn't exist. Current key: 1005215030",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.dispatch(Dispatcher.java:311)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.access$2300(Dispatcher.java:182)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$1.run(Dispatcher.java:899)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the KeyManager does not update the DataEncryptionKey in accordance with the block keys, leading to the use of an expired key during data transfer operations.",
            "StepsToReproduce": [
                "Set up a cluster with a Kerberos ticket lifetime of 10 hours and a block token expiration of 10 hours.",
                "Run a long-running balancer operation that exceeds the key expiration time (20-30 hours).",
                "Attempt to move blocks during this operation."
            ],
            "ExpectedBehavior": "The balancer should successfully move blocks without encountering an InvalidEncryptionKeyException, as the DataEncryptionKey should be updated in sync with the block keys.",
            "ObservedBehavior": "The balancer fails to move blocks and throws an InvalidEncryptionKeyException due to the use of an expired DataEncryptionKey.",
            "Suggestions": "Implement a mechanism to ensure that the DataEncryptionKey is updated in sync with the block keys managed by the KeyManager. This could involve modifying the KeyManager to listen for updates to block keys and refresh the DataEncryptionKey accordingly.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java",
                    "hadoop-hdfs-project/hadoop-hdfs-server/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient",
                    "org.apache.hadoop.hdfs.server.balancer.Dispatcher"
                ],
                "methods": [
                    "SaslDataTransferClient.getEncryptedStreams",
                    "SaslDataTransferClient.send",
                    "SaslDataTransferClient.checkTrustAndSend",
                    "Dispatcher.PendingMove.dispatch"
                ]
            },
            "possible_fix": "Modify the KeyManager to ensure that it updates the DataEncryptionKey whenever block keys are updated. This may involve adding a listener or callback mechanism that triggers the update of the DataEncryptionKey in the KeyManager."
        },
        "possible_fix_code": {
            "SaslDataTransferClient.checkTrustAndSend": "  private IOStreamPair checkTrustAndSend(InetAddress addr,\n      OutputStream underlyingOut, InputStream underlyingIn,\n      DataEncryptionKeyFactory encryptionKeyFactory,\n      Token<BlockTokenIdentifier> accessToken, DatanodeID datanodeId)\n      throws IOException {\n    if (!trustedChannelResolver.isTrusted() &&\n        !trustedChannelResolver.isTrusted(addr)) {\n      // The encryption key factory only returns a key if encryption is enabled.\n      DataEncryptionKey encryptionKey =\n          encryptionKeyFactory.newDataEncryptionKey();\n\n      // Check if the encryption key is expired and refresh if necessary\n      if (encryptionKey.isExpired()) {\n          encryptionKey = encryptionKeyFactory.refreshDataEncryptionKey();\n      }\n\n      return send(addr, underlyingOut, underlyingIn, encryptionKey, accessToken,\n          datanodeId);\n    } else {\n      LOG.debug(\n          \"SASL client skipping handshake on trusted connection for addr = {}, \"\n              + \"datanodeId = {}\", addr, datanodeId);\n      return null;\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-3936.json",
        "creation_time": "2012-09-14T05:35:12.000+0000",
        "bug_report": {
            "Title": "MiniDFSCluster shutdown races with BlocksMap usage",
            "Description": "The issue arises during the shutdown of the MiniDFSCluster, where a race condition occurs between the BlockManager (BM) and the NameNode (FSN). The FSN holds a lock while attempting to close the BM, which leads to a deadlock situation as the BM tries to acquire the FSN lock to access the BlocksMap. This results in a NullPointerException when the BM attempts to retrieve a BlockCollection from the BlocksMap, which is not properly synchronized during the shutdown process.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap.getBlockCollection(BlocksMap.java:101)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1132)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1107)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3061)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3023)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is a race condition between the BlockManager and the NameNode during the shutdown process, leading to a NullPointerException when the BlockManager attempts to access the BlocksMap without proper synchronization.",
            "StepsToReproduce": [
                "Start a MiniDFSCluster.",
                "Trigger a shutdown of the MiniDFSCluster while there are ongoing block replication tasks.",
                "Observe the logs for NullPointerException related to BlocksMap."
            ],
            "ExpectedBehavior": "The MiniDFSCluster should shut down gracefully without throwing exceptions, ensuring that all resources are released properly.",
            "ObservedBehavior": "The MiniDFSCluster experiences a fatal exception during shutdown, resulting in an unexpected exit due to a NullPointerException.",
            "Suggestions": "Implement proper synchronization mechanisms to ensure that the BlockManager and NameNode do not interfere with each other during shutdown. Consider using a shutdown hook or a more robust locking strategy to prevent race conditions.",
            "problem_location": {
                "files": [
                    "MiniDFSCluster.java",
                    "BlocksMap.java",
                    "BlockManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap",
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager",
                    "org.apache.hadoop.hdfs.server.namenode.NameNode"
                ],
                "methods": [
                    "BlocksMap.getBlockCollection",
                    "BlockManager.computeReplicationWorkForBlocks",
                    "BlockManager.computeReplicationWork",
                    "BlockManager.computeDatanodeWork"
                ]
            },
            "possible_fix": "To fix the issue, ensure that the BlockManager's access to the BlocksMap is properly synchronized during the shutdown process. This can be achieved by introducing a shutdown flag that prevents the BlockManager from attempting to access the BlocksMap while the NameNode is in the process of shutting down. Additionally, consider refactoring the shutdown logic to ensure that all threads are properly joined before releasing locks."
        },
        "possible_fix_code": {
            "BlockManager.computeReplicationWorkForBlocks": "  int computeReplicationWorkForBlocks(List<List<Block>> blocksToReplicate) {\n    int requiredReplication, numEffectiveReplicas;\n    List<DatanodeDescriptor> containingNodes, liveReplicaNodes;\n    DatanodeDescriptor srcNode;\n    BlockCollection bc = null;\n    int additionalReplRequired;\n\n    int scheduledWork = 0;\n    List<ReplicationWork> work = new LinkedList<ReplicationWork>();\n\n    namesystem.writeLock();\n    try {\n      synchronized (neededReplications) {\n        for (int priority = 0; priority < blocksToReplicate.size(); priority++) {\n          for (Block block : blocksToReplicate.get(priority)) {\n            // Check if the BlockManager is shutting down\n            if (isShuttingDown) {\n              return scheduledWork; // Exit early if shutting down\n            }\n            // block should belong to a file\n            bc = blocksMap.getBlockCollection(block);\n            // abandoned block or block reopened for append\n            if(bc == null || bc instanceof MutableBlockCollection) {\n              neededReplications.remove(block, priority); // remove from neededReplications\n              neededReplications.decrementReplicationIndex(priority);\n              continue;\n            }\n\n            requiredReplication = bc.getReplication();\n\n            // get a source data-node\n            containingNodes = new ArrayList<DatanodeDescriptor>();\n            liveReplicaNodes = new ArrayList<DatanodeDescriptor>();\n            NumberReplicas numReplicas = new NumberReplicas();\n            srcNode = chooseSourceDatanode(\n                block, containingNodes, liveReplicaNodes, numReplicas);\n            if(srcNode == null) { // block can not be replicated from any node\n              LOG.debug(\"Block \" + block + \" cannot be repl from any node\");\n              continue;\n            }\n\n            assert liveReplicaNodes.size() == numReplicas.liveReplicas();\n            // do not schedule more if enough replicas is already pending\n            numEffectiveReplicas = numReplicas.liveReplicas() +\n                                    pendingReplications.getNumReplicas(block);\n\n            if (numEffectiveReplicas >= requiredReplication) {\n              if ( (pendingReplications.getNumReplicas(block) > 0) ||\n                   (blockHasEnoughRacks(block)) ) {\n                neededReplications.remove(block, priority); // remove from neededReplications\n                neededReplications.decrementReplicationIndex(priority);\n                NameNode.stateChangeLog.info(\"BLOCK* \"\n                    + \"Removing block \" + block\n                    + \" from neededReplications as it has enough replicas.\");\n                continue;\n              }\n            }\n\n            if (numReplicas.liveReplicas() < requiredReplication) {\n              additionalReplRequired = requiredReplication\n                  - numEffectiveReplicas;\n            } else {\n              additionalReplRequired = 1; // Needed on a new rack\n            }\n            work.add(new ReplicationWork(block, bc, srcNode,\n                containingNodes, liveReplicaNodes, additionalReplRequired,\n                priority));\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    HashMap<Node, Node> excludedNodes\n        = new HashMap<Node, Node>();\n    for(ReplicationWork rw : work){\n      // Exclude all of the containing nodes from being targets.\n      // This list includes decommissioning or corrupt nodes.\n      excludedNodes.clear();\n      for (DatanodeDescriptor dn : rw.containingNodes) {\n        excludedNodes.put(dn, dn);\n      }\n\n      // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n      // It is costly to extract the filename for which chooseTargets is called,\n      // so for now we pass in the block collection itself.\n      rw.targets = blockplacement.chooseTarget(rw.bc,\n          rw.additionalReplRequired, rw.srcNode, rw.liveReplicaNodes,\n          excludedNodes, rw.block.getNumBytes());\n    }\n\n    namesystem.writeLock();\n    try {\n      for(ReplicationWork rw : work){\n        DatanodeDescriptor[] targets = rw.targets;\n        if(targets == null || targets.length == 0){\n          rw.targets = null;\n          continue;\n        }\n\n        synchronized (neededReplications) {\n          Block block = rw.block;\n          int priority = rw.priority;\n          // Recheck since global lock was released\n          // block should belong to a file\n          bc = blocksMap.getBlockCollection(block);\n          // abandoned block or block reopened for append\n          if(bc == null || bc instanceof MutableBlockCollection) {\n            neededReplications.remove(block, priority); // remove from neededReplications\n            rw.targets = null;\n            neededReplications.decrementReplicationIndex(priority);\n            continue;\n          }\n          requiredReplication = bc.getReplication();\n\n          // do not schedule more if enough replicas is already pending\n          NumberReplicas numReplicas = countNodes(block);\n          numEffectiveReplicas = numReplicas.liveReplicas() +\n            pendingReplications.getNumReplicas(block);\n\n          if (numEffectiveReplicas >= requiredReplication) {\n            if ( (pendingReplications.getNumReplicas(block) > 0) ||\n                 (blockHasEnoughRacks(block)) ) {\n              neededReplications.remove(block, priority); // remove from neededReplications\n              neededReplications.decrementReplicationIndex(priority);\n              rw.targets = null;\n              NameNode.stateChangeLog.info(\"BLOCK* \"\n                  + \"Removing block \" + block\n                  + \" from neededReplications as it has enough replicas.\");\n              continue;\n            }\n          }\n\n          if ( (numReplicas.liveReplicas() >= requiredReplication) &&\n               (!blockHasEnoughRacks(block)) ) {\n            if (rw.srcNode.getNetworkLocation().equals(targets[0].getNetworkLocation())) {\n              //No use continuing, unless a new rack in this case\n              continue;\n            }\n          }\n\n          // Add block to the to be replicated list\n          rw.srcNode.addBlockToBeReplicated(block, targets);\n          scheduledWork++;\n\n          for (DatanodeDescriptor dn : targets) {\n            dn.incBlocksScheduled();\n          }\n\n          // Move the block-replication into a \"pending\" state.\n          // The reason we use 'pending' is so we can retry\n          // replications that fail after an appropriate amount of time.\n          pendingReplications.add(block, targets.length);\n          if(NameNode.stateChangeLog.isDebugEnabled()) {\n            NameNode.stateChangeLog.debug(\n                \"BLOCK* block \" + block\n                + \" is moved from neededReplications to pendingReplications\");\n          }\n\n          // remove from neededReplications\n          if(numEffectiveReplicas + targets.length >= requiredReplication) {\n            neededReplications.remove(block, priority); // remove from neededReplications\n            neededReplications.decrementReplicationIndex(priority);\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    if (NameNode.stateChangeLog.isInfoEnabled()) {\n      // log which blocks have been scheduled for replication\n      for(ReplicationWork rw : work){\n        DatanodeDescriptor[] targets = rw.targets;\n        if (targets != null && targets.length != 0) {\n          StringBuilder targetList = new StringBuilder(\"datanode(s)\");\n          for (int k = 0; k < targets.length; k++) {\n            targetList.append(' ');\n            targetList.append(targets[k]);\n          }\n          NameNode.stateChangeLog.info(\n                  \"BLOCK* ask \"\n                  + rw.srcNode + \" to replicate \"\n                  + rw.block + \" to \" + targetList);\n        }\n      }\n    }\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n        NameNode.stateChangeLog.debug(\n          \"BLOCK* neededReplications = \" + neededReplications.size()\n          + \" pendingReplications = \" + pendingReplications.size());\n    }\n\n    return scheduledWork;\n  }"
        }
    },
    {
        "filename": "HDFS-6348.json",
        "creation_time": "2014-05-07T12:34:54.000+0000",
        "bug_report": {
            "Title": "SecondaryNameNode not terminating properly on runtime exceptions",
            "Description": "The Secondary NameNode fails to terminate when a RuntimeException occurs during startup due to invalid configuration. Specifically, a ClassNotFoundException is thrown when the system attempts to load a class that is not present. This results in the RMI thread remaining active, preventing the JVM from exiting as it is not a daemon thread.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found",
                "at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1900)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.getInstance(BlockPlacementPolicy.java:199)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:256)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:635)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:260)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init>(SecondaryNameNode.java:205)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:695)",
                "Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found",
                "at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1868)",
                "at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1892)",
                "Caused by: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found",
                "at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1774)",
                "at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1866)"
            ],
            "RootCause": "The root cause of the issue is that the Secondary NameNode does not handle the RuntimeException properly during its initialization phase, specifically when a required class cannot be found. This leads to the RMI thread remaining active, preventing the JVM from shutting down.",
            "StepsToReproduce": [
                "1. Configure the Secondary NameNode with an invalid class name for the block placement policy.",
                "2. Start the Secondary NameNode.",
                "3. Observe that the process does not terminate despite the RuntimeException being thrown."
            ],
            "ExpectedBehavior": "The Secondary NameNode should terminate gracefully when a RuntimeException occurs during startup, releasing all resources and threads.",
            "ObservedBehavior": "The Secondary NameNode remains alive, and the JVM does not exit due to the RMI thread being active.",
            "Suggestions": "Implement proper exception handling in the SecondaryNameNode's initialization method to ensure that all threads are terminated when a RuntimeException occurs.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode",
                    "org.apache.hadoop.conf.Configuration"
                ],
                "methods": [
                    "SecondaryNameNode.main",
                    "SecondaryNameNode.initialize",
                    "Configuration.getClass"
                ]
            },
            "possible_fix": "In the 'initialize' method of the SecondaryNameNode class, add a catch block for RuntimeException that calls a method to terminate the RMI thread and exit the JVM. For example:\n\n```java\ntry {\n    // existing initialization code\n} catch (RuntimeException e) {\n    LOG.fatal(\"Initialization failed, terminating...\", e);\n    terminate(1);\n}\n```"
        },
        "possible_fix_code": {
            "SecondaryNameNode.initialize": "  private void initialize(final Configuration conf,\n      CommandLineOpts commandLineOpts) throws IOException {\n    final InetSocketAddress infoSocAddr = getHttpAddress(conf);\n    final String infoBindAddress = infoSocAddr.getHostName();\n    UserGroupInformation.setConfiguration(conf);\n    if (UserGroupInformation.isSecurityEnabled()) {\n      SecurityUtil.login(conf,\n          DFSConfigKeys.DFS_SECONDARY_NAMENODE_KEYTAB_FILE_KEY,\n          DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY, infoBindAddress);\n    }\n    // initiate Java VM metrics\n    DefaultMetricsSystem.initialize(\"SecondaryNameNode\");\n    JvmMetrics.create(\"SecondaryNameNode\",\n        conf.get(DFSConfigKeys.DFS_METRICS_SESSION_ID_KEY),\n        DefaultMetricsSystem.instance());\n\n    // Create connection to the namenode.\n    shouldRun = true;\n    nameNodeAddr = NameNode.getServiceAddress(conf, true);\n\n    this.conf = conf;\n    this.namenode = NameNodeProxies.createNonHAProxy(conf, nameNodeAddr, \n        NamenodeProtocol.class, UserGroupInformation.getCurrentUser(),\n        true).getProxy();\n\n    // initialize checkpoint directories\n    fsName = getInfoServer();\n    checkpointDirs = FSImage.getCheckpointDirs(conf,\n                                  \"/tmp/hadoop/dfs/namesecondary\");\n    checkpointEditsDirs = FSImage.getCheckpointEditsDirs(conf,\n                                  \"/tmp/hadoop/dfs/namesecondary\");    \n    checkpointImage = new CheckpointStorage(conf, checkpointDirs, checkpointEditsDirs);\n    checkpointImage.recoverCreate(commandLineOpts.shouldFormat());\n    checkpointImage.deleteTempEdits();\n    \n    try {\n        namesystem = new FSNamesystem(conf, checkpointImage, true);\n    } catch (RuntimeException e) {\n        LOG.fatal(\"Initialization failed, terminating...\", e);\n        terminate(1);\n    }\n\n    // Disable quota checks\n    namesystem.dir.disableQuotaChecks();\n\n    // Initialize other scheduling parameters from the configuration\n    checkpointConf = new CheckpointConf(conf);\n\n    final InetSocketAddress httpAddr = infoSocAddr;\n\n    final String httpsAddrString = conf.get(\n        DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTPS_ADDRESS_KEY,\n        DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTPS_ADDRESS_DEFAULT);\n    InetSocketAddress httpsAddr = NetUtils.createSocketAddr(httpsAddrString);\n\n    HttpServer2.Builder builder = DFSUtil.httpServerTemplateForNNAndJN(conf,\n        httpAddr, httpsAddr, \"secondary\",\n        DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_INTERNAL_SPNEGO_PRINCIPAL_KEY,\n        DFSConfigKeys.DFS_SECONDARY_NAMENODE_KEYTAB_FILE_KEY);\n\n    nameNodeStatusBeanName = MBeans.register(\"SecondaryNameNode\",\n            \"SecondaryNameNodeInfo\", this);\n\n    infoServer = builder.build();\n\n    infoServer.setAttribute(\"secondary.name.node\", this);\n    infoServer.setAttribute(\"name.system.image\", checkpointImage);\n    infoServer.setAttribute(JspHelper.CURRENT_CONF, conf);\n    infoServer.addInternalServlet(\"imagetransfer\", ImageServlet.PATH_SPEC,\n        ImageServlet.class, true);\n    infoServer.start();\n\n    LOG.info(\"Web server init done\");\n\n    HttpConfig.Policy policy = DFSUtil.getHttpPolicy(conf);\n    int connIdx = 0;\n    if (policy.isHttpEnabled()) {\n      InetSocketAddress httpAddress = infoServer.getConnectorAddress(connIdx++);\n      conf.set(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,\n          NetUtils.getHostPortString(httpAddress));\n    }\n\n    if (policy.isHttpsEnabled()) {\n      InetSocketAddress httpsAddress = infoServer.getConnectorAddress(connIdx);\n      conf.set(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTPS_ADDRESS_KEY,\n          NetUtils.getHostPortString(httpsAddress));\n    }\n\n    LOG.info(\"Checkpoint Period   :\" + checkpointConf.getPeriod() + \" secs \"\n        + \"(\" + checkpointConf.getPeriod() / 60 + \" min)\");\n    LOG.info(\"Log Size Trigger    :\" + checkpointConf.getTxnCount() + \" txns\");\n  }"
        }
    },
    {
        "filename": "HDFS-7884.json",
        "creation_time": "2015-03-04T15:47:50.000+0000",
        "bug_report": {
            "Title": "NullPointerException in BlockSender",
            "Description": "A NullPointerException occurs in the BlockSender constructor when attempting to obtain a volume reference for a block. This issue arises when the 'datanode.data.getVolume(block)' call returns null, indicating that the block does not have an associated volume, which leads to the exception at line 264 of BlockSender.java.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:264)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:506)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:249)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the NullPointerException is that the method 'datanode.data.getVolume(block)' returns null, which indicates that the block does not have a valid volume reference.",
            "StepsToReproduce": [
                "Attempt to read a block that does not exist or is not properly initialized in the datanode.",
                "Ensure that the block being requested does not have an associated volume."
            ],
            "ExpectedBehavior": "The system should handle the case where a block does not have an associated volume gracefully, either by returning an error message or by not attempting to create a BlockSender instance.",
            "ObservedBehavior": "The system throws a NullPointerException when trying to create a BlockSender instance due to a null volume reference.",
            "Suggestions": "Implement a null check for the volume reference before attempting to obtain it. If the volume reference is null, handle the situation appropriately by logging an error and returning a meaningful response to the client.",
            "problem_location": {
                "files": [
                    "BlockSender.java",
                    "DataXceiver.java",
                    "Receiver.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.BlockSender",
                    "org.apache.hadoop.hdfs.server.datanode.DataXceiver",
                    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver"
                ],
                "methods": [
                    "BlockSender.<init>",
                    "DataXceiver.readBlock",
                    "Receiver.opReadBlock",
                    "Receiver.processOp"
                ]
            },
            "possible_fix": "In BlockSender.java, modify the constructor to include a null check for the volume reference:\n\n```java\nif (datanode.data.getVolume(block) == null) {\n    throw new IOException(\"No volume found for block: \" + block);\n}\nthis.volumeRef = datanode.data.getVolume(block).obtainReference();\n```"
        },
        "possible_fix_code": {
            "BlockSender.<init>": "  public BlockSender(ExtendedBlock block, long blockOffset, long length,\n      boolean sendChecksum, boolean isRecovery, boolean isShortCircuit,\n      Datanode datanode, String clientTraceFmt,\n      CachingStrategy cachingStrategy) throws IOException {\n    this.block = block;\n    this.blockOffset = blockOffset;\n    this.length = length;\n    this.sendChecksum = sendChecksum;\n    this.isRecovery = isRecovery;\n    this.isShortCircuit = isShortCircuit;\n    this.datanode = datanode;\n    this.clientTraceFmt = clientTraceFmt;\n\n    // Check for null volume reference\n    Volume volume = datanode.data.getVolume(block);\n    if (volume == null) {\n        throw new IOException(\"No volume found for block: \" + block);\n    }\n    this.volumeRef = volume.obtainReference();\n  }"
        }
    },
    {
        "filename": "HDFS-7996.json",
        "creation_time": "2015-03-26T21:19:55.000+0000",
        "bug_report": {
            "Title": "After swapping a volume, BlockReceiver reports ReplicaNotFoundException",
            "Description": "The issue arises when a disk is removed from an actively writing DataNode. The BlockReceiver, which is responsible for managing block data, throws a ReplicaNotFoundException because the replicas are removed from memory before the BlockReceiver can finalize the block. This occurs due to the timing of the volume removal and the block finalization process, where the FsVolumeReference is released prematurely.",
            "StackTrace": [
                "org.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Cannot append to a non-existent replica BP-51301509-10.20.202.114-1427296597742:blk_1073742979_2160",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo(FsDatasetImpl.java:615)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.finalizeBlock(FsDatasetImpl.java:1362)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.finalizeBlock(BlockReceiver.java:1281)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1241)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the FsVolumeReference is released in BlockReceiver.this.close() before the finalizeBlock method is called, leading to the removal of the replica from memory while it is still needed for finalization.",
            "StepsToReproduce": [
                "1. Start a DataNode and begin writing data to a disk.",
                "2. While the DataNode is actively writing, remove the disk from the DataNode.",
                "3. Observe the logs for the ReplicaNotFoundException."
            ],
            "ExpectedBehavior": "The BlockReceiver should be able to finalize the block without throwing a ReplicaNotFoundException, even if the volume is removed.",
            "ObservedBehavior": "The BlockReceiver throws a ReplicaNotFoundException when attempting to finalize a block after the volume has been removed.",
            "Suggestions": "Implement a mechanism to ensure that the BlockReceiver does not release the FsVolumeReference until after the block has been finalized. This may involve reordering the operations in the finalizeBlock method or adding synchronization to prevent premature release.",
            "problem_location": {
                "files": [
                    "FsDatasetImpl.java",
                    "BlockReceiver.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl",
                    "org.apache.hadoop.hdfs.server.datanode.BlockReceiver"
                ],
                "methods": [
                    "FsDatasetImpl.finalizeBlock",
                    "FsDatasetImpl.getReplicaInfo",
                    "BlockReceiver.PacketResponder.finalizeBlock"
                ]
            },
            "possible_fix": "Modify the finalizeBlock method in BlockReceiver to ensure that the FsVolumeReference is not released until after the block has been finalized. This could involve changing the order of operations or adding a check to ensure that the block is still valid before proceeding with the finalization."
        },
        "possible_fix_code": {
            "BlockReceiver$PacketResponder.finalizeBlock": "public void finalizeBlock(ExtendedBlock b) throws IOException {\n    // Ensure that the FsVolumeReference is not released until after the block has been finalized\n    try {\n        // Finalize the block first\n        FsDatasetImpl dataset = blockReceiver.getDataset();\n        dataset.finalizeBlock(b);\n    } finally {\n        // Now it's safe to release the FsVolumeReference\n        blockReceiver.close();\n    }\n}"
        }
    },
    {
        "filename": "HDFS-4302.json",
        "creation_time": "2012-12-11T19:44:59.000+0000",
        "bug_report": {
            "Title": "Precondition in EditLogFileInputStream's length() method is checked too early in NameNode startup, causing fatal exception",
            "Description": "During the startup of the NameNode in standby mode with DEBUG logging enabled, the system attempts to log the length of the edit log before the EditLogFileInputStream's advertisedSize is initialized. This premature access leads to an IllegalStateException due to a failed precondition check in the length() method of EditLogFileInputStream, which expects the input stream to be available before the length can be determined.",
            "StackTrace": [
                "2012-12-11 10:45:33,464 FATAL namenode.NameNode (NameNode.java:main(1224)) - Exception in namenode join",
                "java.lang.IllegalStateException: must get input stream before length is available",
                "at com.google.common.base.Preconditions.checkState(Preconditions.java:145)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$URLLog.length(EditLogFileInputStream.java:405)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.length(EditLogFileInputStream.java:258)",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:125)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:88)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:697)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:604)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:447)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:400)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:434)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:606)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1153)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1219)"
            ],
            "RootCause": "The root cause of the issue is that the length() method of EditLogFileInputStream is called before the input stream is properly initialized, specifically before the advertisedSize is set. This occurs during the logging of the edit log length in FSEditLogLoader.loadEditRecords.",
            "StepsToReproduce": [
                "Start the NameNode in standby mode with DEBUG logging enabled.",
                "Observe the logs for the exception related to the EditLogFileInputStream's length method."
            ],
            "ExpectedBehavior": "The NameNode should start up without throwing an exception, and the edit log length should be logged correctly after the input stream is initialized.",
            "ObservedBehavior": "The NameNode throws an IllegalStateException during startup, causing it to shut down.",
            "Suggestions": "Delay the logging of the edit log length until after the EditLogFileInputStream has been fully initialized. This can be achieved by moving the logging statement to a point in the code where the input stream is guaranteed to be ready.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileInputStream.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader",
                    "org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream"
                ],
                "methods": [
                    "FSEditLogLoader.loadEditRecords",
                    "EditLogFileInputStream.length"
                ]
            },
            "possible_fix": "Modify the FSEditLogLoader.loadEditRecords method to ensure that the logging of the edit log length occurs after the EditLogFileInputStream has been initialized. For example, check if the input stream is ready before logging its length."
        },
        "possible_fix_code": {
            "FSEditLogLoader.loadEditRecords": "  long loadEditRecords(EditLogInputStream in, boolean closeOnExit,\n                      long expectedStartingTxId, MetaRecoveryContext recovery)\n      throws IOException {\n    FSDirectory fsDir = fsNamesys.dir;\n\n    EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts =\n      new EnumMap<FSEditLogOpCodes, Holder<Integer>>(FSEditLogOpCodes.class);\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Acquiring write lock to replay edit log\");\n    }\n\n    fsNamesys.writeLock();\n    fsDir.writeLock();\n\n    long recentOpcodeOffsets[] = new long[4];\n    Arrays.fill(recentOpcodeOffsets, -1);\n    \n    long expectedTxId = expectedStartingTxId;\n    long numEdits = 0;\n    long lastTxId = in.getLastTxId();\n    long numTxns = (lastTxId - expectedStartingTxId) + 1;\n    long lastLogTime = now();\n\n    // Delay logging the edit log length until after the input stream is initialized\n    try {\n      while (true) {\n        try {\n          FSEditLogOp op;\n          try {\n            op = in.readOp();\n            if (op == null) {\n              break;\n            }\n          } catch (Throwable e) {\n            check203UpgradeFailure(in.getVersion(), e);\n            String errorMessage =\n              formatEditLogReplayError(in, recentOpcodeOffsets, expectedTxId);\n            FSImage.LOG.error(errorMessage, e);\n            if (recovery == null) {\n              throw new EditLogInputException(errorMessage, e, numEdits);\n            }\n            MetaRecoveryContext.editLogLoaderPrompt(\n                \"We failed to read txId \" + expectedTxId,\n                recovery, \"skipping the bad section in the log\");\n            in.resync();\n            continue;\n          }\n          recentOpcodeOffsets[(int)(numEdits % recentOpcodeOffsets.length)] =\n            in.getPosition();\n          if (op.hasTransactionId()) {\n            if (op.getTransactionId() > expectedTxId) { \n              MetaRecoveryContext.editLogLoaderPrompt(\"There appears \" +\n                  \"to be a gap in the edit log.  We expected txid \" +\n                  expectedTxId + \", but got txid \" +\n                  op.getTransactionId() + \".\", recovery, \"ignoring missing \" +\n                  \" transaction IDs\");\n            } else if (op.getTransactionId() < expectedTxId) { \n              MetaRecoveryContext.editLogLoaderPrompt(\"There appears \" +\n                  \"to be an out-of-order edit in the edit log.  We \" +\n                  \"expected txid \" + expectedTxId + \", but got txid \" +\n                  op.getTransactionId() + \".\", recovery,\n                  \"skipping the out-of-order edit\");\n              continue;\n            }\n          }\n          try {\n            applyEditLogOp(op, fsDir, in.getVersion());\n          } catch (Throwable e) {\n            LOG.error(\"Encountered exception on operation \" + op, e);\n            MetaRecoveryContext.editLogLoaderPrompt(\"Failed to \" +\n             \"apply edit log operation \" + op + \": error \" +\n             e.getMessage(), recovery, \"applying edits\");\n          }\n          // Now that the operation has been successfully decoded and\n          // applied, update our bookkeeping.\n          incrOpCount(op.opCode, opCounts);\n          if (op.hasTransactionId()) {\n            lastAppliedTxId = op.getTransactionId();\n            expectedTxId = lastAppliedTxId + 1;\n          } else {\n            expectedTxId = lastAppliedTxId = expectedStartingTxId;\n          }\n          // log progress\n          if (op.hasTransactionId()) {\n            long now = now();\n            if (now - lastLogTime > REPLAY_TRANSACTION_LOG_INTERVAL) {\n              long deltaTxId = lastAppliedTxId - expectedStartingTxId + 1;\n              int percent = Math.round((float) deltaTxId / numTxns * 100);\n              LOG.info(\"replaying edit log: \" + deltaTxId + \"/\" + numTxns\n                  + \" transactions completed. (\" + percent + \"%)\");\n              lastLogTime = now;\n            }\n          }\n          numEdits++;\n        } catch (MetaRecoveryContext.RequestStopException e) {\n          MetaRecoveryContext.LOG.warn(\"Stopped reading edit log at \" +\n              in.getPosition() + \"/\"  + in.length());\n          break;\n        }\n      }\n    } finally {\n      if(closeOnExit) {\n        in.close();\n      }\n      fsDir.writeUnlock();\n      fsNamesys.writeUnlock();\n\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"replaying edit log finished\");\n      }\n\n      if (FSImage.LOG.isDebugEnabled()) {\n        dumpOpCounts(opCounts);\n      }\n    }\n    return numEdits;\n  }"
        }
    },
    {
        "filename": "HDFS-11849.json",
        "creation_time": "2017-05-18T11:31:48.000+0000",
        "bug_report": {
            "Title": "JournalNode startup failure exception should be logged in log file",
            "Description": "The JournalNode fails to start due to a Kerberos login failure, which is not being logged in the log file. The exception thrown is a LoginException indicating a login failure for the specified user and keytab. This issue arises during the execution of the `start` method in the `JournalNode` class, where the `SecurityUtil.login` method is called to perform the Kerberos authentication. If the login fails, the exception should be caught and logged appropriately, but it appears that the logging mechanism is not functioning as expected.",
            "StackTrace": [
                "Exception in thread \"main\" java.io.IOException: Login failure for xxx/yyyy@ZZZZ.COM from keytab dummy.keytab: javax.security.auth.login.LoginException: host1",
                "at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:994)",
                "at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:281)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNode.start(JournalNode.java:153)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNode.run(JournalNode.java:132)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNode.main(JournalNode.java:318)"
            ],
            "RootCause": "The root cause of the issue is that the exception thrown during the Kerberos login process is not being logged. Specifically, the `LoginException` is caught in the `loginUserFromKeytab` method, but it is wrapped in a `KerberosAuthException` and thrown without being logged.",
            "StepsToReproduce": [
                "Configure the JournalNode with an invalid keytab file or incorrect principal.",
                "Start the JournalNode.",
                "Observe the logs to check if the login failure exception is recorded."
            ],
            "ExpectedBehavior": "The JournalNode should log any exceptions that occur during the startup process, including Kerberos login failures.",
            "ObservedBehavior": "The JournalNode fails to start due to a Kerberos login failure, but the exception is not logged in the log file.",
            "Suggestions": "Modify the `loginUserFromKeytab` method to log the `LoginException` before throwing the `KerberosAuthException`. Additionally, ensure that the logging configuration is set up correctly to capture such exceptions.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.UserGroupInformation",
                    "org.apache.hadoop.hdfs.qjournal.server.JournalNode"
                ],
                "methods": [
                    "UserGroupInformation.loginUserFromKeytab",
                    "SecurityUtil.login",
                    "JournalNode.start"
                ]
            },
            "possible_fix": "In the `loginUserFromKeytab` method, add a logging statement to log the `LoginException` before throwing the `KerberosAuthException`. For example:\n\n```java\ncatch (LoginException le) {\n    LOG.error(\"Login failed for user \" + user + \" using keytab file \" + path, le);\n    // existing code to throw KerberosAuthException\n}\n```"
        },
        "possible_fix_code": {
            "UserGroupInformation.loginUserFromKeytab": "  static void loginUserFromKeytab(String user,\n                                  String path\n                                  ) throws IOException {\n    if (!isSecurityEnabled())\n      return;\n\n    keytabFile = path;\n    keytabPrincipal = user;\n    Subject subject = new Subject();\n    LoginContext login; \n    long start = 0;\n    try {\n      login = newLoginContext(HadoopConfiguration.KEYTAB_KERBEROS_CONFIG_NAME,\n            subject, new HadoopConfiguration());\n      start = Time.now();\n      login.login();\n      metrics.loginSuccess.add(Time.now() - start);\n      loginUser = new UserGroupInformation(subject, false);\n      loginUser.setLogin(login);\n      loginUser.setAuthenticationMethod(AuthenticationMethod.KERBEROS);\n    } catch (LoginException le) {\n      if (start > 0) {\n        metrics.loginFailure.add(Time.now() - start);\n      }\n      LOG.error(\"Login failed for user \" + user + \" using keytab file \" + path, le);\n      KerberosAuthException kae = new KerberosAuthException(LOGIN_FAILURE, le);\n      kae.setUser(user);\n      kae.setKeytabFile(path);\n      throw kae;\n    }\n    LOG.info(\"Login successful for user \" + keytabPrincipal\n        + \" using keytab file \" + keytabFile);\n  }"
        }
    },
    {
        "filename": "HDFS-4841.json",
        "creation_time": "2013-05-22T17:01:36.000+0000",
        "bug_report": {
            "Title": "FsShell commands using secure webhdfs fail ClientFinalizer shutdown hook",
            "Description": "When executing FsShell commands with the webhdfs:// URI while security is enabled, a warning is generated indicating that the ShutdownHook 'ClientFinalizer' has failed. This occurs due to an IllegalStateException being thrown when attempting to add a shutdown hook during an ongoing shutdown process.",
            "StackTrace": [
                "java.lang.IllegalStateException: Shutdown in progress, cannot add a shutdownHook",
                "at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:152)",
                "at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2400)",
                "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2372)",
                "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:352)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$DtRenewer.getWebHdfs(WebHdfsFileSystem.java:1001)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$DtRenewer.cancel(WebHdfsFileSystem.java:1013)",
                "at org.apache.hadoop.security.token.Token.cancel(Token.java:382)",
                "at org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction.cancel(DelegationTokenRenewer.java:152)",
                "at org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction.access$200(DelegationTokenRenewer.java:58)",
                "at org.apache.hadoop.fs.DelegationTokenRenewer.removeRenewAction(DelegationTokenRenewer.java:241)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.close(WebHdfsFileSystem.java:822)",
                "at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2446)",
                "at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2463)",
                "at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)"
            ],
            "RootCause": "The root cause of the issue is that the ShutdownHookManager is attempting to add a shutdown hook while a shutdown process is already in progress, leading to an IllegalStateException.",
            "StepsToReproduce": [
                "Enable security in the Hadoop configuration.",
                "Run a FsShell command using the webhdfs:// URI.",
                "Observe the warning message regarding the ShutdownHook 'ClientFinalizer' failure."
            ],
            "ExpectedBehavior": "The FsShell command should execute successfully without any warnings or errors related to shutdown hooks.",
            "ObservedBehavior": "The command completes but generates a warning indicating that the ShutdownHook 'ClientFinalizer' has failed due to an ongoing shutdown process.",
            "Suggestions": "Review the shutdown process to ensure that no new shutdown hooks are added during an active shutdown. Consider implementing a check to prevent adding hooks if a shutdown is already in progress.",
            "problem_location": {
                "files": [
                    "ShutdownHookManager.java",
                    "FileSystem.java",
                    "WebHdfsFileSystem.java",
                    "DelegationTokenRenewer.java"
                ],
                "classes": [
                    "org.apache.hadoop.util.ShutdownHookManager",
                    "org.apache.hadoop.fs.FileSystem",
                    "org.apache.hadoop.hdfs.web.WebHdfsFileSystem",
                    "org.apache.hadoop.fs.DelegationTokenRenewer"
                ],
                "methods": [
                    "ShutdownHookManager.addShutdownHook",
                    "FileSystem.get",
                    "WebHdfsFileSystem.close",
                    "DelegationTokenRenewer.removeRenewAction"
                ]
            },
            "possible_fix": "Modify the addShutdownHook method in ShutdownHookManager to check if a shutdown is in progress before attempting to add a new hook. This can prevent the IllegalStateException from being thrown."
        },
        "possible_fix_code": {
            "ShutdownHookManager.addShutdownHook": "  public void addShutdownHook(Runnable shutdownHook, int priority) {\n    if (shutdownHook == null) {\n      throw new IllegalArgumentException(\"shutdownHook cannot be NULL\");\n    }\n    if (shutdownInProgress.get()) {\n      // Log a warning instead of throwing an exception\n      LOG.warn(\"Shutdown in progress, cannot add a shutdownHook\");\n      return; // Prevent adding the shutdown hook\n    }\n    hooks.add(new HookEntry(shutdownHook, priority));\n  }"
        }
    },
    {
        "filename": "HDFS-3384.json",
        "creation_time": "2012-05-08T09:01:10.000+0000",
        "bug_report": {
            "Title": "DataStreamer thread should be closed immediately when failed to setup a PipelineForAppendOrRecovery",
            "Description": "The DataStreamer thread fails to close properly when an exception occurs during the setup of a pipeline for appending or recovering data. This issue arises when a corrupted block is manually introduced, leading to an EOFException in the createBlockOutputStream method. The subsequent NullPointerException indicates that the DataStreamer is not handling the failure scenario correctly, resulting in a failure to abort the operation gracefully.",
            "StackTrace": [
                "2012-04-19 09:33:10,776 INFO  hdfs.DFSClient (DFSOutputStream.java:createBlockOutputStream(1059)) - Exception in createBlockOutputStream",
                "java.io.EOFException: Premature EOF: no length prefix available",
                "\tat org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)",
                "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1039)",
                "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:939)",
                "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)",
                "2012-04-19 09:33:10,807 WARN  hdfs.DFSClient (DFSOutputStream.java:run(549)) - DataStreamer Exception",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:510)",
                "2012-04-19 09:33:10,807 WARN  hdfs.DFSClient (DFSOutputStream.java:hflush(1511)) - Error while syncing",
                "java.io.IOException: All datanodes 10.18.40.20:50010 are bad. Aborting...",
                "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:908)",
                "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)"
            ],
            "RootCause": "The root cause of the issue is that the DataStreamer does not handle the EOFException properly, leading to a NullPointerException when it attempts to continue processing after a failure in the setupPipelineForAppendOrRecovery method.",
            "StepsToReproduce": [
                "1. Write a file to HDFS.",
                "2. Manually corrupt a block of the file.",
                "3. Call the append method on the file."
            ],
            "ExpectedBehavior": "The DataStreamer should close immediately and abort the operation when it fails to set up a pipeline due to an EOFException or any other exception.",
            "ObservedBehavior": "The DataStreamer continues to run and throws a NullPointerException, leading to an unstable state in the system.",
            "Suggestions": "Implement proper exception handling in the setupPipelineForAppendOrRecovery method to ensure that the DataStreamer thread is closed immediately upon encountering an EOFException or any other critical error.",
            "problem_location": {
                "files": [
                    "DFSOutputStream.java",
                    "HdfsProtoUtil.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.DFSOutputStream",
                    "org.apache.hadoop.hdfs.protocol.HdfsProtoUtil"
                ],
                "methods": [
                    "DFSOutputStream.DataStreamer.setupPipelineForAppendOrRecovery",
                    "HdfsProtoUtil.vintPrefixed"
                ]
            },
            "possible_fix": "In the setupPipelineForAppendOrRecovery method, add a catch block for EOFException to close the DataStreamer thread and abort the operation gracefully. For example:\n\ntry {\n    // existing code\n} catch (EOFException e) {\n    // Log the error\n    close(); // Ensure the DataStreamer is closed\n    throw new IOException(\"Failed to setup pipeline\", e);\n}"
        },
        "possible_fix_code": {
            "DFSOutputStream.DataStreamer.setupPipelineForAppendOrRecovery": "public void setupPipelineForAppendOrRecovery() throws IOException {\n    try {\n        // existing code for setting up the pipeline\n    } catch (EOFException e) {\n        // Log the error\n        LOG.error(\"Failed to setup pipeline due to EOFException\", e);\n        close(); // Ensure the DataStreamer is closed\n        throw new IOException(\"Failed to setup pipeline\", e);\n    } catch (Exception e) {\n        // Handle other exceptions if necessary\n        LOG.error(\"Failed to setup pipeline due to unexpected exception\", e);\n        close(); // Ensure the DataStreamer is closed\n        throw new IOException(\"Failed to setup pipeline\", e);\n    }\n}"
        }
    },
    {
        "filename": "HDFS-5657.json",
        "creation_time": "2013-12-11T21:59:46.000+0000",
        "bug_report": {
            "Title": "Race condition causes writeback state error in NFS gateway",
            "Description": "A race condition exists between the NFS gateway's writeback executor thread and the new write handler thread. This condition can lead to a failure in the writeback state check, resulting in an IllegalStateException when the async status of the OpenFileCtx is false. The issue arises when the write handler attempts to execute a write operation while the writeback executor has already reset the async status, causing the system to incorrectly assume that there are pending writes.",
            "StackTrace": [
                "2013-11-26 10:34:07,859 DEBUG nfs3.RpcProgramNfs3 (Nfs3Utils.java:writeChannel(113)) - WRITE_RPC_CALL_END______957880843",
                "2013-11-26 10:34:07,863 DEBUG nfs3.OpenFileCtx (OpenFileCtx.java:offerNextToWrite(832)) - The asyn write task has no pending writes, fileId: 30938",
                "2013-11-26 10:34:07,871 ERROR nfs3.AsyncDataService (AsyncDataService.java:run(136)) - Asyn data service got error:java.lang.IllegalStateException: The openFileCtx has false async status",
                "at com.google.common.base.Preconditions.checkState(Preconditions.java:145)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.executeWriteBack(OpenFileCtx.java:890)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService$WriteBackTask.run(AsyncDataService.java:134)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2013-11-26 10:34:07,901 DEBUG nfs3.RpcProgramNfs3 (RpcProgramNfs3.java:write(707)) - requesed offset=917504 and current filesize=917504",
                "2013-11-26 10:34:07,902 DEBUG nfs3.WriteManager (WriteManager.java:handleWrite(131)) - handleWrite fileId: 30938 offset: 917504 length:65536 stableHow:0"
            ],
            "RootCause": "The root cause of the issue is a race condition between the writeback executor thread and the new write handler thread, which leads to the async status being reset prematurely in the OpenFileCtx class.",
            "StepsToReproduce": [
                "1. Start the NFS gateway.",
                "2. Initiate multiple write operations concurrently.",
                "3. Monitor the logs for writeback state errors."
            ],
            "ExpectedBehavior": "The system should handle concurrent write operations without encountering a writeback state error, maintaining the correct async status throughout the process.",
            "ObservedBehavior": "The system throws an IllegalStateException indicating that the openFileCtx has a false async status, leading to a failure in processing write requests.",
            "Suggestions": "Implement synchronization mechanisms to ensure that the async status is not reset while there are pending write operations. Consider using locks or other concurrency control methods to manage access to the async status.",
            "problem_location": {
                "files": [
                    "OpenFileCtx.java",
                    "AsyncDataService.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx",
                    "org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService"
                ],
                "methods": [
                    "OpenFileCtx.executeWriteBack",
                    "AsyncDataService$WriteBackTask.run"
                ]
            },
            "possible_fix": "To fix the race condition, modify the executeWriteBack method in OpenFileCtx to ensure that the asyncStatus is only reset after confirming that no write operations are pending. This can be achieved by introducing a lock around the critical section where asyncStatus is checked and modified."
        },
        "possible_fix_code": {
            "OpenFileCtx.executeWriteBack": "  void executeWriteBack() {\n    // Introduce a lock to synchronize access to asyncStatus\n    synchronized (this) {\n      Preconditions.checkState(asyncStatus,\n          \"The openFileCtx has false async status\");\n      try {\n        while (activeState) {\n          WriteCtx toWrite = offerNextToWrite();\n          if (toWrite != null) {\n            // Do the write\n            doSingleWrite(toWrite);\n            updateLastAccessTime();\n          } else {\n            break;\n          }\n        }\n        \n        if (!activeState && LOG.isDebugEnabled()) {\n          LOG.debug(\"The openFileCtx is not active anymore, fileId: \"\n              + latestAttr.getFileId());\n        }\n      } finally {\n        // make sure we reset asyncStatus to false only if no writes are pending\n        if (!hasPendingWrites()) {\n          asyncStatus = false;\n        }\n      }\n    }\n  }\n\n  private boolean hasPendingWrites() {\n    // Implement logic to check if there are pending writes\n    // This is a placeholder; actual implementation may vary\n    return false; // Replace with actual check for pending writes\n  }"
        }
    },
    {
        "filename": "HDFS-11827.json",
        "creation_time": "2017-05-16T06:28:49.000+0000",
        "bug_report": {
            "Title": "NPE is thrown when log level changed in BlockPlacementPolicyDefault#chooseRandom() method",
            "Description": "A NullPointerException (NPE) occurs in the chooseRandom() method of the BlockPlacementPolicyDefault class when the log level is changed using the 'hadoop daemonlog' command. This issue arises due to a lack of null checks for certain variables that are expected to be non-null during the execution of this method. The NPE is triggered when the method attempts to access properties or methods of a null object.",
            "StackTrace": [
                "2017-05-16 13:36:51,053 ERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor thread received Runtime exception.",
                "java.lang.NullPointerException",
                "        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:666)",
                "        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:607)",
                "        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:375)",
                "        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:446)",
                "        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:446)",
                "        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:240)",
                "        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:119)",
                "        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithMultiDC.chooseTargetInternal(BlockPlacementPolicyWithMultiDC.java:263)",
                "        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithMultiDC.chooseTarget(BlockPlacementPolicyWithMultiDC.java:214)",
                "        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.chooseTargets(BlockManager.java:3836)",
                "        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.access$200(BlockManager.java:3801)",
                "        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1394)",
                "        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1300)",
                "        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3764)",
                "        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3711)",
                "        at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the NPE is the absence of null checks for the chosenNode and storage variables in the chooseRandom() method. If these variables are null, the method attempts to access their properties or methods, leading to a NullPointerException.",
            "StepsToReproduce": [
                "1. Change the log level of BlockPlacementPolicy using the 'hadoop daemonlog' command.",
                "2. Trigger the replication process that invokes the chooseRandom() method."
            ],
            "ExpectedBehavior": "The system should handle log level changes gracefully without throwing a NullPointerException, allowing the replication process to continue without interruption.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the replication process to fail and log an error message.",
            "Suggestions": "Implement null checks for the chosenNode and storage variables in the chooseRandom() method to prevent the NPE from occurring.",
            "problem_location": {
                "files": [
                    "BlockPlacementPolicyDefault.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault"
                ],
                "methods": [
                    "chooseRandom"
                ]
            },
            "possible_fix": "Add null checks in the chooseRandom() method to ensure that chosenNode and storage are not null before accessing their properties. For example:\n\nprotected DatanodeStorageInfo chooseRandom(int numOfReplicas, String scope, Set<Node> excludedNodes, long blocksize, int maxNodesPerRack, List<DatanodeStorageInfo> results, boolean avoidStaleNodes, EnumMap<StorageType, Integer> storageTypes) throws NotEnoughReplicasException {\n    // Existing code...\n    if (chosenNode == null) {\n        throw new NotEnoughReplicasException(\"No suitable data node found.\");\n    }\n    // Existing code...\n}"
        },
        "possible_fix_code": {
            "chooseRandom": "  protected DatanodeStorageInfo chooseRandom(int numOfReplicas,\n                            String scope,\n                            Set<Node> excludedNodes,\n                            long blocksize,\n                            int maxNodesPerRack,\n                            List<DatanodeStorageInfo> results,\n                            boolean avoidStaleNodes,\n                            EnumMap<StorageType, Integer> storageTypes)\n                            throws NotEnoughReplicasException {\n    StringBuilder builder = null;\n    if (LOG.isDebugEnabled()) {\n      builder = debugLoggingBuilder.get();\n      builder.setLength(0);\n      builder.append(\"[\");\n    }\n    boolean badTarget = false;\n    DatanodeStorageInfo firstChosen = null;\n    while (numOfReplicas > 0) {\n      // the storage type that current node has\n      StorageType includeType = null;\n      DatanodeDescriptor chosenNode = null;\n      if (clusterMap instanceof DFSNetworkTopology) {\n        for (StorageType type : storageTypes.keySet()) {\n          chosenNode = chooseDataNode(scope, excludedNodes, type);\n\n          if (chosenNode != null) {\n            includeType = type;\n            break;\n          }\n        }\n      } else {\n        chosenNode = chooseDataNode(scope, excludedNodes);\n      }\n\n      if (chosenNode == null) {\n        break;\n      }\n      Preconditions.checkState(excludedNodes.add(chosenNode), \"chosenNode \"\n          + chosenNode + \" is already in excludedNodes \" + excludedNodes);\n      if (LOG.isDebugEnabled()) {\n        builder.append(\"\\nNode \").append(NodeBase.getPath(chosenNode))\n            .append(\" [\");\n      }\n      DatanodeStorageInfo storage = null;\n      if (isGoodDatanode(chosenNode, maxNodesPerRack, considerLoad,\n          results, avoidStaleNodes)) {\n        for (Iterator<Map.Entry<StorageType, Integer>> iter = storageTypes\n            .entrySet().iterator(); iter.hasNext();) {\n          Map.Entry<StorageType, Integer> entry = iter.next();\n\n          // If there is one storage type the node has already contained,\n          // then no need to loop through other storage type.\n          if (includeType != null && entry.getKey() != includeType) {\n            continue;\n          }\n\n          storage = chooseStorage4Block(\n              chosenNode, blocksize, results, entry.getKey());\n          if (storage != null) {\n            numOfReplicas--;\n            if (firstChosen == null) {\n              firstChosen = storage;\n            }\n            // add node (subclasses may also add related nodes) to excludedNode\n            addToExcludedNodes(chosenNode, excludedNodes);\n            int num = entry.getValue();\n            if (num == 1) {\n              iter.remove();\n            } else {\n              entry.setValue(num - 1);\n            }\n            break;\n          }\n        }\n\n        if (LOG.isDebugEnabled()) {\n          builder.append(\"\\n]\");\n        }\n\n        // If no candidate storage was found on this DN then set badTarget.\n        badTarget = (storage == null);\n      }\n    }\n    if (numOfReplicas > 0) {\n      String detail = enableDebugLogging;\n      if (LOG.isDebugEnabled()) {\n        if (badTarget && builder != null) {\n          detail = builder.toString();\n          builder.setLength(0);\n        } else {\n          detail = \"\";\n        }\n      }\n      throw new NotEnoughReplicasException(detail);\n    }\n    \n    return firstChosen;\n  }"
        }
    },
    {
        "filename": "HDFS-6804.json",
        "creation_time": "2014-08-01T05:37:23.000+0000",
        "bug_report": {
            "Title": "Race Condition in Block Transfer Leading to Checksum Mismatch",
            "Description": "A race condition occurs during the transfer of blocks between datanodes, which can lead to an 'Unexpected checksum mismatch' exception. This happens when a block is being transferred while another operation (like appending) is simultaneously modifying the block's state. The source datanode logs indicate that the block is transmitted successfully, but the destination datanode reports a checksum mismatch, leading to the source datanode's replica being incorrectly marked as corrupt by the NameNode.",
            "StackTrace": [
                "java.io.IOException: Unexpected checksum mismatch while writing BP-2072804351-192.168.2.104-1406008383435:blk_1073741997_9248 from /192.168.2.101:39495",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:536)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:703)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:575)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:115)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:68)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)"
            ],
            "RootCause": "The root cause of the issue is a race condition between the block transfer and the block append operations. When a block is being transferred, if another operation modifies the block's state, it can lead to a checksum mismatch during the verification process.",
            "StepsToReproduce": [
                "1. Initiate a block transfer from one datanode to another.",
                "2. Simultaneously, start an append operation on the same block.",
                "3. Monitor the logs for checksum mismatch errors."
            ],
            "ExpectedBehavior": "The block transfer should complete successfully without any checksum mismatches, and the block should remain valid on both the source and destination datanodes.",
            "ObservedBehavior": "The destination datanode reports a checksum mismatch, causing the NameNode to mark the source datanode's replica as corrupt, even though the source replica is valid.",
            "Suggestions": "Implement synchronization mechanisms to prevent concurrent modifications of blocks during transfer operations. Consider using locks or other concurrency control methods to ensure that a block is not modified while it is being transferred.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.DataXceiver",
                    "org.apache.hadoop.hdfs.server.datanode.BlockReceiver"
                ],
                "methods": [
                    "DataXceiver.run",
                    "BlockReceiver.receivePacket",
                    "BlockReceiver.receiveBlock",
                    "Receiver.opWriteBlock"
                ]
            },
            "possible_fix": "To fix the race condition, implement a locking mechanism in the BlockReceiver class to ensure that a block cannot be modified while it is being transferred. For example, you could add a synchronized block around the critical sections of the receiveBlock and receivePacket methods to prevent concurrent access."
        },
        "possible_fix_code": {
            "BlockReceiver.receiveBlock": "  void receiveBlock(\n      DataOutputStream mirrOut, // output to next datanode\n      DataInputStream mirrIn,   // input from next datanode\n      DataOutputStream replyOut,  // output to previous datanode\n      String mirrAddr, DataTransferThrottler throttlerArg,\n      DatanodeInfo[] downstreams) throws IOException {\n\n      syncOnClose = datanode.getDnConf().syncOnClose;\n      boolean responderClosed = false;\n      mirrorOut = mirrOut;\n      mirrorAddr = mirrAddr;\n      throttler = throttlerArg;\n\n      // Synchronization to prevent race condition during block transfer\n      synchronized (this) {\n        try {\n          if (isClient && !isTransfer) {\n            responder = new Daemon(datanode.threadGroup, \n                new PacketResponder(replyOut, mirrIn, downstreams));\n            responder.start(); // start thread to processes responses\n          }\n\n          while (receivePacket() >= 0) { /* Receive until the last packet */ }\n\n          // wait for all outstanding packet responses. And then\n          // indicate responder to gracefully shutdown.\n          // Mark that responder has been closed for future processing\n          if (responder != null) {\n            ((PacketResponder)responder.getRunnable()).close();\n            responderClosed = true;\n          }\n\n          // If this write is for a replication or transfer-RBW/Finalized,\n          // then finalize block or convert temporary to RBW.\n          // For client-writes, the block is finalized in the PacketResponder.\n          if (isDatanode || isTransfer) {\n            // close the block/crc files\n            close();\n            block.setNumBytes(replicaInfo.getNumBytes());\n\n            if (stage == BlockConstructionStage.TRANSFER_RBW) {\n              // for TRANSFER_RBW, convert temporary to RBW\n              datanode.data.convertTemporaryToRbw(block);\n            } else {\n              // for isDatnode or TRANSFER_FINALIZED\n              // Finalize the block.\n              datanode.data.finalizeBlock(block);\n            }\n            datanode.metrics.incrBlocksWritten();\n          }\n        } catch (IOException ioe) {\n          if (datanode.isRestarting()) {\n            // Do not throw if shutting down for restart. Otherwise, it will cause\n            // premature termination of responder.\n            LOG.info(\"Shutting down for restart (\" + block + \").\");\n          } else {\n            LOG.info(\"Exception for \" + block, ioe);\n            throw ioe;\n          }\n        } finally {\n          // Clear the previous interrupt state of this thread.\n          Thread.interrupted();\n\n          // If a shutdown for restart was initiated, upstream needs to be notified.\n          // There is no need to do anything special if the responder was closed\n          // normally.\n          if (!responderClosed) { // Data transfer was not complete.\n            if (responder != null) {\n              // In case this datanode is shutting down for quick restart,\n              // send a special ack upstream.\n              if (datanode.isRestarting() && isClient && !isTransfer) {\n                File blockFile = ((ReplicaInPipeline)replicaInfo).getBlockFile();\n                File restartMeta = new File(blockFile.getParent()  + \n                    File.pathSeparator + \".\" + blockFile.getName() + \".restart\");\n                if (restartMeta.exists() && !restartMeta.delete()) {\n                  LOG.warn(\"Failed to delete restart meta file: \" +\n                      restartMeta.getPath());\n                }\n                try {\n                  FileWriter out = new FileWriter(restartMeta);\n                  // write out the current time.\n                  out.write(Long.toString(Time.now() + restartBudget));\n                  out.flush();\n                  out.close();\n                } catch (IOException ioe) {\n                  // The worst case is not recovering this RBW replica. \n                  // Client will fall back to regular pipeline recovery.\n                }\n                try {\n                  ((PacketResponder) responder.getRunnable()).\n                      sendOOBResponse(PipelineAck.getRestartOOBStatus());\n                  // Even if the connection is closed after the ack packet is\n                  // flushed, the client can react to the connection closure \n                  // first. Insert a delay to lower the chance of client \n                  // missing the OOB ack.\n                  Thread.sleep(1000);\n                } catch (InterruptedException ie) {\n                  // It is already going down. Ignore this.\n                } catch (IOException ioe) {\n                  LOG.info(\"Error sending OOB Ack.\", ioe);\n                }\n              }\n              responder.interrupt();\n            }\n            IOUtils.closeStream(this);\n            cleanupBlock();\n          }\n          if (responder != null) {\n            try {\n              responder.interrupt();\n              // join() on the responder should timeout a bit earlier than the\n              // configured deadline. Otherwise, the join() on this thread will\n              // likely timeout as well.\n              long joinTimeout = datanode.getDnConf().getXceiverStopTimeout();\n              joinTimeout = joinTimeout > 1  ? joinTimeout*8/10 : joinTimeout;\n              responder.join(joinTimeout);\n              if (responder.isAlive()) {\n                String msg = \"Join on responder thread \" + responder\n                    + \" timed out\";\n                LOG.warn(msg + \"\\n\" + StringUtils.getStackTrace(responder));\n                throw new IOException(msg);\n              }\n            } catch (InterruptedException e) {\n              responder.interrupt();\n              // do not throw if shutting down for restart.\n              if (!datanode.isRestarting()) {\n                throw new IOException(\"Interrupted receiveBlock\");\n              }\n            }\n            responder = null;\n          }\n        }\n      }\n  }"
        }
    },
    {
        "filename": "HDFS-5843.json",
        "creation_time": "2014-01-28T06:05:16.000+0000",
        "bug_report": {
            "Title": "DFSClient.getFileChecksum() throws IOException if checksum is disabled",
            "Description": "When a file is created with checksum disabled using ChecksumOpt.disabled(), invoking FileSystem.getFileChecksum() results in an IOException. The underlying issue arises from the DataNode attempting to perform arithmetic operations on a zero value for crcPerBlock, leading to an ArithmeticException. This occurs during the block checksum retrieval process, where the DataNode fails to compute the checksum due to the absence of valid checksum data.",
            "StackTrace": [
                "java.io.IOException: Fail to get block MD5 for BP-341493254-192.168.1.10-1390888724459:blk_1073741825_1001",
                "at org.apache.hadoop.hdfs.DFSClient.getFileChecksum(DFSClient.java:1965)",
                "at org.apache.hadoop.hdfs.DFSClient.getFileChecksum(DFSClient.java:1771)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1186)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1194)",
                "java.lang.ArithmeticException: / by zero",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockChecksum(DataXceiver.java:658)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opBlockChecksum(Receiver.java:169)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:77)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)",
                "at java.lang.Thread.run(Thread.java:695)"
            ],
            "RootCause": "The root cause of the issue is that the crcPerBlock value is zero when checksum is disabled, leading to an ArithmeticException during the block checksum calculation in the DataNode.",
            "StepsToReproduce": [
                "1. Create a file with checksum disabled using ChecksumOpt.disabled().",
                "2. Call FileSystem.getFileChecksum() on the created file.",
                "3. Observe the IOException thrown."
            ],
            "ExpectedBehavior": "The system should handle the case where checksums are disabled gracefully, without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown when attempting to retrieve the file checksum for a file created with checksum disabled.",
            "Suggestions": "Implement a check in the blockChecksum method to handle cases where crcPerBlock is zero, preventing division by zero errors.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.DFSClient",
                    "org.apache.hadoop.hdfs.server.datanode.DataXceiver"
                ],
                "methods": [
                    "DFSClient.getFileChecksum",
                    "DataXceiver.blockChecksum"
                ]
            },
            "possible_fix": "In the blockChecksum method, add a check for crcPerBlock before performing any arithmetic operations. For example:\n\nif (crcPerBlock <= 0) {\n    throw new IOException(\"Invalid crcPerBlock value: \" + crcPerBlock);\n}"
        },
        "possible_fix_code": {
            "DataXceiver.blockChecksum": "  public void blockChecksum(final ExtendedBlock block,\n      final Token<BlockTokenIdentifier> blockToken) throws IOException {\n    final DataOutputStream out = new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final LengthInputStream metadataIn = \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn = new DataInputStream(new BufferedInputStream(\n        metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header = BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum = header.getChecksum(); \n      final int bytesPerCRC = checksum.getBytesPerChecksum();\n      final long crcPerBlock = (metadataIn.getLength()\n          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n\n      // Check for zero crcPerBlock to avoid ArithmeticException\n      if (crcPerBlock <= 0) {\n          throw new IOException(\"Invalid crcPerBlock value: \" + crcPerBlock);\n      }\n\n      //compute block checksum\n      final MD5Hash md5 = MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block=\" + block + \", bytesPerCRC=\" + bytesPerCRC\n            + \", crcPerBlock=\" + crcPerBlock + \", md5=\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          .setCrcType(PBHelper.convert(checksum.getChecksumType()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }"
        }
    },
    {
        "filename": "HDFS-8070.json",
        "creation_time": "2015-04-06T19:57:58.000+0000",
        "bug_report": {
            "Title": "Pre-HDFS-7915 DFSClient cannot use short circuit on post-HDFS-7915 DataNode",
            "Description": "The issue arises when a DataNode running version 2.8.0 attempts to communicate with a Client running version 2.7.0 using the ShortCircuitShm layer. The ShortCircuitCache fails to release shared memory slots due to an invalid shared memory segment ID, leading to a deadlock situation during multi-threaded split-generation. This is likely caused by incompatibilities in the ShortCircuitShim wire protocol between the two versions.",
            "StackTrace": [
                "java.io.IOException: ERROR_INVALID: there is no shared memory segment registered with shmId a86ee34576d93c4964005d90b0d97c38",
                "at org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser.run(ShortCircuitCache.java:208)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is the failure of the ShortCircuitCache to release shared memory slots due to an invalid shared memory segment ID, which is likely a result of version incompatibility between the DataNode and Client.",
            "StepsToReproduce": [
                "Upgrade the DataNode to version 2.8.0.",
                "Use a Client running version 2.7.0 to connect to the upgraded DataNode.",
                "Attempt to perform operations that utilize the ShortCircuitShm layer."
            ],
            "ExpectedBehavior": "The Client should successfully communicate with the DataNode using the ShortCircuitShm layer without encountering errors related to shared memory segments.",
            "ObservedBehavior": "The Client encounters an IOException indicating that there is no shared memory segment registered, leading to a failure in releasing short-circuit shared memory slots.",
            "Suggestions": "Consider ensuring that both the DataNode and Client are running compatible versions of Hadoop. If version mismatches are unavoidable, implement a fallback mechanism to handle incompatible ShortCircuitShm communications gracefully.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.DataXceiver",
                    "org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache"
                ],
                "methods": [
                    "DataXceiver.run",
                    "DataXceiver.requestShortCircuitFds",
                    "ShortCircuitCache$SlotReleaser.run"
                ]
            },
            "possible_fix": "To resolve the issue, ensure that both the DataNode and Client are upgraded to the same version of Hadoop. If this is not feasible, modify the ShortCircuitCache to handle cases where the shared memory segment ID is invalid, possibly by implementing a retry mechanism or logging detailed error information for further analysis."
        },
        "possible_fix_code": {
            "ShortCircuitCache$SlotReleaser.run": "  public void run() {\n    try {\n      // Attempt to release the shared memory slot\n      releaseSlot();\n    } catch (IOException e) {\n      // Log the error and handle the invalid shared memory segment ID gracefully\n      LOG.error(\"Failed to release shared memory slot: \" + e.getMessage(), e);\n      // Implement a retry mechanism or fallback logic here if necessary\n      handleInvalidSlotId();\n    }\n  }\n\n  private void handleInvalidSlotId() {\n    // Logic to handle the case when the shared memory segment ID is invalid\n    // This could include retrying the operation or logging detailed information\n    LOG.warn(\"Handling invalid shared memory segment ID. Retrying...\");\n    // Retry logic or alternative handling can be implemented here\n  }\n\n  private void releaseSlot() throws IOException {\n    // Original logic to release the shared memory slot\n    // This is a placeholder for the actual implementation\n  }"
        }
    },
    {
        "filename": "HDFS-1085.json",
        "creation_time": "2010-04-06T22:49:40.000+0000",
        "bug_report": {
            "Title": "hftp read failing silently",
            "Description": "During a large distributed copy operation (distcp) using hftp, several tasks encountered failures due to a mismatch in the expected file size. The log indicates that while attempting to copy a file, the actual size of the copied file was significantly smaller than the expected size. This suggests that the read operation from the source hftp server did not fail outright, but rather resulted in an incomplete or corrupted file being written to the destination.",
            "StackTrace": [
                "2010-04-06 17:56:43,005 INFO org.apache.hadoop.tools.DistCp: FAIL 2010/0/part-00032 : java.io.IOException: File size not matched: copied 193855488 bytes (184.9m) to tmpfile (=hdfs://omehost.com:8020/somepath/part-00032) but expected 1710327403 bytes (1.6g) from hftp://someotherhost/somepath/part-00032",
                "at org.apache.hadoop.tools.DistCp$CopyFilesMapper.copy(DistCp.java:435)",
                "at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:543)",
                "at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:310)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)",
                "at org.apache.hadoop.mapred.Child.main(Child.main:159)"
            ],
            "RootCause": "The root cause appears to be an issue with the hftp read operation that results in an incomplete file being copied. This could be due to network issues, server-side problems, or incorrect handling of the file stream.",
            "StepsToReproduce": [
                "Initiate a large distcp operation using hftp to copy files from one HDFS location to another.",
                "Monitor the logs for any file size mismatch errors during the copy process."
            ],
            "ExpectedBehavior": "The distcp operation should successfully copy files from the source to the destination, with the copied file sizes matching the expected sizes.",
            "ObservedBehavior": "The distcp operation fails with an IOException indicating that the copied file size does not match the expected size, suggesting that the read operation did not complete successfully.",
            "Suggestions": "Investigate the network stability between the source and destination, check the hftp server for any issues, and ensure that the file being copied is not being modified during the copy process.",
            "problem_location": {
                "files": [
                    "DistCp.java"
                ],
                "classes": [
                    "org.apache.hadoop.tools.DistCp"
                ],
                "methods": [
                    "DistCp$CopyFilesMapper.copy",
                    "DistCp$CopyFilesMapper.map"
                ]
            },
            "possible_fix": "Consider adding error handling to verify the integrity of the file after the read operation. Implement checks to ensure that the file size matches the expected size before finalizing the copy operation. Additionally, logging more detailed information about the read process could help diagnose issues."
        },
        "possible_fix_code": {
            "DistCp$CopyFilesMapper.copy": "public void copy(Path src, Path dst) throws IOException {\n    // Create a stream to read from the source\n    InputStream in = null;\n    OutputStream out = null;\n    try {\n        in = hftpFileSystem.open(src);\n        out = hdfsFileSystem.create(dst);\n\n        // Buffer for data transfer\n        byte[] buffer = new byte[4096];\n        int bytesRead;\n        long totalBytesRead = 0;\n\n        // Read from the source and write to the destination\n        while ((bytesRead = in.read(buffer)) > 0) {\n            out.write(buffer, 0, bytesRead);\n            totalBytesRead += bytesRead;\n        }\n\n        // Verify the file size after the copy operation\n        long expectedSize = hftpFileSystem.getFileStatus(src).getLen();\n        if (totalBytesRead != expectedSize) {\n            throw new IOException(\"File size not matched: copied \" + totalBytesRead + \" bytes to \" + dst + \" but expected \" + expectedSize + \" bytes\");\n        }\n    } finally {\n        // Close streams\n        if (in != null) {\n            in.close();\n        }\n        if (out != null) {\n            out.close();\n        }\n    }\n}"
        }
    },
    {
        "filename": "HDFS-12339.json",
        "creation_time": "2017-08-23T03:16:49.000+0000",
        "bug_report": {
            "Title": "NFS Gateway on Shutdown Gives Unregistration Failure. Does Not Unregister with rpcbind Portmapper",
            "Description": "When the NFS Gateway is stopped, an error occurs indicating a failure to unregister with the rpcbind portmapper. The logs show a SocketException due to the socket being closed, which prevents the NFS Gateway from successfully sending the unregistration request. This issue arises during the execution of the SimpleUdpClient's run method, where the socket is expected to be open for communication.",
            "StackTrace": [
                "2017-08-17 18:09:16,529 ERROR org.apache.hadoop.oncrpc.RpcProgram: Unregistration failure with localhost:2049, portmap entry: (PortmapMapping-100003:3:6:2049)",
                "2017-08-17 18:09:16,531 WARN org.apache.hadoop.util.ShutdownHookManager: ShutdownHook 'NfsShutdownHook' failed, java.lang.RuntimeException: Unregistration failure",
                "java.lang.RuntimeException: Unregistration failure",
                "Caused by: java.net.SocketException: Socket is closed",
                "at java.net.DatagramSocket.send(DatagramSocket.java:641)",
                "at org.apache.hadoop.oncrpc.SimpleUdpClient.run(SimpleUdpClient.java:62)"
            ],
            "RootCause": "The root cause of the issue is that the DatagramSocket used for sending the unregistration request is closed when the NFS Gateway attempts to unregister. This results in a SocketException being thrown.",
            "StepsToReproduce": [
                "Start the NFS Gateway.",
                "Attempt to stop the NFS Gateway.",
                "Observe the logs for any errors related to unregistration."
            ],
            "ExpectedBehavior": "The NFS Gateway should successfully unregister with the rpcbind portmapper without any errors.",
            "ObservedBehavior": "The NFS Gateway fails to unregister, resulting in a SocketException and an error message in the logs.",
            "Suggestions": "Ensure that the DatagramSocket is open and not closed before attempting to send the unregistration request. This may involve checking the state of the socket or modifying the shutdown sequence to ensure proper cleanup.",
            "problem_location": {
                "files": [
                    "SimpleUdpClient.java"
                ],
                "classes": [
                    "org.apache.hadoop.oncrpc.SimpleUdpClient"
                ],
                "methods": [
                    "SimpleUdpClient.run"
                ]
            },
            "possible_fix": "Modify the run method in SimpleUdpClient to check if the socket is closed before sending the unregistration request. If the socket is closed, either reopen it or handle the unregistration differently to avoid the SocketException."
        },
        "possible_fix_code": {
            "SimpleUdpClient.run": "  public void run() throws IOException {\n    InetAddress IPAddress = InetAddress.getByName(host);\n    byte[] sendData = request.getBytes();\n    byte[] receiveData = new byte[65535];\n    // Use the provided socket if there is one, else just make a new one.\n    DatagramSocket socket = this.clientSocket == null ?\n        new DatagramSocket() : this.clientSocket;\n\n    try {\n      // Check if the socket is closed before sending the unregistration request\n      if (socket.isClosed()) {\n        throw new IOException(\"Socket is closed, cannot send request.\");\n      }\n\n      DatagramPacket sendPacket = new DatagramPacket(sendData, sendData.length,\n          IPAddress, port);\n      socket.send(sendPacket);\n      socket.setSoTimeout(udpTimeoutMillis);\n      DatagramPacket receivePacket = new DatagramPacket(receiveData,\n          receiveData.length);\n      socket.receive(receivePacket);\n  \n      // Check reply status\n      XDR xdr = new XDR(Arrays.copyOfRange(receiveData, 0,\n          receivePacket.getLength()));\n      RpcReply reply = RpcReply.read(xdr);\n      if (reply.getState() != RpcReply.ReplyState.MSG_ACCEPTED) {\n        throw new IOException(\"Request failed: \" + reply.getState());\n      }\n    } finally {\n      // If the client socket was passed in to this UDP client, it\\'s on the\n      // caller of this UDP client to close that socket.\n      if (this.clientSocket == null) {\n        socket.close();\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-6520.json",
        "creation_time": "2014-06-12T02:32:29.000+0000",
        "bug_report": {
            "Title": "hdfs fsck -move passes invalid length value when creating BlockReader",
            "Description": "When executing the 'fsck -move' command on a corrupted file in HDFS, an IOException is thrown indicating an expected empty end-of-read packet. This occurs because the BlockReader is attempting to read a block that has been corrupted, leading to an invalid packet length being processed. The issue arises during the copying of corrupted blocks to the '/lost+found' directory, where the system fails to handle the corrupted block properly.",
            "StackTrace": [
                "java.io.IOException: Expected empty end-of-read packet! Header: PacketHeader with packetLen=66048 header data: offsetInBlock: 65536 seqno: 1 lastPacketInBlock: false dataLen: 65536",
                "at org.apache.hadoop.hdfs.RemoteBlockReader2.readTrailingEmptyPacket(RemoteBlockReader2.java:259)",
                "at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:220)",
                "at org.apache.hadoop.hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:138)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlock(NamenodeFsck.java:649)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlocksToLostFound(NamenodeFsck.java:543)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:460)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:324)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.fsck(NamenodeFsck.java:233)"
            ],
            "RootCause": "The root cause of the issue is that the BlockReader is trying to read a block that has been corrupted, resulting in an invalid packet length being processed. Specifically, the expected empty end-of-read packet is not received, leading to an IOException.",
            "StepsToReproduce": [
                "1. Set up a pseudo cluster.",
                "2. Copy a file to HDFS.",
                "3. Corrupt a block of the file.",
                "4. Run 'fsck' to check the file system.",
                "5. Run 'fsck -move' to move the corrupted file to '/lost+found'."
            ],
            "ExpectedBehavior": "The corrupted file should be successfully moved to the '/lost+found' directory without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown indicating an expected empty end-of-read packet, preventing the corrupted file from being moved.",
            "Suggestions": "Implement additional checks in the BlockReader to handle corrupted blocks more gracefully. Ensure that the system can recognize and skip over corrupted packets without throwing an exception.",
            "problem_location": {
                "files": [
                    "RemoteBlockReader2.java",
                    "NamenodeFsck.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.RemoteBlockReader2",
                    "org.apache.hadoop.hdfs.server.namenode.NamenodeFsck"
                ],
                "methods": [
                    "RemoteBlockReader2.readTrailingEmptyPacket",
                    "RemoteBlockReader2.readNextPacket",
                    "NamenodeFsck.copyBlock",
                    "NamenodeFsck.copyBlocksToLostFound"
                ]
            },
            "possible_fix": "In the 'readTrailingEmptyPacket' method of 'RemoteBlockReader2', add a condition to handle cases where the packet is not empty or the data length is not zero, allowing the system to skip over corrupted packets instead of throwing an IOException."
        },
        "possible_fix_code": {
            "RemoteBlockReader2.readTrailingEmptyPacket": "  private void readTrailingEmptyPacket() throws IOException {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Reading empty packet at end of read\");\n    }\n    \n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader trailer = packetReceiver.getHeader();\n    // Check if the packet is not the last packet in block or if the data length is not zero\n    if (!trailer.isLastPacketInBlock() || trailer.getDataLen() != 0) {\n      // Log a warning instead of throwing an exception\n      LOG.warn(\"Expected empty end-of-read packet! Header: \" + trailer);\n      return; // Skip this packet and allow processing to continue\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-10715.json",
        "creation_time": "2016-08-02T08:10:23.000+0000",
        "bug_report": {
            "Title": "NPE when applying AvailableSpaceBlockPlacementPolicy",
            "Description": "The issue arises from the `chooseDataNode` method in the `AvailableSpaceBlockPlacementPolicy` class, where it attempts to compare two `DatanodeDescriptor` objects. If either of the nodes returned by `clusterMap.chooseRandom` is null, a NullPointerException (NPE) occurs when the method tries to call `equals` on a null reference. This can happen if there are insufficient available nodes to choose from, leading to a null return value.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.compareDataNode(AvailableSpaceBlockPlacementPolicy.java:95)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.chooseDataNode(AvailableSpaceBlockPlacementPolicy.java:80)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:691)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:665)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseLocalRack(BlockPlacementPolicyDefault.java:572)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:457)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:367)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:242)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:114)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:130)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1606)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3315)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:679)",
                "at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:214)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:489)"
            ],
            "RootCause": "The root cause of the NPE is that the `chooseDataNode` method does not handle the case where `clusterMap.chooseRandom` returns null, leading to a null reference being passed to the `compareDataNode` method.",
            "StepsToReproduce": [
                "Set up an HDFS cluster with limited available datanodes.",
                "Attempt to add a block using the `AvailableSpaceBlockPlacementPolicy`.",
                "Observe the namenode logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The system should handle cases where no suitable datanodes are available without throwing a NullPointerException, possibly by returning an error or retrying the operation.",
            "ObservedBehavior": "The system throws a NullPointerException when attempting to compare two datanodes, causing the block addition process to fail.",
            "Suggestions": "Implement a null check in the `chooseDataNode` method to handle cases where `clusterMap.chooseRandom` returns null. If both nodes are null, the method should return null or handle the situation gracefully.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy",
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault"
                ],
                "methods": [
                    "AvailableSpaceBlockPlacementPolicy.chooseDataNode",
                    "AvailableSpaceBlockPlacementPolicy.compareDataNode"
                ]
            },
            "possible_fix": "In the `chooseDataNode` method, add a null check for the `DatanodeDescriptor` objects returned by `clusterMap.chooseRandom`. If either is null, handle the situation appropriately, such as returning null or throwing a custom exception. Example code change:\n\n```java\nprotected DatanodeDescriptor chooseDataNode(final String scope, final Collection<Node> excludedNode) {\n    DatanodeDescriptor a = (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNode);\n    DatanodeDescriptor b = (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNode);\n    if (a == null && b == null) {\n        // Handle case where no datanodes are available\n        return null; // or throw an exception\n    }\n    int ret = compareDataNode(a, b);\n    if (ret == 0) {\n        return a;\n    } else if (ret < 0) {\n        return (RAND.nextInt(100) < balancedPreference) ? a : b;\n    } else {\n        return (RAND.nextInt(100) < balancedPreference) ? b : a;\n    }\n}\n```"
        },
        "possible_fix_code": {
            "AvailableSpaceBlockPlacementPolicy.chooseDataNode": "  protected DatanodeDescriptor chooseDataNode(final String scope,\n      final Collection<Node> excludedNode) {\n    DatanodeDescriptor a =\n        (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNode);\n    DatanodeDescriptor b =\n        (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNode);\n    if (a == null && b == null) {\n        // Handle case where no datanodes are available\n        return null; // or throw an exception\n    }\n    int ret = compareDataNode(a, b);\n    if (ret == 0) {\n      return a;\n    } else if (ret < 0) {\n      return (RAND.nextInt(100) < balancedPreference) ? a : b;\n    } else {\n      return (RAND.nextInt(100) < balancedPreference) ? b : a;\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-3332.json",
        "creation_time": "2012-04-27T04:42:38.000+0000",
        "bug_report": {
            "Title": "NullPointerException in DN when DirectoryScanner is trying to report bad blocks",
            "Description": "A NullPointerException occurs in the DataNode when the DirectoryScanner attempts to report bad blocks. This issue arises during the reconciliation process where the DirectoryScanner checks for inconsistencies in block metadata and attempts to report any corrupt blocks found. The stack trace indicates that the exception is thrown when trying to initialize a DatanodeID object, which suggests that the bpRegistration field may be null at the time of the reportBadBlocks call.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.protocol.DatanodeID.<init>(DatanodeID.java:66)",
                "at org.apache.hadoop.hdfs.protocol.DatanodeInfo.<init>(DatanodeInfo.java:87)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reportBadBlocks(BPServiceActor.java:238)",
                "at org.apache.hadoop.hdfs.server.datanode.BPOfferService.reportBadBlocks(BPOfferService.java:187)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:559)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.checkAndUpdate(FsDatasetImpl.java:1377)",
                "at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.reconcile(DirectoryScanner.java:318)",
                "at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.run(DirectoryScanner.java:284)"
            ],
            "RootCause": "The root cause of the NullPointerException is that the bpRegistration field in the BPServiceActor is null when attempting to create a new DatanodeInfo object. This likely occurs because the block being reported is associated with a block pool that has not been properly initialized or registered.",
            "StepsToReproduce": [
                "Start a DataNode with HA configuration.",
                "Corrupt a block in the DataNode's storage.",
                "Trigger the DirectoryScanner to run, which will attempt to report the bad block."
            ],
            "ExpectedBehavior": "The DataNode should successfully report bad blocks to the NameNode without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when the DirectoryScanner attempts to report bad blocks, causing the reporting process to fail.",
            "Suggestions": "Ensure that the bpRegistration field is properly initialized before it is used in the reportBadBlocks method. This may involve checking the block pool registration process and ensuring that all necessary fields are set correctly.",
            "problem_location": {
                "files": [
                    "BPServiceActor.java",
                    "DirectoryScanner.java",
                    "DataNode.java",
                    "FsDatasetImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.BPServiceActor",
                    "org.apache.hadoop.hdfs.server.datanode.DirectoryScanner",
                    "org.apache.hadoop.hdfs.server.datanode.DataNode",
                    "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl"
                ],
                "methods": [
                    "BPServiceActor.reportBadBlocks",
                    "DirectoryScanner.reconcile",
                    "DataNode.reportBadBlocks",
                    "FsDatasetImpl.checkAndUpdate"
                ]
            },
            "possible_fix": "In the BPServiceActor class, ensure that the bpRegistration field is initialized properly during the block pool registration process. Additionally, add null checks before creating DatanodeInfo objects to prevent NullPointerExceptions."
        },
        "possible_fix_code": {
            "BPServiceActor.reportBadBlocks": "  void reportBadBlocks(ExtendedBlock block) {\n    if (bpRegistration == null) {\n      LOG.warn(\"bpRegistration is null, cannot report bad blocks.\");\n      return;\n    }\n    DatanodeInfo[] dnArr = { new DatanodeInfo(bpRegistration) };\n    LocatedBlock[] blocks = { new LocatedBlock(block, dnArr) }; \n    \n    try {\n      bpNamenode.reportBadBlocks(blocks);  \n    } catch (IOException e){\n      /* One common reason is that NameNode could be in safe mode.\n       * Should we keep on retrying in that case?\n       */\n      LOG.warn(\"Failed to report bad block \" + block + \" to namenode : \"\n          + \" Exception\", e);\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-6130.json",
        "creation_time": "2014-03-20T07:15:47.000+0000",
        "bug_report": {
            "Title": "NPE when upgrading namenode from fsimages older than -32",
            "Description": "When attempting to upgrade an old Hadoop cluster (version 0.20.2-cdh3u1) to a trunk instance with High Availability (HA) enabled, a NullPointerException (NPE) occurs during the execution of the command 'hdfs namenode -initializeSharedEdits'. The issue arises specifically when the namenode tries to check for reserved names in the filesystem image, leading to a failure in the initialization process.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.isReservedName(FSDirectory.java:2984)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.addToParent(FSImageFormatPBINode.java:205)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeDirectorySection(FSImageFormatPBINode.java:162)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.loadInternal(FSImageFormatProtobuf.java:243)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:168)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.load(FSImageFormat.java:120)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:895)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:881)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:704)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:271)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:894)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:653)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits(NameNode.java:912)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1276)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1360)"
            ],
            "RootCause": "The NullPointerException occurs in the method FSDirectory.isReservedName, which is called during the loading of the filesystem image. This suggests that a required object or value is not properly initialized or is missing, likely due to the incompatibility of the old fsimage format with the current namenode expectations.",
            "StepsToReproduce": [
                "1. Set up an old Hadoop cluster (version 0.20.2-cdh3u1).",
                "2. Configure High Availability (HA) settings.",
                "3. Run the command 'hdfs namenode -initializeSharedEdits'."
            ],
            "ExpectedBehavior": "The namenode should successfully initialize shared edits without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the namenode initialization to fail.",
            "Suggestions": "Investigate the initialization process of the namenode and ensure that all required configurations and states are properly set before invoking methods that check for reserved names. Consider adding null checks or default values where necessary.",
            "problem_location": {
                "files": [
                    "FSDirectory.java",
                    "FSImageFormatPBINode.java",
                    "FSImage.java",
                    "FSNamesystem.java",
                    "NameNode.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSDirectory",
                    "org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode",
                    "org.apache.hadoop.hdfs.server.namenode.FSImage",
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem",
                    "org.apache.hadoop.hdfs.server.namenode.NameNode"
                ],
                "methods": [
                    "FSDirectory.isReservedName",
                    "FSImageFormatPBINode.Loader.addToParent",
                    "FSImage.loadFSImage",
                    "FSNamesystem.loadFromDisk",
                    "NameNode.initializeSharedEdits"
                ]
            },
            "possible_fix": "In the FSDirectory class, ensure that the method isReservedName checks for null or invalid inputs before proceeding with its logic. Additionally, review the loading process in FSImage and FSNamesystem to ensure that all necessary components are initialized correctly before they are accessed."
        },
        "possible_fix_code": {
            "FSDirectory.isReservedName": "  public static boolean isReservedName(String src) {\n    if (src == null) {\n      return false; // or throw an IllegalArgumentException if preferred\n    }\n    return src.startsWith(DOT_RESERVED_PATH_PREFIX);\n  }"
        }
    },
    {
        "filename": "HDFS-2827.json",
        "creation_time": "2012-01-23T09:34:51.000+0000",
        "bug_report": {
            "Title": "Cannot save namespace after renaming a directory above a file with an open lease",
            "Description": "The issue arises when a directory is renamed while a file within it is still open and under lease. Specifically, after creating a directory and a file, renaming the parent directory leads to a failure in checkpointing due to the system being unable to find the file in the new namespace structure. The error indicates that the file path is not recognized in the updated namespace, which suggests a problem with how the lease manager tracks files during directory renames.",
            "StackTrace": [
                "2012-01-23 15:03:14,204 ERROR namenode.FSImage (FSImage.java:run(795)) - Unable to save image for E:\\HDFS-1623\\hadoop-hdfs-project\\hadoop-hdfs\\build\\test\\data\\dfs\\name3",
                "java.io.IOException: saveLeases found path /test1/est/abc.txt but no matching entry in namespace.[/test1/est/abc.txt]",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveFilesUnderConstruction(FSNamesystem.java:4336)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver.save(FSImageFormat.java:588)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImage(FSImage.java:761)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.run(FSImage.java:789)",
                "at java.lang.Thread.run(Unknown Source)"
            ],
            "RootCause": "The root cause of the issue is that the lease manager does not properly update the namespace when a directory containing an open lease is renamed. The file's path is not found in the new namespace, leading to an IOException during the checkpointing process.",
            "StepsToReproduce": [
                "1. Create a new directory using fs.mkdirs(new Path('/test1')).",
                "2. Create a new file in an existing directory using FSDataOutputStream create = fs.create(new Path('/test/abc.txt')).",
                "3. Do not close the file output stream.",
                "4. Rename the existing directory using fs.rename(new Path('/test/'), new Path('/test1/')).",
                "5. Wait for the checkpoint to complete."
            ],
            "ExpectedBehavior": "The system should successfully save the namespace and complete the checkpoint without errors, recognizing the file in the new directory structure.",
            "ObservedBehavior": "The system throws an IOException indicating that it cannot find the file in the new namespace after the directory rename.",
            "Suggestions": "Review the lease management logic in the FSNamesystem class to ensure that it correctly updates the paths of files under lease when their parent directories are renamed. Consider implementing a mechanism to track and update leases during such operations.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem",
                    "org.apache.hadoop.hdfs.server.namenode.FSImage"
                ],
                "methods": [
                    "FSNamesystem.saveFilesUnderConstruction",
                    "FSImage.saveFSImage"
                ]
            },
            "possible_fix": "In the saveFilesUnderConstruction method, ensure that the lease manager updates the paths of files under construction when a directory is renamed. This may involve modifying the logic to check for renamed paths and updating the internal state of the lease manager accordingly."
        },
        "possible_fix_code": {
            "FSNamesystem.saveFilesUnderConstruction": "  void saveFilesUnderConstruction(DataOutputStream out) throws IOException {\n    // This is run by an inferior thread of saveNamespace, which holds a read\n    // lock on our behalf. If we took the read lock here, we could block\n    // for fairness if a writer is waiting on the lock.\n    synchronized (leaseManager) {\n      out.writeInt(leaseManager.countPath()); // write the size\n\n      for (Lease lease : leaseManager.getSortedLeases()) {\n        for(String path : lease.getPaths()) {\n          // verify that path exists in namespace\n          INode node;\n          try {\n            node = dir.getFileINode(path);\n          } catch (UnresolvedLinkException e) {\n            throw new AssertionError(\"Lease files should reside on this FS\");\n          }\n          if (node == null) {\n            throw new IOException(\"saveLeases found path \" + path +\n                                  \" but no matching entry in namespace.\");\n          }\n          if (!node.isUnderConstruction()) {\n            throw new IOException(\"saveLeases found path \" + path +\n                                  \" but is not under construction.\");\n          }\n          INodeFileUnderConstruction cons = (INodeFileUnderConstruction) node;\n          FSImageSerialization.writeINodeUnderConstruction(out, cons, path);\n        }\n      }\n      // Update lease paths for renamed directories\n      leaseManager.updateLeasePaths();\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-11056.json",
        "creation_time": "2016-10-25T23:00:33.000+0000",
        "bug_report": {
            "Title": "Concurrent append and read operations lead to checksum error",
            "Description": "The issue arises when two clients simultaneously perform append and read operations on the same file. The reader client eventually encounters a checksum error due to inconsistencies in the data being read, which is likely caused by the append operation modifying the file while it is being read. This leads to a mismatch between the expected and computed checksums, resulting in an IOException.",
            "StackTrace": [
                "java.io.IOException: No data exists for block BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getBlockInputStream(FsDatasetImpl.java:773)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:400)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:581)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:150)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:102)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:289)",
                "at java.lang.Thread.run(Thread.java:745)",
                "org.apache.hadoop.fs.ChecksumException: Checksum CRC32C not matched for file /tmp/bar.txt at position 0: expected=C893FEDE but computed=69322F90, algorithm=PureJavaCrc32C"
            ],
            "RootCause": "The root cause of the issue is the lack of proper synchronization between the append and read operations on the same file. When one client appends data, it modifies the file's content, which can lead to checksum mismatches when another client reads the file concurrently.",
            "StepsToReproduce": [
                "1. Start two clients: one for appending data to a file and another for reading the same file.",
                "2. Continuously append data to the file from the first client while the second client reads from it.",
                "3. Monitor for checksum errors in the output of the reading client."
            ],
            "ExpectedBehavior": "The reading client should be able to read the file without encountering any checksum errors, regardless of the appending operations being performed by the other client.",
            "ObservedBehavior": "The reading client encounters a checksum error indicating that the data read does not match the expected checksum, leading to an IOException.",
            "Suggestions": "Implement proper locking mechanisms or use a more robust file handling strategy to ensure that read and append operations do not interfere with each other. Consider using a versioning system for file reads to ensure consistency.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Receiver.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAppendOp.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl",
                    "org.apache.hadoop.hdfs.server.datanode.DataXceiver",
                    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver",
                    "org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp"
                ],
                "methods": [
                    "FsDatasetImpl.getBlockInputStream",
                    "DataXceiver.readBlock",
                    "Receiver.opReadBlock",
                    "FSDirAppendOp.appendFile"
                ]
            },
            "possible_fix": "To resolve this issue, implement a locking mechanism around the read and append operations to ensure that they do not occur simultaneously. This can be achieved by using synchronized blocks or other concurrency control methods to manage access to the file during these operations."
        },
        "possible_fix_code": {
            "FsDatasetImpl.getBlockInputStream": "  public InputStream getBlockInputStream(ExtendedBlock b,\n      long seekOffset) throws IOException {\n\n    ReplicaInfo info;\n    synchronized(this) {\n      info = volumeMap.get(b.getBlockPoolId(), b.getLocalBlock());\n    }\n\n    if (info != null && info.getVolume().isTransientStorage()) {\n      ramDiskReplicaTracker.touch(b.getBlockPoolId(), b.getBlockId());\n      datanode.getMetrics().incrRamDiskBlocksReadHits();\n    }\n\n    if(info != null && info.blockDataExists()) {\n      return info.getDataInputStream(seekOffset);\n    } else {\n      throw new IOException(\"No data exists for block \" + b);\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-6825.json",
        "creation_time": "2014-08-06T01:31:34.000+0000",
        "bug_report": {
            "Title": "Edit log corruption due to delayed block removal",
            "Description": "The issue arises when a client attempts to append to a file after the lease has expired. The file is deleted, but pending blocks associated with it are not removed. When the `commitBlockSynchronization()` method is called, it creates an `INodeFile` from these pending blocks without recognizing that the file has already been deleted. This leads to a `FileNotFoundException` being thrown by `FSDirectory.updateSpaceConsumed`, which is not properly handled, resulting in corruption in the edit log.",
            "StackTrace": [
                "2014-08-04 23:49:44,133 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(lastblock=BP-.., newgenerationstamp=..., newlength=..., newtargets=..., closeFile=true, deleteBlock=false)",
                "2014-08-04 23:49:44,133 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Unexpected exception while updating disk space.",
                "java.io.FileNotFoundException: Path not found: /solr/hierarchy/core_node1/data/tlog/tlog.xyz",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateSpaceConsumed(FSDirectory.java:1807)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitOrCompleteLastBlock(FSNamesystem.java:3975)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.closeFileCommitBlocks(FSNamesystem.java:4178)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:4146)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization(NameNodeRpcServer.java:662)",
                "at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolServerSideTranslatorPB.java:270)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)"
            ],
            "RootCause": "The root cause of the issue is that the system does not properly handle the case where a file is deleted while there are still pending blocks associated with it. This leads to an attempt to update disk space for a non-existent file, resulting in a `FileNotFoundException` that is not adequately managed.",
            "StepsToReproduce": [
                "1. Create a file at /solr/hierarchy/core_node1/data/tlog/tlog.xyz.",
                "2. Attempt to append to the file after the lease has expired.",
                "3. Observe that the file is deleted but pending blocks remain.",
                "4. Trigger the `commitBlockSynchronization()` method."
            ],
            "ExpectedBehavior": "The system should properly handle the deletion of files and ensure that all associated pending blocks are also removed, preventing any attempts to update disk space for non-existent files.",
            "ObservedBehavior": "The system throws a `FileNotFoundException` when trying to update disk space for a deleted file, leading to corruption in the edit log.",
            "Suggestions": "Implement checks to ensure that all pending blocks are removed when a file is deleted. Additionally, improve error handling in the `commitOrCompleteLastBlock` and `updateSpaceConsumed` methods to prevent exceptions from being swallowed.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory",
                    "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSDirectory",
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem"
                ],
                "methods": [
                    "FSDirectory.updateSpaceConsumed",
                    "FSNamesystem.commitBlockSynchronization",
                    "FSNamesystem.commitOrCompleteLastBlock",
                    "FSNamesystem.closeFileCommitBlocks"
                ]
            },
            "possible_fix": "In the `FSNamesystem.closeFileCommitBlocks` method, ensure that all pending blocks are removed when a file is deleted. Additionally, modify the `FSDirectory.updateSpaceConsumed` method to handle cases where the file does not exist more gracefully, possibly by logging a warning instead of throwing an exception."
        },
        "possible_fix_code": {
            "FSNamesystem.closeFileCommitBlocks": "  String closeFileCommitBlocks(INodeFile pendingFile, BlockInfo storedBlock)\n      throws IOException {\n    String src = pendingFile.getFullPathName();\n\n    // commit the last block and complete it if it has minimum replicas\n    commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n    // Check if the file is deleted and remove pending blocks if necessary\n    if (pendingFile.isDeleted()) {\n        // Logic to remove pending blocks associated with the deleted file\n        blockManager.removePendingBlocks(pendingFile);\n    }\n\n    //remove lease, close file\n    finalizeINodeFileUnderConstruction(src, pendingFile,\n        Snapshot.findLatestSnapshot(pendingFile, Snapshot.CURRENT_STATE_ID));\n\n    return src;\n  }"
        }
    },
    {
        "filename": "HDFS-5710.json",
        "creation_time": "2014-01-01T04:06:03.000+0000",
        "bug_report": {
            "Title": "FSDirectory#getFullPathName should check inodes against null",
            "Description": "The method FSDirectory#getFullPathName is encountering a NullPointerException when it attempts to retrieve the full path name of an inode that may not exist. This issue arises when the getRelativePathINodes() method returns null, but the getFullPathName() method does not perform a null check on the inodes, leading to the exception. This behavior can occur if the corresponding file has already been deleted, as indicated by the warning logs.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFullPathName(FSDirectory.java:1871)",
                "at org.apache.hadoop.hdfs.server.namenode.INode.getFullPathName(INode.java:482)",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.getName(INodeFile.java:316)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.chooseTarget(BlockPlacementPolicy.java:118)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1259)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1167)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3158)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3112)",
                "at java.lang.Thread.run(Thread.java:724)"
            ],
            "RootCause": "The root cause of the NullPointerException is the lack of a null check for the inodes array returned by getRelativePathINodes() in the FSDirectory#getFullPathName method. When this array is null, the subsequent operations on it lead to the exception.",
            "StepsToReproduce": [
                "1. Attempt to replicate a block that corresponds to a deleted file.",
                "2. Trigger the replication monitor to run, which calls FSDirectory#getFullPathName.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The system should gracefully handle the case where the inode is null and not throw a NullPointerException. Instead, it should log an appropriate warning and skip the replication for that block.",
            "ObservedBehavior": "The system throws a NullPointerException when trying to get the full path name of an inode that is null, causing the replication monitor to fail.",
            "Suggestions": "Implement a null check for the inodes array in the FSDirectory#getFullPathName method before attempting to access its elements. If the inodes array is null, log a warning and return an appropriate response.",
            "problem_location": {
                "files": [
                    "FSDirectory.java",
                    "INode.java",
                    "INodeFile.java",
                    "BlockPlacementPolicy.java",
                    "BlockManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSDirectory",
                    "org.apache.hadoop.hdfs.server.namenode.INode",
                    "org.apache.hadoop.hdfs.server.namenode.INodeFile",
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy",
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager"
                ],
                "methods": [
                    "FSDirectory.getFullPathName",
                    "INode.getFullPathName",
                    "INodeFile.getName",
                    "BlockPlacementPolicy.chooseTarget",
                    "BlockManager.computeReplicationWorkForBlocks"
                ]
            },
            "possible_fix": "In FSDirectory.java, modify the getFullPathName method to include a null check for the inodes array:\n\n```java\nstatic String getFullPathName(INode inode) {\n    INode[] inodes = getFullPathINodes(inode);\n    if (inodes == null) {\n        LOG.warn(\"INode is null, cannot get full path name.\");\n        return null; // or handle appropriately\n    }\n    return getFullPathName(inodes, inodes.length - 1);\n}\n```"
        },
        "possible_fix_code": {
            "FSDirectory.getFullPathName": "  static String getFullPathName(INode inode) {\n    INode[] inodes = getFullPathINodes(inode);\n    if (inodes == null) {\n        LOG.warn(\"INode is null, cannot get full path name.\");\n        return null; // or handle appropriately\n    }\n    return getFullPathName(inodes, inodes.length - 1);\n  }"
        }
    },
    {
        "filename": "HDFS-3555.json",
        "creation_time": "2012-06-20T17:36:48.000+0000",
        "bug_report": {
            "Title": "Idle client socket triggers DN ERROR log (should be INFO or DEBUG)",
            "Description": "The Datanode service is logging a java.net.SocketTimeoutException at the ERROR level when a client socket becomes idle. This exception indicates that the datanode is unable to send data to the client because the client has stopped reading. Since this situation is not critical and does not indicate a failure in the datanode itself, the log level should be adjusted to INFO or DEBUG to avoid unnecessary alarm.",
            "StackTrace": [
                "2012-06-18 17:47:13 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode DatanodeRegistration(x.x.x.x:50010, storageID=DS-196671195-10.10.120.67-50010-1334328338972, infoPort=50075, ipcPort=50020):DataXceiver",
                "java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.10.120.67:50010 remote=/10.10.120.67:59282]",
                "at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:246)",
                "at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:159)",
                "at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:198)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendChunks(BlockSender.java:397)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:493)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:267)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:163)"
            ],
            "RootCause": "The root cause of the issue is that the datanode is logging a SocketTimeoutException at the ERROR level when a client socket is idle. This is due to the datanode's expectation that the client will continuously read data, and when it does not, a timeout occurs. The logging level for this situation should be adjusted to reflect its non-critical nature.",
            "StepsToReproduce": [
                "1. Start the Datanode service.",
                "2. Connect a client to the Datanode and initiate a read operation.",
                "3. Stop the client from reading data (e.g., pause or close the client).",
                "4. Wait for the socket timeout to occur."
            ],
            "ExpectedBehavior": "The Datanode should log the SocketTimeoutException at the INFO or DEBUG level instead of ERROR, indicating that the client is idle without raising an alarm.",
            "ObservedBehavior": "The Datanode logs a SocketTimeoutException at the ERROR level, which is misleading and causes unnecessary concern.",
            "Suggestions": "Change the logging level for the SocketTimeoutException in the Datanode to INFO or DEBUG to accurately reflect the non-critical nature of the situation.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.DataXceiver",
                    "org.apache.hadoop.hdfs.server.datanode.BlockSender"
                ],
                "methods": [
                    "DataXceiver.readBlock",
                    "BlockSender.sendBlock"
                ]
            },
            "possible_fix": "In the DataXceiver class, modify the logging level for the SocketTimeoutException in the readBlock method to INFO or DEBUG. For example, replace the line that logs the exception at ERROR level with a log statement at INFO level, indicating that the client is idle."
        },
        "possible_fix_code": {
            "DataXceiver.readBlock": "  public void readBlock(final ExtendedBlock block,\n      final Token<BlockTokenIdentifier> blockToken,\n      final String clientName,\n      final long blockOffset,\n      final long length) throws IOException {\n    previousOpClientName = clientName;\n\n    OutputStream baseStream = NetUtils.getOutputStream(s, \n        dnConf.socketWriteTimeout);\n    DataOutputStream out = new DataOutputStream(new BufferedOutputStream(\n        baseStream, HdfsConstants.SMALL_BUFFER_SIZE));\n    checkAccess(out, true, block, blockToken,\n        Op.READ_BLOCK, BlockTokenSecretManager.AccessMode.READ);\n  \n    // send the block\n    BlockSender blockSender = null;\n    DatanodeRegistration dnR = \n      datanode.getDNRegistrationForBP(block.getBlockPoolId());\n    final String clientTraceFmt =\n      clientName.length() > 0 && ClientTraceLog.isInfoEnabled()\n        ? String.format(DN_CLIENTTRACE_FORMAT, localAddress, remoteAddress,\n            \"%d\", \"HDFS_READ\", clientName, \"%d\",\n            dnR.getStorageID(), block, \"%d\")\n        : dnR + \" Served block \" + block + \" to \" +\n            remoteAddress;\n\n    updateCurrentThreadName(\"Sending block \" + block);\n    try {\n      try {\n        blockSender = new BlockSender(block, blockOffset, length,\n            true, false, datanode, clientTraceFmt);\n      } catch(IOException e) {\n        String msg = \"opReadBlock \" + block + \" received exception \" + e; \n        LOG.info(msg);\n        sendResponse(s, ERROR, msg, dnConf.socketWriteTimeout);\n        throw e;\n      }\n      \n      // send op status\n      writeSuccessWithChecksumInfo(blockSender,\n          getStreamWithTimeout(s, dnConf.socketWriteTimeout));\n\n      long read = blockSender.sendBlock(out, baseStream, null); // send data\n\n      if (blockSender.didSendEntireByteRange()) {\n        // If we sent the entire range, then we should expect the client\n        // to respond with a Status enum.\n        try {\n          ClientReadStatusProto stat = ClientReadStatusProto.parseFrom(\n              HdfsProtoUtil.vintPrefixed(in));\n          if (!stat.hasStatus()) {\n            LOG.warn(\"Client \" + s.getInetAddress() + \" did not send a valid status \" +\n                     \"code after reading. Will close connection.\");\n            IOUtils.closeStream(out);\n          }\n        } catch (IOException ioe) {\n          LOG.debug(\"Error reading client status response. Will close connection.\", ioe);\n          IOUtils.closeStream(out);\n        }\n      } else {\n        IOUtils.closeStream(out);\n      }\n      datanode.metrics.incrBytesRead((int) read);\n      datanode.metrics.incrBlocksRead();\n    } catch ( SocketException ignored ) {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(dnR + \":Ignoring exception while serving \" + block + \" to \" +\n            remoteAddress, ignored);\n      }\n      // Its ok for remote side to close the connection anytime.\n      datanode.metrics.incrBlocksRead();\n      IOUtils.closeStream(out);\n    } catch ( IOException ioe ) {\n      /* What exactly should we do here?\n       * Earlier version shutdown() datanode if there is disk error.\n       */\n      LOG.info(dnR + \":Got exception while serving \" + block + \" to \"\n          + remoteAddress, ioe);\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(blockSender);\n    }\n\n    //update metrics\n    datanode.metrics.addReadBlockOp(elapsed());\n    datanode.metrics.incrReadsFromClient(isLocal);\n  }"
        }
    },
    {
        "filename": "HDFS-10962.json",
        "creation_time": "2016-10-05T04:31:45.000+0000",
        "bug_report": {
            "Title": "TestRequestHedgingProxyProvider is flaky",
            "Description": "The test 'testHedgingWhenOneFails' in the 'TestRequestHedgingProxyProvider' class intermittently fails due to the mock 'namenodeProtocols' not being invoked as expected. The error indicates that the method 'getStats()' was expected to be called, but there were zero interactions with the mock. This suggests that the test setup may not be correctly simulating the conditions under which 'getStats()' should be invoked, leading to flaky test behavior.",
            "StackTrace": [
                "org.mockito.exceptions.verification.WantedButNotInvoked: Wanted but not invoked: namenodeProtocols.getStats();",
                "at org.apache.hadoop.hdfs.server.namenode.ha.TestRequestHedgingProxyProvider.testHedgingWhenOneFails(TestRequestHedgingProxyProvider.java:78)"
            ],
            "RootCause": "The test is not properly invoking the 'getStats()' method on the mock 'namenodeProtocols', likely due to incorrect test setup or conditions not being met for the method to be called.",
            "StepsToReproduce": [
                "Run the test 'testHedgingWhenOneFails' in the 'TestRequestHedgingProxyProvider' class multiple times.",
                "Observe that the test fails intermittently with the specified error message."
            ],
            "ExpectedBehavior": "The 'getStats()' method on the 'namenodeProtocols' mock should be invoked during the execution of the test, allowing the test to pass consistently.",
            "ObservedBehavior": "The test fails with an error indicating that 'getStats()' was not invoked, leading to inconsistent test results.",
            "Suggestions": "Review the test setup to ensure that the conditions for invoking 'getStats()' are met. Consider adding additional logging or assertions to verify the flow of execution leading up to the method call.",
            "problem_location": {
                "files": [
                    "TestRequestHedgingProxyProvider.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.ha.TestRequestHedgingProxyProvider"
                ],
                "methods": [
                    "testHedgingWhenOneFails"
                ]
            },
            "possible_fix": "Ensure that the test correctly simulates the conditions under which 'getStats()' should be called. This may involve adjusting the mock setup or the test logic to ensure that the method is invoked as expected."
        },
        "possible_fix_code": {
            "testHedgingWhenOneFails": "public void testHedgingWhenOneFails() throws Exception {\n    // Arrange\n    namenodeProtocols = Mockito.mock(NamenodeProtocols.class);\n    // Set up the mock to return a specific value when getStats() is called\n    Mockito.when(namenodeProtocols.getStats()).thenReturn(new Stats(...));\n\n    // Act\n    // Simulate the conditions that would lead to getStats() being called\n    // This might involve invoking methods that are expected to call getStats()\n    someMethodThatShouldCallGetStats(namenodeProtocols);\n\n    // Assert\n    // Verify that getStats() was indeed called\n    Mockito.verify(namenodeProtocols).getStats();\n}"
        }
    },
    {
        "filename": "HDFS-12363.json",
        "creation_time": "2017-08-28T05:34:21.000+0000",
        "bug_report": {
            "Title": "Possible NPE in BlockManager$StorageInfoDefragmenter#scanAndCompactStorages",
            "Description": "The application encountered a NullPointerException (NPE) in the method scanAndCompactStorages of the BlockManager$StorageInfoDefragmenter class. The NPE occurs when attempting to access storage information for a datanode that may not exist or is not properly initialized. This can lead to the application crashing unexpectedly, as seen in the logs where the thread received a runtime exception.",
            "StackTrace": [
                "ERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Thread received Runtime exception.",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.scanAndCompactStorages(BlockManager.java:3897)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.run(BlockManager.java:3852)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the NPE is likely due to the method getDatanode returning null for a datanode identifier, which is then used to call getStorageInfo, leading to the NPE when trying to access properties of a null object.",
            "StepsToReproduce": [
                "1. Start the Hadoop HDFS service.",
                "2. Trigger the storage defragmentation process in BlockManager.",
                "3. Monitor the logs for any NullPointerExceptions."
            ],
            "ExpectedBehavior": "The scanAndCompactStorages method should handle cases where a datanode does not exist or is not initialized, preventing any NullPointerExceptions and allowing the defragmentation process to complete successfully.",
            "ObservedBehavior": "The application crashes with a NullPointerException when attempting to access storage information for a non-existent datanode.",
            "Suggestions": "Implement a null check after retrieving the datanode to ensure that it is not null before attempting to access its storage information. Additionally, consider logging a warning if a datanode is not found instead of allowing the application to crash.",
            "problem_location": {
                "files": [
                    "BlockManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter"
                ],
                "methods": [
                    "scanAndCompactStorages",
                    "run"
                ]
            },
            "possible_fix": "Modify the scanAndCompactStorages method to include a null check for the datanode before accessing its storage information:\n\n```java\nDatanodeStorageInfo storage = datanodeManager.getDatanode(datanodesAndStorages.get(i));\nif (storage != null) {\n    storage = storage.getStorageInfo(datanodesAndStorages.get(i + 1));\n}\nif (storage == null) {\n    // Log a warning and handle the case appropriately\n    LOG.warn(\"Datanode storage info is null for datanode: \" + datanodesAndStorages.get(i));\n    return;\n}\n```"
        },
        "possible_fix_code": {
            "scanAndCompactStorages": "public void scanAndCompactStorages() {\n    // Assuming datanodesAndStorages is a list of datanode identifiers\n    for (int i = 0; i < datanodesAndStorages.size(); i++) {\n        DatanodeStorageInfo storage = datanodeManager.getDatanode(datanodesAndStorages.get(i));\n        if (storage != null) {\n            storage = storage.getStorageInfo(datanodesAndStorages.get(i + 1));\n        }\n        if (storage == null) {\n            // Log a warning and handle the case appropriately\n            LOG.warn(\"Datanode storage info is null for datanode: \" + datanodesAndStorages.get(i));\n            return;\n        }\n        // Continue with the rest of the method logic...\n    }\n}"
        }
    },
    {
        "filename": "HDFS-7916.json",
        "creation_time": "2015-03-11T10:40:00.000+0000",
        "bug_report": {
            "Title": "'reportBadBlocks' from datanodes to standby Node BPServiceActor goes for infinite loop",
            "Description": "The issue arises when the DataNode attempts to report a bad block to the NameNode. If the reporting fails due to an IOException, the action is caught and a BPServiceActorActionException is thrown. This exception is logged, but the action is then re-enqueued for processing, leading to an infinite loop of retries without a successful report. The root of the problem lies in the handling of failed reports in the processQueueMessages method, which does not have a mechanism to stop retrying after a certain number of failures.",
            "StackTrace": [
                "2015-03-11 19:43:41,528 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to report bad block BP-1384821822-10.224.54.68-1422634566395:blk_1079544278_5812006 to namenode: stobdtserver3/10.224.54.70:18010",
                "org.apache.hadoop.hdfs.server.datanode.BPServiceActorActionException: Failed to report bad block BP-1384821822-10.224.54.68-1422634566395:blk_1079544278_5812006 to namenode:",
                "at org.apache.hadoop.hdfs.server.datanode.ReportBadBlockAction.reportTo(ReportBadBlockAction.java:63)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processQueueMessages(BPServiceActor.java:1020)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:762)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:856)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The infinite loop occurs because failed reporting attempts are continuously re-enqueued without a limit on the number of retries, leading to repeated failures without resolution.",
            "StepsToReproduce": [
                "1. Introduce a bad block in the DataNode.",
                "2. Ensure that the NameNode is unreachable or configured to reject bad block reports.",
                "3. Monitor the logs for repeated attempts to report the bad block."
            ],
            "ExpectedBehavior": "The DataNode should report bad blocks to the NameNode successfully or stop retrying after a certain number of failures, logging the issue appropriately.",
            "ObservedBehavior": "The DataNode enters an infinite loop of retrying to report the bad block, flooding the logs with warnings and not resolving the issue.",
            "Suggestions": "Implement a retry limit for reporting bad blocks in the processQueueMessages method. After a certain number of failures, log the issue and stop further attempts to report the same bad block.",
            "problem_location": {
                "files": [
                    "ReportBadBlockAction.java",
                    "BPServiceActor.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.ReportBadBlockAction",
                    "org.apache.hadoop.hdfs.server.datanode.BPServiceActor"
                ],
                "methods": [
                    "ReportBadBlockAction.reportTo",
                    "BPServiceActor.processQueueMessages"
                ]
            },
            "possible_fix": "In the processQueueMessages method, add a counter to track the number of failed attempts for each action. If the counter exceeds a predefined limit (e.g., 5 attempts), log the failure and do not re-enqueue the action."
        },
        "possible_fix_code": {
            "BPServiceActor.processQueueMessages": "  private void processQueueMessages() {\n    LinkedList<BPServiceActorAction> duplicateQueue;\n    synchronized (bpThreadQueue) {\n      duplicateQueue = new LinkedList<BPServiceActorAction>(bpThreadQueue);\n      bpThreadQueue.clear();\n    }\n    int retryLimit = 5; // Set a retry limit\n    while (!duplicateQueue.isEmpty()) {\n      BPServiceActorAction actionItem = duplicateQueue.remove();\n      int attemptCount = 0; // Initialize attempt counter\n      boolean success = false;\n      while (attemptCount < retryLimit && !success) {\n        try {\n          actionItem.reportTo(bpNamenode, bpRegistration);\n          success = true; // Mark as successful if no exception\n        } catch (BPServiceActorActionException baae) {\n          attemptCount++;\n          LOG.warn(baae.getMessage() + nnAddr + \" Attempt: \" + attemptCount, baae);\n          if (attemptCount >= retryLimit) {\n            LOG.error(\"Exceeded retry limit for action: \" + actionItem);\n          } else {\n            // Adding it back to the queue if not present\n            bpThreadEnqueue(actionItem);\n          }\n        }\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-9549.json",
        "creation_time": "2015-12-11T16:25:08.000+0000",
        "bug_report": {
            "Title": "TestCacheDirectives#testExceedsCapacity is flaky",
            "Description": "The test 'TestCacheDirectives.testExceedsCapacity' is failing intermittently in Jenkins due to an assertion that checks if the pending cached list is empty. The error indicates that the pending cached list is not empty when it is expected to be, suggesting a potential issue with cache management or timing in the test setup.",
            "StackTrace": [
                "java.lang.AssertionError: Pending cached list of 127.0.0.1:54134 is not empty, [{blockId=1073741841, replication=1, mark=true}]",
                "\tat org.junit.Assert.fail(Assert.java:88)",
                "\tat org.junit.Assert.assertTrue(Assert.java:41)",
                "\tat org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.checkPendingCachedEmpty(TestCacheDirectives.java:1479)",
                "\tat org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testExceedsCapacity(TestCacheDirectives.java:1502)"
            ],
            "RootCause": "The root cause appears to be related to the timing of cache operations, where the cache is not being cleared or updated as expected before the assertion is made. This could be due to race conditions or improper handling of cache state in the test.",
            "StepsToReproduce": [
                "Run the test suite for TestCacheDirectives in Jenkins.",
                "Observe the frequency of failure for the test testExceedsCapacity."
            ],
            "ExpectedBehavior": "The pending cached list should be empty after the cache operations are completed, allowing the assertion to pass.",
            "ObservedBehavior": "The pending cached list is not empty, leading to an AssertionError during the test.",
            "Suggestions": "Review the cache management logic in the test and ensure that all cache operations are completed before the assertion is made. Consider adding synchronization or waiting mechanisms to ensure the cache is in the expected state.",
            "problem_location": {
                "files": [
                    "TestCacheDirectives.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives"
                ],
                "methods": [
                    "checkPendingCachedEmpty",
                    "testExceedsCapacity"
                ]
            },
            "possible_fix": "Add a synchronization mechanism or a wait condition in the test to ensure that the cache is fully processed before checking its state. For example, use a CountDownLatch or similar construct to wait for cache operations to complete."
        },
        "possible_fix_code": {
            "testExceedsCapacity": "public void testExceedsCapacity() throws Exception {\n    // Setup code for the test\n    // ...\n\n    // Create a CountDownLatch to wait for cache operations to complete\n    final CountDownLatch latch = new CountDownLatch(1);\n\n    // Perform cache operations in a separate thread\n    new Thread(() -> {\n        try {\n            // Simulate cache operations\n            // ...\n        } finally {\n            // Count down the latch to signal completion\n            latch.countDown();\n        }\n    }).start();\n\n    // Wait for the cache operations to complete\n    latch.await();\n\n    // Now check if the pending cached list is empty\n    checkPendingCachedEmpty();\n}"
        }
    },
    {
        "filename": "HDFS-11164.json",
        "creation_time": "2016-11-22T05:58:03.000+0000",
        "bug_report": {
            "Title": "Mover should avoid unnecessary retries if the block is pinned",
            "Description": "The Mover component in the Hadoop HDFS is currently configured to retry moving blocks even when they are pinned, leading to unnecessary retries and log clutter. When a pinned block is attempted to be moved, an IOException is thrown indicating that the block cannot be copied due to its pinned status. This behavior is exacerbated by the configuration setting 'dfs.mover.retry.max.attempts', which causes the Mover to continue retrying until the maximum attempts are reached, despite the fact that pinned blocks cannot be moved. The goal of this bug report is to implement a check that prevents retries for pinned blocks, thereby optimizing the Mover's operation and reducing unnecessary error logging.",
            "StackTrace": [
                "java.io.IOException: Got error, status=ERROR, status message Not able to copy block 1073741825 to /127.0.0.1:19826 because it's pinned",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:118)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.receiveResponse(Dispatcher.java:417)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.dispatch(Dispatcher.java:358)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.access$5(Dispatcher.java:322)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$1.run(Dispatcher.java:1075)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the Mover does not currently check if a block is pinned before attempting to move it, leading to repeated IOException errors and unnecessary retries.",
            "StepsToReproduce": [
                "1. Pin a block in the HDFS.",
                "2. Trigger the Mover to attempt to move the pinned block to another datanode.",
                "3. Observe the logs for repeated IOException messages indicating the block cannot be moved due to its pinned status."
            ],
            "ExpectedBehavior": "The Mover should check if a block is pinned before attempting to move it and avoid any retries if the block is confirmed to be pinned.",
            "ObservedBehavior": "The Mover attempts to move a pinned block, resulting in repeated IOException errors and unnecessary retries until the maximum retry attempts are reached.",
            "Suggestions": "Implement a check in the Mover's logic to determine if a block is pinned before attempting to move it. If the block is pinned, skip the move operation and log a warning instead of retrying.",
            "problem_location": {
                "files": [
                    "Dispatcher.java",
                    "DataTransferProtoUtil.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.balancer.Dispatcher",
                    "org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil"
                ],
                "methods": [
                    "Dispatcher$PendingMove.receiveResponse",
                    "DataTransferProtoUtil.checkBlockOpStatus"
                ]
            },
            "possible_fix": "Add a check in the Mover's logic (specifically in the 'Dispatcher$PendingMove.receiveResponse' method) to verify if the block is pinned before proceeding with the move operation. If it is pinned, log a warning and return early without retrying."
        },
        "possible_fix_code": {
            "Dispatcher$PendingMove.receiveResponse": "public void receiveResponse(DataTransferResponseProto response) {\n    try {\n        // Check if the block is pinned before proceeding\n        if (isBlockPinned(response.getBlockId())) {\n            LOG.warn(\"Skipping move for pinned block: \" + response.getBlockId());\n            return; // Skip the move operation if the block is pinned\n        }\n        DataTransferProtoUtil.checkBlockOpStatus(response.getBlockOpResponse(), \"Moving block: \" + response.getBlockId());\n        // Proceed with the move operation\n        // ... (existing logic for moving the block)\n    } catch (IOException e) {\n        LOG.error(\"Error moving block: \" + response.getBlockId(), e);\n        // Handle the exception as needed\n    }\n}\n\nprivate boolean isBlockPinned(long blockId) {\n    // Logic to determine if the block is pinned\n    // This is a placeholder; actual implementation will depend on the system's state\n    return false; // Replace with actual check\n}"
        }
    },
    {
        "filename": "HDFS-5291.json",
        "creation_time": "2013-10-02T23:52:02.000+0000",
        "bug_report": {
            "Title": "Clients need to retry when Active NN is in SafeMode",
            "Description": "The system enters SafeMode immediately after transitioning to the active state, which leads to HBase region server timeouts and subsequent crashes. This behavior occurs because the NameNode (NN) does not allow write operations while in SafeMode, causing clients to fail their requests. To improve system resilience, clients should be allowed to retry their operations when the Active NameNode is in SafeMode.",
            "StackTrace": [
                "java.lang.InterruptedException: sleep interrupted",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:334)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:279)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:296)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:356)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1463)",
                "at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:454)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:334)"
            ],
            "RootCause": "The root cause of the issue is that the Active NameNode enters SafeMode immediately after activation, preventing write operations and causing client requests to fail. The system does not handle retries for clients in this state, leading to timeouts and crashes.",
            "StepsToReproduce": [
                "1. Start the NameNode in standby mode.",
                "2. Transition the NameNode to active state.",
                "3. Observe the logs for the transition to SafeMode.",
                "4. Attempt to perform write operations from clients during SafeMode."
            ],
            "ExpectedBehavior": "Clients should be able to retry their write operations when the Active NameNode is in SafeMode without causing timeouts or crashes.",
            "ObservedBehavior": "Clients experience timeouts and failures when attempting to perform write operations while the Active NameNode is in SafeMode.",
            "Suggestions": "Implement a retry mechanism for clients when the Active NameNode is in SafeMode. This could involve checking the state of the NameNode and allowing clients to retry their operations after a brief wait period.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.SecurityUtil",
                    "org.apache.hadoop.security.UserGroupInformation",
                    "org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer"
                ],
                "methods": [
                    "SecurityUtil.doAsLoginUserOrFatal",
                    "UserGroupInformation.doAs",
                    "EditLogTailer.EditLogTailerThread.doWork"
                ]
            },
            "possible_fix": "Add a retry mechanism in the client code that checks if the Active NameNode is in SafeMode and retries the operation after a short delay. This can be implemented in the client-side logic that interacts with the NameNode."
        },
        "possible_fix_code": {
            "EditLogTailer.EditLogTailerThread.doWork": "  public void doWork() throws InterruptedException {\n    while (true) {\n      try {\n        // Check if the NameNode is in SafeMode\n        if (isNameNodeInSafeMode()) {\n          // If in SafeMode, wait and retry\n          Thread.sleep(1000); // Wait for 1 second before retrying\n          continue; // Retry the operation\n        }\n        // Existing logic for processing edit logs\n        processEditLogs();\n      } catch (InterruptedException e) {\n        // Handle the interruption\n        Thread.currentThread().interrupt();\n        throw e;\n      } catch (Exception e) {\n        LOG.error(\"Error processing edit logs: \" + e.getMessage(), e);\n      }\n    }\n  }\n\n  private boolean isNameNodeInSafeMode() {\n    // Logic to check if the NameNode is in SafeMode\n    // This is a placeholder; actual implementation will depend on the system's state management\n    return false; // Replace with actual check\n  }\n\n  private void processEditLogs() {\n    // Existing logic for processing edit logs\n  }"
        }
    },
    {
        "filename": "HDFS-12836.json",
        "creation_time": "2017-11-17T20:26:32.000+0000",
        "bug_report": {
            "Title": "startTxId could be greater than endTxId when tailing in-progress edit log",
            "Description": "The issue arises when the configuration {{dfs.ha.tail-edits.in-progress}} is set to true, allowing the edit log tailer to process in-progress edit log segments. In the current implementation, there is a potential for {{remoteLog.getStartTxId()}} to exceed {{endTxId}} due to the logic in the code that sets {{endTxId}} to the minimum of itself and {{committedTxnId}}. This can lead to a situation where the expected transaction ID is not found, resulting in a premature end-of-file error during the replay of the edit log.",
            "StackTrace": [
                "2017-11-17 19:55:41,165 ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: Error replaying edit log at offset 1048576.  Expected transaction ID was 87",
                "Recent opcode offsets: 1048576",
                "org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$PrematureEOFException: got premature end-of-file at txid 86; expected file to go up to 85",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:197)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:189)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:205)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:158)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:882)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:863)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:293)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:427)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:380)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:397)",
                "at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:481)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:393)"
            ],
            "RootCause": "The root cause of the issue is the logic that sets the {{endTxId}} to the minimum of itself and {{committedTxnId}}. This can lead to a scenario where {{remoteLog.getStartTxId()}} is greater than {{endTxId}}, causing the edit log replay to fail with a premature end-of-file error.",
            "StepsToReproduce": [
                "Set the configuration {{dfs.ha.tail-edits.in-progress}} to true.",
                "Start the edit log tailer.",
                "Ensure that there are in-progress edit log segments.",
                "Observe the logs for errors related to transaction IDs."
            ],
            "ExpectedBehavior": "The edit log tailer should successfully process all edit log segments without encountering premature end-of-file errors, ensuring that the transaction IDs are in the expected order.",
            "ObservedBehavior": "The edit log tailer fails with a premature end-of-file error when the start transaction ID is greater than the end transaction ID, leading to inconsistencies in the edit log replay.",
            "Suggestions": "To mitigate this issue, ensure that the logic for setting {{endTxId}} accounts for the possibility of in-progress transactions. Consider adding validation to check that {{remoteLog.getStartTxId()}} is not greater than {{endTxId}} before proceeding with the edit log replay.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogInputStream.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader",
                    "org.apache.hadoop.hdfs.server.namenode.EditLogInputStream",
                    "org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer"
                ],
                "methods": [
                    "FSEditLogLoader.loadEditRecords",
                    "EditLogInputStream.readOp",
                    "EditLogTailer.doTailEdits"
                ]
            },
            "possible_fix": "Modify the logic in the method that sets {{endTxId}} to ensure it does not allow {{remoteLog.getStartTxId()}} to exceed {{endTxId}}. For example, add a check before the call to {{EditLogFileInputStream.fromUrl}} to validate the transaction IDs."
        },
        "possible_fix_code": {
            "FSEditLogLoader.loadFSEdits": "  long loadFSEdits(EditLogInputStream edits, long expectedStartingTxId,\n      StartupOption startOpt, MetaRecoveryContext recovery) throws IOException {\n    StartupProgress prog = NameNode.getStartupProgress();\n    Step step = createStartupProgressStep(edits);\n    prog.beginStep(Phase.LOADING_EDITS, step);\n    fsNamesys.writeLock();\n    try {\n      long startTime = monotonicNow();\n      FSImage.LOG.info(\"Start loading edits file \" + edits.getName());\n\n      // Validate that the expected starting transaction ID is not greater than the last transaction ID\n      long lastTxId = edits.getLastTxId();\n      if (expectedStartingTxId > lastTxId) {\n        throw new IOException(\"Expected starting transaction ID \" + expectedStartingTxId + \n            \" is greater than the last transaction ID \" + lastTxId);\n      }\n\n      long numEdits = loadEditRecords(edits, false, expectedStartingTxId,\n          startOpt, recovery);\n      FSImage.LOG.info(\"Edits file \" + edits.getName() \n          + \" of size \" + edits.length() + \" edits # \" + numEdits \n          + \" loaded in \" + (monotonicNow()-startTime)/1000 + \" seconds\");\n      return numEdits;\n    } finally {\n      edits.close();\n      fsNamesys.writeUnlock(\"loadFSEdits\");\n      prog.endStep(Phase.LOADING_EDITS, step);\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-8113.json",
        "creation_time": "2015-04-09T11:29:31.000+0000",
        "bug_report": {
            "Title": "Add check for null BlockCollection pointers in BlockInfoContiguous structures",
            "Description": "The copy constructor for BlockInfoContiguous can throw a NullPointerException if the BlockCollection (bc) is null. This issue arises when DataNodes attempt to report blocks to the NameNode, leading to failures in block reporting. The stack trace indicates that the NullPointerException occurs during the processing of reported blocks, specifically when accessing properties of a null BlockCollection.",
            "StackTrace": [
                "2015-03-08 19:28:13,442 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: RemoteException in offerService",
                "org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo.(BlockInfo.java:80)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt.(BlockManager.java:1696)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.checkReplicaCorrupt(BlockManager.java:2185)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReportedBlock(BlockManager.java:2047)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.reportDiff(BlockManager.java:1950)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:1823)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:1750)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.blockReport(NameNodeRpcServer.java:1069)",
                "at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.blockReport(DatanodeProtocolServerSideTranslatorPB.java:152)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1623)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)"
            ],
            "RootCause": "The root cause of the issue is that the BlockCollection (bc) in the BlockInfoContiguous constructor is not being checked for null before it is accessed. This leads to a NullPointerException when the constructor attempts to call methods on a null reference.",
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with DataNodes and NameNode.",
                "2. Simulate a scenario where a DataNode reports blocks with a null BlockCollection.",
                "3. Observe the logs for NullPointerException during block reporting."
            ],
            "ExpectedBehavior": "The system should handle null BlockCollection pointers gracefully, either by initializing them or by throwing a controlled exception that can be handled appropriately.",
            "ObservedBehavior": "The system throws a NullPointerException when a DataNode attempts to report blocks with a null BlockCollection, causing block reporting to fail.",
            "Suggestions": "Implement a null check for the BlockCollection in the BlockInfoContiguous constructor. If it is null, either initialize it with a default value or throw a specific exception that can be handled upstream.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo",
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager"
                ],
                "methods": [
                    "BlockInfo.(BlockInfoContiguous.java:80)",
                    "BlockManager.processReportedBlock(BlockManager.java:2047)"
                ]
            },
            "possible_fix": "In the BlockInfoContiguous constructor, add a null check for the 'from.bc' before accessing it. For example:\n\nprotected BlockInfoContiguous(BlockInfoContiguous from) {\n    if (from.bc == null) {\n        throw new IllegalArgumentException(\"BlockCollection cannot be null\");\n    }\n    this.bc = from.bc;\n    this.replication = from.bc.getBlockReplication();\n}"
        },
        "possible_fix_code": {
            "BlockInfoContiguous": "protected BlockInfoContiguous(BlockInfoContiguous from) {\n    if (from.bc == null) {\n        throw new IllegalArgumentException(\"BlockCollection cannot be null\");\n    }\n    this.bc = from.bc;\n    this.replication = from.bc.getBlockReplication();\n}"
        }
    },
    {
        "filename": "HDFS-10512.json",
        "creation_time": "2016-06-09T14:34:04.000+0000",
        "bug_report": {
            "Title": "VolumeScanner may terminate due to NPE in DataNode.reportBadBlocks",
            "Description": "The VolumeScanner is encountering a NullPointerException (NPE) when attempting to report bad blocks in the DataNode's reportBadBlocks method. This issue arises when the volume associated with a block cannot be retrieved, leading to a failure in the reporting process. The NPE occurs specifically when the volume variable is null, which indicates that the DataNode is unable to find the corresponding volume for the block being scanned.",
            "StackTrace": [
                "2016-04-07 20:30:53,830 WARN org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Reporting bad BP-1800173197-10.204.68.5-1444425156296:blk_1170134484_96468685 on /dfs/dn",
                "2016-04-07 20:30:53,831 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/dfs/dn, DS-89b72832-2a8c-48f3-8235-48e6c5eb5ab3) exiting because of exception",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:1018)",
                "at org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler.handle(VolumeScanner.java:287)",
                "at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.scanBlock(VolumeScanner.java:443)",
                "at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:547)",
                "at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:621)",
                "2016-04-07 20:30:53,832 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/dfs/dn, DS-89b72832-2a8c-48f3-8235-48e6c5eb5ab3) exiting."
            ],
            "RootCause": "The root cause of the NPE is that the volume variable in the reportBadBlocks method is null. This occurs when the DataNode is unable to retrieve the volume associated with the block being reported as bad.",
            "StepsToReproduce": [
                "1. Start the VolumeScanner on a DataNode.",
                "2. Ensure that there are blocks that are marked as bad.",
                "3. Trigger the VolumeScanner to report bad blocks.",
                "4. Observe the logs for a NullPointerException in the reportBadBlocks method."
            ],
            "ExpectedBehavior": "The VolumeScanner should successfully report bad blocks without terminating due to exceptions.",
            "ObservedBehavior": "The VolumeScanner terminates unexpectedly due to a NullPointerException when trying to report bad blocks.",
            "Suggestions": "Implement a null check for the volume variable before attempting to access its methods. If the volume is null, log an appropriate warning and handle the situation gracefully without throwing an exception.",
            "problem_location": {
                "files": [
                    "DataNode.java",
                    "VolumeScanner.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.DataNode",
                    "org.apache.hadoop.hdfs.server.datanode.VolumeScanner"
                ],
                "methods": [
                    "DataNode.reportBadBlocks",
                    "VolumeScanner.scanBlock"
                ]
            },
            "possible_fix": "In the reportBadBlocks method, add a null check for the volume variable:\n\npublic void reportBadBlocks(ExtendedBlock block) throws IOException {\n    BPOfferService bpos = getBPOSForBlock(block);\n    FsVolumeSpi volume = getFSDataset().getVolume(block);\n    if (volume == null) {\n        LOG.warn(\"Volume not found for block: \" + block);\n        return; // Handle the null case gracefully\n    }\n    bpos.reportBadBlocks(\n        block, volume.getStorageID(), volume.getStorageType());\n}"
        },
        "possible_fix_code": {
            "DataNode.reportBadBlocks": "  public void reportBadBlocks(ExtendedBlock block) throws IOException{\n    BPOfferService bpos = getBPOSForBlock(block);\n    FsVolumeSpi volume = getFSDataset().getVolume(block);\n    if (volume == null) {\n        LOG.warn(\"Volume not found for block: \" + block);\n        return; // Handle the null case gracefully\n    }\n    bpos.reportBadBlocks(\n        block, volume.getStorageID(), volume.getStorageType());\n  }"
        }
    },
    {
        "filename": "HDFS-10729.json",
        "creation_time": "2016-08-08T02:36:05.000+0000",
        "bug_report": {
            "Title": "Improve log message for edit loading failures caused by FS limit checks.",
            "Description": "The Standby NameNode crashes due to a NullPointerException (NPE) when loading edits, specifically when the maximum directory item limit is exceeded. The NPE occurs because the system attempts to access file encryption information for a directory that has exceeded its item limit, leading to a failure in the edit log loading process. This issue is exacerbated by the fact that the exception is logged at the debug level, and a null value is returned, which is subsequently used, causing the NPE. This behavior is not limited to corrupt edits, as previously suggested in HDFS-7567, but can occur under normal conditions when the directory item limit is exceeded.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFileEncryptionInfo(FSDirectory.java:2914)",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.createFileStatus(FSDirectory.java:2469)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:375)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:230)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:139)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:829)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:810)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:232)"
            ],
            "RootCause": "The root cause of the issue is that the maximum number of items per directory is exceeded, which leads to a MaxDirectoryItemsExceededException being thrown. This exception is logged but not handled properly, resulting in a null return value that causes a NullPointerException when accessed later.",
            "StepsToReproduce": [
                "Set the directory item limit to a value lower than the current number of items in the directory.",
                "Attempt to load edits that include operations for the directory exceeding the item limit.",
                "Observe the crash of the Standby NameNode due to the NullPointerException."
            ],
            "ExpectedBehavior": "The system should handle the MaxDirectoryItemsExceededException gracefully, logging an appropriate error message and preventing the Standby NameNode from crashing.",
            "ObservedBehavior": "The Standby NameNode crashes with a NullPointerException when attempting to load edits that exceed the directory item limit.",
            "Suggestions": "Implement proper exception handling for MaxDirectoryItemsExceededException in the FSDirectory class to prevent null returns. Additionally, improve logging to provide clearer information about the state of the system when such exceptions occur.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSDirectory",
                    "org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader"
                ],
                "methods": [
                    "FSDirectory.getFileEncryptionInfo",
                    "FSDirectory.createFileStatus",
                    "FSEditLogLoader.applyEditLogOp",
                    "FSEditLogLoader.loadEditRecords"
                ]
            },
            "possible_fix": "In the FSDirectory class, modify the verifyMaxDirItems method to handle the MaxDirectoryItemsExceededException properly. Instead of returning null, log an error message and prevent further processing that leads to a NullPointerException. For example:\n\n```java\nvoid verifyMaxDirItems(INodeDirectory parent, String parentPath) throws MaxDirectoryItemsExceededException {\n    final int count = parent.getChildrenList(CURRENT_STATE_ID).size();\n    if (count >= maxDirItems) {\n        final MaxDirectoryItemsExceededException e = new MaxDirectoryItemsExceededException(maxDirItems, count);\n        if (namesystem.isImageLoaded()) {\n            e.setPathName(parentPath);\n            throw e;\n        } else {\n            // Log the error and prevent further processing\n            NameNode.LOG.error(\"FSDirectory.verifyMaxDirItems: \" + e.getLocalizedMessage());\n            return; // Prevent further processing\n        }\n    }\n}\n```"
        },
        "possible_fix_code": {
            "FSDirectory.verifyMaxDirItems": "  void verifyMaxDirItems(INodeDirectory parent, String parentPath)\n      throws MaxDirectoryItemsExceededException {\n    final int count = parent.getChildrenList(CURRENT_STATE_ID).size();\n    if (count >= maxDirItems) {\n      final MaxDirectoryItemsExceededException e\n          = new MaxDirectoryItemsExceededException(maxDirItems, count);\n      if (namesystem.isImageLoaded()) {\n        e.setPathName(parentPath);\n        throw e;\n      } else {\n        // Log the error and prevent further processing\n        NameNode.LOG.error(\"FSDirectory.verifyMaxDirItems: \" + e.getLocalizedMessage());\n        return; // Prevent further processing\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-13040.json",
        "creation_time": "2018-01-19T19:47:02.000+0000",
        "bug_report": {
            "Title": "Kerberized inotify client fails despite kinit properly",
            "Description": "The inotify client fails to retrieve edits from the NameNode even when it has valid Kerberos credentials. This issue arises when the NameNode has been running longer than the Kerberos ticket lifetime, leading to a failure in reading the edit logs. The error occurs because the inotify client uses its own principal, which prevents the NameNode from re-authenticating on behalf of the client. A proxy user approach is suggested to allow the NameNode to retrieve edits on behalf of the client.",
            "StackTrace": [
                "18/01/19 11:23:02 WARN security.UserGroupInformation: PriviledgedActionException as:hdfs@GCE.CLOUDERA.COM (auth:KERBEROS) cause:org.apache.hadoop.ipc.RemoteException(java.io.IOException): We encountered an error reading https://nn2.example.com:8481/getJournal?jid=ns1&segmentTxId=8662&storageInfo=-60%3A353531113%3A0%3Acluster3, https://nn1.example.com:8481/getJournal?jid=ns1&segmentTxId=8662&storageInfo=-60%3A353531113%3A0%3Acluster3. During automatic edit log failover, we noticed that all of the remaining edit log streams are shorter than the current one! The best remaining edit log ends at transaction 8683, but we thought we could read up to transaction 8684. If you continue, metadata will be lost forever!",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:213)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.readOp(NameNodeRpcServer.java:1701)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEditsFromTxid(NameNodeRpcServer.java:1763)",
                "at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getEditsFromTxid(AuthorizationProviderProxyClientProtocol.java:1011)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getEditsFromTxid(ClientNamenodeProtocolServerSideTranslatorPB.java:1490)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2212)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2210)"
            ],
            "RootCause": "The inotify client uses its own Kerberos principal, which prevents the NameNode from re-authenticating on behalf of the client when the NameNode's Kerberos ticket has expired.",
            "StepsToReproduce": [
                "1. Start the NameNodes with valid Kerberos credentials.",
                "2. Allow the NameNodes to run longer than the Kerberos ticket lifetime.",
                "3. Attempt to use the inotify client to retrieve edits from the NameNodes."
            ],
            "ExpectedBehavior": "The inotify client should successfully retrieve edits from the NameNode even after the Kerberos ticket has expired, using a proxy user mechanism.",
            "ObservedBehavior": "The inotify client fails to retrieve edits, resulting in an error indicating that metadata may be lost due to the inability to read the edit logs.",
            "Suggestions": "Implement a proxy user mechanism that allows the NameNode to retrieve edits on behalf of the inotify client, ensuring that the correct principal is used for authentication.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogInputStream.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.EditLogInputStream",
                    "org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer",
                    "org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream",
                    "org.apache.hadoop.security.UserGroupInformation"
                ],
                "methods": [
                    "EditLogInputStream.readOp",
                    "NameNodeRpcServer.readOp",
                    "RedundantEditLogInputStream.nextOp",
                    "UserGroupInformation.doAs"
                ]
            },
            "possible_fix": "Implement a proxy user mechanism in the inotify client to allow the NameNode to authenticate on behalf of the client. This may involve modifying the inotify client to specify the NameNode's principal when making requests."
        },
        "possible_fix_code": {
            "NameNodeRpcServer.getEditsFromTxid": "  public EventBatchList getEditsFromTxid(long txid) throws IOException {\n    checkNNStartup();\n    namesystem.checkOperation(OperationCategory.READ); // only active\n    namesystem.checkSuperuserPrivilege();\n    int maxEventsPerRPC = nn.getConf().getInt(\n        DFSConfigKeys.DFS_NAMENODE_INOTIFY_MAX_EVENTS_PER_RPC_KEY,\n        DFSConfigKeys.DFS_NAMENODE_INOTIFY_MAX_EVENTS_PER_RPC_DEFAULT);\n    FSEditLog log = namesystem.getFSImage().getEditLog();\n    long syncTxid = log.getSyncTxId();\n    boolean readInProgress = syncTxid > 0;\n\n    List<EventBatch> batches = Lists.newArrayList();\n    int totalEvents = 0;\n    long maxSeenTxid = -1;\n    long firstSeenTxid = -1;\n\n    if (syncTxid > 0 && txid > syncTxid) {\n      return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);\n    }\n\n    Collection<EditLogInputStream> streams = null;\n    try {\n      streams = log.selectInputStreams(txid, 0, null, readInProgress);\n    } catch (IllegalStateException e) {\n      LOG.info(\"NN is transitioning from active to standby and FSEditLog \" +\n      \"is closed -- could not read edits\");\n      return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);\n    }\n\n    boolean breakOuter = false;\n    for (EditLogInputStream elis : streams) {\n      try {\n        FSEditLogOp op = null;\n        while ((op = readOp(elis)) != null) {\n          if (syncTxid > 0 && op.getTransactionId() > syncTxid) {\n            breakOuter = true;\n            break;\n          }\n\n          EventBatch eventBatch = InotifyFSEditLogOpTranslator.translate(op);\n          if (eventBatch != null) {\n            batches.add(eventBatch);\n            totalEvents += eventBatch.getEvents().length;\n          }\n          if (op.getTransactionId() > maxSeenTxid) {\n            maxSeenTxid = op.getTransactionId();\n          }\n          if (firstSeenTxid == -1) {\n            firstSeenTxid = op.getTransactionId();\n          }\n          if (totalEvents >= maxEventsPerRPC || (syncTxid > 0 &&\n              op.getTransactionId() == syncTxid)) {\n            breakOuter = true;\n            break;\n          }\n        }\n      } finally {\n        elis.close();\n      }\n      if (breakOuter) {\n        break;\n      }\n    }\n\n    // Implementing proxy user mechanism\n    UserGroupInformation proxyUgi = UserGroupInformation.createProxyUser(\"proxyUser\", UserGroupInformation.getLoginUser());\n    return proxyUgi.doAs(new PrivilegedExceptionAction<EventBatchList>() {\n      @Override\n      public EventBatchList run() throws Exception {\n        return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);\n      }\n    });\n  }"
        }
    },
    {
        "filename": "HDFS-3374.json",
        "creation_time": "2012-05-04T22:08:19.000+0000",
        "bug_report": {
            "Title": "hdfs' TestDelegationToken fails intermittently with a race condition",
            "Description": "The test case for TestDelegationToken is failing intermittently due to a race condition where the MiniDFSCluster is shut down before the DelegationTokenSecretManager can update the master key. This leads to a situation where the system attempts to call 'system.exit' without any available edit streams, resulting in a fatal error. The issue arises from the timing of the shutdown process and the key update process, which are not properly synchronized.",
            "StackTrace": [
                "2012-05-04 15:03:51,521 WARN  common.Storage (FSImage.java:updateRemovedDirs(224)) - Removing storage dir /home/horton/src/hadoop/build/test/data/dfs/name1",
                "2012-05-04 15:03:51,522 FATAL namenode.FSNamesystem (FSEditLog.java:fatalExit(388)) - No edit streams are accessible",
                "java.lang.Exception: No edit streams are accessible",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.fatalExit(FSEditLog.java:388)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.exitIfNoStreams(FSEditLog.java:407)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsAndStorageDir(FSEditLog.java:432)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsStreamsAndStorageDirs(FSEditLog.java:468)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync(FSEditLog.java:1028)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logUpdateMasterKey(FSNamesystem.java:5641)",
                "at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager.logUpdateMasterKey(DelegationTokenSecretManager.java:286)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.updateCurrentKey(AbstractDelegationTokenSecretManager.java:150)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.rollMasterKey(AbstractDelegationTokenSecretManager.java:174)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:385)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is a race condition between the shutdown of the MiniDFSCluster and the update of the master key in the DelegationTokenSecretManager. The shutdown process occurs before the key update can complete, leading to a situation where no edit streams are available when the system attempts to exit.",
            "StepsToReproduce": [
                "Run the TestDelegationToken test case in a MiniDFSCluster environment.",
                "Observe the timing of the MiniDFSCluster shutdown and the key update process."
            ],
            "ExpectedBehavior": "The test should complete successfully without crashing, and the system should be able to update the master key without encountering a fatal error related to edit streams.",
            "ObservedBehavior": "The test fails intermittently with a fatal error indicating that no edit streams are accessible, leading to a system exit.",
            "Suggestions": "Implement proper synchronization between the shutdown process of the MiniDFSCluster and the key update process in the DelegationTokenSecretManager to prevent the race condition.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenSecretManager.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSEditLog",
                    "org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager",
                    "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager"
                ],
                "methods": [
                    "FSEditLog.fatalExit",
                    "FSEditLog.exitIfNoStreams",
                    "AbstractDelegationTokenSecretManager.updateCurrentKey",
                    "AbstractDelegationTokenSecretManager.rollMasterKey",
                    "DelegationTokenSecretManager.logUpdateMasterKey"
                ]
            },
            "possible_fix": "To fix the race condition, ensure that the shutdown process of the MiniDFSCluster waits for the key update process to complete. This can be achieved by adding synchronization mechanisms or using a countdown latch to coordinate the shutdown and key update operations."
        },
        "possible_fix_code": {
            "AbstractDelegationTokenSecretManager.updateCurrentKey": "  private void updateCurrentKey() throws IOException {\n    LOG.info(\"Updating the current master key for generating delegation tokens\");\n    /* Create a new currentKey with an estimated expiry date. */\n    int newCurrentId;\n    synchronized (this) {\n      newCurrentId = currentId + 1;\n    }\n    DelegationKey newKey = new DelegationKey(newCurrentId, System\n        .currentTimeMillis()\n        + keyUpdateInterval + tokenMaxLifetime, generateSecret());\n    //Log must be invoked outside the lock on 'this'\n    logUpdateMasterKey(newKey);\n    synchronized (this) {\n      currentId = newKey.getKeyId();\n      currentKey = newKey;\n      allKeys.put(currentKey.getKeyId(), currentKey);\n    }\n    // Notify any waiting threads that the key update is complete\n    synchronized (this) {\n      this.notifyAll();\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-2359.json",
        "creation_time": "2011-09-23T00:20:53.000+0000",
        "bug_report": {
            "Title": "NPE found in Datanode log while Disk failed during different HDFS operation",
            "Description": "During a distcp operation in an HDFS cluster with multiple DataNodes, a NullPointerException (NPE) occurs in the Datanode logs when three disks are intentionally failed by changing their permissions to 000. The distcp job completes successfully, but the Datanode logs indicate issues with block deletion and processing commands, leading to the NPE.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$LogFileHandler.appendLine(DataBlockScanner.java:788)",
                "at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.updateScanStatusInternal(DataBlockScanner.java:365)",
                "at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.verifiedByClient(DataBlockScanner.java:308)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:205)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:99)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The NPE is likely caused by the DataBlockScanner attempting to log or update the scan status of a block that is no longer valid or accessible due to the disk failures. The failure to find the BlockInfo in the volumeMap leads to a null reference when the log file handler tries to append a line.",
            "StepsToReproduce": [
                "1. Set up a cluster of 4 DataNodes, each with 12 disks.",
                "2. Configure 'dfs.datanode.failed.volumes.tolerated' to 3 in hdfs-site.xml.",
                "3. Start a distcp operation from one HDFS location to another.",
                "4. While the distcp operation is running, change the permissions of three disks on one DataNode to 000.",
                "5. Check the Datanode logs for NullPointerExceptions."
            ],
            "ExpectedBehavior": "The distcp operation should complete without errors, and the Datanode should handle disk failures gracefully without throwing NullPointerExceptions.",
            "ObservedBehavior": "The distcp operation completes successfully, but the Datanode logs show multiple warnings about block deletion failures and a NullPointerException.",
            "Suggestions": "Implement checks in the DataBlockScanner to ensure that it does not attempt to log or update the status of blocks that are invalid or inaccessible. Additionally, improve error handling in the invalidate method to prevent NPEs when blocks are not found.",
            "problem_location": {
                "files": [
                    "FSDataset.java",
                    "DataXceiver.java",
                    "DataNode.java",
                    "DataBlockScanner.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.FSDataset",
                    "org.apache.hadoop.hdfs.server.datanode.DataXceiver",
                    "org.apache.hadoop.hdfs.server.datanode.DataNode",
                    "org.apache.hadoop.hdfs.server.datanode.DataBlockScanner"
                ],
                "methods": [
                    "FSDataset.invalidate",
                    "DataXceiver.readBlock",
                    "DataNode.processCommand",
                    "DataBlockScanner.updateScanStatusInternal"
                ]
            },
            "possible_fix": "In the DataBlockScanner class, add null checks before accessing BlockInfo objects. For example, in the appendLine method, check if the BlockInfo is null before attempting to log its details. Additionally, enhance the invalidate method in FSDataset to handle cases where blocks are not found more gracefully."
        },
        "possible_fix_code": {
            "FSDataset.invalidate": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error = false;\n    for (int i = 0; i < invalidBlks.length; i++) {\n      File f = null;\n      FSVolume v;\n      synchronized (this) {\n        f = getFile(bpid, invalidBlks[i]);\n        ReplicaInfo dinfo = volumeMap.get(bpid, invalidBlks[i]);\n        if (dinfo == null || \n            dinfo.getGenerationStamp() != invalidBlks[i].getGenerationStamp()) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                           + invalidBlks[i] + \n                           \". BlockInfo not found in volumeMap.\");\n          error = true;\n          continue;\n        }\n        v = dinfo.getVolume();\n        if (f == null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Block not found in blockMap.\" +\n                            ((v == null) ? \" \" : \" Block found in volumeMap.\"));\n          error = true;\n          continue;\n        }\n        if (v == null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". No volume for this block.\" +\n                            \" Block found in blockMap. \" + f + \".\");\n          error = true;\n          continue;\n        }\n        File parent = f.getParentFile();\n        if (parent == null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Parent not found for file \" + f + \".\");\n          error = true;\n          continue;\n        }\n        ReplicaState replicaState = dinfo.getState();\n        if (replicaState == ReplicaState.FINALIZED || \n            (replicaState == ReplicaState.RUR && \n                ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() == \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      File metaFile = getMetaFile(f, invalidBlks[i].getGenerationStamp());\n      long dfsBytes = f.length() + metaFile.length();\n\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f, metaFile, dfsBytes,\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-10986.json",
        "creation_time": "2016-10-08T00:50:36.000+0000",
        "bug_report": {
            "Title": "DFSAdmin should log detailed error message if any",
            "Description": "The DFSAdmin tool currently suppresses IOException errors and provides minimal feedback to the user when commands fail. For instance, when attempting to connect to a datanode that is unreachable, the output is limited to 'Datanode unreachable.' without any stack trace or detailed error message. This lack of information makes it difficult for users to diagnose issues. The expected behavior is for the tool to log detailed error messages, including exception stack traces, to aid in troubleshooting.",
            "StackTrace": [
                "java.net.ConnectException: Connection refused",
                "at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)",
                "at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)",
                "at org.apache.hadoop.hdfs.tools.DFSAdmin.run(DFSAdmin.java:2073)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)",
                "at org.apache.hadoop.hdfs.tools.DFSAdmin.main(DFSAdmin.java:2225)"
            ],
            "RootCause": "The DFSAdmin tool is swallowing exceptions without providing sufficient error messages to the user. This is primarily due to the way exceptions are handled in the run method of the DFSAdmin class, where only minimal error messages are printed to stderr.",
            "StepsToReproduce": [
                "Run the command: hdfs dfsadmin -getBalancerBandwidth 127.0.0.1:9866",
                "Observe the output: 'Datanode unreachable.'",
                "Check the logs for any detailed error messages."
            ],
            "ExpectedBehavior": "The tool should provide detailed error messages, including exception stack traces, when an IOException occurs.",
            "ObservedBehavior": "The tool only outputs 'Datanode unreachable.' without any additional context or stack trace.",
            "Suggestions": "Modify the exception handling in the DFSAdmin.run method to ensure that detailed error messages and stack traces are logged when exceptions occur. This can be achieved by ensuring that exceptions are not swallowed and are logged appropriately.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.tools.DFSAdmin"
                ],
                "methods": [
                    "DFSAdmin.run"
                ]
            },
            "possible_fix": "In the DFSAdmin.run method, ensure that all caught exceptions are logged with their stack traces. For example, modify the catch block for Exception to include a log statement that captures the full exception details, rather than just the localized message."
        },
        "possible_fix_code": {
            "DFSAdmin.run": "  public int run(String[] argv) throws Exception {\n\n    if (argv.length < 1) {\n      printUsage(\"\");\n      return -1;\n    }\n\n    int exitCode = -1;\n    int i = 0;\n    String cmd = argv[i++];\n\n    //\n    // verify that we have enough command line parameters\n    //\n    if (\"-safemode\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-allowSnapshot\".equalsIgnoreCase(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-disallowSnapshot\".equalsIgnoreCase(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-report\".equals(cmd)) {\n      if (argv.length < 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-saveNamespace\".equals(cmd)) {\n      if (argv.length != 1 && argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-rollEdits\".equals(cmd)) {\n      if (argv.length != 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-restoreFailedStorage\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-refreshNodes\".equals(cmd)) {\n      if (argv.length != 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-finalizeUpgrade\".equals(cmd)) {\n      if (argv.length != 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (RollingUpgradeCommand.matches(cmd)) {\n      if (argv.length < 1 || argv.length > 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-metasave\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-refreshServiceAcl\".equals(cmd)) {\n      if (argv.length != 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-refresh\".equals(cmd)) {\n      if (argv.length < 3) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-refreshUserToGroupsMappings\".equals(cmd)) {\n      if (argv.length != 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-printTopology\".equals(cmd)) {\n      if(argv.length != 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-refreshNamenodes\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-reconfig\".equals(cmd)) {\n      if (argv.length != 4) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-deleteBlockPool\".equals(cmd)) {\n      if ((argv.length != 3) && (argv.length != 4)) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-setBalancerBandwidth\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-getBalancerBandwidth\".equalsIgnoreCase(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-fetchImage\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-shutdownDatanode\".equals(cmd)) {\n      if ((argv.length != 2) && (argv.length != 3)) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-getDatanodeInfo\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-triggerBlockReport\".equals(cmd)) {\n      if (argv.length < 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    }\n    \n    // initialize DFSAdmin\n    try {\n      init();\n    } catch (RPC.VersionMismatch v) {\n      System.err.println(\"Version Mismatch between client and server\" +\n                         \"... command aborted.\");\n      return exitCode;\n    } catch (IOException e) {\n      System.err.println(\"Bad connection to DFS... command aborted.\");\n      return exitCode;\n    }\n\n    Exception debugException = null;\n    exitCode = 0;\n    try {\n      if (\"-report\".equals(cmd)) {\n        report(argv, i);\n      } else if (\"-safemode\".equals(cmd)) {\n        setSafeMode(argv, i);\n      } else if (\"-allowSnapshot\".equalsIgnoreCase(cmd)) {\n        allowSnapshot(argv);\n      } else if (\"-disallowSnapshot\".equalsIgnoreCase(cmd)) {\n        disallowSnapshot(argv);\n      } else if (\"-saveNamespace\".equals(cmd)) {\n        exitCode = saveNamespace(argv);\n      } else if (\"-rollEdits\".equals(cmd)) {\n        exitCode = rollEdits();\n      } else if (\"-restoreFailedStorage\".equals(cmd)) {\n        exitCode = restoreFailedStorage(argv[i]);\n      } else if (\"-refreshNodes\".equals(cmd)) {\n        exitCode = refreshNodes();\n      } else if (\"-finalizeUpgrade\".equals(cmd)) {\n        exitCode = finalizeUpgrade();\n      } else if (RollingUpgradeCommand.matches(cmd)) {\n        exitCode = RollingUpgradeCommand.run(getDFS(), argv, i);\n      } else if (\"-metasave\".equals(cmd)) {\n        exitCode = metaSave(argv, i);\n      } else if (ClearQuotaCommand.matches(cmd)) {\n        exitCode = new ClearQuotaCommand(argv, i, getDFS()).runAll();\n      } else if (SetQuotaCommand.matches(cmd)) {\n        exitCode = new SetQuotaCommand(argv, i, getDFS()).runAll();\n      } else if (ClearSpaceQuotaCommand.matches(cmd)) {\n        exitCode = new ClearSpaceQuotaCommand(argv, i, getDFS()).runAll();\n      } else if (SetSpaceQuotaCommand.matches(cmd)) {\n        exitCode = new SetSpaceQuotaCommand(argv, i, getDFS()).runAll();\n      } else if (\"-refreshServiceAcl\".equals(cmd)) {\n        exitCode = refreshServiceAcl();\n      } else if (\"-refreshUserToGroupsMappings\".equals(cmd)) {\n        exitCode = refreshUserToGroupsMappings();\n      } else if (\"-refreshSuperUserGroupsConfiguration\".equals(cmd)) {\n        exitCode = refreshSuperUserGroupsConfiguration();\n      } else if (\"-refreshCallQueue\".equals(cmd)) {\n        exitCode = refreshCallQueue();\n      } else if (\"-refresh\".equals(cmd)) {\n        exitCode = genericRefresh(argv, i);\n      } else if (\"-printTopology\".equals(cmd)) {\n        exitCode = printTopology();\n      } else if (\"-refreshNamenodes\".equals(cmd)) {\n        exitCode = refreshNamenodes(argv, i);\n      } else if (\"-deleteBlockPool\".equals(cmd)) {\n        exitCode = deleteBlockPool(argv, i);\n      } else if (\"-setBalancerBandwidth\".equals(cmd)) {\n        exitCode = setBalancerBandwidth(argv, i);\n      } else if (\"-getBalancerBandwidth\".equals(cmd)) {\n        exitCode = getBalancerBandwidth(argv, i);\n      } else if (\"-fetchImage\".equals(cmd)) {\n        exitCode = fetchImage(argv, i);\n      } else if (\"-shutdownDatanode\".equals(cmd)) {\n        exitCode = shutdownDatanode(argv, i);\n      } else if (\"-evictWriters\".equals(cmd)) {\n        exitCode = evictWriters(argv, i);\n      } else if (\"-getDatanodeInfo\".equals(cmd)) {\n        exitCode = getDatanodeInfo(argv, i);\n      } else if (\"-reconfig\".equals(cmd)) {\n        exitCode = reconfig(argv, i);\n      } else if (\"-triggerBlockReport\".equals(cmd)) {\n        exitCode = triggerBlockReport(argv);\n      } else if (\"-help\".equals(cmd)) {\n        if (i < argv.length) {\n          printHelp(argv[i]);\n        } else {\n          printHelp(\"\");\n        }\n      } else {\n        exitCode = -1;\n        System.err.println(cmd.substring(1) + \": Unknown command\");\n        printUsage(\"\");\n      }\n    } catch (IllegalArgumentException arge) {\n      debugException = arge;\n      exitCode = -1;\n      System.err.println(cmd.substring(1) + \": \" + arge.getLocalizedMessage());\n      printUsage(cmd);\n    } catch (RemoteException e) {\n      //\n      // This is a error returned by hadoop server. Print\n      // out the first line of the error message, ignore the stack trace.\n      exitCode = -1;\n      debugException = e;\n      try {\n        String[] content;\n        content = e.getLocalizedMessage().split(\"\\n\");\n        System.err.println(cmd.substring(1) + \": \" + content[0]);\n      } catch (Exception ex) {\n        System.err.println(cmd.substring(1) + \": \" + ex.getLocalizedMessage());\n        debugException = ex;\n      }\n    } catch (Exception e) {\n      exitCode = -1;\n      debugException = e;\n      System.err.println(cmd.substring(1) + \": \" + e.getLocalizedMessage());\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Exception encountered:\", debugException);\n    }\n    return exitCode;\n  }"
        }
    },
    {
        "filename": "HDFS-6455.json",
        "creation_time": "2014-05-28T18:01:15.000+0000",
        "bug_report": {
            "Title": "NFS: Exception should be added in NFS log for invalid separator in nfs.exports.allowed.hosts",
            "Description": "The NFS server fails to start when an invalid separator is used in the 'dfs.nfs.exports.allowed.hosts' property. Instead of logging the error in the NFS log file, the exception is only printed to the console and logged in the nfs.out file. This behavior can lead to confusion as the NFS log does not provide any indication of the failure, making it difficult for users to diagnose the issue.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.IllegalArgumentException: Incorrectly formatted line 'host1 ro:host2 rw'",
                "at org.apache.hadoop.nfs.NfsExports.getMatch(NfsExports.java:356)",
                "at org.apache.hadoop.nfs.NfsExports.<init>(NfsExports.java:151)",
                "at org.apache.hadoop.nfs.NfsExports.getInstance(NfsExports.java:54)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.<init>(RpcProgramNfs3.java:176)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.<init>(Nfs3.java:43)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.main(Nfs3.java:59)"
            ],
            "RootCause": "The root cause of the issue is that the method 'getMatch' in the NfsExports class throws an IllegalArgumentException when it encounters an incorrectly formatted line. The current implementation does not log this exception in the NFS log, leading to a lack of visibility into the error.",
            "StepsToReproduce": [
                "1. Set the property 'dfs.nfs.exports.allowed.hosts' with an invalid separator, e.g., '<property><name>dfs.nfs.exports.allowed.hosts</name><value>host1  ro:host2 rw</value></property>'.",
                "2. Restart the NFS server."
            ],
            "ExpectedBehavior": "The NFS server should log an error message in the NFS log file when an invalid separator is used in the 'dfs.nfs.exports.allowed.hosts' property.",
            "ObservedBehavior": "The NFS server fails to start and prints an exception to the console, but does not log any error message in the NFS log file.",
            "Suggestions": "Modify the NfsExports class to log the IllegalArgumentException in the NFS log file when an invalid format is detected in the 'dfs.nfs.exports.allowed.hosts' property.",
            "problem_location": {
                "files": [
                    "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.nfs.NfsExports.java",
                    "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.java"
                ],
                "classes": [
                    "org.apache.hadoop.nfs.NfsExports",
                    "org.apache.hadoop.hdfs.nfs.nfs3.Nfs3"
                ],
                "methods": [
                    "NfsExports.getMatch",
                    "Nfs3.main"
                ]
            },
            "possible_fix": "In the 'getMatch' method of the NfsExports class, add a logging statement to log the IllegalArgumentException before it is thrown. For example:\n\n```java\nprivate static Match getMatch(String line) {\n    // existing code...\n    default:\n        LOG.error(\"Invalid format for line: '\" + line + \"'\" );\n        throw new IllegalArgumentException(\"Incorrectly formatted line '\" + line + \"'\");\n}\n```"
        },
        "possible_fix_code": {
            "NfsExports.getMatch": "  private static Match getMatch(String line) {\n    String[] parts = line.split(\"\\\\s+\");\n    final String host;\n    AccessPrivilege privilege = AccessPrivilege.READ_ONLY;\n    switch (parts.length) {\n    case 1:\n      host = parts[0].toLowerCase().trim();\n      break;\n    case 2:\n      host = parts[0].toLowerCase().trim();\n      String option = parts[1].trim();\n      if (\"rw\".equalsIgnoreCase(option)) {\n        privilege = AccessPrivilege.READ_WRITE;\n      }\n      break;\n    default:\n      LOG.error(\"Invalid format for line: '\" + line + \"'\");\n      throw new IllegalArgumentException(\"Incorrectly formatted line '\" + line + \"'\");\n    }\n    if (host.equals(\"*\")) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using match all for '\" + host + \"' and \" + privilege);\n      }\n      return new AnonymousMatch(privilege);\n    } else if (CIDR_FORMAT_SHORT.matcher(host).matches()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using CIDR match for '\" + host + \"' and \" + privilege);\n      }\n      return new CIDRMatch(privilege, new SubnetUtils(host).getInfo());\n    } else if (CIDR_FORMAT_LONG.matcher(host).matches()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using CIDR match for '\" + host + \"' and \" + privilege);\n      }\n      String[] pair = host.split(\"/\");\n      return new CIDRMatch(privilege,\n          new SubnetUtils(pair[0], pair[1]).getInfo());\n    } else if (host.contains(\"*\") || host.contains(\"?\") || host.contains(\"[\")\n        || host.contains(\"]\")) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using Regex match for '\" + host + \"' and \" + privilege);\n      }\n      return new RegexMatch(privilege, host);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Using exact match for '\" + host + \"' and \" + privilege);\n    }\n    return new ExactMatch(privilege, host);\n  }"
        }
    },
    {
        "filename": "HDFS-2882.json",
        "creation_time": "2012-02-02T18:11:20.000+0000",
        "bug_report": {
            "Title": "DN continues to start up, even if block pool fails to initialize",
            "Description": "The DataNode (DN) is able to start up even when the block pool fails to initialize due to insufficient disk space. This leads to a situation where the DN continues to operate, resulting in NullPointerExceptions (NPEs) during block reporting. The initialization failure is logged, but the DN does not halt its startup process, which is contrary to expected behavior.",
            "StackTrace": [
                "2012-02-02 09:56:50,499 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool BP-448349972-172.29.5.192-1323816762969 (storage id DS-507718931-172.29.5.194-11072-1297842002148)",
                "java.io.IOException: Mkdirs failed to create /data/1/scratch/todd/styx-datadir/current/BP-448349972-172.29.5.192-1323816762969/tmp",
                "at org.apache.hadoop.hdfs.server.datanode.FSDataset$BlockPoolSlice.<init>(FSDataset.java:335)"
            ],
            "RootCause": "The DataNode does not properly handle the failure of block pool initialization, allowing it to continue running despite critical errors during startup.",
            "StepsToReproduce": [
                "1. Start a DataNode on a machine with insufficient disk space.",
                "2. Observe the logs for initialization failure messages.",
                "3. Note that the DataNode continues to run and generates NPEs during block reporting."
            ],
            "ExpectedBehavior": "The DataNode should halt its startup process if the block pool fails to initialize due to insufficient disk space.",
            "ObservedBehavior": "The DataNode continues to run and generates NullPointerExceptions when attempting to perform block reports.",
            "Suggestions": "Implement a check to halt the DataNode startup process if the block pool initialization fails. Ensure that the DataNode does not proceed to perform operations that depend on a successfully initialized block pool.",
            "problem_location": {
                "files": [
                    "FSDataset.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.FSDataset"
                ],
                "methods": [
                    "FSDataset$BlockPoolSlice.<init>"
                ]
            },
            "possible_fix": "In the FSDataset class, modify the constructor of BlockPoolSlice to throw a runtime exception or call a shutdown method if the initialization fails, preventing the DataNode from continuing its startup process."
        },
        "possible_fix_code": {
            "FSDataset$BlockPoolSlice.<init>": "public BlockPoolSlice(FSDataset dataset, String bpID) throws IOException {\n    // Attempt to initialize the block pool slice\n    if (!initializeBlockPoolSlice(bpID)) {\n        // Log the failure\n        LOG.fatal(\"Initialization failed for block pool \" + bpID);\n        // Throw a runtime exception to halt the DataNode startup process\n        throw new RuntimeException(\"Block pool initialization failed for \" + bpID);\n    }\n    // Continue with the rest of the initialization if successful\n    this.dataset = dataset;\n    this.bpID = bpID;\n    // Additional initialization code...\n}"
        }
    },
    {
        "filename": "HDFS-5185.json",
        "creation_time": "2013-09-11T14:38:23.000+0000",
        "bug_report": {
            "Title": "DataNode Fails to Start Up When One Data Directory is Full",
            "Description": "The DataNode fails to start up if any of the configured data directories are out of space. This results in an IOException when attempting to create necessary directories for the block pool, leading to a fatal error during initialization. The system should be able to continue starting up using the available data directories instead of failing completely.",
            "StackTrace": [
                "2013-09-11 17:48:43,680 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool <registering> (storage id DS-308316523-xx.xx.xx.xx-64015-1378896293604) service to /nn1:65110",
                "java.io.IOException: Mkdirs failed to create /opt/nish/data/current/BP-123456-1234567/tmp",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.<init>(BlockPoolSlice.java:105)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.addBlockPool(FsVolumeImpl.java:216)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addBlockPool(FsVolumeList.java:155)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addBlockPool(FsDatasetImpl.java:1593)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:834)",
                "at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:311)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:217)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:660)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is that the DataNode does not handle the scenario where one of the data directories is full, leading to an IOException when it attempts to create a directory for the block pool.",
            "StepsToReproduce": [
                "Configure a DataNode with multiple data directories.",
                "Fill one of the data directories to its capacity.",
                "Attempt to start the DataNode."
            ],
            "ExpectedBehavior": "The DataNode should start up successfully using the available data directories, even if one or more directories are full.",
            "ObservedBehavior": "The DataNode fails to start up completely, throwing an IOException and logging a fatal error.",
            "Suggestions": "Implement a check during the DataNode startup process to verify the available space in each data directory. If one directory is full, the DataNode should still initialize using the other available directories.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl",
                    "org.apache.hadoop.hdfs.server.datanode.DataNode",
                    "org.apache.hadoop.hdfs.server.datanode.BPOfferService"
                ],
                "methods": [
                    "FsVolumeImpl.addBlockPool",
                    "DataNode.initBlockPool",
                    "BPOfferService.verifyAndSetNamespaceInfo"
                ]
            },
            "possible_fix": "Modify the 'addBlockPool' method in FsVolumeImpl to check for available space before attempting to create directories. If a directory is full, log a warning and skip that directory while allowing the DataNode to continue initialization with the remaining directories."
        },
        "possible_fix_code": {
            "FsVolumeImpl.addBlockPool": "  void addBlockPool(String bpid, Configuration conf) throws IOException {\n    File bpdir = new File(currentDir, bpid);\n    // Check available space before creating the block pool directory\n    if (!isSpaceAvailable(bpdir)) {\n        FsDatasetImpl.LOG.warn(\"Insufficient space in volume \" + this + \". Skipping block pool creation for \" + bpid);\n        return; // Skip this volume if it's full\n    }\n    BlockPoolSlice bp = new BlockPoolSlice(bpid, this, bpdir, conf);\n    bpSlices.put(bpid, bp);\n  }\n\n  private boolean isSpaceAvailable(File dir) {\n    // Check if the directory exists and has available space\n    if (dir.exists()) {\n        long usableSpace = dir.getUsableSpace();\n        return usableSpace > 0; // Ensure there's usable space\n    }\n    return true; // If the directory doesn't exist, we can create it\n  }"
        }
    },
    {
        "filename": "HDFS-13164.json",
        "creation_time": "2018-02-17T00:40:21.000+0000",
        "bug_report": {
            "Title": "File not closed if streamer fails with DSQuotaExceededException",
            "Description": "When a file is created in HDFS and the disk space quota is exceeded, the DataStreamer encounters a DSQuotaExceededException during the process of writing to the stream. This exception prevents the proper closure of the DFSOutputStream, leading to a situation where the file remains in an open-for-write state. The close operation becomes a no-op if the streamer has already encountered the exception, which can result in resource leaks such as unclosed streams and lease renewers.",
            "StackTrace": [
                "org.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /DIR is exceeded: quota = 2000000 B = 1.91 MB but diskspace consumed = 404139552 B = 385.42 MB",
                "at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyDiskspaceQuota(DirectoryWithQuotaFeature.java:149)",
                "at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:159)",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:2124)",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1991)",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1966)",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:463)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3896)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3484)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:686)"
            ],
            "RootCause": "The root cause of the issue is that the close operation on the DFSOutputStream does not handle the case where the DataStreamer has already encountered a DSQuotaExceededException. This results in the stream being left open and potentially leaking resources.",
            "StepsToReproduce": [
                "Set a disk space quota on a directory in HDFS.",
                "Attempt to write a file that exceeds the quota.",
                "Observe the behavior of the DFSOutputStream during the close operation."
            ],
            "ExpectedBehavior": "The DFSOutputStream should properly close and release all resources, even if a DSQuotaExceededException occurs during writing.",
            "ObservedBehavior": "The DFSOutputStream remains open, leading to a 0-sized file in HDFS and potential resource leaks.",
            "Suggestions": "Implement a mechanism to ensure that the close operation can handle exceptions thrown by the DataStreamer, ensuring that resources are released properly.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSDirectory",
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem",
                    "org.apache.hadoop.hdfs.DFSOutputStream"
                ],
                "methods": [
                    "FSDirectory.updateCount",
                    "FSNamesystem.getAdditionalBlock",
                    "DFSOutputStream.closeImpl"
                ]
            },
            "possible_fix": "Modify the closeImpl method in DFSOutputStream to ensure that it can handle the case where the streamer has encountered a DSQuotaExceededException. This may involve checking the state of the streamer and ensuring that resources are released appropriately, even if the close operation is a no-op."
        },
        "possible_fix_code": {
            "DFSOutputStream.closeImpl": "  void closeImpl() throws IOException {\n    if (streamer != null) {\n      try {\n        streamer.close();\n      } catch (DSQuotaExceededException e) {\n        // Log the exception and ensure resources are released\n        LOG.warn(\"DataStreamer encountered a quota exceeded exception during close: \", e);\n      } finally {\n        // Ensure that we always release resources\n        releaseResources();\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-11508.json",
        "creation_time": "2017-03-07T04:55:50.000+0000",
        "bug_report": {
            "Title": "Fix bind failure in SimpleTCPServer & Portmap due to TIME_WAIT state",
            "Description": "The SimpleTCPServer and Portmap are failing to bind to the specified port because the socket is in the TIME_WAIT state. This is causing a BindException, indicating that the address is already in use. To resolve this issue, the socket options should be modified to include the setReuseAddress option, which allows the socket to bind to an address that is in the TIME_WAIT state.",
            "StackTrace": [
                "org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:4242",
                "at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)",
                "at org.apache.hadoop.oncrpc.SimpleTcpServer.run(SimpleTcpServer.java:86)",
                "at org.apache.hadoop.mount.MountdBase.startTCPServer(MountdBase.java:83)",
                "at org.apache.hadoop.mount.MountdBase.start(MountdBase.java:98)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:71)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)",
                "Caused by: java.net.BindException: Address already in use",
                "at sun.nio.ch.Net.bind0(Native Method)",
                "at sun.nio.ch.Net.bind(Net.java:433)",
                "at sun.nio.ch.Net.bind(Net.java:425)",
                "at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)",
                "at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)",
                "at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)",
                "at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)",
                "at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)",
                "at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the TCP socket is in the TIME_WAIT state, which prevents it from being reused for binding to the same port. This results in a BindException when attempting to start the TCP server.",
            "StepsToReproduce": [
                "Attempt to start the SimpleTCPServer on a port that is already in use or has recently been used.",
                "Observe the logs for the BindException indicating that the address is already in use."
            ],
            "ExpectedBehavior": "The SimpleTCPServer should successfully bind to the specified port and start listening for incoming TCP requests.",
            "ObservedBehavior": "The SimpleTCPServer fails to bind to the specified port, resulting in a fatal error and the application exiting.",
            "Suggestions": "Modify the socket options in the SimpleTcpServer to include the setReuseAddress option, allowing the server to bind to the port even if it is in the TIME_WAIT state.",
            "problem_location": {
                "files": [
                    "SimpleTcpServer.java",
                    "MountdBase.java",
                    "Nfs3.java"
                ],
                "classes": [
                    "org.apache.hadoop.oncrpc.SimpleTcpServer",
                    "org.apache.hadoop.mount.MountdBase",
                    "org.apache.hadoop.hdfs.nfs.nfs3.Nfs3"
                ],
                "methods": [
                    "SimpleTcpServer.run",
                    "MountdBase.startTCPServer",
                    "Nfs3.startServiceInternal"
                ]
            },
            "possible_fix": "In the SimpleTcpServer class, modify the run method to include the setReuseAddress option when configuring the server socket. For example:\n\nserver.setOption('child.reuseAddress', true);"
        },
        "possible_fix_code": {
            "SimpleTcpServer.run": "  public void run() {\n    // Configure the Server.\n    ChannelFactory factory;\n    if (workerCount == 0) {\n      // Use default workers: 2 * the number of available processors\n      factory = new NioServerSocketChannelFactory(\n          Executors.newCachedThreadPool(), Executors.newCachedThreadPool());\n    } else {\n      factory = new NioServerSocketChannelFactory(\n          Executors.newCachedThreadPool(), Executors.newCachedThreadPool(),\n          workerCount);\n    }\n\n    server = new ServerBootstrap(factory);\n    server.setPipelineFactory(new ChannelPipelineFactory() {\n\n      @Override\n      public ChannelPipeline getPipeline() throws Exception {\n        return Channels.pipeline(RpcUtil.constructRpcFrameDecoder(),\n            RpcUtil.STAGE_RPC_MESSAGE_PARSER, rpcProgram,\n            RpcUtil.STAGE_RPC_TCP_RESPONSE);\n      }\n    });\n    server.setOption(\"child.tcpNoDelay\", true);\n    server.setOption(\"child.keepAlive\", true);\n    // Set the reuse address option to allow binding in TIME_WAIT state\n    server.setOption(\"child.reuseAddress\", true);\n\n    // Listen to TCP port\n    ch = server.bind(new InetSocketAddress(port));\n    InetSocketAddress socketAddr = (InetSocketAddress) ch.getLocalAddress();\n    boundPort = socketAddr.getPort();\n\n    LOG.info(\"Started listening to TCP requests at port \" + boundPort + \" for \"\n        + rpcProgram + \" with workerCount \" + workerCount);\n  }"
        }
    },
    {
        "filename": "HDFS-2991.json",
        "creation_time": "2012-02-23T02:06:51.000+0000",
        "bug_report": {
            "Title": "Failure to Load Edits: ClassCastException",
            "Description": "During scale testing of the trunk at revision r1291606, an IOException occurred while attempting to replay the edit log. The root of the issue is a ClassCastException that arises when the system attempts to cast an instance of INodeFile to INodeFileUnderConstruction, which is not valid. This indicates a potential inconsistency in the edit log entries being processed, possibly due to a corrupted or improperly formatted edit log.",
            "StackTrace": [
                "java.io.IOException: Error replaying edit log at offset 1354251",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:418)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:93)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:79)",
                "Caused by: java.lang.ClassCastException: org.apache.hadoop.hdfs.server.namenode.INodeFile cannot be cast to org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:213)"
            ],
            "RootCause": "The ClassCastException occurs because the code attempts to cast an INodeFile object to an INodeFileUnderConstruction object, which is not valid. This suggests that the edit log may contain entries that do not match the expected types.",
            "StepsToReproduce": [
                "Perform scale testing on the trunk at revision r1291606.",
                "Attempt to load the edit log that contains the problematic entries."
            ],
            "ExpectedBehavior": "The system should successfully load the edit log without throwing a ClassCastException, processing all entries correctly.",
            "ObservedBehavior": "The system throws a ClassCastException when trying to cast an INodeFile to an INodeFileUnderConstruction, leading to a failure in loading the edit log.",
            "Suggestions": "Investigate the edit log entries for inconsistencies or corruption. Implement type checks before casting to ensure that the object being cast is of the expected type. Consider adding error handling to manage unexpected types gracefully.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader"
                ],
                "methods": [
                    "FSEditLogLoader.loadEditRecords",
                    "FSEditLogLoader.loadFSEdits"
                ]
            },
            "possible_fix": "In the loadEditRecords method, before casting to INodeFileUnderConstruction, check the instance type of the object. If it is not of the expected type, log an error and skip processing that entry to prevent the ClassCastException."
        },
        "possible_fix_code": {
            "FSEditLogLoader.loadEditRecords": "  long loadEditRecords(int logVersion, EditLogInputStream in, boolean closeOnExit,\n                      long expectedStartingTxId)\n      throws IOException, EditLogInputException {\n    FSDirectory fsDir = fsNamesys.dir;\n    long numEdits = 0;\n\n    EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts =\n      new EnumMap<FSEditLogOpCodes, Holder<Integer>>(FSEditLogOpCodes.class);\n\n    fsNamesys.writeLock();\n    fsDir.writeLock();\n\n    long recentOpcodeOffsets[] = new long[4];\n    Arrays.fill(recentOpcodeOffsets, -1);\n\n    try {\n      long txId = expectedStartingTxId - 1;\n\n      try {\n        while (true) {\n          FSEditLogOp op;\n          try {\n            if ((op = in.readOp()) == null) {\n              break;\n            }\n          } catch (IOException ioe) {\n            String errorMessage = formatEditLogReplayError(in, recentOpcodeOffsets);\n            FSImage.LOG.error(errorMessage);\n            throw new EditLogInputException(errorMessage,\n                ioe, numEdits);\n          }\n          recentOpcodeOffsets[(int)(numEdits % recentOpcodeOffsets.length)] =\n            in.getPosition();\n          if (LayoutVersion.supports(Feature.STORED_TXIDS, logVersion)) {\n            long thisTxId = op.txid;\n            if (thisTxId != txId + 1) {\n              throw new IOException(\"Expected transaction ID \" +\n                  (txId + 1) + \" but got \" + thisTxId);\n            }\n            txId = thisTxId;\n          }\n\n          incrOpCount(op.opCode, opCounts);\n          try {\n            // Check the type of the object before casting\n            if (op instanceof INodeFile) {\n              applyEditLogOp(op, fsDir, logVersion);\n            } else {\n              FSImage.LOG.error(\"Unexpected operation type: \" + op.getClass().getName());\n              continue; // Skip processing this entry\n            }\n          } catch (Throwable t) {\n            String errorMessage = formatEditLogReplayError(in, recentOpcodeOffsets);\n            FSImage.LOG.error(errorMessage);\n            throw new IOException(errorMessage, t);\n          }\n          numEdits++;\n        }\n      } catch (IOException ex) {\n        check203UpgradeFailure(logVersion, ex);\n      } finally {\n        if(closeOnExit)\n          in.close();\n      }\n    } finally {\n      fsDir.writeUnlock();\n      fsNamesys.writeUnlock();\n      if (FSImage.LOG.isDebugEnabled()) {\n        dumpOpCounts(opCounts);\n      }\n    }\n    return numEdits;\n  }"
        }
    },
    {
        "filename": "HDFS-4404.json",
        "creation_time": "2013-01-15T01:43:17.000+0000",
        "bug_report": {
            "Title": "Create file failure when the machine of first attempted NameNode is down",
            "Description": "In a distributed Hadoop environment, when the primary NameNode (NN1) is down, attempts to create a file result in a SocketTimeoutException. The client fails to connect to the NameNode, leading to a timeout after 10 seconds. This issue arises because the client does not properly handle the unavailability of the primary NameNode and does not failover to the secondary NameNode (NN2).",
            "StackTrace": [
                "java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]",
                "at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)",
                "at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:524)",
                "at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:474)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:568)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1286)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1156)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:187)",
                "at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1128)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:232)",
                "at test.TestLease.main(TestLease.java:45)"
            ],
            "RootCause": "The primary NameNode (NN1) is down, and the client does not automatically failover to the secondary NameNode (NN2) when attempting to create a file. The connection attempt to the downed NameNode results in a SocketTimeoutException.",
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with two NameNodes (NN1 and NN2) and several DataNodes.",
                "2. Bring down the primary NameNode (NN1).",
                "3. Attempt to create a file in HDFS using the Hadoop client.",
                "4. Observe the SocketTimeoutException in the logs."
            ],
            "ExpectedBehavior": "The client should automatically failover to the secondary NameNode (NN2) and successfully create the file in HDFS.",
            "ObservedBehavior": "The client fails to create the file and throws a SocketTimeoutException due to the unavailability of the primary NameNode.",
            "Suggestions": "Implement automatic failover logic in the client to switch to the secondary NameNode when the primary is down. Ensure that the client can handle exceptions gracefully and retry the operation on the available NameNode.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetUtils.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java"
                ],
                "classes": [
                    "org.apache.hadoop.net.NetUtils",
                    "org.apache.hadoop.hdfs.DFSClient",
                    "org.apache.hadoop.hdfs.DistributedFileSystem"
                ],
                "methods": [
                    "NetUtils.connect",
                    "DFSClient.create",
                    "DistributedFileSystem.create"
                ]
            },
            "possible_fix": "Modify the client logic to include a failover mechanism. For example, in the 'DFSClient.create' method, check if the primary NameNode is reachable before attempting to connect. If not, switch to the secondary NameNode and retry the connection."
        },
        "possible_fix_code": {
            "DFSClient.create": "  public DFSOutputStream create(String src, \n                             FsPermission permission,\n                             EnumSet<CreateFlag> flag, \n                             boolean createParent,\n                             short replication,\n                             long blockSize,\n                             Progressable progress,\n                             int buffersize,\n                             ChecksumOpt checksumOpt) throws IOException {\n    checkOpen();\n    if (permission == null) {\n      permission = FsPermission.getFileDefault();\n    }\n    FsPermission masked = permission.applyUMask(dfsClientConf.uMask);\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(src + \": masked=\" + masked);\n    }\n    // Attempt to connect to the primary NameNode\n    try {\n      final DFSOutputStream result = DFSOutputStream.newStreamForCreate(this,\n          src, masked, flag, createParent, replication, blockSize, progress,\n          buffersize, dfsClientConf.createChecksum(checksumOpt));\n      beginFileLease(src, result);\n      return result;\n    } catch (SocketTimeoutException e) {\n      // If primary NameNode is down, attempt to failover to secondary NameNode\n      LOG.warn(\"Primary NameNode is down, attempting failover to secondary NameNode.\");\n      // Logic to switch to secondary NameNode goes here\n      // For example, you might want to set a different remoteId for the secondary NameNode\n      // and retry the create operation.\n      // This is a placeholder for the actual failover logic.\n      throw new IOException(\"Failed to create file after failover attempt\", e);\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-8276.json",
        "creation_time": "2015-04-28T10:11:35.000+0000",
        "bug_report": {
            "Title": "LazyPersistFileScrubber should be disabled if scrubber interval configured zero",
            "Description": "The current implementation of the FSNamesystem constructor enforces a non-zero value for the configuration parameter 'dfs.namenode.lazypersist.file.scrub.interval.sec'. If this parameter is set to zero, it results in an IllegalArgumentException, causing the namenode startup to fail. The expected behavior is that a zero value should disable the scrubber instead of throwing an exception.",
            "StackTrace": [
                "2015-04-27 23:47:31,744 ERROR org.apache.hadoop.hdfs.server.namenode.FSNamesystem: FSNamesystem initialization failed.",
                "java.lang.IllegalArgumentException: dfs.namenode.lazypersist.file.scrub.interval.sec must be non-zero.",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:828)"
            ],
            "RootCause": "The FSNamesystem constructor does not handle a zero value for the scrub interval correctly, leading to an exception instead of disabling the scrubber.",
            "StepsToReproduce": [
                "Set the configuration parameter 'dfs.namenode.lazypersist.file.scrub.interval.sec' to zero.",
                "Start the namenode.",
                "Observe the startup failure with an IllegalArgumentException."
            ],
            "ExpectedBehavior": "The namenode should start successfully and the LazyPersistFileScrubber should be disabled when the scrub interval is set to zero.",
            "ObservedBehavior": "The namenode fails to start and throws an IllegalArgumentException indicating that the scrub interval must be non-zero.",
            "Suggestions": "Modify the FSNamesystem constructor to check for a zero value of 'dfs.namenode.lazypersist.file.scrub.interval.sec' and disable the scrubber instead of throwing an exception.",
            "problem_location": {
                "files": [
                    "FSNamesystem.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem"
                ],
                "methods": [
                    "FSNamesystem.<init>"
                ]
            },
            "possible_fix": "In the FSNamesystem constructor, add a check for the scrub interval value. If it is zero, set a flag to disable the scrubber instead of throwing an exception. Example code change:\n\nif (scrubIntervalSec <= 0) {\n    this.scrubberEnabled = false;\n} else {\n    this.scrubberEnabled = true;\n}"
        },
        "possible_fix_code": {
            "FSNamesystem.<init>": "public FSNamesystem(Configuration conf) throws IOException {\n    // Other initialization code...\n    long scrubIntervalSec = conf.getLong(\"dfs.namenode.lazypersist.file.scrub.interval.sec\", 0);\n    if (scrubIntervalSec <= 0) {\n        this.scrubberEnabled = false;\n    } else {\n        this.scrubberEnabled = true;\n    }\n    // Continue with the rest of the constructor...\n}"
        }
    },
    {
        "filename": "HDFS-12369.json",
        "creation_time": "2017-08-28T22:56:02.000+0000",
        "bug_report": {
            "Title": "Edit log corruption due to hard lease recovery of not-closed file which has snapshots",
            "Description": "The NameNode fails to start due to a FileNotFoundException caused by a missing file that was not properly closed before being deleted. The issue arises from a sequence of operations where a file was opened, blocks were added, and then deleted without a proper close operation being logged. This leads to inconsistencies in the edit logs, particularly when the LeaseManager attempts to recover the lease of a file that was not closed correctly, resulting in the NameNode being unable to load the necessary file metadata.",
            "StackTrace": [
                "2017-08-17 14:32:18,418 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.",
                "java.io.FileNotFoundException: File does not exist: /home/Events/CancellationSurvey_MySQL/2015/12/31/.part-00000.9nlJ3M",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:66)",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:429)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:232)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:141)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:897)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:750)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:318)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1125)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:789)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:614)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:676)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:844)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:823)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1547)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1615)"
            ],
            "RootCause": "The root cause of the issue is that the file was deleted without being properly closed, leading to a missing file entry in the edit logs. The LeaseManager's recovery process attempted to close the file after it was already deleted, resulting in a FileNotFoundException.",
            "StepsToReproduce": [
                "1. Open a file in HDFS.",
                "2. Add blocks to the file.",
                "3. Delete the file without closing it first.",
                "4. Wait for the LeaseManager to trigger a hard lease recovery.",
                "5. Attempt to start the NameNode."
            ],
            "ExpectedBehavior": "The NameNode should start successfully, loading all necessary metadata and edit logs without errors.",
            "ObservedBehavior": "The NameNode fails to start, throwing a FileNotFoundException for a file that does not exist due to improper closure before deletion.",
            "Suggestions": "Implement checks to ensure that files are properly closed before they can be deleted. Additionally, enhance the logging mechanism to capture all close operations to prevent such inconsistencies in the future.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader",
                    "org.apache.hadoop.hdfs.server.namenode.INodeFile",
                    "org.apache.hadoop.hdfs.server.namenode.FSImage",
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem",
                    "org.apache.hadoop.hdfs.server.namenode.NameNode"
                ],
                "methods": [
                    "FSEditLogLoader.loadEditRecords",
                    "INodeFile.valueOf",
                    "FSImage.loadEdits",
                    "FSNamesystem.loadFSImage",
                    "NameNode.initialize"
                ]
            },
            "possible_fix": "To fix this issue, ensure that the file close operation is logged correctly before any delete operation is executed. Additionally, modify the LeaseManager to handle cases where a file is deleted without being closed, possibly by implementing a rollback mechanism or a warning system to alert users of potential data loss."
        },
        "possible_fix_code": {
            "FSEditLogLoader.loadEditRecords": "  long loadEditRecords(EditLogInputStream in, boolean closeOnExit,\n      long expectedStartingTxId, StartupOption startOpt,\n      MetaRecoveryContext recovery) throws IOException {\n    FSDirectory fsDir = fsNamesys.dir;\n\n    EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts =\n      new EnumMap<FSEditLogOpCodes, Holder<Integer>>(FSEditLogOpCodes.class);\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Acquiring write lock to replay edit log\");\n    }\n\n    fsNamesys.writeLock();\n    fsDir.writeLock();\n\n    long recentOpcodeOffsets[] = new long[4];\n    Arrays.fill(recentOpcodeOffsets, -1);\n    \n    long expectedTxId = expectedStartingTxId;\n    long numEdits = 0;\n    long lastTxId = in.getLastTxId();\n    long numTxns = (lastTxId - expectedStartingTxId) + 1;\n    StartupProgress prog = NameNode.getStartupProgress();\n    Step step = createStartupProgressStep(in);\n    prog.setTotal(Phase.LOADING_EDITS, step, numTxns);\n    Counter counter = prog.getCounter(Phase.LOADING_EDITS, step);\n    long lastLogTime = monotonicNow();\n    long lastInodeId = fsNamesys.dir.getLastInodeId();\n    \n    try {\n      while (true) {\n        try {\n          FSEditLogOp op;\n          try {\n            op = in.readOp();\n            if (op == null) {\n              break;\n            }\n          } catch (Throwable e) {\n            // Handle a problem with our input\n            check203UpgradeFailure(in.getVersion(true), e);\n            String errorMessage =\n              formatEditLogReplayError(in, recentOpcodeOffsets, expectedTxId);\n            FSImage.LOG.error(errorMessage, e);\n            if (recovery == null) {\n               // We will only try to skip over problematic opcodes when in\n               // recovery mode.\n              throw new EditLogInputException(errorMessage, e, numEdits);\n            }\n            MetaRecoveryContext.editLogLoaderPrompt(\n                \"We failed to read txId \" + expectedTxId,\n                recovery, \"skipping the bad section in the log\");\n            in.resync();\n            continue;\n          }\n          recentOpcodeOffsets[(int)(numEdits % recentOpcodeOffsets.length)] =\n            in.getPosition();\n          if (op.hasTransactionId()) {\n            if (op.getTransactionId() > expectedTxId) { \n              MetaRecoveryContext.editLogLoaderPrompt(\"There appears \"+\n                  \"to be a gap in the edit log.  We expected txid \" +\n                  expectedTxId + \", but got txid \" +\n                  op.getTransactionId() + \".\", recovery, \"ignoring missing \" +\n                  \" transaction IDs\");\n            } else if (op.getTransactionId() < expectedTxId) { \n              MetaRecoveryContext.editLogLoaderPrompt(\"There appears \"+\n                  \"to be an out-of-order edit in the edit log.  We \" +\n                  \"expected txid \" + expectedTxId + \", but got txid \" +\n                  op.getTransactionId() + \".\", recovery,\n                  \"skipping the out-of-order edit\");\n              continue;\n            }\n          }\n          try {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"op=\" + op + \", startOpt=\" + startOpt\n                  + \", numEdits=\" + numEdits + \", totalEdits=\" + totalEdits);\n            }\n            long inodeId = applyEditLogOp(op, fsDir, startOpt,\n                in.getVersion(true), lastInodeId);\n            if (lastInodeId < inodeId) {\n              lastInodeId = inodeId;\n            }\n          } catch (RollingUpgradeOp.RollbackException e) {\n            throw e;\n          } catch (Throwable e) {\n            LOG.error(\"Encountered exception on operation \" + op, e);\n            if (recovery == null) {\n              throw e instanceof IOException? (IOException)e: new IOException(e);\n            }\n\n            MetaRecoveryContext.editLogLoaderPrompt(\"Failed to \"+\n             \"apply edit log operation \" + op + \": error \" +\n             e.getMessage(), recovery, \"applying edits\");\n          }\n          // Now that the operation has been successfully decoded and\n          // applied, update our bookkeeping.\n          incrOpCount(op.opCode, opCounts, step, counter);\n          if (op.hasTransactionId()) {\n            lastAppliedTxId = op.getTransactionId();\n            expectedTxId = lastAppliedTxId + 1;\n          } else {\n            expectedTxId = lastAppliedTxId = expectedStartingTxId;\n          }\n          // log progress\n          if (op.hasTransactionId()) {\n            long now = monotonicNow();\n            if (now - lastLogTime > REPLAY_TRANSACTION_LOG_INTERVAL) {\n              long deltaTxId = lastAppliedTxId - expectedStartingTxId + 1;\n              int percent = Math.round((float) deltaTxId / numTxns * 100);\n              LOG.info(\"replaying edit log: \" + deltaTxId + \"/\" + numTxns\n                  + \" transactions completed. (\" + percent + \"%)\");\n              lastLogTime = now;\n            }\n          }\n          numEdits++;\n          totalEdits++;\n        } catch (RollingUpgradeOp.RollbackException e) {\n          LOG.info(\"Stopped at OP_START_ROLLING_UPGRADE for rollback.\");\n          break;\n        } catch (MetaRecoveryContext.RequestStopException e) {\n          MetaRecoveryContext.LOG.warn(\"Stopped reading edit log at \" +\n              in.getPosition() + \"/\"  + in.length());\n          break;\n        }\n      }\n    } finally {\n      fsNamesys.dir.resetLastInodeId(lastInodeId);\n      if(closeOnExit) {\n        in.close();\n      }\n      fsDir.writeUnlock();\n      fsNamesys.writeUnlock(\"loadEditRecords\");\n\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"replaying edit log finished\");\n      }\n\n      if (FSImage.LOG.isDebugEnabled()) {\n        dumpOpCounts(opCounts);\n      }\n    }\n    return numEdits;\n  }"
        }
    },
    {
        "filename": "HDFS-6462.json",
        "creation_time": "2014-05-29T00:19:03.000+0000",
        "bug_report": {
            "Title": "NFS: fsstat request fails with the secure hdfs",
            "Description": "The fsstat request fails in a secure HDFS environment due to a lack of valid Kerberos credentials for the user making the request. The error message indicates that the GSS initiate failed because no valid credentials were provided, which is essential for authentication in a secure setup. This issue arises when the NFS server is started as UserA, who is not properly authenticated to access the NFS service due to misconfigured Kerberos settings.",
            "StackTrace": [
                "2014-05-29 00:09:13,706 WARN  ipc.Client (Client.java:run(672)) - Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1414)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getStats(ClientNamenodeProtocolTranslatorPB.java:554)",
                "at org.apache.hadoop.hdfs.DFSClient.getDiskStatus(DFSClient.java:2165)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.fsstat(RpcProgramNfs3.java:1659)"
            ],
            "RootCause": "The root cause of the issue is the failure to provide valid Kerberos credentials for UserA when attempting to perform the fsstat operation. The error indicates that the system could not find a valid Kerberos ticket granting ticket (TGT) for the user, which is necessary for authentication in a secure environment.",
            "StepsToReproduce": [
                "1) Create user named UserB and UserA",
                "2) Create group named GroupB",
                "3) Add root and UserB users to GroupB, ensuring UserA is not in GroupB",
                "4) Set the following properties in hdfs-site.xml and core-site.xml as specified in the original report",
                "5) Start the NFS server as UserA",
                "6) Mount NFS as the root user",
                "7) Run the command 'df /tmp/tmp_mnt/'"
            ],
            "ExpectedBehavior": "The expected behavior is that the 'df' command should return the disk usage statistics for the mounted NFS directory without any errors.",
            "ObservedBehavior": "The observed behavior is that the command fails with an 'Input/output error' and the NFS logs indicate a failure due to missing Kerberos credentials.",
            "Suggestions": "Ensure that UserA has a valid Kerberos ticket before starting the NFS server. This can be done by running 'kinit UserA' with the appropriate keytab file. Additionally, verify that the keytab file is correctly configured and accessible.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3",
                    "org.apache.hadoop.ipc.Client"
                ],
                "methods": [
                    "RpcProgramNfs3.fsstat",
                    "Client.call"
                ]
            },
            "possible_fix": "To resolve the issue, ensure that UserA has a valid Kerberos ticket by executing 'kinit -kt /tmp/keytab/UserA.keytab UserA@EXAMPLE.COM' before starting the NFS server. This will provide the necessary credentials for the secure connection."
        },
        "possible_fix_code": {
            "RpcProgramNfs3.fsstat": "  public FSSTAT3Response fsstat(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    FSSTAT3Response response = new FSSTAT3Response(Nfs3Status.NFS3_OK);\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    // Ensure UserA has valid Kerberos credentials\n    if (!UserGroupInformation.isLoginKeytabBased()) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    FSSTAT3Request request = null;\n    try {\n      request = new FSSTAT3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid FSSTAT request\");\n      return new FSSTAT3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    FileHandle handle = request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS FSSTAT fileId: \" + handle.getFileId());\n    }\n\n    try {\n      // Use superUserClient to get file system status\n      FsStatus fsStatus = superUserClient.getDiskStatus();\n      long totalBytes = fsStatus.getCapacity();\n      long freeBytes = fsStatus.getRemaining();\n      \n      Nfs3FileAttributes attrs = writeManager.getFileAttr(dfsClient, handle,\n          iug);\n      if (attrs == null) {\n        LOG.info(\"Can't get path for fileId:\" + handle.getFileId());\n        return new FSSTAT3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n      \n      long maxFsObjects = config.getLong(\"dfs.max.objects\", 0);\n      if (maxFsObjects == 0) {\n        // A value of zero in HDFS indicates no limit to the number\n        // of objects that dfs supports. Using Integer.MAX_VALUE instead of\n        // Long.MAX_VALUE so 32bit client won't complain.\n        maxFsObjects = Integer.MAX_VALUE;\n      }\n      \n      return new FSSTAT3Response(Nfs3Status.NFS3_OK, attrs, totalBytes,\n          freeBytes, freeBytes, maxFsObjects, maxFsObjects, maxFsObjects, 0);\n    } catch (RemoteException r) {\n      LOG.warn(\"Exception \", r);\n      IOException io = r.unwrapRemoteException();\n      /**\n       * AuthorizationException can be thrown if the user can't be proxy'd.\n       */\n      if (io instanceof AuthorizationException) {\n        return new FSSTAT3Response(Nfs3Status.NFS3ERR_ACCES);\n      } else {\n        return new FSSTAT3Response(Nfs3Status.NFS3ERR_IO);\n      }\n    } catch (IOException e) {\n      LOG.warn(\"Exception \", e);\n      return new FSSTAT3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-5425.json",
        "creation_time": "2013-10-25T06:53:28.000+0000",
        "bug_report": {
            "Title": "Renaming underconstruction file with snapshots can make NN failure on restart",
            "Description": "The NameNode fails to restart after performing snapshot operations such as creating or renaming snapshots. This issue arises when the NameNode attempts to load the filesystem image, leading to an IllegalStateException due to inconsistencies in the snapshot state. The exception occurs during the replacement of child nodes in the snapshot directory structure, indicating that the state of the filesystem is not as expected after the rename operation.",
            "StackTrace": [
                "2013-10-24 21:07:03,040 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join",
                "java.lang.IllegalStateException",
                "\tat com.google.common.base.Preconditions.checkState(Preconditions.java:133)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$ChildrenDiff.replace(INodeDirectoryWithSnapshot.java:82)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$ChildrenDiff.access$700(INodeDirectoryWithSnapshot.java:62)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$DirectoryDiffList.replaceChild(INodeDirectoryWithSnapshot.java:397)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$DirectoryDiffList.access$900(INodeDirectoryWithSnapshot.java:376)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot.replaceChild(INodeDirectoryWithSnapshot.java:598)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedReplaceINodeFile(FSDirectory.java:1548)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.replaceINodeFile(FSDirectory.java:1537)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.loadFilesUnderConstruction(FSImageFormat.java:855)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.load(FSImageFormat.java:350)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:910)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:899)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:751)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:720)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:266)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:784)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:563)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:422)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:472)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:670)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:655)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1245)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1311)"
            ],
            "RootCause": "The root cause of the issue is an IllegalStateException thrown during the replacement of child nodes in the snapshot directory structure. This occurs because the state of the filesystem is inconsistent after renaming an under-construction file, which leads to a failure when the NameNode attempts to load the filesystem image on restart.",
            "StepsToReproduce": [
                "1. Perform snapshot operations such as createSnapshot or renameSnapshot on an under-construction file.",
                "2. Restart the NameNode.",
                "3. Observe the failure and the IllegalStateException in the logs."
            ],
            "ExpectedBehavior": "The NameNode should successfully restart and load the filesystem image without throwing any exceptions, maintaining the integrity of the snapshot and under-construction files.",
            "ObservedBehavior": "The NameNode fails to restart and throws an IllegalStateException, indicating an issue with the state of the filesystem related to snapshots.",
            "Suggestions": "Review the logic in the snapshot handling code, particularly in the methods responsible for replacing child nodes in the snapshot directory structure. Ensure that the state of the filesystem is consistent before performing operations that modify the structure.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeDirectoryWithSnapshot.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSDirectory",
                    "org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot",
                    "org.apache.hadoop.hdfs.server.namenode.FSImage",
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem",
                    "org.apache.hadoop.hdfs.server.namenode.NameNode"
                ],
                "methods": [
                    "FSDirectory.unprotectedReplaceINodeFile",
                    "INodeDirectoryWithSnapshot.replaceChild",
                    "FSImage.loadFSImage",
                    "FSNamesystem.loadFSImage",
                    "NameNode.loadNamesystem"
                ]
            },
            "possible_fix": "To fix the issue, ensure that the state of the filesystem is validated before performing operations that modify the snapshot structure. This may involve adding checks in the 'replaceChild' method of 'INodeDirectoryWithSnapshot' to ensure that the old child node is in a valid state before replacement. Additionally, consider implementing a rollback mechanism to revert changes if an operation fails."
        },
        "possible_fix_code": {
            "INodeDirectoryWithSnapshot.replaceChild": "  public void replaceChild(final INode oldChild, final INode newChild,\n      final INodeMap inodeMap) {\n    // Check if the oldChild is in a valid state before replacement\n    Preconditions.checkState(oldChild.isValid(), \"Old child is not in a valid state for replacement.\");\n    super.replaceChild(oldChild, newChild, inodeMap);\n    diffs.replaceChild(ListType.CREATED, oldChild, newChild);\n  }"
        }
    },
    {
        "filename": "HDFS-13145.json",
        "creation_time": "2018-02-13T19:22:56.000+0000",
        "bug_report": {
            "Title": "SBN crash when transitioning to ANN with in-progress edit tailing enabled",
            "Description": "The issue arises when transitioning from Standby NameNode (SBN) to Active NameNode (ANN) while edit log tailing is enabled. During this transition, if the ANN crashes, the SBN attempts to transition to ANN but fails due to an IllegalStateException. This occurs because the commit transaction ID lags behind the end transaction ID, leading to a failure in the check within the openForWrite method of FSEditLog.",
            "StackTrace": [
                "java.lang.IllegalStateException: Cannot start writing at txid 24312595802 when there is a stream available for read: ......",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.openForWrite(FSEditLog.java:329)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1196)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1839)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1707)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1622)",
                "at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)",
                "at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:4460)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:851)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:794)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:249)",
                "2018-02-13 00:43:20,728 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1"
            ],
            "RootCause": "The root cause of the crash is that the commit transaction ID (commitTxnId) is not updated correctly due to the absence of a dummy batch during the transition, causing it to lag behind the end transaction ID (endTxId). This leads to a failure in the openForWrite method when it checks for available streams for reading.",
            "StepsToReproduce": [
                "Enable in-progress edit log tailing.",
                "Trigger a transition from SBN to ANN.",
                "Simulate a crash of the ANN during the transition."
            ],
            "ExpectedBehavior": "The system should successfully transition from SBN to ANN without crashing, even if the ANN is temporarily unavailable.",
            "ObservedBehavior": "The SBN crashes with an IllegalStateException when attempting to transition to ANN due to a mismatch in transaction IDs.",
            "Suggestions": "Ensure that a dummy batch is sent to update the commit transaction ID during the transition to prevent the commitTxnId from lagging behind the endTxId.",
            "problem_location": {
                "files": [
                    "FSEditLog.java",
                    "FSNamesystem.java",
                    "NameNode.java",
                    "ActiveState.java",
                    "HAState.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSEditLog",
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem",
                    "org.apache.hadoop.hdfs.server.namenode.NameNode",
                    "org.apache.hadoop.hdfs.server.namenode.ha.ActiveState",
                    "org.apache.hadoop.hdfs.server.namenode.ha.HAState"
                ],
                "methods": [
                    "FSEditLog.openForWrite",
                    "FSNamesystem.startActiveServices",
                    "NameNode.transitionToActive",
                    "ActiveState.enterState",
                    "HAState.setStateInternal"
                ]
            },
            "possible_fix": "Add a check to ensure that a dummy batch is sent during the transition to update the commit transaction ID. This can be done by modifying the transition logic in the FSNamesystem class to always include a dummy batch when transitioning to active state."
        },
        "possible_fix_code": {
            "FSNamesystem.startActiveServices": "  void startActiveServices() throws IOException {\n    startingActiveService = true;\n    LOG.info(\"Starting services required for active state\");\n    writeLock();\n    try {\n      FSEditLog editLog = getFSImage().getEditLog();\n      \n      if (!editLog.isOpenForWrite()) {\n        // During startup, we\\'re already open for write during initialization.\n        editLog.initJournalsForWrite();\n        // May need to recover\n        editLog.recoverUnclosedStreams();\n        \n        LOG.info(\"Catching up to latest edits from old active before \" +\n            \"taking over writer role in edits logs\");\n        editLogTailer.catchupDuringFailover();\n        \n        blockManager.setPostponeBlocksFromFuture(false);\n        blockManager.getDatanodeManager().markAllDatanodesStale();\n        blockManager.clearQueues();\n        blockManager.processAllPendingDNMessages();\n\n        // Only need to re-process the queue, If not in SafeMode.\n        if (!isInSafeMode()) {\n          LOG.info(\"Reprocessing replication and invalidation queues\");\n          blockManager.initializeReplQueues();\n        }\n\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"NameNode metadata after re-processing \" +\n              \"replication and invalidation queues during failover:\\n\" +\n              metaSaveAsString());\n        }\n\n        long nextTxId = getFSImage().getLastAppliedTxId() + 1;\n        LOG.info(\"Will take over writing edit logs at txnid \" + \n            nextTxId);\n        editLog.setNextTxId(nextTxId);\n\n        // Ensure a dummy batch is sent to update commitTxnId\n        editLog.openForWrite(getEffectiveLayoutVersion());\n        editLog.sendDummyBatch(); // Added line to send a dummy batch\n      }\n\n      // Initialize the quota.\n      dir.updateCountForQuota();\n      // Enable quota checks.\n      dir.enableQuotaChecks();\n      dir.ezManager.startReencryptThreads();\n\n      if (haEnabled) {\n        // Renew all of the leases before becoming active.\n        leaseManager.renewAllLeases();\n      }\n      leaseManager.startMonitor();\n      startSecretManagerIfNecessary();\n\n      //ResourceMonitor required only at ActiveNN. See HDFS-2914\n      this.nnrmthread = new Daemon(new NameNodeResourceMonitor());\n      nnrmthread.start();\n\n      nnEditLogRoller = new Daemon(new NameNodeEditLogRoller(\n          editLogRollerThreshold, editLogRollerInterval));\n      nnEditLogRoller.start();\n\n      if (lazyPersistFileScrubIntervalSec > 0) {\n        lazyPersistFileScrubber = new Daemon(new LazyPersistFileScrubber(\n            lazyPersistFileScrubIntervalSec));\n        lazyPersistFileScrubber.start();\n      } else {\n        LOG.warn(\"Lazy persist file scrubber is disabled,\" +\n            \" configured scrub interval is zero.\");\n      }\n\n      cacheManager.startMonitorThread();\n      blockManager.getDatanodeManager().setShouldSendCachingCommands(true);\n      if (provider != null) {\n        edekCacheLoader = Executors.newSingleThreadExecutor(\n            new ThreadFactoryBuilder().setDaemon(true)\n                .setNameFormat(\"Warm Up EDEK Cache Thread #%d\")\n                .build());\n        FSDirEncryptionZoneOp.warmUpEdekCache(edekCacheLoader, dir,\n            edekCacheLoaderDelay, edekCacheLoaderInterval);\n      }\n    } finally {\n      startingActiveService = false;\n      blockManager.checkSafeMode();\n      writeUnlock(\"startActiveServices\");\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-8807.json",
        "creation_time": "2015-07-22T20:45:55.000+0000",
        "bug_report": {
            "Title": "dfs.datanode.data.dir does not handle spaces between storageType and URI correctly",
            "Description": "The DataNode fails to start when there is a space between the storage type and the file URI in the configuration property 'dfs.datanode.data.dir'. This misconfiguration leads to a parsing error, as the URI is not recognized correctly due to the leading space. The error occurs during the initialization of the storage location, where the URI is expected to conform to a specific format without leading spaces.",
            "StackTrace": [
                "2015-07-22 13:34:36,812 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain",
                "java.lang.IllegalArgumentException: java.net.URISyntaxException: Illegal character in scheme name at index 0:  file://tmp/hadoop-aengineer/disk1/dfs/data",
                "at org.apache.hadoop.fs.Path.initialize(Path.java:204)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:170)",
                "at org.apache.hadoop.hdfs.server.datanode.StorageLocation.parse(StorageLocation.java:97)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.getStorageLocations(DataNode.java:2314)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2298)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2349)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2529)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2553)",
                "Caused by: java.net.URISyntaxException: Illegal character in scheme name at index 0:  file://tmp/hadoop-aengineer/disk1/dfs/data",
                "at java.net.URI$Parser.fail(URI.java:2829)",
                "at java.net.URI$Parser.checkChars(URI.java:3002)",
                "at java.net.URI$Parser.checkChar(URI.java:3012)",
                "at java.net.URI$Parser.parse(URI.java:3028)",
                "at java.net.URI.<init>(URI.java:753)",
                "at org.apache.hadoop.fs.Path.initialize(Path.java:201)",
                "... 7 more"
            ],
            "RootCause": "The root cause of the issue is the presence of a leading space in the URI string, which causes the URI parser to throw a URISyntaxException. The StorageLocation.parse method does not handle this case, leading to an IllegalArgumentException when the DataNode attempts to initialize the storage location.",
            "StepsToReproduce": [
                "1. Configure the 'dfs.datanode.data.dir' property in the Hadoop configuration with a space between the storage type and the URI, e.g., '[DISK] file://tmp/hadoop-aengineer/disk1/dfs/data'.",
                "2. Start the DataNode.",
                "3. Observe the startup failure and check the logs for the parsing error."
            ],
            "ExpectedBehavior": "The DataNode should start successfully without any parsing errors, regardless of the presence of spaces in the configuration string.",
            "ObservedBehavior": "The DataNode fails to start and logs a parsing error due to an IllegalArgumentException caused by an improperly formatted URI.",
            "Suggestions": "To mitigate this issue, the configuration parser should trim whitespace from the storage type and URI before processing. Additionally, documentation should clarify the expected format for the 'dfs.datanode.data.dir' property.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/StorageLocation.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Path.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.DataNode",
                    "org.apache.hadoop.hdfs.server.datanode.StorageLocation",
                    "org.apache.hadoop.fs.Path"
                ],
                "methods": [
                    "DataNode.instantiateDataNode",
                    "StorageLocation.parse",
                    "Path.initialize"
                ]
            },
            "possible_fix": "In the StorageLocation.parse method, add a line to trim the rawLocation string before processing it. For example:\n\npublic static StorageLocation parse(String rawLocation) throws IOException, SecurityException {\n    rawLocation = rawLocation.trim(); // Trim whitespace\n    Matcher matcher = regex.matcher(rawLocation);\n    // ... rest of the method remains unchanged\n}"
        },
        "possible_fix_code": {
            "StorageLocation.parse": "  public static StorageLocation parse(String rawLocation)\n      throws IOException, SecurityException {\n    rawLocation = rawLocation.trim(); // Trim whitespace\n    Matcher matcher = regex.matcher(rawLocation);\n    StorageType storageType = StorageType.DEFAULT;\n    String location = rawLocation;\n\n    if (matcher.matches()) {\n      String classString = matcher.group(1);\n      location = matcher.group(2);\n      if (!classString.isEmpty()) {\n        storageType =\n            StorageType.valueOf(StringUtils.toUpperCase(classString));\n      }\n    }\n\n    return new StorageLocation(storageType, new Path(location).toUri());\n  }"
        }
    },
    {
        "filename": "HDFS-3436.json",
        "creation_time": "2012-05-17T10:36:31.000+0000",
        "bug_report": {
            "Title": "Adding new DataNode to existing pipeline fails during Append/Recovery",
            "Description": "When attempting to append to a file in a Hadoop cluster with an existing pipeline of DataNodes, the operation fails if one of the DataNodes (specifically DN3) is stopped. The failure occurs during the process of adding a new DataNode to the existing pipeline, leading to an IOException due to a bad connection acknowledgment and premature EOF errors. This indicates that the system is unable to properly handle the recovery of the pipeline when a DataNode is unavailable.",
            "StackTrace": [
                "java.io.IOException: Bad connect ack with firstBadLink as *******:50010",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1053)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:943)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)",
                "java.io.EOFException: Premature EOF: no length prefix available",
                "at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)"
            ],
            "RootCause": "The root cause of the issue is that the system fails to properly handle the addition of a new DataNode to an existing pipeline when one of the DataNodes is down. Specifically, the DataNode being added is not recognized as a valid replica, leading to an IOException indicating that the block is neither a RBW (Replica Being Written) nor Finalized.",
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with 4 DataNodes.",
                "2. Write a file to the first three DataNodes (DN1, DN2, DN3).",
                "3. Stop DN3.",
                "4. Attempt to append to the file."
            ],
            "ExpectedBehavior": "The system should successfully append to the file by adding a new DataNode to the existing pipeline, even if one DataNode is down.",
            "ObservedBehavior": "The append operation fails with an IOException indicating a bad connection acknowledgment and premature EOF errors.",
            "Suggestions": "Review the logic in the methods responsible for handling DataNode addition to ensure that the system can gracefully handle the absence of a DataNode during append operations. Consider implementing additional checks to validate the state of the DataNodes in the pipeline before attempting to append.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Receiver.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/HdfsProtoUtil.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.DataXceiver",
                    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver",
                    "org.apache.hadoop.hdfs.protocol.HdfsProtoUtil"
                ],
                "methods": [
                    "DataXceiver.run",
                    "Receiver.opTransferBlock",
                    "HdfsProtoUtil.vintPrefixed",
                    "DataXceiver.transferBlock",
                    "DataNode.transferReplicaForPipelineRecovery"
                ]
            },
            "possible_fix": "In the method 'transferReplicaForPipelineRecovery', ensure that the system can handle cases where a DataNode is not in a valid state for appending. This may involve adding checks for the state of the DataNode and implementing a fallback mechanism to handle the absence of a DataNode more gracefully."
        },
        "possible_fix_code": {
            "transferReplicaForPipelineRecovery": "  void transferReplicaForPipelineRecovery(final ExtendedBlock b,\n      final DatanodeInfo[] targets, final String client) throws IOException {\n    final long storedGS;\n    final long visible;\n    final BlockConstructionStage stage;\n\n    //get replica information\n    synchronized(data) {\n      if (data.isValidRbw(b)) {\n        stage = BlockConstructionStage.TRANSFER_RBW;\n      } else if (data.isValidBlock(b)) {\n        stage = BlockConstructionStage.TRANSFER_FINALIZED;\n      } else {\n        final String r = data.getReplicaString(b.getBlockPoolId(), b.getBlockId());\n        throw new IOException(b + \" is neither a RBW nor a Finalized, r=\" + r);\n      }\n\n      storedGS = data.getStoredBlock(b.getBlockPoolId(),\n          b.getBlockId()).getGenerationStamp();\n      if (storedGS < b.getGenerationStamp()) {\n        throw new IOException(\n            storedGS + \" = storedGS < b.getGenerationStamp(), b=\" + b);\n      }\n      visible = data.getReplicaVisibleLength(b);\n    }\n\n    // Check if any target is in a valid state before proceeding\n    if (targets.length == 0 || !areTargetsValid(targets)) {\n      throw new IOException(\"No valid targets available for pipeline recovery.\");\n    }\n\n    //set storedGS and visible length\n    b.setGenerationStamp(storedGS);\n    b.setNumBytes(visible);\n\n    if (targets.length > 0) {\n      new DataTransfer(targets, b, stage, client).run();\n    }\n  }\n\n  private boolean areTargetsValid(DatanodeInfo[] targets) {\n    for (DatanodeInfo target : targets) {\n      if (!target.isAlive()) {\n        return false;\n      }\n    }\n    return true;\n  }"
        }
    }
]