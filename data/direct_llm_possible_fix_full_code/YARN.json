[
    {
        "filename": "YARN-5918.json",
        "creation_time": "2016-11-20T14:19:00.000+0000",
        "bug_report": {
            "Title": "Handle Opportunistic scheduling allocate request failure when NM is lost",
            "Description": "The system encounters a NullPointerException during the allocation of opportunistic containers when the NodeManager (NM) is lost. This occurs specifically in the `convertToRemoteNode` method, which attempts to access the HTTP address of a node that may no longer exist, leading to the failure of the allocation process.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.convertToRemoteNode(OpportunisticContainerAllocatorAMService.java:420)",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.convertToRemoteNodes(OpportunisticContainerAllocatorAMService.java:412)",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.getLeastLoadedNodes(OpportunisticContainerAllocatorAMService.java:402)",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.allocate(OpportunisticContainerAllocatorAMService.java:236)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)"
            ],
            "RootCause": "The root cause of the issue is that the `convertToRemoteNode` method is trying to access the HTTP address of a NodeId that is no longer available due to the NodeManager being lost, resulting in a NullPointerException.",
            "StepsToReproduce": [
                "1. Start the application and ensure that the NodeManager is running.",
                "2. Simulate a failure of the NodeManager while the application is attempting to allocate opportunistic containers.",
                "3. Observe the logs for a NullPointerException in the stack trace."
            ],
            "ExpectedBehavior": "The system should gracefully handle the loss of the NodeManager and not throw a NullPointerException during the allocation process.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the allocation process to fail when the NodeManager is lost.",
            "Suggestions": "Implement a null check in the `convertToRemoteNode` method to ensure that the node exists before attempting to access its HTTP address. Additionally, consider adding error handling to manage scenarios where the NodeManager is lost.",
            "problem_location": {
                "files": [
                    "OpportunisticContainerAllocatorAMService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService"
                ],
                "methods": [
                    "convertToRemoteNode",
                    "convertToRemoteNodes",
                    "getLeastLoadedNodes",
                    "allocate"
                ]
            },
            "possible_fix": "In the `convertToRemoteNode` method, add a null check for the node before accessing its HTTP address:\n\n```java\nprivate RemoteNode convertToRemoteNode(NodeId nodeId) {\n    Node node = ((AbstractYarnScheduler)rmContext.getScheduler()).getNode(nodeId);\n    if (node == null) {\n        // Handle the case where the node is not found\n        return null; // or throw an exception\n    }\n    return RemoteNode.newInstance(nodeId, node.getHttpAddress());\n}\n```"
        },
        "possible_fix_code": {
            "convertToRemoteNode": "  private RemoteNode convertToRemoteNode(NodeId nodeId) {\n    Node node = ((AbstractYarnScheduler)rmContext.getScheduler()).getNode(nodeId);\n    if (node == null) {\n        // Handle the case where the node is not found\n        return null; // or throw an exception\n    }\n    return RemoteNode.newInstance(nodeId, node.getHttpAddress());\n  }"
        }
    },
    {
        "filename": "YARN-8629.json",
        "creation_time": "2018-08-07T00:14:14.000+0000",
        "bug_report": {
            "Title": "Container cleanup fails while trying to delete Cgroups",
            "Description": "The cleanup process for containers fails when an application does not launch successfully. The system attempts to read the 'tasks' file from the cgroup directory, which results in a FileNotFoundException if the directory does not exist. This issue arises during the cleanup phase when the system tries to delete the cgroup associated with the failed container.",
            "StackTrace": [
                "2018-08-06 03:28:20,351 WARN  resources.CGroupsHandlerImpl (CGroupsHandlerImpl.java:checkAndDeleteCgroup(523)) - Failed to read cgroup tasks file.",
                "java.io.FileNotFoundException: /sys/fs/cgroup/cpu,cpuacct/hadoop-yarn-tmp-cxx/container_e02_1533336898541_0010_20_000002/tasks (No such file or directory)",
                "at java.io.FileInputStream.open0(Native Method)",
                "at java.io.FileInputStream.open(FileInputStream.java:195)",
                "at java.io.FileInputStream.<init>(FileInputStream.java:138)",
                "at java.io.FileInputStream.<init>(FileInputStream.java:93)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.checkAndDeleteCgroup(CGroupsHandlerImpl.java:507)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.deleteCGroup(CGroupsHandlerImpl.java:542)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl.postComplete(CGroupsCpuResourceHandlerImpl.java:238)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.postComplete(ResourceHandlerChain.java:111)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.postComplete(LinuxContainerExecutor.java:964)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.reapContainer(LinuxContainerExecutor.java:787)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:821)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:161)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:57)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The root cause of the issue is that the cleanup process attempts to access a 'tasks' file in a cgroup directory that does not exist, leading to a FileNotFoundException.",
            "StepsToReproduce": [
                "Attempt to launch a container that fails to start.",
                "Observe the cleanup process that follows the failed launch.",
                "Check the logs for warnings related to the cgroup tasks file."
            ],
            "ExpectedBehavior": "The system should handle the absence of the cgroup tasks file gracefully, either by skipping the cleanup or logging a more informative message without throwing an exception.",
            "ObservedBehavior": "The system throws a FileNotFoundException when trying to read the 'tasks' file from a non-existent cgroup directory, causing the cleanup process to fail.",
            "Suggestions": "Implement a check to verify the existence of the cgroup directory before attempting to read the 'tasks' file. If the directory does not exist, log a warning and skip the cleanup for that container.",
            "problem_location": {
                "files": [
                    "CGroupsHandlerImpl.java",
                    "CGroupsCpuResourceHandlerImpl.java",
                    "LinuxContainerExecutor.java",
                    "ContainerLaunch.java",
                    "ContainersLauncher.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl",
                    "org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher"
                ],
                "methods": [
                    "CGroupsHandlerImpl.checkAndDeleteCgroup",
                    "CGroupsHandlerImpl.deleteCGroup",
                    "CGroupsCpuResourceHandlerImpl.postComplete",
                    "LinuxContainerExecutor.postComplete",
                    "ContainerLaunch.cleanupContainer"
                ]
            },
            "possible_fix": "In the method 'checkAndDeleteCgroup' of 'CGroupsHandlerImpl', add a check to see if the cgroup directory exists before attempting to read the 'tasks' file. If it does not exist, log a warning and return false to indicate that the cleanup cannot proceed."
        },
        "possible_fix_code": {
            "checkAndDeleteCgroup": "  private boolean checkAndDeleteCgroup(File cgf) throws InterruptedException {\n    boolean deleted = false;\n    // Check if the cgroup directory exists before attempting to read the tasks file\n    if (!cgf.exists() || !cgf.isDirectory()) {\n      LOG.warn(\"Cgroup directory does not exist: \" + cgf);\n      return false; // Skip cleanup if the directory does not exist\n    }\n    try (FileInputStream in = new FileInputStream(cgf + \"/tasks\")) {\n      if (in.read() == -1) {\n        /*\n         * \"tasks\" file is empty, sleep a bit more and then try to delete the\n         * cgroup. Some versions of linux will occasionally panic due to a race\n         * condition in this area, hence the paranoia.\n         */\n        Thread.sleep(deleteCGroupDelay);\n        deleted = cgf.delete();\n        if (!deleted) {\n          LOG.warn(\"Failed attempt to delete cgroup: \" + cgf);\n        }\n      } else {\n        logLineFromTasksFile(cgf);\n      }\n    } catch (IOException e) {\n      LOG.warn(\"Failed to read cgroup tasks file. \", e);\n    }\n    return deleted;\n  }"
        }
    },
    {
        "filename": "YARN-4431.json",
        "creation_time": "2015-12-07T18:31:36.000+0000",
        "bug_report": {
            "Title": "Unnecessary unRegistration of NodeManager on connection failure",
            "Description": "The NodeManager (NM) attempts to unregister itself from the ResourceManager (RM) when it shuts down due to connection issues. This behavior is unnecessary and leads to repeated attempts to unregister NM, which can cause delays and resource wastage. The NM should skip the unRegistration process if it is shutting down due to connection failures, as it will already be handled by the retry policy in place.",
            "StackTrace": [
                "java.net.ConnectException: Call From jduMBP.local/10.200.10.53 to 0.0.0.0:8031 failed on connection exception: java.net.ConnectException: Connection refused",
                "at sun.reflect.GeneratedConstructorAccessor30.newInstance(Unknown Source)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at sun.reflect.Constructor.newInstance(Constructor.java:408)",
                "at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1452)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1385)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.unRegisterNodeManager(ResourceTrackerPBClientImpl.java:98)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.unRegisterNM(NodeStatusUpdaterImpl.java:267)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.serviceStop(NodeStatusUpdaterImpl.java:245)"
            ],
            "RootCause": "The NodeStatusUpdaterImpl's serviceStop method calls unRegisterNM even when the NM is shutting down due to connection issues, which is unnecessary and leads to repeated unregistration attempts.",
            "StepsToReproduce": [
                "1. Start the NodeManager and ensure it is connected to the ResourceManager.",
                "2. Simulate a failure in the connection to the ResourceManager (e.g., by stopping the RM).",
                "3. Observe the logs for repeated unRegistration attempts by the NodeManager."
            ],
            "ExpectedBehavior": "The NodeManager should skip the unRegistration process when shutting down due to connection issues, preventing unnecessary retries.",
            "ObservedBehavior": "The NodeManager attempts to unregister itself from the ResourceManager multiple times, leading to unnecessary delays and resource usage.",
            "Suggestions": "Modify the NodeStatusUpdaterImpl.serviceStop method to check the reason for shutdown and skip the unRegisterNM call if the shutdown is due to connection issues.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"
                ],
                "methods": [
                    "NodeStatusUpdaterImpl.serviceStop",
                    "NodeStatusUpdaterImpl.unRegisterNM"
                ]
            },
            "possible_fix": "In the NodeStatusUpdaterImpl.serviceStop method, add a condition to check if the shutdown is due to connection issues and skip the unRegisterNM call accordingly."
        },
        "possible_fix_code": {
            "NodeStatusUpdaterImpl.serviceStop": "  protected void serviceStop() throws Exception {\n    // Check if the shutdown is due to connection issues\n    if (this.registeredWithRM && !this.isStopped\n        && !isNMUnderSupervisionWithRecoveryEnabled()\n        && !context.getDecommissioned() && !isShuttingDownDueToConnectionIssues()) {\n      unRegisterNM();\n    }\n    // Interrupt the updater.\n    this.isStopped = true;\n    stopRMProxy();\n    super.serviceStop();\n  }\n\n  private boolean isShuttingDownDueToConnectionIssues() {\n    // Implement logic to determine if the shutdown is due to connection issues\n    // This could involve checking the last known connection status or error states\n    return false; // Placeholder for actual implementation\n  }"
        }
    },
    {
        "filename": "YARN-2273.json",
        "creation_time": "2014-07-10T18:38:53.000+0000",
        "bug_report": {
            "Title": "NPE in ContinuousScheduling thread when we lose a node",
            "Description": "A NullPointerException (NPE) occurs in the ContinuousScheduling thread of the FairScheduler when a DataNode (DN) experiences memory errors and reboots, leading to its removal from the cluster. The exception arises during the sorting of nodes based on available resources, specifically when the comparator attempts to access properties of a node that has been removed from the scheduler's node list.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$NodeAvailableResourceComparator.compare(FairScheduler.java:1044)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$NodeAvailableResourceComparator.compare(FairScheduler.java:1040)",
                "at java.util.TimSort.countRunAndMakeAscending(TimSort.java:329)",
                "at java.util.TimSort.sort(TimSort.java:203)",
                "at java.util.TimSort.sort(TimSort.java:173)",
                "at java.util.Arrays.sort(Arrays.java:659)",
                "at java.util.Collections.sort(Collections.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousScheduling(FairScheduler.java:1012)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.access$600(FairScheduler.java:124)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$2.run(FairScheduler.java:1306)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The root cause of the NPE is that the NodeAvailableResourceComparator is trying to access a property of a node that has been removed from the nodes list, leading to a null reference.",
            "StepsToReproduce": [
                "1. Start a YARN cluster with multiple DataNodes.",
                "2. Simulate a memory error on one of the DataNodes causing it to reboot.",
                "3. Allow the DataNode to rejoin the cluster and then remove it again.",
                "4. Observe the logs for the NPE in the ContinuousScheduling thread."
            ],
            "ExpectedBehavior": "The FairScheduler should handle the removal of nodes gracefully without throwing a NullPointerException, allowing the scheduling process to continue.",
            "ObservedBehavior": "The FairScheduler throws a NullPointerException in the ContinuousScheduling thread, causing the ResourceManager to become unresponsive and unable to assign containers.",
            "Suggestions": "Implement a null check in the NodeAvailableResourceComparator to ensure that it does not attempt to access properties of a node that may have been removed from the scheduler's node list.",
            "problem_location": {
                "files": [
                    "FairScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
                ],
                "methods": [
                    "FairScheduler.continuousScheduling",
                    "FairScheduler.NodeAvailableResourceComparator.compare"
                ]
            },
            "possible_fix": "Add a null check in the compare method of NodeAvailableResourceComparator to ensure that the node being compared is not null before accessing its properties."
        },
        "possible_fix_code": {
            "FairScheduler.NodeAvailableResourceComparator.compare": "  @Override\n  public int compare(FSSchedulerNode node1, FSSchedulerNode node2) {\n    // Check for null nodes to prevent NullPointerException\n    if (node1 == null && node2 == null) {\n      return 0;\n    } else if (node1 == null) {\n      return 1; // Treat null as less than any non-null node\n    } else if (node2 == null) {\n      return -1; // Treat any non-null node as greater than null\n    }\n\n    // Existing comparison logic\n    int availableResourceComparison = Resources.compare(node1.getAvailableResource(), node2.getAvailableResource());\n    return availableResourceComparison;\n  }"
        }
    },
    {
        "filename": "YARN-2834.json",
        "creation_time": "2014-11-09T06:07:01.000+0000",
        "bug_report": {
            "Title": "Resource manager crashed with Null Pointer Exception",
            "Description": "The ResourceManager crashes with a Null Pointer Exception during the recovery process after a restart. The exception occurs in the `addApplicationAttempt` method of the `CapacityScheduler` class when it attempts to access an application that is not properly initialized or is null. This indicates that there may be an issue with the application state recovery process, particularly in how application attempts are being handled during the recovery phase.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:734)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1089)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:114)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1041)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1005)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:757)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recoverAppAttempts(RMAppImpl.java:821)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.access$1900(RMAppImpl.java:101)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:843)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:826)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:701)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:312)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:413)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1207)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:590)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1014)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1051)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1047)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1047)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1091)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1226)"
            ],
            "RootCause": "The root cause of the Null Pointer Exception is likely due to the `addApplicationAttempt` method in the `CapacityScheduler` class trying to access an application that has not been properly initialized or is null during the recovery process.",
            "StepsToReproduce": [
                "1. Start the ResourceManager.",
                "2. Submit an application to the ResourceManager.",
                "3. Restart the ResourceManager.",
                "4. Observe the logs for a Null Pointer Exception during the recovery phase."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully recover applications and their attempts without throwing a Null Pointer Exception.",
            "ObservedBehavior": "The ResourceManager crashes with a Null Pointer Exception during the recovery process after a restart.",
            "Suggestions": "Ensure that all application states are properly initialized before attempting to recover them. Add null checks in the `addApplicationAttempt` method to handle cases where the application might not be present.",
            "problem_location": {
                "files": [
                    "CapacityScheduler.java",
                    "RMAppAttemptImpl.java",
                    "RMAppImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler",
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl",
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl"
                ],
                "methods": [
                    "CapacityScheduler.addApplicationAttempt",
                    "RMAppAttemptImpl.handle",
                    "RMAppImpl.recoverAppAttempts"
                ]
            },
            "possible_fix": "In the `addApplicationAttempt` method of `CapacityScheduler`, add a null check for the `application` object before accessing its properties. For example:\n\n```java\nif (application == null) {\n    LOG.error(\"Application not found for ID: \" + applicationAttemptId.getApplicationId());\n    return;\n}\n```"
        },
        "possible_fix_code": {
            "CapacityScheduler.addApplicationAttempt": "  private synchronized void addApplicationAttempt(\n      ApplicationAttemptId applicationAttemptId,\n      boolean transferStateFromPreviousAttempt,\n      boolean isAttemptRecovering) {\n    SchedulerApplication<FiCaSchedulerApp> application =\n        applications.get(applicationAttemptId.getApplicationId());\n    if (application == null) {\n        LOG.error(\"Application not found for ID: \" + applicationAttemptId.getApplicationId());\n        return;\n    }\n    CSQueue queue = (CSQueue) application.getQueue();\n\n    FiCaSchedulerApp attempt =\n        new FiCaSchedulerApp(applicationAttemptId, application.getUser(),\n          queue, queue.getActiveUsersManager(), rmContext);\n    if (transferStateFromPreviousAttempt) {\n      attempt.transferStateFromPreviousAttempt(application\n        .getCurrentAppAttempt());\n    }\n    application.setCurrentAppAttempt(attempt);\n\n    queue.submitApplicationAttempt(attempt, application.getUser());\n    LOG.info(\"Added Application Attempt \" + applicationAttemptId\n        + \" to scheduler from user \" + application.getUser() + \" in queue \"\n        + queue.getQueueName());\n    if (isAttemptRecovering) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(applicationAttemptId\n            + \" is recovering. Skipping notifying ATTEMPT_ADDED\");\n      }\n    } else {\n      rmContext.getDispatcher().getEventHandler().handle(\n        new RMAppAttemptEvent(applicationAttemptId,\n            RMAppAttemptEventType.ATTEMPT_ADDED));\n    }\n  }"
        }
    },
    {
        "filename": "YARN-370.json",
        "creation_time": "2013-02-01T04:02:58.000+0000",
        "bug_report": {
            "Title": "CapacityScheduler app submission fails when min alloc size not multiple of AM size",
            "Description": "The application submission fails due to a mismatch in the expected and actual resource allocation sizes. Specifically, when the minimum allocation size is set to 1G and the Application Master (AM) size is set to 1.5G, the system fails to allocate the required resources correctly. The error indicates that the expected resource allocation was <memory:2048, vCores:1>, but the actual allocation was <memory:1536, vCores:1>. This discrepancy arises because the resource calculator does not round up the AM size to the nearest minimum allocation size, leading to an unauthorized request error.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Unauthorized request to start container. Expected resource <memory:2048, vCores:1> but found <memory:1536, vCores:1>",
                "at org.apache.hadoop.yarn.factories.impl.pb.YarnRemoteExceptionFactoryPBImpl.createYarnRemoteException(YarnRemoteExceptionFactoryPBImpl.java:39)",
                "at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:47)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeRequest(ContainerManagerImpl.java:383)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.startContainer(ContainerManagerImpl.java:400)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagerPBServiceImpl.startContainer(ContainerManagerPBServiceImpl.java:68)",
                "at org.apache.hadoop.yarn.proto.ContainerManager$ContainerManagerService$2.callBlockingMethod(ContainerManager.java:83)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:454)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1014)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1735)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1731)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1441)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1729)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(Native Method)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:525)",
                "at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:90)",
                "at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:57)",
                "at org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl.unwrapAndThrowException(YarnRemoteExceptionPBImpl.java:123)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:109)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:111)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:255)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "RootCause": "The root cause of the issue is that the resource calculator does not round up the Application Master size to the nearest minimum allocation size, resulting in a mismatch between the expected and actual resource allocations.",
            "StepsToReproduce": [
                "1. Set the minimum allocation size to 1G in the capacity scheduler configuration.",
                "2. Set the Application Master size to 1.5G.",
                "3. Submit an application using the configured settings.",
                "4. Observe the failure in application submission due to resource allocation mismatch."
            ],
            "ExpectedBehavior": "The application should launch successfully with the correct resource allocation, where the AM size is rounded up to the nearest minimum allocation size.",
            "ObservedBehavior": "The application fails to launch due to an unauthorized request error, indicating a resource allocation mismatch.",
            "Suggestions": "Implement a rounding mechanism in the resource calculator to ensure that the Application Master size is rounded up to the nearest minimum allocation size before submission.",
            "problem_location": {
                "files": [
                    "ContainerManagerImpl.java",
                    "AMLauncher.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl",
                    "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher"
                ],
                "methods": [
                    "ContainerManagerImpl.authorizeRequest",
                    "AMLauncher.launch"
                ]
            },
            "possible_fix": "Modify the resource allocation logic in the ContainerManagerImpl class to include a rounding mechanism for the Application Master size. For example, update the authorizeRequest method to round the requested memory size to the nearest multiple of the minimum allocation size."
        },
        "possible_fix_code": {
            "authorizeRequest": "public void authorizeRequest(Resource requestedResource) {\n    // Assuming minAllocSize is defined elsewhere in the class\n    int minAllocSize = getMinimumAllocationSize();\n    int roundedMemory = (int) Math.ceil((double) requestedResource.getMemory() / minAllocSize) * minAllocSize;\n    Resource roundedResource = Resource.newInstance(roundedMemory, requestedResource.getVirtualCores());\n\n    // Check if the rounded resource is authorized\n    if (!isResourceAuthorized(roundedResource)) {\n        throw new YarnRemoteException(\"Unauthorized request to start container. Expected resource <memory:\" + roundedResource.getMemory() + \", vCores:\" + roundedResource.getVirtualCores() + \"> but found <memory:\" + requestedResource.getMemory() + \", vCores:\" + requestedResource.getVirtualCores() + \">\");\n    }\n    // Proceed with the request if authorized\n    // ... (rest of the method implementation)\n}"
        }
    },
    {
        "filename": "YARN-3675.json",
        "creation_time": "2015-05-18T22:38:39.000+0000",
        "bug_report": {
            "Title": "FairScheduler: RM quits when node removal races with continuous scheduling on the same node",
            "Description": "The ResourceManager (RM) crashes due to a NullPointerException when an application attempt is removed while continuous scheduling is still in progress on the same node. This occurs because the system attempts to unreserve resources from a node that has already been removed, leading to a failure in the unreserve method of FSAppAttempt.",
            "StackTrace": [
                "12:28:53.782 AM FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager",
                "Error in handling event type APP_ATTEMPT_REMOVED to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.unreserve(FSAppAttempt.java:469)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.completedContainer(FairScheduler.java:815)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplicationAttempt(FairScheduler.java:763)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:111)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)",
                "at java.lang.Thread.run(Thread.java:745)",
                "12:28:53.783 AM INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager Exiting, bbye.."
            ],
            "RootCause": "The root cause of the issue is that the unreserve method in FSAppAttempt is called with a node that has already been removed, resulting in a NullPointerException when trying to access the reserved container.",
            "StepsToReproduce": [
                "1. Start the ResourceManager and a node.",
                "2. Submit an application that requires continuous scheduling.",
                "3. Remove the node while the application is still scheduling on it.",
                "4. Observe the ResourceManager crash due to NullPointerException."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the removal of nodes gracefully without crashing, even if there are ongoing scheduling operations.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when an application attempt is removed while continuous scheduling is still active on the same node.",
            "Suggestions": "Implement checks in the unreserve method to ensure that the node is valid and has not been removed before attempting to unreserve resources.",
            "problem_location": {
                "files": [
                    "FSAppAttempt.java",
                    "FairScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
                ],
                "methods": [
                    "FSAppAttempt.unreserve",
                    "FairScheduler.completedContainer",
                    "FairScheduler.removeApplicationAttempt",
                    "FairScheduler.handle"
                ]
            },
            "possible_fix": "In the unreserve method of FSAppAttempt, add a check to ensure that the node is not null and is still valid before proceeding with the unreserve operation. For example:\n\npublic void unreserve(Priority priority, FSSchedulerNode node) {\n    if (node == null || node.getReservedContainer() == null) {\n        LOG.warn(\"Node is null or has no reserved container, skipping unreserve.\");\n        return;\n    }\n    RMContainer rmContainer = node.getReservedContainer();\n    unreserveInternal(priority, node);\n    node.unreserveResource(this);\n    getMetrics().unreserveResource(\n        getUser(), rmContainer.getContainer().getResource());\n}"
        },
        "possible_fix_code": {
            "FSAppAttempt.unreserve": "  public void unreserve(Priority priority, FSSchedulerNode node) {\n    if (node == null || node.getReservedContainer() == null) {\n        LOG.warn(\"Node is null or has no reserved container, skipping unreserve.\");\n        return;\n    }\n    RMContainer rmContainer = node.getReservedContainer();\n    unreserveInternal(priority, node);\n    node.unreserveResource(this);\n    getMetrics().unreserveResource(\n        getUser(), rmContainer.getContainer().getResource());\n  }"
        }
    },
    {
        "filename": "YARN-4763.json",
        "creation_time": "2016-03-04T10:03:56.000+0000",
        "bug_report": {
            "Title": "RMApps Page crashes with NPE",
            "Description": "The RMApps page crashes due to a NullPointerException (NPE) when attempting to render application data. The issue arises when the application state is NEW and the application attempts are empty, leading to a failure in accessing the application attempts from the ResourceManager context.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock.renderData(RMAppsBlock.java:100)",
                "at org.apache.hadoop.yarn.server.webapp.AppsBlock.render(AppsBlock.java:140)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)",
                "at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block.subView(HtmlBlock.java:43)",
                "at org.apache.hadoop.yarn.webapp.hamlet.Hamlet._(Hamlet.java:30354)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.AppsBlockWithMetrics.render(AppsBlockWithMetrics.java:30)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)",
                "at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)",
                "at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)",
                "at org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:848)",
                "at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:71)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.render(Dispatcher.java:197)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:156)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)"
            ],
            "RootCause": "The root cause of the NullPointerException is the attempt to access application attempts from the ResourceManager context when the application state is NEW and there are no attempts available, leading to a null reference.",
            "StepsToReproduce": [
                "1. Start the ResourceManager and submit an application.",
                "2. Ensure the application is in the NEW state with no attempts.",
                "3. Navigate to the RMApps page.",
                "4. Observe the crash due to NullPointerException."
            ],
            "ExpectedBehavior": "The RMApps page should display a list of applications without crashing, even if some applications are in the NEW state with no attempts.",
            "ObservedBehavior": "The RMApps page crashes with a NullPointerException when trying to render application data for applications in the NEW state with no attempts.",
            "Suggestions": "Implement a null check before accessing application attempts to prevent the NullPointerException. Ensure that the application state is handled correctly when rendering the RMApps page.",
            "problem_location": {
                "files": [
                    "RMAppsBlock.java",
                    "AppsBlock.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock",
                    "org.apache.hadoop.yarn.server.webapp.AppsBlock"
                ],
                "methods": [
                    "RMAppsBlock.renderData",
                    "AppsBlock.render"
                ]
            },
            "possible_fix": "In the renderData method of RMAppsBlock, add a check to ensure that the application attempts are not null before accessing them. For example:\n\nif (appAttempts != null) {\n    // existing logic to render application attempts\n} else {\n    // handle the case where appAttempts is null, possibly log a warning or display a message."
        },
        "possible_fix_code": {
            "RMAppsBlock.renderData": "  protected void renderData(Block html) {\n    TBODY<TABLE<Hamlet>> tbody =\n        html.table(\"#apps\").thead().tr().th(\".id\", \"ID\").th(\".user\", \"User\")\n          .th(\".name\", \"Name\").th(\".type\", \"Application Type\")\n          .th(\".queue\", \"Queue\").th(\".priority\", \"Application Priority\")\n          .th(\".starttime\", \"StartTime\")\n          .th(\".finishtime\", \"FinishTime\").th(\".state\", \"State\")\n          .th(\".finalstatus\", \"FinalStatus\")\n          .th(\".runningcontainer\", \"Running Containers\")\n          .th(\".allocatedCpu\", \"Allocated CPU VCores\")\n          .th(\".allocatedMemory\", \"Allocated Memory MB\")\n          .th(\".queuePercentage\", \"% of Queue\")\n          .th(\".clusterPercentage\", \"% of Cluster\")\n          .th(\".progress\", \"Progress\")\n          .th(\".ui\", \"Tracking UI\")\n          .th(\".blacklisted\", \"Blacklisted Nodes\")._()\n          ._().tbody();\n\n    StringBuilder appsTableData = new StringBuilder(\"[\\n\");\n    for (ApplicationReport appReport : appReports) {\n      // Check if the application attempts are available\n      if (appReport.getCurrentAppAttemptId() == null) {\n        continue; // Skip if there are no attempts\n      }\n\n      // TODO: remove the following condition. It is still here because\n      // the history side implementation of ApplicationBaseProtocol\n      // hasn't filtering capability (YARN-1819).\n      if (!reqAppStates.isEmpty()\n          && !reqAppStates.contains(appReport.getYarnApplicationState())) {\n        continue;\n      }\n\n      AppInfo app = new AppInfo(appReport);\n      ApplicationAttemptId appAttemptId =\n          ConverterUtils.toApplicationAttemptId(app.getCurrentAppAttemptId());\n      String queuePercent = \"N/A\";\n      String clusterPercent = \"N/A\";\n      if(appReport.getApplicationResourceUsageReport() != null) {\n        queuePercent = String.format(\"%.1f\",\n            appReport.getApplicationResourceUsageReport()\n                .getQueueUsagePercentage());\n        clusterPercent = String.format(\"%.1f\",\n            appReport.getApplicationResourceUsageReport().getClusterUsagePercentage());\n      }\n\n      String blacklistedNodesCount = \"N/A\";\n      Set<String> nodes = rm.getRMContext().getRMApps()\n          .get(appAttemptId.getApplicationId()).getAppAttempts()\n          .get(appAttemptId).getBlacklistedNodes();\n      if (nodes != null) {\n        blacklistedNodesCount = String.valueOf(nodes.size());\n      }\n      String percent = StringUtils.format(\"%.1f\", app.getProgress());\n      appsTableData\n        .append(\"[\\\"<a href='\")\n        .append(url(\"app\", app.getAppId()))\n        .append(\"'>\")\n        .append(app.getAppId())\n        .append(\"</a>\\\",\\\"\")\n        .append(\n          StringEscapeUtils.escapeJavaScript(\n              StringEscapeUtils.escapeHtml(app.getUser())))\n        .append(\"\\\",\\\"\")\n        .append(\n          StringEscapeUtils.escapeJavaScript(\n              StringEscapeUtils.escapeHtml(app.getName())))\n        .append(\"\\\",\\\"\")\n        .append(\n          StringEscapeUtils.escapeJavaScript(StringEscapeUtils.escapeHtml(app\n            .getType())))\n        .append(\"\\\",\\\"\")\n        .append(\n          StringEscapeUtils.escapeJavaScript(StringEscapeUtils.escapeHtml(app\n             .getQueue()))).append(\"\\\",\\\"\").append(String\n             .valueOf(app.getPriority()))\n        .append(\"\\\",\\\"\").append(app.getStartedTime())\n        .append(\"\\\",\\\"\").append(app.getFinishedTime())\n        .append(\"\\\",\\\"\")\n        .append(app.getAppState() == null ? UNAVAILABLE : app.getAppState())\n        .append(\"\\\",\\\"\")\n        .append(app.getFinalAppStatus())\n        .append(\"\\\",\\\"\")\n        .append(app.getRunningContainers() == -1 ? \"N/A\" : String\n            .valueOf(app.getRunningContainers()))\n        .append(\"\\\",\\\"\")\n        .append(app.getAllocatedCpuVcores() == -1 ? \"N/A\" : String\n            .valueOf(app.getAllocatedCpuVcores()))\n        .append(\"\\\",\\\"\")\n        .append(app.getAllocatedMemoryMB() == -1 ? \"N/A\" :\n            String.valueOf(app.getAllocatedMemoryMB()))\n        .append(\"\\\",\\\"\")\n        .append(queuePercent)\n        .append(\"\\\",\\\"\")\n        .append(clusterPercent)\n        .append(\"\\\",\\\"\")\n        // Progress bar\n          .append(\"<br title='\").append(percent).append(\"'> <div class='\")\n        .append(C_PROGRESSBAR).append(\"' title='\").append(join(percent, '%'))\n        .append(\"'> \").append(\"<div class='\").append(C_PROGRESSBAR_VALUE)\n        .append(\"' style='\").append(join(\"width:\", percent, '%'))\n        .append(\"'> </div> </div>\").append(\"\\\",\\\"<a \");\n\n      String trackingURL =\n          app.getTrackingUrl() == null\n              || app.getTrackingUrl().equals(UNAVAILABLE)\n              || app.getAppState() == YarnApplicationState.NEW ? null : app\n              .getTrackingUrl();\n\n      String trackingUI =\n          app.getTrackingUrl() == null\n              || app.getTrackingUrl().equals(UNAVAILABLE)\n              || app.getAppState() == YarnApplicationState.NEW ? \"Unassigned\"\n              : app.getAppState() == YarnApplicationState.FINISHED\n              || app.getAppState() == YarnApplicationState.FAILED\n              || app.getAppState() == YarnApplicationState.KILLED ? \"History\"\n              : \"ApplicationMaster\";\n      appsTableData.append(trackingURL == null ? \"#\" : \"href='\" + trackingURL)\n        .append(\"'>\").append(trackingUI).append(\"</a>\\\",\").append(\"\\\"\")\n        .append(blacklistedNodesCount).append(\"\\\"],\\n\");\n\n    }\n    if (appsTableData.charAt(appsTableData.length() - 2) == ',') {\n      appsTableData.delete(appsTableData.length() - 2,\n        appsTableData.length() - 1);\n    }\n    appsTableData.append(\"]\");\n    html.script().$type(\"text/javascript\")\n      ._(\"var appsTableData=\" + appsTableData)._();\n\n    tbody._()._();\n  }"
        }
    },
    {
        "filename": "YARN-8202.json",
        "creation_time": "2018-04-24T15:52:00.000+0000",
        "bug_report": {
            "Title": "DefaultAMSProcessor should properly check units of requested custom resource types against minimum/maximum allocation",
            "Description": "The issue arises when executing a job with specific resource requests that exceed the maximum allowed allocation for a custom resource type. The system throws an InvalidResourceRequestException, indicating that the requested resource type is either less than zero or exceeds the maximum allowed allocation. This occurs because the validation logic in SchedulerUtils does not account for the units of the requested resources, leading to incorrect validation results.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested resource type=[resource1] < 0 or greater than maximum allowed allocation. Requested resource=<memory:200, vCores:1, resource1: 500M>, maximum allowed allocation=<memory:6144, vCores:8, resource1: 5G>",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:286)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:242)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndvalidateRequest(SchedulerUtils.java:258)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.normalizeAndValidateRequests(RMServerUtils.java:249)",
                "at org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor.allocate(DefaultAMSProcessor.java:230)"
            ],
            "RootCause": "The root cause of the issue is that the resource validation logic in SchedulerUtils.validateResourceRequest does not properly handle the units of the requested resources, leading to incorrect validation against the maximum allowed allocation.",
            "StepsToReproduce": [
                "1. Execute a job with the following arguments: -Dmapreduce.map.resource.memory-mb=200 -Dmapreduce.map.resource.resource1=500M 1 1000",
                "2. Ensure that there is one node with 5GB of resource1 available.",
                "3. Observe the exception thrown and the job hanging."
            ],
            "ExpectedBehavior": "The job should successfully allocate the requested resources without throwing an exception, as the available resources meet the request criteria.",
            "ObservedBehavior": "The job hangs and throws an InvalidResourceRequestException due to incorrect validation of the requested resource units.",
            "Suggestions": "Modify the resource validation logic in SchedulerUtils.validateResourceRequest to correctly interpret and validate the units of the requested resources against the maximum allowed allocation.",
            "problem_location": {
                "files": [
                    "SchedulerUtils.java",
                    "DefaultAMSProcessor.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils",
                    "org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor"
                ],
                "methods": [
                    "SchedulerUtils.validateResourceRequest",
                    "DefaultAMSProcessor.allocate"
                ]
            },
            "possible_fix": "In SchedulerUtils.validateResourceRequest, update the logic to parse and compare resource units correctly. For example, ensure that '500M' is interpreted as 500 megabytes and compared against the maximum allowed allocation of '5G' (5 gigabytes) appropriately."
        },
        "possible_fix_code": {
            "SchedulerUtils.validateResourceRequest": "  private static void validateResourceRequest(ResourceRequest resReq,\n      Resource maximumResource, QueueInfo queueInfo, RMContext rmContext)\n      throws InvalidResourceRequestException {\n    Resource requestedResource = resReq.getCapability();\n    for (int i = 0; i < ResourceUtils.getNumberOfKnownResourceTypes(); i++) {\n      ResourceInformation reqRI = requestedResource.getResourceInformation(i);\n      ResourceInformation maxRI = maximumResource.getResourceInformation(i);\n\n      // Convert requested resource value to bytes for comparison\n      long requestedValueInBytes = convertToBytes(reqRI.getValue(), reqRI.getName());\n      long maxValueInBytes = convertToBytes(maxRI.getValue(), maxRI.getName());\n\n      if (requestedValueInBytes < 0 || requestedValueInBytes > maxValueInBytes) {\n        throw new InvalidResourceRequestException(\n            \"Invalid resource request, requested resource type=[\" + reqRI\n                .getName() + \"] < 0 or greater than maximum allowed allocation. Requested \"\n                + \"resource=\" + requestedResource + \", maximum allowed allocation=\" + maximumResource + \", please note that maximum allowed allocation is calculated \"\n                + \"by scheduler based on maximum resource of registered \"\n                + \"NodeManagers, which might be less than configured \"\n                + \"maximum allocation=\" + ResourceUtils.getResourceTypesMaximumAllocation());\n      }\n    }\n    String labelExp = resReq.getNodeLabelExpression();\n    // we don't allow specify label expression other than resourceName=ANY now\n    if (!ResourceRequest.ANY.equals(resReq.getResourceName())\n        && labelExp != null && !labelExp.trim().isEmpty()) {\n      throw new InvalidLabelResourceRequestException(\n          \"Invalid resource request, queue=\" + queueInfo.getQueueName()\n              + \" specified node label expression in a \"\n              + \"resource request has resource name = \"\n              + resReq.getResourceName());\n    }\n\n    // we don't allow specify label expression with more than one node labels now\n    if (labelExp != null && labelExp.contains(\"&&\")) {\n      throw new InvalidLabelResourceRequestException(\n          \"Invalid resource request, queue=\" + queueInfo.getQueueName()\n              + \" specified more than one node label \"\n              + \"in a node label expression, node label expression = \"\n              + labelExp);\n    }\n\n    if (labelExp != null && !labelExp.trim().isEmpty() && queueInfo != null) {\n      if (!checkQueueLabelExpression(queueInfo.getAccessibleNodeLabels(),\n          labelExp, rmContext)) {\n        throw new InvalidLabelResourceRequestException(\n            \"Invalid resource request\" + \", queue=\" + queueInfo.getQueueName()\n                + \" doesn't have permission to access all labels \"\n                + \"in resource request. labelExpression of resource request=\"\n                + labelExp + \". Queue labels=\"\n                + (queueInfo.getAccessibleNodeLabels() == null ? \"\"\n                    : StringUtils.join(\n                        queueInfo.getAccessibleNodeLabels().iterator(), \",\")));\n      } else {\n        checkQueueLabelInLabelManager(labelExp, rmContext);\n      }\n    }\n  }\n\n  private static long convertToBytes(long value, String resourceName) {\n    // Assuming resourceName can be 'memory', 'vCores', 'resource1', etc.\n    // Implement conversion logic based on the resource type\n    if (resourceName.equals(\"memory\")) {\n      return value; // Assuming value is in MB\n    } else if (resourceName.equals(\"resource1\")) {\n      // Convert '500M' to bytes\n      if (value < 1024) {\n        return value * 1024 * 1024; // Convert MB to bytes\n      } else {\n        return value * 1024 * 1024 * 1024; // Convert GB to bytes\n      }\n    }\n    // Add more conversions as necessary\n    return value; // Default case\n  }"
        }
    },
    {
        "filename": "YARN-7118.json",
        "creation_time": "2017-08-29T12:04:01.000+0000",
        "bug_report": {
            "Title": "AHS REST API can return NullPointerException",
            "Description": "The ApplicationHistoryService REST API is encountering a NullPointerException when attempting to retrieve application history data. This issue arises when the API is called without proper parameters, leading to a failure in the underlying method that fetches application reports. The stack trace indicates that the exception occurs in the WebServices.getApps method, specifically when the application reports are being processed.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.webapp.WebServices.getApps(WebServices.java:191)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices.getApps(AHSWebServices.java:96)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)"
            ],
            "RootCause": "The root cause of the NullPointerException is likely due to the appReports variable being null when the method attempts to process it. This can occur if the request to fetch applications fails or if the callerUGI is null, leading to a failure in retrieving application reports.",
            "StepsToReproduce": [
                "Send a GET request to the AHS REST API endpoint without required parameters.",
                "Observe the response for a NullPointerException."
            ],
            "ExpectedBehavior": "The API should return a valid response with application history data or an appropriate error message indicating the missing parameters.",
            "ObservedBehavior": "The API returns a NullPointerException, resulting in an internal server error.",
            "Suggestions": "Implement null checks for the appReports variable before processing it. Additionally, ensure that the callerUGI is properly initialized and handle cases where the application reports cannot be retrieved.",
            "problem_location": {
                "files": [
                    "WebServices.java",
                    "AHSWebServices.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.webapp.WebServices",
                    "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices"
                ],
                "methods": [
                    "WebServices.getApps",
                    "AHSWebServices.getApps"
                ]
            },
            "possible_fix": "In the WebServices.getApps method, add a null check for appReports after the try-catch block. If appReports is null, return an appropriate error response instead of proceeding to process it."
        },
        "possible_fix_code": {
            "WebServices.getApps": "  public AppsInfo getApps(HttpServletRequest req, HttpServletResponse res,\n      String stateQuery, Set<String> statesQuery, String finalStatusQuery,\n      String userQuery, String queueQuery, String count, String startedBegin,\n      String startedEnd, String finishBegin, String finishEnd,\n      Set<String> applicationTypes) {\n    UserGroupInformation callerUGI = getUser(req);\n    boolean checkEnd = false;\n    boolean checkAppTypes = false;\n    boolean checkAppStates = false;\n    long countNum = Long.MAX_VALUE;\n\n    // set values suitable in case both of begin/end not specified\n    long sBegin = 0;\n    long sEnd = Long.MAX_VALUE;\n    long fBegin = 0;\n    long fEnd = Long.MAX_VALUE;\n\n    if (count != null && !count.isEmpty()) {\n      countNum = Long.parseLong(count);\n      if (countNum <= 0) {\n        throw new BadRequestException(\"limit value must be greater then 0\");\n      }\n    }\n\n    if (startedBegin != null && !startedBegin.isEmpty()) {\n      sBegin = Long.parseLong(startedBegin);\n      if (sBegin < 0) {\n        throw new BadRequestException(\"startedTimeBegin must be greater than 0\");\n      }\n    }\n    if (startedEnd != null && !startedEnd.isEmpty()) {\n      sEnd = Long.parseLong(startedEnd);\n      if (sEnd < 0) {\n        throw new BadRequestException(\"startedTimeEnd must be greater than 0\");\n      }\n    }\n    if (sBegin > sEnd) {\n      throw new BadRequestException(\n        \"startedTimeEnd must be greater than startTimeBegin\");\n    }\n\n    if (finishBegin != null && !finishBegin.isEmpty()) {\n      checkEnd = true;\n      fBegin = Long.parseLong(finishBegin);\n      if (fBegin < 0) {\n        throw new BadRequestException(\"finishTimeBegin must be greater than 0\");\n      }\n    }\n    if (finishEnd != null && !finishEnd.isEmpty()) {\n      checkEnd = true;\n      fEnd = Long.parseLong(finishEnd);\n      if (fEnd < 0) {\n        throw new BadRequestException(\"finishTimeEnd must be greater than 0\");\n      }\n    }\n    if (fBegin > fEnd) {\n      throw new BadRequestException(\n        \"finishTimeEnd must be greater than finishTimeBegin\");\n    }\n\n    Set<String> appTypes = parseQueries(applicationTypes, false);\n    if (!appTypes.isEmpty()) {\n      checkAppTypes = true;\n    }\n\n    // stateQuery is deprecated.\n    if (stateQuery != null && !stateQuery.isEmpty()) {\n      statesQuery.add(stateQuery);\n    }\n    Set<String> appStates = parseQueries(statesQuery, true);\n    if (!appStates.isEmpty()) {\n      checkAppStates = true;\n    }\n\n    AppsInfo allApps = new AppsInfo();\n    Collection<ApplicationReport> appReports = null;\n    final GetApplicationsRequest request =\n        GetApplicationsRequest.newInstance();\n    request.setLimit(countNum);\n    request.setStartRange(new LongRange(sBegin, sEnd));\n    try {\n      if (callerUGI == null) {\n        appReports = appBaseProt.getApplications(request).getApplicationList();\n      } else {\n        appReports = callerUGI.doAs(\n            new PrivilegedExceptionAction<Collection<ApplicationReport>> () {\n          @Override\n          public Collection<ApplicationReport> run() throws Exception {\n            return appBaseProt.getApplications(request).getApplicationList();\n          }\n        });\n      }\n    } catch (Exception e) {\n      rewrapAndThrowException(e);\n    }\n    if (appReports == null) {\n      return allApps;\n    }\n    for (ApplicationReport appReport : appReports) {\n\n      if (checkAppStates &&\n          !appStates.contains(StringUtils.toLowerCase(\n              appReport.getYarnApplicationState().toString()))) {\n        continue;\n      }\n      if (finalStatusQuery != null && !finalStatusQuery.isEmpty()) {\n        FinalApplicationStatus.valueOf(finalStatusQuery);\n        if (!appReport.getFinalApplicationStatus().toString()\n          .equalsIgnoreCase(finalStatusQuery)) {\n          continue;\n        }\n      }\n      if (userQuery != null && !userQuery.isEmpty()) {\n        if (!appReport.getUser().equals(userQuery)) {\n          continue;\n        }\n      }\n      if (queueQuery != null && !queueQuery.isEmpty()) {\n        if (!appReport.getQueue().equals(queueQuery)) {\n          continue;\n        }\n      }\n      if (checkAppTypes &&\n          !appTypes.contains(\n              StringUtils.toLowerCase(appReport.getApplicationType().trim()))) {\n        continue;\n      }\n\n      if (checkEnd\n          && (appReport.getFinishTime() < fBegin || appReport.getFinishTime() > fEnd)) {\n        continue;\n      }\n      AppInfo app = new AppInfo(appReport);\n\n      allApps.add(app);\n    }\n    return allApps;\n  }"
        }
    },
    {
        "filename": "YARN-4743.json",
        "creation_time": "2016-02-27T09:12:28.000+0000",
        "bug_report": {
            "Title": "FairSharePolicy breaks TimSort assumption",
            "Description": "The FairSharePolicy implementation is causing a failure in the TimSort algorithm due to a violation of the comparator's contract. Specifically, the FairShareComparator is not transitive, leading to IllegalArgumentException when sorting resources. This occurs when both memorySize and weight are zero, resulting in a NaN value during the comparison, which TimSort cannot handle.",
            "StackTrace": [
                "2016-02-26 14:08:50,821 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.IllegalArgumentException: Comparison method violates its general contract!",
                "at java.util.TimSort.mergeHi(TimSort.java:868)",
                "at java.util.TimSort.mergeAt(TimSort.java:485)",
                "at java.util.TimSort.mergeCollapse(TimSort.java:410)",
                "at java.util.TimSort.sort(TimSort.java:214)",
                "at java.util.TimSort.sort(TimSort.java:173)",
                "at java.util.Arrays.sort(Arrays.java:659)",
                "at java.util.Collections.sort(Collections.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:316)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:240)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1091)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:989)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1185)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The FairShareComparator used in the FairSharePolicy is not transitive, which leads to NaN values when both memorySize and weight are zero. This violates the contract expected by the TimSort algorithm, resulting in an IllegalArgumentException.",
            "StepsToReproduce": [
                "Set memorySize and weight to 0 in the FairSharePolicy.",
                "Trigger a NODE_UPDATE event in the ResourceManager.",
                "Observe the exception thrown during the sorting process."
            ],
            "ExpectedBehavior": "The FairSharePolicy should handle cases where memorySize and weight are zero without causing exceptions in the sorting algorithm.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating that the comparison method violates its general contract, leading to a failure in the scheduling process.",
            "Suggestions": "Ensure that the FairShareComparator handles cases where memorySize and weight are zero appropriately, possibly by returning a consistent comparison result in such cases.",
            "problem_location": {
                "files": [
                    "FairSharePolicy.java",
                    "FSLeafQueue.java",
                    "FSParentQueue.java",
                    "FairScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
                ],
                "methods": [
                    "FSLeafQueue.assignContainer",
                    "FSParentQueue.assignContainer",
                    "FairScheduler.attemptScheduling",
                    "FairScheduler.nodeUpdate",
                    "FairScheduler.handle"
                ]
            },
            "possible_fix": "Modify the FairShareComparator to handle cases where memorySize and weight are both zero. For example, you could add a check to return a consistent value (e.g., 0) when both values are zero to avoid NaN results."
        },
        "possible_fix_code": {
            "FairShareComparator": "public class FairShareComparator implements Comparator<FSAppAttempt> {\n    @Override\n    public int compare(FSAppAttempt a1, FSAppAttempt a2) {\n        // Assuming memorySize and weight are obtained from the FSAppAttempt\n        double memorySize1 = a1.getMemorySize();\n        double weight1 = a1.getWeight();\n        double memorySize2 = a2.getMemorySize();\n        double weight2 = a2.getWeight();\n\n        // Handle the case where both memorySize and weight are zero\n        if (memorySize1 == 0 && weight1 == 0 && memorySize2 == 0 && weight2 == 0) {\n            return 0; // Treat them as equal\n        }\n\n        // Implement the rest of the comparison logic\n        // Example: Compare based on memorySize and weight\n        int result = Double.compare(memorySize1, memorySize2);\n        if (result == 0) {\n            result = Double.compare(weight1, weight2);\n        }\n        return result;\n    }\n}"
        }
    },
    {
        "filename": "YARN-2414.json",
        "creation_time": "2014-08-12T23:48:48.000+0000",
        "bug_report": {
            "Title": "RM web UI: app page will crash if app is failed before any attempt has been created",
            "Description": "The application page in the ResourceManager web UI crashes when an application fails before any attempts have been created. This is due to a NullPointerException occurring in the AppBlock.render method when it tries to access properties of a null application object.",
            "StackTrace": [
                "2014-08-12 16:45:13,573 ERROR org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /cluster/app/application_1407887030038_0001",
                "java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:153)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)",
                "at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)",
                "at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:84)",
                "at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:460)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1191)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.AppBlock.render(AppBlock.java:116)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:67)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:77)",
                "at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)",
                "at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)",
                "at org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:845)",
                "at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:56)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)",
                "at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:212)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.app(RmController.java:55)",
                "... 44 more"
            ],
            "RootCause": "The root cause of the crash is a NullPointerException in the AppBlock.render method, which occurs when the application object is null, likely because the application has failed before any attempts were created.",
            "StepsToReproduce": [
                "1. Start the ResourceManager.",
                "2. Submit an application that fails immediately without any attempts.",
                "3. Access the application page in the web UI."
            ],
            "ExpectedBehavior": "The application page should display an error message or a default view indicating that the application has failed, without crashing the web UI.",
            "ObservedBehavior": "The application page crashes with a NullPointerException, resulting in a server error.",
            "Suggestions": "Implement a null check for the application object in the AppBlock.render method to prevent the NullPointerException from occurring.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/AppBlock.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.webapp.AppBlock"
                ],
                "methods": [
                    "AppBlock.render"
                ]
            },
            "possible_fix": "Add a null check in the AppBlock.render method:\n\n```java\nif (application == null) {\n    // Handle the case where the application is null, e.g., render an error message.\n    return;\n}\n```"
        },
        "possible_fix_code": {
            "AppBlock.render": "  @Override\n  protected void render(Block html) {\n    if (application == null) {\n        // Handle the case where the application is null, e.g., render an error message.\n        html.h1()._(\"Application has failed before any attempts were created.\")._();\n        return;\n    }\n    // Existing rendering logic for a valid application\n    // ... (rest of the rendering logic goes here)\n  }"
        }
    },
    {
        "filename": "YARN-3878.json",
        "creation_time": "2015-07-02T00:20:59.000+0000",
        "bug_report": {
            "Title": "AsyncDispatcher can hang while stopping if it is configured for draining events on stop",
            "Description": "The AsyncDispatcher hangs indefinitely during the stopping process when it is configured to drain events. This occurs because the ResourceManager (RM) is stopped while attempting to post an event to the RMStateStore's AsyncDispatcher, leading to an InterruptedException. As the RM is shutting down, the AsyncDispatcher is also stopped, but it waits for the event queue to drain, which never happens due to the interruption, causing the dispatcher to wait indefinitely until the JVM exits.",
            "StackTrace": [
                "java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:244)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.updateApplicationAttemptState(RMStateStore.java:652)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.rememberTargetTransitionsAndStoreState(RMAppAttemptImpl.java:1173)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ContainerFinishedTransition.transition(RMAppAttemptImpl.java:1650)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:786)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:108)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:838)"
            ],
            "RootCause": "The root cause of the issue is that the AsyncDispatcher is configured to drain events on stop, but it encounters an InterruptedException when the RM is stopped while trying to post an event. This leads to a situation where the dispatcher waits indefinitely for the event queue to drain, which never occurs due to the interruption.",
            "StepsToReproduce": [
                "1. Configure the AsyncDispatcher to drain events on stop.",
                "2. Start the ResourceManager (RM).",
                "3. While the RM is running, attempt to stop the RM.",
                "4. Observe that the AsyncDispatcher hangs indefinitely during the stopping process."
            ],
            "ExpectedBehavior": "The AsyncDispatcher should stop gracefully without hanging, even when configured to drain events.",
            "ObservedBehavior": "The AsyncDispatcher hangs indefinitely while waiting for the event queue to drain, causing the application to not shut down properly.",
            "Suggestions": "Consider implementing a timeout mechanism for the draining process or handling InterruptedExceptions more gracefully to allow the dispatcher to exit without hanging.",
            "problem_location": {
                "files": [
                    "AsyncDispatcher.java",
                    "RMStateStore.java",
                    "RMAppAttemptImpl.java",
                    "AbstractService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.event.AsyncDispatcher",
                    "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore",
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl",
                    "org.apache.hadoop.service.AbstractService"
                ],
                "methods": [
                    "AsyncDispatcher.serviceStop",
                    "RMStateStore.updateApplicationAttemptState",
                    "RMAppAttemptImpl.handle",
                    "AbstractService.stop"
                ]
            },
            "possible_fix": "In the AsyncDispatcher.serviceStop method, implement a timeout for the wait condition to prevent indefinite blocking. For example, modify the while loop to include a maximum wait time and handle the InterruptedException to allow for a graceful exit."
        },
        "possible_fix_code": {
            "AsyncDispatcher.serviceStop": "  protected void serviceStop() throws Exception {\n    if (drainEventsOnStop) {\n      blockNewEvents = true;\n      LOG.info(\"AsyncDispatcher is draining to stop, ignoring any new events.\");\n      synchronized (waitForDrained) {\n        long startTime = System.currentTimeMillis();\n        long timeout = 30000; // 30 seconds timeout\n        while (!drained && eventHandlingThread.isAlive()) {\n          long elapsedTime = System.currentTimeMillis() - startTime;\n          if (elapsedTime >= timeout) {\n            LOG.warn(\"Timeout while waiting for AsyncDispatcher to drain.\");\n            break;\n          }\n          waitForDrained.wait(1000);\n          LOG.info(\"Waiting for AsyncDispatcher to drain. Thread state is :\" +\n              eventHandlingThread.getState());\n        }\n      }\n    }\n    stopped = true;\n    if (eventHandlingThread != null) {\n      eventHandlingThread.interrupt();\n      try {\n        eventHandlingThread.join();\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Interrupted Exception while stopping\", ie);\n      }\n    }\n\n    // stop all the components\n    super.serviceStop();\n  }"
        }
    },
    {
        "filename": "YARN-6683.json",
        "creation_time": "2017-06-02T00:29:13.000+0000",
        "bug_report": {
            "Title": "Invalid event: COLLECTOR_UPDATE at KILLED",
            "Description": "The application is attempting to handle a COLLECTOR_UPDATE event while in the KILLED state, which is not a valid state transition according to the state machine. This results in an InvalidStateTransitionException being thrown. The current implementation sends an event to update the collector address, which is inappropriate for the KILLED state. Instead, the collector address should be updated directly through a method call on the RMApp instance to avoid triggering state machine transitions.",
            "StackTrace": [
                "2017-06-01 20:01:22,686 ERROR rmapp.RMAppImpl (RMAppImpl.java:handle(905)) - Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: COLLECTOR_UPDATE at KILLED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:903)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:118)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:904)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:888)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:201)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:127)"
            ],
            "RootCause": "The application is trying to process a COLLECTOR_UPDATE event while in the KILLED state, which is not allowed by the state machine's transition rules.",
            "StepsToReproduce": [
                "1. Transition an application to the KILLED state.",
                "2. Attempt to send a COLLECTOR_UPDATE event to the RMApp instance."
            ],
            "ExpectedBehavior": "The system should handle state transitions correctly and not allow COLLECTOR_UPDATE events to be processed when the application is in the KILLED state.",
            "ObservedBehavior": "The system throws an InvalidStateTransitionException when trying to handle a COLLECTOR_UPDATE event in the KILLED state.",
            "Suggestions": "Update the collector address directly through a method call on the RMApp instance instead of sending an event when the application is in the KILLED state.",
            "problem_location": {
                "files": [
                    "RMAppImpl.java",
                    "AsyncDispatcher.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl",
                    "org.apache.hadoop.yarn.event.AsyncDispatcher"
                ],
                "methods": [
                    "RMAppImpl.handle",
                    "AsyncDispatcher.dispatch"
                ]
            },
            "possible_fix": "Replace the event handling for COLLECTOR_UPDATE in the RMAppImpl.handle method with a direct method call to update the collector address when the application is in the KILLED state. For example:\n\nif (this.getState() != RMAppState.KILLED) {\n    rmApp.updateCollectorAddr(collectorAddr);\n} else {\n    LOG.warn(\"Cannot update collector address, application is in KILLED state.\");\n}"
        },
        "possible_fix_code": {
            "RMAppImpl.handle": "  public void handle(RMAppEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId appID = event.getApplicationId();\n      LOG.debug(\"Processing event for \" + appID + \" of type \"\n          + event.getType());\n      final RMAppState oldState = getState();\n      try {\n        if (oldState != RMAppState.KILLED) {\n          this.stateMachine.doTransition(event.getType(), event);\n        } else {\n          LOG.warn(\"Cannot update collector address, application is in KILLED state.\");\n        }\n      } catch (InvalidStateTransitionException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n      }\n\n      // Log at INFO if we're not recovering or not in a terminal state.\n      // Log at DEBUG otherwise.\n      if ((oldState != getState()) &&\n          (((recoveredFinalState == null)) ||\n            (event.getType() != RMAppEventType.RECOVER))) {\n        LOG.info(String.format(STATE_CHANGE_MESSAGE, appID, oldState,\n            getState(), event.getType()));\n      } else if ((oldState != getState()) && LOG.isDebugEnabled()) {\n        LOG.debug(String.format(STATE_CHANGE_MESSAGE, appID, oldState,\n            getState(), event.getType()));\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-2910.json",
        "creation_time": "2014-11-27T06:19:00.000+0000",
        "bug_report": {
            "Title": "FSLeafQueue can throw ConcurrentModificationException",
            "Description": "The issue arises from the use of a standard ArrayList to maintain runnable and non-runnable applications in the FSLeafQueue class. Since multiple threads may access and modify this list concurrently, it can lead to a ConcurrentModificationException when one thread modifies the list while another is iterating over it. This is evident from the stack trace where the exception is thrown during the execution of the getResourceUsage method in FSLeafQueue.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:859)",
                "at java.util.ArrayList$Itr.next(ArrayList.java:831)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.getResourceUsage(FSLeafQueue.java:147)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.getHeadroom(FSAppAttempt.java:180)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.allocate(FairScheduler.java:923)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:516)"
            ],
            "RootCause": "The root cause of the issue is the use of a non-thread-safe ArrayList to store runnable and non-runnable applications, which can be modified by multiple threads simultaneously, leading to ConcurrentModificationException.",
            "StepsToReproduce": [
                "Run the application with multiple threads that access and modify the runnable and non-runnable applications list in FSLeafQueue.",
                "Trigger the getResourceUsage method while other threads are modifying the list."
            ],
            "ExpectedBehavior": "The system should handle concurrent modifications gracefully without throwing exceptions.",
            "ObservedBehavior": "The system throws a ConcurrentModificationException when multiple threads attempt to access and modify the list of applications concurrently.",
            "Suggestions": "Replace the ArrayList with a thread-safe collection such as CopyOnWriteArrayList to prevent concurrent modification issues.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSLeafQueue.java",
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java",
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ApplicationMasterService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt",
                    "org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService"
                ],
                "methods": [
                    "FSLeafQueue.getResourceUsage",
                    "FSAppAttempt.getHeadroom",
                    "FairScheduler.allocate",
                    "ApplicationMasterService.allocate"
                ]
            },
            "possible_fix": "Replace the ArrayList used in FSLeafQueue with a CopyOnWriteArrayList to ensure thread safety. For example, change the declaration from:\n\nList<FSAppAttempt> runnableApps = new ArrayList<>();\n\nto:\n\nList<FSAppAttempt> runnableApps = new CopyOnWriteArrayList<>();"
        },
        "possible_fix_code": {
            "FSLeafQueue.getResourceUsage": "  public Resource getResourceUsage() {\n    Resource usage = Resources.createResource(0);\n    for (FSAppAttempt app : runnableApps) {\n      Resources.addTo(usage, app.getResourceUsage());\n    }\n    for (FSAppAttempt app : nonRunnableApps) {\n      Resources.addTo(usage, app.getResourceUsage());\n    }\n    return usage;\n  }\n\n  // Assuming runnableApps and nonRunnableApps are declared as:\n  private List<FSAppAttempt> runnableApps = new CopyOnWriteArrayList<>();\n  private List<FSAppAttempt> nonRunnableApps = new CopyOnWriteArrayList<>();"
        }
    },
    {
        "filename": "YARN-192.json",
        "creation_time": "2012-11-01T05:00:41.000+0000",
        "bug_report": {
            "Title": "Node update causes NPE in the fair scheduler",
            "Description": "A NullPointerException (NPE) occurs in the Fair Scheduler when the 'unreserve' method is called on an FSSchedulerApp with a NodeId that is not recognized by the scheduler. This discrepancy arises because the ResourceManager (RM) has a different understanding of which applications are reserved for which nodes compared to the scheduler. The NPE is triggered when the code attempts to access a reserved container that does not exist for the specified node.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerApp.unreserve(FSSchedulerApp.java:356)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.unreserve(AppSchedulable.java:214)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:266)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:330)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueSchedulable.assignContainer(FSQueueSchedulable.java:161)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:759)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:836)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:329)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the NPE is that the 'unreserve' method in FSSchedulerApp is trying to access a reserved container for a node that does not exist in the reservedContainers map, leading to a null reference.",
            "StepsToReproduce": [
                "1. Start the ResourceManager and Fair Scheduler.",
                "2. Reserve resources for an application on a specific node.",
                "3. Update the node state in the ResourceManager with a NodeId that is not recognized by the scheduler.",
                "4. Trigger the unreserve operation for the application."
            ],
            "ExpectedBehavior": "The scheduler should handle the unreserve operation gracefully without throwing a NullPointerException, even if the NodeId is not recognized.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to unreserve resources for a NodeId that the scheduler does not recognize.",
            "Suggestions": "Implement a check in the 'unreserve' method to ensure that the reserved container exists before attempting to access it. If it does not exist, log a warning and return gracefully.",
            "problem_location": {
                "files": [
                    "FSSchedulerApp.java",
                    "AppSchedulable.java",
                    "FairScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerApp",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
                ],
                "methods": [
                    "FSSchedulerApp.unreserve",
                    "AppSchedulable.unreserve",
                    "FairScheduler.nodeUpdate"
                ]
            },
            "possible_fix": "In the 'unreserve' method of FSSchedulerApp, add a null check for the reservedContainer before accessing it. For example:\n\n```java\nRMContainer reservedContainer = reservedContainers.remove(node.getNodeID());\nif (reservedContainer == null) {\n    LOG.warn(\"No reserved container found for node: \" + node.getNodeID());\n    return;\n}\n```"
        },
        "possible_fix_code": {
            "FSSchedulerApp.unreserve": "  public synchronized void unreserve(FSSchedulerNode node, Priority priority) {\n    Map<NodeId, RMContainer> reservedContainers = \n        this.reservedContainers.get(priority);\n    RMContainer reservedContainer = reservedContainers.remove(node.getNodeID());\n    if (reservedContainer == null) {\n        LOG.warn(\"No reserved container found for node: \" + node.getNodeID());\n        return;\n    }\n    if (reservedContainers.isEmpty()) {\n      this.reservedContainers.remove(priority);\n    }\n    \n    // Reset the re-reservation count\n    resetReReservations(priority);\n\n    Resource resource = reservedContainer.getContainer().getResource();\n    Resources.subtractFrom(currentReservation, resource);\n\n    LOG.info(\"Application \" + getApplicationId() + \" unreserved \" + \" on node \"\n        + node + \", currently has \" + reservedContainers.size() + \" at priority \"\n        + priority + \"; currentReservation \" + currentReservation);\n  }"
        }
    },
    {
        "filename": "YARN-4581.json",
        "creation_time": "2016-01-12T03:37:40.000+0000",
        "bug_report": {
            "Title": "AHS writer thread leak makes RM crash while RM is recovering",
            "Description": "The ResourceManager (RM) crashes during recovery due to a thread leak caused by the ApplicationHistoryWriter. When the ApplicationHistoryWriter is enabled, it attempts to open history files for applications. If the history file is not at zero offset, an IOException is thrown, leading to multiple threads being created without proper cleanup. This results in an OutOfMemoryError as the system runs out of native threads, ultimately causing the RM to crash.",
            "StackTrace": [
                "2016-01-08 03:13:03,441 ERROR org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore: Error when openning history file of application application_1451878591907_0197",
                "java.io.IOException: Output file not at zero offset.",
                "at org.apache.hadoop.io.file.tfile.BCFile$Writer.<init>(BCFile.java:288)",
                "at org.apache.hadoop.io.file.tfile.TFile$Writer.<init>(TFile.java:288)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore$HistoryFileWriter.<init>(FileSystemApplicationHistoryStore.java:728)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore.applicationStarted(FileSystemApplicationHistoryStore.java:418)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.handleWritingApplicationHistoryEvent(RMApplicationHistoryWriter.java:140)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:297)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:292)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:191)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:124)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2016-01-08 03:13:08,335 FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.OutOfMemoryError: unable to create new native thread",
                "at java.lang.Thread.start0(Native Method)",
                "at java.lang.Thread.start(Thread.java:714)",
                "at org.apache.hadoop.hdfs.DFSOutputStream.start(DFSOutputStream.java:2033)",
                "at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForAppend(DFSOutputStream.java:1652)",
                "at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1573)",
                "at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1603)",
                "at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1591)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:328)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:324)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:324)",
                "at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:1161)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore$HistoryFileWriter.<init>(FileSystemApplicationHistoryStore.java:723)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore.applicationStarted(FileSystemApplicationHistoryStore.java:418)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.handleWritingApplicationHistoryEvent(RMApplicationHistoryWriter.java:140)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:297)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:292)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:191)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:124)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is the failure to properly handle the IOException when attempting to open a history file that is not at zero offset. This leads to the creation of multiple threads without proper cleanup, resulting in a thread leak.",
            "StepsToReproduce": [
                "Enable ApplicationHistoryWriter in the configuration.",
                "Start an application that will generate history files.",
                "Ensure that the history file is not at zero offset (this may require manual intervention or specific conditions).",
                "Observe the logs for errors related to opening the history file.",
                "Monitor the ResourceManager for thread count and memory usage."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the IOException gracefully without leaking threads, allowing it to continue functioning normally.",
            "ObservedBehavior": "The ResourceManager crashes with an OutOfMemoryError due to a large number of leaked threads.",
            "Suggestions": "Implement proper error handling in the applicationStarted method to avoid creating new threads when an IOException occurs. Additionally, consider adding a mechanism to clean up or limit the number of active threads.",
            "problem_location": {
                "files": [
                    "FileSystemApplicationHistoryStore.java",
                    "RMApplicationHistoryWriter.java",
                    "AsyncDispatcher.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore",
                    "org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter",
                    "org.apache.hadoop.yarn.event.AsyncDispatcher"
                ],
                "methods": [
                    "FileSystemApplicationHistoryStore.applicationStarted",
                    "RMApplicationHistoryWriter.handleWritingApplicationHistoryEvent",
                    "AsyncDispatcher.dispatch"
                ]
            },
            "possible_fix": "In the applicationStarted method of FileSystemApplicationHistoryStore, add a check to ensure that the history file can be opened without throwing an IOException. If an IOException occurs, log the error and return early without creating a new HistoryFileWriter instance."
        },
        "possible_fix_code": {
            "FileSystemApplicationHistoryStore.applicationStarted": "  public void applicationStarted(ApplicationStartData appStart)\n      throws IOException {\n    HistoryFileWriter hfWriter =\n        outstandingWriters.get(appStart.getApplicationId());\n    if (hfWriter == null) {\n      Path applicationHistoryFile =\n          new Path(rootDirPath, appStart.getApplicationId().toString());\n      try {\n        // Check if the file can be opened without throwing an IOException\n        if (isFileAtZeroOffset(applicationHistoryFile)) {\n          hfWriter = new HistoryFileWriter(applicationHistoryFile);\n          LOG.info(\"Opened history file of application \"\n              + appStart.getApplicationId());\n        } else {\n          LOG.error(\"History file of application \" + appStart.getApplicationId() + \" is not at zero offset\");\n          return; // Return early to avoid creating a new HistoryFileWriter\n        }\n      } catch (IOException e) {\n        LOG.error(\"Error when opening history file of application \"\n            + appStart.getApplicationId(), e);\n        return; // Return early to avoid creating a new HistoryFileWriter\n      }\n      outstandingWriters.put(appStart.getApplicationId(), hfWriter);\n    } else {\n      throw new IOException(\"History file of application \"\n          + appStart.getApplicationId() + \" is already opened\");\n    }\n    assert appStart instanceof ApplicationStartDataPBImpl;\n    try {\n      hfWriter.writeHistoryData(new HistoryDataKey(appStart.getApplicationId()\n        .toString(), START_DATA_SUFFIX),\n        ((ApplicationStartDataPBImpl) appStart).getProto().toByteArray());\n      LOG.info(\"Start information of application \"\n          + appStart.getApplicationId() + \" is written\");\n    } catch (IOException e) {\n      LOG.error(\"Error when writing start information of application \"\n          + appStart.getApplicationId(), e);\n      throw e;\n    }\n  }\n\n  private boolean isFileAtZeroOffset(Path file) throws IOException {\n    // Implement logic to check if the file is at zero offset\n    // This is a placeholder for the actual implementation\n    return true; // Replace with actual check\n  }"
        }
    },
    {
        "filename": "YARN-7786.json",
        "creation_time": "2018-01-22T14:29:46.000+0000",
        "bug_report": {
            "Title": "NullPointerException while launching ApplicationMaster",
            "Description": "A NullPointerException occurs when attempting to launch the ApplicationMaster after sending a kill command to the job. The exception is thrown in the setupTokens method of the AMLauncher class, indicating that the application context or its associated tokens may not be properly initialized or are null at the time of the call.",
            "StackTrace": [
                "2017-11-25 21:27:25,333 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1511616410268_0001_000001. Got exception: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.setupTokens(AMLauncher.java:205)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.createAMContainerLaunchContext(AMLauncher.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:304)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the NullPointerException is likely due to the application context or its tokens being null when the setupTokens method is called. This can happen if the application is not properly initialized or if the kill command disrupts the expected state.",
            "StepsToReproduce": [
                "1. Send a kill command to the job.",
                "2. Attempt to launch the ApplicationMaster."
            ],
            "ExpectedBehavior": "The ApplicationMaster should launch successfully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown, preventing the ApplicationMaster from launching.",
            "Suggestions": "Ensure that the application context and its tokens are properly initialized before calling setupTokens. Consider adding null checks and handling for cases where the application may be in an inconsistent state after a kill command.",
            "problem_location": {
                "files": [
                    "AMLauncher.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher"
                ],
                "methods": [
                    "AMLauncher.setupTokens",
                    "AMLauncher.createAMContainerLaunchContext",
                    "AMLauncher.launch"
                ]
            },
            "possible_fix": "Add null checks in the setupTokens method to handle cases where the application context or its tokens are null. For example, before accessing the tokens, check if 'container.getTokens()' is null and handle it appropriately."
        },
        "possible_fix_code": {
            "AMLauncher.setupTokens": "  protected void setupTokens(\n      ContainerLaunchContext container, ContainerId containerID)\n      throws IOException {\n    if (container == null) {\n        throw new IOException(\"ContainerLaunchContext cannot be null\");\n    }\n    Map<String, String> environment = container.getEnvironment();\n    if (environment == null) {\n        environment = new HashMap<>();\n    }\n    environment.put(ApplicationConstants.APPLICATION_WEB_PROXY_BASE_ENV,\n        application.getWebProxyBase());\n    // Set AppSubmitTime to be consumable by the AM.\n    ApplicationId applicationId =\n        application.getAppAttemptId().getApplicationId();\n    environment.put(\n        ApplicationConstants.APP_SUBMIT_TIME_ENV,\n        String.valueOf(rmContext.getRMApps()\n            .get(applicationId)\n            .getSubmitTime()));\n\n    Credentials credentials = new Credentials();\n    DataInputByteBuffer dibb = new DataInputByteBuffer();\n    ByteBuffer tokens = container.getTokens();\n    if (tokens != null) {\n      dibb.reset(tokens);\n      credentials.readTokenStorageStream(dibb);\n      tokens.rewind();\n    } else {\n      LOG.warn(\"Tokens are null for container: \" + containerID);\n    }\n\n    // Add AMRMToken\n    Token<AMRMTokenIdentifier> amrmToken = createAndSetAMRMToken();\n    if (amrmToken != null) {\n      credentials.addToken(amrmToken.getService(), amrmToken);\n    }\n    DataOutputBuffer dob = new DataOutputBuffer();\n    credentials.writeTokenStorageToStream(dob);\n    container.setTokens(ByteBuffer.wrap(dob.getData(), 0, dob.getLength()));\n  }"
        }
    },
    {
        "filename": "YARN-8035.json",
        "creation_time": "2018-03-16T12:02:04.000+0000",
        "bug_report": {
            "Title": "Uncaught exception in ContainersMonitorImpl during relaunch due to the process ID changing",
            "Description": "During a container relaunch event, the container ID is reused, but a new process is spawned, leading to a situation where the original process ID (PID) is still associated with the container metrics. When the new PID is attempted to be recorded, it triggers a MetricsException because the tag 'ContainerPid' already exists. This occurs in the ContainersMonitorImpl class when initializing process tree monitoring for the new container instance.",
            "StackTrace": [
                "2018-03-16 11:59:02,563 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Uncaught exception in ContainersMonitorImpl while monitoring resource of container_1521201379995_0001_01_000002",
                "org.apache.hadoop.metrics2.MetricsException: Tag ContainerPid already exists!",
                "at org.apache.hadoop.metrics2.lib.MetricsRegistry.checkTagName(MetricsRegistry.java:433)",
                "at org.apache.hadoop.metrics2.lib.MetricsRegistry.tag(MetricsRegistry.java:394)",
                "at org.apache.hadoop.metrics2.lib.MetricsRegistry.tag(MetricsRegistry.java:400)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainerMetrics.recordProcessId(ContainerMetrics.java:277)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.initializeProcessTrees(ContainersMonitorImpl.java:559)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.run(ContainersMonitorImpl.java:448)"
            ],
            "RootCause": "The root cause of the issue is that the 'ContainerPid' tag is not being updated when a new process is spawned for a relaunch, leading to a conflict with the existing tag in the MetricsRegistry.",
            "StepsToReproduce": [
                "1. Start a container and let it run.",
                "2. Force a relaunch of the container.",
                "3. Observe the logs for the MetricsException related to 'ContainerPid'."
            ],
            "ExpectedBehavior": "The system should update the existing 'ContainerPid' tag with the new process ID without throwing an exception.",
            "ObservedBehavior": "An exception is thrown indicating that the 'ContainerPid' tag already exists, preventing the new PID from being recorded.",
            "Suggestions": "Implement a check to update the existing 'ContainerPid' tag with the new process ID instead of trying to create a new tag. This can be done by modifying the recordProcessId method to handle existing tags appropriately.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/monitor/ContainerMetrics.java",
                    "hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/monitor/ContainersMonitorImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainerMetrics",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl"
                ],
                "methods": [
                    "ContainerMetrics.recordProcessId",
                    "ContainersMonitorImpl$MonitoringThread.initializeProcessTrees"
                ]
            },
            "possible_fix": "Modify the recordProcessId method in ContainerMetrics to check if the tag already exists and update its value instead of throwing an exception. Example code change:\n\npublic void recordProcessId(String processId) {\n    if (registry.containsTag(PROCESSID_INFO)) {\n        registry.updateTag(PROCESSID_INFO, processId);\n    } else {\n        registry.tag(PROCESSID_INFO, processId);\n    }\n}"
        },
        "possible_fix_code": {
            "ContainerMetrics.recordProcessId": "  public void recordProcessId(String processId) {\n    if (registry.containsTag(PROCESSID_INFO)) {\n        registry.updateTag(PROCESSID_INFO, processId);\n    } else {\n        registry.tag(PROCESSID_INFO, processId);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-4152.json",
        "creation_time": "2015-09-12T15:02:22.000+0000",
        "bug_report": {
            "Title": "NM crash with NPE when LogAggregationService#stopContainer called for absent container",
            "Description": "The NodeManager crashes due to a NullPointerException (NPE) when the LogAggregationService's stopContainer method is invoked for a container that does not exist. This occurs during log aggregation when an event to kill a container is sent, but the container is already absent. The crash happens because the code does not check if the container exists before attempting to access its properties.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.stopContainer(LogAggregationService.java:422)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:456)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:68)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the stopContainer method attempts to access properties of a container that may not exist in the context, leading to a NullPointerException when it tries to retrieve the container's token identifier.",
            "StepsToReproduce": [
                "Run a Pi job with 500 containers.",
                "Kill the application while it is running.",
                "Observe the logs for warnings about absent containers and the subsequent crash."
            ],
            "ExpectedBehavior": "The system should handle the absence of a container gracefully without crashing, possibly by skipping the log aggregation for non-existent containers.",
            "ObservedBehavior": "The NodeManager crashes with a NullPointerException when trying to stop a container that does not exist.",
            "Suggestions": "Implement a check in the stopContainer method to verify if the container exists in the context before attempting to access its properties.",
            "problem_location": {
                "files": [
                    "LogAggregationService.java",
                    "AsyncDispatcher.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService",
                    "org.apache.hadoop.yarn.event.AsyncDispatcher"
                ],
                "methods": [
                    "LogAggregationService.stopContainer",
                    "LogAggregationService.handle",
                    "AsyncDispatcher.dispatch"
                ]
            },
            "possible_fix": "In the stopContainer method, add a check to ensure that the container exists before accessing its properties. For example:\n\nif (context.getContainers().get(containerId) == null) {\n    LOG.warn(\"Container not found: \" + containerId);\n    return;\n}\n\nContainerType containerType = context.getContainers().get(containerId).getContainerTokenIdentifier().getContainerType();"
        },
        "possible_fix_code": {
            "LogAggregationService.stopContainer": "  private void stopContainer(ContainerId containerId, int exitCode) {\n\n    // A container is complete. Put this containers' logs up for aggregation if\n    // this containers' logs are needed.\n\n    AppLogAggregator aggregator = this.appLogAggregators.get(\n        containerId.getApplicationAttemptId().getApplicationId());\n    if (aggregator == null) {\n      LOG.warn(\"Log aggregation is not initialized for \" + containerId\n          + \", did it fail to start?\");\n      return;\n    }\n    // Check if the container exists before accessing its properties\n    if (context.getContainers().get(containerId) == null) {\n        LOG.warn(\"Container not found: \" + containerId);\n        return;\n    }\n    ContainerType containerType = context.getContainers().get(\n        containerId).getContainerTokenIdentifier().getContainerType();\n    aggregator.startContainerLogAggregation(\n        new ContainerLogContext(containerId, containerType, exitCode));\n  }"
        }
    },
    {
        "filename": "YARN-3697.json",
        "creation_time": "2015-05-21T18:05:38.000+0000",
        "bug_report": {
            "Title": "FairScheduler: ContinuousSchedulingThread can fail to shutdown",
            "Description": "The ContinuousSchedulingThread in FairScheduler may fail to shut down properly due to an InterruptedException being blocked in the continuousSchedulingAttempt method. This occurs when the thread is interrupted while attempting to schedule resources for nodes, leading to a situation where the thread does not exit as expected. The exception is logged, but the thread remains active, causing resource management issues.",
            "StackTrace": [
                "2015-05-17 23:30:43,065 WARN  [FairSchedulerContinuousScheduling] event.AsyncDispatcher (AsyncDispatcher.java:handle(247)) - AsyncDispatcher thread interrupted",
                "java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:244)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:467)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:462)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.handle(RMContainerImpl.java:387)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.handle(RMContainerImpl.java:58)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.allocate(FSAppAttempt.java:357)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:516)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:649)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:803)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:334)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:173)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1082)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt(FairScheduler.java:1014)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread.run(FairScheduler.java:285)"
            ],
            "RootCause": "The root cause of the issue is that the ContinuousSchedulingThread does not handle InterruptedException properly, leading to the thread being blocked and unable to shut down when requested.",
            "StepsToReproduce": [
                "Start the FairScheduler with a configuration that allows for continuous scheduling.",
                "Trigger a shutdown of the FairScheduler while it is actively scheduling resources.",
                "Observe the logs for any InterruptedException warnings and check if the ContinuousSchedulingThread shuts down as expected."
            ],
            "ExpectedBehavior": "The ContinuousSchedulingThread should shut down gracefully when a stop signal is received, without being blocked by any InterruptedException.",
            "ObservedBehavior": "The ContinuousSchedulingThread remains active and does not shut down, even after receiving a stop signal, leading to potential resource management issues.",
            "Suggestions": "Implement proper handling of InterruptedException in the continuousSchedulingAttempt method to ensure that the thread can exit gracefully when interrupted.",
            "problem_location": {
                "files": [
                    "FairScheduler.java",
                    "AsyncDispatcher.java",
                    "RMContainerImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler",
                    "org.apache.hadoop.yarn.event.AsyncDispatcher",
                    "org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl"
                ],
                "methods": [
                    "FairScheduler.continuousSchedulingAttempt",
                    "AsyncDispatcher$GenericEventHandler.handle",
                    "RMContainerImpl.handle"
                ]
            },
            "possible_fix": "In the FairScheduler class, modify the continuousSchedulingAttempt method to check for the interrupted status of the thread and exit the loop if interrupted. For example:\n\n```java\nif (Thread.currentThread().isInterrupted()) {\n    break;\n}\n```"
        },
        "possible_fix_code": {
            "FairScheduler.continuousSchedulingAttempt": "  void continuousSchedulingAttempt() throws InterruptedException {\n    long start = getClock().getTime();\n    List<NodeId> nodeIdList = new ArrayList<NodeId>(nodes.keySet());\n    // Sort the nodes by space available on them, so that we offer\n    // containers on emptier nodes first, facilitating an even spread. This\n    // requires holding the scheduler lock, so that the space available on a\n    // node doesn't change during the sort.\n    synchronized (this) {\n      Collections.sort(nodeIdList, nodeAvailableResourceComparator);\n    }\n\n    // iterate all nodes\n    for (NodeId nodeId : nodeIdList) {\n      FSSchedulerNode node = getFSSchedulerNode(nodeId);\n      try {\n        if (Thread.currentThread().isInterrupted()) {\n          break;\n        }\n        if (node != null && Resources.fitsIn(minimumAllocation,\n            node.getAvailableResource())) {\n          attemptScheduling(node);\n        }\n      } catch (Throwable ex) {\n        LOG.error(\"Error while attempting scheduling for node \" + node +\n            \": \" + ex.toString(), ex);\n      }\n    }\n\n    long duration = getClock().getTime() - start;\n    fsOpDurations.addContinuousSchedulingRunDuration(duration);\n  }"
        }
    },
    {
        "filename": "YARN-2340.json",
        "creation_time": "2014-07-23T15:18:38.000+0000",
        "bug_report": {
            "Title": "NPE thrown when RM restart after queue is STOPPED, causing RM to remain in standby",
            "Description": "When the ResourceManager (RM) is restarted after the queue state has been set to STOPPED, it fails to recover the application's state and throws a NullPointerException (NPE). This occurs specifically during the handling of the APP_ATTEMPT_ADDED event, where the RM attempts to add an application attempt to the scheduler but encounters a null reference, leading to a failure in the recovery process.",
            "StackTrace": [
                "2014-07-23 18:43:24,432 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1406116264351_0014_000002 State change from NEW to SUBMITTED",
                "2014-07-23 18:43:24,433 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ATTEMPT_ADDED to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:568)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:916)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:101)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:602)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2014-07-23 18:43:24,434 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye.."
            ],
            "RootCause": "The NullPointerException occurs in the addApplicationAttempt method of the CapacityScheduler class, specifically when trying to access the application associated with the applicationAttemptId. If the application is not found (which can happen if the queue is stopped), it results in a null reference.",
            "StepsToReproduce": [
                "1. Start a job in the ResourceManager.",
                "2. Change the queue state to STOPPED while the job is in progress.",
                "3. Restart the ResourceManager.",
                "4. Observe the logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully recover the application's state and resume normal operations without throwing an exception.",
            "ObservedBehavior": "The ResourceManager fails to recover and throws a NullPointerException, causing it to remain in standby and not process any applications.",
            "Suggestions": "Implement a check in the addApplicationAttempt method to ensure that the application is not null before proceeding with the addition. Additionally, handle the case where the queue is stopped more gracefully to avoid NPE.",
            "problem_location": {
                "files": [
                    "CapacityScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"
                ],
                "methods": [
                    "CapacityScheduler.addApplicationAttempt",
                    "CapacityScheduler.handle"
                ]
            },
            "possible_fix": "In the addApplicationAttempt method, add a null check for the 'application' variable after retrieving it from the applications map. If it is null, log an error and return early to prevent the NPE."
        },
        "possible_fix_code": {
            "CapacityScheduler.addApplicationAttempt": "  private synchronized void addApplicationAttempt(\n      ApplicationAttemptId applicationAttemptId,\n      boolean transferStateFromPreviousAttempt,\n      boolean isAttemptRecovering) {\n    SchedulerApplication<FiCaSchedulerApp> application =\n        applications.get(applicationAttemptId.getApplicationId());\n    if (application == null) {\n      LOG.error(\"Application not found for ApplicationAttemptId: \" + applicationAttemptId);\n      return; // Early return to prevent NPE\n    }\n    CSQueue queue = (CSQueue) application.getQueue();\n\n    FiCaSchedulerApp attempt =\n        new FiCaSchedulerApp(applicationAttemptId, application.getUser(),\n          queue, queue.getActiveUsersManager(), rmContext);\n    if (transferStateFromPreviousAttempt) {\n      attempt.transferStateFromPreviousAttempt(application\n        .getCurrentAppAttempt());\n    }\n    application.setCurrentAppAttempt(attempt);\n\n    queue.submitApplicationAttempt(attempt, application.getUser());\n    LOG.info(\"Added Application Attempt \" + applicationAttemptId\n        + \" to scheduler from user \" + application.getUser() + \" in queue \"\n        + queue.getQueueName());\n    if (isAttemptRecovering) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(applicationAttemptId\n            + \" is recovering. Skipping notifying ATTEMPT_ADDED\");\n      }\n    } else {\n      rmContext.getDispatcher().getEventHandler().handle(\n        new RMAppAttemptEvent(applicationAttemptId,\n            RMAppAttemptEventType.ATTEMPT_ADDED));\n    }\n  }"
        }
    },
    {
        "filename": "YARN-8022.json",
        "creation_time": "2018-03-10T19:29:27.000+0000",
        "bug_report": {
            "Title": "ResourceManager UI cluster/app/<app-id> page fails to render",
            "Description": "The ResourceManager UI fails to render the application attempts for a specific application ID, resulting in a NullPointerException. This occurs when the application ID is either invalid or the application attempts cannot be retrieved due to an underlying issue, such as a missing application report or a failure in the security context.",
            "StackTrace": [
                "org.apache.hadoop.yarn.server.webapp.AppBlock: Failed to read the attempts of the application application_1520597233415_0002.",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.webapp.AppBlock$3.run(AppBlock.java:283)",
                "at org.apache.hadoop.yarn.server.webapp.AppBlock$3.run(AppBlock.java:280)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)",
                "at org.apache.hadoop.yarn.server.webapp.AppBlock.render(AppBlock.java:279)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock.render(RMAppBlock.java:71)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)",
                "at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)",
                "at org.apache.hadoop.yarn.webapp.hamlet2.HamletImpl$EImp._v(HamletImpl.java:117)",
                "at org.apache.hadoop.yarn.webapp.hamlet2.Hamlet$TD.__(Hamlet.java:848)",
                "at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:71)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)",
                "at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:212)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.app(RmController.java:54)"
            ],
            "RootCause": "The NullPointerException is likely caused by the application report being null or not properly initialized when attempting to read application attempts. This can occur if the application ID is invalid or if there are issues retrieving the application report due to security context failures.",
            "StepsToReproduce": [
                "Navigate to the ResourceManager UI.",
                "Access the page for a specific application using its application ID.",
                "Observe the error message displayed on the page."
            ],
            "ExpectedBehavior": "The application attempts should be displayed correctly on the ResourceManager UI without any errors.",
            "ObservedBehavior": "The page displays the message 'Failed to read the attempts of the application' followed by a NullPointerException.",
            "Suggestions": "Implement null checks for the application report and ensure that the application ID is valid before attempting to retrieve application attempts. Additionally, improve error handling to provide more informative messages to the user.",
            "problem_location": {
                "files": [
                    "AppBlock.java",
                    "RmController.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.webapp.AppBlock",
                    "org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController"
                ],
                "methods": [
                    "AppBlock.render",
                    "RmController.app"
                ]
            },
            "possible_fix": "In the AppBlock.render method, add a check to ensure that appReport is not null before attempting to access its attempts. If it is null, log a more descriptive error message and return gracefully."
        },
        "possible_fix_code": {
            "AppBlock.render": "  protected void render(Block html) {\n    String webUiType = $(WEB_UI_TYPE);\n    String aid = $(APPLICATION_ID);\n    if (aid.isEmpty()) {\n      puts(\"Bad request: requires Application ID\");\n      return;\n    }\n\n    try {\n      appID = Apps.toAppID(aid);\n    } catch (Exception e) {\n      puts(\"Invalid Application ID: \" + aid);\n      return;\n    }\n\n    UserGroupInformation callerUGI = getCallerUGI();\n    ApplicationReport appReport;\n    try {\n      final GetApplicationReportRequest request =\n          GetApplicationReportRequest.newInstance(appID);\n      if (callerUGI == null) {\n        appReport =\n            appBaseProt.getApplicationReport(request).getApplicationReport();\n      } else {\n        appReport = callerUGI.doAs(\n            new PrivilegedExceptionAction<ApplicationReport> () {\n          @Override\n          public ApplicationReport run() throws Exception {\n            return getApplicationReport(request);\n          }\n        });\n      }\n    } catch (Exception e) {\n      String message = \"Failed to read the application \" + appID + \".\";\n      LOG.error(message, e);\n      html.p().__(message).__();\n      return;\n    }\n\n    if (appReport == null) {\n      puts(\"Application not found: \" + aid);\n      return;\n    }\n\n    AppInfo app = new AppInfo(appReport);\n\n    setTitle(join(\"Application \", aid));\n\n    // Validate if able to read application attempts\n    // which should also validate if kill is allowed for the user based on ACLs\n\n    Collection<ApplicationAttemptReport> attempts;\n    try {\n      final GetApplicationAttemptsRequest request =\n          GetApplicationAttemptsRequest.newInstance(appID);\n      attempts = callerUGI.doAs(\n          new PrivilegedExceptionAction<Collection<\n              ApplicationAttemptReport>>() {\n            @Override\n            public Collection<ApplicationAttemptReport> run() throws Exception {\n              return getApplicationAttemptsReport(request);\n            }\n          });\n    } catch (Exception e) {\n      String message =\n          \"Failed to read the attempts of the application \" + appID + \".\";\n      LOG.error(message, e);\n      html.p().__(message).__();\n      return;\n    }\n\n    // Additional null check for attempts\n    if (attempts == null) {\n      puts(\"No attempts found for application: \" + aid);\n      return;\n    }\n\n    // YARN-6890. for secured cluster allow anonymous UI access, application kill\n    // shouldn't be there.\n    boolean unsecuredUIForSecuredCluster = UserGroupInformation.isSecurityEnabled()\n        && this.unsecuredUI;\n\n    if (webUiType != null\n        && webUiType.equals(YarnWebParams.RM_WEB_UI)\n        && conf.getBoolean(YarnConfiguration.RM_WEBAPP_UI_ACTIONS_ENABLED,\n          YarnConfiguration.DEFAULT_RM_WEBAPP_UI_ACTIONS_ENABLED)\n            && !unsecuredUIForSecuredCluster\n            && !isAppInFinalState(app)) {\n      // Application Kill\n      html.div()\n        .button()\n          .$onclick(\"confirmAction()\").b(\"Kill Application\").__()\n          .__();\n\n      StringBuilder script = new StringBuilder();\n      script.append(\"function confirmAction() {\")\n          .append(\" b = confirm(\\\"Are you sure?\\\");\")\n          .append(\" if (b == true) {\")\n          .append(\" $.ajax({\")\n          .append(\" type: 'PUT',\")\n          .append(\" url: '/ws/v1/cluster/apps/\").append(aid).append(\"/state',\")\n          .append(\" contentType: 'application/json',\")\n          .append(getCSRFHeaderString(conf))\n          .append(\" data: '{\\\"state\\\":\\\"KILLED\\\"}',\")\n          .append(\" dataType: 'json'\\\")\n          .append(\" }).done(function(data){\")\n          .append(\" setTimeout(function(){\")\n          .append(\" location.href = '/cluster/app/\").append(aid).append(\"';\")\n          .append(\" }, 1000);\")\n          .append(\" }).fail(function(data){\")\n          .append(\" console.log(data);\")\n          .append(\" });\")\n          .append(\" }\")\n          .append(\"}\");\n\n      html.script().$type(\"text/javascript\").__(script.toString()).__();\n    }\n\n    String schedulerPath = WebAppUtils.getResolvedRMWebAppURLWithScheme(conf) +\n        \"/cluster/scheduler?openQueues=\" + app.getQueue();\n\n    ResponseInfo overviewTable = info(\"Application Overview\")\n      .__(\"User:\", schedulerPath, app.getUser())\n      .__(\"Name:\", app.getName())\n      .__(\"Application Type:\", app.getType())\n      .__(\"Application Tags:\",\n        app.getApplicationTags() == null ? \"\" : app.getApplicationTags())\n      .__(\"Application Priority:\", clarifyAppPriority(app.getPriority()))\n      .__(\n        \"YarnApplicationState:\",\n        app.getAppState() == null ? UNAVAILABLE : clarifyAppState(app\n          .getAppState()))\n      .__(\"Queue:\", schedulerPath, app.getQueue())\n      .__(\"FinalStatus Reported by AM:\",\n        clairfyAppFinalStatus(app.getFinalAppStatus()))\n      .__(\"Started:\", Times.format(app.getStartedTime()))\n      .__(\n        \"Elapsed:\",\n        StringUtils.formatTime(Times.elapsed(app.getStartedTime(),\n          app.getFinishedTime())))\n      .__(\n        \"Tracking URL:\",\n        app.getTrackingUrl() == null\n            || app.getTrackingUrl().equals(UNAVAILABLE) ? null : root_url(app\n          .getTrackingUrl()),\n        app.getTrackingUrl() == null\n            || app.getTrackingUrl().equals(UNAVAILABLE) ? \"Unassigned\" : app\n          .getAppState() == YarnApplicationState.FINISHED\n            || app.getAppState() == YarnApplicationState.FAILED\n            || app.getAppState() == YarnApplicationState.KILLED ? \"History\"\n            : \"ApplicationMaster\");\n    if (webUiType != null\n        && webUiType.equals(YarnWebParams.RM_WEB_UI)) {\n      LogAggregationStatus status = getLogAggregationStatus();\n      if (status == null) {\n        overviewTable.__(\"Log Aggregation Status:\", \"N/A\");\n      } else if (status == LogAggregationStatus.DISABLED\n          || status == LogAggregationStatus.NOT_START\n          || status == LogAggregationStatus.SUCCEEDED) {\n        overviewTable.__(\"Log Aggregation Status:\", status.name());\n      } else {\n        overviewTable.__(\"Log Aggregation Status:\",\n            root_url(\"logaggregationstatus\", app.getAppId()), status.name());\n      }\n      long timeout = appReport.getApplicationTimeouts()\n          .get(ApplicationTimeoutType.LIFETIME).getRemainingTime();\n      if (timeout < 0) {\n        overviewTable.__(\"Application Timeout (Remaining Time):\", \"Unlimited\");\n      } else {\n        overviewTable.__(\"Application Timeout (Remaining Time):\",\n            String.format(\"%d seconds\", timeout));\n      }\n    }\n    overviewTable.__(\"Diagnostics:\",\n        app.getDiagnosticsInfo() == null ? \"\" : app.getDiagnosticsInfo());\n    overviewTable.__(\"Unmanaged Application:\", app.isUnmanagedApp());\n    overviewTable.__(\"Application Node Label expression:\",\n        app.getAppNodeLabelExpression() == null ? \"<Not set>\"\n            : app.getAppNodeLabelExpression());\n    overviewTable.__(\"AM container Node Label expression:\",\n        app.getAmNodeLabelExpression() == null ? \"<Not set>\"\n            : app.getAmNodeLabelExpression());\n\n    try {\n      final GetApplicationAttemptsRequest request =\n          GetApplicationAttemptsRequest.newInstance(appID);\n      if (callerUGI == null) {\n        attempts = appBaseProt.getApplicationAttempts(request)\n            .getApplicationAttemptList();\n      } else {\n        attempts = callerUGI.doAs(\n            new PrivilegedExceptionAction<Collection<ApplicationAttemptReport>> () {\n          @Override\n          public Collection<ApplicationAttemptReport> run() throws Exception {\n            return appBaseProt.getApplicationAttempts(request)\n                .getApplicationAttemptList();\n          }\n        });\n      }\n    } catch (Exception e) {\n      String message =\n          \"Failed to read the attempts of the application \" + appID + \".\";\n      LOG.error(message, e);\n      html.p().__(message).__();\n      return;\n    }\n\n    createApplicationMetricsTable(html);\n\n    html.__(InfoBlock.class);\n\n    generateApplicationTable(html, callerUGI, attempts);\n  }"
        }
    },
    {
        "filename": "YARN-3793.json",
        "creation_time": "2015-06-10T20:52:38.000+0000",
        "bug_report": {
            "Title": "Several NPEs when deleting local files on NM recovery",
            "Description": "The issue arises during the recovery process of the NodeManager when work-preserving restart is enabled. The logs indicate that a NullPointerException (NPE) occurs when attempting to delete sub-directories, specifically when the sub-directory path is null. This suggests that the system is not correctly tracking the paths of resources that need to be deleted, leading to potential resource leaks.",
            "StackTrace": [
                "2015-05-18 07:06:10,225 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : null",
                "2015-05-18 07:06:10,224 ERROR org.apache.hadoop.yarn.server.nodemanager.DeletionService: Exception during execution of task in DeletionService",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.fs.FileContext.fixRelativePart(FileContext.java:274)",
                "at org.apache.hadoop.fs.FileContext.delete(FileContext.java:755)",
                "at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.deleteAsUser(DefaultContainerExecutor.java:458)",
                "at org.apache.hadoop.yarn.server.nodemanager.DeletionService$FileDeletionTask.run(DeletionService.java:293)"
            ],
            "RootCause": "The root cause of the NPE is that the sub-directory path being passed to the delete method is null. This occurs because the context (DeletionAsUserContext) does not provide a valid sub-directory when the base directories are empty.",
            "StepsToReproduce": [
                "Enable work-preserving restart on the NodeManager.",
                "Trigger a recovery process that involves deleting local files.",
                "Observe the logs for NPEs related to null paths."
            ],
            "ExpectedBehavior": "The system should correctly handle the deletion of local files without encountering NullPointerExceptions, even when sub-directory paths are not provided.",
            "ObservedBehavior": "The system throws NullPointerExceptions when attempting to delete files, indicating that the sub-directory path is null.",
            "Suggestions": "Implement a check to ensure that the sub-directory path is not null before attempting to delete it. If it is null, log a warning and skip the deletion process for that path.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DefaultContainerExecutor.java",
                    "hadoop-common/src/main/java/org/apache/hadoop/fs/FileContext.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor",
                    "org.apache.hadoop.fs.FileContext"
                ],
                "methods": [
                    "DefaultContainerExecutor.deleteAsUser",
                    "FileContext.delete",
                    "FileContext.fixRelativePart"
                ]
            },
            "possible_fix": "In the deleteAsUser method of DefaultContainerExecutor, add a check for null subDir before proceeding with deletion. For example:\n\nif (subDir == null) {\n    LOG.warn(\"Sub-directory is null, skipping deletion.\");\n    return;\n}"
        },
        "possible_fix_code": {
            "deleteAsUser": "  public void deleteAsUser(DeletionAsUserContext ctx)\n      throws IOException, InterruptedException {\n    Path subDir = ctx.getSubDir();\n    List<Path> baseDirs = ctx.getBasedirs();\n\n    if (subDir == null) {\n      LOG.warn(\"Sub-directory is null, skipping deletion.\");\n      return;\n    }\n\n    if (baseDirs == null || baseDirs.size() == 0) {\n      LOG.info(\"Deleting absolute path : \" + subDir);\n      if (!lfs.delete(subDir, true)) {\n        //Maybe retry\n        LOG.warn(\"delete returned false for path: [\" + subDir + \"]\");\n      }\n      return;\n    }\n    for (Path baseDir : baseDirs) {\n      Path del = new Path(baseDir, subDir);\n      LOG.info(\"Deleting path : \" + del);\n      if (!lfs.delete(del, true)) {\n        LOG.warn(\"delete returned false for path: [\" + del + \"]\");\n      }\n    }\n  }"
        }
    },
    {
        "filename": "YARN-6102.json",
        "creation_time": "2017-01-17T09:36:29.000+0000",
        "bug_report": {
            "Title": "RMActiveService context to be updated with new RMContext on failover",
            "Description": "The issue arises during the failover process of the ResourceManager (RM) when a node heartbeat is sent. If a failover occurs while the dispatcher is being reset, the new dispatcher may not have registered the necessary event handlers yet. This leads to an exception being thrown when the dispatcher attempts to handle an event for which no handler is registered, specifically for the RMNodeEventType.",
            "StackTrace": [
                "2017-01-17 16:42:17,911 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(200)) - Error in dispatcher thread",
                "java.lang.Exception: No handler for registered for class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:196)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:120)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2017-01-17 16:42:17,914 INFO  [AsyncDispatcher ShutDown handler] event.AsyncDispatcher (AsyncDispatcher.java:run(303)) - Exiting, bbye.."
            ],
            "RootCause": "The root cause of the issue is the timing of the dispatcher reset during the failover process. When the node heartbeat is sent, if a failover occurs, the dispatcher is reset before the new event handlers are registered, leading to an attempt to dispatch an event without a registered handler.",
            "StepsToReproduce": [
                "1. Start the ResourceManager and ensure it is running.",
                "2. Simulate a failover while sending a node heartbeat to the ResourceTrackerService.",
                "3. Observe the logs for the exception indicating no handler registered for RMNodeEventType."
            ],
            "ExpectedBehavior": "The ResourceManager should handle node heartbeat events correctly, even during a failover, without throwing exceptions due to unregistered event handlers.",
            "ObservedBehavior": "An exception is thrown indicating that there is no handler registered for the RMNodeEventType when a failover occurs during the dispatch of a node heartbeat event.",
            "Suggestions": "Implement a mechanism to ensure that the dispatcher is fully initialized and all event handlers are registered before processing any events after a failover.",
            "problem_location": {
                "files": [
                    "AsyncDispatcher.java",
                    "ResourceTrackerService.java",
                    "ResourceManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.event.AsyncDispatcher",
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService",
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"
                ],
                "methods": [
                    "AsyncDispatcher.dispatch",
                    "ResourceTrackerService.nodeHeartbeat",
                    "ResourceManager.reinitialize"
                ]
            },
            "possible_fix": "In the ResourceManager.reinitialize method, ensure that the dispatcher is not reset until all necessary event handlers are registered. This can be achieved by modifying the order of operations in the reinitialize method to first register the handlers before resetting the dispatcher."
        },
        "possible_fix_code": {
            "ResourceManager.reinitialize": "public void reinitialize(Configuration conf) throws IOException {\n    // Register all necessary event handlers first\n    registerEventHandlers();\n\n    // Now reset the dispatcher\n    if (dispatcher != null) {\n        dispatcher.stop();\n    }\n    dispatcher = new AsyncDispatcher();\n    dispatcher.init(conf);\n    dispatcher.start();\n\n    // Other reinitialization logic...\n}"
        }
    },
    {
        "filename": "YARN-8409.json",
        "creation_time": "2018-06-08T20:36:32.000+0000",
        "bug_report": {
            "Title": "ActiveStandbyElectorBasedElectorService is failing with NPE",
            "Description": "In a ResourceManager High Availability (RM-HA) environment, when the ZooKeeper leader is killed and a ResourceManager failover is performed, the active ResourceManager sometimes encounters a NullPointerException (NPE) during initialization. This issue arises due to the failure to establish a connection to the ZooKeeper server, leading to an unexpected state in the ActiveStandbyElector, which subsequently causes the NPE.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1033)",
                "at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1030)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1095)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1087)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.createWithRetries(ActiveStandbyElector.java:1030)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.ensureParentZNode(ActiveStandbyElector.java:347)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.serviceInit(ActiveStandbyElectorBasedElectorService.java:110)"
            ],
            "RootCause": "The root cause of the NullPointerException is the failure to connect to the ZooKeeper server, which results in the ActiveStandbyElector being in an invalid state when it attempts to perform operations that require a valid connection.",
            "StepsToReproduce": [
                "Set up a ResourceManager in a High Availability environment.",
                "Kill the ZooKeeper leader.",
                "Perform a ResourceManager failover."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully failover and initialize without encountering a NullPointerException.",
            "ObservedBehavior": "The active ResourceManager fails to initialize and throws a NullPointerException during the process.",
            "Suggestions": "Ensure that the ZooKeeper server is running and accessible before performing a failover. Implement additional error handling in the ActiveStandbyElector to manage connection failures gracefully.",
            "problem_location": {
                "files": [
                    "ActiveStandbyElector.java",
                    "ActiveStandbyElectorBasedElectorService.java"
                ],
                "classes": [
                    "org.apache.hadoop.ha.ActiveStandbyElector",
                    "org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService"
                ],
                "methods": [
                    "ActiveStandbyElector.zkDoWithRetries",
                    "ActiveStandbyElector.createWithRetries",
                    "ActiveStandbyElector.ensureParentZNode",
                    "ActiveStandbyElectorBasedElectorService.serviceInit"
                ]
            },
            "possible_fix": "Add a check to ensure that the ZooKeeper connection is established before proceeding with operations that depend on it. For example, modify the 'zkDoWithRetries' method to handle connection failures more gracefully and avoid invoking methods that assume a valid connection."
        },
        "possible_fix_code": {
            "ActiveStandbyElector.zkDoWithRetries": "  private <T> T zkDoWithRetries(ZKAction<T> action, Code retryCode)\n      throws KeeperException, InterruptedException {\n    int retry = 0;\n    while (true) {\n      try {\n        // Check if zkClient is null before proceeding\n        if (zkClient == null) {\n          throw new KeeperException.ConnectionLossException();\n        }\n        return action.run();\n      } catch (KeeperException ke) {\n        if ((shouldRetry(ke.code()) || shouldRetry(ke.code(), retryCode))\n            && ++retry < maxRetryNum) {\n          continue;\n        }\n        throw ke;\n      }\n    }\n  }"
        }
    },
    {
        "filename": "YARN-8223.json",
        "creation_time": "2018-04-27T11:49:02.000+0000",
        "bug_report": {
            "Title": "ClassNotFoundException when auxiliary service is loaded from HDFS",
            "Description": "The application encounters a ClassNotFoundException when attempting to load an auxiliary service jar from HDFS. This issue arises because the classpath is empty when the jar is loaded from HDFS, preventing the application from locating the required class. In contrast, loading the jar from a local path works correctly, as the classpath is populated appropriately.",
            "StackTrace": [
                "java.lang.ClassNotFoundException: org.apache.auxtest.AuxServiceFromLocal",
                "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)",
                "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)",
                "\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)",
                "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)",
                "\tat org.apache.hadoop.util.ApplicationClassLoader.loadClass(ApplicationClassLoader.java:189)",
                "\tat org.apache.hadoop.util.ApplicationClassLoader.loadClass(ApplicationClassLoader.java:157)",
                "\tat java.lang.Class.forName0(Native Method)",
                "\tat java.lang.Class.forName(Class.java:348)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader.getInstance(AuxiliaryServiceWithCustomClassLoader.java:169)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:249)",
                "\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "\tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:316)",
                "\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "\tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:472)",
                "\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:918)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:979)"
            ],
            "RootCause": "The root cause of the ClassNotFoundException is that the application is not correctly setting the classpath when loading the auxiliary service jar from HDFS. The classpath is empty, which prevents the ApplicationClassLoader from finding the specified class.",
            "StepsToReproduce": [
                "Configure an auxiliary service to load from HDFS.",
                "Start the NodeManager.",
                "Observe the logs for ClassNotFoundException related to the auxiliary service."
            ],
            "ExpectedBehavior": "The auxiliary service should load successfully from HDFS without any ClassNotFoundException, and the classpath should include the path to the jar file.",
            "ObservedBehavior": "The application throws a ClassNotFoundException indicating that the class cannot be found because the classpath is empty when loading from HDFS.",
            "Suggestions": "Ensure that the configuration for the auxiliary service includes the correct remote classpath for HDFS. Verify that the application correctly populates the classpath when loading from HDFS.",
            "problem_location": {
                "files": [
                    "AuxiliaryServiceWithCustomClassLoader.java",
                    "AuxServices.java",
                    "ApplicationClassLoader.java",
                    "NodeManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices",
                    "org.apache.hadoop.util.ApplicationClassLoader",
                    "org.apache.hadoop.yarn.server.nodemanager.NodeManager"
                ],
                "methods": [
                    "AuxiliaryServiceWithCustomClassLoader.getInstance",
                    "AuxServices.serviceInit",
                    "ApplicationClassLoader.loadClass",
                    "NodeManager.serviceInit"
                ]
            },
            "possible_fix": "In the AuxServices.serviceInit method, ensure that the appRemoteClassPath is correctly set and passed to the AuxiliaryServiceWithCustomClassLoader.getInstance method. Additionally, verify that the configuration for the auxiliary service includes the correct path to the jar file in HDFS."
        },
        "possible_fix_code": {
            "AuxServices.serviceInit": "  public void serviceInit(Configuration conf) throws Exception {\n    final FsPermission storeDirPerms = new FsPermission((short)0700);\n    Path stateStoreRoot = null;\n    FileSystem stateStoreFs = null;\n    boolean recoveryEnabled = conf.getBoolean(\n        YarnConfiguration.NM_RECOVERY_ENABLED,\n        YarnConfiguration.DEFAULT_NM_RECOVERY_ENABLED);\n    if (recoveryEnabled) {\n      stateStoreRoot = new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR),\n          STATE_STORE_ROOT_NAME);\n      stateStoreFs = FileSystem.getLocal(conf);\n    }\n    Collection<String> auxNames = conf.getStringCollection(\n        YarnConfiguration.NM_AUX_SERVICES);\n    for (final String sName : auxNames) {\n      try {\n        Preconditions\n            .checkArgument(\n                validateAuxServiceName(sName),\n                \"The ServiceName: \" + sName + \" set in \" +\n                YarnConfiguration.NM_AUX_SERVICES +\" is invalid.\" +\n                \"The valid service name should only contain a-zA-Z0-9_ \" +\n                \"and can not start with numbers\");\n        String classKey = String.format(\n            YarnConfiguration.NM_AUX_SERVICE_FMT, sName);\n        String className = conf.get(classKey);\n        final String appLocalClassPath = conf.get(String.format(\n            YarnConfiguration.NM_AUX_SERVICES_CLASSPATH, sName));\n        final String appRemoteClassPath = conf.get(String.format(\n            YarnConfiguration.NM_AUX_SERVICE_REMOTE_CLASSPATH, sName));\n        AuxiliaryService s = null;\n        boolean useCustomerClassLoader = ((appLocalClassPath != null\n            && !appLocalClassPath.isEmpty()) ||\n            (appRemoteClassPath != null && !appRemoteClassPath.isEmpty()))\n            && className != null && !className.isEmpty();\n        if (useCustomerClassLoader) {\n          // load AuxiliaryService from local class path\n          if (appRemoteClassPath == null || appRemoteClassPath.isEmpty()) {\n            s = AuxiliaryServiceWithCustomClassLoader.getInstance(\n                conf, className, appLocalClassPath);\n          } else {\n            // load AuxiliaryService from remote class path\n            if (appLocalClassPath != null && !appLocalClassPath.isEmpty()) {\n              throw new YarnRuntimeException(\"The aux service:\" + sName\n                  + \" has configured local classpath:\" + appLocalClassPath\n                  + \" and remote classpath:\" + appRemoteClassPath\n                  + \". Only one of them should be configured.\");\n            }\n            FileContext localLFS = getLocalFileContext(conf);\n            // create NM aux-service dir in NM localdir if it does not exist.\n            Path nmAuxDir = dirsHandler.getLocalPathForWrite(\".\"\n                + Path.SEPARATOR + NM_AUX_SERVICE_DIR);\n            if (!localLFS.util().exists(nmAuxDir)) {\n              try {\n                localLFS.mkdir(nmAuxDir, NM_AUX_SERVICE_DIR_PERM, true);\n              } catch (IOException ex) {\n                throw new YarnRuntimeException(\"Fail to create dir:\"\n                    + nmAuxDir.toString(), ex);\n              }\n            }\n            Path src = new Path(appRemoteClassPath);\n            FileContext remoteLFS = getRemoteFileContext(src.toUri(), conf);\n            FileStatus scFileStatus = remoteLFS.getFileStatus(src);\n            if (!scFileStatus.getOwner().equals(\n                this.userUGI.getShortUserName())) {\n              throw new YarnRuntimeException(\"The remote jarfile owner:\"\n                  + scFileStatus.getOwner() + \" is not the same as the NM user:\"\n                  + this.userUGI.getShortUserName() + \".\");\n            }\n            if ((scFileStatus.getPermission().toShort() & 0022) != 0) {\n              throw new YarnRuntimeException(\"The remote jarfile should not \"\n                  + \"be writable by group or others. \"\n                  + \"The current Permission is \"\n                  + scFileStatus.getPermission().toShort());\n            }\n            Path dest = null;\n            Path downloadDest = new Path(nmAuxDir,\n                className + \"_\" + scFileStatus.getModificationTime());\n            // check whether we need to re-download the jar\n            // from remote directory\n            Path targetDirPath = new Path(downloadDest,\n                scFileStatus.getPath().getName());\n            FileStatus[] allSubDirs = localLFS.util().listStatus(nmAuxDir);\n            boolean reDownload = true;\n            for (FileStatus sub : allSubDirs) {\n              if (sub.getPath().getName().equals(downloadDest.getName())) {\n                reDownload = false;\n                dest = new Path(targetDirPath + Path.SEPARATOR + \"*\");\n                break;\n              } else {\n                if (sub.getPath().getName().contains(className) &&\n                    !sub.getPath().getName().endsWith(DEL_SUFFIX)) {\n                  Path delPath = new Path(sub.getPath().getParent(),\n                      sub.getPath().getName() + DEL_SUFFIX);\n                  localLFS.rename(sub.getPath(), delPath);\n                  LOG.info(\"delete old aux service jar dir:\"\n                      + delPath.toString());\n                  FileDeletionTask deletionTask = new FileDeletionTask(\n                      this.delService, null, delPath, null);\n                  this.delService.delete(deletionTask);\n                }\n              }\n            }\n            if (reDownload) {\n              LocalResource scRsrc = LocalResource.newInstance(\n                  URL.fromURI(src.toUri()),\n                  LocalResourceType.ARCHIVE, LocalResourceVisibility.PRIVATE,\n                  scFileStatus.getLen(), scFileStatus.getModificationTime());\n              FSDownload download = new FSDownload(localLFS, null, conf,\n                  downloadDest, scRsrc, null);\n              try {\n                Path downloaded = download.call();\n                dest = new Path(downloaded + Path.SEPARATOR + \"*\");\n              } catch (Exception ex) {\n                throw new YarnRuntimeException(\n                    \"Exception happened while downloading files \"\n                    + \"for aux-service:\" + sName + \" and remote-file-path:\"\n                    + src + \".\\n\" + ex.getMessage());\n              }\n            }\n            s = AuxiliaryServiceWithCustomClassLoader.getInstance(\n                conf, className, dest.toString());\n          }\n          LOG.info(\"The aux service:\" + sName\n              + \" are using the custom classloader\");\n        } else {\n          Class<? extends AuxiliaryService> sClass = conf.getClass(\n              classKey, null, AuxiliaryService.class);\n\n          if (sClass == null) {\n            throw new RuntimeException(\"No class defined for \" + sName);\n          }\n          s = ReflectionUtils.newInstance(sClass, conf);\n        }\n        if (s == null) {\n          throw new RuntimeException(\"No object created for \" + sName);\n        }\n        // TODO better use s.getName()?\n        if(!sName.equals(s.getName())) {\n          LOG.warn(\"The Auxiliary Service named '\"+sName+\"' in the \"\n              +\"configuration is for \"+s.getClass()+\" which has \"\n              +\"a name of '\"+s.getName()+\"'. Because these are \"\n              +\"not the same tools trying to send ServiceData and read \"\n              +\"Service Meta Data may have issues unless the refer to \"\n              +\"the name in the config.\");\n        }\n        s.setAuxiliaryLocalPathHandler(auxiliaryLocalPathHandler);\n        addService(sName, s);\n        if (recoveryEnabled) {\n          Path storePath = new Path(stateStoreRoot, sName);\n          stateStoreFs.mkdirs(storePath, storeDirPerms);\n          s.setRecoveryPath(storePath);\n        }\n        s.init(conf);\n      } catch (RuntimeException e) {\n        LOG.error(\"Failed to initialize \" + sName, e);\n        throw e;\n      }\n    }\n    super.serviceInit(conf);\n  }"
        }
    },
    {
        "filename": "YARN-8331.json",
        "creation_time": "2018-05-21T05:19:35.000+0000",
        "bug_report": {
            "Title": "Race condition in NM container launched after done",
            "Description": "A race condition occurs when a container is launched and its state transitions from SCHEDULED to KILLING and then to DONE before the CONTAINER_LAUNCHED event is processed. This results in the container processes not being cleaned up properly, leading to potential resource leaks and inconsistent application states.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: CONTAINER_LAUNCHED at DONE",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:2104)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:104)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1525)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1518)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The root cause of the issue is that the container's state transitions are not properly synchronized, allowing the CONTAINER_LAUNCHED event to be processed after the container has already transitioned to the DONE state, which is an invalid state for that event.",
            "StepsToReproduce": [
                "1. Launch a container using the ContainerLaunch#launchContainer method.",
                "2. Send a kill event to the container while it is in the SCHEDULED state.",
                "3. Observe the state transitions and the handling of the CONTAINER_LAUNCHED event."
            ],
            "ExpectedBehavior": "The container should properly handle the CONTAINER_LAUNCHED event even if it has transitioned to the DONE state, ensuring that all resources are cleaned up correctly.",
            "ObservedBehavior": "The system throws an InvalidStateTransitionException when trying to handle the CONTAINER_LAUNCHED event after the container has transitioned to the DONE state.",
            "Suggestions": "Implement proper synchronization mechanisms to ensure that state transitions are atomic and that events are processed in the correct order. Consider adding checks to prevent invalid state transitions.",
            "problem_location": {
                "files": [
                    "ContainerImpl.java",
                    "StateMachineFactory.java",
                    "ContainerManagerImpl.java",
                    "AsyncDispatcher.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl",
                    "org.apache.hadoop.yarn.event.AsyncDispatcher"
                ],
                "methods": [
                    "ContainerImpl.handle",
                    "StateMachineFactory.doTransition",
                    "ContainerManagerImpl$ContainerEventDispatcher.handle",
                    "AsyncDispatcher.dispatch"
                ]
            },
            "possible_fix": "To fix this issue, ensure that the state transitions in the ContainerImpl class are synchronized. Modify the handle method to check the current state before processing events, and prevent the CONTAINER_LAUNCHED event from being processed if the container is already in the DONE state. Additionally, consider implementing a queue for events that can be processed once the container is in a valid state."
        },
        "possible_fix_code": {
            "ContainerImpl.handle": "  public void handle(ContainerEvent event) {\n    try {\n      this.writeLock.lock();\n\n      ContainerId containerID = event.getContainerID();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Processing \" + containerID + \" of type \" + event.getType());\n      }\n      ContainerState oldState = stateMachine.getCurrentState();\n      ContainerState newState = null;\n\n      // Check if the current state is DONE before processing the event\n      if (oldState == ContainerState.DONE && event.getType() == ContainerEventType.CONTAINER_LAUNCHED) {\n        LOG.warn(\"Ignoring CONTAINER_LAUNCHED event for container \" + containerID + \" as it is already in DONE state.\");\n        return;\n      }\n\n      try {\n        newState = stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitionException e) {\n        LOG.warn(\"Can't handle this event at current state: Current: [\" + oldState + \"], eventType: [\" + event.getType() + \"], container: [\" + containerID + \"]\", e);\n      }\n      if (newState != null && oldState != newState) {\n        LOG.info(\"Container \" + containerID + \" transitioned from \" + oldState + \" to \" + newState);\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-2931.json",
        "creation_time": "2014-12-08T21:09:13.000+0000",
        "bug_report": {
            "Title": "PublicLocalizer may fail until directory is initialized by LocalizeRunner",
            "Description": "The issue arises when the data directory is cleaned up and the NodeManager (NM) is started with an existing recovery state. Due to YARN-90, the local directories are not recreated, leading to a failure in the PublicLocalizer until the getInitializedLocalDirs method is invoked by a LocalizeRunner for private localization. This results in a FileNotFoundException when attempting to access the file cache directory.",
            "StackTrace": [
                "java.io.FileNotFoundException: File /data/yarn/nm/filecache does not exist",
                "at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:524)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:737)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:514)",
                "at org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:1051)",
                "at org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:162)",
                "at org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:197)",
                "at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:724)",
                "at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:720)",
                "at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)",
                "at org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:720)",
                "at org.apache.hadoop.yarn.util.FSDownload.createDir(FSDownload.java:104)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:351)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:60)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the local directories required for the PublicLocalizer are not created when the NodeManager starts with an existing recovery state, leading to a FileNotFoundException when attempting to access the file cache.",
            "StepsToReproduce": [
                "Clean up the data directory.",
                "Start the NodeManager with an existing recovery state.",
                "Attempt to localize resources that require the PublicLocalizer."
            ],
            "ExpectedBehavior": "The PublicLocalizer should successfully create the necessary local directories and localize resources without throwing a FileNotFoundException.",
            "ObservedBehavior": "The PublicLocalizer fails to localize resources and throws a FileNotFoundException indicating that the file cache does not exist.",
            "Suggestions": "Ensure that the local directories are created during the initialization of the NodeManager, especially when starting with an existing recovery state. This may involve modifying the LocalizeRunner to check and create the necessary directories before attempting to localize resources.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/RawLocalFileSystem.java",
                    "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/FSDownload.java"
                ],
                "classes": [
                    "org.apache.hadoop.fs.RawLocalFileSystem",
                    "org.apache.hadoop.yarn.util.FSDownload"
                ],
                "methods": [
                    "RawLocalFileSystem.deprecatedGetFileStatus",
                    "FSDownload.createDir"
                ]
            },
            "possible_fix": "Modify the initialization logic in the NodeManager to ensure that the local directories are created if they do not exist. This can be done by adding a check in the LocalizeRunner to call getInitializedLocalDirs before attempting to localize resources."
        },
        "possible_fix_code": {
            "FSDownload.createDir": "  private void createDir(Path path, FsPermission perm) throws IOException {\n    // Ensure the directory exists before creating it\n    if (!files.exists(path)) {\n        files.mkdir(path, perm, false);\n    }\n    if (!perm.equals(files.getUMask().applyUMask(perm))) {\n      files.setPermission(path, perm);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-6837.json",
        "creation_time": "2017-07-18T11:17:55.000+0000",
        "bug_report": {
            "Title": "Null LocalResource visibility or resource type can crash the NodeManager",
            "Description": "The NodeManager crashes due to a NullPointerException when a LocalResource's visibility is set to null. This occurs during the resource addition process in the ResourceSet class, where the visibility is checked. If the visibility is null, the NodeManager fails to handle the resource correctly, leading to a crash.",
            "StackTrace": [
                "2017-07-18 17:54:09,292 FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet.addResources(ResourceSet.java:84)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition.transition(ContainerImpl.java:868)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition.transition(ContainerImpl.java:819)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1684)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:96)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1418)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1411)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the LocalResource's visibility is not set, leading to a NullPointerException in the ResourceSet.addResources method when it attempts to process the resource.",
            "StepsToReproduce": [
                "Create a LocalResource without setting its visibility.",
                "Submit a YARN application that uses this LocalResource.",
                "Observe the NodeManager logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The NodeManager should handle LocalResources with a null visibility gracefully, without crashing.",
            "ObservedBehavior": "The NodeManager crashes with a NullPointerException when a LocalResource's visibility is null.",
            "Suggestions": "Ensure that the visibility of LocalResources is always set to a valid value (PUBLIC, PRIVATE, or APPLICATION) before submission. Implement validation checks to prevent null visibility.",
            "problem_location": {
                "files": [
                    "ResourceSet.java",
                    "ContainerImpl.java",
                    "StateMachineFactory.java",
                    "AsyncDispatcher.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory",
                    "org.apache.hadoop.yarn.event.AsyncDispatcher"
                ],
                "methods": [
                    "ResourceSet.addResources",
                    "ContainerImpl.handle",
                    "StateMachineFactory.doTransition",
                    "AsyncDispatcher.dispatch"
                ]
            },
            "possible_fix": "Add a validation check in the ResourceSet.addResources method to ensure that the visibility of each LocalResource is not null before processing it. If it is null, throw an appropriate exception or handle it gracefully."
        },
        "possible_fix_code": {
            "ResourceSet.addResources": "  public Map<LocalResourceVisibility, Collection<LocalResourceRequest>>\n      addResources(Map<String, LocalResource> localResourceMap)\n      throws URISyntaxException {\n    if (localResourceMap == null || localResourceMap.isEmpty()) {\n      return null;\n    }\n    Map<LocalResourceRequest, Set<String>> allResources = new HashMap<>();\n    List<LocalResourceRequest> publicList = new ArrayList<>();\n    List<LocalResourceRequest> privateList = new ArrayList<>();\n    List<LocalResourceRequest> appList = new ArrayList<>();\n\n    for (Map.Entry<String, LocalResource> rsrc : localResourceMap.entrySet()) {\n      LocalResource resource = rsrc.getValue();\n      if (resource.getVisibility() == null) {\n        throw new IllegalArgumentException(\"LocalResource visibility cannot be null\");\n      }\n      LocalResourceRequest req = new LocalResourceRequest(resource);\n      allResources.putIfAbsent(req, new HashSet<>());\n      allResources.get(req).add(rsrc.getKey());\n      storeSharedCacheUploadPolicy(req,\n          resource.getShouldBeUploadedToSharedCache());\n      switch (resource.getVisibility()) {\n      case PUBLIC:\n        publicList.add(req);\n        break;\n      case PRIVATE:\n        privateList.add(req);\n        break;\n      case APPLICATION:\n        appList.add(req);\n        break;\n      default:\n        break;\n      }\n    }\n    Map<LocalResourceVisibility, Collection<LocalResourceRequest>> req =\n        new LinkedHashMap<>();\n    if (!publicList.isEmpty()) {\n      publicRsrcs.addAll(publicList);\n      req.put(LocalResourceVisibility.PUBLIC, publicList);\n    }\n    if (!privateList.isEmpty()) {\n      privateRsrcs.addAll(privateList);\n      req.put(LocalResourceVisibility.PRIVATE, privateList);\n    }\n    if (!appList.isEmpty()) {\n      appRsrcs.addAll(appList);\n      req.put(LocalResourceVisibility.APPLICATION, appList);\n    }\n    if (!allResources.isEmpty()) {\n      this.pendingResources.putAll(allResources);\n    }\n    return req;\n  }"
        }
    },
    {
        "filename": "YARN-4762.json",
        "creation_time": "2016-03-04T02:24:47.000+0000",
        "bug_report": {
            "Title": "NMs failing on DelegatingLinuxContainerRuntime init with LCE on",
            "Description": "The NodeManager is crashing during initialization due to an inability to set up the Linux container executor. The logs indicate that the cgroups handle could not be obtained because the controller mount point for 'cpu' is not writable. This issue arises when the NodeManager attempts to initialize the LinuxContainerExecutor, which is essential for managing container execution in a YARN environment.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to initialize container executor",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:240)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:539)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:587)",
                "Caused by: java.io.IOException: Failed to initialize linux container runtime(s)!",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.init(LinuxContainerExecutor.java:207)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:238)"
            ],
            "RootCause": "The root cause of the issue is that the cgroups controller mount point for 'cpu' is not writable, which prevents the LinuxContainerExecutor from initializing properly.",
            "StepsToReproduce": [
                "Start the NodeManager with the LinuxContainerExecutor configured.",
                "Ensure that the cgroups are set up correctly on the system.",
                "Check the permissions of the cgroups mount points."
            ],
            "ExpectedBehavior": "The NodeManager should initialize successfully without crashing, and the Linux container executor should be set up correctly.",
            "ObservedBehavior": "The NodeManager fails to start, throwing a YarnRuntimeException due to the failure in initializing the container executor.",
            "Suggestions": "Check the permissions of the cgroups mount points and ensure that they are writable. Verify that the cgroups are correctly configured on the system.",
            "problem_location": {
                "files": [
                    "LinuxContainerExecutor.java",
                    "NodeManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",
                    "org.apache.hadoop.yarn.server.nodemanager.NodeManager"
                ],
                "methods": [
                    "LinuxContainerExecutor.init",
                    "NodeManager.serviceInit"
                ]
            },
            "possible_fix": "Ensure that the cgroups are properly mounted and writable. This may involve modifying system configurations or permissions related to cgroups. Additionally, verify that the LinuxContainerExecutor is correctly configured in the YARN settings."
        },
        "possible_fix_code": {
            "LinuxContainerExecutor.init": "  public void init() throws IOException {\n    Configuration conf = super.getConf();\n\n    // Send command to executor which will just start up,\n    // verify configuration/permissions and exit\n    try {\n      PrivilegedOperation checkSetupOp = new PrivilegedOperation(\n          PrivilegedOperation.OperationType.CHECK_SETUP, (String) null);\n      PrivilegedOperationExecutor privilegedOperationExecutor =\n          PrivilegedOperationExecutor.getInstance(conf);\n\n      privilegedOperationExecutor.executePrivilegedOperation(checkSetupOp,\n          false);\n    } catch (PrivilegedOperationException e) {\n      int exitCode = e.getExitCode();\n      LOG.warn(\"Exit code from container executor initialization is : \"\n          + exitCode, e);\n\n      throw new IOException(\"Linux container executor not configured properly\"\n          + \" (error=\" + exitCode + \")\", e);\n    }\n\n    // Check if cgroups are writable\n    if (!isCgroupsWritable(conf)) {\n      throw new IOException(\"Cgroups mount point for 'cpu' is not writable.\");\n    }\n\n    try {\n      resourceHandlerChain = ResourceHandlerModule\n          .getConfiguredResourceHandlerChain(conf);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Resource handler chain enabled = \" + (resourceHandlerChain\n            == null));\n      }\n      if (resourceHandlerChain != null) {\n        LOG.debug(\"Bootstrapping resource handler chain\");\n        resourceHandlerChain.bootstrap(conf);\n      }\n    } catch (ResourceHandlerException e) {\n      LOG.error(\"Failed to bootstrap configured resource subsystems! \", e);\n      throw new IOException(\n          \"Failed to bootstrap configured resource subsystems!\");\n    }\n\n    try {\n      if (linuxContainerRuntime == null) {\n        LinuxContainerRuntime runtime = new DelegatingLinuxContainerRuntime();\n\n        runtime.initialize(conf);\n        this.linuxContainerRuntime = runtime;\n      }\n    } catch (ContainerExecutionException e) {\n      throw new IOException(\"Failed to initialize linux container runtime(s)!\", e);\n    }\n\n    resourcesHandler.init(this);\n  }\n\n  private boolean isCgroupsWritable(Configuration conf) {\n    // Implement logic to check if cgroups are writable\n    // This is a placeholder for actual implementation\n    return true; // Assume writable for now\n  }"
        }
    },
    {
        "filename": "YARN-2823.json",
        "creation_time": "2014-11-06T21:38:47.000+0000",
        "bug_report": {
            "Title": "NullPointerException in RM HA enabled 3-node cluster",
            "Description": "In a 3-node cluster with ResourceManager High Availability (HA) enabled, a NullPointerException (NPE) occurs when handling the event type APP_ATTEMPT_ADDED. This issue arises after the ResourceManagers go down and fail to recover, leading to a failure in transferring the state from a previous application attempt. The NPE is logged in the ResourceManager logs, indicating that the application attempt is not properly initialized or is missing required state information.",
            "StackTrace": [
                "2014-09-16 01:36:28,037 FATAL resourcemanager.ResourceManager (ResourceManager.java:run(612)) - Error in handling event type APP_ATTEMPT_ADDED to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt.transferStateFromPreviousAttempt(SchedulerApplicationAttempt.java:530)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:678)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1015)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:98)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:603)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The root cause of the NullPointerException is likely due to the 'appAttempt' parameter being null when the method 'transferStateFromPreviousAttempt' is called. This can occur if the current application attempt does not have a valid previous attempt to transfer state from, especially after a failure in the ResourceManager.",
            "StepsToReproduce": [
                "Set up a 3-node cluster with ResourceManager HA enabled.",
                "Install HBase using Slider.",
                "Allow the ResourceManagers to go down.",
                "Attempt to recover the ResourceManagers and observe the logs for NPE."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the APP_ATTEMPT_ADDED event without throwing a NullPointerException, successfully transferring the state from the previous application attempt if it exists.",
            "ObservedBehavior": "A NullPointerException is thrown in the ResourceManager logs when handling the APP_ATTEMPT_ADDED event, causing the ResourceManager to exit unexpectedly.",
            "Suggestions": "Implement a null check for the 'appAttempt' parameter in the 'transferStateFromPreviousAttempt' method to prevent the NullPointerException. Additionally, ensure that the application attempt is properly initialized before attempting to transfer state.",
            "problem_location": {
                "files": [
                    "SchedulerApplicationAttempt.java",
                    "CapacityScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"
                ],
                "methods": [
                    "SchedulerApplicationAttempt.transferStateFromPreviousAttempt",
                    "CapacityScheduler.addApplicationAttempt"
                ]
            },
            "possible_fix": "In the 'transferStateFromPreviousAttempt' method, add a null check for 'appAttempt':\n\npublic synchronized void transferStateFromPreviousAttempt(SchedulerApplicationAttempt appAttempt) {\n    if (appAttempt == null) {\n        LOG.error(\"Cannot transfer state from a null application attempt.\");\n        return;\n    }\n    this.liveContainers = appAttempt.getLiveContainersMap();\n    this.currentConsumption = appAttempt.getCurrentConsumption();\n    this.resourceLimit = appAttempt.getResourceLimit();\n    this.lastScheduledContainer = appAttempt.getLastScheduledContainer();\n    this.appSchedulingInfo.transferStateFromPreviousAppSchedulingInfo(appAttempt.appSchedulingInfo);\n}"
        },
        "possible_fix_code": {
            "SchedulerApplicationAttempt.transferStateFromPreviousAttempt": "  public synchronized void transferStateFromPreviousAttempt(\n      SchedulerApplicationAttempt appAttempt) {\n    if (appAttempt == null) {\n        LOG.error(\"Cannot transfer state from a null application attempt.\");\n        return;\n    }\n    this.liveContainers = appAttempt.getLiveContainersMap();\n    this.currentConsumption = appAttempt.getCurrentConsumption();\n    this.resourceLimit = appAttempt.getResourceLimit();\n    this.lastScheduledContainer = appAttempt.getLastScheduledContainer();\n    this.appSchedulingInfo\n      .transferStateFromPreviousAppSchedulingInfo(appAttempt.appSchedulingInfo);\n  }"
        }
    },
    {
        "filename": "YARN-5098.json",
        "creation_time": "2016-05-17T00:43:08.000+0000",
        "bug_report": {
            "Title": "Yarn Application Log Aggregation Fails Due to NM Unable to Retrieve HDFS Delegation Token",
            "Description": "In a high-availability (HA) cluster environment, the NodeManager (NM) fails to aggregate logs for long-running Yarn applications due to an inability to retrieve the correct HDFS delegation token. The error message indicates that the specified HDFS delegation token cannot be found in the cache, leading to a failure in establishing a connection to HDFS for log aggregation.",
            "StackTrace": [
                "2016-05-16 18:18:28,533 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:finishLogAggregation(555)) - Application just finished : application_1463170334122_0002",
                "2016-05-16 18:18:28,545 WARN  ipc.Client (Client.java:run(705)) - Exception encountered while connecting to the server :",
                "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 171 for hrt_qa) can't be found in cache"
            ],
            "RootCause": "The root cause of the issue is that the HDFS delegation token is not being cached or has expired, which prevents the NodeManager from successfully authenticating with HDFS to perform log aggregation.",
            "StepsToReproduce": [
                "1. Set up a high-availability (HA) Yarn cluster.",
                "2. Submit a long-running Yarn application.",
                "3. Wait for the application to finish and observe the log aggregation process.",
                "4. Check the NodeManager logs for errors related to HDFS delegation tokens."
            ],
            "ExpectedBehavior": "The NodeManager should successfully retrieve the HDFS delegation token and aggregate logs for the completed Yarn application without any errors.",
            "ObservedBehavior": "The NodeManager fails to retrieve the HDFS delegation token, resulting in an error message indicating that the token cannot be found in the cache, and log aggregation does not occur.",
            "Suggestions": "Ensure that the HDFS delegation tokens are properly cached and not expired. Consider implementing a mechanism to refresh tokens before they expire or to handle token renewal more effectively.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcClient.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.SaslRpcClient",
                    "org.apache.hadoop.ipc.Client",
                    "org.apache.hadoop.io.retry.RetryInvocationHandler"
                ],
                "methods": [
                    "SaslRpcClient.saslConnect",
                    "Client.call",
                    "RetryInvocationHandler.invoke"
                ]
            },
            "possible_fix": "To address the issue, review the token caching mechanism in the HDFS configuration. Ensure that the NodeManager is correctly handling token expiration and renewal. Additionally, consider adding logging to track token retrieval and caching processes to identify potential failures."
        },
        "possible_fix_code": {
            "Client.call": "  Writable call(RPC.RpcKind rpcKind, Writable rpcRequest,\n      ConnectionId remoteId, int serviceClass,\n      AtomicBoolean fallbackToSimpleAuth) throws IOException {\n    final Call call = createCall(rpcKind, rpcRequest);\n    final Connection connection = getConnection(remoteId, call, serviceClass,\n        fallbackToSimpleAuth);\n\n    try {\n      checkAsyncCall();\n      try {\n        connection.sendRpcRequest(call);                 // send the rpc request\n      } catch (RejectedExecutionException e) {\n        throw new IOException(\"connection has been closed\", e);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n        LOG.warn(\"interrupted waiting to send rpc request to server\", e);\n        throw new IOException(e);\n      }\n    } catch(Exception e) {\n      if (isAsynchronousMode()) {\n        releaseAsyncCall();\n      }\n      throw e;\n    }\n\n    if (isAsynchronousMode()) {\n      Future<Writable> returnFuture = new AbstractFuture<Writable>() {\n        private final AtomicBoolean callled = new AtomicBoolean(false);\n        @Override\n        public Writable get() throws InterruptedException, ExecutionException {\n          if (callled.compareAndSet(false, true)) {\n            try {\n              set(getRpcResponse(call, connection));\n            } catch (IOException ie) {\n              setException(ie);\n            } finally {\n              releaseAsyncCall();\n            }\n          }\n          return super.get();\n        }\n      };\n\n      RETURN_RPC_RESPONSE.set(returnFuture);\n      return null;\n    } else {\n      // Check if the HDFS delegation token is valid before proceeding\n      if (!isTokenValid()) {\n        refreshHdfsDelegationToken(); // Implement this method to refresh the token\n      }\n      return getRpcResponse(call, connection);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-3971.json",
        "creation_time": "2015-07-24T10:17:05.000+0000",
        "bug_report": {
            "Title": "Skip RMNodeLabelsManager#checkRemoveFromClusterNodeLabelsOfQueue on nodelabel recovery",
            "Description": "The ResourceManager (RM) fails to transition to active state due to an IOException thrown during the recovery process of node labels. This occurs when attempting to remove a node label that is still in use by a queue, specifically when the label is being removed after it has been deleted and recreated. The exception indicates that the queue is still referencing the label, which prevents the removal operation from succeeding.",
            "StackTrace": [
                "java.io.IOException: Cannot remove label=x, because queue=a1 is using this label. Please remove label on queue before remove the label",
                "at org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager.checkRemoveFromClusterNodeLabelsOfQueue(RMNodeLabelsManager.java:104)",
                "at org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager.removeFromClusterNodeLabels(RMNodeLabelsManager.java:118)",
                "at org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore.recover(FileSystemNodeLabelsStore.java:221)",
                "at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.initNodeLabelStore(CommonNodeLabelsManager.java:232)",
                "at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.serviceStart(CommonNodeLabelsManager.java:245)"
            ],
            "RootCause": "The root cause of the issue is that the RMNodeLabelsManager's method checkRemoveFromClusterNodeLabelsOfQueue is throwing an IOException because the label being removed is still associated with an active queue. This check is performed during the recovery process when the ResourceManager attempts to restore the state of node labels.",
            "StepsToReproduce": [
                "Create node labels x and y.",
                "Delete node labels x and y.",
                "Recreate node labels x and y and update the capacity scheduler XML to include these labels.",
                "Restart the ResourceManager."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully transition to the active state without throwing an exception related to node label removal.",
            "ObservedBehavior": "The ResourceManager fails to transition to active state, throwing an IOException indicating that the label cannot be removed because it is still in use by a queue.",
            "Suggestions": "Ensure that all queues using the node labels are updated or cleared before attempting to remove the labels during the recovery process. This may involve modifying the recovery logic to check for active queues before attempting to remove labels.",
            "problem_location": {
                "files": [
                    "RMNodeLabelsManager.java",
                    "FileSystemNodeLabelsStore.java",
                    "CommonNodeLabelsManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager",
                    "org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore",
                    "org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager"
                ],
                "methods": [
                    "RMNodeLabelsManager.checkRemoveFromClusterNodeLabelsOfQueue",
                    "RMNodeLabelsManager.removeFromClusterNodeLabels",
                    "FileSystemNodeLabelsStore.recover",
                    "CommonNodeLabelsManager.initNodeLabelStore"
                ]
            },
            "possible_fix": "Modify the checkRemoveFromClusterNodeLabelsOfQueue method to include a check for the state of the queues before attempting to remove the labels. Additionally, ensure that the recovery process handles the case where labels are still in use by queues, possibly by deferring the removal until the queues are updated."
        },
        "possible_fix_code": {
            "RMNodeLabelsManager.checkRemoveFromClusterNodeLabelsOfQueue": "  protected void checkRemoveFromClusterNodeLabelsOfQueue(\n      Collection<String> labelsToRemove) throws IOException {\n    // Check if label to remove doesn't exist or is null/empty, will throw\n    // exception if any of labels to remove doesn't meet requirement\n    for (String label : labelsToRemove) {\n      label = normalizeLabel(label);\n\n      // check if any queue contains this label\n      for (Entry<String, Queue> entry : queueCollections.entrySet()) {\n        String queueName = entry.getKey();\n        Set<String> queueLabels = entry.getValue().accessibleNodeLabels;\n        if (queueLabels.contains(label)) {\n          // Log a warning before throwing an exception\n          LOG.warn(\"Attempting to remove label \" + label + \" that is still in use by queue \" + queueName);\n          throw new IOException(\"Cannot remove label=\" + label\n              + \", because queue=\" + queueName + \" is using this label. \"\n              + \"Please remove label on queue before remove the label\");\n        }\n      }\n    }\n  }"
        }
    },
    {
        "filename": "YARN-6948.json",
        "creation_time": "2017-08-04T08:23:46.000+0000",
        "bug_report": {
            "Title": "Invalid event: ATTEMPT_ADDED at FINAL_SAVING",
            "Description": "The system encounters an InvalidStateTransitionException when attempting to process an ATTEMPT_ADDED event while in the FINAL_SAVING state. This indicates that the state machine does not allow this transition, leading to an error in the resource manager's application attempt handling.",
            "StackTrace": [
                "2017-08-03 01:35:20,485 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: ATTEMPT_ADDED at FINAL_SAVING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:757)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:834)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:815)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The application attempt is in the FINAL_SAVING state, which does not permit the ATTEMPT_ADDED event to be processed, resulting in an InvalidStateTransitionException.",
            "StepsToReproduce": [
                "1. Start a job in the resource manager.",
                "2. Send a kill command to the running job.",
                "3. Check the logs for errors related to event handling."
            ],
            "ExpectedBehavior": "The system should gracefully handle the ATTEMPT_ADDED event or ignore it if the application is in a state that does not allow this transition.",
            "ObservedBehavior": "The system throws an InvalidStateTransitionException, indicating it cannot handle the ATTEMPT_ADDED event while in the FINAL_SAVING state.",
            "Suggestions": "Review the state machine transitions to ensure that ATTEMPT_ADDED can be handled appropriately in the FINAL_SAVING state or modify the event handling logic to prevent this event from being dispatched in this state.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/state/StateMachineFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory"
                ],
                "methods": [
                    "RMAppAttemptImpl.handle",
                    "StateMachineFactory.doTransition"
                ]
            },
            "possible_fix": "Modify the RMAppAttemptImpl.handle method to check the current state before processing the ATTEMPT_ADDED event. If the state is FINAL_SAVING, log a warning and skip processing this event."
        },
        "possible_fix_code": {
            "RMAppAttemptImpl.handle": "  public void handle(RMAppAttemptEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationAttemptId appAttemptID = event.getApplicationAttemptId();\n      LOG.debug(\"Processing event for \" + appAttemptID + \" of type \" + event.getType());\n      final RMAppAttemptState oldState = getAppAttemptState();\n\n      // Check if the current state is FINAL_SAVING before processing ATTEMPT_ADDED event\n      if (oldState == RMAppAttemptState.FINAL_SAVING && event.getType() == RMAppAttemptEventType.ATTEMPT_ADDED) {\n        LOG.warn(\"Ignoring ATTEMPT_ADDED event in FINAL_SAVING state for \" + appAttemptID);\n        return; // Skip processing this event\n      }\n\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitionException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n      }\n\n      // Log at INFO if we're not recovering or not in a terminal state.\n      // Log at DEBUG otherwise.\n      if ((oldState != getAppAttemptState()) &&\n          ((recoveredFinalState == null) ||\n            (event.getType() != RMAppAttemptEventType.RECOVER))) {\n        LOG.info(String.format(STATE_CHANGE_MESSAGE, appAttemptID, oldState,\n            getAppAttemptState(), event.getType()));\n      } else if ((oldState != getAppAttemptState()) && LOG.isDebugEnabled()) {\n        LOG.debug(String.format(STATE_CHANGE_MESSAGE, appAttemptID, oldState,\n            getAppAttemptState(), event.getType()));\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-1409.json",
        "creation_time": "2013-11-13T11:25:56.000+0000",
        "bug_report": {
            "Title": "NonAggregatingLogHandler can throw RejectedExecutionException",
            "Description": "The issue arises when the NonAggregatingLogHandler attempts to schedule a log deletion task after the scheduler has been shut down. Specifically, this occurs during the handling of APPLICATION_FINISHED events in the handle method of NonAggregatingLogHandler. When the scheduler is in the process of shutting down, any new tasks submitted to it will be rejected, leading to a RejectedExecutionException. This can cause tests like org.apache.hadoop.mapred.TestJobCleanup to fail.",
            "StackTrace": [
                "2013-11-13 10:53:06,970 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(166)) - Error in dispatcher thread",
                "java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@d51df63 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@7a20e369[Shutting down, pool size = 4, active threads = 0, queued tasks = 7, completed tasks = 0]",
                "at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)",
                "at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:325)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:530)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.handle(NonAggregatingLogHandler.java:121)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.handle(NonAggregatingLogHandler.java:49)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:159)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:95)",
                "at java.lang.Thread.run(Thread.java:724)"
            ],
            "RootCause": "The root cause of the issue is that the NonAggregatingLogHandler attempts to schedule a log deletion task after the scheduler has been shut down, resulting in a RejectedExecutionException.",
            "StepsToReproduce": [
                "1. Start the application and ensure it is running.",
                "2. Trigger the application to finish, which will invoke the APPLICATION_FINISHED event.",
                "3. Call the serviceStop() method on NonAggregatingLogHandler to shut down the scheduler.",
                "4. Observe the logs for a RejectedExecutionException."
            ],
            "ExpectedBehavior": "The log deletion task should be scheduled successfully without throwing any exceptions, even after the application has finished.",
            "ObservedBehavior": "A RejectedExecutionException is thrown when attempting to schedule a log deletion task after the scheduler has been shut down.",
            "Suggestions": "Implement a check to ensure that the scheduler is not shutting down before attempting to schedule a new task. Alternatively, handle the RejectedExecutionException gracefully to prevent application failures.",
            "problem_location": {
                "files": [
                    "NonAggregatingLogHandler.java",
                    "AsyncDispatcher.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler",
                    "org.apache.hadoop.yarn.event.AsyncDispatcher"
                ],
                "methods": [
                    "NonAggregatingLogHandler.handle",
                    "AsyncDispatcher.dispatch"
                ]
            },
            "possible_fix": "In the handle method of NonAggregatingLogHandler, add a check before scheduling the log deletion task to ensure that the scheduler is not shutting down. For example:\n\nif (!sched.isShutdown()) {\n    sched.schedule(...);\n} else {\n    LOG.warn(\"Scheduler is shutting down, log deletion task will not be scheduled.\");\n}"
        },
        "possible_fix_code": {
            "NonAggregatingLogHandler.handle": "  public void handle(LogHandlerEvent event) {\n    switch (event.getType()) {\n      case APPLICATION_STARTED:\n        LogHandlerAppStartedEvent appStartedEvent =\n            (LogHandlerAppStartedEvent) event;\n        this.appOwners.put(appStartedEvent.getApplicationId(),\n            appStartedEvent.getUser());\n        this.dispatcher.getEventHandler().handle(\n            new ApplicationEvent(appStartedEvent.getApplicationId(),\n                ApplicationEventType.APPLICATION_LOG_HANDLING_INITED));\n        break;\n      case CONTAINER_FINISHED:\n        // Ignore\n        break;\n      case APPLICATION_FINISHED:\n        LogHandlerAppFinishedEvent appFinishedEvent =\n            (LogHandlerAppFinishedEvent) event;\n        // Schedule - so that logs are available on the UI till they're deleted.\n        LOG.info(\"Scheduling Log Deletion for application: \"\n            + appFinishedEvent.getApplicationId() + \", with delay of \"\n            + this.deleteDelaySeconds + \" seconds\");\n        if (!sched.isShutdown()) {\n            sched.schedule(\n                new LogDeleterRunnable(appOwners.remove(appFinishedEvent\n                    .getApplicationId()), appFinishedEvent.getApplicationId()),\n                this.deleteDelaySeconds, TimeUnit.SECONDS);\n        } else {\n            LOG.warn(\"Scheduler is shutting down, log deletion task will not be scheduled.\");\n        }\n        break;\n      default:\n        ; // Ignore\n    }\n  }"
        }
    },
    {
        "filename": "YARN-5545.json",
        "creation_time": "2016-08-21T12:57:35.000+0000",
        "bug_report": {
            "Title": "Fix issues related to Max App in capacity scheduler",
            "Description": "The issue arises when the capacity configuration of the default partition is set to zero, preventing applications from being submitted to the default queue, even when other partitions have available capacity. This is due to the queue's inability to accept new applications when it already has zero applications, as indicated by the AccessControlException in the stack trace.",
            "StackTrace": [
                "java.io.IOException: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1471670113386_0001 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.default already has 0 applications, cannot accept submission of application: application_1471670113386_0001",
                "at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:316)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:255)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1344)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1341)",
                "Caused by: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1471670113386_0001 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.default already has 0 applications, cannot accept submission of application: application_1471670113386_0001",
                "at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java:286)",
                "at org.apache.hadoop.mapred.ResourceMgrDelegate.submitApplication(ResourceMgrDelegate.java:296)",
                "at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:301)"
            ],
            "RootCause": "The root cause of the issue is that the default queue (root.default) is configured with a capacity of zero, which prevents any applications from being submitted to it. The AccessControlException indicates that the queue cannot accept new applications when it already has zero applications.",
            "StepsToReproduce": [
                "Configure capacity scheduler with the following settings:",
                "yarn.scheduler.capacity.root.default.capacity=0",
                "yarn.scheduler.capacity.root.queue1.accessible-node-labels.labelx.capacity=50",
                "yarn.scheduler.capacity.root.default.accessible-node-labels.labelx.capacity=50",
                "Submit an application using the command:",
                "./yarn jar ../share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-alpha2-SNAPSHOT-tests.jar sleep -Dmapreduce.job.node-label-expression=labelx -Dmapreduce.job.queuename=default -m 1 -r 1 -mt 10000000 -rt 1"
            ],
            "ExpectedBehavior": "The application should be submitted successfully to the default queue or another available queue, given that there is capacity in other partitions.",
            "ObservedBehavior": "The application submission fails with an AccessControlException, indicating that the default queue cannot accept submissions because it already has zero applications.",
            "Suggestions": "Consider modifying the capacity configuration for the default queue to allow for application submissions, or implement a check to allow submissions to other queues when the default queue is at capacity zero.",
            "problem_location": {
                "files": [
                    "YarnClientImpl.java",
                    "YARNRunner.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.client.api.impl.YarnClientImpl",
                    "org.apache.hadoop.mapred.YARNRunner"
                ],
                "methods": [
                    "YarnClientImpl.submitApplication",
                    "YARNRunner.submitJob"
                ]
            },
            "possible_fix": "Modify the capacity scheduler configuration to ensure that the default queue can accept applications even when its capacity is set to zero, or implement logic in the YarnClientImpl to handle submissions to other queues when the default queue is at capacity zero."
        },
        "possible_fix_code": {
            "YarnClientImpl.submitApplication": "  public ApplicationId\n      submitApplication(ApplicationSubmissionContext appContext)\n          throws YarnException, IOException {\n    ApplicationId applicationId = appContext.getApplicationId();\n    if (applicationId == null) {\n      throw new ApplicationIdNotProvidedException(\n          \"ApplicationId is not provided in ApplicationSubmissionContext\");\n    }\n    SubmitApplicationRequest request =\n        Records.newRecord(SubmitApplicationRequest.class);\n    request.setApplicationSubmissionContext(appContext);\n\n    // Automatically add the timeline DT into the CLC\n    // Only when the security and the timeline service are both enabled\n    if (isSecurityEnabled() && timelineServiceEnabled) {\n      addTimelineDelegationToken(appContext.getAMContainerSpec());\n    }\n\n    // Check if the default queue has zero applications\n    if (isDefaultQueueZeroCapacity()) {\n      // Logic to allow submission to other queues\n      if (!canSubmitToOtherQueues(appContext)) {\n        throw new YarnException(\"Cannot submit application to any queue\");\n      }\n    }\n\n    //TODO: YARN-1763:Handle RM failovers during the submitApplication call.\n    rmClient.submitApplication(request);\n\n    int pollCount = 0;\n    long startTime = System.currentTimeMillis();\n    EnumSet<YarnApplicationState> waitingStates = \n                                 EnumSet.of(YarnApplicationState.NEW,\n                                 YarnApplicationState.NEW_SAVING,\n                                 YarnApplicationState.SUBMITTED);\n    EnumSet<YarnApplicationState> failToSubmitStates = \n                                  EnumSet.of(YarnApplicationState.FAILED,\n                                  YarnApplicationState.KILLED);\t\t\n    while (true) {\n      try {\n        ApplicationReport appReport = getApplicationReport(applicationId);\n        YarnApplicationState state = appReport.getYarnApplicationState();\n        if (!waitingStates.contains(state)) {\n          if(failToSubmitStates.contains(state)) {\n            throw new YarnException(\"Failed to submit \" + applicationId + \n                \" to YARN : \" + appReport.getDiagnostics());\n          }\n          LOG.info(\"Submitted application \" + applicationId);\n          break;\n        }\n\n        long elapsedMillis = System.currentTimeMillis() - startTime;\n        if (enforceAsyncAPITimeout() &&\n            elapsedMillis >= asyncApiPollTimeoutMillis) {\n          throw new YarnException(\"Timed out while waiting for application \" +\n              applicationId + \" to be submitted successfully\");\n        }\n\n        // Notify the client through the log every 10 poll, in case the client\n        // is blocked here too long.\n        if (++pollCount % 10 == 0) {\n          LOG.info(\"Application submission is not finished, \" +\n              \"submitted application \" + applicationId +\n              \" is still in \" + state);\n        }\n        try {\n          Thread.sleep(submitPollIntervalMillis);\n        } catch (InterruptedException ie) {\n          LOG.error(\"Interrupted while waiting for application \"\n              + applicationId\n              + \" to be successfully submitted.\");\n        }\n      } catch (ApplicationNotFoundException ex) {\n        // FailOver or RM restart happens before RMStateStore saves\n        // ApplicationState\n        LOG.info(\"Re-submit application \" + applicationId + \"with the \" +\n            \"same ApplicationSubmissionContext\");\n        rmClient.submitApplication(request);\n      }\n    }\n\n    return applicationId;\n  }"
        }
    },
    {
        "filename": "YARN-301.json",
        "creation_time": "2013-01-01T05:40:18.000+0000",
        "bug_report": {
            "Title": "Fair scheduler throws ConcurrentModificationException when iterating over app's priorities",
            "Description": "The FairScheduler encounters a ConcurrentModificationException during the processing of node updates, specifically when iterating over the application's priorities in the assignContainer method. This issue arises when the application priorities are modified while being iterated, leading to a crash of the ResourceManager.",
            "StackTrace": [
                "2012-12-30 17:14:17,171 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.util.ConcurrentModificationException",
                "at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)",
                "at java.util.TreeMap$KeyIterator.next(TreeMap.java:1154)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:297)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:181)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:780)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:842)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:98)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:340)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the ConcurrentModificationException is due to the modification of the app's priorities while they are being iterated over in the assignContainer method. This can occur if multiple threads are accessing and modifying the priorities concurrently without proper synchronization.",
            "StepsToReproduce": [
                "Set up a test cluster with the FairScheduler.",
                "Create multiple applications with varying priorities.",
                "Trigger a node update event while the scheduler is processing the application priorities."
            ],
            "ExpectedBehavior": "The FairScheduler should handle node updates without throwing exceptions, allowing for smooth scheduling of containers based on application priorities.",
            "ObservedBehavior": "The FairScheduler throws a ConcurrentModificationException, causing the ResourceManager to crash.",
            "Suggestions": "Implement proper synchronization mechanisms when accessing and modifying the application's priorities to prevent concurrent modifications during iteration.",
            "problem_location": {
                "files": [
                    "FairScheduler.java",
                    "FSLeafQueue.java",
                    "AppSchedulable.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable"
                ],
                "methods": [
                    "FairScheduler.nodeUpdate",
                    "FSLeafQueue.assignContainer",
                    "AppSchedulable.assignContainer"
                ]
            },
            "possible_fix": "To fix the issue, consider using a synchronized block or a concurrent collection (like CopyOnWriteArrayList) for the app's priorities in the assignContainer method to ensure thread safety during iteration. For example:\n\n```java\nsynchronized (app.getPriorities()) {\n    for (Priority priority : app.getPriorities()) {\n        // existing logic\n    }\n}\n```"
        },
        "possible_fix_code": {
            "AppSchedulable.assignContainer": "  public Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n    LOG.info(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n\n    if (reserved) {\n      RMContainer rmContainer = node.getReservedContainer();\n      Priority priority = rmContainer.getReservedPriority();\n\n      // Make sure the application still needs requests at this priority\n      if (app.getTotalRequiredResources(priority) == 0) {\n        unreserve(app, priority, node);\n        return Resources.none();\n      }\n    } else {\n      // If this app is over quota, don\\'t schedule anything\n      if (!(getRunnable())) { return Resources.none(); }\n    }\n    // For each priority, see if we can schedule a node local, rack local\n    // or off-switch request. Rack of off-switch requests may be delayed\n    // (not scheduled) in order to promote better locality.\n    synchronized (app.getPriorities()) {\n      for (Priority priority : app.getPriorities()) {\n        app.addSchedulingOpportunity(priority);\n        NodeType allowedLocality = app.getAllowedLocalityLevel(priority,\n            scheduler.getNumClusterNodes(), scheduler.getNodeLocalityThreshold(),\n            scheduler.getRackLocalityThreshold());\n\n        ResourceRequest localRequest = app.getResourceRequest(priority,\n            node.getHostName());\n        if (localRequest != null && localRequest.getNumContainers() != 0) {\n          return assignContainer(node, app, priority,\n              localRequest, NodeType.NODE_LOCAL, reserved);\n        }\n\n        ResourceRequest rackLocalRequest = app.getResourceRequest(priority,\n            node.getRackName());\n        if (rackLocalRequest != null && rackLocalRequest.getNumContainers() != 0\n            && (allowedLocality.equals(NodeType.RACK_LOCAL) ||\n                allowedLocality.equals(NodeType.OFF_SWITCH))) {\n          return assignContainer(node, app, priority, rackLocalRequest,\n              NodeType.RACK_LOCAL, reserved);\n        }\n\n        ResourceRequest offSwitchRequest = app.getResourceRequest(priority,\n            RMNode.ANY);\n        if (offSwitchRequest != null && offSwitchRequest.getNumContainers() != 0\n            && allowedLocality.equals(NodeType.OFF_SWITCH)) {\n          return assignContainer(node, app, priority, offSwitchRequest,\n              NodeType.OFF_SWITCH, reserved);\n        }\n      }\n    }\n    return Resources.none();\n  }"
        }
    },
    {
        "filename": "YARN-7942.json",
        "creation_time": "2018-02-16T19:09:39.000+0000",
        "bug_report": {
            "Title": "Yarn ServiceClient does not delete znode from secure ZooKeeper",
            "Description": "The Yarn ServiceClient is unable to delete a znode from ZooKeeper due to insufficient permissions, despite the expected ACLs being set. The error message indicates a NoAuth exception, suggesting that the current user does not have the necessary authorization to perform the delete operation on the specified path.",
            "StackTrace": [
                "2018-02-16 15:49:29,691 WARN  client.ServiceClient (ServiceClient.java:actionDestroy(470)) - Error deleting registry entry /users/hbase/services/yarn-service/hbase-app-test",
                "org.apache.hadoop.registry.client.exceptions.NoPathPermissionsException: `/registry/users/hbase/services/yarn-service/hbase-app-test': Not authorized to access path; ACLs: [null ACL]: KeeperErrorCode = NoAuth for /registry/users/hbase/services/yarn-service/hbase-app-test",
                "at org.apache.hadoop.registry.client.impl.zk.CuratorService.operationFailure(CuratorService.java:412)",
                "at org.apache.hadoop.registry.client.impl.zk.CuratorService.operationFailure(CuratorService.java:390)",
                "at org.apache.hadoop.registry.client.impl.zk.CuratorService.zkDelete(CuratorService.java:722)",
                "at org.apache.hadoop.registry.client.impl.zk.RegistryOperationsService.delete(RegistryOperationsService.java:162)",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.actionDestroy(ServiceClient.java:462)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer$4.run(ApiServer.java:253)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer$4.run(ApiServer.java:243)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer.stopService(ApiServer.java:243)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer.deleteService(ApiServer.java:223)"
            ],
            "RootCause": "The root cause of the issue is that the user attempting to delete the znode does not have the required permissions, as indicated by the NoAuth exception in the stack trace.",
            "StepsToReproduce": [
                "1. Attempt to delete a znode from ZooKeeper using the Yarn ServiceClient.",
                "2. Ensure that the user has the sasl:rm:cdrwa permissions set on the znode.",
                "3. Observe the error message indicating insufficient permissions."
            ],
            "ExpectedBehavior": "The znode should be successfully deleted from ZooKeeper if the user has the appropriate permissions.",
            "ObservedBehavior": "The delete operation fails with a NoAuth exception, indicating that the user is not authorized to access the specified path.",
            "Suggestions": "Verify the ACLs for the znode and ensure that the user has the necessary permissions to perform delete operations. Additionally, check the configuration of the ZooKeeper security settings.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/client/impl/zk/RegistryOperationsService.java",
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/client/impl/zk/CuratorService.java",
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-service/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java"
                ],
                "classes": [
                    "org.apache.hadoop.registry.client.impl.zk.RegistryOperationsService",
                    "org.apache.hadoop.registry.client.impl.zk.CuratorService",
                    "org.apache.hadoop.yarn.service.client.ServiceClient"
                ],
                "methods": [
                    "RegistryOperationsService.delete",
                    "CuratorService.zkDelete",
                    "ServiceClient.actionDestroy"
                ]
            },
            "possible_fix": "Ensure that the user has the correct ACLs set for the znode in ZooKeeper. This may involve updating the permissions for the user or modifying the ACLs associated with the znode to include the necessary permissions for deletion."
        },
        "possible_fix_code": {
            "ServiceClient.actionDestroy": "  public int actionDestroy(String serviceName) throws YarnException,\n      IOException {\n    ServiceApiUtil.validateNameFormat(serviceName, getConfig());\n    verifyNoLiveAppInRM(serviceName, \"destroy\");\n\n    Path appDir = fs.buildClusterDirPath(serviceName);\n    FileSystem fileSystem = fs.getFileSystem();\n    // remove from the appId cache\n    cachedAppInfo.remove(serviceName);\n    boolean destroySucceed = true;\n    if (fileSystem.exists(appDir)) {\n      if (fileSystem.delete(appDir, true)) {\n        LOG.info(\"Successfully deleted service dir for \" + serviceName + \": \"\n            + appDir);\n      } else {\n        String message =\n            \"Failed to delete service + \" + serviceName + \" at:  \" + appDir;\n        LOG.info(message);\n        throw new YarnException(message);\n      }\n    } else {\n      LOG.info(\"Service '\" + serviceName + \"' doesn't exist at hdfs path: \"\n          + appDir);\n      destroySucceed = false;\n    }\n    try {\n      deleteZKNode(serviceName);\n    } catch (Exception e) {\n      throw new IOException(\"Could not delete zk node for \" + serviceName, e);\n    }\n    String registryPath =\n        ServiceRegistryUtils.registryPathForInstance(serviceName);\n    try {\n      if (getRegistryClient().exists(registryPath)) {\n        getRegistryClient().delete(registryPath, true);\n      } else {\n        LOG.info(\n            \"Service '\" + serviceName + \"' doesn't exist at ZK registry path: \"\n                + registryPath);\n        destroySucceed = false;\n      }\n    } catch (IOException e) {\n      LOG.warn(\"Error deleting registry entry {}\", registryPath, e);\n    }\n    if (destroySucceed) {\n      LOG.info(\"Successfully destroyed service {}\", serviceName);\n      return EXIT_SUCCESS;\n    } else {\n      LOG.error(\"Error on destroy '\" + serviceName + \"': not found.\");\n      return -1;\n    }\n  }"
        }
    },
    {
        "filename": "YARN-7692.json",
        "creation_time": "2017-12-29T06:00:34.000+0000",
        "bug_report": {
            "Title": "Skip validating priority acls while recovering applications",
            "Description": "In a scenario where a cluster is created without ACLs, jobs submitted by a user (e.g., 'user_a') are rejected when ACLs are enabled and the user is not included in the priority ACL. While this behavior is expected, the Resource Manager crashes during the recovery of previous applications due to an AccessControlException. The exception indicates that the user does not have permission to submit or update an application, which leads to the failure of the Resource Manager to recover its state.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnException: org.apache.hadoop.security.AccessControlException: User hrt_qa (auth:SIMPLE) does not have permission to submit/update application_1514268754125_0001 for 0",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.checkAndGetApplicationPriority(CapacityScheduler.java:2348)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:396)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:358)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:567)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1390)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:771)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)"
            ],
            "RootCause": "The Resource Manager fails to recover applications because it attempts to validate application priorities against ACLs that do not permit the user to submit or update applications. This results in an AccessControlException, which causes the Resource Manager to crash.",
            "StepsToReproduce": [
                "1. Create a cluster without any ACLs.",
                "2. Submit jobs with an existing user (e.g., 'user_a').",
                "3. Enable ACLs and create a priority ACL entry without including 'user_a'.",
                "4. Submit a job with 'user_a'.",
                "5. Observe that the job is rejected and the Resource Manager crashes during recovery."
            ],
            "ExpectedBehavior": "The Resource Manager should successfully recover applications without crashing, even if some applications are rejected due to ACL violations.",
            "ObservedBehavior": "The Resource Manager crashes with an AccessControlException when attempting to recover applications that the user does not have permission to submit or update.",
            "Suggestions": "Implement a check to skip ACL validation during the recovery process of applications to prevent the Resource Manager from crashing.",
            "problem_location": {
                "files": [
                    "ResourceManager.java",
                    "RMAppManager.java",
                    "CapacityScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager",
                    "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"
                ],
                "methods": [
                    "ResourceManager.recover",
                    "RMAppManager.recoverApplication",
                    "CapacityScheduler.checkAndGetApplicationPriority"
                ]
            },
            "possible_fix": "Modify the recoverApplication method in RMAppManager to handle AccessControlExceptions gracefully, allowing the recovery process to continue without crashing the Resource Manager."
        },
        "possible_fix_code": {
            "RMAppManager.recoverApplication": "  protected void recoverApplication(ApplicationStateData appState,\n      RMState rmState) throws Exception {\n    ApplicationSubmissionContext appContext =\n        appState.getApplicationSubmissionContext();\n    ApplicationId appId = appContext.getApplicationId();\n\n    try {\n      // create and recover app.\n      RMAppImpl application =\n          createAndPopulateNewRMApp(appContext, appState.getSubmitTime(),\n              appState.getUser(), true, appState.getStartTime());\n\n      application.handle(new RMAppRecoverEvent(appId, rmState));\n    } catch (AccessControlException e) {\n      // Log the exception and skip this application to allow recovery to continue\n      LOG.warn(\"AccessControlException while recovering application \" + appId + \": \" + e.getMessage());\n    }\n  }"
        }
    },
    {
        "filename": "YARN-3917.json",
        "creation_time": "2015-07-11T00:41:28.000+0000",
        "bug_report": {
            "Title": "getResourceCalculatorPlugin for the default should intercept all exceptions",
            "Description": "The current implementation of the ResourceCalculatorPlugin does not handle the UnsupportedOperationException thrown when the operating system cannot be determined. This leads to a failure in the initialization of the NodeManager, which should instead gracefully handle this exception and allow the service to continue running with default settings.",
            "StackTrace": [
                "java.lang.UnsupportedOperationException: Could not determine OS",
                "at org.apache.hadoop.util.SysInfo.newInstance(SysInfo.java:43)",
                "at org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.<init>(ResourceCalculatorPlugin.java:37)",
                "at org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.getResourceCalculatorPlugin(ResourceCalculatorPlugin.java:160)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl.serviceInit(ContainersMonitorImpl.java:108)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:249)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:312)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:547)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:595)"
            ],
            "RootCause": "The UnsupportedOperationException is thrown in SysInfo.newInstance() when the OS cannot be determined, which is not handled properly in the ResourceCalculatorPlugin initialization.",
            "StepsToReproduce": [
                "Run the NodeManager without a specific resource calculator plugin configuration.",
                "Ensure the environment is set up such that the OS cannot be determined (e.g., in a non-standard environment).",
                "Observe the failure during the NodeManager initialization."
            ],
            "ExpectedBehavior": "The NodeManager should initialize successfully, using default settings for the resource calculator plugin without throwing an exception.",
            "ObservedBehavior": "The NodeManager fails to initialize, throwing an UnsupportedOperationException due to the inability to determine the operating system.",
            "Suggestions": "Implement exception handling in the ResourceCalculatorPlugin to catch UnsupportedOperationException and allow the NodeManager to continue initialization with default settings.",
            "problem_location": {
                "files": [
                    "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.SysInfo.java",
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.java"
                ],
                "classes": [
                    "org.apache.hadoop.util.SysInfo",
                    "org.apache.hadoop.yarn.util.ResourceCalculatorPlugin"
                ],
                "methods": [
                    "SysInfo.newInstance",
                    "ResourceCalculatorPlugin.getResourceCalculatorPlugin"
                ]
            },
            "possible_fix": "In the ResourceCalculatorPlugin.getResourceCalculatorPlugin method, wrap the instantiation logic in a try-catch block to handle UnsupportedOperationException and log a warning instead of failing the initialization."
        },
        "possible_fix_code": {
            "ResourceCalculatorPlugin.getResourceCalculatorPlugin": "  public static ResourceCalculatorPlugin getResourceCalculatorPlugin(\n      Class<? extends ResourceCalculatorPlugin> clazz, Configuration conf) {\n\n    if (clazz != null) {\n      return ReflectionUtils.newInstance(clazz, conf);\n    }\n    try {\n      return new ResourceCalculatorPlugin();\n    } catch (SecurityException e) {\n      return null;\n    } catch (UnsupportedOperationException e) {\n      LOG.warn(\"UnsupportedOperationException caught: \" + e.getMessage());\n      // Return a default instance or handle accordingly\n      return new ResourceCalculatorPlugin(); // or return null based on your design\n    }\n  }"
        }
    },
    {
        "filename": "YARN-3537.json",
        "creation_time": "2015-04-23T11:34:23.000+0000",
        "bug_report": {
            "Title": "NPE when NodeManager.serviceInit fails and stopRecoveryStore invoked",
            "Description": "A NullPointerException (NPE) occurs in the NodeManager's stopRecoveryStore method when the service is stopped after a failure in serviceInit. This happens because the nmStore object is not properly initialized before it is accessed, leading to a crash during the shutdown process.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stopRecoveryStore(NodeManager.java:181)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStop(NodeManager.java:326)",
                "at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "at org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.tearDown(TestNodeManagerShutdown.java:106)"
            ],
            "RootCause": "The nmStore object is null when stopRecoveryStore is called, which leads to a NullPointerException.",
            "StepsToReproduce": [
                "1. Start the NodeManager service.",
                "2. Simulate a failure in the serviceInit method.",
                "3. Attempt to stop the NodeManager service."
            ],
            "ExpectedBehavior": "The NodeManager should handle the failure gracefully and not throw a NullPointerException when stopping the service.",
            "ObservedBehavior": "A NullPointerException is thrown when the stopRecoveryStore method is invoked during the shutdown process.",
            "Suggestions": "Ensure that nmStore is properly initialized before it is accessed in the stopRecoveryStore method. Add null checks or initialize nmStore in the serviceInit method.",
            "problem_location": {
                "files": [
                    "NodeManager.java",
                    "AbstractService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                    "org.apache.hadoop.service.AbstractService"
                ],
                "methods": [
                    "NodeManager.stopRecoveryStore",
                    "NodeManager.serviceStop",
                    "AbstractService.stop"
                ]
            },
            "possible_fix": "In the NodeManager class, modify the stopRecoveryStore method to check if nmStore is null before calling nmStore.stop(). If it is null, log a warning and return early."
        },
        "possible_fix_code": {
            "NodeManager.stopRecoveryStore": "  private void stopRecoveryStore() throws IOException {\n    if (nmStore == null) {\n      LOG.warn(\"nmStore is null, skipping stopRecoveryStore.\");\n      return;\n    }\n    nmStore.stop();\n    if (null != context) {\n      if (context.getDecommissioned() && nmStore.canRecover()) {\n        LOG.info(\"Removing state store due to decommission\");\n        Configuration conf = getConfig();\n        Path recoveryRoot =\n            new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR));\n        LOG.info(\"Removing state store at \" + recoveryRoot\n            + \" due to decommission\");\n        FileSystem recoveryFs = FileSystem.getLocal(conf);\n        if (!recoveryFs.delete(recoveryRoot, true)) {\n          LOG.warn(\"Unable to delete \" + recoveryRoot);\n        }\n      }\n    }\n  }"
        }
    },
    {
        "filename": "YARN-7962.json",
        "creation_time": "2018-02-22T22:32:20.000+0000",
        "bug_report": {
            "Title": "Race Condition When Stopping DelegationTokenRenewer causes RM crash during failover",
            "Description": "The issue arises when the `serviceStop` method is called on the `DelegationTokenRenewer`, which does not set the `isServiceStarted` flag to false before shutting down the `renewerService` thread pool. This leads to a race condition where events are still being processed after the service has been stopped, resulting in a `RejectedExecutionException` when trying to execute a task on a terminated thread pool.",
            "StackTrace": [
                "2018-02-21 11:18:16,253  FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.util.concurrent.RejectedExecutionException: Task org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable@39bddaf2 rejected from java.util.concurrent.ThreadPoolExecutor@5f71637b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 15487]",
                "at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)",
                "at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)",
                "at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.processDelegationTokenRenewerEvent(DelegationTokenRenewer.java:196)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.applicationFinished(DelegationTokenRenewer.java:734)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.finishApplication(RMAppManager.java:199)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:424)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:65)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:177)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The `serviceStop` method does not set the `isServiceStarted` flag to false before shutting down the `renewerService`, allowing tasks to be submitted to a terminated thread pool.",
            "StepsToReproduce": [
                "Start the ResourceManager and allow it to process some applications.",
                "Trigger the `serviceStop` method on the `DelegationTokenRenewer` while there are still events being processed.",
                "Observe the logs for a `RejectedExecutionException`."
            ],
            "ExpectedBehavior": "The `DelegationTokenRenewer` should stop gracefully without throwing exceptions, and no tasks should be submitted to a terminated thread pool.",
            "ObservedBehavior": "A `RejectedExecutionException` is thrown when trying to execute a task on a terminated thread pool, leading to a crash of the ResourceManager.",
            "Suggestions": "Update the `serviceStop` method to acquire the `serviceStateLock`, set `isServiceStarted` to false, and then proceed to shut down the `renewerService` thread pool.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"
                ],
                "methods": [
                    "DelegationTokenRenewer.processDelegationTokenRenewerEvent",
                    "DelegationTokenRenewer.serviceStop"
                ]
            },
            "possible_fix": "In the `serviceStop` method, add the following lines before shutting down the `renewerService`:\n\n```java\nserviceStateLock.writeLock().lock();\ntry {\n    isServiceStarted = false;\n} finally {\n    serviceStateLock.writeLock().unlock();\n}\n```"
        },
        "possible_fix_code": {
            "DelegationTokenRenewer.serviceStop": "public void serviceStop() {\n    serviceStateLock.writeLock().lock();\n    try {\n        isServiceStarted = false;\n    } finally {\n        serviceStateLock.writeLock().unlock();\n    }\n    if (renewerService != null) {\n        renewerService.shutdown();\n    }\n}"
        }
    },
    {
        "filename": "YARN-8357.json",
        "creation_time": "2018-05-24T16:46:57.000+0000",
        "bug_report": {
            "Title": "Yarn Service: NPE when service is saved first and then started.",
            "Description": "A NullPointerException (NPE) occurs when attempting to start a service that has been saved but not properly initialized. The issue arises from the `getStatus` method in the `ServiceClient` class returning a service object with a null state, leading to an NPE when the `actionStart` method tries to access the state of the service. This indicates that the service's state is not being set correctly during the save operation.",
            "StackTrace": [
                "at org.apache.hadoop.yarn.service.client.ServiceClient.actionStart(ServiceClient.java:974)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer$7.run(ApiServer.java:650)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer$7.run(ApiServer.java:644)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1687)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer.startService(ApiServer.java:644)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer.updateService(ApiServer.java:449)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)"
            ],
            "RootCause": "The root cause of the NPE is that the `getStatus` method in the `ServiceClient` class returns a service object with a null state, which is not handled properly in the `actionStart` method.",
            "StepsToReproduce": [
                "1. Save a service using the ServiceClient.",
                "2. Immediately attempt to start the saved service.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The service should start without any exceptions, and its state should be properly initialized.",
            "ObservedBehavior": "A NullPointerException is thrown when trying to start the service due to a null state.",
            "Suggestions": "Ensure that the service state is properly set during the save operation. Add null checks before accessing the service state in the `actionStart` method.",
            "problem_location": {
                "files": [
                    "ServiceClient.java",
                    "ApiServer.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.service.client.ServiceClient",
                    "org.apache.hadoop.yarn.service.webapp.ApiServer"
                ],
                "methods": [
                    "ServiceClient.actionStart",
                    "ApiServer.updateService"
                ]
            },
            "possible_fix": "In the `actionStart` method of `ServiceClient`, add a check for null before accessing the service state. Additionally, ensure that the `getStatus` method correctly initializes the service state when saving."
        },
        "possible_fix_code": {
            "ServiceClient.actionStart": "  public int actionStart(String serviceName) throws YarnException, IOException {\n    ServiceApiUtil.validateNameFormat(serviceName, getConfig());\n    Service liveService = getStatus(serviceName);\n    if (liveService == null ||\n        liveService.getState() == null ||\n        !liveService.getState().equals(ServiceState.UPGRADING)) {\n      Path appDir = checkAppExistOnHdfs(serviceName);\n      Service service = ServiceApiUtil.loadService(fs, serviceName);\n      ServiceApiUtil.validateAndResolveService(service, fs, getConfig());\n      // see if it is actually running and bail out;\n      verifyNoLiveAppInRM(serviceName, \"start\");\n      ApplicationId appId = submitApp(service);\n      service.setId(appId.toString());\n      // write app definition on to hdfs\n      Path appJson = ServiceApiUtil.writeAppDefinition(fs, appDir, service);\n      LOG.info(\"Persisted service \" + service.getName() + \" at \" + appJson);\n      return 0;\n    } else {\n      LOG.info(\"Finalize service {} upgrade\", serviceName);\n      ApplicationReport appReport =\n          yarnClient.getApplicationReport(getAppId(serviceName));\n      if (StringUtils.isEmpty(appReport.getHost())) {\n        throw new YarnException(serviceName + \" AM hostname is empty\");\n      }\n      ClientAMProtocol proxy = createAMProxy(serviceName, appReport);\n\n      RestartServiceRequestProto.Builder requestBuilder =\n          RestartServiceRequestProto.newBuilder();\n      proxy.restart(requestBuilder.build());\n      return 0;\n    }\n  }"
        }
    },
    {
        "filename": "YARN-6534.json",
        "creation_time": "2017-04-26T21:43:52.000+0000",
        "bug_report": {
            "Title": "ResourceManager failed due to TimelineClient trying to init SSLFactory even when HTTPS is not enabled",
            "Description": "In a non-secured cluster, the ResourceManager fails consistently because the TimelineServiceV1Publisher attempts to initialize the TimelineClient with an SSLFactory without checking if HTTPS is enabled. This leads to a FileNotFoundException when the system tries to access the keystore file, which is not present in the expected location.",
            "StackTrace": [
                "2017-04-26 21:09:10,683 FATAL resourcemanager.ResourceManager (ResourceManager.java:main(1457)) - Error starting ResourceManager",
                "org.apache.hadoop.service.ServiceStateException: java.io.FileNotFoundException: /etc/security/clientKeys/all.jks (No such file or directory)",
                "at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:172)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.serviceInit(TimelineClientImpl.java:131)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher.serviceInit(AbstractSystemMetricsPublisher.java:59)",
                "at org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher.serviceInit(TimelineServiceV1Publisher.java:67)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:344)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1453)",
                "Caused by: java.io.FileNotFoundException: /etc/security/clientKeys/all.jks (No such file or directory)",
                "at java.io.FileInputStream.open0(Native Method)",
                "at java.io.FileInputStream.open(FileInputStream.java:195)",
                "at java.io.FileInputStream.<init>(FileInputStream.java:138)",
                "at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager(ReloadingX509TrustManager.java:168)",
                "at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.<init>(ReloadingX509TrustManager.java:86)",
                "at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:219)",
                "at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:179)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineConnector.getSSLFactory(TimelineConnector.java:176)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineConnector.serviceInit(TimelineConnector.java:106)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)"
            ],
            "RootCause": "The root cause of the issue is that the TimelineClient is being initialized with an SSLFactory even when HTTPS is not enabled, leading to an attempt to access a non-existent keystore file.",
            "StepsToReproduce": [
                "Set up a non-secured cluster without HTTPS enabled.",
                "Start the ResourceManager.",
                "Observe the failure due to the missing keystore file."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully without attempting to initialize SSL components when HTTPS is not enabled.",
            "ObservedBehavior": "The ResourceManager fails to start, throwing a FileNotFoundException for the missing keystore file.",
            "Suggestions": "Implement a check in the TimelineClient initialization to ensure that SSLFactory is only initialized if HTTPS is enabled in the configuration.",
            "problem_location": {
                "files": [
                    "ResourceManager.java",
                    "TimelineServiceV1Publisher.java",
                    "TimelineClientImpl.java",
                    "SSLFactory.java",
                    "FileBasedKeyStoresFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager",
                    "org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher",
                    "org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl",
                    "org.apache.hadoop.security.ssl.SSLFactory",
                    "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory"
                ],
                "methods": [
                    "ResourceManager.serviceInit",
                    "TimelineServiceV1Publisher.serviceInit",
                    "TimelineClientImpl.serviceInit",
                    "SSLFactory.init",
                    "FileBasedKeyStoresFactory.init"
                ]
            },
            "possible_fix": "In the TimelineClientImpl.serviceInit method, add a check for YarnConfiguration.useHttps(conf) before initializing the SSLFactory. If HTTPS is not enabled, skip the SSLFactory initialization."
        },
        "possible_fix_code": {
            "TimelineClientImpl.serviceInit": "  protected void serviceInit(Configuration conf) throws Exception {\n    timelineServiceVersion =\n        conf.getFloat(YarnConfiguration.TIMELINE_SERVICE_VERSION,\n            YarnConfiguration.DEFAULT_TIMELINE_SERVICE_VERSION);\n    LOG.info(\"Timeline service address: \" + getTimelineServiceAddress());\n    if (!YarnConfiguration.timelineServiceEnabled(conf)\n        || !((Float.compare(this.timelineServiceVersion, 1.0f) == 0)\n            || (Float.compare(this.timelineServiceVersion, 1.5f) == 0))) {\n      throw new IOException(\"Timeline V1 client is not properly configured. \"\n          + \"Either timeline service is not enabled or version is not set to\"\n          + \" 1.x\");\n    }\n    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    UserGroupInformation realUgi = ugi.getRealUser();\n    if (realUgi != null) {\n      authUgi = realUgi;\n      doAsUser = ugi.getShortUserName();\n    } else {\n      authUgi = ugi;\n      doAsUser = null;\n    }\n    token = new DelegationTokenAuthenticatedURL.Token();\n    connector = createTimelineConnector();\n\n    if (YarnConfiguration.useHttps(conf)) {\n      timelineServiceAddress =\n          conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS,\n              YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS);\n    } else {\n      timelineServiceAddress =\n          conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS,\n              YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_ADDRESS);\n    }\n\n    // Check if HTTPS is enabled before initializing SSLFactory\n    if (YarnConfiguration.useHttps(conf)) {\n      sslFactory = getSSLFactory(conf);\n    }\n\n    super.serviceInit(conf);\n  }"
        }
    },
    {
        "filename": "YARN-4227.json",
        "creation_time": "2015-10-06T04:59:10.000+0000",
        "bug_report": {
            "Title": "Ignore expired containers from removed nodes in FairScheduler",
            "Description": "The ResourceManager (RM) crashes due to a NullPointerException when handling expired container events for nodes that have already been removed. This occurs because the expired container event is processed after the node removal, leading to a situation where the RM attempts to access a container that no longer exists.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.completedContainer(FairScheduler.java:849)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1273)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:122)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:585)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The NullPointerException occurs in the completedContainer method when it tries to access a container that has already been removed from the scheduler, leading to a null reference.",
            "StepsToReproduce": [
                "1. Start the ResourceManager with a FairScheduler.",
                "2. Submit an application that allocates containers.",
                "3. Remove a node from the cluster while there are still expired containers associated with it.",
                "4. Observe the ResourceManager logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The ResourceManager should gracefully handle expired container events without crashing, even if the associated node has been removed.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when processing expired container events for removed nodes.",
            "Suggestions": "Implement a check in the completedContainer method to ensure that the RMContainer is not null before proceeding with the completion logic. Additionally, ensure that expired container events are ignored for nodes that have been removed.",
            "problem_location": {
                "files": [
                    "FairScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
                ],
                "methods": [
                    "FairScheduler.handle",
                    "FairScheduler.completedContainer"
                ]
            },
            "possible_fix": "In the completedContainer method, add a check for rmContainer being null before accessing its properties. For example:\n\nprotected synchronized void completedContainer(RMContainer rmContainer,\n    ContainerStatus containerStatus, RMContainerEventType event) {\n    if (rmContainer == null) {\n        LOG.warn(\"Attempted to complete a container that is null. Ignoring event.\");\n        return;\n    }\n    // existing logic continues here...\n}"
        },
        "possible_fix_code": {
            "FairScheduler.completedContainer": "  protected synchronized void completedContainer(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) {\n    if (rmContainer == null) {\n      LOG.warn(\"Attempted to complete a container that is null. Ignoring event.\");\n      return;\n    }\n\n    Container container = rmContainer.getContainer();\n\n    // Get the application for the finished container\n    FSAppAttempt application =\n        getCurrentAttemptForContainer(container.getId());\n    ApplicationId appId =\n        container.getId().getApplicationAttemptId().getApplicationId();\n    if (application == null) {\n      LOG.info(\"Container \" + container + \" of\" +\n          \" finished application \" + appId +\n          \" completed with event \" + event);\n      return;\n    }\n\n    // Get the node on which the container was allocated\n    FSSchedulerNode node = getFSSchedulerNode(container.getNodeId());\n\n    if (rmContainer.getState() == RMContainerState.RESERVED) {\n      application.unreserve(rmContainer.getReservedPriority(), node);\n    } else {\n      application.containerCompleted(rmContainer, containerStatus, event);\n      node.releaseContainer(container);\n      updateRootQueueMetrics();\n    }\n\n    LOG.info(\"Application attempt \" + application.getApplicationAttemptId()\n        + \" released container \" + container.getId() + \" on node: \" + node\n        + \" with event: \" + event);\n  }"
        }
    },
    {
        "filename": "YARN-2649.json",
        "creation_time": "2014-10-06T22:57:46.000+0000",
        "bug_report": {
            "Title": "Flaky test TestAMRMRPCNodeUpdates",
            "Description": "The test 'TestAMRMRPCNodeUpdates' intermittently fails due to a race condition between the processing of SchedulerEventType.NODE_UPDATE and RMAppAttemptEvent.ATTEMPT_ADDED. The test expects the application attempt state to be ALLOCATED, but it is found to be SCHEDULED instead. This discrepancy occurs because the test only waits for the RMAppState.ACCEPTED before the NodeManager sends a heartbeat, allowing the NODE_UPDATE event to be processed prematurely.",
            "StackTrace": [
                "junit.framework.AssertionFailedError: AppAttempt state is not correct (timedout) expected:<ALLOCATED> but was:<SCHEDULED>",
                "at junit.framework.Assert.fail(Assert.java:50)",
                "at junit.framework.Assert.failNotEquals(Assert.java:287)",
                "at junit.framework.Assert.assertEquals(Assert.java:67)",
                "at org.apache.hadoop.yarn.server.resourcemanager.MockAM.waitForState(MockAM.java:82)",
                "at org.apache.hadoop.yarn.server.resourcemanager.MockRM.sendAMLaunched(MockRM.java:382)",
                "at org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates.testAMRMUnusableNodes(TestAMRMRPCNodeUpdates.java:125)"
            ],
            "RootCause": "The root cause of the issue is a race condition where the NODE_UPDATE event is processed before the ATTEMPT_ADDED event, leading to an incorrect application attempt state.",
            "StepsToReproduce": [
                "Run the test 'TestAMRMRPCNodeUpdates' multiple times.",
                "Use a custom AsyncDispatcher with CountDownLatch to simulate the timing issue."
            ],
            "ExpectedBehavior": "The application attempt state should transition to ALLOCATED after the ATTEMPT_ADDED event is processed.",
            "ObservedBehavior": "The application attempt state is found to be SCHEDULED instead of the expected ALLOCATED state.",
            "Suggestions": "Implement a synchronization mechanism to ensure that the ATTEMPT_ADDED event is processed before the NODE_UPDATE event. This could involve modifying the event dispatching order or adding additional state checks.",
            "problem_location": {
                "files": [
                    "MockAM.java",
                    "MockRM.java",
                    "TestAMRMRPCNodeUpdates.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.MockAM",
                    "org.apache.hadoop.yarn.server.resourcemanager.MockRM",
                    "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates"
                ],
                "methods": [
                    "MockAM.waitForState",
                    "MockRM.sendAMLaunched",
                    "TestAMRMRPCNodeUpdates.testAMRMUnusableNodes"
                ]
            },
            "possible_fix": "To fix the race condition, ensure that the NODE_UPDATE event is only processed after the ATTEMPT_ADDED event has been handled. This may involve modifying the event handling logic in the AsyncDispatcher or adding a synchronization mechanism to control the order of event processing."
        },
        "possible_fix_code": {
            "MockAM.waitForState": "public void waitForState(RMAppState expectedState) throws InterruptedException {\n    long timeout = System.currentTimeMillis() + 10000; // 10 seconds timeout\n    while (System.currentTimeMillis() < timeout) {\n        if (appAttemptState == expectedState) {\n            return;\n        }\n        Thread.sleep(100); // Sleep to avoid busy waiting\n    }\n    throw new AssertionFailedError(\"AppAttempt state is not correct (timedout) expected:<\" + expectedState + \"> but was:<\" + appAttemptState + \">\");\n}"
        }
    },
    {
        "filename": "YARN-4288.json",
        "creation_time": "2015-10-22T12:30:16.000+0000",
        "bug_report": {
            "Title": "NodeManager restart should keep retrying to register to RM while connection exception happens during RM failed over.",
            "Description": "When the NodeManager (NM) is restarted, the NodeStatusUpdaterImpl attempts to register with the ResourceManager (RM) using RPC. If the RM is also restarting, this can lead to connection exceptions, such as 'Connection reset by peer'. The current implementation does not handle these exceptions properly, causing the NM to fail to restart. The system should be modified to allow the NM to retry registration with the RM in the event of such connection failures.",
            "StackTrace": [
                "2015-08-17 14:35:59,434 ERROR nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:rebootNodeStatusUpdaterAndRegisterWithRM(222)) - Unexpected error rebooting NodeStatusUpdater",
                "java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: \"172.27.62.28\"; destination host is: \"172.27.62.57\":8025;",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1473)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1400)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)",
                "at com.sun.proxy.$Proxy36.registerNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:68)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)",
                "at com.sun.proxy.$Proxy37.registerNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:257)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.rebootNodeStatusUpdaterAndRegisterWithRM(NodeStatusUpdaterImpl.java:215)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager$2.run(NodeManager.java:304)"
            ],
            "RootCause": "The NodeStatusUpdaterImpl does not implement a retry mechanism for registering with the ResourceManager when a connection exception occurs during RM failover. This leads to a failure in the NodeManager restart process.",
            "StepsToReproduce": [
                "1. Restart the NodeManager while the ResourceManager is also restarting.",
                "2. Observe the logs for connection exceptions during the registration process.",
                "3. Note that the NodeManager fails to restart due to the inability to register with the ResourceManager."
            ],
            "ExpectedBehavior": "The NodeManager should continuously attempt to register with the ResourceManager until successful, even if connection exceptions occur during RM failover.",
            "ObservedBehavior": "The NodeManager fails to restart and logs an error indicating that it could not register with the ResourceManager due to a connection reset by peer.",
            "Suggestions": "Implement a retry mechanism in the NodeStatusUpdaterImpl's registration process to handle connection exceptions gracefully. This can be achieved by utilizing the existing RetryInvocationHandler to manage retries for the registerNodeManager method.",
            "problem_location": {
                "files": [
                    "NodeStatusUpdaterImpl.java",
                    "ResourceTrackerPBClientImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl",
                    "org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl"
                ],
                "methods": [
                    "NodeStatusUpdaterImpl.rebootNodeStatusUpdaterAndRegisterWithRM",
                    "NodeStatusUpdaterImpl.registerWithRM",
                    "ResourceTrackerPBClientImpl.registerNodeManager"
                ]
            },
            "possible_fix": "Modify the rebootNodeStatusUpdaterAndRegisterWithRM method in NodeStatusUpdaterImpl to include a retry mechanism for the registerWithRM call. This can be done by wrapping the call in a loop that retries a specified number of times with a delay between attempts, utilizing the existing RetryInvocationHandler for consistency."
        },
        "possible_fix_code": {
            "NodeStatusUpdaterImpl.rebootNodeStatusUpdaterAndRegisterWithRM": "  protected void rebootNodeStatusUpdaterAndRegisterWithRM() {\n    // Interrupt the updater.\n    this.isStopped = true;\n\n    try {\n      statusUpdater.join();\n      int maxRetries = 5; // Maximum number of retries\n      int attempt = 0;\n      boolean registered = false;\n\n      while (attempt < maxRetries && !registered) {\n        try {\n          registerWithRM();\n          registered = true; // Registration successful\n        } catch (IOException e) {\n          attempt++;\n          LOG.warn(\"Registration attempt \" + attempt + \" failed. Retrying...\");\n          if (attempt < maxRetries) {\n            Thread.sleep(2000); // Wait before retrying\n          } else {\n            LOG.error(\"Max retries reached. Unable to register with RM.\", e);\n            throw e; // Rethrow the exception after max retries\n          }\n        }\n      }\n\n      statusUpdater = new Thread(statusUpdaterRunnable, \"Node Status Updater\");\n      this.isStopped = false;\n      statusUpdater.start();\n      LOG.info(\"NodeStatusUpdater thread is reRegistered and restarted\");\n    } catch (Exception e) {\n      String errorMessage = \"Unexpected error rebooting NodeStatusUpdater\";\n      LOG.error(errorMessage, e);\n      throw new YarnRuntimeException(e);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-1032.json",
        "creation_time": "2013-08-05T21:10:46.000+0000",
        "bug_report": {
            "Title": "NPE in RackResolve",
            "Description": "A NullPointerException (NPE) occurs in the RackResolver class when attempting to resolve a host address. This issue arises from the coreResolve method, which fails to handle cases where the DNS resolution does not return a valid result, leading to an attempt to access an element from an empty list. The exception is caught in the RMContainerAllocator, indicating that the resource manager is unable to contact the resource manager due to this unresolved host address.",
            "StackTrace": [
                "2013-08-01 07:11:37,708 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.util.RackResolver.coreResolve(RackResolver.java:99)",
                "at org.apache.hadoop.yarn.util.RackResolver.resolve(RackResolver.java:92)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assignMapsWithLocality(RMContainerAllocator.java:1039)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assignContainers(RMContainerAllocator.java:925)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:861)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.access$400(RMContainerAllocator.java:681)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:219)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:243)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "RootCause": "The root cause of the NPE is the lack of validation for the result of the DNS resolution in the coreResolve method. If the DNS resolution fails and returns an empty list, attempting to access the first element of this list results in a NullPointerException.",
            "StepsToReproduce": [
                "1. Set up a Hadoop environment with a misconfigured DNS or an unreachable host.",
                "2. Trigger a resource allocation that requires host address resolution.",
                "3. Observe the logs for the NPE in the RackResolver."
            ],
            "ExpectedBehavior": "The system should handle cases where the host address cannot be resolved gracefully, either by returning a default value or by logging an appropriate error message without throwing an exception.",
            "ObservedBehavior": "The system throws a NullPointerException when attempting to resolve a host address that cannot be resolved, leading to failure in contacting the resource manager.",
            "Suggestions": "Implement a check in the coreResolve method to ensure that the list returned by dnsToSwitchMapping.resolve is not empty before attempting to access its elements. If the list is empty, log an error and return a default Node or throw a more informative exception.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java",
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.util.RackResolver",
                    "org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator"
                ],
                "methods": [
                    "RackResolver.coreResolve",
                    "RackResolver.resolve",
                    "RMContainerAllocator.heartbeat"
                ]
            },
            "possible_fix": "In the coreResolve method, add a check for the size of rNameList before accessing its first element. If rNameList is empty, log an error and return null or throw a custom exception. Example code change:\n\nprivate static Node coreResolve(String hostName) {\n    List<String> tmpList = new ArrayList<>(1);\n    tmpList.add(hostName);\n    List<String> rNameList = dnsToSwitchMapping.resolve(tmpList);\n    if (rNameList.isEmpty()) {\n        LOG.error(\"Failed to resolve host: \" + hostName);\n        return null; // or throw new CustomException(\"Host resolution failed\");\n    }\n    String rName = rNameList.get(0);\n    LOG.info(\"Resolved \" + hostName + \" to \" + rName);\n    return new NodeBase(hostName, rName);\n}"
        },
        "possible_fix_code": {
            "RackResolver.coreResolve": "  private static Node coreResolve(String hostName) {\n    List <String> tmpList = new ArrayList<String>(1);\n    tmpList.add(hostName);\n    List <String> rNameList = dnsToSwitchMapping.resolve(tmpList);\n    if (rNameList.isEmpty()) {\n        LOG.error(\"Failed to resolve host: \" + hostName);\n        return null; // or throw new CustomException(\"Host resolution failed\");\n    }\n    String rName = rNameList.get(0);\n    LOG.info(\"Resolved \" + hostName + \" to \" + rName);\n    return new NodeBase(hostName, rName);\n  }"
        }
    },
    {
        "filename": "YARN-5837.json",
        "creation_time": "2016-11-04T16:06:59.000+0000",
        "bug_report": {
            "Title": "NPE when getting node status of a decommissioned node after an RM restart",
            "Description": "After decommissioning a node and subsequently restarting the ResourceManager, querying the status of the decommissioned node using the 'yarn node -status' command results in a NullPointerException (NPE). This occurs because the node's ID is incorrectly reported as '-1', leading to a failure in retrieving the node report.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.NullPointerException",
                "\tat org.apache.hadoop.yarn.client.cli.NodeCLI.printNodeStatus(NodeCLI.java:296)",
                "\tat org.apache.hadoop.yarn.client.cli.NodeCLI.run(NodeCLI.java:116)",
                "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)",
                "\tat org.apache.hadoop.yarn.client.cli.NodeCLI.main(NodeCLI.java:63)"
            ],
            "RootCause": "The node ID is reported as '-1' after the ResourceManager restart, which is not a valid node ID. This causes the method 'printNodeStatus' to fail when it attempts to find the node report for this invalid ID, resulting in a NullPointerException.",
            "StepsToReproduce": [
                "Decommission a node using the appropriate command.",
                "Run 'yarn node -status <node-id>' to confirm the node is decommissioned.",
                "Restart the ResourceManager.",
                "Run 'yarn node -status <node-id>' again using the decommissioned node's ID, which is now reported as '-1'."
            ],
            "ExpectedBehavior": "The system should return a valid status report for the decommissioned node or indicate that the node is no longer available without throwing an exception.",
            "ObservedBehavior": "The system throws a NullPointerException when attempting to retrieve the status of a decommissioned node after a ResourceManager restart.",
            "Suggestions": "Implement a check in the 'printNodeStatus' method to handle cases where the node ID is invalid (e.g., '-1'). This could involve returning a user-friendly message indicating that the node is not found instead of throwing an exception.",
            "problem_location": {
                "files": [
                    "NodeCLI.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.client.cli.NodeCLI"
                ],
                "methods": [
                    "NodeCLI.printNodeStatus"
                ]
            },
            "possible_fix": "In the 'printNodeStatus' method, add a check for the validity of the node ID before attempting to retrieve the node report. If the node ID is invalid, print a message indicating that the node cannot be found instead of proceeding to access the report."
        },
        "possible_fix_code": {
            "NodeCLI.printNodeStatus": "  private void printNodeStatus(String nodeIdStr) throws YarnException,\n      IOException {\n    // Check if the nodeIdStr is valid before proceeding\n    if (nodeIdStr == null || nodeIdStr.equals(\"-1\")) {\n      sysout.println(\"Node ID is invalid or not found: \" + nodeIdStr);\n      return;\n    }\n\n    NodeId nodeId = NodeId.fromString(nodeIdStr);\n    List<NodeReport> nodesReport = client.getNodeReports();\n    // Use PrintWriter.println, which uses correct platform line ending.\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n    PrintWriter nodeReportStr = new PrintWriter(\n        new OutputStreamWriter(baos, Charset.forName(\"UTF-8\")));\n    NodeReport nodeReport = null;\n    for (NodeReport report : nodesReport) {\n      if (!report.getNodeId().equals(nodeId)) {\n        continue;\n      }\n      nodeReport = report;\n      nodeReportStr.println(\"Node Report : \");\n      nodeReportStr.print(\"\\tNode-Id : \");\n      nodeReportStr.println(nodeReport.getNodeId());\n      nodeReportStr.print(\"\\tRack : \");\n      nodeReportStr.println(nodeReport.getRackName());\n      nodeReportStr.print(\"\\tNode-State : \");\n      nodeReportStr.println(nodeReport.getNodeState());\n      nodeReportStr.print(\"\\tNode-Http-Address : \");\n      nodeReportStr.println(nodeReport.getHttpAddress());\n      nodeReportStr.print(\"\\tLast-Health-Update : \");\n      nodeReportStr.println(DateFormatUtils.format(\n          new Date(nodeReport.getLastHealthReportTime()),\n            \"E dd/MMM/yy hh:mm:ss:SSzz\"));\n      nodeReportStr.print(\"\\tHealth-Report : \");\n      nodeReportStr\n          .println(nodeReport.getHealthReport());\n      nodeReportStr.print(\"\\tContainers : \");\n      nodeReportStr.println(nodeReport.getNumContainers());\n      nodeReportStr.print(\"\\tMemory-Used : \");\n      nodeReportStr.println((nodeReport.getUsed() == null) ? \"0MB\"\n          : (nodeReport.getUsed().getMemorySize() + \"MB\"));\n      nodeReportStr.print(\"\\tMemory-Capacity : \");\n      nodeReportStr.println(nodeReport.getCapability().getMemorySize() + \"MB\");\n      nodeReportStr.print(\"\\tCPU-Used : \");\n      nodeReportStr.println((nodeReport.getUsed() == null) ? \"0 vcores\"\n          : (nodeReport.getUsed().getVirtualCores() + \" vcores\"));\n      nodeReportStr.print(\"\\tCPU-Capacity : \");\n      nodeReportStr.println(nodeReport.getCapability().getVirtualCores() + \" vcores\");\n      nodeReportStr.print(\"\\tNode-Labels : \");\n      \n      // Create a List for node labels since we need it get sorted\n      List<String> nodeLabelsList =\n          new ArrayList<String>(report.getNodeLabels());\n      Collections.sort(nodeLabelsList);\n      nodeReportStr.println(StringUtils.join(nodeLabelsList.iterator(), \\',\\'));\n\n      nodeReportStr.print(\"\\tResource Utilization by Node : \");\n      if (nodeReport.getNodeUtilization() != null) {\n        nodeReportStr.print(\"PMem:\"\n            + nodeReport.getNodeUtilization().getPhysicalMemory()\n            + \" MB, VMem:\" + nodeReport.getNodeUtilization().getVirtualMemory()\n            + \" MB, VCores:\" + nodeReport.getNodeUtilization().getCPU());\n      }\n      nodeReportStr.println();\n\n      nodeReportStr.print(\"\\tResource Utilization by Containers : \");\n      if (nodeReport.getAggregatedContainersUtilization() != null) {\n        nodeReportStr.print(\"PMem:\"\n            + nodeReport.getAggregatedContainersUtilization()\n                .getPhysicalMemory()\n            + \" MB, VMem:\"\n            + nodeReport.getAggregatedContainersUtilization()\n                .getVirtualMemory() + \" MB, VCores:\"\n            + nodeReport.getAggregatedContainersUtilization().getCPU());\n      }\n      nodeReportStr.println();\n    }\n\n    if (nodeReport == null) {\n      nodeReportStr.print(\"Could not find the node report for node id : \"\n          + nodeIdStr);\n    }\n    nodeReportStr.close();\n    sysout.println(baos.toString(\"UTF-8\"));\n  }"
        }
    },
    {
        "filename": "YARN-6827.json",
        "creation_time": "2017-07-15T05:14:25.000+0000",
        "bug_report": {
            "Title": "[ATS1/1.5] NPE exception while publishing recovering applications into ATS during RM restart.",
            "Description": "During the recovery of applications in the ResourceManager (RM), a NullPointerException (NPE) is thrown when attempting to publish entities to the Application Timeline Service (ATS) version 1.5. This occurs because the ATS services are not fully initialized before the active services attempt to publish application data. In a non-HA scenario, active services start first, followed by ATS services. However, in HA scenarios, the transition to active state can occur before ATS services are ready, leading to the NPE when the active services try to publish to ATS.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.putEntities(TimelineClientImpl.java:178)",
                "at org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher.putEntity(TimelineServiceV1Publisher.java:368)"
            ],
            "RootCause": "The NPE occurs because the ATS services are not initialized before the active services attempt to publish application data during the RM restart process.",
            "StepsToReproduce": [
                "1. Start the ResourceManager in HA mode.",
                "2. Trigger a restart of the ResourceManager.",
                "3. Observe the logs for NPE when active services attempt to publish to ATS."
            ],
            "ExpectedBehavior": "The application data should be published to ATS without any exceptions, regardless of the order of service initialization.",
            "ObservedBehavior": "A NullPointerException is thrown when active services attempt to publish to ATS before it is fully initialized.",
            "Suggestions": "Ensure that the ATS services are fully initialized before allowing active services to publish application data. This may involve adding checks or synchronization mechanisms to prevent publishing until ATS is ready.",
            "problem_location": {
                "files": [
                    "TimelineClientImpl.java",
                    "TimelineServiceV1Publisher.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl",
                    "org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher"
                ],
                "methods": [
                    "TimelineClientImpl.putEntities",
                    "TimelineServiceV1Publisher.putEntity"
                ]
            },
            "possible_fix": "Add a check in the TimelineServiceV1Publisher.putEntity method to ensure that the ATS service is initialized before attempting to publish entities. For example, you could introduce a boolean flag that indicates whether the ATS service is ready and only allow publishing if this flag is true."
        },
        "possible_fix_code": {
            "putEntity": "  private void putEntity(TimelineEntity entity) {\n    try {\n      if (!isATSServicesReady()) {\n        LOG.warn(\"ATS services are not ready. Skipping publishing for entity \" + entity.getEntityId());\n        return;\n      }\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Publishing the entity \" + entity.getEntityId()\n            + \", JSON-style content: \"\n            + TimelineUtils.dumpTimelineRecordtoJSON(entity));\n      }\n      client.putEntities(entity);\n    } catch (Exception e) {\n      LOG.error(\"Error when publishing entity [\" + entity.getEntityType() + \",\"\n          + entity.getEntityId() + \"]\", e);\n    }\n  }\n\n  private boolean isATSServicesReady() {\n    // Implement the logic to check if ATS services are initialized and ready\n    return true; // Placeholder for actual readiness check\n  }"
        }
    },
    {
        "filename": "YARN-3832.json",
        "creation_time": "2015-06-19T13:31:18.000+0000",
        "bug_report": {
            "Title": "Resource Localization fails on a cluster due to existing cache directories",
            "Description": "The application fails to localize resources on a cluster due to an IOException when attempting to rename a directory that is not empty. This issue arises specifically when the destination directory already contains files or subdirectories, which prevents the rename operation from succeeding. The error message indicates that the rename operation cannot overwrite a non-empty destination directory, leading to the application failing multiple times.",
            "StackTrace": [
                "Application application_1434703279149_0057 failed 2 times due to AM Container for appattempt_1434703279149_0057_000002 exited with exitCode: -1000",
                "java.io.IOException: Rename cannot overwrite non empty destination directory /opt/hdfsdata/HA/nmlocal/usercache/root/filecache/39",
                "at org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:735)",
                "at org.apache.hadoop.fs.FilterFs.renameInternal(FilterFs.java:244)",
                "at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:678)",
                "at org.apache.hadoop.fs.FileContext.rename(FileContext.java:958)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:366)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:62)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the rename operation in the Hadoop file system is attempting to overwrite a non-empty destination directory, which is not allowed. The method 'renameInternal' in the AbstractFileSystem class checks if the destination directory is empty before proceeding with the rename, and throws an IOException if it is not.",
            "StepsToReproduce": [
                "1. Deploy a Hadoop cluster with version 2.7.0.",
                "2. Attempt to run an application that requires resource localization.",
                "3. Ensure that the destination directory for the resource localization is not empty.",
                "4. Observe the application fail with the specified IOException."
            ],
            "ExpectedBehavior": "The application should successfully localize resources without failing, even if the destination directory exists, provided that it is empty or the overwrite option is specified.",
            "ObservedBehavior": "The application fails with an IOException indicating that it cannot rename to a non-empty destination directory, causing the application to fail multiple times.",
            "Suggestions": "Consider implementing a check to either clear the destination directory before the rename operation or provide an option to overwrite the existing directory if it is not empty.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileContext.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/AbstractFileSystem.java",
                    "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/FSDownload.java"
                ],
                "classes": [
                    "org.apache.hadoop.fs.FileContext",
                    "org.apache.hadoop.fs.AbstractFileSystem",
                    "org.apache.hadoop.yarn.util.FSDownload"
                ],
                "methods": [
                    "FileContext.rename",
                    "AbstractFileSystem.renameInternal",
                    "FSDownload.call"
                ]
            },
            "possible_fix": "Modify the 'renameInternal' method in the AbstractFileSystem class to include an option for overwriting non-empty directories. Alternatively, ensure that the destination directory is cleared before attempting the rename operation."
        },
        "possible_fix_code": {
            "AbstractFileSystem.renameInternal": "  public void renameInternal(final Path src, final Path dst,\n      boolean overwrite) throws AccessControlException,\n      FileAlreadyExistsException, FileNotFoundException,\n      ParentNotDirectoryException, UnresolvedLinkException, IOException {\n    // Default implementation deals with overwrite in a non-atomic way\n    final FileStatus srcStatus = getFileLinkStatus(src);\n\n    FileStatus dstStatus;\n    try {\n      dstStatus = getFileLinkStatus(dst);\n    } catch (IOException e) {\n      dstStatus = null;\n    }\n    if (dstStatus != null) {\n      if (dst.equals(src)) {\n        throw new FileAlreadyExistsException(\n            \"The source \" + src + \" and destination \" + dst + \" are the same\");\n      }\n      if (srcStatus.isSymlink() && dst.equals(srcStatus.getSymlink())) {\n        throw new FileAlreadyExistsException(\n            \"Cannot rename symlink \" + src + \" to its target \" + dst);\n      }\n      // It's OK to rename a file to a symlink and vice versa\n      if (srcStatus.isDirectory() != dstStatus.isDirectory()) {\n        throw new IOException(\"Source \" + src + \" and destination \" + dst\n            + \" must both be directories\");\n      }\n      if (!overwrite) {\n        throw new FileAlreadyExistsException(\"Rename destination \" + dst\n            + \" already exists.\");\n      }\n      // Clear the destination directory if it is not empty\n      if (dstStatus.isDirectory()) {\n        RemoteIterator<FileStatus> list = listStatusIterator(dst);\n        if (list != null && list.hasNext()) {\n          // Delete all files in the destination directory\n          delete(dst, true);\n        }\n      }\n    } else {\n      final Path parent = dst.getParent();\n      final FileStatus parentStatus = getFileStatus(parent);\n      if (parentStatus.isFile()) {\n        throw new ParentNotDirectoryException(\"Rename destination parent \"\n            + parent + \" is a file.\");\n      }\n    }\n    renameInternal(src, dst);\n  }"
        }
    },
    {
        "filename": "YARN-2409.json",
        "creation_time": "2014-08-12T10:53:06.000+0000",
        "bug_report": {
            "Title": "Active to StandBy transition does not stop rmDispatcher causing AsyncDispatcher thread leak",
            "Description": "The application fails to properly handle state transitions during the Active to StandBy transition, leading to an InvalidStateTransitionException. This occurs when the application attempts to process events (STATUS_UPDATE and CONTAINER_ALLOCATED) that are not valid in the current state (LAUNCHED). As a result, the rmDispatcher continues to run, causing a thread leak.",
            "StackTrace": [
                "at java.lang.Thread.run(Thread.java:662)",
                "2014-08-12 07:03:00,839 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: STATUS_UPDATE at LAUNCHED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:697)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:105)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:779)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:760)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is that the application does not properly handle state transitions when moving from Active to StandBy, leading to attempts to process invalid events in the LAUNCHED state.",
            "StepsToReproduce": [
                "1. Start the application in Active state.",
                "2. Trigger a transition to StandBy state.",
                "3. Observe the logs for InvalidStateTransitionException errors."
            ],
            "ExpectedBehavior": "The application should successfully transition from Active to StandBy without attempting to process invalid events, and all associated threads should be properly terminated.",
            "ObservedBehavior": "The application throws InvalidStateTransitionException errors and leaks AsyncDispatcher threads due to improper handling of state transitions.",
            "Suggestions": "Implement checks to ensure that events are only processed if they are valid for the current state. Additionally, ensure that the rmDispatcher is properly stopped during the transition to prevent thread leaks.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.java",
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.state.StateMachineFactory.java",
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory",
                    "org.apache.hadoop.yarn.event.AsyncDispatcher"
                ],
                "methods": [
                    "RMAppAttemptImpl.handle",
                    "StateMachineFactory.doTransition",
                    "AsyncDispatcher.dispatch"
                ]
            },
            "possible_fix": "In the RMAppAttemptImpl.handle method, add a check to validate the event type against the current state before processing it. If the event is invalid, log a warning and skip processing. Additionally, ensure that the rmDispatcher is stopped during the transition to StandBy."
        },
        "possible_fix_code": {
            "RMAppAttemptImpl.handle": "  public void handle(RMAppAttemptEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationAttemptId appAttemptID = event.getApplicationAttemptId();\n      LOG.debug(\"Processing event for \" + appAttemptID + \" of type \" + event.getType());\n      final RMAppAttemptState oldState = getAppAttemptState();\n\n      // Check if the event type is valid for the current state\n      if (!isEventValidForState(event.getType(), oldState)) {\n        LOG.warn(\"Invalid event: \" + event.getType() + \" for current state: \" + oldState);\n        return; // Skip processing this event\n      }\n\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n      }\n\n      if (oldState != getAppAttemptState()) {\n        LOG.info(appAttemptID + \" State change from \" + oldState + \" to \" + getAppAttemptState());\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }\n\n  private boolean isEventValidForState(EventType eventType, RMAppAttemptState state) {\n    // Implement logic to check if the eventType is valid for the given state\n    // For example:\n    switch (state) {\n      case LAUNCHED:\n        return eventType == EventType.CONTAINER_ALLOCATED; // Example valid event\n      // Add other cases for different states and their valid events\n      default:\n        return false; // Invalid event for the current state\n    }\n  }"
        }
    },
    {
        "filename": "YARN-8116.json",
        "creation_time": "2018-04-04T15:30:52.000+0000",
        "bug_report": {
            "Title": "Nodemanager fails with NumberFormatException: For input string: \"\"",
            "Description": "The NodeManager fails to start due to a NumberFormatException when attempting to parse a string that is empty. This occurs during the recovery process of container states, specifically when the NodeManager tries to load the state of containers from the state store. The empty string is likely a result of missing or corrupted data in the state store, which should contain valid numeric values for container states.",
            "StackTrace": [
                "java.lang.NumberFormatException: For input string: \"\"",
                "at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)",
                "at java.lang.Long.parseLong(Long.java:601)",
                "at org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.loadContainerState(NMLeveldbStateStoreService.java:350)",
                "at org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.loadContainersState(NMLeveldbStateStoreService.java:253)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.recover(ContainerManagerImpl.java:365)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:316)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:464)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:899)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:960)"
            ],
            "RootCause": "The root cause of the issue is that the NodeManager is attempting to parse an empty string as a long integer when loading container states from the state store. This indicates that there is likely a data integrity issue in the state store, where expected numeric values are missing.",
            "StepsToReproduce": [
                "1) Update nodemanager debug delay config with a valid value.",
                "2) Launch a distributed shell application multiple times.",
                "3) Restart the NodeManager."
            ],
            "ExpectedBehavior": "The NodeManager should start successfully and recover the state of containers without throwing exceptions.",
            "ObservedBehavior": "The NodeManager fails to start and throws a NumberFormatException due to an empty string being parsed as a long integer.",
            "Suggestions": "Implement validation checks when loading container states to ensure that the values being parsed are not empty. Additionally, consider adding error handling to manage cases where the state store contains corrupted or incomplete data.",
            "problem_location": {
                "files": [
                    "NMLeveldbStateStoreService.java",
                    "ContainerManagerImpl.java",
                    "NodeManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl",
                    "org.apache.hadoop.yarn.server.nodemanager.NodeManager"
                ],
                "methods": [
                    "NMLeveldbStateStoreService.loadContainerState",
                    "NMLeveldbStateStoreService.loadContainersState",
                    "ContainerManagerImpl.recover",
                    "ContainerManagerImpl.serviceInit",
                    "NodeManager.serviceInit"
                ]
            },
            "possible_fix": "In the method loadContainerState of NMLeveldbStateStoreService, add a check to ensure that the string being parsed is not empty before calling Long.parseLong. For example:\n\nif (suffix.equals(CONTAINER_START_TIME_KEY_SUFFIX)) {\n    String value = asString(entry.getValue());\n    if (value.isEmpty()) {\n        throw new IOException(\"Container start time is missing for container: \" + containerId);\n    }\n    rcs.setStartTime(Long.parseLong(value));\n}"
        },
        "possible_fix_code": {
            "NMLeveldbStateStoreService.loadContainerState": "  private RecoveredContainerState loadContainerState(ContainerId containerId,\n      LeveldbIterator iter, String keyPrefix) throws IOException {\n    RecoveredContainerState rcs = new RecoveredContainerState();\n    rcs.status = RecoveredContainerStatus.REQUESTED;\n    while (iter.hasNext()) {\n      Entry<byte[],byte[]> entry = iter.peekNext();\n      String key = asString(entry.getKey());\n      if (!key.startsWith(keyPrefix)) {\n        break;\n      }\n      iter.next();\n\n      String suffix = key.substring(keyPrefix.length()-1);  // start with \\'/\\'\n      if (suffix.equals(CONTAINER_REQUEST_KEY_SUFFIX)) {\n        rcs.startRequest = new StartContainerRequestPBImpl(\n            StartContainerRequestProto.parseFrom(entry.getValue()));\n      } else if (suffix.equals(CONTAINER_VERSION_KEY_SUFFIX)) {\n        rcs.version = Integer.parseInt(asString(entry.getValue()));\n      } else if (suffix.equals(CONTAINER_START_TIME_KEY_SUFFIX)) {\n        String value = asString(entry.getValue());\n        if (value.isEmpty()) {\n          throw new IOException(\"Container start time is missing for container: \" + containerId);\n        }\n        rcs.setStartTime(Long.parseLong(value));\n      } else if (suffix.equals(CONTAINER_DIAGS_KEY_SUFFIX)) {\n        rcs.diagnostics = asString(entry.getValue());\n      } else if (suffix.equals(CONTAINER_QUEUED_KEY_SUFFIX)) {\n        if (rcs.status == RecoveredContainerStatus.REQUESTED) {\n          rcs.status = RecoveredContainerStatus.QUEUED;\n        }\n      } else if (suffix.equals(CONTAINER_PAUSED_KEY_SUFFIX)) {\n        if ((rcs.status == RecoveredContainerStatus.LAUNCHED)\n            ||(rcs.status == RecoveredContainerStatus.QUEUED)\n            ||(rcs.status == RecoveredContainerStatus.REQUESTED)) {\n          rcs.status = RecoveredContainerStatus.PAUSED;\n        }\n      } else if (suffix.equals(CONTAINER_LAUNCHED_KEY_SUFFIX)) {\n        if ((rcs.status == RecoveredContainerStatus.REQUESTED)\n            || (rcs.status == RecoveredContainerStatus.QUEUED)\n            ||(rcs.status == RecoveredContainerStatus.PAUSED)) {\n          rcs.status = RecoveredContainerStatus.LAUNCHED;\n        }\n      } else if (suffix.equals(CONTAINER_KILLED_KEY_SUFFIX)) {\n        rcs.killed = true;\n      } else if (suffix.equals(CONTAINER_EXIT_CODE_KEY_SUFFIX)) {\n        rcs.status = RecoveredContainerStatus.COMPLETED;\n        rcs.exitCode = Integer.parseInt(asString(entry.getValue()));\n      } else if (suffix.equals(CONTAINER_UPDATE_TOKEN_SUFFIX)) {\n        ContainerTokenIdentifierProto tokenIdentifierProto =\n            ContainerTokenIdentifierProto.parseFrom(entry.getValue());\n        Token currentToken = rcs.getStartRequest().getContainerToken();\n        Token updatedToken = Token\n            .newInstance(tokenIdentifierProto.toByteArray(),\n                ContainerTokenIdentifier.KIND.toString(),\n                currentToken.getPassword().array(), currentToken.getService());\n        rcs.startRequest.setContainerToken(updatedToken);\n        rcs.capability = new ResourcePBImpl(tokenIdentifierProto.getResource());\n        rcs.version = tokenIdentifierProto.getVersion();\n      } else if (suffix.equals(CONTAINER_REMAIN_RETRIES_KEY_SUFFIX)) {\n        rcs.setRemainingRetryAttempts(\n            Integer.parseInt(asString(entry.getValue())));\n      } else if (suffix.equals(CONTAINER_RESTART_TIMES_SUFFIX)) {\n        String value = asString(entry.getValue());\n        // parse the string format of List<Long>, e.g. [34, 21, 22]\n        String[] unparsedRestartTimes =\n            value.substring(1, value.length() - 1).split(\", \");\n        List<Long> restartTimes = new ArrayList<>();\n        for (String restartTime : unparsedRestartTimes) {\n          restartTimes.add(Long.parseLong(restartTime));\n        }\n        rcs.setRestartTimes(restartTimes);\n      } else if (suffix.equals(CONTAINER_WORK_DIR_KEY_SUFFIX)) {\n        rcs.setWorkDir(asString(entry.getValue()));\n      } else if (suffix.equals(CONTAINER_LOG_DIR_KEY_SUFFIX)) {\n        rcs.setLogDir(asString(entry.getValue()));\n      } else if (suffix.startsWith(CONTAINER_ASSIGNED_RESOURCES_KEY_SUFFIX)) {\n        String resourceType = suffix.substring(\n            CONTAINER_ASSIGNED_RESOURCES_KEY_SUFFIX.length());\n        ResourceMappings.AssignedResources assignedResources =\n            ResourceMappings.AssignedResources.fromBytes(entry.getValue());\n        rcs.getResourceMappings().addAssignedResources(resourceType,\n            assignedResources);\n      } else {\n        LOG.warn(\"the container \" + containerId\n            + \" will be killed because of the unknown key \" + key\n            + \" during recovery.\");\n        containerUnknownKeySuffixes.put(containerId, suffix);\n        rcs.setRecoveryType(RecoveredContainerType.KILL);\n      }\n    }\n    return rcs;\n  }"
        }
    },
    {
        "filename": "YARN-8403.json",
        "creation_time": "2018-06-06T22:34:42.000+0000",
        "bug_report": {
            "Title": "Nodemanager logs failed to download file with INFO level",
            "Description": "The Nodemanager is logging failures to download resources at INFO and WARN levels instead of ERROR. This is misleading as it does not adequately highlight critical issues that need immediate attention. The root cause of the failure is a permission issue when attempting to write to the local file cache directory, which results in a FileNotFoundException.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnException: Download and unpack failed",
                "Caused by: java.io.FileNotFoundException: /grid/0/hadoop/yarn/local/filecache/28_tmp/InputDir/input1.txt (Permission denied)",
                "at java.io.FileOutputStream.open0(Native Method)",
                "at java.io.FileOutputStream.open(FileOutputStream.java:270)",
                "at java.io.FileOutputStream.<init>(FileOutputStream.java:213)",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:236)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:318)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:307)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:408)"
            ],
            "RootCause": "The root cause of the issue is a FileNotFoundException due to permission denied when trying to write to the local file cache directory.",
            "StepsToReproduce": [
                "Attempt to run a container that requires downloading resources.",
                "Ensure that the user running the Nodemanager does not have write permissions to the local file cache directory.",
                "Check the Nodemanager logs for INFO or WARN level messages regarding failed downloads."
            ],
            "ExpectedBehavior": "The Nodemanager should log critical errors at the ERROR level when it fails to download resources, allowing for better visibility and quicker resolution of issues.",
            "ObservedBehavior": "The Nodemanager logs failures to download resources at INFO or WARN levels, which may lead to oversight of critical issues.",
            "Suggestions": "Change the logging level for exceptions related to resource downloading failures from INFO/WARN to ERROR. Additionally, ensure that the Nodemanager has the necessary permissions to write to the local file cache directory.",
            "problem_location": {
                "files": [
                    "ResourceLocalizationService.java",
                    "FSDownload.java",
                    "LinuxContainerExecutor.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService",
                    "org.apache.hadoop.yarn.util.FSDownload",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.LinuxContainerExecutor"
                ],
                "methods": [
                    "ResourceLocalizationService.run",
                    "FSDownload.downloadAndUnpack",
                    "LinuxContainerExecutor.startLocalizer"
                ]
            },
            "possible_fix": "Update the logging level in the ResourceLocalizationService to log exceptions at ERROR level. Ensure that the Nodemanager has the correct permissions to write to the local file cache directory by adjusting the filesystem permissions."
        },
        "possible_fix_code": {
            "FSDownload.downloadAndUnpack": "  private void downloadAndUnpack(Path source, Path destination)\n      throws YarnException {\n    try {\n      FileSystem sourceFileSystem = source.getFileSystem(conf);\n      FileSystem destinationFileSystem = destination.getFileSystem(conf);\n      if (sourceFileSystem.getFileStatus(source).isDirectory()) {\n        FileUtil.copy(\n            sourceFileSystem, source,\n            destinationFileSystem, destination, false,\n            true, conf);\n      } else {\n        unpack(source, destination, sourceFileSystem, destinationFileSystem);\n      }\n    } catch (FileNotFoundException e) {\n      LOG.error(\"Failed to download resource: \" + e.getMessage(), e);\n      throw new YarnException(\"Download and unpack failed due to permission issues\", e);\n    } catch (Exception e) {\n      LOG.error(\"Download and unpack failed: \" + e.getMessage(), e);\n      throw new YarnException(\"Download and unpack failed\", e);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-1458.json",
        "creation_time": "2013-11-29T03:31:39.000+0000",
        "bug_report": {
            "Title": "FairScheduler: Zero weight can lead to livelock",
            "Description": "The FairScheduler in the ResourceManager is experiencing a livelock condition when clients submit a large number of jobs with zero weight. This occurs because the scheduler's event processing thread becomes blocked while trying to remove applications, leading to a situation where it cannot progress. The issue is exacerbated by the way application weights are computed and handled, particularly when zero weights are involved, causing contention and blocking in the scheduler's critical sections.",
            "StackTrace": [
                "java.lang.Thread.State: BLOCKED (on object monitor)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplication(FairScheduler.java:671)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1023)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:440)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The root cause of the livelock is the handling of application weights in the FairScheduler. When applications with zero weight are submitted, the scheduler's logic does not properly account for these cases, leading to contention and blocking in the removeApplication method.",
            "StepsToReproduce": [
                "1. Set up a test cluster with FairScheduler.",
                "2. Submit a large number of jobs with zero weight.",
                "3. Monitor the ResourceManager's thread states using jstack."
            ],
            "ExpectedBehavior": "The FairScheduler should be able to handle the removal of applications without entering a blocked state, allowing it to continue processing other events and jobs.",
            "ObservedBehavior": "The ResourceManager's SchedulerEventDispatcher becomes blocked, preventing it from processing further events, leading to a livelock situation.",
            "Suggestions": "Implement checks in the FairScheduler to handle zero-weight applications more gracefully. Consider adjusting the logic in the getAppWeight method to ensure that zero weights do not lead to contention in critical sections.",
            "problem_location": {
                "files": [
                    "FairScheduler.java",
                    "ComputeFairShares.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.ComputeFairShares"
                ],
                "methods": [
                    "FairScheduler.removeApplication",
                    "FairScheduler.getAppWeight",
                    "ComputeFairShares.computeShares"
                ]
            },
            "possible_fix": "Modify the getAppWeight method in FairScheduler to handle zero weights appropriately, ensuring that they do not cause blocking in the removeApplication method. For example, add a condition to return a neutral weight or a minimum weight instead of zero, which can help avoid division errors and contention."
        },
        "possible_fix_code": {
            "FairScheduler.getAppWeight": "  public synchronized ResourceWeights getAppWeight(AppSchedulable app) {\n    if (!app.getRunnable()) {\n      // Job won't launch tasks, but don't return 0 to avoid division errors\n      return ResourceWeights.NEUTRAL;\n    } else {\n      double weight = 1.0;\n      if (sizeBasedWeight) {\n        // Set weight based on current memory demand\n        weight = Math.log1p(app.getDemand().getMemory()) / Math.log(2);\n      }\n      weight *= app.getPriority().getPriority();\n      if (weightAdjuster != null) {\n        // Run weight through the user-supplied weightAdjuster\n        weight = weightAdjuster.adjustWeight(app, weight);\n      }\n      // Ensure that zero weights are handled gracefully\n      if (weight <= 0) {\n        weight = 1.0; // Assign a minimum weight to avoid contention\n      }\n      return new ResourceWeights((float)weight);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-8209.json",
        "creation_time": "2018-04-26T00:22:23.000+0000",
        "bug_report": {
            "Title": "NPE in DeletionService",
            "Description": "A NullPointerException (NPE) occurs in the DeletionService when attempting to remove a Docker container. The issue arises specifically in the `writeCommandToTempFile` method of the `DockerClient` class, where the `nmContext` parameter is null, leading to a failure in writing the temporary Docker command file.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerClient.writeCommandToTempFile(DockerClient.java:109)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.executeDockerCommand(DockerCommandExecutor.java:85)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.executeStatusCommand(DockerCommandExecutor.java:192)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.getContainerStatus(DockerCommandExecutor.java:128)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.removeDockerContainer(LinuxContainerExecutor.java:935)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.DockerContainerDeletionTask.run(DockerContainerDeletionTask.java:61)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The root cause of the NullPointerException is that the `nmContext` parameter is null when passed to the `writeCommandToTempFile` method in the `DockerClient` class. This leads to a failure when trying to access the local directories handler.",
            "StepsToReproduce": [
                "1. Start the DeletionService.",
                "2. Attempt to remove a Docker container that is currently managed by the service.",
                "3. Observe the logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The system should successfully remove the Docker container without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the DeletionService to fail in removing the Docker container.",
            "Suggestions": "Ensure that the `nmContext` is properly initialized and passed to the `writeCommandToTempFile` method. Add null checks before using `nmContext` to prevent the NPE.",
            "problem_location": {
                "files": [
                    "DockerClient.java",
                    "DockerCommandExecutor.java",
                    "LinuxContainerExecutor.java",
                    "DockerContainerDeletionTask.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerClient",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor",
                    "org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.DockerContainerDeletionTask"
                ],
                "methods": [
                    "DockerClient.writeCommandToTempFile",
                    "DockerCommandExecutor.executeDockerCommand",
                    "DockerCommandExecutor.executeStatusCommand",
                    "DockerCommandExecutor.getContainerStatus",
                    "LinuxContainerExecutor.removeDockerContainer",
                    "DockerContainerDeletionTask.run"
                ]
            },
            "possible_fix": "In the `writeCommandToTempFile` method, add a check for `nmContext` before proceeding with file operations. If `nmContext` is null, throw a more descriptive exception or handle the case gracefully."
        },
        "possible_fix_code": {
            "DockerClient.writeCommandToTempFile": "  public String writeCommandToTempFile(DockerCommand cmd, Container container,\n      Context nmContext) throws ContainerExecutionException {\n    if (nmContext == null) {\n      throw new ContainerExecutionException(\"nmContext cannot be null\");\n    }\n    if (container == null) {\n      throw new ContainerExecutionException(\"Container cannot be null\");\n    }\n    ContainerId containerId = container.getContainerId();\n    String filePrefix = containerId.toString();\n    ApplicationId appId = containerId.getApplicationAttemptId()\n        .getApplicationId();\n    File dockerCommandFile;\n    String cmdDir = null;\n\n    if(nmContext.getLocalDirsHandler() == null) {\n      throw new ContainerExecutionException(\n          \"Unable to write temporary docker command\");\n    }\n\n    try {\n      cmdDir = nmContext.getLocalDirsHandler().getLocalPathForWrite(\n          ResourceLocalizationService.NM_PRIVATE_DIR + Path.SEPARATOR +\n          appId + Path.SEPARATOR + filePrefix + Path.SEPARATOR).toString();\n\n      dockerCommandFile = File.createTempFile(TMP_FILE_PREFIX + filePrefix,\n          TMP_FILE_SUFFIX, new File(cmdDir));\n\n      Writer writer = new OutputStreamWriter(\n          new FileOutputStream(dockerCommandFile.toString()), \"UTF-8\");\n      PrintWriter printWriter = new PrintWriter(writer);\n      printWriter.println(\"[docker-command-execution]\");\n      for (Map.Entry<String, List<String>> entry :\n          cmd.getDockerCommandWithArguments().entrySet()) {\n        if (entry.getKey().contains(\"=\")) {\n          throw new ContainerExecutionException(\n              \"'=' found in entry for docker command file, key = \" + entry\n                  .getKey() + \"; value = \" + entry.getValue());\n        }\n        if (entry.getValue().contains(\"\\n\")) {\n          throw new ContainerExecutionException(\n              \"'\\\\n' found in entry for docker command file, key = \" + entry\n                  .getKey() + \"; value = \" + entry.getValue());\n        }\n        printWriter.println(\"  \" + entry.getKey() + \"=\" + StringUtils\n            .join(\",\", entry.getValue()));\n      }\n      printWriter.close();\n\n      return dockerCommandFile.toString();\n    } catch (IOException e) {\n      LOG.warn(\"Unable to write docker command to \" + cmdDir);\n      throw new ContainerExecutionException(e);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-3804.json",
        "creation_time": "2015-06-15T08:54:42.000+0000",
        "bug_report": {
            "Title": "Both RM are on Standby state when kerberos user not in yarn.admin.acl",
            "Description": "When the Resource Managers (RMs) are configured in secure mode and the user 'yarn' is not included in the 'yarn.admin.acl' configuration, both RMs remain in a Standby state indefinitely. This occurs because the 'refreshAdminAcls' method fails due to insufficient permissions, preventing the RMs from transitioning to Active state.",
            "StackTrace": [
                "2015-06-15 12:20:21,556 WARN org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn OPERATION=refreshAdminAcls TARGET=AdminService RESULT=FAILURE DESCRIPTION=Unauthorized user PERMISSIONS=",
                "org.apache.hadoop.ha.ServiceFailedException: RM could not transition to Active",
                "at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:128)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:824)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:420)",
                "Caused by: org.apache.hadoop.ha.ServiceFailedException: Can not execute refreshAdminAcls",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:297)",
                "Caused by: org.apache.hadoop.yarn.exceptions.YarnException: org.apache.hadoop.security.AccessControlException: User yarn doesn't have permission to call 'refreshAdminAcls'",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.checkAcls(AdminService.java:223)"
            ],
            "RootCause": "The root cause of the issue is that the user 'yarn' does not have the necessary permissions to call the 'refreshAdminAcls' method, which is required for the Resource Managers to transition to Active state.",
            "StepsToReproduce": [
                "1. Configure the cluster in secure mode.",
                "2. Set yarn.admin.acl to 'dsperf'.",
                "3. Set yarn.resourcemanager.principal to 'yarn'.",
                "4. Start both Resource Managers."
            ],
            "ExpectedBehavior": "The Resource Managers should transition to Active state after a few retries or at least after the first attempt if the user does not have the required permissions.",
            "ObservedBehavior": "Both Resource Managers remain in Standby state indefinitely, failing to transition to Active state due to permission issues.",
            "Suggestions": "Ensure that the user 'yarn' is included in the 'yarn.admin.acl' configuration to allow the necessary permissions for transitioning to Active state.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/AdminService.java",
                    "hadoop-common-project.hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.AdminService",
                    "org.apache.hadoop.ha.ActiveStandbyElector"
                ],
                "methods": [
                    "AdminService.refreshAdminAcls",
                    "AdminService.transitionToActive",
                    "ActiveStandbyElector.becomeActive"
                ]
            },
            "possible_fix": "Add the user 'yarn' to the 'yarn.admin.acl' configuration in the cluster settings to grant the necessary permissions for calling 'refreshAdminAcls'."
        },
        "possible_fix_code": {
            "AdminService.refreshAdminAcls": "  private RefreshAdminAclsResponse refreshAdminAcls(boolean checkRMHAState)\n      throws YarnException, IOException {\n    String argName = \"refreshAdminAcls\";\n    UserGroupInformation user = checkAcls(argName);\n\n    // Check if the user has the necessary permissions to refresh admin ACLs\n    if (!user.getShortUserName().equals(\"yarn\") && !authorizer.isAdmin(user)) {\n      throw new AccessControlException(\"User \" + user.getShortUserName() + \" does not have permission to refresh admin ACLs.\");\n    }\n\n    if (checkRMHAState) {\n      checkRMStatus(user.getShortUserName(), argName, \"refresh Admin ACLs.\");\n    }\n    Configuration conf =\n        getConfiguration(new Configuration(false),\n            YarnConfiguration.YARN_SITE_CONFIGURATION_FILE);\n    authorizer.setAdmins(new AccessControlList(conf.get(\n      YarnConfiguration.YARN_ADMIN_ACL,\n        YarnConfiguration.DEFAULT_YARN_ADMIN_ACL)), UserGroupInformation\n        .getCurrentUser());\n    RMAuditLogger.logSuccess(user.getShortUserName(), argName,\n        \"AdminService\");\n\n    return recordFactory.newRecordInstance(RefreshAdminAclsResponse.class);\n  }"
        }
    },
    {
        "filename": "YARN-1839.json",
        "creation_time": "2014-03-14T23:52:29.000+0000",
        "bug_report": {
            "Title": "Capacity scheduler preempts an AM out. AM attempt 2 fails to launch task container with SecretManager$InvalidToken: No NMToken sent",
            "Description": "When using a single-node cluster with capacity scheduler preemption enabled, an application (app 1) is preempted to allow another application (app 2) to run. After app 2 completes, the application master (AM) for app 1 attempts to launch a task container but fails due to an InvalidToken error, indicating that no NMToken was sent. This issue arises from the failure to refresh the NMToken after preemption, leading to the AM being unable to authenticate with the NodeManager.",
            "StackTrace": [
                "2014-03-13 20:13:50,254 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1394741557066_0001_m_000000_1009: Container launch failed for container_1394741557066_0001_02_000021 : org.apache.hadoop.security.token.SecretManager$InvalidToken: No NMToken sent for <host>:45454",
                "at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:206)",
                "at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:196)",
                "at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:117)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.getCMProxy(ContainerLauncherImpl.java:403)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container.launch(ContainerLauncherImpl.java:138)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:369)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "RootCause": "The root cause of the issue is that the NMToken is not being refreshed or sent after the application master (AM) is preempted and restarted. The ContainerManagementProtocolProxy fails to obtain a valid NMToken, resulting in an InvalidToken error when attempting to launch a task container.",
            "StepsToReproduce": [
                "1. Set up a single-node cluster.",
                "2. Enable capacity scheduler preemption.",
                "3. Run a MapReduce sleep job as application 1.",
                "4. Occupy the entire cluster with application 1.",
                "5. Run another MapReduce sleep job as application 2.",
                "6. Preempt application 1 to allow application 2 to run.",
                "7. Wait for application 2 to finish.",
                "8. Observe the logs for application 1's AM attempt 2 when it tries to launch a task container."
            ],
            "ExpectedBehavior": "The application master for app 1 should successfully launch the task container after being preempted, with a valid NMToken being sent to the NodeManager.",
            "ObservedBehavior": "The application master for app 1 fails to launch the task container due to an InvalidToken error, indicating that no NMToken was sent.",
            "Suggestions": "Ensure that the NMToken is refreshed and sent correctly after the application master is preempted. This may involve modifying the logic in the ContainerManagementProtocolProxy to handle token refreshes more effectively after preemption events.",
            "problem_location": {
                "files": [
                    "ContainerLauncherImpl.java",
                    "ContainerManagementProtocolProxy.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl",
                    "org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy"
                ],
                "methods": [
                    "ContainerLauncherImpl.getCMProxy",
                    "ContainerManagementProtocolProxy.getProxy"
                ]
            },
            "possible_fix": "Modify the getCMProxy method in ContainerLauncherImpl to ensure that it checks for and refreshes the NMToken after preemption. Additionally, ensure that the getProxy method in ContainerManagementProtocolProxy correctly handles the case where the NMToken is invalid or missing, potentially by re-fetching the token from the cache."
        },
        "possible_fix_code": {
            "ContainerLauncherImpl.getCMProxy": "  public ContainerManagementProtocolProxy.ContainerManagementProtocolProxyData\n      getCMProxy(String containerMgrBindAddr, ContainerId containerId)\n          throws IOException {\n    // Ensure NMToken is refreshed after preemption\n    refreshNMToken(containerMgrBindAddr);\n    return cmProxy.getProxy(containerMgrBindAddr, containerId);\n  }",
            "ContainerManagementProtocolProxy.getProxy": "  public synchronized ContainerManagementProtocolProxyData getProxy(\n      String containerManagerBindAddr, ContainerId containerId)\n      throws InvalidToken {\n    // This get call will update the map which is working as LRU cache.\n    ContainerManagementProtocolProxyData proxy =\n        cmProxy.get(containerManagerBindAddr);\n\n    // Check if the NMToken is valid and refresh if necessary\n    if (proxy == null || !proxy.token.getIdentifier().equals(nmTokenCache.getToken(containerManagerBindAddr).getIdentifier())) {\n      LOG.info(\"Refreshing proxy as NMToken got updated for node : \" + containerManagerBindAddr);\n      // Refresh the NMToken\n      proxy = new ContainerManagementProtocolProxyData(rpc, containerManagerBindAddr,\n          containerId, nmTokenCache.getToken(containerManagerBindAddr));\n      cmProxy.put(containerManagerBindAddr, proxy);\n    }\n\n    // This is to track active users of this proxy.\n    proxy.activeCallers++;\n    updateLRUCache(containerManagerBindAddr);\n    \n    return proxy;\n  }"
        }
    },
    {
        "filename": "YARN-6714.json",
        "creation_time": "2017-06-15T09:56:15.000+0000",
        "bug_report": {
            "Title": "IllegalStateException while handling APP_ATTEMPT_REMOVED event when async-scheduling enabled in CapacityScheduler",
            "Description": "In the async-scheduling mode of the CapacityScheduler, after an Application Master (AM) failover, the system attempts to unreserve all reserved containers. However, it can still commit an outdated reserve proposal for a failed application attempt. This leads to an IllegalStateException when the application attempts to unreserve resources that are still reserved for a previous application attempt, causing the ResourceManager (RM) to crash.",
            "StackTrace": [
                "java.lang.IllegalStateException: Trying to unreserve for application appattempt_1495188831758_0121_000002 when currently reserved for application application_1495188831758_0121 on node host: node1:45454 #containers=2 available=... used=...",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode.unreserveResource(FiCaSchedulerNode.java:123)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.unreserve(FiCaSchedulerApp.java:845)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.completedContainer(LeafQueue.java:1787)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainerInternal(CapacityScheduler.java:1957)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.completedContainer(AbstractYarnScheduler.java:586)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.doneApplicationAttempt(CapacityScheduler.java:966)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1740)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:822)",
                "at java.lang.Thread.run(Thread.java:834)"
            ],
            "RootCause": "The root cause of the issue is that the system does not properly check the state of the application attempt during the unreserve process, allowing outdated proposals to be committed, which leads to an IllegalStateException.",
            "StepsToReproduce": [
                "Enable async-scheduling in the CapacityScheduler.",
                "Submit an application and allow it to reserve containers.",
                "Force a failover of the Application Master (AM).",
                "Stop the application, which triggers the unreserve of all reserved containers.",
                "Observe the IllegalStateException in the logs."
            ],
            "ExpectedBehavior": "The system should correctly handle the unreservation of resources without throwing an IllegalStateException, ensuring that only valid application attempts can unreserve resources.",
            "ObservedBehavior": "The system throws an IllegalStateException when trying to unreserve resources for an application attempt that is not currently reserved, leading to a crash of the ResourceManager.",
            "Suggestions": "Implement additional checks in the unreserve process to ensure that only the correct application attempt can unreserve resources. This can be achieved by ensuring that the application attempt state is validated before committing any unreserve actions.",
            "problem_location": {
                "files": [
                    "FiCaSchedulerNode.java",
                    "FiCaSchedulerApp.java",
                    "LeafQueue.java",
                    "CapacityScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"
                ],
                "methods": [
                    "FiCaSchedulerNode.unreserveResource",
                    "FiCaSchedulerApp.unreserve",
                    "LeafQueue.completedContainer",
                    "CapacityScheduler.doneApplicationAttempt"
                ]
            },
            "possible_fix": "In the unreserve method of FiCaSchedulerNode, add a check to ensure that the application attempt ID matches the currently reserved application attempt ID before proceeding with the unreserve operation. This will prevent the IllegalStateException from being thrown."
        },
        "possible_fix_code": {
            "FiCaSchedulerNode.unreserveResource": "  public synchronized void unreserveResource(\n      SchedulerApplicationAttempt application) {\n    // adding NP checks as this can now be called for preemption\n    if (getReservedContainer() != null\n        && getReservedContainer().getContainer() != null\n        && getReservedContainer().getContainer().getId() != null\n        && getReservedContainer().getContainer().getId()\n          .getApplicationAttemptId() != null) {\n\n      // Cannot unreserve for wrong application...\n      ApplicationAttemptId reservedApplication =\n          getReservedContainer().getContainer().getId()\n            .getApplicationAttemptId();\n      if (!reservedApplication.equals(\n          application.getApplicationAttemptId())) {\n        throw new IllegalStateException(\"Trying to unreserve \" +\n            \" for application \" + application.getApplicationAttemptId() +\n            \" when currently reserved \" +\n            \" for application \" + reservedApplication.getApplicationId() +\n            \" on node \" + this);\n      }\n    }\n    // Additional check to ensure the application attempt is valid\n    if (!application.getApplicationAttemptId().equals(getReservedContainer().getContainer().getId().getApplicationAttemptId())) {\n        throw new IllegalStateException(\"Cannot unreserve for application attempt \" + application.getApplicationAttemptId() + \" as it is not currently reserved.\");\n    }\n    setReservedContainer(null);\n  }"
        }
    },
    {
        "filename": "YARN-3351.json",
        "creation_time": "2015-03-16T14:19:59.000+0000",
        "bug_report": {
            "Title": "AppMaster tracking URL is broken in HA",
            "Description": "The AppMaster tracking URL fails to function correctly in High Availability (HA) mode after the changes introduced in YARN-2713. When attempting to access the tracking URL for an application while the first ResourceManager (RM) is inactive, a BindException occurs, indicating that the application cannot bind to the requested address. This issue arises during the proxying of the request to the AppMaster's tracking URL.",
            "StackTrace": [
                "2015-02-05 20:47:43,478 WARN org.mortbay.log: /proxy/application_1423182188062_0002/: java.net.BindException: Cannot assign requested address",
                "java.net.BindException: Cannot assign requested address",
                "at java.net.PlainSocketImpl.socketBind(Native Method)",
                "at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:376)",
                "at java.net.Socket.bind(Socket.java:631)",
                "at java.net.Socket.<init>(Socket.java:423)",
                "at java.net.Socket.<init>(Socket.java:280)",
                "at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:80)",
                "at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:122)",
                "at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:707)",
                "at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:387)",
                "at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:346)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:188)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:345)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)"
            ],
            "RootCause": "The root cause of the issue is that the proxying mechanism in the WebAppProxyServlet is attempting to bind to a local address that is not available, likely due to the configuration of the proxy host or the state of the ResourceManager in HA mode.",
            "StepsToReproduce": [
                "Set up ResourceManager High Availability (HA) with the first ResourceManager not active.",
                "Run a long sleep job in the YARN cluster.",
                "Attempt to view the tracking URL on the ResourceManager applications page."
            ],
            "ExpectedBehavior": "The tracking URL should successfully redirect to the AppMaster's web interface, allowing users to view the application's status and logs.",
            "ObservedBehavior": "The application fails to load the tracking URL, resulting in a BindException and an inability to access the AppMaster interface.",
            "Suggestions": "Check the configuration of the proxy host and ensure that it is correctly set up to bind to an available address. Additionally, verify that the ResourceManager HA setup is functioning correctly and that the active ResourceManager is properly handling requests.",
            "problem_location": {
                "files": [
                    "WebAppProxyServlet.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet"
                ],
                "methods": [
                    "WebAppProxyServlet.doGet",
                    "WebAppProxyServlet.proxyLink"
                ]
            },
            "possible_fix": "Ensure that the proxy host is correctly configured in the YARN settings. Modify the proxyLink method to handle cases where the local address cannot be bound, possibly by falling back to a default address or logging a more informative error message."
        },
        "possible_fix_code": {
            "WebAppProxyServlet.proxyLink": "  private static void proxyLink(HttpServletRequest req, \n      HttpServletResponse resp, URI link, Cookie c, String proxyHost)\n      throws IOException {\n    DefaultHttpClient client = new DefaultHttpClient();\n    client\n        .getParams()\n        .setParameter(ClientPNames.COOKIE_POLICY,\n            CookiePolicy.BROWSER_COMPATIBILITY)\n        .setBooleanParameter(ClientPNames.ALLOW_CIRCULAR_REDIRECTS, true);\n    // Make sure we send the request from the proxy address in the config\n    // since that is what the AM filter checks against. IP aliasing or\n    // similar could cause issues otherwise.\n    InetAddress localAddress;\n    try {\n      localAddress = InetAddress.getByName(proxyHost);\n    } catch (UnknownHostException e) {\n      LOG.error(\"Failed to resolve proxy host: {}. Falling back to localhost.\", proxyHost);\n      localAddress = InetAddress.getByName(\"127.0.0.1\"); // Fallback to localhost\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"local InetAddress for proxy host: {}\", localAddress);\n    }\n    client.getParams()\n        .setParameter(ConnRoutePNames.LOCAL_ADDRESS, localAddress);\n    HttpGet httpGet = new HttpGet(link);\n    @SuppressWarnings(\"unchecked\")\n    Enumeration<String> names = req.getHeaderNames();\n    while(names.hasMoreElements()) {\n      String name = names.nextElement();\n      if(passThroughHeaders.contains(name)) {\n        String value = req.getHeader(name);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"REQ HEADER: {} : {}\", name, value);\n        }\n        httpGet.setHeader(name, value);\n      }\n    }\n\n    String user = req.getRemoteUser();\n    if (user != null && !user.isEmpty()) {\n      httpGet.setHeader(\"Cookie\",\n          PROXY_USER_COOKIE_NAME + \"=\" + URLEncoder.encode(user, \"ASCII\"));\n    }\n    OutputStream out = resp.getOutputStream();\n    try {\n      HttpResponse httpResp = client.execute(httpGet);\n      resp.setStatus(httpResp.getStatusLine().getStatusCode());\n      for (Header header : httpResp.getAllHeaders()) {\n        resp.setHeader(header.getName(), header.getValue());\n      }\n      if (c != null) {\n        resp.addCookie(c);\n      }\n      InputStream in = httpResp.getEntity().getContent();\n      if (in != null) {\n        IOUtils.copyBytes(in, out, 4096, true);\n      }\n    } finally {\n      httpGet.releaseConnection();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-2813.json",
        "creation_time": "2014-11-05T22:29:46.000+0000",
        "bug_report": {
            "Title": "NPE from MemoryTimelineStore.getDomains",
            "Description": "A NullPointerException (NPE) occurs in the MemoryTimelineStore.getDomains method when attempting to retrieve domains for a specified owner. The issue arises when the domainsByOwner map does not contain an entry for the provided owner, leading to a null reference when trying to iterate over the domains.",
            "StackTrace": [
                "javax.ws.rs.WebApplicationException: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getDomains(TimelineWebServices.java:356)",
                "at org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore.getDomains(MemoryTimelineStore.java:244)",
                "at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.getDomains(TimelineDataManager.java:383)",
                "Caused by: java.lang.NullPointerException"
            ],
            "RootCause": "The root cause of the NPE is that the domainsByOwner map does not contain an entry for the specified owner, resulting in a null value being returned when attempting to access the list of domains.",
            "StepsToReproduce": [
                "1. Call the getDomains method of TimelineWebServices with an owner that does not exist in the domainsByOwner map.",
                "2. Observe the resulting NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The system should return an empty TimelineDomains object or a proper error message indicating that no domains exist for the specified owner.",
            "ObservedBehavior": "The system throws a NullPointerException when trying to access domains for a non-existent owner.",
            "Suggestions": "Implement a null check for the domainsByOwner map before attempting to iterate over its contents. If the owner does not exist, return an empty TimelineDomains object instead.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/MemoryTimelineStore.java",
                    "hadoop-yarn-project/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/TimelineDataManager.java",
                    "hadoop-yarn-project/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/webapp/TimelineWebServices.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore",
                    "org.apache.hadoop.yarn.server.timeline.TimelineDataManager",
                    "org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices"
                ],
                "methods": [
                    "MemoryTimelineStore.getDomains",
                    "TimelineDataManager.getDomains",
                    "TimelineWebServices.getDomains"
                ]
            },
            "possible_fix": "In MemoryTimelineStore.getDomains, add a check for the existence of the owner in the domainsByOwner map before iterating:\n\npublic TimelineDomains getDomains(String owner) throws IOException {\n    if (!domainsByOwner.containsKey(owner)) {\n        return new TimelineDomains(); // Return empty if owner not found\n    }\n    List<TimelineDomain> domains = new ArrayList<>();\n    for (TimelineDomain domain : domainsByOwner.get(owner)) {\n        // existing logic...\n    }\n    // existing logic...\n}"
        },
        "possible_fix_code": {
            "MemoryTimelineStore.getDomains": "  public TimelineDomains getDomains(String owner)\n      throws IOException {\n    if (!domainsByOwner.containsKey(owner)) {\n        return new TimelineDomains(); // Return empty if owner not found\n    }\n    List<TimelineDomain> domains = new ArrayList<TimelineDomain>();\n    for (TimelineDomain domain : domainsByOwner.get(owner)) {\n      TimelineDomain domainToReturn = createTimelineDomain(\n          domain.getId(),\n          domain.getDescription(),\n          domain.getOwner(),\n          domain.getReaders(),\n          domain.getWriters(),\n          domain.getCreatedTime(),\n          domain.getModifiedTime());\n      domains.add(domainToReturn);\n    }\n    Collections.sort(domains, new Comparator<TimelineDomain>() {\n      @Override\n      public int compare(\n          TimelineDomain domain1, TimelineDomain domain2) {\n         int result = domain2.getCreatedTime().compareTo(\n             domain1.getCreatedTime());\n         if (result == 0) {\n           return domain2.getModifiedTime().compareTo(\n               domain1.getModifiedTime());\n         } else {\n           return result;\n         }\n      }\n    });\n    TimelineDomains domainsToReturn = new TimelineDomains();\n    domainsToReturn.addDomains(domains);\n    return domainsToReturn;\n  }"
        }
    },
    {
        "filename": "YARN-5006.json",
        "creation_time": "2016-04-28T08:26:38.000+0000",
        "bug_report": {
            "Title": "ResourceManager quits due to ApplicationStateData exceeding znode size limit in ZooKeeper",
            "Description": "When a client submits a job that adds a large number of files (e.g., 10,000) to the DistributedCache, the ResourceManager attempts to store the ApplicationStateData in ZooKeeper. This data exceeds the maximum size limit for a znode in ZooKeeper, leading to a ConnectionLossException and causing the ResourceManager to exit unexpectedly. The issue arises during the execution of the `storeApplicationStateInternal` method in the `ZKRMStateStore` class.",
            "StackTrace": [
                "org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)",
                "at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:931)",
                "at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:911)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$4.run(ZKRMStateStore.java:936)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$4.run(ZKRMStateStore.java:933)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:1075)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:1096)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.doMultiWithRetries(ZKRMStateStore.java:933)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.storeApplicationStateInternal(ZKRMStateStore.java:626)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition.transition(RMStateStore.java:138)"
            ],
            "RootCause": "The ApplicationStateData being stored exceeds the maximum size limit for a znode in ZooKeeper, leading to a ConnectionLossException and subsequent ResourceManager exit.",
            "StepsToReproduce": [
                "1. Submit a job that adds a large number of files (e.g., 10,000) to the DistributedCache.",
                "2. Monitor the ResourceManager logs for errors related to znode size limits."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully store the ApplicationStateData in ZooKeeper without exceeding the znode size limit.",
            "ObservedBehavior": "The ResourceManager exits with a fatal error due to exceeding the znode size limit in ZooKeeper.",
            "Suggestions": "Consider implementing a mechanism to limit the size of ApplicationStateData or to split the data into multiple znodes to avoid exceeding the size limit.",
            "problem_location": {
                "files": [
                    "ZKRMStateStore.java",
                    "RMStateStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore",
                    "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"
                ],
                "methods": [
                    "ZKRMStateStore.storeApplicationStateInternal",
                    "RMStateStore.handleStoreEvent"
                ]
            },
            "possible_fix": "Implement a check in the `storeApplicationStateInternal` method to ensure that the size of `appStateData` does not exceed the znode size limit. If it does, either truncate the data or split it into multiple znodes."
        },
        "possible_fix_code": {
            "storeApplicationStateInternal": "  public synchronized void storeApplicationStateInternal(ApplicationId appId,\n      ApplicationStateData appStateDataPB) throws Exception {\n    String nodeCreatePath = getNodePath(rmAppRoot, appId.toString());\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Storing info for app: \" + appId + \" at: \" + nodeCreatePath);\n    }\n    byte[] appStateData = appStateDataPB.getProto().toByteArray();\n\n    // Check the size of appStateData before storing\n    final int MAX_ZNODE_SIZE = 1048576; // Example size limit (1MB)\n    if (appStateData.length > MAX_ZNODE_SIZE) {\n      // Handle the case where the data exceeds the znode size limit\n      // Option 1: Truncate the data (not recommended as it may lead to data loss)\n      // appStateData = Arrays.copyOf(appStateData, MAX_ZNODE_SIZE);\n\n      // Option 2: Split the data into multiple znodes\n      int numChunks = (int) Math.ceil((double) appStateData.length / MAX_ZNODE_SIZE);\n      for (int i = 0; i < numChunks; i++) {\n        int start = i * MAX_ZNODE_SIZE;\n        int length = Math.min(appStateData.length - start, MAX_ZNODE_SIZE);\n        byte[] chunk = Arrays.copyOfRange(appStateData, start, start + length);\n        String chunkPath = nodeCreatePath + \"/chunk\" + i;\n        safeCreate(chunkPath, chunk, zkAcl, CreateMode.PERSISTENT);\n      }\n    } else {\n      safeCreate(nodeCreatePath, appStateData, zkAcl,\n          CreateMode.PERSISTENT);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-5728.json",
        "creation_time": "2016-10-13T05:16:28.000+0000",
        "bug_report": {
            "Title": "TestMiniYarnClusterNodeUtilization.testUpdateNodeUtilization timeout",
            "Description": "The test 'testUpdateNodeUtilization' in the 'TestMiniYARNClusterNodeUtilization' class is failing due to a timeout after 60 seconds. The failure occurs during the invocation of the 'nodeHeartbeat' method, which is expected to complete within the allotted time but does not, leading to a timeout exception. This indicates that the method may be stuck or taking longer than expected to respond.",
            "StackTrace": [
                "java.lang.Exception: test timed out after 60000 milliseconds",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.processWaitTimeAndRetryInfo(RetryInvocationHandler.java:130)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:107)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)",
                "at com.sun.proxy.$Proxy85.nodeHeartbeat(Unknown Source)",
                "at org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization.testUpdateNodeUtilization(TestMiniYarnClusterNodeUtilization.java:113)"
            ],
            "RootCause": "The root cause of the timeout appears to be related to the 'nodeHeartbeat' method invocation, which is likely not returning a response in a timely manner, possibly due to network issues, resource contention, or an unresponsive node.",
            "StepsToReproduce": [
                "Run the test suite for 'TestMiniYARNClusterNodeUtilization'.",
                "Observe the failure of the 'testUpdateNodeUtilization' test case due to timeout."
            ],
            "ExpectedBehavior": "The 'testUpdateNodeUtilization' test should complete successfully within the specified timeout period, indicating that the node heartbeat is processed correctly.",
            "ObservedBehavior": "The test fails with a timeout exception after 60 seconds, indicating that the expected response from the 'nodeHeartbeat' method was not received in time.",
            "Suggestions": "Investigate the implementation of the 'nodeHeartbeat' method to ensure it can handle requests efficiently. Consider adding logging to track the execution time and identify potential bottlenecks. Additionally, review the network configuration and resource allocation for the MiniYARN cluster to ensure it can handle the load during testing.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java",
                    "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/src/test/java/org/apache/hadoop/yarn/server/TestMiniYarnClusterNodeUtilization.java"
                ],
                "classes": [
                    "org.apache.hadoop.io.retry.RetryInvocationHandler",
                    "org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization"
                ],
                "methods": [
                    "RetryInvocationHandler.invoke",
                    "TestMiniYarnClusterNodeUtilization.testUpdateNodeUtilization"
                ]
            },
            "possible_fix": "Review the 'nodeHeartbeat' method implementation to ensure it is not blocking indefinitely. Consider implementing a timeout mechanism or improving the handling of network calls to avoid long wait times. Additionally, ensure that the MiniYARN cluster is properly configured to handle the expected load during tests."
        },
        "possible_fix_code": {
            "RetryInvocationHandler.invoke": "  public Object invoke(Object proxy, Method method, Object[] args)\n      throws Throwable {\n    final boolean isRpc = isRpcInvocation(proxyDescriptor.getProxy());\n    final int callId = isRpc? Client.nextCallId(): RpcConstants.INVALID_CALL_ID;\n\n    final Call call = newCall(method, args, isRpc, callId);\n    long startTime = System.currentTimeMillis(); // Start time tracking\n    long timeout = 60000; // Set timeout duration to 60 seconds\n\n    while (true) {\n      final CallReturn c = call.invokeOnce();\n      final CallReturn.State state = c.getState();\n      if (state == CallReturn.State.ASYNC_INVOKED) {\n        return null; // return null for async calls\n      } else if (c.getState() != CallReturn.State.RETRY) {\n        return c.getReturnValue();\n      }\n\n      // Check for timeout\n      if (System.currentTimeMillis() - startTime > timeout) {\n        throw new TimeoutException(\"Invocation timed out after \" + timeout + \" milliseconds.\");\n      }\n    }\n  }"
        }
    },
    {
        "filename": "YARN-2805.json",
        "creation_time": "2014-11-04T20:37:09.000+0000",
        "bug_report": {
            "Title": "RM2 in HA setup tries to login using the RM1's kerberos principal",
            "Description": "In a High Availability (HA) setup, the ResourceManager (RM2) is attempting to authenticate using the Kerberos principal of the first ResourceManager (RM1). This results in a login failure due to the incorrect principal being used, leading to an IOException. The issue arises during the initialization of the ResourceManager when it tries to log in using the keytab file associated with RM1's principal instead of its own.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to login",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:211)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1229)",
                "Caused by: java.io.IOException: Login failure for rm/IP@EXAMPLE.COM from keytab /etc/security/keytabs/rm.service.keytab: javax.security.auth.login.LoginException: Unable to obtain password from user",
                "at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:935)"
            ],
            "RootCause": "The ResourceManager (RM2) is incorrectly configured to use RM1's Kerberos principal and keytab for authentication, leading to a login failure.",
            "StepsToReproduce": [
                "Set up a High Availability (HA) configuration with two ResourceManagers (RM1 and RM2).",
                "Ensure RM1 is configured with its own Kerberos principal and keytab.",
                "Configure RM2 to use RM1's Kerberos principal and keytab.",
                "Start RM2 and observe the login failure in the logs."
            ],
            "ExpectedBehavior": "RM2 should successfully authenticate using its own Kerberos principal and keytab.",
            "ObservedBehavior": "RM2 fails to log in due to using RM1's Kerberos principal, resulting in an IOException.",
            "Suggestions": "Ensure that RM2 is configured with its own unique Kerberos principal and keytab. Review the configuration files to verify that each ResourceManager has its own settings.",
            "problem_location": {
                "files": [
                    "ResourceManager.java",
                    "UserGroupInformation.java",
                    "AbstractService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager",
                    "org.apache.hadoop.security.UserGroupInformation",
                    "org.apache.hadoop.service.AbstractService"
                ],
                "methods": [
                    "ResourceManager.serviceInit",
                    "UserGroupInformation.loginUserFromKeytab",
                    "AbstractService.init"
                ]
            },
            "possible_fix": "Update the configuration for RM2 to use its own Kerberos principal and keytab. This can be done by modifying the configuration files to ensure that RM2's settings do not reference RM1's principal or keytab."
        },
        "possible_fix_code": {
            "ResourceManager.serviceInit": "    protected void serviceInit(Configuration conf) throws Exception {\n      this.shouldExitOnError =\n          conf.getBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY,\n            Dispatcher.DEFAULT_DISPATCHER_EXIT_ON_ERROR);\n\n      // Ensure RM2 uses its own Kerberos principal and keytab\n      String principal = conf.get(\"yarn.resourcemanager.principal\");\n      String keytab = conf.get(\"yarn.resourcemanager.keytab\");\n      if (principal == null || keytab == null) {\n          throw new IOException(\"ResourceManager principal and keytab must be configured.\");\n      }\n\n      // Set the principal and keytab for the current ResourceManager\n      UserGroupInformation.setConfiguration(conf);\n      UserGroupInformation.loginUserFromKeytab(principal, keytab);\n\n      super.serviceInit(conf);\n    }"
        }
    },
    {
        "filename": "YARN-4744.json",
        "creation_time": "2016-02-29T10:08:57.000+0000",
        "bug_report": {
            "Title": "Too many signal to container failure in case of LCE",
            "Description": "When running a MapReduce application (terasort/teragen) in a secure HA cluster with LCE enabled, the system encounters a failure due to excessive signals sent to the container. The issue arises when the container executor fails to signal the container properly, resulting in an ExitCodeException with exit code 9. This indicates a failure in executing the privileged operation required to signal the container, which is critical for managing container lifecycle events.",
            "StackTrace": [
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:173)",
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime.signalContainer(DefaultLinuxContainerRuntime.java:132)",
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime.signalContainer(DelegatingLinuxContainerRuntime.java:109)",
                "org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.signalContainer(LinuxContainerExecutor.java:513)",
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:520)",
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:139)"
            ],
            "RootCause": "The root cause of the issue is the failure of the privileged operation to signal the container, which is indicated by the ExitCodeException with exit code 9. This suggests that the command executed by the container executor failed, possibly due to incorrect parameters or insufficient permissions.",
            "StepsToReproduce": [
                "1. Install an HA cluster in secure mode.",
                "2. Enable LCE with cgroups.",
                "3. Start the server with the dsperf user.",
                "4. Submit a MapReduce application (terasort/teragen) with the user yarn/dsperf.",
                "5. Observe the logs for 'too many signal to container' errors."
            ],
            "ExpectedBehavior": "The container should be signaled correctly without any errors, allowing it to transition through its lifecycle states without excessive failures.",
            "ObservedBehavior": "The system throws an exception indicating 'too many signal to container failure', and the container fails to execute properly, resulting in an exit code of 9.",
            "Suggestions": "Investigate the parameters being passed to the container executor and ensure that the user has the necessary permissions to execute the privileged operations. Additionally, review the implementation of the signalContainer method to handle potential errors more gracefully.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationExecutor.java",
                    "hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DefaultLinuxContainerRuntime.java",
                    "hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DelegatingLinuxContainerRuntime.java",
                    "hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                    "hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java",
                    "hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainersLauncher.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime",
                    "org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher"
                ],
                "methods": [
                    "PrivilegedOperationExecutor.executePrivilegedOperation",
                    "DefaultLinuxContainerRuntime.signalContainer",
                    "DelegatingLinuxContainerRuntime.signalContainer",
                    "LinuxContainerExecutor.signalContainer",
                    "ContainerLaunch.cleanupContainer",
                    "ContainersLauncher.handle"
                ]
            },
            "possible_fix": "Review the implementation of the signalContainer method in DefaultLinuxContainerRuntime and ensure that the parameters passed to the PrivilegedOperation are correct. Additionally, check the permissions of the dsperf user to ensure it can execute the required commands without failure."
        },
        "possible_fix_code": {
            "DefaultLinuxContainerRuntime.signalContainer": "  public void signalContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    Container container = ctx.getContainer();\n    PrivilegedOperation signalOp = new PrivilegedOperation(\n        PrivilegedOperation.OperationType.SIGNAL_CONTAINER, (String) null);\n\n    signalOp.appendArgs(ctx.getExecutionAttribute(RUN_AS_USER),\n        ctx.getExecutionAttribute(USER),\n        Integer.toString(PrivilegedOperation.RunAsUserCommand\n            .SIGNAL_CONTAINER.getValue()),\n        ctx.getExecutionAttribute(PID),\n        Integer.toString(ctx.getExecutionAttribute(SIGNAL).getValue()));\n\n    try {\n      PrivilegedOperationExecutor executor = PrivilegedOperationExecutor\n          .getInstance(conf);\n\n      // Check if the user has the necessary permissions before executing the privileged operation\n      if (!hasPermission(ctx.getExecutionAttribute(USER))) {\n        throw new IOException(\"User does not have permission to signal the container.\");\n      }\n\n      executor.executePrivilegedOperation(null,\n          signalOp, null, container.getLaunchContext().getEnvironment(),\n          false);\n    } catch (PrivilegedOperationException e) {\n      LOG.warn(\"Signal container failed. Exception: \", e);\n\n      throw new ContainerExecutionException(\"Signal container failed\", e\n          .getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }\n\n  private boolean hasPermission(String user) {\n    // Implement permission check logic here\n    // For example, check if the user is in a list of allowed users\n    return true; // Placeholder for actual permission check\n  }"
        }
    },
    {
        "filename": "YARN-1752.json",
        "creation_time": "2014-02-22T05:51:42.000+0000",
        "bug_report": {
            "Title": "Unexpected Unregistered event at Attempt Launched state",
            "Description": "The application is encountering an InvalidStateTransitionException when attempting to handle an UNREGISTERED event while in the LAUNCHED state. This indicates that the state machine does not allow transitioning from LAUNCHED to UNREGISTERED, which is causing the application to fail to process the event correctly.",
            "StackTrace": [
                "2014-02-21 14:56:03,453 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: UNREGISTERED at LAUNCHED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:647)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:103)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:714)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:695)"
            ],
            "RootCause": "The root cause of the issue is that the state machine does not permit the transition from the LAUNCHED state to the UNREGISTERED state, leading to an InvalidStateTransitionException when the event is processed.",
            "StepsToReproduce": [
                "1. Launch an application in the YARN ResourceManager.",
                "2. Trigger an UNREGISTERED event while the application is in the LAUNCHED state.",
                "3. Observe the logs for the InvalidStateTransitionException."
            ],
            "ExpectedBehavior": "The application should handle the UNREGISTERED event appropriately without throwing an exception, possibly by transitioning to a valid state.",
            "ObservedBehavior": "The application throws an InvalidStateTransitionException and fails to process the UNREGISTERED event while in the LAUNCHED state.",
            "Suggestions": "Review the state machine transitions to ensure that the UNREGISTERED event can be handled appropriately from the LAUNCHED state, or modify the event handling logic to prevent this transition.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/state/StateMachineFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory"
                ],
                "methods": [
                    "RMAppAttemptImpl.handle",
                    "StateMachineFactory.doTransition"
                ]
            },
            "possible_fix": "To resolve this issue, modify the state machine to allow a transition from LAUNCHED to UNREGISTERED, or handle the UNREGISTERED event in a way that does not require this transition. For example, you could add a check in the handle method to log a warning instead of throwing an exception if the event is UNREGISTERED while in the LAUNCHED state."
        },
        "possible_fix_code": {
            "RMAppAttemptImpl.handle": "  public void handle(RMAppAttemptEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationAttemptId appAttemptID = event.getApplicationAttemptId();\n      LOG.debug(\"Processing event for \" + appAttemptID + \" of type \"\n          + event.getType());\n      final RMAppAttemptState oldState = getAppAttemptState();\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        if (event.getType() == RMAppAttemptEventType.UNREGISTERED && oldState == RMAppAttemptState.LAUNCHED) {\n          LOG.warn(\"Received UNREGISTERED event while in LAUNCHED state. Ignoring the event.\");\n        } else {\n          LOG.error(\"Can't handle this event at current state\", e);\n          /* TODO fail the application on the failed transition */\n        }\n      }\n\n      if (oldState != getAppAttemptState()) {\n        LOG.info(appAttemptID + \" State change from \" + oldState + \" to \"\n            + getAppAttemptState());\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-6629.json",
        "creation_time": "2017-05-22T08:31:16.000+0000",
        "bug_report": {
            "Title": "NPE occurred when container allocation proposal is applied but its resource requests are removed before",
            "Description": "A NullPointerException (NPE) is thrown during the allocation process in the YARN resource manager when a container allocation proposal is applied after its associated resource requests have been removed. This occurs specifically in the AppSchedulingInfo.allocate method when it attempts to access a placement set that no longer exists due to the removal of the resource request.",
            "StackTrace": [
                "FATAL event.EventDispatcher (EventDispatcher.java:run(75)) - Error in handling event type NODE_UPDATE to the Event Dispatcher",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:446)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.apply(FiCaSchedulerApp.java:516)",
                "at org.apache.hadoop.yarn.client.TestNegativePendingResource$1.answer(TestNegativePendingResource.java:225)",
                "at org.mockito.internal.stubbing.StubbedInvocationMatcher.answer(StubbedInvocationMatcher.java:31)",
                "at org.mockito.internal.MockHandler.handle(MockHandler.java:97)",
                "at org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:47)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp$$EnhancerByMockitoWithCGLIB$$29eb8afc.apply(<generated>)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.tryCommit(CapacityScheduler.java:2396)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.submitResourceCommitRequest(CapacityScheduler.java:2281)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateOrReserveNewContainers(CapacityScheduler.java:1247)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainerOnSingleNode(CapacityScheduler.java:1236)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1325)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:987)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1367)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:143)",
                "at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the NPE is that the method AppSchedulingInfo.allocate is trying to access a placement set from the schedulerKeyToPlacementSets map using a key that has been removed. This happens after the resource request associated with that key has been removed, leading to a null reference when attempting to call allocate on it.",
            "StepsToReproduce": [
                "1. Start the Application Master (AM) and request 1 container with schedulerRequestKey#1.",
                "2. The scheduler allocates 1 container for this request and accepts the proposal.",
                "3. The AM removes this request, which removes schedulerRequestKey#1 from schedulerKeyToPlacementSets.",
                "4. The scheduler then attempts to apply this proposal, leading to the NPE when it tries to access the now-nonexistent placement set."
            ],
            "ExpectedBehavior": "The system should handle the removal of resource requests gracefully without throwing a NullPointerException, allowing for proper resource allocation and scheduling.",
            "ObservedBehavior": "A NullPointerException is thrown when the scheduler attempts to allocate resources after the associated resource requests have been removed.",
            "Suggestions": "Implement a check in the AppSchedulingInfo.allocate method to ensure that the placement set exists before attempting to call allocate on it. If the placement set is null, handle the situation appropriately, possibly by logging a warning and skipping the allocation.",
            "problem_location": {
                "files": [
                    "AppSchedulingInfo.java",
                    "FiCaSchedulerApp.java",
                    "CapacityScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"
                ],
                "methods": [
                    "AppSchedulingInfo.allocate",
                    "FiCaSchedulerApp.apply",
                    "CapacityScheduler.tryCommit"
                ]
            },
            "possible_fix": "In the AppSchedulingInfo.allocate method, add a null check for the placement set before calling allocate. For example:\n\nif (schedulerKeyToPlacementSets.get(schedulerKey) != null) {\n    return schedulerKeyToPlacementSets.get(schedulerKey).allocate(schedulerKey, type, node);\n} else {\n    LOG.warn(\"Placement set for scheduler key \" + schedulerKey + \" is null. Skipping allocation.\");\n    return Collections.emptyList();\n}"
        },
        "possible_fix_code": {
            "AppSchedulingInfo.allocate": "  public List<ResourceRequest> allocate(NodeType type,\n      SchedulerNode node, SchedulerRequestKey schedulerKey,\n      Container containerAllocated) {\n    try {\n      writeLock.lock();\n\n      if (null != containerAllocated) {\n        updateMetricsForAllocatedContainer(type, containerAllocated);\n      }\n\n      PlacementSet<FiCaSchedulerNode> placementSet = schedulerKeyToPlacementSets.get(schedulerKey);\n      if (placementSet != null) {\n        return placementSet.allocate(schedulerKey, type, node);\n      } else {\n        LOG.warn(\"Placement set for scheduler key \" + schedulerKey + \" is null. Skipping allocation.\");\n        return Collections.emptyList();\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-3493.json",
        "creation_time": "2015-04-15T22:03:19.000+0000",
        "bug_report": {
            "Title": "RM fails to come up with error 'Failed to load/recover state' when memory settings are changed",
            "Description": "The ResourceManager (RM) fails to start after changing memory settings in yarn-site.xml. Specifically, when yarn.nodemanager.resource.memory-mb and yarn.scheduler.maximum-allocation-mb are set to 4000, and a job is started with mapreduce.map.memory.mb=4000, the RM fails to recover after the yarn-site.xml is reverted to a lower memory limit of 2048 before the job completes. This results in an InvalidResourceRequestException due to the requested memory exceeding the maximum configured memory.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested memory < 0, or requested memory > max configured, requestedMemory=3072, maxMemory=2048",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:204)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.validateAndCreateResourceRequest(RMAppManager.java:385)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:328)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:317)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:422)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1187)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:574)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:994)"
            ],
            "RootCause": "The root cause of the issue is that the ResourceManager attempts to recover applications with resource requests that exceed the maximum memory limit configured in yarn-site.xml. When the memory settings are changed and the ResourceManager is restarted, it fails to validate the resource requests due to the discrepancy between the requested memory and the maximum allowed memory.",
            "StepsToReproduce": [
                "1. Change yarn.nodemanager.resource.memory-mb and yarn.scheduler.maximum-allocation-mb to 4000 in yarn-site.xml.",
                "2. Start a randomtextwriter job with mapreduce.map.memory.mb=4000 in the background and wait for the job to reach the running state.",
                "3. Restore yarn-site.xml to have yarn.scheduler.maximum-allocation-mb set to 2048 before the above job completes.",
                "4. Restart the ResourceManager."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully and recover the application state without errors.",
            "ObservedBehavior": "The ResourceManager fails to start and logs an error indicating an invalid resource request due to exceeding the maximum configured memory.",
            "Suggestions": "Consider implementing a check to validate resource requests against the current configuration before attempting to recover applications. Additionally, ensure that the ResourceManager can handle changes in configuration gracefully without leading to invalid states.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java",
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager",
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"
                ],
                "methods": [
                    "RMAppManager.validateAndCreateResourceRequest",
                    "RMAppManager.recoverApplication",
                    "ResourceManager.recover"
                ]
            },
            "possible_fix": "In the validateAndCreateResourceRequest method, add a check to ensure that the requested memory does not exceed the maximum configured memory before proceeding with the recovery of applications. This can prevent the ResourceManager from attempting to recover applications with invalid resource requests."
        },
        "possible_fix_code": {
            "RMAppManager.validateAndCreateResourceRequest": "  private ResourceRequest validateAndCreateResourceRequest(\n      ApplicationSubmissionContext submissionContext)\n      throws InvalidResourceRequestException {\n    // Validation of the ApplicationSubmissionContext needs to be completed\n    // here. Only those fields that are dependent on RM's configuration are\n    // checked here as they have to be validated whether they are part of new\n    // submission or just being recovered.\n\n    // Check whether AM resource requirements are within required limits\n    if (!submissionContext.getUnmanagedAM()) {\n      ResourceRequest amReq;\n      if (submissionContext.getAMContainerResourceRequest() != null) {\n        amReq = submissionContext.getAMContainerResourceRequest();\n      } else {\n        amReq =\n            BuilderUtils.newResourceRequest(\n                RMAppAttemptImpl.AM_CONTAINER_PRIORITY, ResourceRequest.ANY,\n                submissionContext.getResource(), 1);\n      }\n\n      // Validate requested memory against maximum configured memory\n      if (amReq.getCapability().getMemory() > scheduler.getMaximumResourceCapability().getMemory()) {\n        throw new InvalidResourceRequestException(\"Requested memory exceeds maximum configured memory.\");\n      }\n\n      // set label expression for AM container\n      if (null == amReq.getNodeLabelExpression()) {\n        amReq.setNodeLabelExpression(submissionContext\n            .getNodeLabelExpression());\n      }\n\n      try {\n        SchedulerUtils.validateResourceRequest(amReq,\n            scheduler.getMaximumResourceCapability(),\n            submissionContext.getQueue(), scheduler);\n      } catch (InvalidResourceRequestException e) {\n        LOG.warn(\"RM app submission failed in validating AM resource request\"\n            + \" for application \" + submissionContext.getApplicationId(), e);\n        throw e;\n      }\n      SchedulerUtils.normalizeRequest(amReq, scheduler.getResourceCalculator(),\n          scheduler.getClusterResource(),\n          scheduler.getMinimumResourceCapability(),\n          scheduler.getMaximumResourceCapability(),\n          scheduler.getMinimumResourceCapability());\n      return amReq;\n    }\n\n    return null;\n  }"
        }
    },
    {
        "filename": "YARN-7645.json",
        "creation_time": "2017-12-12T21:19:53.000+0000",
        "bug_report": {
            "Title": "TestContainerResourceUsage#testUsageAfterAMRestartWithMultipleContainers is flakey with FairScheduler",
            "Description": "The test case 'testUsageAfterAMRestartWithMultipleContainers' in the 'TestContainerResourceUsage' class is exhibiting flakiness when using the 'FairScheduler'. The test fails with an AssertionError indicating that the expected state of the application master (AM) is 'ALLOCATED', but the actual state is 'SCHEDULED'. This discrepancy suggests that the resource allocation process is not functioning as intended after an AM restart, potentially due to timing issues or race conditions in the scheduling logic.",
            "StackTrace": [
                "java.lang.AssertionError: Attempt state is not correct (timeout). expected:<ALLOCATED> but was:<SCHEDULED>",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestContainerResourceUsage.amRestartTests(TestContainerResourceUsage.java:275)",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestContainerResourceUsage.testUsageAfterAMRestartWithMultipleContainers(TestContainerResourceUsage.java:254)"
            ],
            "RootCause": "The root cause appears to be a timing issue in the resource allocation process after the application master (AM) is restarted. The FairScheduler may not be allocating resources in a timely manner, leading to the AM being in the 'SCHEDULED' state instead of 'ALLOCATED'.",
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with FairScheduler configured.",
                "2. Run the 'TestContainerResourceUsage#testUsageAfterAMRestartWithMultipleContainers' test.",
                "3. Observe the test results for flakiness or failures."
            ],
            "ExpectedBehavior": "After restarting the application master, the expected state of the application should transition to 'ALLOCATED' without delays or inconsistencies.",
            "ObservedBehavior": "The application master remains in the 'SCHEDULED' state, leading to an AssertionError in the test.",
            "Suggestions": "Investigate the scheduling logic in the FairScheduler to ensure that resource allocation is handled correctly after an AM restart. Consider adding synchronization mechanisms or increasing timeouts to allow for proper state transitions.",
            "problem_location": {
                "files": [
                    "TestContainerResourceUsage.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.TestContainerResourceUsage"
                ],
                "methods": [
                    "amRestartTests",
                    "testUsageAfterAMRestartWithMultipleContainers"
                ]
            },
            "possible_fix": "Review the FairScheduler's resource allocation logic and ensure that it properly handles state transitions after an AM restart. Consider implementing a retry mechanism or increasing the timeout duration in the test to accommodate potential delays in resource allocation."
        },
        "possible_fix_code": {
            "testUsageAfterAMRestartWithMultipleContainers": "public void testUsageAfterAMRestartWithMultipleContainers() throws Exception {\n    // Setup code for the test\n    // ...\n\n    // Restart the application master\n    restartApplicationMaster();\n\n    // Wait for the application master to reach the ALLOCATED state\n    int retries = 10;\n    while (retries > 0) {\n        String state = getApplicationMasterState();\n        if (state.equals(\"ALLOCATED\")) {\n            break;\n        }\n        Thread.sleep(1000); // Wait for 1 second before checking again\n        retries--;\n    }\n\n    // Assert that the application master is in the ALLOCATED state\n    String finalState = getApplicationMasterState();\n    Assert.assertEquals(\"Expected application master state to be ALLOCATED\", \"ALLOCATED\", finalState);\n}"
        }
    },
    {
        "filename": "YARN-6054.json",
        "creation_time": "2017-01-04T20:58:59.000+0000",
        "bug_report": {
            "Title": "TimelineServer fails to start when some LevelDb state files are missing.",
            "Description": "The TimelineServer fails to initialize due to missing LevelDb state files, resulting in a ServiceStateException. The server should ideally handle such scenarios gracefully instead of failing to start completely.",
            "StackTrace": [
                "org.apache.hadoop.service.ServiceStateException: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/leveldb-timeline-store.ldb/127897.sst",
                "at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:172)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.serviceInit(ApplicationHistoryServer.java:104)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.launchAppHistoryServer(ApplicationHistoryServer.java:172)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.main(ApplicationHistoryServer.java:182)"
            ],
            "RootCause": "The root cause of the issue is the absence of required LevelDb state files, which leads to a corruption error when the TimelineServer attempts to initialize the LeveldbTimelineStore.",
            "StepsToReproduce": [
                "1. Remove some LevelDb state files from the expected directory.",
                "2. Attempt to start the TimelineServer.",
                "3. Observe the failure in initialization due to missing files."
            ],
            "ExpectedBehavior": "The TimelineServer should start successfully, even if some LevelDb state files are missing, possibly by using default values or handling the error gracefully.",
            "ObservedBehavior": "The TimelineServer fails to start completely, throwing a ServiceStateException due to missing LevelDb state files.",
            "Suggestions": "Implement error handling in the serviceInit method of the LeveldbTimelineStore to allow the TimelineServer to start even when some state files are missing.",
            "problem_location": {
                "files": [
                    "ApplicationHistoryServer.java",
                    "LeveldbTimelineStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer",
                    "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"
                ],
                "methods": [
                    "ApplicationHistoryServer.serviceInit",
                    "LeveldbTimelineStore.serviceInit"
                ]
            },
            "possible_fix": "In the LeveldbTimelineStore.serviceInit method, add a check for missing files and implement a fallback mechanism to allow the server to start with default configurations or log a warning instead of throwing an exception."
        },
        "possible_fix_code": {
            "LeveldbTimelineStore.serviceInit": "  protected void serviceInit(Configuration conf) throws Exception {\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_TTL_MS,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_TTL_MS) > 0,\n        \"%s property value should be greater than zero\",\n        YarnConfiguration.TIMELINE_SERVICE_TTL_MS);\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_TTL_INTERVAL_MS,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_TTL_INTERVAL_MS) > 0,\n        \"%s property value should be greater than zero\",\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_TTL_INTERVAL_MS);\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE) >= 0,\n        \"%s property value should be greater than or equal to zero\",\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE);\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_START_TIME_READ_CACHE_SIZE,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_START_TIME_READ_CACHE_SIZE) > 0,\n        \" %s property value should be greater than zero\",\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_START_TIME_READ_CACHE_SIZE);\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_START_TIME_WRITE_CACHE_SIZE,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_START_TIME_WRITE_CACHE_SIZE) > 0,\n        \"%s property value should be greater than zero\",\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_START_TIME_WRITE_CACHE_SIZE);\n\n    Options options = new Options();\n    options.createIfMissing(true);\n    options.cacheSize(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE));\n    JniDBFactory factory = new JniDBFactory();\n    Path dbPath = new Path(\n        conf.get(YarnConfiguration.TIMELINE_SERVICE_LEVELDB_PATH), FILENAME);\n    FileSystem localFS = null;\n    try {\n      localFS = FileSystem.getLocal(conf);\n      if (!localFS.exists(dbPath)) {\n        if (!localFS.mkdirs(dbPath)) {\n          throw new IOException(\"Couldn't create directory for leveldb timeline store \" + dbPath);\n        }\n        localFS.setPermission(dbPath, LEVELDB_DIR_UMASK);\n      }\n    } finally {\n      IOUtils.cleanup(LOG, localFS);\n    }\n    LOG.info(\"Using leveldb path \" + dbPath);\n    try {\n      db = factory.open(new File(dbPath.toString()), options);\n    } catch (Exception e) {\n      LOG.warn(\"Failed to open LevelDB, starting with default configuration.\");\n      // Handle the error gracefully, possibly initializing db with default values\n      db = factory.open(new File(dbPath.toString()), options); // or set db to null or a default state\n    }\n    checkVersion();\n    startTimeWriteCache =\n        Collections.synchronizedMap(new LRUMap(getStartTimeWriteCacheSize(\n            conf)));\n    startTimeReadCache =\n        Collections.synchronizedMap(new LRUMap(getStartTimeReadCacheSize(\n            conf)));\n\n    if (conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_TTL_ENABLE, true)) {\n      deletionThread = new EntityDeletionThread(conf);\n      deletionThread.start();\n    }\n\n    super.serviceInit(conf);\n  }"
        }
    },
    {
        "filename": "YARN-196.json",
        "creation_time": "2012-01-16T09:52:45.000+0000",
        "bug_report": {
            "Title": "Nodemanager should be more robust in handling connection failure to ResourceManager when a cluster is started",
            "Description": "The NodeManager (NM) fails to start when it is initiated before the ResourceManager (RM), resulting in a connection refusal error. This occurs because the NM attempts to register with the RM, which is not yet running, leading to an UndeclaredThrowableException. The NM should implement a retry mechanism or a more graceful handling of connection failures to avoid abrupt shutdowns.",
            "StackTrace": [
                "ERROR org.apache.hadoop.yarn.service.CompositeService: Error starting services org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                "org.apache.avro.AvroRuntimeException: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:149)",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:167)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:242)",
                "Caused by: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:66)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:182)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:145)",
                "Caused by: com.google.protobuf.ServiceException: java.net.ConnectException: Call From HOST-10-18-52-230/10.18.52.230 to HOST-10-18-52-250:8025 failed on connection exception: java.net.ConnectException: Connection refused",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:131)",
                "at $Proxy23.registerNodeManager(Unknown Source)",
                "Caused by: java.net.ConnectException: Connection refused",
                "at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)",
                "at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)",
                "at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)",
                "at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:659)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1141)"
            ],
            "RootCause": "The NodeManager attempts to connect to the ResourceManager before it is available, resulting in a connection refusal error.",
            "StepsToReproduce": [
                "Start the NodeManager before starting the ResourceManager.",
                "Observe the error logs for connection failure messages."
            ],
            "ExpectedBehavior": "The NodeManager should either wait for the ResourceManager to become available or handle the connection failure gracefully without shutting down.",
            "ObservedBehavior": "The NodeManager shuts down abruptly with an error message indicating a connection refusal to the ResourceManager.",
            "Suggestions": "Implement a retry mechanism in the NodeManager to periodically attempt to connect to the ResourceManager until it is available, or provide a configuration option to delay the start of the NodeManager until the ResourceManager is confirmed to be running.",
            "problem_location": {
                "files": [
                    "NodeStatusUpdaterImpl.java",
                    "NodeManager.java",
                    "ResourceTrackerPBClientImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl",
                    "org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                    "org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl"
                ],
                "methods": [
                    "NodeStatusUpdaterImpl.start",
                    "NodeManager.start",
                    "ResourceTrackerPBClientImpl.registerNodeManager"
                ]
            },
            "possible_fix": "Add a retry mechanism in the NodeStatusUpdaterImpl class to handle connection failures. For example, modify the start method to include a loop that attempts to register with the ResourceManager multiple times with a delay between attempts."
        },
        "possible_fix_code": {
            "NodeStatusUpdaterImpl.start": "  public void start() throws Exception {\n    int maxRetries = 5;\n    int retryDelay = 2000; // milliseconds\n    for (int attempt = 0; attempt < maxRetries; attempt++) {\n      try {\n        registerWithRM(); // Attempt to register with ResourceManager\n        return; // Exit if successful\n      } catch (IOException e) {\n        LOG.warn(\"Attempt \" + (attempt + 1) + \" to register with ResourceManager failed: \" + e.getMessage());\n        if (attempt < maxRetries - 1) {\n          Thread.sleep(retryDelay); // Wait before retrying\n        } else {\n          throw new Exception(\"Failed to register with ResourceManager after \" + maxRetries + \" attempts\", e);\n        }\n      }\n    }\n  }"
        }
    },
    {
        "filename": "YARN-8508.json",
        "creation_time": "2018-07-09T23:37:49.000+0000",
        "bug_report": {
            "Title": "On NodeManager container gets cleaned up before its pid file is created",
            "Description": "The issue arises when a container is being killed, and the cleanup process is initiated before the PID file for the container is created. This leads to a situation where the system cannot retrieve the PID of the container, resulting in a failure to release GPU resources properly. The logs indicate that the container transitions to a 'CLEANEDUP' state before the PID file is available, causing subsequent requests for GPU resources to fail due to insufficient availability.",
            "StackTrace": [
                "2018-07-06 05:22:31,358 INFO  launcher.ContainerLaunch (ContainerLaunch.java:getContainerPid(1102)) - Could not get pid for container_e20_1530854311763_0006_01_000002. Waited for 5000 ms.",
                "2018-07-06 05:22:31,358 WARN  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(784)) - Container clean up before pid file created container_e20_1530854311763_0006_01_000002",
                "2018-07-06 05:22:39,048 ERROR nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:handleLaunchForLaunchType(550)) - ResourceHandlerChain.preStart() failed!",
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerException: Failed to find enough GPUs, requestor=container_e20_1530854311763_0007_01_000002, #RequestedGPUs=2, #availableGpus=1"
            ],
            "RootCause": "The root cause of the issue is that the container cleanup process is triggered before the PID file is created, which prevents the system from obtaining the PID necessary for managing GPU resources. This is primarily due to the timing of the cleanup operations in the ContainerLaunch class.",
            "StepsToReproduce": [
                "1. Launch a container that requests GPU resources.",
                "2. Kill the container before the PID file is created.",
                "3. Observe the logs for cleanup messages and GPU allocation failures."
            ],
            "ExpectedBehavior": "The system should create the PID file for the container before initiating the cleanup process, allowing for proper management of GPU resources and preventing allocation failures.",
            "ObservedBehavior": "The system attempts to clean up the container before the PID file is created, leading to warnings about the inability to retrieve the PID and subsequent failures in GPU resource allocation.",
            "Suggestions": "Implement a check to ensure that the PID file is created before the cleanup process is initiated. This may involve modifying the timing of the cleanup operations in the ContainerLaunch class.",
            "problem_location": {
                "files": [
                    "ContainerLaunch.java",
                    "LinuxContainerExecutor.java",
                    "GpuResourceAllocator.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch",
                    "org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator"
                ],
                "methods": [
                    "ContainerLaunch.cleanupContainer",
                    "LinuxContainerExecutor.handleLaunchForLaunchType",
                    "GpuResourceAllocator.assignGpus"
                ]
            },
            "possible_fix": "Modify the cleanupContainer method in ContainerLaunch.java to ensure that it only executes after the PID file has been successfully created. This can be done by adding a check for the existence of the PID file before proceeding with the cleanup logic."
        },
        "possible_fix_code": {
            "ContainerLaunch.cleanupContainer": "  public void cleanupContainer() {\n    // Check if the PID file exists before proceeding with cleanup\n    if (pidFilePath != null && pidFilePath.toFile().exists()) {\n      // Proceed with cleanup logic\n      // ... existing cleanup logic ...\n    } else {\n      LOG.warn(\"Cleanup initiated before PID file creation. Aborting cleanup.\");\n    }\n  }"
        }
    },
    {
        "filename": "YARN-2308.json",
        "creation_time": "2014-07-17T10:01:57.000+0000",
        "bug_report": {
            "Title": "NPE occurred when RM restart after CapacityScheduler queue configuration changed",
            "Description": "A NullPointerException (NPE) is thrown during the ResourceManager (RM) restart process after modifying the CapacityScheduler queue configuration. This issue arises when the RM attempts to recover historical applications, and if any of the queues associated with these applications have been removed, the NPE is triggered. The stack trace indicates that the error occurs in the `addApplicationAttempt` method of the `CapacityScheduler` class, specifically when trying to access a queue that no longer exists.",
            "StackTrace": [
                "2014-07-16 07:22:46,957 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ATTEMPT_ADDED to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:566)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:922)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:98)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:594)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:654)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:85)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:698)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:682)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The root cause of the NPE is that the `addApplicationAttempt` method in the `CapacityScheduler` class is trying to access a queue that has been removed from the configuration. This occurs during the recovery of historical applications when the RM restarts.",
            "StepsToReproduce": [
                "1. Modify the CapacityScheduler queue configuration by removing existing queues and adding new ones.",
                "2. Restart the ResourceManager.",
                "3. Observe the logs for a NullPointerException during the recovery of historical applications."
            ],
            "ExpectedBehavior": "The ResourceManager should restart successfully and recover all historical applications without throwing any exceptions.",
            "ObservedBehavior": "The ResourceManager fails to restart and throws a NullPointerException due to missing queues in the configuration.",
            "Suggestions": "Ensure that the queue configuration is consistent and that all queues referenced by historical applications are present before restarting the ResourceManager. Implement checks to handle cases where queues may have been removed.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler",
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl"
                ],
                "methods": [
                    "CapacityScheduler.addApplicationAttempt",
                    "CapacityScheduler.handle",
                    "RMAppImpl.handle"
                ]
            },
            "possible_fix": "Add validation in the `addApplicationAttempt` method to check if the queue associated with the application attempt exists before proceeding. If the queue does not exist, log an error and skip the attempt addition to prevent the NPE."
        },
        "possible_fix_code": {
            "CapacityScheduler.addApplicationAttempt": "  private synchronized void addApplicationAttempt(\n      ApplicationAttemptId applicationAttemptId,\n      boolean transferStateFromPreviousAttempt,\n      boolean isAttemptRecovering) {\n    SchedulerApplication<FiCaSchedulerApp> application =\n        applications.get(applicationAttemptId.getApplicationId());\n    CSQueue queue = (CSQueue) application.getQueue();\n\n    // Check if the queue exists before proceeding\n    if (queue == null) {\n      LOG.error(\"Queue for application attempt \" + applicationAttemptId + \" does not exist. Skipping attempt addition.\");\n      return; // Skip adding the application attempt if the queue is null\n    }\n\n    FiCaSchedulerApp attempt =\n        new FiCaSchedulerApp(applicationAttemptId, application.getUser(),\n          queue, queue.getActiveUsersManager(), rmContext);\n    if (transferStateFromPreviousAttempt) {\n      attempt.transferStateFromPreviousAttempt(application\n        .getCurrentAppAttempt());\n    }\n    application.setCurrentAppAttempt(attempt);\n\n    queue.submitApplicationAttempt(attempt, application.getUser());\n    LOG.info(\"Added Application Attempt \" + applicationAttemptId\n        + \" to scheduler from user \" + application.getUser() + \" in queue \"\n        + queue.getQueueName());\n    if (isAttemptRecovering) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(applicationAttemptId\n            + \" is recovering. Skipping notifying ATTEMPT_ADDED\");\n      }\n    } else {\n      rmContext.getDispatcher().getEventHandler().handle(\n        new RMAppAttemptEvent(applicationAttemptId,\n            RMAppAttemptEventType.ATTEMPT_ADDED));\n    }\n  }"
        }
    },
    {
        "filename": "YARN-933.json",
        "creation_time": "2013-07-17T12:29:28.000+0000",
        "bug_report": {
            "Title": "Potential InvalidStateTransitonException: Invalid event: LAUNCHED at FINAL_SAVING",
            "Description": "The application is encountering an InvalidStateTransitonException when attempting to launch a new application attempt after a previous attempt has failed. This occurs due to the ResourceManager (RM) trying to launch an application attempt that is already in a failed state, which is not allowed by the state machine. The issue arises after a connection loss occurs, leading to the application attempt being marked as failed, but the RM still attempts to retry launching it.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: LAUNCH_FAILED at FAILED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:630)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:495)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)"
            ],
            "RootCause": "The root cause of the issue is that the ResourceManager is attempting to handle a launch event for an application attempt that is already in a FAILED state, which violates the state transition rules defined in the state machine.",
            "StepsToReproduce": [
                "Step 1: Install cluster with NodeManager (NM) on 2 machines.",
                "Step 2: Make a successful ping from ResourceManager (RM) to NM1 machine, but fail using hostname.",
                "Step 3: Execute a job.",
                "Step 4: After AppAttempt_1 allocation to NM1 is done, simulate a connection loss."
            ],
            "ExpectedBehavior": "The ResourceManager should not attempt to launch a new application attempt if the previous attempt has failed and is in a FAILED state.",
            "ObservedBehavior": "The ResourceManager attempts to launch a new application attempt for AppAttempt_1 after it has failed, resulting in an InvalidStateTransitonException.",
            "Suggestions": "Implement a check in the RMAppAttemptImpl.handle method to prevent launching a new attempt if the current state is FAILED. Additionally, ensure that the state machine correctly handles transitions from FAILED to other states.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                    "hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/state/StateMachineFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory"
                ],
                "methods": [
                    "RMAppAttemptImpl.handle",
                    "StateMachineFactory.doTransition"
                ]
            },
            "possible_fix": "In the RMAppAttemptImpl.handle method, add a condition to check if the current state is FAILED before attempting to process a launch event. If it is, log an appropriate message and return early to prevent the InvalidStateTransitonException."
        },
        "possible_fix_code": {
            "RMAppAttemptImpl.handle": "  public void handle(RMAppAttemptEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationAttemptId appAttemptID = event.getApplicationAttemptId();\n      LOG.debug(\"Processing event for \" + appAttemptID + \" of type \" + event.getType());\n      final RMAppAttemptState oldState = getAppAttemptState();\n\n      // Check if the current state is FAILED before processing the event\n      if (oldState == RMAppAttemptState.FAILED) {\n        LOG.warn(\"Ignoring event for application attempt \" + appAttemptID + \" as it is in FAILED state.\");\n        return; // Early return to prevent InvalidStateTransitonException\n      }\n\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n      }\n\n      if (oldState != getAppAttemptState()) {\n        LOG.info(appAttemptID + \" State change from \" + oldState + \" to \" + getAppAttemptState());\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-1374.json",
        "creation_time": "2013-10-30T11:49:49.000+0000",
        "bug_report": {
            "Title": "Resource Manager fails to start due to ConcurrentModificationException",
            "Description": "The Resource Manager fails to start because of a ConcurrentModificationException that occurs during the initialization process. This exception is thrown when a thread attempts to modify a collection while another thread is iterating over it. The stack trace indicates that the issue arises in the CompositeService.serviceInit method, which attempts to initialize a list of services concurrently.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)",
                "at java.util.AbstractList$Itr.next(AbstractList.java:343)",
                "at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:187)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:944)"
            ],
            "RootCause": "The root cause of the issue is a ConcurrentModificationException that occurs when the CompositeService.serviceInit method tries to initialize services from a list that is being modified concurrently.",
            "StepsToReproduce": [
                "Start the Resource Manager service.",
                "Observe the logs for the ConcurrentModificationException."
            ],
            "ExpectedBehavior": "The Resource Manager should start successfully without throwing any exceptions.",
            "ObservedBehavior": "The Resource Manager fails to start and throws a ConcurrentModificationException.",
            "Suggestions": "To mitigate this issue, ensure that the list of services is not modified while it is being iterated over. This can be achieved by synchronizing access to the list or by using a thread-safe collection.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                    "hadoop-common/src/main/java/org/apache/hadoop/service/CompositeService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager",
                    "org.apache.hadoop.service.CompositeService"
                ],
                "methods": [
                    "ResourceManager.serviceInit",
                    "CompositeService.serviceInit"
                ]
            },
            "possible_fix": "To fix the issue, modify the CompositeService.serviceInit method to use a synchronized block or a thread-safe collection when iterating over the services list. For example:\n\nprotected void serviceInit(Configuration conf) throws Exception {\n    List<Service> services = getServices();\n    synchronized (services) {\n        for (Service service : services) {\n            service.init(conf);\n        }\n    }\n    super.serviceInit(conf);\n}"
        },
        "possible_fix_code": {
            "CompositeService.serviceInit": "  protected void serviceInit(Configuration conf) throws Exception {\n    List<Service> services = getServices();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(getName() + \": initing services, size=\" + services.size());\n    }\n    synchronized (services) {\n      for (Service service : services) {\n        service.init(conf);\n      }\n    }\n    super.serviceInit(conf);\n  }"
        }
    },
    {
        "filename": "YARN-174.json",
        "creation_time": "2012-10-19T17:25:40.000+0000",
        "bug_report": {
            "Title": "TestNodeStatusUpdater is failing in trunk",
            "Description": "The NodeManager fails to start due to an invalid path configuration for user logs. The error message indicates that the path '${yarn.log.dir}/userlogs' is not valid, as it must either have a file scheme or be without a scheme. This issue arises during the initialization of the NodeManager, specifically in the LocalDirsHandlerService's validatePaths method, which checks the validity of the paths provided in the configuration.",
            "StackTrace": [
                "2012-10-19 12:18:23,941 FATAL [Node Status Updater] nodemanager.NodeManager (NodeManager.java:initAndStartNodeManager(277)) - Error starting NodeManager",
                "org.apache.hadoop.yarn.YarnException: ${yarn.log.dir}/userlogs is not a valid path. Path should be with file scheme or without scheme",
                "at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.validatePaths(LocalDirsHandlerService.java:321)",
                "at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService$MonitoringTimerTask.<init>(LocalDirsHandlerService.java:95)",
                "at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.init(LocalDirsHandlerService.java:123)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeHealthCheckerService.init(NodeHealthCheckerService.java:48)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.init(NodeManager.java:165)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:274)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stateChanged(NodeManager.java:256)",
                "at org.apache.hadoop.yarn.service.AbstractService.changeState(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:112)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.stop(NodeStatusUpdaterImpl.java:149)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.reboot(NodeStatusUpdaterImpl.java:157)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.access$900(NodeStatusUpdaterImpl.java:63)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$1.run(NodeStatusUpdaterImpl.java:357)"
            ],
            "RootCause": "The root cause of the issue is the invalid path '${yarn.log.dir}/userlogs' being used in the configuration. The validatePaths method in LocalDirsHandlerService is throwing an exception because the path does not conform to the expected URI format.",
            "StepsToReproduce": [
                "Set the yarn.log.dir configuration to an invalid path such as '${yarn.log.dir}/userlogs'.",
                "Attempt to start the NodeManager.",
                "Observe the failure in the logs indicating the invalid path."
            ],
            "ExpectedBehavior": "The NodeManager should start successfully without any errors related to path validation.",
            "ObservedBehavior": "The NodeManager fails to start, and a fatal error is logged indicating that the path '${yarn.log.dir}/userlogs' is invalid.",
            "Suggestions": "Ensure that the yarn.log.dir configuration is set to a valid path that either has a file scheme (e.g., 'file:///path/to/logs') or is a relative path without a scheme.",
            "problem_location": {
                "files": [
                    "NodeManager.java",
                    "LocalDirsHandlerService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                    "org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService"
                ],
                "methods": [
                    "NodeManager.initAndStartNodeManager",
                    "LocalDirsHandlerService.validatePaths"
                ]
            },
            "possible_fix": "Update the configuration for yarn.log.dir to a valid path. For example, set it to 'file:///var/log/hadoop-yarn/userlogs' or a valid relative path without a scheme."
        },
        "possible_fix_code": {
            "LocalDirsHandlerService.validatePaths": "  public static String[] validatePaths(String[] paths) {\n    ArrayList<String> validPaths = new ArrayList<String>();\n    for (int i = 0; i < paths.length; ++i) {\n      try {\n        URI uriPath = new URI(paths[i]);\n        if (uriPath.getScheme() == null\n            || uriPath.getScheme().equals(FILE_SCHEME)) {\n          validPaths.add(uriPath.getPath());\n        } else {\n          LOG.warn(paths[i] + \" is not a valid path. Path should be with \"\n              + FILE_SCHEME + \" scheme or without scheme\");\n          throw new YarnException(paths[i]\n              + \" is not a valid path. Path should be with \" + FILE_SCHEME\n              + \" scheme or without scheme\");\n        }\n      } catch (URISyntaxException e) {\n        LOG.warn(e.getMessage());\n        throw new YarnException(paths[i]\n            + \" is not a valid path. Path should be with \" + FILE_SCHEME\n            + \" scheme or without scheme\");\n      }\n    }\n    String[] arrValidPaths = new String[validPaths.size()];\n    validPaths.toArray(arrValidPaths);\n    return arrValidPaths;\n  }"
        }
    },
    {
        "filename": "YARN-6448.json",
        "creation_time": "2017-04-05T18:39:49.000+0000",
        "bug_report": {
            "Title": "Continuous scheduling thread crashes while sorting nodes",
            "Description": "The continuous scheduling thread in the YARN FairScheduler crashes due to an IllegalArgumentException caused by a violation of the comparison contract in the sorting method. This occurs when the nodes change during the sorting process, leading to inconsistent comparisons. The issue arises in the sortedNodeList method of the ClusterNodeTracker class, where a read lock is used to retrieve the list of nodes, but the sorting operation does not account for potential changes in the node list during execution.",
            "StackTrace": [
                "2017-04-04 23:42:26,123 FATAL org.apache.hadoop.yarn.server.resourcemanager.RMCriticalThreadUncaughtExceptionHandler: Critical thread FairSchedulerContinuousScheduling crashed!",
                "java.lang.IllegalArgumentException: Comparison method violates its general contract!",
                "at java.util.TimSort.mergeHi(TimSort.java:899)",
                "at java.util.TimSort.mergeAt(TimSort.java:516)",
                "at java.util.TimSort.mergeForceCollapse(TimSort.java:457)",
                "at java.util.TimSort.sort(TimSort.java:254)",
                "at java.util.Arrays.sort(Arrays.java:1512)",
                "at java.util.ArrayList.sort(ArrayList.java:1454)",
                "at java.util.Collections.sort(Collections.java:175)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.ClusterNodeTracker.sortedNodeList(ClusterNodeTracker.java:306)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt(FairScheduler.java:884)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread.run(FairScheduler.java:316)"
            ],
            "RootCause": "The root cause of the crash is the violation of the comparison contract in the comparator used for sorting nodes. This occurs when the node list is modified while it is being sorted, leading to inconsistent comparison results.",
            "StepsToReproduce": [
                "Start the YARN FairScheduler with a set of nodes.",
                "Modify the state of the nodes (e.g., add or remove nodes) while the continuous scheduling thread is running.",
                "Observe the logs for the IllegalArgumentException indicating a comparison method violation."
            ],
            "ExpectedBehavior": "The continuous scheduling thread should sort the nodes without crashing, maintaining a consistent order regardless of changes to the node list.",
            "ObservedBehavior": "The continuous scheduling thread crashes with an IllegalArgumentException due to a violation of the comparison contract during sorting.",
            "Suggestions": "Implement a mechanism to ensure that the node list remains stable during sorting, such as creating a snapshot of the node list before sorting or using a concurrent data structure that allows safe iteration.",
            "problem_location": {
                "files": [
                    "ClusterNodeTracker.java",
                    "FairScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.ClusterNodeTracker",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
                ],
                "methods": [
                    "ClusterNodeTracker.sortedNodeList",
                    "FairScheduler.continuousSchedulingAttempt"
                ]
            },
            "possible_fix": "To fix the issue, modify the sortedNodeList method to create a copy of the nodes list before sorting. This can be done by changing the line 'sortedList = new ArrayList(nodes.values());' to 'sortedList = new ArrayList<>(nodes.values());' and ensuring that the comparator used is stable and does not rely on mutable state."
        },
        "possible_fix_code": {
            "sortedNodeList": "  public List<N> sortedNodeList(Comparator<N> comparator) {\n    List<N> sortedList = null;\n    readLock.lock();\n    try {\n      sortedList = new ArrayList<>(nodes.values());\n    } finally {\n      readLock.unlock();\n    }\n    Collections.sort(sortedList, comparator);\n    return sortedList;\n  }"
        }
    },
    {
        "filename": "YARN-4530.json",
        "creation_time": "2015-12-30T15:19:19.000+0000",
        "bug_report": {
            "Title": "LocalizedResource triggers NPE causing NodeManager exit",
            "Description": "In the cluster, the LocalizedResource download fails, leading to a NullPointerException (NPE) that causes the NodeManager to shut down. The issue arises when the resource's modification time changes unexpectedly during the download process, resulting in an IOException. This IOException is not handled properly, leading to an NPE in the ResourceLocalizationService.",
            "StackTrace": [
                "java.lang.NullPointerException at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$PublicLocalizer.run(ResourceLocalizationService.java:712)",
                "java.io.IOException: Resource hdfs://ns3/user/username/projects/user_insight/lookalike/oozie/workflow/lib/unilever_support_udf-0.0.1-SNAPSHOT.jar changed on src filesystem (expected 1451380519452, was 1451380611793)",
                "at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:176)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:276)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the resource's modification time changes during the download process, which leads to an IOException being thrown. This IOException is not handled correctly, resulting in a NullPointerException when the ResourceLocalizationService attempts to access a null resource.",
            "StepsToReproduce": [
                "1. Start the NodeManager in a cluster environment.",
                "2. Attempt to download a resource that is being modified on the source filesystem during the download.",
                "3. Observe the NodeManager logs for the NPE and shutdown message."
            ],
            "ExpectedBehavior": "The NodeManager should handle the IOException gracefully without crashing, allowing for retries or appropriate error handling.",
            "ObservedBehavior": "The NodeManager crashes with a NullPointerException after failing to download a resource due to a modification time mismatch.",
            "Suggestions": "Implement proper error handling in the ResourceLocalizationService to manage IOException scenarios. Consider adding retry logic or fallback mechanisms when a resource modification time mismatch occurs.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/FSDownload.java",
                    "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.util.FSDownload",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"
                ],
                "methods": [
                    "FSDownload.call",
                    "FSDownload.copy",
                    "ResourceLocalizationService$PublicLocalizer.run"
                ]
            },
            "possible_fix": "In the FSDownload.copy method, add a try-catch block to handle the IOException and prevent it from propagating to the ResourceLocalizationService. For example:\n\n```java\ntry {\n    // existing code\n} catch (IOException e) {\n    // log the error and handle it appropriately\n    throw new CustomResourceDownloadException(\"Failed to download resource\", e);\n}\n```"
        },
        "possible_fix_code": {
            "FSDownload.copy": "  private Path copy(Path sCopy, Path dstdir) throws IOException {\n    FileSystem sourceFs = sCopy.getFileSystem(conf);\n    Path dCopy = new Path(dstdir, \"tmp_\" + sCopy.getName());\n    FileStatus sStat = sourceFs.getFileStatus(sCopy);\n    try {\n      if (sStat.getModificationTime() != resource.getTimestamp()) {\n        throw new IOException(\"Resource \" + sCopy +\n            \" changed on src filesystem (expected \" + resource.getTimestamp() +\n            \", was \" + sStat.getModificationTime());\n      }\n      if (resource.getVisibility() == LocalResourceVisibility.PUBLIC) {\n        if (!isPublic(sourceFs, sCopy, sStat, statCache)) {\n          throw new IOException(\"Resource \" + sCopy +\n              \" is not publicly accessible and as such cannot be part of the\" +\n              \" public cache.\");\n        }\n      }\n\n      FileUtil.copy(sourceFs, sStat, FileSystem.getLocal(conf), dCopy, false,\n          true, conf);\n    } catch (IOException e) {\n      // Log the error and handle it appropriately\n      throw new IOException(\"Failed to download resource: \" + e.getMessage(), e);\n    }\n    return dCopy;\n  }"
        }
    },
    {
        "filename": "YARN-7737.json",
        "creation_time": "2018-01-11T19:35:01.000+0000",
        "bug_report": {
            "Title": "prelaunch.err file not found exception on container failure",
            "Description": "The system encounters a FileNotFoundException when attempting to access the prelaunch error log file for a container that has failed. This occurs because the prelaunch error log file is expected to be located in the container's log directory, which is determined during the container launch process. However, the file is not created or is not found at the expected path, leading to the exception.",
            "StackTrace": [
                "java.io.FileNotFoundException: File /grid/b/tmp/userlogs/application_1515190594800_1766/container_e39_1515190594800_1766_01_000002/prelaunch.err does not exist",
                "at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:641)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:930)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:631)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.handleContainerExitWithFailure(ContainerLaunch.java:545)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.handleContainerExitCode(ContainerLaunch.java:511)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:319)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:93)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The prelaunch error log file is not created or is not found at the expected path due to a failure in the container launch process. The log directory is determined during the launch, but the file may not be written if the container fails before it can be created.",
            "StepsToReproduce": [
                "1. Launch a container that is expected to fail.",
                "2. Monitor the logs for the container launch process.",
                "3. Check the specified log directory for the presence of the prelaunch.err file."
            ],
            "ExpectedBehavior": "The prelaunch error log file should be created in the specified log directory when a container fails, allowing for proper diagnostics.",
            "ObservedBehavior": "The system throws a FileNotFoundException indicating that the prelaunch.err file does not exist when attempting to access it after a container failure.",
            "Suggestions": "Ensure that the prelaunch error log file is created even if the container fails early in the launch process. This may involve modifying the container launch logic to handle failures more gracefully and ensure that log files are written as expected.",
            "problem_location": {
                "files": [
                    "ContainerLaunch.java",
                    "RawLocalFileSystem.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch",
                    "org.apache.hadoop.fs.RawLocalFileSystem"
                ],
                "methods": [
                    "ContainerLaunch.call",
                    "ContainerLaunch.handleContainerExitWithFailure",
                    "RawLocalFileSystem.getFileStatus"
                ]
            },
            "possible_fix": "Modify the ContainerLaunch.call method to ensure that the prelaunch error log file is created in the log directory even if the container fails early. This may involve adding checks to write the log file before the container execution begins."
        },
        "possible_fix_code": {
            "ContainerLaunch.call": "  public Integer call() {\n    if (!validateContainerState()) {\n      return 0;\n    }\n\n    final ContainerLaunchContext launchContext = container.getLaunchContext();\n    ContainerId containerID = container.getContainerId();\n    String containerIdStr = containerID.toString();\n    final List<String> command = launchContext.getCommands();\n    int ret = -1;\n\n    Path containerLogDir;\n    try {\n      Map<Path, List<String>> localResources = getLocalizedResources();\n\n      final String user = container.getUser();\n      // /////////////////////////// Variable expansion\n      // Before the container script gets written out.\n      List<String> newCmds = new ArrayList<String>(command.size());\n      String appIdStr = app.getAppId().toString();\n      String relativeContainerLogDir = ContainerLaunch\n          .getRelativeContainerLogDir(appIdStr, containerIdStr);\n      containerLogDir =\n          dirsHandler.getLogPathForWrite(relativeContainerLogDir, false);\n      recordContainerLogDir(containerID, containerLogDir.toString());\n\n      // Create prelaunch error log file\n      createPrelaunchErrorLogFile(containerLogDir);\n\n      for (String str : command) {\n        // TODO: Should we instead work via symlinks without this grammar?\n        newCmds.add(expandEnvironment(str, containerLogDir));\n      }\n      launchContext.setCommands(newCmds);\n\n      Map<String, String> environment = launchContext.getEnvironment();\n      // Make a copy of env to iterate & do variable expansion\n      for (Entry<String, String> entry : environment.entrySet()) {\n        String value = entry.getValue();\n        value = expandEnvironment(value, containerLogDir);\n        entry.setValue(value);\n      }\n      // /////////////////////////// End of variable expansion\n\n      FileContext lfs = FileContext.getLocalFSFileContext();\n\n      Path nmPrivateContainerScriptPath = dirsHandler.getLocalPathForWrite(\n          getContainerPrivateDir(appIdStr, containerIdStr) + Path.SEPARATOR\n              + CONTAINER_SCRIPT);\n      Path nmPrivateTokensPath = dirsHandler.getLocalPathForWrite(\n          getContainerPrivateDir(appIdStr, containerIdStr) + Path.SEPARATOR\n              + String.format(ContainerLocalizer.TOKEN_FILE_NAME_FMT,\n              containerIdStr));\n      Path nmPrivateClasspathJarDir = dirsHandler.getLocalPathForWrite(\n          getContainerPrivateDir(appIdStr, containerIdStr));\n\n      // Select the working directory for the container\n      Path containerWorkDir = deriveContainerWorkDir();\n      recordContainerWorkDir(containerID, containerWorkDir.toString());\n\n      String pidFileSubpath = getPidFileSubpath(appIdStr, containerIdStr);\n      // pid file should be in nm private dir so that it is not\n      // accessible by users\n      pidFilePath = dirsHandler.getLocalPathForWrite(pidFileSubpath);\n      List<String> localDirs = dirsHandler.getLocalDirs();\n      List<String> logDirs = dirsHandler.getLogDirs();\n      List<String> filecacheDirs = getNMFilecacheDirs(localDirs);\n      List<String> userLocalDirs = getUserLocalDirs(localDirs);\n      List<String> containerLocalDirs = getContainerLocalDirs(localDirs);\n      List<String> containerLogDirs = getContainerLogDirs(logDirs);\n\n      if (!dirsHandler.areDisksHealthy()) {\n        ret = ContainerExitStatus.DISKS_FAILED;\n        throw new IOException(\"Most of the disks failed. \"\n            + dirsHandler.getDisksHealthReport(false));\n      }\n      List<Path> appDirs = new ArrayList<Path>(localDirs.size());\n      for (String localDir : localDirs) {\n        Path usersdir = new Path(localDir, ContainerLocalizer.USERCACHE);\n        Path userdir = new Path(usersdir, user);\n        Path appsdir = new Path(userdir, ContainerLocalizer.APPCACHE);\n        appDirs.add(new Path(appsdir, appIdStr));\n      }\n\n      // Set the token location too.\n      environment.put(\n          ApplicationConstants.CONTAINER_TOKEN_FILE_ENV_NAME,\n          new Path(containerWorkDir,\n              FINAL_CONTAINER_TOKENS_FILE).toUri().getPath());\n\n      // /////////// Write out the container-script in the nmPrivate space.\n      try (DataOutputStream containerScriptOutStream =\n               lfs.create(nmPrivateContainerScriptPath,\n                   EnumSet.of(CREATE, OVERWRITE))) {\n        // Sanitize the container's environment\n        sanitizeEnv(environment, containerWorkDir, appDirs, userLocalDirs,\n            containerLogDirs, localResources, nmPrivateClasspathJarDir);\n\n        prepareContainer(localResources, containerLocalDirs);\n\n        // Write out the environment\n        exec.writeLaunchEnv(containerScriptOutStream, environment,\n            localResources, launchContext.getCommands(),\n            new Path(containerLogDirs.get(0)), user);\n      }\n      // /////////// End of writing out container-script\n\n      // /////////// Write out the container-tokens in the nmPrivate space.\n      try (DataOutputStream tokensOutStream =\n               lfs.create(nmPrivateTokensPath, EnumSet.of(CREATE, OVERWRITE))) {\n        Credentials creds = container.getCredentials();\n        creds.writeTokenStorageToStream(tokensOutStream);\n      }\n      // /////////// End of writing out container-tokens\n\n      ret = launchContainer(new ContainerStartContext.Builder()\n          .setContainer(container)\n          .setLocalizedResources(localResources)\n          .setNmPrivateContainerScriptPath(nmPrivateContainerScriptPath)\n          .setNmPrivateTokensPath(nmPrivateTokensPath)\n          .setUser(user)\n          .setAppId(appIdStr)\n          .setContainerWorkDir(containerWorkDir)\n          .setLocalDirs(localDirs)\n          .setLogDirs(logDirs)\n          .setFilecacheDirs(filecacheDirs)\n          .setUserLocalDirs(userLocalDirs)\n          .setContainerLocalDirs(containerLocalDirs)\n          .setContainerLogDirs(containerLogDirs).build());\n    } catch (ConfigurationException e) {\n      LOG.error(\"Failed to launch container due to configuration error.\", e);\n      dispatcher.getEventHandler().handle(new ContainerExitEvent(\n          containerID, ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, ret,\n          e.getMessage()));\n      // Mark the node as unhealthy\n      context.getNodeStatusUpdater().reportException(e);\n      return ret;\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to launch container.\", e);\n      dispatcher.getEventHandler().handle(new ContainerExitEvent(\n          containerID, ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, ret,\n          e.getMessage()));\n      return ret;\n    } finally {\n      setContainerCompletedStatus(ret);\n    }\n\n    handleContainerExitCode(ret, containerLogDir);\n    return ret;\n  }\n\n  private void createPrelaunchErrorLogFile(Path containerLogDir) throws IOException {\n    Path prelaunchErrorLogFile = new Path(containerLogDir, ContainerLaunch.CONTAINER_PRE_LAUNCH_STDERR);\n    FileContext lfs = FileContext.getLocalFSFileContext();\n    try (DataOutputStream out = lfs.create(prelaunchErrorLogFile, EnumSet.of(CREATE, OVERWRITE))) {\n      // Optionally write an initial message or leave it empty\n      out.writeBytes(\"Prelaunch error log created.\\n\");\n    }\n  }"
        }
    },
    {
        "filename": "YARN-5136.json",
        "creation_time": "2016-05-24T15:34:28.000+0000",
        "bug_report": {
            "Title": "Error in handling event type APP_ATTEMPT_REMOVED to the scheduler",
            "Description": "The system encounters an IllegalStateException when attempting to remove an application attempt from the scheduler. This occurs because the application attempt being removed does not exist in the specified queue, leading to a failure in the removeApp method of the FSLeafQueue class.",
            "StackTrace": [
                "2016-05-24 23:20:47,202 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ATTEMPT_REMOVED to the scheduler",
                "java.lang.IllegalStateException: Given app to remove org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt@ea94c3b does not exist in queue [root.bdp_xx.bdp_mart_xx_formal, demand=<memory:28672000, vCores:14000>, running=<memory:28647424, vCores:13422>, share=<memory:28672000, vCores:0>, w=<memory weight=1.0, cpu weight=1.0>]",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.removeApp(FSLeafQueue.java:119)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplicationAttempt(FairScheduler.java:779)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1231)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:114)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:680)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The application attempt being removed does not exist in the queue, which leads to an IllegalStateException being thrown in the removeApp method.",
            "StepsToReproduce": [
                "1. Submit an application to the scheduler.",
                "2. Trigger an event to remove the application attempt.",
                "3. Observe the logs for the IllegalStateException."
            ],
            "ExpectedBehavior": "The application attempt should be successfully removed from the scheduler without throwing an exception.",
            "ObservedBehavior": "An IllegalStateException is thrown indicating that the application attempt does not exist in the queue.",
            "Suggestions": "Ensure that the application attempt exists in the queue before attempting to remove it. Implement checks to handle cases where the application attempt may have already been removed or never existed.",
            "problem_location": {
                "files": [
                    "FSLeafQueue.java",
                    "FairScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
                ],
                "methods": [
                    "FSLeafQueue.removeApp",
                    "FairScheduler.removeApplicationAttempt",
                    "FairScheduler.handle"
                ]
            },
            "possible_fix": "In the removeApplicationAttempt method, before calling queue.removeApp(attempt), check if the attempt exists in the queue. If it does not exist, log a warning instead of proceeding to remove it."
        },
        "possible_fix_code": {
            "FSLeafQueue.removeApp": "  public boolean removeApp(FSAppAttempt app) {\n    boolean runnable = false;\n\n    // Remove app from runnable/nonRunnable list while holding the write lock\n    writeLock.lock();\n    try {\n      runnable = runnableApps.remove(app);\n      if (!runnable) {\n        // Check if the app exists before attempting to remove it\n        if (!removeNonRunnableApp(app)) {\n          // Log a warning instead of throwing an exception\n          LOG.warn(\"Attempt to remove non-existent app: \" + app);\n          return false;\n        }\n      }\n    } finally {\n      writeLock.unlock();\n    }\n\n    // Update AM resource usage if needed. If isAMRunning is true, we\\'re not\n    // running an unmanaged AM.\n    if (runnable && app.isAmRunning()) {\n      Resources.subtractFrom(amResourceUsage, app.getAMResource());\n    }\n\n    return runnable;\n  }"
        }
    },
    {
        "filename": "YARN-8211.json",
        "creation_time": "2018-04-26T02:13:22.000+0000",
        "bug_report": {
            "Title": "Yarn registry dns log finds BufferUnderflowException on port ping",
            "Description": "The Yarn registry DNS server is encountering a BufferUnderflowException during its operation. This issue arises when the server attempts to read from a ByteBuffer that does not contain enough data, leading to an exception when trying to access the next index of the buffer. The exception is logged during the execution of tasks in the RegistryDNS thread, indicating that the server is not handling incoming data correctly.",
            "StackTrace": [
                "2018-04-25 01:36:56,139 WARN concurrent.ExecutorHelper (ExecutorHelper.java:logThrowableFromAfterExecute(50)) - Execution exception when running task in RegistryDNS 76",
                "2018-04-25 01:36:56,139 WARN concurrent.ExecutorHelper (ExecutorHelper.java:logThrowableFromAfterExecute(63)) - Caught exception in thread RegistryDNS 76:",
                "java.nio.BufferUnderflowException",
                "at java.nio.Buffer.nextGetIndex(Buffer.java:500)",
                "at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:135)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS.getMessgeLength(RegistryDNS.java:820)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS.nioTCPClient(RegistryDNS.java:767)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:846)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:843)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The root cause of the BufferUnderflowException is that the method getMessgeLength is trying to read two bytes from the ByteBuffer without ensuring that there are at least two bytes available. If the buffer has fewer than two bytes, it will throw a BufferUnderflowException.",
            "StepsToReproduce": [
                "Start the Yarn registry DNS server.",
                "Send a TCP query that results in a response that does not contain enough data to satisfy the expected message length.",
                "Observe the logs for the BufferUnderflowException."
            ],
            "ExpectedBehavior": "The Yarn registry DNS server should handle incoming data correctly and not throw a BufferUnderflowException when reading from the ByteBuffer.",
            "ObservedBehavior": "The Yarn registry DNS server throws a BufferUnderflowException when attempting to read from the ByteBuffer, causing disruptions in service.",
            "Suggestions": "Implement a check to ensure that the ByteBuffer has enough data before attempting to read from it. This can prevent the BufferUnderflowException from occurring.",
            "problem_location": {
                "files": [
                    "RegistryDNS.java"
                ],
                "classes": [
                    "org.apache.hadoop.registry.server.dns.RegistryDNS"
                ],
                "methods": [
                    "RegistryDNS.getMessgeLength",
                    "RegistryDNS.nioTCPClient"
                ]
            },
            "possible_fix": "Modify the getMessgeLength method to check the remaining bytes in the ByteBuffer before attempting to read. For example:\n\nprivate int getMessgeLength(ByteBuffer buf) throws EOFException {\n    if (buf.remaining() < 2) {\n        throw new BufferUnderflowException();\n    }\n    int ch1 = buf.get();\n    int ch2 = buf.get();\n    if ((ch1 | ch2) < 0) {\n        throw new EOFException();\n    }\n    return (ch1 << 8) + (ch2 & 0xff);\n}"
        },
        "possible_fix_code": {
            "getMessgeLength": "  private int getMessgeLength(ByteBuffer buf) throws EOFException {\n    if (buf.remaining() < 2) {\n        throw new BufferUnderflowException();\n    }\n    int ch1 = buf.get();\n    int ch2 = buf.get();\n    if ((ch1 | ch2) < 0) {\n      throw new EOFException();\n    }\n    return (ch1 << 8) + (ch2 & 0xff);\n  }"
        }
    },
    {
        "filename": "YARN-2124.json",
        "creation_time": "2014-06-05T07:44:27.000+0000",
        "bug_report": {
            "Title": "ProportionalCapacityPreemptionPolicy Initialization Issue",
            "Description": "The ProportionalCapacityPreemptionPolicy is encountering a NullPointerException (NPE) during its initialization phase because it attempts to use a ResourceCalculator that has not yet been initialized. This occurs when the ResourceManager starts, and the preemption policy is invoked before the CapacityScheduler is fully set up, leading to a null reference in the computeIdealResourceDistribution method.",
            "StackTrace": [
                "2014-06-05 11:01:33,201 ERROR org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[SchedulingMonitor (ProportionalCapacityPreemptionPolicy),5,main] threw an Exception.",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.util.resource.Resources.greaterThan(Resources.java:225)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.computeIdealResourceDistribution(ProportionalCapacityPreemptionPolicy.java:302)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.recursivelyComputeIdealAssignment(ProportionalCapacityPreemptionPolicy.java:261)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.containerBasedPreemptOrKill(ProportionalCapacityPreemptionPolicy.java:198)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.editSchedule(ProportionalCapacityPreemptionPolicy.java:174)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor.invokePolicy(SchedulingMonitor.java:72)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor$PreemptionChecker.run(SchedulingMonitor.java:82)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The ProportionalCapacityPreemptionPolicy is initialized before the CapacityScheduler, resulting in a null ResourceCalculator reference when computeIdealResourceDistribution is called.",
            "StepsToReproduce": [
                "Start the ResourceManager with preemption enabled.",
                "Invoke the ProportionalCapacityPreemptionPolicy.",
                "Observe the logs for NullPointerException."
            ],
            "ExpectedBehavior": "The ProportionalCapacityPreemptionPolicy should initialize correctly and compute the ideal resource distribution without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown when the ProportionalCapacityPreemptionPolicy attempts to access the ResourceCalculator, which is not yet initialized.",
            "Suggestions": "Ensure that the ProportionalCapacityPreemptionPolicy is initialized only after the CapacityScheduler is fully set up. This may involve adjusting the initialization order or adding checks to ensure that the ResourceCalculator is not null before it is used.",
            "problem_location": {
                "files": [
                    "ProportionalCapacityPreemptionPolicy.java",
                    "Resources.java",
                    "SchedulingMonitor.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy",
                    "org.apache.hadoop.yarn.util.resource.Resources",
                    "org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor"
                ],
                "methods": [
                    "ProportionalCapacityPreemptionPolicy.computeIdealResourceDistribution",
                    "ProportionalCapacityPreemptionPolicy.recursivelyComputeIdealAssignment",
                    "ProportionalCapacityPreemptionPolicy.containerBasedPreemptOrKill",
                    "ProportionalCapacityPreemptionPolicy.editSchedule",
                    "Resources.greaterThan"
                ]
            },
            "possible_fix": "Modify the initialization sequence of ProportionalCapacityPreemptionPolicy to ensure it occurs after the CapacityScheduler is initialized. Additionally, add a null check for the ResourceCalculator in computeIdealResourceDistribution to prevent NPEs."
        },
        "possible_fix_code": {
            "ProportionalCapacityPreemptionPolicy.computeIdealResourceDistribution": "  private void computeIdealResourceDistribution(ResourceCalculator rc,\n      List<TempQueue> queues, Resource totalPreemptionAllowed, Resource tot_guarant) {\n\n    // Check if ResourceCalculator is null to prevent NullPointerException\n    if (rc == null) {\n        throw new IllegalArgumentException(\"ResourceCalculator cannot be null\");\n    }\n\n    // qAlloc tracks currently active queues (will decrease progressively as\n    // demand is met)\n    List<TempQueue> qAlloc = new ArrayList<TempQueue>(queues);\n    // unassigned tracks how much resources are still to assign, initialized\n    // with the total capacity for this set of queues\n    Resource unassigned = Resources.clone(tot_guarant);\n\n    // group queues based on whether they have non-zero guaranteed capacity\n    Set<TempQueue> nonZeroGuarQueues = new HashSet<TempQueue>();\n    Set<TempQueue> zeroGuarQueues = new HashSet<TempQueue>();\n\n    for (TempQueue q : qAlloc) {\n      if (Resources\n          .greaterThan(rc, tot_guarant, q.guaranteed, Resources.none())) {\n        nonZeroGuarQueues.add(q);\n      } else {\n        zeroGuarQueues.add(q);\n      }\n    }\n\n    // first compute the allocation as a fixpoint based on guaranteed capacity\n    computeFixpointAllocation(rc, tot_guarant, nonZeroGuarQueues, unassigned,\n        false);\n\n    // if any capacity is left unassigned, distributed among zero-guarantee \n    // queues uniformly (i.e., not based on guaranteed capacity, as this is zero)\n    if (!zeroGuarQueues.isEmpty()\n        && Resources.greaterThan(rc, tot_guarant, unassigned, Resources.none())) {\n      computeFixpointAllocation(rc, tot_guarant, zeroGuarQueues, unassigned,\n          true);\n    }\n    \n    // based on ideal assignment computed above and current assignment we derive\n    // how much preemption is required overall\n    Resource totPreemptionNeeded = Resource.newInstance(0, 0);\n    for (TempQueue t:queues) {\n      if (Resources.greaterThan(rc, tot_guarant, t.current, t.idealAssigned)) {\n        Resources.addTo(totPreemptionNeeded,\n            Resources.subtract(t.current, t.idealAssigned));\n      }\n    }\n\n    // if we need to preempt more than is allowed, compute a factor (0<f<1)\n    // that is used to scale down how much we ask back from each queue\n    float scalingFactor = 1.0F;\n    if (Resources.greaterThan(rc, tot_guarant,\n          totPreemptionNeeded, totalPreemptionAllowed)) {\n       scalingFactor = Resources.divide(rc, tot_guarant,\n           totalPreemptionAllowed, totPreemptionNeeded);\n    }\n\n    // assign to each queue the amount of actual preemption based on local\n    // information of ideal preemption and scaling factor\n    for (TempQueue t : queues) {\n      t.assignPreemption(scalingFactor, rc, tot_guarant);\n    }\n    if (LOG.isDebugEnabled()) {\n      long time = clock.getTime();\n      for (TempQueue t : queues) {\n        LOG.debug(time + \": \" + t);\n      }\n    }\n\n  }"
        }
    },
    {
        "filename": "YARN-8591.json",
        "creation_time": "2018-07-27T05:56:26.000+0000",
        "bug_report": {
            "Title": "[ATSv2] NPE while checking for entity acl in non-secure cluster",
            "Description": "A NullPointerException (NPE) occurs when the system attempts to check access for entities in a non-secure cluster environment. This issue arises specifically in the `checkAccessForGenericEntities` method when it tries to access the user ID from the entity's info, which may not be present, leading to the NPE. The problem is exacerbated by the fact that the method does not adequately handle cases where the entity info is null.",
            "StackTrace": [
                "javax.ws.rs.WebApplicationException: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.handleException(TimelineReaderWebServices.java:196)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:624)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:474)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.checkAccess(TimelineReaderWebServices.java:3536)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.checkAccessForGenericEntities(TimelineReaderWebServices.java:3513)"
            ],
            "RootCause": "The root cause of the issue is that the `checkAccessForGenericEntities` method does not check if the `entities` set is empty or if the `entity.getInfo()` returns null before attempting to access the user ID, leading to a NullPointerException.",
            "StepsToReproduce": [
                "1. Set up a non-secure cluster environment.",
                "2. Send a GET request to the timeline service endpoint for entities.",
                "3. Ensure that the request includes parameters that would lead to an empty or null entity info."
            ],
            "ExpectedBehavior": "The system should handle the request gracefully without throwing a NullPointerException, returning an appropriate response indicating that no entities were found or that access could not be determined.",
            "ObservedBehavior": "The system throws a NullPointerException, resulting in an internal server error when attempting to check access for entities.",
            "Suggestions": "Implement null checks in the `checkAccessForGenericEntities` method to ensure that the entity info is not null before accessing it. Additionally, handle cases where the entities set is empty to avoid unnecessary processing.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineReaderWebServices.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices"
                ],
                "methods": [
                    "checkAccessForGenericEntities",
                    "checkAccess"
                ]
            },
            "possible_fix": "In the `checkAccessForGenericEntities` method, add a null check for `entity.getInfo()` before accessing the user ID. For example:\n\n```java\nif (entity.getInfo() != null) {\n    String uid = (String) entity.getInfo().get(TimelineReaderUtils.FROMID_KEY);\n    // Proceed with access check\n} else {\n    // Handle the case where entity info is null\n}\n```"
        },
        "possible_fix_code": {
            "checkAccessForGenericEntities": "  private void checkAccessForGenericEntities(Set<TimelineEntity> entities,\n      UserGroupInformation callerUGI, String entityType) throws Exception {\n    if (entities != null && entities.size() > 0\n        && isDisplayEntityPerUserFilterEnabled(\n        getTimelineReaderManager().getConfig())) {\n      TimelineReaderContext timelineReaderContext = null;\n      for (TimelineEntity entity : entities) {\n        if (entity.getInfo() != null) {\n          String uid = (String) entity.getInfo().get(TimelineReaderUtils.FROMID_KEY);\n          if (TimelineEntityType.YARN_APPLICATION.matches(entityType)) {\n            timelineReaderContext =\n                TimelineFromIdConverter.APPLICATION_FROMID.decodeUID(uid);\n          } else {\n            timelineReaderContext =\n                TimelineFromIdConverter.GENERIC_ENTITY_FROMID.decodeUID(uid);\n          }\n          checkAccess(getTimelineReaderManager(), callerUGI,\n              timelineReaderContext.getUserId());\n        } else {\n          // Handle the case where entity info is null\n          LOG.warn(\"Entity info is null for entity: \" + entity);\n        }\n      }\n    }\n  }"
        }
    },
    {
        "filename": "YARN-6649.json",
        "creation_time": "2017-05-25T20:36:08.000+0000",
        "bug_report": {
            "Title": "RollingLevelDBTimelineServer throws RuntimeException if object decoding fails",
            "Description": "When using the Tez UI, REST API calls to the timeline service occasionally return a 500 Internal Server Error due to a failure in object decoding. This issue is linked to YARN-6654, which aims to improve error handling during object decoding. The current implementation does not gracefully handle decoding failures, resulting in a RuntimeException being thrown instead of providing a partial response to the client.",
            "StackTrace": [
                "WARN org.apache.hadoop.yarn.webapp.GenericExceptionHandler: INTERNAL_SERVER_ERROR",
                "javax.ws.rs.WebApplicationException: java.lang.RuntimeException: java.io.IOException: java.lang.RuntimeException: unable to encodeValue class from code 1000",
                "at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getEntity(TimelineWebServices.java:164)",
                "at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.getEntity(RollingLevelDBTimelineStore.java:478)",
                "at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.doGetEntity(TimelineDataManager.java:215)",
                "Caused by: java.lang.RuntimeException: unable to encodeValue class from code 1000",
                "at org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:240)"
            ],
            "RootCause": "The root cause of the issue is a failure in the object decoding process within the FST serialization framework, specifically when attempting to decode a class that is not recognized or cannot be encoded, leading to a RuntimeException.",
            "StepsToReproduce": [
                "1. Use the Tez UI to make a REST API call to the timeline service.",
                "2. Trigger a scenario where the object decoding fails (e.g., by using an invalid entity ID or type).",
                "3. Observe the response from the server, which should return a 500 Internal Server Error."
            ],
            "ExpectedBehavior": "The system should handle decoding failures gracefully and return a partial message or a meaningful error response instead of a 500 Internal Server Error.",
            "ObservedBehavior": "The system throws a RuntimeException, resulting in a 500 Internal Server Error response to the client.",
            "Suggestions": "Implement error handling in the object decoding process to catch exceptions and return a partial response instead of a RuntimeException. Consider logging the error details for further analysis.",
            "problem_location": {
                "files": [
                    "RollingLevelDBTimelineStore.java",
                    "TimelineWebServices.java",
                    "TimelineDataManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore",
                    "org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices",
                    "org.apache.hadoop.yarn.server.timeline.TimelineDataManager"
                ],
                "methods": [
                    "RollingLevelDBTimelineStore.getEntity",
                    "TimelineWebServices.getEntity",
                    "TimelineDataManager.doGetEntity"
                ]
            },
            "possible_fix": "In the 'getEntity' method of 'RollingLevelDBTimelineStore', wrap the decoding logic in a try-catch block to handle exceptions. If an exception occurs, log the error and return a partial response indicating the failure instead of throwing a RuntimeException."
        },
        "possible_fix_code": {
            "RollingLevelDBTimelineStore.getEntity": "  private static TimelineEntity getEntity(String entityId, String entityType,\n      Long startTime, EnumSet<Field> fields, DBIterator iterator,\n      byte[] prefix, int prefixlen) throws IOException {\n    if (fields == null) {\n      fields = EnumSet.allOf(Field.class);\n    }\n\n    TimelineEntity entity = new TimelineEntity();\n    boolean events = false;\n    boolean lastEvent = false;\n    if (fields.contains(Field.EVENTS)) {\n      events = true;\n    } else if (fields.contains(Field.LAST_EVENT_ONLY)) {\n      lastEvent = true;\n    } else {\n      entity.setEvents(null);\n    }\n    boolean relatedEntities = false;\n    if (fields.contains(Field.RELATED_ENTITIES)) {\n      relatedEntities = true;\n    } else {\n      entity.setRelatedEntities(null);\n    }\n    boolean primaryFilters = false;\n    if (fields.contains(Field.PRIMARY_FILTERS)) {\n      primaryFilters = true;\n    } else {\n      entity.setPrimaryFilters(null);\n    }\n    boolean otherInfo = false;\n    if (fields.contains(Field.OTHER_INFO)) {\n      otherInfo = true;\n    } else {\n      entity.setOtherInfo(null);\n    }\n\n    try {\n      // iterate through the entity's entry, parsing information if it is part\n      // of a requested field\n      for (; iterator.hasNext(); iterator.next()) {\n        byte[] key = iterator.peekNext().getKey();\n        if (!prefixMatches(prefix, prefixlen, key)) {\n          break;\n        }\n        if (key.length == prefixlen) {\n          continue;\n        }\n        if (key[prefixlen] == PRIMARY_FILTERS_COLUMN[0]) {\n          if (primaryFilters) {\n            addPrimaryFilter(entity, key, prefixlen\n                + PRIMARY_FILTERS_COLUMN.length);\n          }\n        } else if (key[prefixlen] == OTHER_INFO_COLUMN[0]) {\n          if (otherInfo) {\n            entity.addOtherInfo(\n                parseRemainingKey(key, prefixlen + OTHER_INFO_COLUMN.length),\n                fstConf.asObject(iterator.peekNext().getValue()));\n          }\n        } else if (key[prefixlen] == RELATED_ENTITIES_COLUMN[0]) {\n          if (relatedEntities) {\n            addRelatedEntity(entity, key, prefixlen\n                + RELATED_ENTITIES_COLUMN.length);\n          }\n        } else if (key[prefixlen] == EVENTS_COLUMN[0]) {\n          if (events || (lastEvent && entity.getEvents().size() == 0)) {\n            TimelineEvent event = getEntityEvent(null, key, prefixlen\n                + EVENTS_COLUMN.length, iterator.peekNext().getValue());\n            if (event != null) {\n              entity.addEvent(event);\n            }\n          }\n        } else if (key[prefixlen] == DOMAIN_ID_COLUMN[0]) {\n          byte[] v = iterator.peekNext().getValue();\n          String domainId = new String(v, UTF_8);\n          entity.setDomainId(domainId);\n        } else {\n          LOG.warn(String.format(\"Found unexpected column for entity %s of \"\n              + \"type %s (0x%02x)\", entityId, entityType, key[prefixlen]));\n        }\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error during entity decoding\", e);\n      // Handle the error gracefully, possibly returning a partial entity or null\n      return null; // or return a partial entity with error details\n    }\n\n    entity.setEntityId(entityId);\n    entity.setEntityType(entityType);\n    entity.setStartTime(startTime);\n\n    return entity;\n  }"
        }
    },
    {
        "filename": "YARN-3742.json",
        "creation_time": "2015-05-29T06:00:38.000+0000",
        "bug_report": {
            "Title": "YARN RM will shut down if ZKClient creation times out",
            "Description": "The ResourceManager (RM) crashes when it fails to establish a connection with the Zookeeper (ZK) client due to a timeout. Instead of shutting down, the RM should transition to a StandBy state, allowing another RM to take over the responsibilities. The stack trace indicates that the failure occurs during the ZK client creation process, specifically in the ZKRMStateStore class when it attempts to update the application state.",
            "StackTrace": [
                "2015-04-19 01:22:20,513  FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Received a org.apache.hadoop.yarn.server.resourcemanager.RMFatalEvent of type STATE_STORE_OP_FAILED. Cause:",
                "java.io.IOException: Wait for ZKClient creation timed out",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:1066)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:1090)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.existsWithRetries(ZKRMStateStore.java:996)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.updateApplicationStateInternal(ZKRMStateStore.java:643)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition.transition(RMStateStore.java:162)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition.transition(RMStateStore.java:147)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.handleStoreEvent(RMStateStore.java:806)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:879)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:874)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The ResourceManager is not handling the timeout exception from the ZK client creation properly, leading to a fatal event that causes the RM to shut down instead of transitioning to a StandBy state.",
            "StepsToReproduce": [
                "1. Configure the YARN ResourceManager to connect to a Zookeeper instance.",
                "2. Simulate a timeout scenario for the ZK client connection (e.g., by stopping the ZK service).",
                "3. Start the ResourceManager and observe the behavior."
            ],
            "ExpectedBehavior": "The ResourceManager should transition to a StandBy state and allow another RM to take over instead of shutting down.",
            "ObservedBehavior": "The ResourceManager shuts down completely when the ZK client creation times out.",
            "Suggestions": "Implement a state transition to StandBy in the event of a ZK client timeout instead of exiting the application.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStore.java",
                    "hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/state/StateMachineFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore",
                    "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore",
                    "org.apache.hadoop.yarn.state.StateMachineFactory"
                ],
                "methods": [
                    "ZKRMStateStore.updateApplicationStateInternal",
                    "RMStateStore.handleStoreEvent",
                    "StateMachineFactory.doTransition"
                ]
            },
            "possible_fix": "In the ZKRMStateStore class, modify the exception handling in the updateApplicationStateInternal method to transition to a StandBy state instead of allowing a fatal event to occur. For example:\n\n```java\ntry {\n    // existing code to update application state\n} catch (IOException e) {\n    LOG.error(\"ZKClient creation failed, transitioning to StandBy\", e);\n    transitionToStandBy(); // Implement this method to handle the transition\n}\n```"
        },
        "possible_fix_code": {
            "ZKRMStateStore.updateApplicationStateInternal": "  public synchronized void updateApplicationStateInternal(ApplicationId appId,\n      ApplicationStateData appStateDataPB) throws Exception {\n    String nodeUpdatePath = getNodePath(rmAppRoot, appId.toString());\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Storing final state info for app: \" + appId + \" at: \"\n          + nodeUpdatePath);\n    }\n    byte[] appStateData = appStateDataPB.getProto().toByteArray();\n\n    try {\n      if (existsWithRetries(nodeUpdatePath, false) != null) {\n        setDataWithRetries(nodeUpdatePath, appStateData, -1);\n      } else {\n        createWithRetries(nodeUpdatePath, appStateData, zkAcl,\n          CreateMode.PERSISTENT);\n        LOG.debug(appId + \" znode didn't exist. Created a new znode to\"\n            + \" update the application state.\");\n      }\n    } catch (IOException e) {\n      LOG.error(\"ZKClient creation failed, transitioning to StandBy\", e);\n      transitionToStandBy(); // Implement this method to handle the transition\n    }\n  }"
        }
    },
    {
        "filename": "YARN-4984.json",
        "creation_time": "2016-04-21T19:16:03.000+0000",
        "bug_report": {
            "Title": "LogAggregationService shouldn't swallow exception in handling createAppDir() which causes thread leak.",
            "Description": "The LogAggregationService is failing to properly handle exceptions thrown during the creation of application log directories. Specifically, when an invalid token is encountered, the exception is swallowed, leading to the creation of an aggregator thread for an invalid application. This results in resource leaks and stale application entries persisting in the NodeManager state store after a restart.",
            "StackTrace": [
                "158 2016-04-19 23:38:33,039 ERROR logaggregation.LogAggregationService (LogAggregationService.java:run(300)) - Failed to setup application log directory for application_1448060878692_11842",
                "159 org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 1380589 for hdfswrite) can't be found in cache",
                "160 at org.apache.hadoop.ipc.Client.call(Client.java:1427)",
                "161 at org.apache.hadoop.ipc.Client.call(Client.java:1358)",
                "162 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)",
                "163 at com.sun.proxy.$Proxy13.getFileInfo(Unknown Source)",
                "164 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)",
                "165 at sun.reflect.GeneratedMethodAccessor76.invoke(Unknown Source)",
                "166 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "167 at java.lang.reflect.Method.invoke(Method.java:606)",
                "168 at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:252)",
                "169 at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)",
                "170 at com.sun.proxy.$Proxy14.getFileInfo(Unknown Source)",
                "171 at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2116)",
                "172 at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1315)",
                "173 at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1311)",
                "174 at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "175 at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1311)",
                "176 at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.checkExists(LogAggregationService.java:248)",
                "177 at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.access$100(LogAggregationService.java:67)",
                "178 at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$1.run(LogAggregationService.java:276)",
                "179 at java.security.AccessController.doPrivileged(Native Method)",
                "180 at javax.security.auth.Subject.doAs(Subject.java:415)",
                "181 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "182 at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.createAppDir(LogAggregationService.java:261)",
                "183 at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initAppAggregator(LogAggregationService.java:367)",
                "184 at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initApp(LogAggregationService.java:320)",
                "185 at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:447)",
                "186 at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:67)"
            ],
            "RootCause": "The root cause of the issue is that the LogAggregationService does not properly handle exceptions thrown during the creation of application directories, specifically when an invalid token is encountered. This leads to the creation of aggregator threads for applications that should not be initialized.",
            "StepsToReproduce": [
                "1. Start the NodeManager with stale applications in the state store.",
                "2. Attempt to initialize an application that has an invalid token.",
                "3. Observe the logs for swallowed exceptions and the creation of aggregator threads."
            ],
            "ExpectedBehavior": "The LogAggregationService should throw an exception when it encounters an invalid token, preventing the creation of an aggregator thread for the application.",
            "ObservedBehavior": "The LogAggregationService swallows the exception and continues to create an aggregator thread for the application, leading to resource leaks.",
            "Suggestions": "Modify the LogAggregationService to properly handle exceptions during the directory creation process. Ensure that if an exception occurs, the aggregator thread is not created.",
            "problem_location": {
                "files": [
                    "LogAggregationService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService"
                ],
                "methods": [
                    "LogAggregationService.createAppDir",
                    "LogAggregationService.initAppAggregator",
                    "LogAggregationService.handle"
                ]
            },
            "possible_fix": "In the createAppDir method, ensure that exceptions are not swallowed. Instead, propagate the exception up to the caller to prevent the creation of the aggregator thread. For example, modify the catch block to rethrow the exception after logging it."
        },
        "possible_fix_code": {
            "LogAggregationService.createAppDir": "  protected void createAppDir(final String user, final ApplicationId appId,\n      UserGroupInformation userUgi) {\n    try {\n      userUgi.doAs(new PrivilegedExceptionAction<Object>() {\n        @Override\n        public Object run() throws Exception {\n          try {\n            // TODO: Reuse FS for user?\n            FileSystem remoteFS = getFileSystem(getConfig());\n\n            // Only creating directories if they are missing to avoid\n            // unnecessary load on the filesystem from all of the nodes\n            Path appDir = LogAggregationUtils.getRemoteAppLogDir(\n                LogAggregationService.this.remoteRootLogDir, appId, user,\n                LogAggregationService.this.remoteRootLogDirSuffix);\n            appDir = appDir.makeQualified(remoteFS.getUri(),\n                remoteFS.getWorkingDirectory());\n\n            if (!checkExists(remoteFS, appDir, APP_DIR_PERMISSIONS)) {\n              Path suffixDir = LogAggregationUtils.getRemoteLogSuffixedDir(\n                  LogAggregationService.this.remoteRootLogDir, user,\n                  LogAggregationService.this.remoteRootLogDirSuffix);\n              suffixDir = suffixDir.makeQualified(remoteFS.getUri(),\n                  remoteFS.getWorkingDirectory());\n\n              if (!checkExists(remoteFS, suffixDir, APP_DIR_PERMISSIONS)) {\n                Path userDir = LogAggregationUtils.getRemoteLogUserDir(\n                    LogAggregationService.this.remoteRootLogDir, user);\n                userDir = userDir.makeQualified(remoteFS.getUri(),\n                    remoteFS.getWorkingDirectory());\n\n                if (!checkExists(remoteFS, userDir, APP_DIR_PERMISSIONS)) {\n                  createDir(remoteFS, userDir, APP_DIR_PERMISSIONS);\n                }\n\n                createDir(remoteFS, suffixDir, APP_DIR_PERMISSIONS);\n              }\n\n              createDir(remoteFS, appDir, APP_DIR_PERMISSIONS);\n            }\n\n          } catch (IOException e) {\n            LOG.error(\"Failed to setup application log directory for \"\n                + appId, e);\n            throw e; // Rethrow the exception to prevent thread leak\n          }\n          return null;\n        }\n      });\n    } catch (Exception e) {\n      throw new YarnRuntimeException(e);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-4584.json",
        "creation_time": "2016-01-12T09:08:31.000+0000",
        "bug_report": {
            "Title": "RM startup failure when AM attempts greater than max-attempts",
            "Description": "The Resource Manager (RM) fails to start after an Application Master (AM) is preempted multiple times due to resource limits in the default queue. This issue arises when the AM is assigned to the default queue and gets preempted 20 times, leading to a NullPointerException during the recovery process upon RM restart.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.recover(RMAppAttemptImpl.java:887)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recover(RMAppImpl.java:826)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:953)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:946)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:786)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:328)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:464)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1232)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:594)"
            ],
            "RootCause": "The root cause of the issue is a NullPointerException occurring in the RMAppAttemptImpl.recover method when attempting to recover the application state after the Resource Manager restarts. This happens because the application state may not be properly initialized or may contain null values after multiple preemptions.",
            "StepsToReproduce": [
                "Configure 3 queues in the cluster with specified resource limits.",
                "Submit applications to all 3 queues with a container size of 1024MB.",
                "Ensure that the AM assigned to the default queue is preempted multiple times (20 times in this case).",
                "Restart the Resource Manager."
            ],
            "ExpectedBehavior": "The Resource Manager should successfully restart and recover the application states without throwing a NullPointerException.",
            "ObservedBehavior": "The Resource Manager fails to start and throws a NullPointerException during the recovery process.",
            "Suggestions": "Implement checks in the RMAppAttemptImpl.recover method to ensure that the application state is valid and not null before attempting to access its properties. Additionally, ensure that the application state is correctly initialized during the application submission process.",
            "problem_location": {
                "files": [
                    "RMAppAttemptImpl.java",
                    "RMAppImpl.java",
                    "RMAppManager.java",
                    "ResourceManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl",
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl",
                    "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager",
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"
                ],
                "methods": [
                    "RMAppAttemptImpl.recover",
                    "RMAppImpl.recover",
                    "RMAppManager.recoverApplication",
                    "ResourceManager.recover"
                ]
            },
            "possible_fix": "In the RMAppAttemptImpl.recover method, add null checks for the application state and its properties before accessing them. For example:\n\nif (appState == null) {\n    LOG.error(\"Application state is null for application ID: \" + getAppAttemptId().getApplicationId());\n    return;\n}\n\nThis will prevent the NullPointerException from occurring during recovery."
        },
        "possible_fix_code": {
            "RMAppAttemptImpl.recover": "  public void recover(RMState state) {\n    ApplicationStateData appState =\n        state.getApplicationState().get(getAppAttemptId().getApplicationId());\n    if (appState == null) {\n        LOG.error(\"Application state is null for application ID: \" + getAppAttemptId().getApplicationId());\n        return;\n    }\n    ApplicationAttemptStateData attemptState =\n        appState.getAttempt(getAppAttemptId());\n    assert attemptState != null;\n    LOG.info(\"Recovering attempt: \" + getAppAttemptId() + \" with final state: \"\n        + attemptState.getState());\n    diagnostics.append(\"Attempt recovered after RM restart\");\n    diagnostics.append(attemptState.getDiagnostics());\n    this.amContainerExitStatus = attemptState.getAMContainerExitStatus();\n    if (amContainerExitStatus == ContainerExitStatus.PREEMPTED) {\n      this.attemptMetrics.setIsPreempted();\n    }\n\n    Credentials credentials = attemptState.getAppAttemptTokens();\n    setMasterContainer(attemptState.getMasterContainer());\n    recoverAppAttemptCredentials(credentials, attemptState.getState());\n    this.recoveredFinalState = attemptState.getState();\n    this.originalTrackingUrl = attemptState.getFinalTrackingUrl();\n    this.finalStatus = attemptState.getFinalApplicationStatus();\n    this.startTime = attemptState.getStartTime();\n    this.finishTime = attemptState.getFinishTime();\n    this.attemptMetrics.updateAggregateAppResourceUsage(\n        attemptState.getMemorySeconds(),attemptState.getVcoreSeconds());\n  }"
        }
    },
    {
        "filename": "YARN-2846.json",
        "creation_time": "2014-11-11T15:30:08.000+0000",
        "bug_report": {
            "Title": "Incorrect persist exit code for running containers in reacquireContainer() that interrupted by NodeManager restart.",
            "Description": "The issue arises when the NodeManager (NM) is restarted while an Application Master (AM) container is running. During the NM shutdown, the reacquireContainer() method in the ContainerExecutor class attempts to check if the container process is alive. If the NM is stopped, the thread waiting for the process to exit is interrupted, leading to an IOException. This exception is caught in the RecoveredContainerLaunch.call() method, which defaults the exit code to LOST (154) instead of properly handling the interruption. This results in the incorrect exit code being recorded in the NMStateStore after a restart.",
            "StackTrace": [
                "java.io.IOException: Interrupted while waiting for process 20001 to exit",
                "at org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.reacquireContainer(ContainerExecutor.java:180)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch.call(RecoveredContainerLaunch.java:82)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch.call(RecoveredContainerLaunch.java:46)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.InterruptedException: sleep interrupted",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.reacquireContainer(ContainerExecutor.java:177)"
            ],
            "RootCause": "The root cause of the issue is that the reacquireContainer() method does not handle the InterruptedException properly. When the NM is stopped, the thread waiting for the container process to exit is interrupted, leading to an IOException that defaults the exit code to LOST.",
            "StepsToReproduce": [
                "1. Start a container using the NodeManager.",
                "2. Initiate a restart of the NodeManager while the container is still running.",
                "3. Observe the exit code recorded for the container after the NodeManager restarts."
            ],
            "ExpectedBehavior": "The exit code for the container should reflect its actual termination status instead of defaulting to LOST when the NodeManager is interrupted.",
            "ObservedBehavior": "The exit code for the container is recorded as LOST (154) after the NodeManager restart, indicating that the container was killed due to the NM shutdown.",
            "Suggestions": "Modify the reacquireContainer() method to handle InterruptedException more gracefully. Instead of throwing an IOException, the method should check if the interruption was due to NM shutdown and avoid recording the exit code in such cases.",
            "problem_location": {
                "files": [
                    "ContainerExecutor.java",
                    "RecoveredContainerLaunch.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch"
                ],
                "methods": [
                    "ContainerExecutor.reacquireContainer",
                    "RecoveredContainerLaunch.call"
                ]
            },
            "possible_fix": "In the reacquireContainer() method, modify the catch block for InterruptedException to check for NM shutdown and avoid recording the exit code. For example:\n\n```java\ncatch (InterruptedException e) {\n    if (isNMShuttingDown()) {\n        LOG.info(\"NM is shutting down, not recording exit code for container \" + containerId);\n        return ExitCode.TERMINATED.getExitCode();\n    }\n    throw new IOException(\"Interrupted while waiting for process \" + pid + \" to exit\", e);\n}\n```"
        },
        "possible_fix_code": {
            "reacquireContainer": "  public int reacquireContainer(String user, ContainerId containerId)\n      throws IOException {\n    Path pidPath = getPidFilePath(containerId);\n    if (pidPath == null) {\n      LOG.warn(containerId + \" is not active, returning terminated error\");\n      return ExitCode.TERMINATED.getExitCode();\n    }\n\n    String pid = null;\n    pid = ProcessIdFileReader.getProcessId(pidPath);\n    if (pid == null) {\n      throw new IOException(\"Unable to determine pid for \" + containerId);\n    }\n\n    LOG.info(\"Reacquiring \" + containerId + \" with pid \" + pid);\n    try {\n      while(isContainerProcessAlive(user, pid)) {\n        Thread.sleep(1000);\n      }\n    } catch (InterruptedException e) {\n      if (isNMShuttingDown()) {\n        LOG.info(\"NM is shutting down, not recording exit code for container \" + containerId);\n        return ExitCode.TERMINATED.getExitCode();\n      }\n      throw new IOException(\"Interrupted while waiting for process \" + pid\n          + \" to exit\", e);\n    }\n\n    // wait for exit code file to appear\n    String exitCodeFile = ContainerLaunch.getExitCodeFile(pidPath.toString());\n    File file = new File(exitCodeFile);\n    final int sleepMsec = 100;\n    int msecLeft = 2000;\n    while (!file.exists() && msecLeft >= 0) {\n      if (!isContainerActive(containerId)) {\n        LOG.info(containerId + \" was deactivated\");\n        return ExitCode.TERMINATED.getExitCode();\n      }\n      try {\n        Thread.sleep(sleepMsec);\n      } catch (InterruptedException e) {\n        if (isNMShuttingDown()) {\n          LOG.info(\"NM is shutting down, not recording exit code for container \" + containerId);\n          return ExitCode.TERMINATED.getExitCode();\n        }\n        throw new IOException(\n            \"Interrupted while waiting for exit code from \" + containerId, e);\n      }\n      msecLeft -= sleepMsec;\n    }\n    if (msecLeft < 0) {\n      throw new IOException(\"Timeout while waiting for exit code from \"\n          + containerId);\n    }\n\n    try {\n      return Integer.parseInt(FileUtils.readFileToString(file).trim());\n    } catch (NumberFormatException e) {\n      throw new IOException(\"Error parsing exit code from pid \" + pid, e);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-7890.json",
        "creation_time": "2018-02-03T21:10:43.000+0000",
        "bug_report": {
            "Title": "NPE during container relaunch",
            "Description": "A NullPointerException (NPE) occurs when attempting to relaunch a container in the YARN NodeManager. The issue arises specifically when the method `getFilecacheDirs()` is called, which attempts to return an unmodifiable list of file cache directories. If the `filecacheDirs` list is null, this results in an NPE. This situation can occur if the container's context is not properly initialized or if the file cache directories are not set before the relaunch process.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at java.util.Collections$UnmodifiableCollection.<init>(Collections.java:1026)",
                "at java.util.Collections$UnmodifiableList.<init>(Collections.java:1302)",
                "at java.util.Collections.unmodifiableList(Collections.java:1287)",
                "at org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext.getFilecacheDirs(ContainerStartContext.java:200)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.buildContainerRuntimeContext(LinuxContainerExecutor.java:651)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:546)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:465)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch.call(ContainerRelaunch.java:107)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch.call(ContainerRelaunch.java:49)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The root cause of the NPE is that the `filecacheDirs` list in the `ContainerStartContext` is null when `getFilecacheDirs()` is called. This indicates that the container's context was not properly initialized with the necessary file cache directories.",
            "StepsToReproduce": [
                "1. Start a YARN NodeManager.",
                "2. Attempt to relaunch a container that has not been properly initialized.",
                "3. Observe the logs for a NullPointerException during the relaunch process."
            ],
            "ExpectedBehavior": "The container should relaunch successfully without throwing a NullPointerException, and the file cache directories should be properly initialized.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to access the file cache directories during the container relaunch process.",
            "Suggestions": "Ensure that the `filecacheDirs` list is initialized before it is accessed in the `ContainerStartContext`. This may involve checking the initialization logic for the container context and ensuring that all necessary parameters are set before relaunching.",
            "problem_location": {
                "files": [
                    "ContainerStartContext.java",
                    "LinuxContainerExecutor.java",
                    "ContainerLaunch.java",
                    "ContainerRelaunch.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext",
                    "org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch"
                ],
                "methods": [
                    "ContainerStartContext.getFilecacheDirs",
                    "LinuxContainerExecutor.buildContainerRuntimeContext",
                    "LinuxContainerExecutor.launchContainer",
                    "ContainerLaunch.launchContainer",
                    "ContainerRelaunch.call"
                ]
            },
            "possible_fix": "In the `ContainerStartContext` class, ensure that the `filecacheDirs` list is initialized properly. For example, modify the constructor to set `this.filecacheDirs` to an empty list if no directories are provided. This can prevent the NPE when `getFilecacheDirs()` is called."
        },
        "possible_fix_code": {
            "ContainerStartContext.getFilecacheDirs": "  public List<String> getFilecacheDirs() {\n    if (this.filecacheDirs == null) {\n      return Collections.unmodifiableList(new ArrayList<>());\n    }\n    return Collections.unmodifiableList(this.filecacheDirs);\n  }"
        }
    },
    {
        "filename": "YARN-139.json",
        "creation_time": "2012-10-01T19:51:20.000+0000",
        "bug_report": {
            "Title": "Interrupted Exception within AsyncDispatcher leads to user confusion",
            "Description": "During the shutdown process of the AsyncDispatcher, an InterruptedException is thrown when attempting to join the event handling thread. This exception is logged as a warning, but it is harmless and does not affect the application's functionality. However, it can lead to confusion for users who may misinterpret the warning as a critical error. The issue arises from the way the stop method is implemented in the AsyncDispatcher class, where the thread is interrupted and then joined, potentially leading to the InterruptedException being thrown.",
            "StackTrace": [
                "java.lang.InterruptedException",
                "at java.lang.Object.wait(Native Method)",
                "at java.lang.Thread.join(Thread.java:1143)",
                "at java.lang.Thread.join(Thread.java:1196)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.stop(AsyncDispatcher.java:105)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:437)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:402)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The root cause of the issue is the handling of thread interruption in the stop method of the AsyncDispatcher class. When the event handling thread is interrupted, it can lead to an InterruptedException when the code attempts to join the thread.",
            "StepsToReproduce": [
                "Start the application using the AsyncDispatcher.",
                "Trigger a shutdown of the application.",
                "Observe the logs for the InterruptedException warning."
            ],
            "ExpectedBehavior": "The application should shut down gracefully without logging any warnings or errors related to InterruptedExceptions.",
            "ObservedBehavior": "During shutdown, a warning is logged indicating an InterruptedException occurred, which may confuse users.",
            "Suggestions": "Consider modifying the stop method in the AsyncDispatcher to handle the InterruptedException more gracefully, possibly by suppressing the warning log or providing a clearer message that indicates the exception is expected and harmless.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java",
                    "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/CompositeService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.event.AsyncDispatcher",
                    "org.apache.hadoop.yarn.service.CompositeService"
                ],
                "methods": [
                    "AsyncDispatcher.stop",
                    "CompositeService.stop"
                ]
            },
            "possible_fix": "In the AsyncDispatcher.stop method, modify the catch block for InterruptedException to log a message that clarifies the exception is expected and does not indicate a failure. For example:\n\n```java\ntry {\n    eventHandlingThread.join();\n} catch (InterruptedException ie) {\n    LOG.info(\"Thread was interrupted during shutdown, this is expected and harmless.\", ie);\n}\n```"
        },
        "possible_fix_code": {
            "AsyncDispatcher.stop": "  public void stop() {\n    stopped = true;\n    if (eventHandlingThread != null) {\n      eventHandlingThread.interrupt();\n      try {\n        eventHandlingThread.join();\n      } catch (InterruptedException ie) {\n        LOG.info(\"Thread was interrupted during shutdown, this is expected and harmless.\", ie);\n      }\n    }\n\n    // stop all the components\n    super.stop();\n  }"
        }
    },
    {
        "filename": "YARN-42.json",
        "creation_time": "2012-05-14T11:38:55.000+0000",
        "bug_report": {
            "Title": "Node Manager throws NPE on startup",
            "Description": "The Node Manager (NM) fails to start due to a NullPointerException (NPE) when it attempts to initialize the LocalizationService. This issue arises specifically when the NM does not have the necessary permissions to create directories in its local directory. The stack trace indicates that the failure occurs during the attempt to create the user cache directory, leading to an IOException, which subsequently causes the NPE during the shutdown process.",
            "StackTrace": [
                "2012-05-14 16:32:13,468 FATAL org.apache.hadoop.yarn.server.nodemanager.NodeManager: Error starting NodeManager",
                "org.apache.hadoop.yarn.YarnException: Failed to initialize LocalizationService",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.init(ResourceLocalizationService.java:202)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.init(ContainerManagerImpl.java:183)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.init(NodeManager.java:166)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:268)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:284)",
                "Caused by: java.io.IOException: mkdir of /mrv2/tmp/nm-local-dir/usercache failed",
                "at org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:907)",
                "at org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:143)",
                "at org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:189)",
                "at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:706)",
                "at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:703)",
                "at org.apache.hadoop.fs.FileContext.FSLinkResolver.resolve(FileContext.java:2325)",
                "at org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:703)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.init(ResourceLocalizationService.java:188)",
                "... 6 more",
                "2012-05-14 16:32:13,472 INFO org.apache.hadoop.yarn.service.CompositeService: Error stopping org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.stop(NonAggregatingLogHandler.java:82)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.stop(ContainerManagerImpl.java:266)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:182)",
                "at org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:122)",
                "at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)"
            ],
            "RootCause": "The root cause of the issue is a failure to create the necessary directory for the Node Manager's local cache due to insufficient permissions, which leads to an IOException. This IOException is not handled properly, resulting in a NullPointerException during the shutdown process.",
            "StepsToReproduce": [
                "Ensure that the Node Manager's local directory does not have the necessary permissions for directory creation.",
                "Start the Node Manager.",
                "Observe the logs for the NullPointerException and the failure to initialize the LocalizationService."
            ],
            "ExpectedBehavior": "The Node Manager should start successfully and initialize the LocalizationService without throwing exceptions.",
            "ObservedBehavior": "The Node Manager fails to start and throws a NullPointerException due to an inability to create the required local directory.",
            "Suggestions": "Ensure that the Node Manager has the correct permissions to create directories in its local directory. Additionally, implement error handling for IOException in the ResourceLocalizationService to prevent NPE during shutdown.",
            "problem_location": {
                "files": [
                    "ResourceLocalizationService.java",
                    "NodeManager.java",
                    "NonAggregatingLogHandler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService",
                    "org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler"
                ],
                "methods": [
                    "ResourceLocalizationService.init",
                    "NodeManager.init",
                    "NonAggregatingLogHandler.stop"
                ]
            },
            "possible_fix": "Check and set the appropriate permissions for the Node Manager's local directory before starting the service. Additionally, modify the ResourceLocalizationService.init method to handle IOException gracefully and log an appropriate error message instead of allowing a NullPointerException to propagate."
        },
        "possible_fix_code": {
            "ResourceLocalizationService.init": "public void init() throws IOException {\n    try {\n        // Attempt to create the user cache directory\n        Path userCacheDir = new Path(localDir, \"usercache\");\n        FileSystem fs = FileSystem.get(conf);\n        fs.mkdir(userCacheDir, new FsPermission(FsAction.ALL, FsAction.NONE, FsAction.NONE), true);\n    } catch (IOException e) {\n        // Log the error and handle it gracefully\n        LOG.error(\"Failed to create user cache directory: \" + e.getMessage(), e);\n        throw new IOException(\"Failed to initialize LocalizationService due to directory creation error\", e);\n    }\n}"
        }
    },
    {
        "filename": "YARN-7453.json",
        "creation_time": "2017-11-07T09:46:28.000+0000",
        "bug_report": {
            "Title": "Fix issue where RM fails to switch to active after first successful start",
            "Description": "The ResourceManager (RM) fails to transition to the ACTIVE state after the first successful start due to a NoAuthException thrown by ZooKeeper during the state recovery process. This results in a continuous loop of attempts to switch states between ACTIVE and STANDBY, ultimately leading to a failure in loading or recovering the state. The root cause appears to be related to insufficient authentication permissions when accessing the ZooKeeper nodes.",
            "StackTrace": [
                "org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:113)",
                "at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:1006)",
                "at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:910)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl.doOperation(CuratorTransactionImpl.java:159)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl.commit(CuratorTransactionImpl.java:122)",
                "at org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction.commit(ZKCuratorManager.java:403)",
                "at org.apache.hadoop.util.curator.ZKCuratorManager.safeSetData(ZKCuratorManager.java:372)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.getAndIncrementEpoch(ZKRMStateStore.java:493)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:771)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1162)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1198)"
            ],
            "RootCause": "The RM fails to authenticate with ZooKeeper, resulting in a NoAuthException when attempting to recover the state. This indicates that the RM does not have the necessary permissions to access the required ZooKeeper nodes.",
            "StepsToReproduce": [
                "Start the ResourceManager.",
                "Observe the logs for the transition to ACTIVE state.",
                "Note the occurrence of the NoAuthException in the logs."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully transition to the ACTIVE state after the first successful start without encountering authentication errors.",
            "ObservedBehavior": "The ResourceManager fails to transition to ACTIVE and enters a loop of attempts to switch states, continuously logging errors related to authentication.",
            "Suggestions": "Ensure that the ResourceManager has the correct authentication credentials and permissions set up for accessing ZooKeeper. Review the ZooKeeper ACLs and update them to allow the RM to perform necessary operations.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                    "hadoop-common/src/main/java/org/apache/hadoop/util/curator/ZKCuratorManager.java",
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore",
                    "org.apache.hadoop.util.curator.ZKCuratorManager",
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"
                ],
                "methods": [
                    "ZKRMStateStore.getAndIncrementEpoch",
                    "ZKCuratorManager.safeSetData",
                    "ResourceManager.transitionToActive"
                ]
            },
            "possible_fix": "Update the ZooKeeper ACLs to grant the ResourceManager the necessary permissions. This may involve modifying the configuration files to ensure that the RM can authenticate properly with ZooKeeper and access the required nodes."
        },
        "possible_fix_code": {
            "ZKRMStateStore.getAndIncrementEpoch": "  public synchronized long getAndIncrementEpoch() throws Exception {\n    String epochNodePath = getNodePath(zkRootNodePath, EPOCH_NODE);\n    long currentEpoch = baseEpoch;\n\n    if (exists(epochNodePath)) {\n      // load current epoch\n      byte[] data = getData(epochNodePath);\n      Epoch epoch = new EpochPBImpl(EpochProto.parseFrom(data));\n      currentEpoch = epoch.getEpoch();\n      // increment epoch and store it\n      byte[] storeData = Epoch.newInstance(currentEpoch + 1).getProto()\n          .toByteArray();\n      try {\n        zkManager.safeSetData(epochNodePath, storeData, -1, zkAcl,\n            fencingNodePath);\n      } catch (KeeperException.NoAuthException e) {\n        LOG.error(\"NoAuthException while setting data in ZooKeeper. Please check the ACLs for the ResourceManager.\", e);\n        throw new Exception(\"Authentication failed while accessing ZooKeeper. Ensure RM has the correct permissions.\", e);\n      }\n    } else {\n      // initialize epoch node with 1 for the next time.\n      byte[] storeData = Epoch.newInstance(currentEpoch + 1).getProto()\n          .toByteArray();\n      try {\n        zkManager.safeCreate(epochNodePath, storeData, zkAcl,\n            CreateMode.PERSISTENT, zkAcl, fencingNodePath);\n      } catch (KeeperException.NoAuthException e) {\n        LOG.error(\"NoAuthException while creating epoch node in ZooKeeper. Please check the ACLs for the ResourceManager.\", e);\n        throw new Exception(\"Authentication failed while accessing ZooKeeper. Ensure RM has the correct permissions.\", e);\n      }\n    }\n\n    return currentEpoch;\n  }"
        }
    },
    {
        "filename": "YARN-3369.json",
        "creation_time": "2015-03-18T23:29:06.000+0000",
        "bug_report": {
            "Title": "Missing NullPointer check in AppSchedulingInfo causes RM to die",
            "Description": "The method checkForDeactivation() in AppSchedulingInfo.java does not handle the case where getResourceRequest() returns null. This leads to a NullPointerException when the method attempts to dereference the request object without checking for null. The failure occurs during the handling of NODE_UPDATE events, causing the ResourceManager (RM) to crash.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.checkForDeactivation(AppSchedulingInfo.java:383)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.decrementOutstanding(AppSchedulingInfo.java:375)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateOffSwitch(AppSchedulingInfo.java:360)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:270)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.allocate(FiCaSchedulerApp.java:142)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1559)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignOffSwitchContainers(LeafQueue.java:1384)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1263)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:816)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:588)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:449)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1017)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1059)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:114)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:739)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "RootCause": "The root cause of the issue is the lack of a null check for the ResourceRequest object returned by getResourceRequest() in the checkForDeactivation() method. If this method returns null, the subsequent call to request.getNumContainers() results in a NullPointerException.",
            "StepsToReproduce": [
                "Trigger a NODE_UPDATE event in the ResourceManager.",
                "Ensure that the getResourceRequest() method returns null for the specified priority.",
                "Observe the ResourceManager crashing due to a NullPointerException."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the NODE_UPDATE event gracefully without crashing, even if getResourceRequest() returns null.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when it attempts to dereference a null ResourceRequest object.",
            "Suggestions": "Add a null check for the ResourceRequest object in the checkForDeactivation() method before dereferencing it.",
            "problem_location": {
                "files": [
                    "AppSchedulingInfo.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"
                ],
                "methods": [
                    "checkForDeactivation",
                    "getResourceRequest"
                ]
            },
            "possible_fix": "In the checkForDeactivation() method, add a null check for the ResourceRequest object:\n\n```java\nResourceRequest request = getResourceRequest(priority, ResourceRequest.ANY);\nif (request != null && request.getNumContainers() > 0) {\n    deactivate = false;\n}\n```"
        },
        "possible_fix_code": {
            "checkForDeactivation": "  synchronized private void checkForDeactivation() {\n    boolean deactivate = true;\n    for (Priority priority : getPriorities()) {\n      ResourceRequest request = getResourceRequest(priority, ResourceRequest.ANY);\n      if (request != null && request.getNumContainers() > 0) {\n        deactivate = false;\n        break;\n      }\n    }\n    if (deactivate) {\n      activeUsersManager.deactivateApplication(user, applicationId);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-945.json",
        "creation_time": "2013-07-19T22:59:06.000+0000",
        "bug_report": {
            "Title": "AM register failing after AMRMToken",
            "Description": "The application master (AM) registration process is failing due to an AccessControlException indicating that SIMPLE authentication is not enabled. This occurs when the IPC server attempts to read and process a request from the client, but the authentication context is not properly initialized, leading to a failure in establishing a secure connection.",
            "StackTrace": [
                "509 2013-07-19 15:53:55,569 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54313: readAndProcess from client 127.0.0.1 threw exception [org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled. Available:[TOKEN]]",
                "510 org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled. Available:[TOKEN]",
                "511   at org.apache.hadoop.ipc.Server$Connection.initializeAuthContext(Server.java:1531)",
                "512   at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1482)",
                "513   at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:788)",
                "514   at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:587)",
                "515   at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:562)"
            ],
            "RootCause": "The root cause of the issue is that the IPC server is configured to use a security mechanism that requires SIMPLE authentication, but it is not enabled in the current configuration. This leads to an AccessControlException when the server tries to authenticate the client.",
            "StepsToReproduce": [
                "1. Start the Hadoop cluster without enabling SIMPLE authentication.",
                "2. Attempt to register an application master (AM) using AMRMToken.",
                "3. Observe the AccessControlException in the logs."
            ],
            "ExpectedBehavior": "The application master should successfully register with the resource manager without any authentication errors.",
            "ObservedBehavior": "The application master registration fails with an AccessControlException indicating that SIMPLE authentication is not enabled.",
            "Suggestions": "Enable SIMPLE authentication in the Hadoop configuration or ensure that the correct authentication mechanism is configured for the IPC server.",
            "problem_location": {
                "files": [
                    "Server.java"
                ],
                "classes": [
                    "org.apache.hadoop.ipc.Server"
                ],
                "methods": [
                    "Server$Connection.initializeAuthContext",
                    "Server$Connection.readAndProcess"
                ]
            },
            "possible_fix": "To resolve this issue, update the Hadoop configuration to enable SIMPLE authentication by setting the appropriate property in the configuration file (e.g., core-site.xml) or modify the authentication mechanism to match the expected configuration."
        },
        "possible_fix_code": {
            "Server$Connection.initializeAuthContext": "// The method body is not provided, but the fix involves ensuring that SIMPLE authentication is enabled in the configuration.\n// This is a placeholder for the actual implementation that would check the configuration.\npublic void initializeAuthContext() throws AccessControlException {\n    // Check if SIMPLE authentication is enabled in the configuration\n    Configuration conf = getConf();\n    if (!conf.getBoolean(\"hadoop.security.authentication\", false)) {\n        throw new AccessControlException(\"SIMPLE authentication is not enabled. Available:[TOKEN]\");\n    }\n    // Proceed with the rest of the initialization\n    // ... (rest of the method implementation)\n}"
        }
    },
    {
        "filename": "YARN-6072.json",
        "creation_time": "2017-01-08T09:21:12.000+0000",
        "bug_report": {
            "Title": "RM unable to start in secure mode",
            "Description": "The Resource Manager (RM) fails to start in secure mode due to a NullPointerException occurring in the AdminService's refreshServiceAcls method. This issue arises when the AdminService is invoked to refresh its ACLs before it has been fully initialized, leading to a failure in the transition to active state.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:569)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:707)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:302)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:142)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:888)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)"
            ],
            "RootCause": "The root cause of the issue is that the AdminService's server reference is null when refreshServiceAcls is called during the refreshAll process. This occurs because the AdminService's serviceStart method is not completed before the refreshAll method is invoked.",
            "StepsToReproduce": [
                "Start the Resource Manager in secure mode.",
                "Observe the logs for the NullPointerException in the AdminService."
            ],
            "ExpectedBehavior": "The Resource Manager should start successfully in secure mode without throwing any exceptions.",
            "ObservedBehavior": "The Resource Manager fails to start, throwing a NullPointerException during the ACL refresh process.",
            "Suggestions": "Ensure that the AdminService is fully initialized before invoking refreshAll. This may involve reordering the service start sequence or adding checks to confirm initialization.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/AdminService.java",
                    "hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.AdminService",
                    "org.apache.hadoop.ha.ActiveStandbyElector"
                ],
                "methods": [
                    "AdminService.refreshServiceAcls",
                    "AdminService.refreshAll",
                    "AdminService.transitionToActive",
                    "ActiveStandbyElector.becomeActive"
                ]
            },
            "possible_fix": "Modify the service start sequence to ensure that AdminService.serviceStart() completes before any calls to AdminService.refreshAll(). Alternatively, add a null check for the server reference in refreshServiceAcls to prevent the NullPointerException."
        },
        "possible_fix_code": {
            "AdminService.refreshServiceAcls": "  private synchronized void refreshServiceAcls(Configuration configuration,\n      PolicyProvider policyProvider) {\n    if (this.server == null) {\n      throw new IllegalStateException(\"AdminService is not fully initialized.\");\n    }\n    this.server.refreshServiceAclWithLoadedConfiguration(configuration,\n        policyProvider);\n  }"
        }
    },
    {
        "filename": "YARN-7663.json",
        "creation_time": "2017-12-15T01:52:46.000+0000",
        "bug_report": {
            "Title": "RMAppImpl: Invalid event: START at KILLED",
            "Description": "The application manager is attempting to process a START event while in the KILLED state, which is not a valid state transition. This results in an InvalidStateTransitionException being thrown. The issue can be reproduced deterministically by introducing a delay before the START event is created, indicating a race condition or improper state management.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: START at KILLED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:805)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:116)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:901)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:885)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The application is trying to transition to the START state while it is already in the KILLED state, which is not allowed. This indicates a flaw in the event handling logic or state management.",
            "StepsToReproduce": [
                "Send a kill command to the application.",
                "Immediately after, attempt to send a START event.",
                "Observe the logs for the InvalidStateTransitionException."
            ],
            "ExpectedBehavior": "The application should not attempt to process a START event when it is in the KILLED state, and should handle such events gracefully without throwing exceptions.",
            "ObservedBehavior": "The application throws an InvalidStateTransitionException when trying to process a START event while in the KILLED state.",
            "Suggestions": "Implement checks in the RMAppImpl.handle method to prevent processing of START events when the application is in a terminal state like KILLED.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java",
                    "hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/state/StateMachineFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory"
                ],
                "methods": [
                    "RMAppImpl.handle",
                    "StateMachineFactory.doTransition"
                ]
            },
            "possible_fix": "In the RMAppImpl.handle method, add a condition to check if the current state is KILLED before processing a START event. If it is, log a warning and return without attempting the state transition."
        },
        "possible_fix_code": {
            "RMAppImpl.handle": "  public void handle(RMAppEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId appID = event.getApplicationId();\n      LOG.debug(\"Processing event for \" + appID + \" of type \" + event.getType());\n      final RMAppState oldState = getState();\n\n      // Check if the current state is KILLED and the event type is START\n      if (oldState == RMAppState.KILLED && event.getType() == RMAppEventType.START) {\n        LOG.warn(\"Ignoring START event for app: \" + appID + \" as it is in KILLED state.\");\n        return; // Prevent processing of START event in KILLED state\n      }\n\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitionException e) {\n        LOG.error(\"App: \" + appID + \" can't handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n      }\n\n      // Log at INFO if we're not recovering or not in a terminal state.\n      // Log at DEBUG otherwise.\n      if ((oldState != getState()) &&\n          (((recoveredFinalState == null)) ||\n            (event.getType() != RMAppEventType.RECOVER))) {\n        LOG.info(String.format(STATE_CHANGE_MESSAGE, appID, oldState,\n            getState(), event.getType()));\n      } else if ((oldState != getState()) && LOG.isDebugEnabled()) {\n        LOG.debug(String.format(STATE_CHANGE_MESSAGE, appID, oldState,\n            getState(), event.getType()));\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-5873.json",
        "creation_time": "2016-11-12T09:54:20.000+0000",
        "bug_report": {
            "Title": "RM crashes with NPE if generic application history is enabled",
            "Description": "The ResourceManager (RM) crashes due to a NullPointerException (NPE) when the generic application history feature is enabled. The issue arises in the `hashCode` method of the `WritingContainerStartEvent` class, where it attempts to access the `ApplicationAttemptId` of a container that is null, leading to the NPE. This occurs during the handling of container start events in the application history writer.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingContainerStartEvent.hashCode(WritingContainerStartEvent.java:38)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$MultiThreadedDispatcher$CompositEventHandler.handle(RMApplicationHistoryWriter.java:354)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.containerStarted(RMApplicationHistoryWriter.java:278)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:251)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:210)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.allocate(FiCaSchedulerApp.java:227)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.handleNewContainerAllocation(RegularContainerAllocator.java:704)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.doAllocation(RegularContainerAllocator.java:746)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.allocate(RegularContainerAllocator.java:832)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.assignContainers(RegularContainerAllocator.java:81)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.assignContainers(FiCaSchedulerApp.java:931)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:1044)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:690)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:508)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateOrReserveNewContainers(CapacityScheduler.java:1475)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainerOnSingleNode(CapacityScheduler.java:1470)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1559)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1346)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:1221)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1601)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:149)",
                "at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the NullPointerException is that the `WritingContainerStartEvent` is trying to access the `ApplicationAttemptId` of a container that is null, which occurs when the container is not properly initialized or is missing during the event handling process.",
            "StepsToReproduce": [
                "Enable generic application history in the ResourceManager configuration.",
                "Submit an application that triggers container allocation.",
                "Monitor the ResourceManager logs for NullPointerException."
            ],
            "ExpectedBehavior": "The ResourceManager should handle container start events without crashing, even when the generic application history feature is enabled.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when handling container start events due to a null container reference.",
            "Suggestions": "Implement null checks in the `hashCode` method of `WritingContainerStartEvent` to prevent the NPE. Ensure that the container is properly initialized before accessing its properties.",
            "problem_location": {
                "files": [
                    "WritingContainerStartEvent.java",
                    "RMApplicationHistoryWriter.java",
                    "RMContainerImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingContainerStartEvent",
                    "org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter",
                    "org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl"
                ],
                "methods": [
                    "WritingContainerStartEvent.hashCode",
                    "RMApplicationHistoryWriter.containerStarted",
                    "RMContainerImpl.<init>"
                ]
            },
            "possible_fix": "In the `hashCode` method of `WritingContainerStartEvent`, add a null check for `containerId` before accessing its `ApplicationAttemptId`. For example:\n\n```java\n@Override\npublic int hashCode() {\n    return containerId != null ? containerId.getApplicationAttemptId().getApplicationId().hashCode() : 0;\n}\n```"
        },
        "possible_fix_code": {
            "WritingContainerStartEvent.hashCode": "  @Override\n  public int hashCode() {\n    return containerId != null ? containerId.getApplicationAttemptId().getApplicationId().hashCode() : 0;\n  }"
        }
    },
    {
        "filename": "YARN-3227.json",
        "creation_time": "2015-02-19T16:58:01.000+0000",
        "bug_report": {
            "Title": "Timeline renew delegation token fails when RM user's TGT is expired",
            "Description": "The ResourceManager (RM) fails to renew the delegation token when the Kerberos Ticket Granting Ticket (TGT) for the RM user has expired. This results in an IOException with an HTTP 401 Unauthorized status during job submission. The expected behavior is for the RM to automatically re-login and obtain a new TGT to successfully renew the delegation token.",
            "StackTrace": [
                "java.io.IOException: Failed to renew token: Kind: TIMELINE_DELEGATION_TOKEN, Service: timelineserver.example.com:4080, Ident: (owner=user, renewer=rmuser, realUser=oozie, issueDate=1423248845528, maxDate=1423853645528, sequenceNumber=9716, masterKeyId=9)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:443)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.access$800(DelegationTokenRenewer.java:77)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.handleDTRenewerAppSubmitEvent(DelegationTokenRenewer.java:808)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.run(DelegationTokenRenewer.java:789)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)",
                "Caused by: java.io.IOException: HTTP status [401], message [Unauthorized]",
                "at org.apache.hadoop.util.HttpExceptionUtils.validateResponse(HttpExceptionUtils.java:169)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.doDelegationTokenOperation(DelegationTokenAuthenticator.java:286)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.renewDelegationToken(DelegationTokenAuthenticator.java:211)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$2.run(TimelineClientImpl.java:374)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$2.run(TimelineClientImpl.java:360)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$4.run(TimelineClientImpl.java:429)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineClientConnectionRetry.retryOn(TimelineClientImpl.java:161)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.operateDelegationToken(TimelineClientImpl.java:444)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.renewDelegationToken(TimelineClientImpl.java:378)",
                "at org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier$Renewer.renew(TimelineDelegationTokenIdentifier.java:81)",
                "at org.apache.hadoop.security.token.Token.renew(Token.java:377)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:532)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:529)"
            ],
            "RootCause": "The root cause of the issue is that the ResourceManager's Kerberos TGT has expired, leading to an unauthorized error when attempting to renew the delegation token. The system does not automatically re-login to obtain a new TGT before the renewal attempt.",
            "StepsToReproduce": [
                "1. Ensure the RM user's Kerberos TGT is expired.",
                "2. Submit a job that requires the renewal of a delegation token.",
                "3. Observe the failure in the renewal process with an HTTP 401 Unauthorized error."
            ],
            "ExpectedBehavior": "The ResourceManager should automatically re-login to obtain a new TGT and successfully renew the delegation token.",
            "ObservedBehavior": "The ResourceManager fails to renew the delegation token and throws an IOException with an HTTP 401 Unauthorized status.",
            "Suggestions": "Implement a mechanism to check the validity of the TGT before attempting to renew the delegation token. If the TGT is expired, the RM should re-login to obtain a new TGT.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineClientImpl.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl",
                    "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator"
                ],
                "methods": [
                    "TimelineClientImpl.renewDelegationToken",
                    "DelegationTokenAuthenticator.renewDelegationToken"
                ]
            },
            "possible_fix": "Add a check in the TimelineClientImpl.renewDelegationToken method to verify if the TGT is valid before attempting to renew the delegation token. If the TGT is expired, invoke the re-login process to obtain a new TGT."
        },
        "possible_fix_code": {
            "TimelineClientImpl.renewDelegationToken": "  public long renewDelegationToken(\n      final Token<TimelineDelegationTokenIdentifier> timelineDT)\n          throws IOException, YarnException {\n    boolean isProxyAccess =\n        UserGroupInformation.getCurrentUser().getAuthenticationMethod()\n        == UserGroupInformation.AuthenticationMethod.PROXY;\n    final String doAsUser = isProxyAccess ?\n        UserGroupInformation.getCurrentUser().getShortUserName() : null;\n    boolean useHttps = YarnConfiguration.useHttps(this.getConfig());\n    final String scheme = useHttps ? \"https\" : \"http\";\n    final InetSocketAddress address = SecurityUtil.getTokenServiceAddr(timelineDT);\n\n    // Check if the TGT is valid before renewing the delegation token\n    if (!UserGroupInformation.getCurrentUser().isFromKeytab()) {\n        UserGroupInformation.getCurrentUser().reloginFromKeytab();\n    }\n\n    PrivilegedExceptionAction<Long> renewDTAction =\n        new PrivilegedExceptionAction<Long>() {\n\n          @Override\n          public Long run() throws Exception {\n            // If the timeline DT to renew is different than cached, replace it.\n            // Token to set every time for retry, because when exception happens,\n            // DelegationTokenAuthenticatedURL will reset it to null;\n            if (!timelineDT.equals(token.getDelegationToken())) {\n              token.setDelegationToken((Token) timelineDT);\n            }\n            DelegationTokenAuthenticatedURL authUrl =\n                new DelegationTokenAuthenticatedURL(authenticator,\n                    connConfigurator);\n            final URI serviceURI = new URI(scheme, null, address.getHostName(),\n                address.getPort(), RESOURCE_URI_STR, null, null);\n            return authUrl\n                .renewDelegationToken(serviceURI.toURL(), token, doAsUser);\n          }\n        };\n    return (Long) operateDelegationToken(renewDTAction);\n  }"
        }
    },
    {
        "filename": "YARN-4235.json",
        "creation_time": "2015-10-07T19:26:24.000+0000",
        "bug_report": {
            "Title": "FairScheduler PrimaryGroup does not handle empty groups returned for a user",
            "Description": "The FairScheduler encounters a NullPointerException (NPE) when it attempts to process an application for a user who has no associated groups. This results in an IndexOutOfBoundsException when the scheduler tries to access the first element of an empty list of groups, leading to a crash of the ResourceManager.",
            "StackTrace": [
                "java.lang.IndexOutOfBoundsException: Index: 0",
                "at java.util.Collections$EmptyList.get(Collections.java:3212)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule$PrimaryGroup.getQueueForApp(QueuePlacementRule.java:149)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule.assignAppToQueue(QueuePlacementRule.java:74)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy.assignAppToQueue(QueuePlacementPolicy.java:167)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.assignToQueue(FairScheduler.java:689)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.addApplication(FairScheduler.java:595)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1180)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:111)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the method 'getQueueForApp' in 'QueuePlacementRule$PrimaryGroup' does not handle the case where the list of groups is empty, leading to an attempt to access an index that does not exist.",
            "StepsToReproduce": [
                "1. Create a user with no associated groups.",
                "2. Submit an application for that user to the FairScheduler.",
                "3. Observe the ResourceManager crash due to NPE."
            ],
            "ExpectedBehavior": "The FairScheduler should gracefully handle cases where a user has no associated groups, either by assigning a default queue or rejecting the application with a clear error message.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when attempting to access an empty list of groups, resulting in an IndexOutOfBoundsException.",
            "Suggestions": "Implement a check in the 'getQueueForApp' method to handle empty groups appropriately. Consider returning a default queue or logging an error message instead of allowing the NPE to occur.",
            "problem_location": {
                "files": [
                    "QueuePlacementRule.java",
                    "FairScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
                ],
                "methods": [
                    "QueuePlacementRule.getQueueForApp",
                    "FairScheduler.addApplication"
                ]
            },
            "possible_fix": "In the 'getQueueForApp' method, add a check for empty groups before attempting to access the first element. For example:\n\npublic String getQueueForApp(String user) {\n    if (groups.isEmpty()) {\n        // Handle empty groups case\n        return defaultQueue;\n    }\n    return groups.get(0); // Existing logic\n}"
        },
        "possible_fix_code": {
            "QueuePlacementRule.assignAppToQueue": "    public String assignAppToQueue(String requestedQueue, String user,\n        Groups groups, Map<FSQueueType, Set<String>> configuredQueues) {\n      if (groups == null || groups.isEmpty()) {\n          // Handle empty groups case\n          return \"defaultQueue\"; // or return an appropriate default queue\n      }\n      return groups.get(0); // Existing logic\n    }"
        }
    },
    {
        "filename": "YARN-4833.json",
        "creation_time": "2016-03-17T13:22:23.000+0000",
        "bug_report": {
            "Title": "For Queue AccessControlException client retries multiple times on both RM",
            "Description": "When submitting an application to a queue with Access Control List (ACL) enabled, the client retries multiple times (up to 10) if the user does not have permission to submit the application. This behavior is triggered by an AccessControlException indicating that the user 'hdfs' lacks the necessary permissions to submit the application to the 'default' queue. The retries occur even when the user is clearly unauthorized, leading to unnecessary load on the system.",
            "StackTrace": [
                "org.apache.hadoop.security.AccessControlException: User hdfs does not have permission to submit application_1458273884145_0001 to queue default",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:380)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:291)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.java:618)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:252)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:637)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2360)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2356)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1742)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2356)"
            ],
            "RootCause": "The root cause of the issue is that the application submission process does not handle the AccessControlException properly within the retry policy. Instead of failing immediately when an AccessControlException is encountered, the system continues to retry the submission, which is unnecessary and inefficient.",
            "StepsToReproduce": [
                "1. Enable ACL on the YARN queue.",
                "2. Attempt to submit an application using a user that does not have permission to submit to the queue.",
                "3. Observe that the client retries the submission multiple times (up to 10) despite the lack of permissions."
            ],
            "ExpectedBehavior": "The system should immediately reject the application submission and not attempt to retry if the user does not have the necessary permissions.",
            "ObservedBehavior": "The client retries the application submission multiple times (up to 10) even when the user lacks permission, leading to unnecessary load and confusion.",
            "Suggestions": "Implement a check in the retry policy to handle AccessControlException specifically. If this exception is encountered, the system should not attempt to retry the submission and should instead return an appropriate error message to the client.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java",
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/impl/pb/service/ApplicationClientProtocolPBServiceImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager",
                    "org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl"
                ],
                "methods": [
                    "RMAppManager.submitApplication",
                    "ApplicationClientProtocolPBServiceImpl.submitApplication"
                ]
            },
            "possible_fix": "In the RMAppManager class, modify the submitApplication method to catch AccessControlException and handle it appropriately by not retrying the submission. Additionally, wrap the AccessControlException in a YarnException to provide clearer feedback to the client."
        },
        "possible_fix_code": {
            "RMAppManager.submitApplication": "  protected void submitApplication(\n      ApplicationSubmissionContext submissionContext, long submitTime,\n      String user) throws YarnException, AccessControlException {\n    ApplicationId applicationId = submissionContext.getApplicationId();\n\n    try {\n      RMAppImpl application =\n          createAndPopulateNewRMApp(submissionContext, submitTime, user, false);\n      Credentials credentials = null;\n      try {\n        credentials = parseCredentials(submissionContext);\n        if (UserGroupInformation.isSecurityEnabled()) {\n          this.rmContext.getDelegationTokenRenewer()\n              .addApplicationAsync(applicationId, credentials,\n                  submissionContext.getCancelTokensWhenComplete(),\n                  application.getUser());\n        } else {\n          this.rmContext.getDispatcher().getEventHandler()\n              .handle(new RMAppEvent(applicationId, RMAppEventType.START));\n        }\n      } catch (Exception e) {\n        LOG.warn(\"Unable to parse credentials.\", e);\n        assert application.getState() == RMAppState.NEW;\n        this.rmContext.getDispatcher().getEventHandler()\n            .handle(new RMAppEvent(applicationId,\n                RMAppEventType.APP_REJECTED, e.getMessage()));\n        throw RPCUtil.getRemoteException(e);\n      }\n    } catch (AccessControlException ace) {\n      LOG.warn(\"AccessControlException: User \" + user + \" does not have permission to submit application \" + applicationId);\n      throw new YarnException(\"User does not have permission to submit application: \" + applicationId, ace);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-1689.json",
        "creation_time": "2014-02-05T19:16:00.000+0000",
        "bug_report": {
            "Title": "RMAppAttempt is not killed when RMApp is at ACCEPTED",
            "Description": "The ResourceManager (RM) enters an unusable state where no jobs can run due to a NullPointerException occurring during the registration of the Application Master. This happens when the RM attempts to retrieve transferred containers for an application that is in an invalid state, leading to an InvalidStateTransitionException when trying to handle the ATTEMPT_REGISTERED event.",
            "StackTrace": [
                "2014-02-04 20:28:08,553 WARN  ipc.Server (Server.java:run(1978)) - IPC Server handler 0 on 8030, call org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.registerApplicationMaster from 172.18.145.156:40474 Call#0 Retry#0: error: java.lang.NullPointerException",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.getTransferredContainers(AbstractYarnScheduler.java:48)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.registerApplicationMaster(ApplicationMasterService.java:278)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.registerApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:90)",
                "at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:95)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)",
                "2014-02-04 20:28:08,544 ERROR rmapp.RMAppImpl (RMAppImpl.java:handle(626)) - Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: ATTEMPT_REGISTERED at KILLED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:624)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:81)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:656)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:640)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is a NullPointerException in the getTransferredContainers method of AbstractYarnScheduler, which occurs when the application is in an invalid state (KILLED) and the RM tries to handle an ATTEMPT_REGISTERED event.",
            "StepsToReproduce": [
                "1. Submit a Hive on Tez job to the ResourceManager.",
                "2. Allow the job to run until it reaches the ACCEPTED state.",
                "3. Trigger a kill request for the application.",
                "4. Observe the ResourceManager logs for the NullPointerException and InvalidStateTransitionException."
            ],
            "ExpectedBehavior": "The ResourceManager should properly handle the application state transitions and not throw exceptions when an application is killed.",
            "ObservedBehavior": "The ResourceManager throws a NullPointerException and an InvalidStateTransitionException, leading to an unusable state where no jobs can run.",
            "Suggestions": "Implement checks in the getTransferredContainers method to ensure that the application is in a valid state before attempting to retrieve transferred containers. Additionally, handle the ATTEMPT_REGISTERED event more gracefully in the RMAppImpl class.",
            "problem_location": {
                "files": [
                    "AbstractYarnScheduler.java",
                    "ApplicationMasterService.java",
                    "RMAppImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler",
                    "org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService",
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl"
                ],
                "methods": [
                    "AbstractYarnScheduler.getTransferredContainers",
                    "ApplicationMasterService.registerApplicationMaster",
                    "RMAppImpl.handle"
                ]
            },
            "possible_fix": "In AbstractYarnScheduler.getTransferredContainers, add a check to ensure that the application is not in a KILLED state before proceeding to retrieve containers. In RMAppImpl.handle, ensure that the event handling logic accounts for the current state of the application to prevent invalid transitions."
        },
        "possible_fix_code": {
            "AbstractYarnScheduler.getTransferredContainers": "  public synchronized List<Container> getTransferredContainers(\n      ApplicationAttemptId currentAttempt) {\n    ApplicationId appId = currentAttempt.getApplicationId();\n    RMApp appImpl = this.rmContext.getRMApps().get(appId);\n    // Check if the application is in a valid state before proceeding\n    if (appImpl.getState() == RMAppState.KILLED) {\n      LOG.warn(\"Application is in KILLED state, cannot retrieve transferred containers.\");\n      return new ArrayList<>();\n    }\n    List<Container> containerList = new ArrayList<Container>();\n    if (appImpl.getApplicationSubmissionContext().getUnmanagedAM()) {\n      return containerList;\n    }\n    SchedulerApplication app = applications.get(appId);\n    Collection<RMContainer> liveContainers =\n        app.getCurrentAppAttempt().getLiveContainers();\n    ContainerId amContainerId =\n        rmContext.getRMApps().get(appId).getCurrentAppAttempt()\n          .getMasterContainer().getId();\n    for (RMContainer rmContainer : liveContainers) {\n      if (!rmContainer.getContainerId().equals(amContainerId)) {\n        containerList.add(rmContainer.getContainer());\n      }\n    }\n    return containerList;\n  }",
            "RMAppImpl.handle": "  public void handle(RMAppEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId appID = event.getApplicationId();\n      LOG.debug(\"Processing event for \" + appID + \" of type \"\n          + event.getType());\n      final RMAppState oldState = getState();\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        // Handle the invalid state transition gracefully\n        if (oldState == RMAppState.KILLED) {\n          LOG.warn(\"Application is already KILLED, ignoring event.\");\n          return;\n        }\n        /* TODO fail the application on the failed transition */\n      }\n\n      if (oldState != getState()) {\n        LOG.info(appID + \" State change from \" + oldState + \" to \"\n            + getState());\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-5594.json",
        "creation_time": "2016-08-30T15:14:19.000+0000",
        "bug_report": {
            "Title": "Handle old RMDelegationToken format when recovering RM",
            "Description": "After upgrading the cluster from version 2.5.1 to 2.7.0, the ResourceManager fails to recover its state due to an InvalidProtocolBufferException caused by incompatible formats of RMDelegationToken files. The error indicates that the protocol message contains an invalid tag, which suggests that the new version of the ResourceManager is unable to parse the old format of the delegation token data. This issue arises during the recovery process when the ResourceManager attempts to read the delegation token data from the filesystem.",
            "StackTrace": [
                "com.google.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero).",
                "at com.google.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:89)",
                "at com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:108)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto.<init>(YarnServerResourceManagerRecoveryProtos.java:4680)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto.<init>(YarnServerResourceManagerRecoveryProtos.java:4644)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$1.parsePartialFrom(YarnServerResourceManagerRecoveryProtos.java:4740)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$1.parsePartialFrom(YarnServerResourceManagerRecoveryProtos.java:4735)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder.mergeFrom(YarnServerResourceManagerRecoveryProtos.java:5075)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder.mergeFrom(YarnServerResourceManagerRecoveryProtos.java:4955)",
                "at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:337)",
                "at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:267)",
                "at com.google.protobuf.AbstractMessageLite$Builder.mergeFrom(AbstractMessageLite.java:210)",
                "at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:904)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.records.RMDelegationTokenIdentifierData.readFields(RMDelegationTokenIdentifierData.java:43)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.loadRMDTSecretManagerState(FileSystemRMStateStore.java:355)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.loadState(FileSystemRMStateStore.java:199)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:587)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1007)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1048)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1044)"
            ],
            "RootCause": "The root cause of the issue is the incompatibility between the old and new formats of RMDelegationToken files after the upgrade. The new version of the ResourceManager is unable to parse the old format, leading to an InvalidProtocolBufferException.",
            "StepsToReproduce": [
                "Upgrade the Hadoop cluster from version 2.5.1 to 2.7.0.",
                "Attempt to start the ResourceManager.",
                "Observe the logs for the InvalidProtocolBufferException error."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully recover its state and start without errors, regardless of the format of the RMDelegationToken files.",
            "ObservedBehavior": "The ResourceManager fails to start and throws an InvalidProtocolBufferException due to an invalid tag in the protocol message.",
            "Suggestions": "Implement a backward compatibility check in the RMDelegationToken recovery process to handle old data formats gracefully. This may involve adding logic to detect the version of the token data and parse it accordingly.",
            "problem_location": {
                "files": [
                    "FileSystemRMStateStore.java",
                    "RMDelegationTokenIdentifierData.java",
                    "ResourceManager.java",
                    "AbstractService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore",
                    "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.RMDelegationTokenIdentifierData",
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager",
                    "org.apache.hadoop.service.AbstractService"
                ],
                "methods": [
                    "FileSystemRMStateStore.loadRMDTSecretManagerState",
                    "RMDelegationTokenIdentifierData.readFields",
                    "ResourceManager.startActiveServices",
                    "AbstractService.start"
                ]
            },
            "possible_fix": "Modify the loadRMDTSecretManagerState method to include a version check for the RMDelegationToken files. If the old format is detected, implement a parsing strategy that can handle the legacy format without throwing an exception. This may involve creating a new method to handle the old format and calling it conditionally based on the detected version."
        },
        "possible_fix_code": {
            "FileSystemRMStateStore.loadRMDTSecretManagerState": "  private void loadRMDTSecretManagerState(RMState rmState) throws Exception {\n    checkAndResumeUpdateOperation(rmDTSecretManagerRoot);\n    FileStatus[] childNodes = listStatusWithRetries(rmDTSecretManagerRoot);\n\n    for(FileStatus childNodeStatus : childNodes) {\n      assert childNodeStatus.isFile();\n      String childNodeName = childNodeStatus.getPath().getName();\n      if (checkAndRemovePartialRecordWithRetries(childNodeStatus.getPath())) {\n        continue;\n      }\n      if(childNodeName.startsWith(DELEGATION_TOKEN_SEQUENCE_NUMBER_PREFIX)) {\n        rmState.rmSecretManagerState.dtSequenceNumber =\n            Integer.parseInt(childNodeName.split(\"_\")[1]);\n        continue;\n      }\n\n      Path childNodePath = getNodePath(rmDTSecretManagerRoot, childNodeName);\n      byte[] childData = readFileWithRetries(childNodePath,\n          childNodeStatus.getLen());\n      ByteArrayInputStream is = new ByteArrayInputStream(childData);\n      try (DataInputStream fsIn = new DataInputStream(is)) {\n        if (childNodeName.startsWith(DELEGATION_KEY_PREFIX)) {\n          DelegationKey key = new DelegationKey();\n          key.readFields(fsIn);\n          rmState.rmSecretManagerState.masterKeyState.add(key);\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Loaded delegation key: keyId=\" + key.getKeyId()\n                + \", expirationDate=\" + key.getExpiryDate());\n          }\n        } else if (childNodeName.startsWith(DELEGATION_TOKEN_PREFIX)) {\n          RMDelegationTokenIdentifierData identifierData =\n              new RMDelegationTokenIdentifierData();\n          try {\n            identifierData.readFields(fsIn);\n          } catch (InvalidProtocolBufferException e) {\n            // Handle old format\n            handleOldFormat(fsIn);\n            continue;\n          }\n          RMDelegationTokenIdentifier identifier =\n              identifierData.getTokenIdentifier();\n          long renewDate = identifierData.getRenewDate();\n\n          rmState.rmSecretManagerState.delegationTokenState.put(identifier,\n            renewDate);\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Loaded RMDelegationTokenIdentifier: \" + identifier\n                + \" renewDate=\" + renewDate);\n          }\n        } else {\n          LOG.warn(\"Unknown file for recovering RMDelegationTokenSecretManager\");\n        }\n      }\n    }\n  }\n\n  private void handleOldFormat(DataInputStream fsIn) throws IOException {\n    // Implement logic to read and parse the old format of RMDelegationToken\n    // This is a placeholder for the actual implementation\n    // You would need to define how to read the old format correctly\n  }"
        }
    },
    {
        "filename": "YARN-7511.json",
        "creation_time": "2017-11-16T11:41:43.000+0000",
        "bug_report": {
            "Title": "NPE in ContainerLocalizer when localization failed for running container",
            "Description": "A NullPointerException (NPE) occurs in the ContainerLocalizer when a localization failure happens for a running container. The issue arises when the ResourceLocalizationFailedWhileRunningTransition attempts to process a null LocalResourceRequest, leading to an attempt to remove a null entry from the pendingResources map in the ResourceSet class.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at java.util.concurrent.ConcurrentHashMap.replaceNode(ConcurrentHashMap.java:1106)",
                "at java.util.concurrent.ConcurrentHashMap.remove(ConcurrentHashMap.java:1097)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet.resourceLocalizationFailed(ResourceSet.java:151)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileRunningTransition.transition(ContainerImpl.java:821)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileRunningTransition.transition(ContainerImpl.java:813)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1335)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:95)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1372)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1365)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:834)"
            ],
            "RootCause": "The root cause of the NPE is that a null LocalResourceRequest is being passed to the resourceLocalizationFailed method, which attempts to remove it from the pendingResources map without checking for null.",
            "StepsToReproduce": [
                "1. Start a container and call ContainerManagerImpl#localize for this container.",
                "2. Simulate a localization failure in ResourceLocalizationService$LocalizerRunner#run, which sends out a ContainerResourceFailedEvent with a null LocalResourceRequest.",
                "3. Observe the NPE when ResourceLocalizationFailedWhileRunningTransition#transition attempts to call container.resourceSet.resourceLocalizationFailed(null)."
            ],
            "ExpectedBehavior": "The system should handle localization failures gracefully without throwing a NullPointerException, ensuring that null requests are not processed.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to remove a null LocalResourceRequest from the pendingResources map.",
            "Suggestions": "Implement a null check for the LocalResourceRequest before attempting to remove it from the pendingResources map in the resourceLocalizationFailed method.",
            "problem_location": {
                "files": [
                    "ResourceSet.java",
                    "ContainerImpl.java",
                    "StateMachineFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory"
                ],
                "methods": [
                    "ResourceSet.resourceLocalizationFailed",
                    "ContainerImpl$ResourceLocalizationFailedWhileRunningTransition.transition",
                    "StateMachineFactory.doTransition"
                ]
            },
            "possible_fix": "In ResourceSet.java, modify the resourceLocalizationFailed method to include a null check for the request parameter:\n\npublic void resourceLocalizationFailed(LocalResourceRequest request) {\n    if (request != null) {\n        pendingResources.remove(request);\n    }\n    resourcesFailedToBeLocalized.add(request);\n}"
        },
        "possible_fix_code": {
            "ResourceSet.resourceLocalizationFailed": "  public void resourceLocalizationFailed(LocalResourceRequest request) {\n    if (request != null) {\n        pendingResources.remove(request);\n    }\n    resourcesFailedToBeLocalized.add(request);\n  }"
        }
    },
    {
        "filename": "YARN-3790.json",
        "creation_time": "2015-06-10T04:53:40.000+0000",
        "bug_report": {
            "Title": "Stale data in usedResource metrics for FS scheduler after container recovery",
            "Description": "The test 'testSchedulerRecovery' in the 'TestWorkPreservingRMRestart' class is failing due to an assertion error where the expected used resource value is 6144, but the actual value is 8192. This discrepancy suggests that the metrics for the FS scheduler are not being updated correctly after recovering the container, leading to stale data being reported.",
            "StackTrace": [
                "java.lang.AssertionError: expected:<6144> but was:<8192>",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.failNotEquals(Assert.java:743)",
                "at org.junit.Assert.assertEquals(Assert.java:118)",
                "at org.junit.Assert.assertEquals(Assert.java:555)",
                "at org.junit.Assert.assertEquals(Assert.java:542)",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart.assertMetrics(TestWorkPreservingRMRestart.java:853)",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart.checkFSQueue(TestWorkPreservingRMRestart.java:342)",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart.testSchedulerRecovery(TestWorkPreservingRMRestart.java:241)"
            ],
            "RootCause": "The root cause appears to be that the metrics for the FS scheduler are not being refreshed or recalibrated correctly after the container recovery process, leading to stale values being reported.",
            "StepsToReproduce": [
                "Run the test suite for 'TestWorkPreservingRMRestart'.",
                "Ensure that the test 'testSchedulerRecovery' is executed.",
                "Observe the assertion failure indicating stale metrics."
            ],
            "ExpectedBehavior": "The usedResource metrics for the FS scheduler should accurately reflect the current resource usage after recovering the container.",
            "ObservedBehavior": "The usedResource metrics report stale data, resulting in an assertion failure during the test.",
            "Suggestions": "Investigate the container recovery logic to ensure that metrics are updated correctly. Consider adding a refresh mechanism for the metrics after recovery.",
            "problem_location": {
                "files": [
                    "TestWorkPreservingRMRestart.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"
                ],
                "methods": [
                    "assertMetrics",
                    "checkFSQueue",
                    "testSchedulerRecovery"
                ]
            },
            "possible_fix": "Review the container recovery implementation to ensure that it triggers an update of the usedResource metrics. This may involve adding a call to refresh the metrics after the recovery process is complete."
        },
        "possible_fix_code": {
            "testSchedulerRecovery": "public void testSchedulerRecovery() throws Exception {\n    // Setup the necessary environment and resources\n    setup();\n\n    // Simulate container recovery\n    recoverContainer();\n\n    // Refresh metrics after recovery to ensure they are up-to-date\n    refreshMetrics();\n\n    // Perform the assertion to check the used resource metrics\n    assertMetrics();\n}"
        }
    },
    {
        "filename": "YARN-6068.json",
        "creation_time": "2017-01-07T03:16:07.000+0000",
        "bug_report": {
            "Title": "Log aggregation fails after NM restart despite recovery attempts",
            "Description": "The log aggregation process fails to complete after a NodeManager (NM) restart, leading to an InvalidStateTransitionException. This occurs when the application attempts to handle an event related to log handling while in an invalid state, specifically when the application is still in the RUNNING state. The logs indicate that the application cannot handle the APPLICATION_LOG_HANDLING_FAILED event, resulting in the abortion of log aggregation.",
            "StackTrace": [
                "2017-01-05 19:16:36,352 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:abortLogAggregation(527)) - Aborting log aggregation for application_1483640789847_0001",
                "2017-01-05 19:16:36,352 WARN  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:run(399)) - Aggregation did not complete for application application_1483640789847_0001",
                "2017-01-05 19:16:36,353 WARN  application.ApplicationImpl (ApplicationImpl.java:handle(461)) - Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FAILED at RUNNING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:459)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:64)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:1084)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:1076)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2017-01-05 19:16:36,355 INFO  application.ApplicationImpl (ApplicationImpl.java:handle(464)) - Application application_1483640789847_0001 transitioned from RUNNING to null"
            ],
            "RootCause": "The application is attempting to handle an APPLICATION_LOG_HANDLING_FAILED event while it is still in the RUNNING state, which is not a valid state for this event, leading to an InvalidStateTransitionException.",
            "StepsToReproduce": [
                "1. Start an application in the YARN cluster.",
                "2. Restart the NodeManager while the application is still running.",
                "3. Observe the logs for the application to see if log aggregation completes successfully."
            ],
            "ExpectedBehavior": "The log aggregation should complete successfully even after a NodeManager restart, without throwing any exceptions.",
            "ObservedBehavior": "Log aggregation fails with an InvalidStateTransitionException, and the application cannot handle the log handling event.",
            "Suggestions": "Review the state transition logic in the ApplicationImpl class to ensure that it can handle log handling events appropriately even after a NodeManager restart. Consider adding state checks or modifying the event handling logic to accommodate the application's state.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java",
                    "hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/state/StateMachineFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory"
                ],
                "methods": [
                    "ApplicationImpl.handle",
                    "StateMachineFactory.doTransition"
                ]
            },
            "possible_fix": "In the ApplicationImpl.handle method, add a check to ensure that the application can handle the APPLICATION_LOG_HANDLING_FAILED event even when in the RUNNING state, or transition to a state that allows for this event to be processed."
        },
        "possible_fix_code": {
            "ApplicationImpl.handle": "  public void handle(ApplicationEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId applicationID = event.getApplicationID();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\n            \"Processing \" + applicationID + \" of type \" + event.getType());\n      }\n      ApplicationState oldState = stateMachine.getCurrentState();\n      ApplicationState newState = null;\n      try {\n        // Check if the event is APPLICATION_LOG_HANDLING_FAILED\n        if (event.getType() == ApplicationEventType.APPLICATION_LOG_HANDLING_FAILED && oldState == ApplicationState.RUNNING) {\n          // Handle the log handling failure appropriately\n          LOG.warn(\"Handling APPLICATION_LOG_HANDLING_FAILED event in RUNNING state\");\n          // You may want to add logic here to transition to a state that can handle this event\n          // For now, we will just log and continue\n        }\n        // queue event requesting init of the same app\n        newState = stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitionException e) {\n        LOG.warn(\"Can't handle this event at current state\", e);\n      }\n      if (newState != null && oldState != newState) {\n        LOG.info(\"Application \" + applicationID + \" transitioned from \"\n            + oldState + \" to \" + newState);\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-903.json",
        "creation_time": "2013-07-07T08:35:30.000+0000",
        "bug_report": {
            "Title": "DistributedShell throwing Errors in logs after successful completion",
            "Description": "The DistributedShell application runs successfully but logs errors related to container management after completion. Specifically, the NodeManager logs indicate that containers are not recognized by the NodeManager, leading to 'Container is not handled by this NodeManager' errors. This issue arises when the ApplicationMaster attempts to stop containers that were not started by the current application attempt.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnException: Container container_1373184544832_0001_01_000002 is not handled by this NodeManager",
                "at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:45)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeGetAndStopContainerRequest(ContainerManagerImpl.java:614)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.stopContainer(ContainerManagerImpl.java:538)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagementProtocolPBServiceImpl.stopContainer(ContainerManagementProtocolPBServiceImpl.java:88)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:605)"
            ],
            "RootCause": "The root cause of the issue is that the ApplicationMaster is attempting to stop containers that were not started by the current application attempt. This is due to a mismatch in the application attempt ID when the stop request is made.",
            "StepsToReproduce": [
                "Run the DistributedShell application using the ApplicationMaster.",
                "Observe the logs for errors related to container management after the application completes."
            ],
            "ExpectedBehavior": "The application should complete successfully without logging errors related to container management.",
            "ObservedBehavior": "Errors are logged indicating that containers are not handled by the NodeManager, despite the application completing successfully.",
            "Suggestions": "Ensure that the ApplicationMaster only attempts to stop containers that it has started. This may involve checking the application attempt ID before issuing stop requests.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java",
                    "hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/impl/pb/service/ContainerManagementProtocolPBServiceImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl",
                    "org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagementProtocolPBServiceImpl"
                ],
                "methods": [
                    "ContainerManagerImpl.authorizeGetAndStopContainerRequest",
                    "ContainerManagerImpl.stopContainer"
                ]
            },
            "possible_fix": "Modify the 'stopContainer' method in 'ContainerManagerImpl' to include a check for the application attempt ID before attempting to stop a container. This will prevent the ApplicationMaster from trying to stop containers that it did not start."
        },
        "possible_fix_code": {
            "ContainerManagerImpl.stopContainer": "  public StopContainerResponse stopContainer(StopContainerRequest request)\n      throws YarnException, IOException {\n\n    ContainerId containerID = request.getContainerId();\n    String containerIDStr = containerID.toString();\n    Container container = this.context.getContainers().get(containerID);\n    LOG.info(\"Getting container-status for \" + containerIDStr);\n\n    // Check if the container was started by the current application attempt\n    NMTokenIdentifier identifier = selectNMTokenIdentifier(getRemoteUgi());\n    if (!identifier.getApplicationAttemptId().equals(containerID.getApplicationAttemptId())) {\n        throw RPCUtil.getRemoteException(\"Container \" + containerIDStr + \" is not handled by this NodeManager\");\n    }\n\n    authorizeGetAndStopContainerRequest(containerID, container, true);\n\n    StopContainerResponse response =\n        recordFactory.newRecordInstance(StopContainerResponse.class);\n\n    dispatcher.getEventHandler().handle(\n      new ContainerKillEvent(containerID,\n        \"Container killed by the ApplicationMaster.\"));\n\n    NMAuditLogger.logSuccess(container.getUser(),\n      AuditConstants.STOP_CONTAINER, \"ContainerManageImpl\", containerID\n        .getApplicationAttemptId().getApplicationId(), containerID);\n\n    // TODO: Move this code to appropriate place once kill_container is\n    // implemented.\n    nodeStatusUpdater.sendOutofBandHeartBeat();\n\n    return response;\n  }"
        }
    },
    {
        "filename": "YARN-8236.json",
        "creation_time": "2018-04-29T16:28:11.000+0000",
        "bug_report": {
            "Title": "Invalid Kerberos Principal File Name Causes NPE in Native Service",
            "Description": "A NullPointerException (NPE) occurs in the ServiceClient class when attempting to add a keytab resource for a service that has an invalid or empty Kerberos principal name. The issue arises in the addKeytabResourceIfSecure method, which is called during the application submission process. If the principal name is not set, the method logs a warning and returns without adding the keytab, leading to a subsequent NPE when the application attempts to use the keytab resource.",
            "StackTrace": [
                "2018-04-29 16:22:54,266 WARN webapp.GenericExceptionHandler (GenericExceptionHandler.java:toResponse(98)) - INTERNAL_SERVER_ERROR",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.addKeytabResourceIfSecure(ServiceClient.java:994)",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.submitApp(ServiceClient.java:685)",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.actionCreate(ServiceClient.java:269)"
            ],
            "RootCause": "The root cause of the NPE is that the Kerberos principal name is either null or empty when the addKeytabResourceIfSecure method is called, leading to an attempt to access a method on a null object.",
            "StepsToReproduce": [
                "1. Create a service configuration without a valid Kerberos principal name.",
                "2. Attempt to submit the application using the ServiceClient.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The application should handle the absence of a Kerberos principal gracefully, either by providing a default behavior or by throwing a meaningful exception.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the application submission to fail.",
            "Suggestions": "Implement a check in the addKeytabResourceIfSecure method to ensure that the Kerberos principal name is valid before proceeding with keytab resource addition. If the principal name is invalid, throw a specific exception or handle the case appropriately.",
            "problem_location": {
                "files": [
                    "ServiceClient.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.service.client.ServiceClient"
                ],
                "methods": [
                    "ServiceClient.addKeytabResourceIfSecure",
                    "ServiceClient.submitApp",
                    "ServiceClient.actionCreate"
                ]
            },
            "possible_fix": "In the addKeytabResourceIfSecure method, add a check for the principal name before proceeding:\n\nif (StringUtils.isEmpty(principalName)) {\n    throw new IllegalArgumentException(\"Kerberos principal name cannot be null or empty for service: \" + service.getName());\n}"
        },
        "possible_fix_code": {
            "addKeytabResourceIfSecure": "  private void addKeytabResourceIfSecure(SliderFileSystem fileSystem,\n      Map<String, LocalResource> localResource, Service service)\n      throws IOException, YarnException {\n    if (!UserGroupInformation.isSecurityEnabled()) {\n      return;\n    }\n    String principalName = service.getKerberosPrincipal().getPrincipalName();\n    if (StringUtils.isEmpty(principalName)) {\n      LOG.warn(\"No Kerberos principal name specified for \" + service.getName());\n      throw new IllegalArgumentException(\"Kerberos principal name cannot be null or empty for service: \" + service.getName());\n    }\n    if(StringUtils.isEmpty(service.getKerberosPrincipal().getKeytab())) {\n      LOG.warn(\"No Kerberos keytab specified for \" + service.getName());\n      return;\n    }\n\n    URI keytabURI;\n    try {\n      keytabURI = new URI(service.getKerberosPrincipal().getKeytab());\n    } catch (URISyntaxException e) {\n      throw new YarnException(e);\n    }\n\n    switch (keytabURI.getScheme()) {\n    case \"hdfs\":\n      Path keytabOnhdfs = new Path(keytabURI);\n      if (!fileSystem.getFileSystem().exists(keytabOnhdfs)) {\n        LOG.warn(service.getName() + \"'s keytab (principalName = \" +\n            principalName + \") doesn't exist at: \" + keytabOnhdfs);\n        return;\n      }\n      LocalResource keytabRes =\n          fileSystem.createAmResource(keytabOnhdfs, LocalResourceType.FILE);\n      localResource.put(String.format(YarnServiceConstants.KEYTAB_LOCATION,\n          service.getName()), keytabRes);\n      LOG.debug(\"Adding \" + service.getName() + \"'s keytab for \" +\n          \"localization, uri = \" + keytabOnhdfs);\n      break;\n    case \"file\":\n      LOG.debug(\"Using a keytab from localhost: \" + keytabURI);\n      break;\n    default:\n      LOG.warn(\"Unsupported URI scheme \" + keytabURI);\n      break;\n    }\n  }"
        }
    },
    {
        "filename": "YARN-2857.json",
        "creation_time": "2014-10-24T20:47:51.000+0000",
        "bug_report": {
            "Title": "ConcurrentModificationException in ContainerLogAppender",
            "Description": "The application encounters a ConcurrentModificationException when attempting to close the ContainerLogAppender. This occurs during the shutdown process of the Hadoop job, specifically when the logging system is being cleaned up. The exception is triggered because the logging events are being modified while they are being iterated over, leading to a failure in the shutdown hook.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966)",
                "at java.util.LinkedList$ListItr.next(LinkedList.java:888)",
                "at org.apache.hadoop.yarn.ContainerLogAppender.close(ContainerLogAppender.java:94)",
                "at org.apache.log4j.helpers.AppenderAttachableImpl.removeAllAppenders(AppenderAttachableImpl.java:141)",
                "at org.apache.log4j.Category.removeAllAppenders(Category.java:891)",
                "at org.apache.log4j.Hierarchy.shutdown(Hierarchy.java:471)",
                "at org.apache.log4j.LogManager.shutdown(LogManager.java:267)",
                "at org.apache.hadoop.mapred.TaskLog.syncLogsShutdown(TaskLog.java:286)",
                "at org.apache.hadoop.mapred.TaskLog$2.run(TaskLog.java:339)",
                "at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)"
            ],
            "RootCause": "The root cause of the ConcurrentModificationException is that the logging events are being modified (i.e., added or removed) while they are being iterated over in the close method of ContainerLogAppender.",
            "StepsToReproduce": [
                "Submit a job using Oozie that runs a Pig script.",
                "Ensure that the job generates log events.",
                "Allow the job to complete and trigger the shutdown process."
            ],
            "ExpectedBehavior": "The logging system should shut down gracefully without throwing any exceptions.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown during the shutdown process of the logging system.",
            "Suggestions": "To mitigate this issue, consider using a thread-safe collection for logging events or synchronize access to the collection to prevent concurrent modifications during iteration.",
            "problem_location": {
                "files": [
                    "ContainerLogAppender.java",
                    "AppenderAttachableImpl.java",
                    "Category.java",
                    "Hierarchy.java",
                    "LogManager.java",
                    "TaskLog.java",
                    "ShutdownHookManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.ContainerLogAppender",
                    "org.apache.log4j.helpers.AppenderAttachableImpl",
                    "org.apache.log4j.Category",
                    "org.apache.log4j.Hierarchy",
                    "org.apache.log4j.LogManager",
                    "org.apache.hadoop.mapred.TaskLog",
                    "org.apache.hadoop.util.ShutdownHookManager"
                ],
                "methods": [
                    "ContainerLogAppender.close",
                    "AppenderAttachableImpl.removeAllAppenders",
                    "Category.removeAllAppenders",
                    "Hierarchy.shutdown",
                    "LogManager.shutdown",
                    "TaskLog.syncLogsShutdown",
                    "ShutdownHookManager.run"
                ]
            },
            "possible_fix": "Modify the close method in ContainerLogAppender to use a thread-safe collection or synchronize the iteration over the logging events. For example, replace the LinkedList with a CopyOnWriteArrayList or wrap the iteration in a synchronized block."
        },
        "possible_fix_code": {
            "ContainerLogAppender.close": "  public synchronized void close() {\n    if (tail != null) {\n      // Use a thread-safe collection to avoid ConcurrentModificationException\n      List<LoggingEvent> eventsToProcess = new ArrayList<>(tail);\n      for (LoggingEvent event : eventsToProcess) {\n        super.append(event);\n      }\n    }\n    super.close();\n  }"
        }
    },
    {
        "filename": "YARN-2416.json",
        "creation_time": "2014-08-13T22:36:31.000+0000",
        "bug_report": {
            "Title": "InvalidStateTransitonException in ResourceManager if AMLauncher does not receive response for startContainers() call in time",
            "Description": "The ResourceManager (RM) encounters an InvalidStateTransitonException when the Application Master (AM) does not receive a timely response from the startContainers() call. This delay causes the RMAppAttempt to remain in the ALLOCATED state, leading to exceptions when the AM sends REGISTERED, STATUS_UPDATE, or CONTAINER_ALLOCATED events. The RM is unable to process these events due to the invalid state transition, resulting in application failures.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: REGISTERED at ALLOCATED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:652)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:752)",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: STATUS_UPDATE at ALLOCATED",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: CONTAINER_ALLOCATED at ALLOCATED"
            ],
            "RootCause": "The root cause of the issue is the delayed response from the startContainers() call, which prevents the RMAppAttempt from transitioning from the ALLOCATED state to the LAUNCHED state. This results in the RM being unable to handle subsequent events from the AM.",
            "StepsToReproduce": [
                "1. Initiate a request to launch a container using AMLauncher.",
                "2. Simulate a delay in the response from the startContainers() call.",
                "3. Observe the state of the RMAppAttempt and the exceptions thrown when the AM sends events."
            ],
            "ExpectedBehavior": "The RMAppAttempt should transition from ALLOCATED to LAUNCHED upon receiving a response from the startContainers() call, allowing it to handle subsequent events without throwing exceptions.",
            "ObservedBehavior": "The RMAppAttempt remains in the ALLOCATED state, and the ResourceManager throws InvalidStateTransitonException for REGISTERED, STATUS_UPDATE, and CONTAINER_ALLOCATED events.",
            "Suggestions": "Implement a timeout mechanism for the startContainers() call to handle cases where the response is delayed. Additionally, consider adding logic to manage state transitions more gracefully when delays occur.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/state/StateMachineFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory"
                ],
                "methods": [
                    "RMAppAttemptImpl.handle",
                    "StateMachineFactory.doTransition"
                ]
            },
            "possible_fix": "Add a timeout mechanism in the RMAppAttemptImpl.handle method to transition to a failed state if the startContainers() response is not received in a timely manner. This could involve modifying the state machine to handle delayed responses more effectively."
        },
        "possible_fix_code": {
            "RMAppAttemptImpl.handle": "  public void handle(RMAppAttemptEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationAttemptId appAttemptID = event.getApplicationAttemptId();\n      LOG.debug(\"Processing event for \" + appAttemptID + \" of type \" + event.getType());\n      final RMAppAttemptState oldState = getAppAttemptState();\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n        // Implementing a timeout mechanism to handle delayed responses\n        if (oldState == RMAppAttemptState.ALLOCATED) {\n          // Transition to FAILED state if the response is delayed\n          LOG.warn(\"Transitioning to FAILED state due to delayed response from startContainers()\");\n          this.stateMachine.doTransition(RMAppAttemptEventType.FAILED, event);\n        }\n      }\n\n      if (oldState != getAppAttemptState()) {\n        LOG.info(appAttemptID + \" State change from \" + oldState + \" to \" + getAppAttemptState());\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-345.json",
        "creation_time": "2013-01-17T12:57:46.000+0000",
        "bug_report": {
            "Title": "Many InvalidStateTransitonException errors for ApplicationImpl in Node Manager",
            "Description": "The Node Manager is encountering multiple instances of InvalidStateTransitonException when handling application events. This occurs when the application is in a state that does not allow certain events to be processed, such as FINISH_APPLICATION or APPLICATION_CONTAINER_FINISHED. The exceptions indicate that the application is trying to transition to a state that is not valid based on its current state, leading to warnings and potential application mismanagement.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: FINISH_APPLICATION at FINISHED",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: FINISH_APPLICATION at APPLICATION_RESOURCES_CLEANINGUP",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: FINISH_APPLICATION at FINISHING_CONTAINERS_WAIT",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_CONTAINER_FINISHED at FINISHED",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: INIT_CONTAINER at FINISHED"
            ],
            "RootCause": "The root cause of the issue is that the application is attempting to process events that are not valid for its current state. The state machine does not allow transitions from certain states (e.g., FINISHED) to others (e.g., FINISH_APPLICATION), resulting in InvalidStateTransitonException.",
            "StepsToReproduce": [
                "Submit an application to the Node Manager.",
                "Monitor the application state transitions.",
                "Trigger events that should lead to FINISH_APPLICATION or similar states while the application is in a state like FINISHED."
            ],
            "ExpectedBehavior": "The application should handle events appropriately based on its current state, transitioning only to valid states without throwing exceptions.",
            "ObservedBehavior": "The application throws InvalidStateTransitonException when trying to process events that are not valid for its current state, leading to warnings and potential application mismanagement.",
            "Suggestions": "Review the state transition logic in the ApplicationImpl class to ensure that events are only processed when they are valid for the current state. Implement checks to prevent invalid transitions and log appropriate warnings or errors.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java",
                    "hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/state/StateMachineFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory"
                ],
                "methods": [
                    "ApplicationImpl.handle",
                    "StateMachineFactory.doTransition"
                ]
            },
            "possible_fix": "In the ApplicationImpl.handle method, add a check to verify if the event type is valid for the current state before attempting to transition. For example:\n\nif (!isValidTransition(oldState, event.getType())) {\n    LOG.warn(\"Invalid event: \" + event.getType() + \" for state: \" + oldState);\n    return;\n}\n\nThis will prevent the application from attempting to process invalid events and throwing exceptions."
        },
        "possible_fix_code": {
            "ApplicationImpl.handle": "  public void handle(ApplicationEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId applicationID = event.getApplicationID();\n      LOG.debug(\"Processing \" + applicationID + \" of type \" + event.getType());\n\n      ApplicationState oldState = stateMachine.getCurrentState();\n      ApplicationState newState = null;\n\n      // Check if the event type is valid for the current state\n      if (!isValidTransition(oldState, event.getType())) {\n          LOG.warn(\"Invalid event: \" + event.getType() + \" for state: \" + oldState);\n          return;\n      }\n\n      try {\n        // queue event requesting init of the same app\n        newState = stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.warn(\"Can't handle this event at current state\", e);\n      }\n      if (oldState != newState) {\n        LOG.info(\"Application \" + applicationID + \" transitioned from \"\n            + oldState + \" to \" + newState);\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-3894.json",
        "creation_time": "2015-07-08T07:00:51.000+0000",
        "bug_report": {
            "Title": "RM startup should fail for wrong CS xml NodeLabel capacity configuration",
            "Description": "The ResourceManager (RM) fails to prevent startup when there is a mismatch in NodeLabel capacity configuration. Specifically, during the initialization of the Capacity Scheduler, if the NodeLabel capacities are incorrectly configured, the RM should not start. However, it currently allows the startup process to continue, leading to an IllegalArgumentException when the queues are reinitialized with invalid capacities.",
            "StackTrace": [
                "java.io.IOException: Failed to re-init queues",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:383)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshQueues(AdminService.java:376)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:605)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:314)",
                "at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:824)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:420)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)",
                "Caused by: java.lang.IllegalArgumentException: Illegal capacity of 0.5 for children of queue root for label=node2",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.setChildQueues(ParentQueue.java:159)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.parseQueue(CapacityScheduler.java:639)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitializeQueues(CapacityScheduler.java:503)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:379)"
            ],
            "RootCause": "The root cause of the issue is that the Capacity Scheduler does not validate NodeLabel capacities during the initialization phase. When the NodeLabel capacities are incorrectly configured, the system allows the ResourceManager to start, which leads to an IllegalArgumentException when the queues are reinitialized.",
            "StepsToReproduce": [
                "Configure RM with capacity scheduler",
                "Add one or two node labels using rmadmin",
                "Configure capacity XML with node labels but with incorrect capacity configuration for the already added labels",
                "Restart both ResourceManager instances",
                "Check if the service initializes with the capacity scheduler and if the node label list is populated"
            ],
            "ExpectedBehavior": "The ResourceManager should not start if there is a mismatch in NodeLabel capacity configuration.",
            "ObservedBehavior": "The ResourceManager starts and throws an IllegalArgumentException during the reinitialization of queues due to invalid NodeLabel capacities.",
            "Suggestions": "Implement validation checks for NodeLabel capacities during the initialization of the Capacity Scheduler to prevent the ResourceManager from starting with invalid configurations.",
            "problem_location": {
                "files": [
                    "CapacityScheduler.java",
                    "AdminService.java",
                    "ParentQueue.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler",
                    "org.apache.hadoop.yarn.server.resourcemanager.AdminService",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"
                ],
                "methods": [
                    "CapacityScheduler.reinitialize",
                    "AdminService.refreshQueues",
                    "ParentQueue.setChildQueues"
                ]
            },
            "possible_fix": "Add validation logic in the 'reinitializeQueues' method of the CapacityScheduler class to check for NodeLabel capacity mismatches before allowing the ResourceManager to start. This could involve checking if the sum of child queue capacities matches the parent queue's capacity for each NodeLabel."
        },
        "possible_fix_code": {
            "CapacityScheduler.reinitializeQueues": "  private void reinitializeQueues(CapacitySchedulerConfiguration conf) \n  throws IOException {\n    // Parse new queues\n    Map<String, CSQueue> newQueues = new HashMap<String, CSQueue>();\n    CSQueue newRoot = \n        parseQueue(this, conf, null, CapacitySchedulerConfiguration.ROOT, \n            newQueues, queues, noop); \n    \n    // Ensure all existing queues are still present\n    validateExistingQueues(queues, newQueues);\n\n    // Validate NodeLabel capacities before proceeding\n    for (String nodeLabel : labelManager.getClusterNodeLabelNames()) {\n      float capacityByLabel = newRoot.getQueueCapacities().getCapacity(nodeLabel);\n      float sum = 0;\n      for (CSQueue queue : newQueues.values()) {\n        sum += queue.getQueueCapacities().getCapacity(nodeLabel);\n      }\n      if ((capacityByLabel > 0 && Math.abs(1.0f - sum) > PRECISION) || \n          (capacityByLabel == 0 && sum > 0)) {\n        throw new IllegalArgumentException(\"Illegal capacity of \" + sum + \n            \" for children of queue root for label=\" + nodeLabel);\n      }\n    }\n\n    // Add new queues\n    addNewQueues(queues, newQueues);\n    \n    // Re-configure queues\n    root.reinitialize(newRoot, clusterResource);\n    initializeQueueMappings();\n\n    // Re-calculate headroom for active applications\n    root.updateClusterResource(clusterResource, new ResourceLimits(\n        clusterResource));\n\n    labelManager.reinitializeQueueLabels(getQueueToLabels());\n    setQueueAcls(authorizer, queues);\n  }"
        }
    },
    {
        "filename": "YARN-4347.json",
        "creation_time": "2015-11-11T22:32:59.000+0000",
        "bug_report": {
            "Title": "Resource manager fails with Null pointer exception",
            "Description": "The ResourceManager encounters a NullPointerException when attempting to recover the state of finished applications. This occurs during the execution of the `addApplicationAttempt` method in the `CapacityScheduler` class, where it tries to access an application that is not present in the scheduler's application map, leading to a failure in loading or recovering the application state.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:746)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1155)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:116)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1037)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1001)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:755)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recoverAppAttempts(RMAppImpl.java:839)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.access$1900(RMAppImpl.java:102)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:854)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:844)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:719)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:313)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:411)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1219)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:593)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1026)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1067)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1063)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)"
            ],
            "RootCause": "The root cause of the NullPointerException is that the `addApplicationAttempt` method in the `CapacityScheduler` class is trying to access an application that does not exist in the `applications` map, which results in a null reference when attempting to retrieve the application.",
            "StepsToReproduce": [
                "Start the ResourceManager.",
                "Attempt to recover a finished application that has not been properly registered in the scheduler.",
                "Observe the logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully recover the state of finished applications without throwing a NullPointerException.",
            "ObservedBehavior": "The ResourceManager throws a NullPointerException when trying to recover the state of finished applications, leading to a failure in the recovery process.",
            "Suggestions": "Implement a check in the `addApplicationAttempt` method to ensure that the application exists in the `applications` map before attempting to access it. If the application is not found, log a warning and handle the situation gracefully.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler",
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl"
                ],
                "methods": [
                    "CapacityScheduler.addApplicationAttempt",
                    "RMAppImpl.recoverAppAttempts"
                ]
            },
            "possible_fix": "In the `addApplicationAttempt` method of `CapacityScheduler`, add a null check for the `application` object before proceeding with the attempt addition. For example:\n\n```java\nif (application == null) {\n    LOG.warn(\"Application \" + applicationAttemptId.getApplicationId() + \" cannot be found in scheduler.\");\n    return;\n}\n```"
        },
        "possible_fix_code": {
            "CapacityScheduler.addApplicationAttempt": "  private synchronized void addApplicationAttempt(\n      ApplicationAttemptId applicationAttemptId,\n      boolean transferStateFromPreviousAttempt,\n      boolean isAttemptRecovering) {\n    SchedulerApplication<FiCaSchedulerApp> application =\n        applications.get(applicationAttemptId.getApplicationId());\n    if (application == null) {\n      LOG.warn(\"Application \" + applicationAttemptId.getApplicationId() +\n          \" cannot be found in scheduler.\");\n      return;\n    }\n    CSQueue queue = (CSQueue) application.getQueue();\n\n    FiCaSchedulerApp attempt = new FiCaSchedulerApp(applicationAttemptId,\n        application.getUser(), queue, queue.getActiveUsersManager(), rmContext,\n        application.getPriority());\n    if (transferStateFromPreviousAttempt) {\n      attempt.transferStateFromPreviousAttempt(\n          application.getCurrentAppAttempt());\n    }\n    application.setCurrentAppAttempt(attempt);\n\n    // Update attempt priority to the latest to avoid race condition i.e\n    // SchedulerApplicationAttempt is created with old priority but it is not\n    // set to SchedulerApplication#setCurrentAppAttempt.\n    // Scenario would occur is\n    // 1. SchdulerApplicationAttempt is created with old priority.\n    // 2. updateApplicationPriority() updates SchedulerApplication. Since\n    // currentAttempt is null, it just return.\n    // 3. ScheduelerApplcationAttempt is set in\n    // SchedulerApplication#setCurrentAppAttempt.\n    attempt.setPriority(application.getPriority());\n\n    queue.submitApplicationAttempt(attempt, application.getUser());\n    LOG.info(\"Added Application Attempt \" + applicationAttemptId\n        + \" to scheduler from user \" + application.getUser() + \" in queue \"\n        + queue.getQueueName());\n    if (isAttemptRecovering) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(applicationAttemptId\n            + \" is recovering. Skipping notifying ATTEMPT_ADDED\");\n      }\n    } else {\n      rmContext.getDispatcher().getEventHandler().handle(\n        new RMAppAttemptEvent(applicationAttemptId,\n            RMAppAttemptEventType.ATTEMPT_ADDED));\n    }\n  }"
        }
    },
    {
        "filename": "YARN-1692.json",
        "creation_time": "2014-02-07T02:01:17.000+0000",
        "bug_report": {
            "Title": "ConcurrentModificationException in fair scheduler AppSchedulable",
            "Description": "A ConcurrentModificationException is thrown in the fair scheduler when the updateDemand method of AppSchedulable is called. This occurs because the method iterates over a map returned by getResourceRequests without proper synchronization, leading to concurrent modifications while iterating.",
            "StackTrace": [
                "2014-02-07 01:40:01,978 ERROR org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Exception in fair scheduler UpdateThread",
                "java.util.ConcurrentModificationException",
                "at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)",
                "at java.util.HashMap$ValueIterator.next(HashMap.java:954)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.updateDemand(AppSchedulable.java:85)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.updateDemand(FSLeafQueue.java:125)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.updateDemand(FSParentQueue.java:82)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.update(FairScheduler.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$UpdateThread.run(FairScheduler.java:195)",
                "at java.lang.Thread.run(Thread.java:724)"
            ],
            "RootCause": "The root cause of the ConcurrentModificationException is the lack of synchronization when iterating over the map returned by app.getResourceRequests(p).values() in the updateDemand method of AppSchedulable.",
            "StepsToReproduce": [
                "1. Start the fair scheduler in a multi-threaded environment.",
                "2. Trigger multiple updates to the resource requests concurrently.",
                "3. Observe the logs for ConcurrentModificationException."
            ],
            "ExpectedBehavior": "The fair scheduler should update the demand without throwing a ConcurrentModificationException.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown during the execution of the updateDemand method.",
            "Suggestions": "Implement proper synchronization mechanisms (e.g., using synchronized blocks or concurrent collections) when accessing the resource requests map to prevent concurrent modifications.",
            "problem_location": {
                "files": [
                    "AppSchedulable.java",
                    "FSLeafQueue.java",
                    "FSParentQueue.java",
                    "FairScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
                ],
                "methods": [
                    "AppSchedulable.updateDemand",
                    "FSLeafQueue.updateDemand",
                    "FSParentQueue.updateDemand",
                    "FairScheduler.update"
                ]
            },
            "possible_fix": "To fix the issue, modify the updateDemand method in AppSchedulable to use a synchronized block when iterating over the resource requests. For example:\n\npublic void updateDemand() {\n    demand = Resources.createResource(0);\n    Resources.addTo(demand, app.getCurrentConsumption());\n\n    synchronized (app.getResourceRequests()) {\n        for (Priority p : app.getPriorities()) {\n            for (ResourceRequest r : app.getResourceRequests(p).values()) {\n                Resource total = Resources.multiply(r.getCapability(), r.getNumContainers());\n                Resources.addTo(demand, total);\n            }\n        }\n    }\n}"
        },
        "possible_fix_code": {
            "AppSchedulable.updateDemand": "  public void updateDemand() {\n    demand = Resources.createResource(0);\n    // Demand is current consumption plus outstanding requests\n    Resources.addTo(demand, app.getCurrentConsumption());\n\n    // Add up outstanding resource requests with synchronization\n    synchronized (app.getResourceRequests()) {\n      for (Priority p : app.getPriorities()) {\n        for (ResourceRequest r : app.getResourceRequests(p).values()) {\n          Resource total = Resources.multiply(r.getCapability(), r.getNumContainers());\n          Resources.addTo(demand, total);\n        }\n      }\n    }\n  }"
        }
    },
    {
        "filename": "YARN-7697.json",
        "creation_time": "2018-01-03T19:28:50.000+0000",
        "bug_report": {
            "Title": "NM goes down with OOM due to leak in log-aggregation",
            "Description": "The NodeManager (NM) encounters an OutOfMemoryError (OOM) due to excessive memory consumption during the log aggregation process. The error occurs when the LogAggregationService attempts to load indexed logs metadata, leading to a fatal shutdown of the service. This issue is likely caused by a memory leak in the log aggregation components, particularly in the LogAggregationIndexedFileController class, which fails to manage memory effectively when handling large volumes of log data.",
            "StackTrace": [
                "2017-12-29 01:43:50,601 FATAL yarn.YarnUncaughtExceptionHandler (YarnUncaughtExceptionHandler.java:uncaughtException(51)) - Thread Thread[LogAggregationService #0,5,main] threw an Error.  Shutting down now...",
                "java.lang.OutOfMemoryError: Java heap space",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.loadIndexedLogsMeta(LogAggregationIndexedFileController.java:823)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.loadIndexedLogsMeta(LogAggregationIndexedFileController.java:840)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.initializeWriterInRolling(LogAggregationIndexedFileController.java:293)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.access$600(LogAggregationIndexedFileController.java:98)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController$1.run(LogAggregationIndexedFileController.java:216)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.initializeWriter(LogAggregationIndexedFileController.java:197)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainers(AppLogAggregatorImpl.java:205)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregation(AppLogAggregatorImpl.java:312)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run(AppLogAggregatorImpl.java:284)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$1.run(LogAggregationService.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The root cause of the OutOfMemoryError is likely due to the LogAggregationIndexedFileController's failure to efficiently manage memory when loading indexed logs metadata, particularly when handling large log files or a high volume of log aggregation requests.",
            "StepsToReproduce": [
                "1. Start the NodeManager with a high volume of log aggregation requests.",
                "2. Monitor the memory usage of the NodeManager.",
                "3. Wait for the log aggregation process to initiate.",
                "4. Observe the OutOfMemoryError in the logs."
            ],
            "ExpectedBehavior": "The NodeManager should handle log aggregation without running out of memory, successfully aggregating logs and maintaining stability.",
            "ObservedBehavior": "The NodeManager crashes with an OutOfMemoryError during the log aggregation process, leading to a service shutdown.",
            "Suggestions": "Investigate and optimize memory management in the LogAggregationIndexedFileController, particularly in methods that handle large log files. Consider implementing better caching strategies or limiting the size of logs being processed at one time.",
            "problem_location": {
                "files": [
                    "LogAggregationIndexedFileController.java",
                    "AppLogAggregatorImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl"
                ],
                "methods": [
                    "LogAggregationIndexedFileController.loadIndexedLogsMeta",
                    "LogAggregationIndexedFileController.initializeWriterInRolling",
                    "AppLogAggregatorImpl.uploadLogsForContainers"
                ]
            },
            "possible_fix": "To mitigate the OutOfMemoryError, consider implementing a memory limit for log aggregation processes, optimizing the loadIndexedLogsMeta method to handle large datasets more efficiently, and ensuring that resources are released properly after use."
        },
        "possible_fix_code": {
            "LogAggregationIndexedFileController.loadIndexedLogsMeta": "  private IndexedLogsMeta loadIndexedLogsMeta(Path remoteLogPath)\n      throws IOException {\n    // Implementing a memory-efficient way to load indexed logs metadata\n    // by limiting the size of data processed at once.\n    IndexedLogsMeta meta = new IndexedLogsMeta();\n    try (FSDataInputStream inputStream = fc.open(remoteLogPath)) {\n        // Read metadata in chunks to avoid OOM\n        byte[] buffer = new byte[1024 * 1024]; // 1MB buffer\n        int bytesRead;\n        while ((bytesRead = inputStream.read(buffer)) != -1) {\n            // Process the chunk of data read\n            // Here you would parse the buffer and update the meta accordingly\n            // For example, if the buffer contains log entries, parse them and add to meta\n        }\n    } catch (IOException e) {\n        LOG.error(\"Error loading indexed logs metadata from \" + remoteLogPath, e);\n        throw e;\n    }\n    return meta;\n  }"
        }
    },
    {
        "filename": "YARN-7382.json",
        "creation_time": "2017-10-23T23:36:59.000+0000",
        "bug_report": {
            "Title": "NoSuchElementException in FairScheduler after failover causes RM crash",
            "Description": "During the execution of a MapReduce job, if a ResourceManager (RM) failover occurs while the map tasks are at 100% completion, the active RM crashes due to a NoSuchElementException. This happens when the FairScheduler attempts to access the next pending ask from the AppSchedulingInfo, but the schedulerKeys set is empty, leading to an attempt to retrieve the first key from an empty collection.",
            "StackTrace": [
                "java.util.NoSuchElementException",
                "at java.util.concurrent.ConcurrentSkipListMap.firstKey(ConcurrentSkipListMap.java:2036)",
                "at java.util.concurrent.ConcurrentSkipListSet.first(ConcurrentSkipListSet.java:396)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.getNextPendingAsk(AppSchedulingInfo.java:371)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.isOverAMShareLimit(FSAppAttempt.java:901)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:1326)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:371)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:221)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1019)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:887)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1104)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:128)",
                "at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The root cause of the issue is that the FairScheduler's AppSchedulingInfo is trying to access the first key of an empty schedulerKeys set, which results in a NoSuchElementException. This occurs after a failover when there are no pending asks available.",
            "StepsToReproduce": [
                "1. Start a MapReduce job that runs for a significant duration.",
                "2. Trigger a ResourceManager failover while the job is running.",
                "3. Wait for the map tasks to reach 100% completion."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the failover gracefully and continue to manage the job without crashing.",
            "ObservedBehavior": "The ResourceManager crashes with a NoSuchElementException, leaving the cluster without any active ResourceManagers.",
            "Suggestions": "Implement a check in the getNextPendingAsk method to ensure that the schedulerKeys set is not empty before attempting to access its first key. This will prevent the NoSuchElementException from occurring.",
            "problem_location": {
                "files": [
                    "FSAppAttempt.java",
                    "AppSchedulingInfo.java",
                    "FairScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
                ],
                "methods": [
                    "FSAppAttempt.isOverAMShareLimit",
                    "AppSchedulingInfo.getNextPendingAsk",
                    "FairScheduler.attemptScheduling"
                ]
            },
            "possible_fix": "In the AppSchedulingInfo.getNextPendingAsk method, add a check to ensure that the schedulerKeys set is not empty before calling first(). If it is empty, return a default PendingAsk or handle the situation appropriately to avoid the exception."
        },
        "possible_fix_code": {
            "AppSchedulingInfo.getNextPendingAsk": "  public PendingAsk getNextPendingAsk() {\n    try {\n      readLock.lock();\n      if (schedulerKeys.isEmpty()) {\n        // Handle the case when there are no pending asks\n        return new PendingAsk(0, ResourceRequest.ANY); // or return null, depending on the desired behavior\n      }\n      SchedulerRequestKey firstRequestKey = schedulerKeys.first();\n      return getPendingAsk(firstRequestKey, ResourceRequest.ANY);\n    } finally {\n      readLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-1094.json",
        "creation_time": "2013-08-23T19:06:17.000+0000",
        "bug_report": {
            "Title": "RM restart throws Null pointer Exception in Secure Env",
            "Description": "When the Resource Manager (RM) restart feature is enabled and the RM is restarted while a job is running, a NullPointerException occurs during the recovery process. This happens specifically in the DelegationTokenRenewer class when it attempts to set a timer for token renewal, indicating that a required object is not initialized properly.",
            "StackTrace": [
                "2013-08-23 17:57:40,705 INFO  resourcemanager.RMAppManager (RMAppManager.java:recover(370)) - Recovering application application_1377280618693_0001",
                "2013-08-23 17:57:40,763 ERROR resourcemanager.ResourceManager (ResourceManager.java:serviceStart(617)) - Failed to load/recover state",
                "java.lang.NullPointerException",
                "        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.setTimerForTokenRenewal(DelegationTokenRenewer.java:371)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.addApplication(DelegationTokenRenewer.java:307)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:291)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:371)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:819)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:613)",
                "        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:832)",
                "2013-08-23 17:57:40,766 INFO  util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 1"
            ],
            "RootCause": "The NullPointerException is caused by the DelegationTokenRenewer trying to access an uninitialized or null object when setting the timer for token renewal during the recovery process of the Resource Manager.",
            "StepsToReproduce": [
                "1. Enable the RM restart feature.",
                "2. Start a job in the Resource Manager.",
                "3. Restart the Resource Manager while the job is still running."
            ],
            "ExpectedBehavior": "The Resource Manager should restart successfully and recover the running job without throwing any exceptions.",
            "ObservedBehavior": "The Resource Manager fails to start and throws a NullPointerException during the recovery process.",
            "Suggestions": "Ensure that all necessary objects are properly initialized before they are accessed in the DelegationTokenRenewer class. Specifically, check the initialization of the delegation token renewer and any related objects during the recovery process.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java",
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer",
                    "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager",
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"
                ],
                "methods": [
                    "DelegationTokenRenewer.setTimerForTokenRenewal",
                    "DelegationTokenRenewer.addApplication",
                    "RMAppManager.submitApplication",
                    "RMAppManager.recover",
                    "ResourceManager.recover",
                    "ResourceManager.serviceStart"
                ]
            },
            "possible_fix": "In the DelegationTokenRenewer class, ensure that all required fields are initialized before they are accessed in the setTimerForTokenRenewal method. For example, check if the token or context being used is null and handle it appropriately to prevent the NullPointerException."
        },
        "possible_fix_code": {
            "DelegationTokenRenewer.setTimerForTokenRenewal": "  public void setTimerForTokenRenewal(DelegationToken token) {\n    if (token == null) {\n        LOG.error(\"Cannot set timer for token renewal: token is null\");\n        return;\n    }\n    // Assuming there is a context or some other required object that needs to be initialized\n    if (this.renewalContext == null) {\n        LOG.error(\"Cannot set timer for token renewal: renewal context is not initialized\");\n        return;\n    }\n    // Proceed with setting the timer for token renewal\n    long delay = calculateRenewalDelay(token);\n    this.renewalTimer.schedule(new TimerTask() {\n        @Override\n        public void run() {\n            renewToken(token);\n        }\n    }, delay);\n  }"
        }
    },
    {
        "filename": "YARN-7269.json",
        "creation_time": "2017-09-28T23:56:42.000+0000",
        "bug_report": {
            "Title": "Tracking URL in the app state does not get redirected to ApplicationMaster for Running applications",
            "Description": "The application fails to redirect the tracking URL to the ApplicationMaster for running applications, resulting in a ServletException. The root of the issue lies in the inability of the system to determine the proxy server for redirection, which is critical for the proper functioning of the application. This is indicated by the exception thrown in the findRedirectUrl method of the AmIpFilter class.",
            "StackTrace": [
                "org.mortbay.log: /ws/v1/mapreduce/info",
                "javax.servlet.ServletException: Could not determine the proxy server for redirection",
                "at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.findRedirectUrl(AmIpFilter.java:199)",
                "at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:141)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1426)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(ServletHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(ServletHandler.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)"
            ],
            "RootCause": "The root cause of the issue is that the findRedirectUrl method in the AmIpFilter class is unable to determine a valid proxy server address for redirection, leading to a ServletException.",
            "StepsToReproduce": [
                "1. Start a running application in the system.",
                "2. Attempt to access the tracking URL for the application.",
                "3. Observe the exception thrown indicating the inability to determine the proxy server."
            ],
            "ExpectedBehavior": "The tracking URL should successfully redirect to the ApplicationMaster without any exceptions.",
            "ObservedBehavior": "The application throws a ServletException indicating that it could not determine the proxy server for redirection.",
            "Suggestions": "Ensure that the proxy server configuration is correctly set up in the Yarn configuration. Verify that the proxyUriBases are populated correctly and that the application can access the necessary resources.",
            "problem_location": {
                "files": [
                    "AmIpFilter.java",
                    "NoCacheFilter.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter",
                    "org.apache.hadoop.http.NoCacheFilter"
                ],
                "methods": [
                    "AmIpFilter.doFilter",
                    "AmIpFilter.findRedirectUrl",
                    "NoCacheFilter.doFilter"
                ]
            },
            "possible_fix": "In the findRedirectUrl method, ensure that the proxyUriBases are correctly populated and that the application has access to the necessary configuration. If the proxyUriBases are empty, provide a fallback mechanism to handle such cases gracefully."
        },
        "possible_fix_code": {
            "findRedirectUrl": "  public String findRedirectUrl() throws ServletException {\n    String addr = null;\n    if (proxyUriBases.size() == 1) {\n      // external proxy or not RM HA\n      addr = proxyUriBases.values().iterator().next();\n    } else {\n      // RM HA\n      YarnConfiguration conf = new YarnConfiguration();\n      for (String rmId : getRmIds(conf)) {\n        String url = getUrlByRmId(conf, rmId);\n        if (isValidUrl(url)) {\n          addr = url;\n          break;\n        }\n      }\n    }\n\n    // Check if addr is still null and provide a fallback mechanism\n    if (addr == null) {\n      // Fallback mechanism: log a warning and set a default proxy address if available\n      LOG.warn(\"No valid proxy server found, using default proxy address\");\n      addr = \"http://default-proxy-address:8080\"; // Replace with actual default if available\n    }\n\n    if (addr == null) {\n      throw new ServletException(\n          \"Could not determine the proxy server for redirection\");\n    }\n    return addr;\n  }"
        }
    },
    {
        "filename": "YARN-7249.json",
        "creation_time": "2017-09-25T16:49:46.000+0000",
        "bug_report": {
            "Title": "Fix CapacityScheduler NPE issue when a container preempted while the node is being removed",
            "Description": "The issue arises when a node is being removed from the scheduler while a container running on that node is being preempted. This leads to a race condition where a null node is passed to the leaf queue, resulting in a NullPointerException (NPE). The proposed fix is to implement a null check for the node in the CapacityScheduler to prevent this scenario.",
            "StackTrace": [
                "2017-08-31 02:51:24,748 FATAL resourcemanager.ResourceManager (ResourceManager.java:run(714)) - Error in handling event type KILL_RESERVED_CONTAINER to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.completedContainer(LeafQueue.java:1308)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainerInternal(CapacityScheduler.java:1469)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.completedContainer(AbstractYarnScheduler.java:497)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.killReservedContainer(CapacityScheduler.java:1505)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1341)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:127)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:705)"
            ],
            "RootCause": "The root cause of the issue is a race condition that occurs when a node is being removed while a container is being preempted, leading to a null node being passed to the LeafQueue's completedContainer method.",
            "StepsToReproduce": [
                "1. Start the CapacityScheduler with a node that has running containers.",
                "2. Trigger the removal of the node from the scheduler.",
                "3. Simultaneously, trigger the preemption of a container running on that node.",
                "4. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The scheduler should handle the preemption of containers gracefully without throwing a NullPointerException, even when a node is being removed.",
            "ObservedBehavior": "A NullPointerException is thrown when a container is preempted while the node it is running on is being removed, causing the scheduler to fail in handling the event.",
            "Suggestions": "Implement a null check for the node in the completedContainer method of the CapacityScheduler to prevent passing a null node to the LeafQueue.",
            "problem_location": {
                "files": [
                    "CapacityScheduler.java",
                    "LeafQueue.java",
                    "AbstractYarnScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler"
                ],
                "methods": [
                    "CapacityScheduler.completedContainerInternal",
                    "LeafQueue.completedContainer",
                    "AbstractYarnScheduler.completedContainer"
                ]
            },
            "possible_fix": "In the completedContainerInternal method of CapacityScheduler, add a null check for the node before proceeding with the container completion logic. For example:\n\nif (node == null) {\n    LOG.warn(\"Node is null, skipping container completion.\");\n    return;\n}"
        },
        "possible_fix_code": {
            "CapacityScheduler.completedContainerInternal": "  protected void completedContainerInternal(\n      RMContainer rmContainer, ContainerStatus containerStatus,\n      RMContainerEventType event) {\n    Container container = rmContainer.getContainer();\n    ContainerId containerId = container.getId();\n\n    // Get the application for the finished container\n    FiCaSchedulerApp application = getCurrentAttemptForContainer(\n        container.getId());\n    ApplicationId appId =\n        containerId.getApplicationAttemptId().getApplicationId();\n    if (application == null) {\n      LOG.info(\n          \"Container \" + container + \" of\" + \" finished application \" + appId\n              + \" completed with event \" + event);\n      return;\n    }\n\n    // Get the node on which the container was allocated\n    FiCaSchedulerNode node = getNode(container.getNodeId());\n    if (node == null) {\n      LOG.warn(\"Node is null, skipping container completion.\");\n      return;\n    }\n\n    // Inform the queue\n    LeafQueue queue = (LeafQueue) application.getQueue();\n    queue.completedContainer(getClusterResource(), application, node,\n        rmContainer, containerStatus, event, null, true);\n  }"
        }
    },
    {
        "filename": "YARN-4598.json",
        "creation_time": "2016-01-15T06:48:48.000+0000",
        "bug_report": {
            "Title": "Invalid event: RESOURCE_FAILED at CONTAINER_CLEANEDUP_AFTER_KILL",
            "Description": "The issue arises when a container in the YARN NodeManager transitions to the state 'CONTAINER_CLEANEDUP_AFTER_KILL' and subsequently receives a 'RESOURCE_FAILED' event. The current state machine does not allow this transition, leading to an InvalidStateTransitionException. This indicates a flaw in the state management logic where certain events are not appropriately handled based on the container's state.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: RESOURCE_FAILED at CONTAINER_CLEANEDUP_AFTER_KILL",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1127)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1078)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175)"
            ],
            "RootCause": "The root cause of the issue is that the state machine does not permit the 'RESOURCE_FAILED' event to be processed when the container is in the 'CONTAINER_CLEANEDUP_AFTER_KILL' state, resulting in an InvalidStateTransitionException.",
            "StepsToReproduce": [
                "1. Start a YARN application that creates containers.",
                "2. Forcefully kill a container to transition it to 'CONTAINER_CLEANEDUP_AFTER_KILL'.",
                "3. Trigger a 'RESOURCE_FAILED' event for the killed container."
            ],
            "ExpectedBehavior": "The system should handle the 'RESOURCE_FAILED' event gracefully, allowing for appropriate state transitions or logging without throwing an exception.",
            "ObservedBehavior": "The system throws an InvalidStateTransitionException when attempting to process a 'RESOURCE_FAILED' event while in the 'CONTAINER_CLEANEDUP_AFTER_KILL' state.",
            "Suggestions": "Review the state transition logic in the state machine to allow for the 'RESOURCE_FAILED' event to be handled appropriately when in the 'CONTAINER_CLEANEDUP_AFTER_KILL' state. Consider adding a state transition rule or modifying the event handling logic.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.java",
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.state.StateMachineFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory"
                ],
                "methods": [
                    "ContainerImpl.handle",
                    "StateMachineFactory.doTransition"
                ]
            },
            "possible_fix": "Modify the state machine to allow the 'RESOURCE_FAILED' event to be processed when the container is in the 'CONTAINER_CLEANEDUP_AFTER_KILL' state. This may involve adding a new state transition rule in the state machine configuration."
        },
        "possible_fix_code": {
            "ContainerImpl.handle": "  public void handle(ContainerEvent event) {\n    try {\n      this.writeLock.lock();\n\n      ContainerId containerID = event.getContainerID();\n      LOG.debug(\"Processing \" + containerID + \" of type \" + event.getType());\n\n      ContainerState oldState = stateMachine.getCurrentState();\n      ContainerState newState = null;\n      try {\n        newState = stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitionException e) {\n        if (oldState == ContainerState.CONTAINER_CLEANEDUP_AFTER_KILL && event.getType() == EventType.RESOURCE_FAILED) {\n          LOG.info(\"Handling RESOURCE_FAILED event in CONTAINER_CLEANEDUP_AFTER_KILL state\");\n          // Handle the RESOURCE_FAILED event appropriately here\n          // For example, log the event or transition to a new state if applicable\n          newState = oldState; // Keep the state the same or transition as needed\n        } else {\n          LOG.warn(\"Can't handle this event at current state: Current: [\" + oldState + \"], eventType: [\" + event.getType() + \"]\", e);\n        }\n      }\n      if (oldState != newState) {\n        LOG.info(\"Container \" + containerID + \" transitioned from \" + oldState + \" to \" + newState);\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-1149.json",
        "creation_time": "2013-09-04T21:46:58.000+0000",
        "bug_report": {
            "Title": "NM throws InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FINISHED at RUNNING",
            "Description": "The NodeManager (NM) encounters an InvalidStateTransitonException when it receives a kill signal for an application that has completed execution, but the log aggregation process has not yet started. This occurs because the application is still in the RUNNING state when the APPLICATION_LOG_HANDLING_FINISHED event is triggered, which is not a valid transition for that state.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FINISHED at RUNNING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:425)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:697)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:134)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The application is in the RUNNING state when the APPLICATION_LOG_HANDLING_FINISHED event is received, which is not a valid state transition according to the state machine defined in the application.",
            "StepsToReproduce": [
                "1. Start an application in the NodeManager.",
                "2. Send a kill signal to the application before log aggregation has started.",
                "3. Observe the logs for the InvalidStateTransitonException."
            ],
            "ExpectedBehavior": "The NodeManager should handle the APPLICATION_LOG_HANDLING_FINISHED event gracefully, transitioning the application state appropriately without throwing an exception.",
            "ObservedBehavior": "The NodeManager throws an InvalidStateTransitonException when it receives the APPLICATION_LOG_HANDLING_FINISHED event while the application is still in the RUNNING state.",
            "Suggestions": "Implement a check in the ApplicationImpl.handle method to ensure that the APPLICATION_LOG_HANDLING_FINISHED event can only be processed when the application is in a valid state for that event.",
            "problem_location": {
                "files": [
                    "ApplicationImpl.java",
                    "StateMachineFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory"
                ],
                "methods": [
                    "ApplicationImpl.handle",
                    "StateMachineFactory.doTransition"
                ]
            },
            "possible_fix": "In the ApplicationImpl.handle method, add a condition to check if the current state allows for the APPLICATION_LOG_HANDLING_FINISHED event. If not, log a warning and return without processing the event."
        },
        "possible_fix_code": {
            "ApplicationImpl.handle": "  public void handle(ApplicationEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId applicationID = event.getApplicationID();\n      LOG.debug(\"Processing \" + applicationID + \" of type \" + event.getType());\n\n      ApplicationState oldState = stateMachine.getCurrentState();\n      ApplicationState newState = null;\n\n      // Check if the current state allows for the APPLICATION_LOG_HANDLING_FINISHED event\n      if (event.getType() == ApplicationEventType.APPLICATION_LOG_HANDLING_FINISHED && oldState == ApplicationState.RUNNING) {\n        LOG.warn(\"Cannot handle APPLICATION_LOG_HANDLING_FINISHED event while in RUNNING state.\");\n        return; // Exit early without processing the event\n      }\n\n      try {\n        // queue event requesting init of the same app\n        newState = stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.warn(\"Can't handle this event at current state\", e);\n      }\n      if (oldState != newState) {\n        LOG.info(\"Application \" + applicationID + \" transitioned from \" + oldState + \" to \" + newState);\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-7818.json",
        "creation_time": "2018-01-25T18:42:55.000+0000",
        "bug_report": {
            "Title": "Remove privileged operation warnings during container launch for the ContainerRuntimes",
            "Description": "During the launch of containers in the Dshell application, a failure occurs when the application master (AM) is restarted. The containers fail to launch with exit code 143, indicating that they were terminated. This issue arises specifically when the node manager (NM) is restarted while the application is running, leading to a failure in executing privileged operations required for container launch.",
            "StackTrace": [
                "2018-01-24 09:48:30,547 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1516787230461_0001_01_000003 transitioned from RUNNING to KILLING",
                "2018-01-24 09:48:30,547 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(668)) - Cleaning up container container_e04_1516787230461_0001_01_000003",
                "2018-01-24 09:48:30,552 WARN  privileged.PrivilegedOperationExecutor (PrivilegedOperationExecutor.java:executePrivilegedOperation(174)) - Shell execution returned exit code: 143.",
                "2018-01-24 09:48:30,553 WARN  runtime.DefaultLinuxContainerRuntime (DefaultLinuxContainerRuntime.java:launchContainer(127)) - Launch container failed. Exception: org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=143:",
                "2018-01-24 09:48:30,553 WARN  nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:launchContainer(557)) - Exit code from container container_e04_1516787230461_0001_01_000003 is : 143"
            ],
            "RootCause": "The root cause of the issue is that the container launch process is unable to execute the required privileged operations after the node manager is restarted. The exit code 143 indicates that the process was terminated, likely due to the container being marked for killing or due to a timeout in the execution of the privileged operation.",
            "StepsToReproduce": [
                "1) Run Dshell Application using the command: yarn org.apache.hadoop.yarn.applications.distributedshell.Client -jar /usr/hdp/3.0.0.0-751/hadoop-yarn/hadoop-yarn-applications-distributedshell-*.jar -keep_containers_across_application_attempts -timeout 900000 -shell_command 'sleep 110' -num_containers 4",
                "2) Identify the host where the Application Master (AM) is running.",
                "3) Locate the containers launched by the application.",
                "4) Restart the Node Manager (NM) where the AM is running.",
                "5) Validate that a new attempt is not started and that the previously launched containers are in RUNNING state."
            ],
            "ExpectedBehavior": "The containers should remain in the RUNNING state after the Node Manager is restarted, and a new application attempt should not be initiated.",
            "ObservedBehavior": "The containers fail to launch and transition to a KILLING state, resulting in an exit code of 143.",
            "Suggestions": "Review the handling of privileged operations during container launch, especially in scenarios where the Node Manager is restarted. Ensure that the container launch process can gracefully handle such interruptions.",
            "problem_location": {
                "files": [
                    "DefaultLinuxContainerRuntime.java",
                    "ContainerLaunch.java",
                    "PrivilegedOperationExecutor.java",
                    "LinuxContainerExecutor.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor",
                    "org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor"
                ],
                "methods": [
                    "DefaultLinuxContainerRuntime.launchContainer",
                    "ContainerLaunch.launchContainer",
                    "PrivilegedOperationExecutor.executePrivilegedOperation",
                    "LinuxContainerExecutor.launchContainer"
                ]
            },
            "possible_fix": "Consider implementing a retry mechanism for privileged operations during container launch, especially after a Node Manager restart. Additionally, ensure that the container state is properly managed to prevent premature termination."
        },
        "possible_fix_code": {
            "DefaultLinuxContainerRuntime.launchContainer": "  public void launchContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    PrivilegedOperation launchOp = new PrivilegedOperation(\n        PrivilegedOperation.OperationType.LAUNCH_CONTAINER);\n\n    //All of these arguments are expected to be available in the runtime context\n    launchOp.appendArgs(ctx.getExecutionAttribute(RUN_AS_USER),\n        ctx.getExecutionAttribute(USER),\n        Integer.toString(PrivilegedOperation.\n            RunAsUserCommand.LAUNCH_CONTAINER.getValue()),\n        ctx.getExecutionAttribute(APPID),\n        ctx.getExecutionAttribute(CONTAINER_ID_STR),\n        ctx.getExecutionAttribute(CONTAINER_WORK_DIR).toString(),\n        ctx.getExecutionAttribute(NM_PRIVATE_CONTAINER_SCRIPT_PATH).toUri()\n            .getPath(),\n        ctx.getExecutionAttribute(NM_PRIVATE_TOKENS_PATH).toUri().getPath(),\n        ctx.getExecutionAttribute(PID_FILE_PATH).toString(),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            ctx.getExecutionAttribute(LOCAL_DIRS)),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            ctx.getExecutionAttribute(LOG_DIRS)),\n        ctx.getExecutionAttribute(RESOURCES_OPTIONS));\n\n    String tcCommandFile = ctx.getExecutionAttribute(TC_COMMAND_FILE);\n\n    if (tcCommandFile != null) {\n      launchOp.appendArgs(tcCommandFile);\n    }\n\n    // Retry mechanism for privileged operations\n    int retryCount = 3;\n    while (retryCount > 0) {\n      try {\n        // List<String> -> stored as List -> fetched/converted to List<String>\n        // we can't do better here thanks to type-erasure\n        @SuppressWarnings(\"unchecked\")\n        List<String> prefixCommands = (List<String>) ctx.getExecutionAttribute(\n            CONTAINER_LAUNCH_PREFIX_COMMANDS);\n\n        privilegedOperationExecutor.executePrivilegedOperation(prefixCommands,\n              launchOp, null, null, false, false);\n        return; // Exit if successful\n      } catch (PrivilegedOperationException e) {\n        LOG.warn(\"Launch container failed. Exception: \", e);\n        retryCount--;\n        if (retryCount == 0) {\n          throw new ContainerExecutionException(\"Launch container failed after retries\", e\n              .getExitCode(), e.getOutput(), e.getErrorOutput());\n        }\n        // Optionally add a delay before retrying\n        try {\n          Thread.sleep(1000); // wait for 1 second before retrying\n        } catch (InterruptedException ie) {\n          Thread.currentThread().interrupt(); // Restore interrupted status\n        }\n      }\n    }\n  }"
        }
    }
]