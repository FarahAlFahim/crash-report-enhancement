[
    {
        "filename": "MAPREDUCE-6633.json",
        "creation_time": "2016-02-10T21:12:09.000+0000",
        "stack_trace": "```\nException running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#29\n\tat org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n\tat com.hadoop.compression.lzo.LzoDecompressor.setInput(LzoDecompressor.java:196)\n\tat org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:104)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)\n\tat org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:192)\n\tat org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.shuffle(InMemoryMapOutput.java:97)\n\tat org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:537)\n\tat org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:336)\n\tat org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.Shuffle.run": "  public RawKeyValueIterator run() throws IOException, InterruptedException {\n    // Scale the maximum events we fetch per RPC call to mitigate OOM issues\n    // on the ApplicationMaster when a thundering herd of reducers fetch events\n    // TODO: This should not be necessary after HADOOP-8942\n    int eventsPerReducer = Math.max(MIN_EVENTS_TO_FETCH,\n        MAX_RPC_OUTSTANDING_EVENTS / jobConf.getNumReduceTasks());\n    int maxEventsToFetch = Math.min(MAX_EVENTS_TO_FETCH, eventsPerReducer);\n\n    // Start the map-completion events fetcher thread\n    final EventFetcher<K,V> eventFetcher = \n      new EventFetcher<K,V>(reduceId, umbilical, scheduler, this,\n          maxEventsToFetch);\n    eventFetcher.start();\n    \n    // Start the map-output fetcher threads\n    boolean isLocal = localMapFiles != null;\n    final int numFetchers = isLocal ? 1 :\n      jobConf.getInt(MRJobConfig.SHUFFLE_PARALLEL_COPIES, 5);\n    Fetcher<K,V>[] fetchers = new Fetcher[numFetchers];\n    if (isLocal) {\n      fetchers[0] = new LocalFetcher<K, V>(jobConf, reduceId, scheduler,\n          merger, reporter, metrics, this, reduceTask.getShuffleSecret(),\n          localMapFiles);\n      fetchers[0].start();\n    } else {\n      for (int i=0; i < numFetchers; ++i) {\n        fetchers[i] = new Fetcher<K,V>(jobConf, reduceId, scheduler, merger, \n                                       reporter, metrics, this, \n                                       reduceTask.getShuffleSecret());\n        fetchers[i].start();\n      }\n    }\n    \n    // Wait for shuffle to complete successfully\n    while (!scheduler.waitUntilDone(PROGRESS_FREQUENCY)) {\n      reporter.progress();\n      \n      synchronized (this) {\n        if (throwable != null) {\n          throw new ShuffleError(\"error in shuffle in \" + throwingThreadName,\n                                 throwable);\n        }\n      }\n    }\n\n    // Stop the event-fetcher thread\n    eventFetcher.shutDown();\n    \n    // Stop the map-output fetcher threads\n    for (Fetcher<K,V> fetcher : fetchers) {\n      fetcher.shutDown();\n    }\n    \n    // stop the scheduler\n    scheduler.close();\n\n    copyPhase.complete(); // copy is already complete\n    taskStatus.setPhase(TaskStatus.Phase.SORT);\n    reduceTask.statusUpdate(umbilical);\n\n    // Finish the on-going merges...\n    RawKeyValueIterator kvIter = null;\n    try {\n      kvIter = merger.close();\n    } catch (Throwable e) {\n      throw new ShuffleError(\"Error while doing final merge \" , e);\n    }\n\n    // Sanity check\n    synchronized (this) {\n      if (throwable != null) {\n        throw new ShuffleError(\"error in shuffle in \" + throwingThreadName,\n                               throwable);\n      }\n    }\n    \n    return kvIter;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.Shuffle.close": "  public void close(){\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.run": "  public void run(JobConf job, final TaskUmbilicalProtocol umbilical)\n    throws IOException, InterruptedException, ClassNotFoundException {\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n\n    if (isMapOrReduce()) {\n      copyPhase = getProgress().addPhase(\"copy\");\n      sortPhase  = getProgress().addPhase(\"sort\");\n      reducePhase = getProgress().addPhase(\"reduce\");\n    }\n    // start thread that will handle communication with parent\n    TaskReporter reporter = startReporter(umbilical);\n    \n    boolean useNewApi = job.getUseNewReducer();\n    initialize(job, getJobID(), reporter, useNewApi);\n\n    // check if it is a cleanupJobTask\n    if (jobCleanup) {\n      runJobCleanupTask(umbilical, reporter);\n      return;\n    }\n    if (jobSetup) {\n      runJobSetupTask(umbilical, reporter);\n      return;\n    }\n    if (taskCleanup) {\n      runTaskCleanupTask(umbilical, reporter);\n      return;\n    }\n    \n    // Initialize the codec\n    codec = initCodec();\n    RawKeyValueIterator rIter = null;\n    ShuffleConsumerPlugin shuffleConsumerPlugin = null;\n    \n    Class combinerClass = conf.getCombinerClass();\n    CombineOutputCollector combineCollector = \n      (null != combinerClass) ? \n     new CombineOutputCollector(reduceCombineOutputCounter, reporter, conf) : null;\n\n    Class<? extends ShuffleConsumerPlugin> clazz =\n          job.getClass(MRConfig.SHUFFLE_CONSUMER_PLUGIN, Shuffle.class, ShuffleConsumerPlugin.class);\n\t\t\t\t\t\n    shuffleConsumerPlugin = ReflectionUtils.newInstance(clazz, job);\n    LOG.info(\"Using ShuffleConsumerPlugin: \" + shuffleConsumerPlugin);\n\n    ShuffleConsumerPlugin.Context shuffleContext = \n      new ShuffleConsumerPlugin.Context(getTaskID(), job, FileSystem.getLocal(job), umbilical, \n                  super.lDirAlloc, reporter, codec, \n                  combinerClass, combineCollector, \n                  spilledRecordsCounter, reduceCombineInputCounter,\n                  shuffledMapsCounter,\n                  reduceShuffleBytes, failedShuffleCounter,\n                  mergedMapOutputsCounter,\n                  taskStatus, copyPhase, sortPhase, this,\n                  mapOutputFile, localMapFiles);\n    shuffleConsumerPlugin.init(shuffleContext);\n\n    rIter = shuffleConsumerPlugin.run();\n\n    // free up the data structures\n    mapOutputFilesOnDisk.clear();\n    \n    sortPhase.complete();                         // sort is complete\n    setPhase(TaskStatus.Phase.REDUCE); \n    statusUpdate(umbilical);\n    Class keyClass = job.getMapOutputKeyClass();\n    Class valueClass = job.getMapOutputValueClass();\n    RawComparator comparator = job.getOutputValueGroupingComparator();\n\n    if (useNewApi) {\n      runNewReducer(job, umbilical, reporter, rIter, comparator, \n                    keyClass, valueClass);\n    } else {\n      runOldReducer(job, umbilical, reporter, rIter, comparator, \n                    keyClass, valueClass);\n    }\n\n    shuffleConsumerPlugin.close();\n    done(umbilical, reporter);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.newInstance": "         public Writable newInstance() { return new ReduceTask(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.runOldReducer": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runOldReducer(JobConf job,\n                     TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator<INKEY> comparator,\n                     Class<INKEY> keyClass,\n                     Class<INVALUE> valueClass) throws IOException {\n    Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer = \n      ReflectionUtils.newInstance(job.getReducerClass(), job);\n    // make output collector\n    String finalName = getOutputName(getPartition());\n\n    RecordWriter<OUTKEY, OUTVALUE> out = new OldTrackingRecordWriter<OUTKEY, OUTVALUE>(\n        this, job, reporter, finalName);\n    final RecordWriter<OUTKEY, OUTVALUE> finalOut = out;\n    \n    OutputCollector<OUTKEY,OUTVALUE> collector = \n      new OutputCollector<OUTKEY,OUTVALUE>() {\n        public void collect(OUTKEY key, OUTVALUE value)\n          throws IOException {\n          finalOut.write(key, value);\n          // indicate that progress update needs to be sent\n          reporter.progress();\n        }\n      };\n    \n    // apply reduce function\n    try {\n      //increment processed counter only if skipping feature is enabled\n      boolean incrProcCount = SkipBadRecords.getReducerMaxSkipGroups(job)>0 &&\n        SkipBadRecords.getAutoIncrReducerProcCount(job);\n      \n      ReduceValuesIterator<INKEY,INVALUE> values = isSkipping() ? \n          new SkippingReduceValuesIterator<INKEY,INVALUE>(rIter, \n              comparator, keyClass, valueClass, \n              job, reporter, umbilical) :\n          new ReduceValuesIterator<INKEY,INVALUE>(rIter, \n          comparator, keyClass, valueClass,\n          job, reporter);\n      values.informReduceProgress();\n      while (values.more()) {\n        reduceInputKeyCounter.increment(1);\n        reducer.reduce(values.getKey(), values, collector, reporter);\n        if(incrProcCount) {\n          reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n              SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);\n        }\n        values.nextKey();\n        values.informReduceProgress();\n      }\n\n      reducer.close();\n      reducer = null;\n      \n      out.close(reporter);\n      out = null;\n    } finally {\n      IOUtils.cleanup(LOG, reducer);\n      closeQuietly(out, reporter);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.close": "      public void close() throws IOException {\n        rawIter.close();\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.getProgress": "      public Progress getProgress() {\n        return rawIter.getProgress();\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.runNewReducer": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runNewReducer(JobConf job,\n                     final TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator<INKEY> comparator,\n                     Class<INKEY> keyClass,\n                     Class<INVALUE> valueClass\n                     ) throws IOException,InterruptedException, \n                              ClassNotFoundException {\n    // wrap value iterator to report progress.\n    final RawKeyValueIterator rawIter = rIter;\n    rIter = new RawKeyValueIterator() {\n      public void close() throws IOException {\n        rawIter.close();\n      }\n      public DataInputBuffer getKey() throws IOException {\n        return rawIter.getKey();\n      }\n      public Progress getProgress() {\n        return rawIter.getProgress();\n      }\n      public DataInputBuffer getValue() throws IOException {\n        return rawIter.getValue();\n      }\n      public boolean next() throws IOException {\n        boolean ret = rawIter.next();\n        reporter.setProgress(rawIter.getProgress().getProgress());\n        return ret;\n      }\n    };\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,\n          getTaskID(), reporter);\n    // make a reducer\n    org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer =\n      (org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE>)\n        ReflectionUtils.newInstance(taskContext.getReducerClass(), job);\n    org.apache.hadoop.mapreduce.RecordWriter<OUTKEY,OUTVALUE> trackedRW = \n      new NewTrackingRecordWriter<OUTKEY, OUTVALUE>(this, taskContext);\n    job.setBoolean(\"mapred.skip.on\", isSkipping());\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.Reducer.Context \n         reducerContext = createReduceContext(reducer, job, getTaskID(),\n                                               rIter, reduceInputKeyCounter, \n                                               reduceInputValueCounter, \n                                               trackedRW,\n                                               committer,\n                                               reporter, comparator, keyClass,\n                                               valueClass);\n    try {\n      reducer.run(reducerContext);\n    } finally {\n      trackedRW.close(reducerContext);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.initCodec": "  private CompressionCodec initCodec() {\n    // check if map-outputs are to be compressed\n    if (conf.getCompressMapOutput()) {\n      Class<? extends CompressionCodec> codecClass =\n        conf.getMapOutputCompressorClass(DefaultCodec.class);\n      return ReflectionUtils.newInstance(codecClass, conf);\n    } \n\n    return null;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.run": "              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.setEncryptedSpillKeyIfRequired": "  public static void setEncryptedSpillKeyIfRequired(Task task) throws\n          Exception {\n    if ((task != null) && (task.getEncryptedSpillKey() != null) && (task\n            .getEncryptedSpillKey().length > 1)) {\n      Credentials creds =\n              UserGroupInformation.getCurrentUser().getCredentials();\n      TokenCache.setEncryptedSpillKey(task.getEncryptedSpillKey(), creds);\n      UserGroupInformation.getCurrentUser().addCredentials(creds);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause == null) {\n        throw new RuntimeException(\"PrivilegedActionException with no \" +\n                \"underlying cause. UGI [\" + this + \"]\" +\": \" + pae, pae);\n      } else if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.main": "  public static void main(String[] args) throws Throwable {\n    Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\n    LOG.debug(\"Child starting\");\n\n    final JobConf job = new JobConf(MRJobConfig.JOB_CONF_FILE);\n    // Initing with our JobConf allows us to avoid loading confs twice\n    Limits.init(job);\n    UserGroupInformation.setConfiguration(job);\n\n    String host = args[0];\n    int port = Integer.parseInt(args[1]);\n    final InetSocketAddress address =\n        NetUtils.createSocketAddrForHost(host, port);\n    final TaskAttemptID firstTaskid = TaskAttemptID.forName(args[2]);\n    long jvmIdLong = Long.parseLong(args[3]);\n    JVMId jvmId = new JVMId(firstTaskid.getJobID(),\n        firstTaskid.getTaskType() == TaskType.MAP, jvmIdLong);\n    \n    CallerContext.setCurrent(\n        new CallerContext.Builder(\"mr_\" + firstTaskid.toString()).build());\n\n    // initialize metrics\n    DefaultMetricsSystem.initialize(\n        StringUtils.camelize(firstTaskid.getTaskType().name()) +\"Task\");\n\n    // Security framework already loaded the tokens into current ugi\n    Credentials credentials =\n        UserGroupInformation.getCurrentUser().getCredentials();\n    LOG.info(\"Executing with tokens:\");\n    for (Token<?> token: credentials.getAllTokens()) {\n      LOG.info(token);\n    }\n\n    // Create TaskUmbilicalProtocol as actual task owner.\n    UserGroupInformation taskOwner =\n      UserGroupInformation.createRemoteUser(firstTaskid.getJobID().toString());\n    Token<JobTokenIdentifier> jt = TokenCache.getJobToken(credentials);\n    SecurityUtil.setTokenService(jt, address);\n    taskOwner.addToken(jt);\n    final TaskUmbilicalProtocol umbilical =\n      taskOwner.doAs(new PrivilegedExceptionAction<TaskUmbilicalProtocol>() {\n      @Override\n      public TaskUmbilicalProtocol run() throws Exception {\n        return (TaskUmbilicalProtocol)RPC.getProxy(TaskUmbilicalProtocol.class,\n            TaskUmbilicalProtocol.versionID, address, job);\n      }\n    });\n\n    // report non-pid to application master\n    JvmContext context = new JvmContext(jvmId, \"-1000\");\n    LOG.debug(\"PID: \" + System.getenv().get(\"JVM_PID\"));\n    Task task = null;\n    UserGroupInformation childUGI = null;\n    ScheduledExecutorService logSyncer = null;\n\n    try {\n      int idleLoopCount = 0;\n      JvmTask myTask = null;;\n      // poll for new task\n      for (int idle = 0; null == myTask; ++idle) {\n        long sleepTimeMilliSecs = Math.min(idle * 500, 1500);\n        LOG.info(\"Sleeping for \" + sleepTimeMilliSecs\n            + \"ms before retrying again. Got null now.\");\n        MILLISECONDS.sleep(sleepTimeMilliSecs);\n        myTask = umbilical.getTask(context);\n      }\n      if (myTask.shouldDie()) {\n        return;\n      }\n\n      task = myTask.getTask();\n      YarnChild.taskid = task.getTaskID();\n\n      // Create the job-conf and set credentials\n      configureTask(job, task, credentials, jt);\n\n      // log the system properties\n      String systemPropsToLog = MRApps.getSystemPropertiesToLog(job);\n      if (systemPropsToLog != null) {\n        LOG.info(systemPropsToLog);\n      }\n\n      // Initiate Java VM metrics\n      JvmMetrics.initSingleton(jvmId.toString(), job.getSessionId());\n      childUGI = UserGroupInformation.createRemoteUser(System\n          .getenv(ApplicationConstants.Environment.USER.toString()));\n      // Add tokens to new user so that it may execute its task correctly.\n      childUGI.addCredentials(credentials);\n\n      // set job classloader if configured before invoking the task\n      MRApps.setJobClassLoader(job);\n\n      logSyncer = TaskLog.createLogSyncer();\n\n      // Create a final reference to the task for the doAs block\n      final Task taskFinal = task;\n      childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n        @Override\n        public Object run() throws Exception {\n          // use job-specified working directory\n          setEncryptedSpillKeyIfRequired(taskFinal);\n          FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory());\n          taskFinal.run(job, umbilical); // run the task\n          return null;\n        }\n      });\n    } catch (FSError e) {\n      LOG.fatal(\"FSError from child\", e);\n      if (!ShutdownHookManager.get().isShutdownInProgress()) {\n        umbilical.fsError(taskid, e.getMessage());\n      }\n    } catch (Exception exception) {\n      LOG.warn(\"Exception running child : \"\n          + StringUtils.stringifyException(exception));\n      try {\n        if (task != null) {\n          // do cleanup for the task\n          if (childUGI == null) { // no need to job into doAs block\n            task.taskCleanup(umbilical);\n          } else {\n            final Task taskFinal = task;\n            childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n              @Override\n              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }\n            });\n          }\n        }\n      } catch (Exception e) {\n        LOG.info(\"Exception cleaning up: \" + StringUtils.stringifyException(e));\n      }\n      // Report back any failures, for diagnostic purposes\n      if (taskid != null) {\n        if (!ShutdownHookManager.get().isShutdownInProgress()) {\n          umbilical.fatalError(taskid,\n              StringUtils.stringifyException(exception));\n        }\n      }\n    } catch (Throwable throwable) {\n      LOG.fatal(\"Error running child : \"\n    \t        + StringUtils.stringifyException(throwable));\n      if (taskid != null) {\n        if (!ShutdownHookManager.get().isShutdownInProgress()) {\n          Throwable tCause = throwable.getCause();\n          String cause =\n              tCause == null ? throwable.getMessage() : StringUtils\n                  .stringifyException(tCause);\n          umbilical.fatalError(taskid, cause);\n        }\n      }\n    } finally {\n      RPC.stopProxy(umbilical);\n      DefaultMetricsSystem.shutdown();\n      TaskLog.syncLogsShutdown(logSyncer);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.configureTask": "  private static void configureTask(JobConf job, Task task,\n      Credentials credentials, Token<JobTokenIdentifier> jt) throws IOException {\n    job.setCredentials(credentials);\n    \n    ApplicationAttemptId appAttemptId =\n        ConverterUtils.toContainerId(\n            System.getenv(Environment.CONTAINER_ID.name()))\n            .getApplicationAttemptId();\n    LOG.debug(\"APPLICATION_ATTEMPT_ID: \" + appAttemptId);\n    // Set it in conf, so as to be able to be used the the OutputCommitter.\n    job.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID,\n        appAttemptId.getAttemptId());\n\n    // set tcp nodelay\n    job.setBoolean(\"ipc.client.tcpnodelay\", true);\n    job.setClass(MRConfig.TASK_LOCAL_OUTPUT_CLASS,\n        YarnOutputFiles.class, MapOutputFile.class);\n    // set the jobToken and shuffle secrets into task\n    task.setJobTokenSecret(\n        JobTokenSecretManager.createSecretKey(jt.getPassword()));\n    byte[] shuffleSecret = TokenCache.getShuffleSecretKey(credentials);\n    if (shuffleSecret == null) {\n      LOG.warn(\"Shuffle secret missing from task credentials.\"\n          + \" Using job token secret as shuffle secret.\");\n      shuffleSecret = jt.getPassword();\n    }\n    task.setShuffleSecret(\n        JobTokenSecretManager.createSecretKey(shuffleSecret));\n\n    // setup the child's MRConfig.LOCAL_DIR.\n    configureLocalDirs(task, job);\n\n    // setup the child's attempt directories\n    // Do the task-type specific localization\n    task.localizeConfiguration(job);\n\n    // Set up the DistributedCache related configs\n    MRApps.setupDistributedCacheLocal(job);\n\n    // Overwrite the localized task jobconf which is linked to in the current\n    // work-dir.\n    Path localTaskFile = new Path(MRJobConfig.JOB_CONF_FILE);\n    writeLocalJobFile(localTaskFile, job);\n    task.setJobFile(localTaskFile.toString());\n    task.setConf(job);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.BlockDecompressorStream.decompress": "  protected int decompress(byte[] b, int off, int len) throws IOException {\n    // Check if we are the beginning of a block\n    if (noUncompressedBytes == originalBlockSize) {\n      // Get original data size\n      try {\n        originalBlockSize =  rawReadInt();\n      } catch (IOException ioe) {\n        return -1;\n      }\n      noUncompressedBytes = 0;\n      // EOF if originalBlockSize is 0\n      // This will occur only when decompressing previous compressed empty file\n      if (originalBlockSize == 0) {\n        eof = true;\n        return -1;\n      }\n    }\n\n    int n = 0;\n    while ((n = decompressor.decompress(b, off, len)) == 0) {\n      if (decompressor.finished() || decompressor.needsDictionary()) {\n        if (noUncompressedBytes >= originalBlockSize) {\n          eof = true;\n          return -1;\n        }\n      }\n      if (decompressor.needsInput()) {\n        int m;\n        try {\n          m = getCompressedData();\n        } catch (EOFException e) {\n          eof = true;\n          return -1;\n        }\n        // Send the read data to the decompressor\n        decompressor.setInput(buffer, 0, m);\n      }\n    }\n\n    // Note the no. of decompressed bytes read from 'current' block\n    noUncompressedBytes += n;\n\n    return n;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.BlockDecompressorStream.getCompressedData": "  protected int getCompressedData() throws IOException {\n    checkStream();\n\n    // Get the size of the compressed chunk (always non-negative)\n    int len = rawReadInt();\n\n    // Read len bytes from underlying stream \n    if (len > buffer.length) {\n      buffer = new byte[len];\n    }\n    int n = 0, off = 0;\n    while (n < len) {\n      int count = in.read(buffer, off + n, len - n);\n      if (count < 0) {\n        throw new EOFException(\"Unexpected end of block in input stream\");\n      }\n      n += count;\n    }\n\n    return len;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.BlockDecompressorStream.rawReadInt": "  private int rawReadInt() throws IOException {\n    int b1 = in.read();\n    int b2 = in.read();\n    int b3 = in.read();\n    int b4 = in.read();\n    if ((b1 | b2 | b3 | b4) < 0)\n      throw new EOFException();\n    return ((b1 << 24) + (b2 << 16) + (b3 << 8) + (b4 << 0));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.DecompressorStream.read": "  public int read(byte[] b, int off, int len) throws IOException {\n    checkStream();\n    \n    if ((off | len | (off + len) | (b.length - (off + len))) < 0) {\n      throw new IndexOutOfBoundsException();\n    } else if (len == 0) {\n      return 0;\n    }\n\n    return decompress(b, off, len);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.DecompressorStream.checkStream": "  protected void checkStream() throws IOException {\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.DecompressorStream.decompress": "  protected int decompress(byte[] b, int off, int len) throws IOException {\n    int n = 0;\n\n    while ((n = decompressor.decompress(b, off, len)) == 0) {\n      if (decompressor.needsDictionary()) {\n        eof = true;\n        return -1;\n      }\n\n      if (decompressor.finished()) {\n        // First see if there was any leftover buffered input from previous\n        // stream; if not, attempt to refill buffer.  If refill -> EOF, we're\n        // all done; else reset, fix up input buffer, and get ready for next\n        // concatenated substream/\"member\".\n        int nRemaining = decompressor.getRemaining();\n        if (nRemaining == 0) {\n          int m = getCompressedData();\n          if (m == -1) {\n            // apparently the previous end-of-stream was also end-of-file:\n            // return success, as if we had never called getCompressedData()\n            eof = true;\n            return -1;\n          }\n          decompressor.reset();\n          decompressor.setInput(buffer, 0, m);\n          lastBytesSent = m;\n        } else {\n          // looks like it's a concatenated stream:  reset low-level zlib (or\n          // other engine) and buffers, then \"resend\" remaining input data\n          decompressor.reset();\n          int leftoverOffset = lastBytesSent - nRemaining;\n          assert (leftoverOffset >= 0);\n          // this recopies userBuf -> direct buffer if using native libraries:\n          decompressor.setInput(buffer, leftoverOffset, nRemaining);\n          // NOTE:  this is the one place we do NOT want to save the number\n          // of bytes sent (nRemaining here) into lastBytesSent:  since we\n          // are resending what we've already sent before, offset is nonzero\n          // in general (only way it could be zero is if it already equals\n          // nRemaining), which would then screw up the offset calculation\n          // _next_ time around.  IOW, getRemaining() is in terms of the\n          // original, zero-offset bufferload, so lastBytesSent must be as\n          // well.  Cheesy ASCII art:\n          //\n          //          <------------ m, lastBytesSent ----------->\n          //          +===============================================+\n          // buffer:  |1111111111|22222222222222222|333333333333|     |\n          //          +===============================================+\n          //     #1:  <-- off -->|<-------- nRemaining --------->\n          //     #2:  <----------- off ----------->|<-- nRem. -->\n          //     #3:  (final substream:  nRemaining == 0; eof = true)\n          //\n          // If lastBytesSent is anything other than m, as shown, then \"off\"\n          // will be calculated incorrectly.\n        }\n      } else if (decompressor.needsInput()) {\n        int m = getCompressedData();\n        if (m == -1) {\n          throw new EOFException(\"Unexpected end of input stream\");\n        }\n        decompressor.setInput(buffer, 0, m);\n        lastBytesSent = m;\n      }\n    }\n\n    return n;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.IOUtils.readFully": "  public static void readFully(InputStream in, byte[] buf,\n      int off, int len) throws IOException {\n    int toRead = len;\n    while (toRead > 0) {\n      int ret = in.read(buf, off, toRead);\n      if (ret < 0) {\n        throw new IOException( \"Premature EOF from inputStream\");\n      }\n      toRead -= ret;\n      off += ret;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput": "  private TaskAttemptID[] copyMapOutput(MapHost host,\n                                DataInputStream input,\n                                Set<TaskAttemptID> remaining,\n                                boolean canRetry) throws IOException {\n    MapOutput<K,V> mapOutput = null;\n    TaskAttemptID mapId = null;\n    long decompressedLength = -1;\n    long compressedLength = -1;\n    \n    try {\n      long startTime = Time.monotonicNow();\n      int forReduce = -1;\n      //Read the shuffle header\n      try {\n        ShuffleHeader header = new ShuffleHeader();\n        header.readFields(input);\n        mapId = TaskAttemptID.forName(header.mapId);\n        compressedLength = header.compressedLength;\n        decompressedLength = header.uncompressedLength;\n        forReduce = header.forReduce;\n      } catch (IllegalArgumentException e) {\n        badIdErrs.increment(1);\n        LOG.warn(\"Invalid map id \", e);\n        //Don't know which one was bad, so consider all of them as bad\n        return remaining.toArray(new TaskAttemptID[remaining.size()]);\n      }\n\n      InputStream is = input;\n      is = CryptoUtils.wrapIfNecessary(jobConf, is, compressedLength);\n      compressedLength -= CryptoUtils.cryptoPadding(jobConf);\n      decompressedLength -= CryptoUtils.cryptoPadding(jobConf);\n      \n      // Do some basic sanity verification\n      if (!verifySanity(compressedLength, decompressedLength, forReduce,\n          remaining, mapId)) {\n        return new TaskAttemptID[] {mapId};\n      }\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"header: \" + mapId + \", len: \" + compressedLength + \n            \", decomp len: \" + decompressedLength);\n      }\n      \n      // Get the location for the map output - either in-memory or on-disk\n      try {\n        mapOutput = merger.reserve(mapId, decompressedLength, id);\n      } catch (IOException ioe) {\n        // kill this reduce attempt\n        ioErrs.increment(1);\n        scheduler.reportLocalError(ioe);\n        return EMPTY_ATTEMPT_ID_ARRAY;\n      }\n      \n      // Check if we can shuffle *now* ...\n      if (mapOutput == null) {\n        LOG.info(\"fetcher#\" + id + \" - MergeManager returned status WAIT ...\");\n        //Not an error but wait to process data.\n        return EMPTY_ATTEMPT_ID_ARRAY;\n      } \n      \n      // The codec for lz0,lz4,snappy,bz2,etc. throw java.lang.InternalError\n      // on decompression failures. Catching and re-throwing as IOException\n      // to allow fetch failure logic to be processed\n      try {\n        // Go!\n        LOG.info(\"fetcher#\" + id + \" about to shuffle output of map \"\n            + mapOutput.getMapId() + \" decomp: \" + decompressedLength\n            + \" len: \" + compressedLength + \" to \" + mapOutput.getDescription());\n        mapOutput.shuffle(host, is, compressedLength, decompressedLength,\n            metrics, reporter);\n      } catch (java.lang.InternalError e) {\n        LOG.warn(\"Failed to shuffle for fetcher#\"+id, e);\n        throw new IOException(e);\n      }\n      \n      // Inform the shuffle scheduler\n      long endTime = Time.monotonicNow();\n      // Reset retryStartTime as map task make progress if retried before.\n      retryStartTime = 0;\n      \n      scheduler.copySucceeded(mapId, host, compressedLength, \n                              startTime, endTime, mapOutput);\n      // Note successful shuffle\n      remaining.remove(mapId);\n      metrics.successFetch();\n      return null;\n    } catch (IOException ioe) {\n      if (mapOutput != null) {\n        mapOutput.abort();\n      }\n\n      if (canRetry) {\n        checkTimeoutOrRetry(host, ioe);\n      } \n      \n      ioErrs.increment(1);\n      if (mapId == null || mapOutput == null) {\n        LOG.warn(\"fetcher#\" + id + \" failed to read map header\" + \n                 mapId + \" decomp: \" + \n                 decompressedLength + \", \" + compressedLength, ioe);\n        if(mapId == null) {\n          return remaining.toArray(new TaskAttemptID[remaining.size()]);\n        } else {\n          return new TaskAttemptID[] {mapId};\n        }\n      }\n        \n      LOG.warn(\"Failed to shuffle output of \" + mapId + \n               \" from \" + host.getHostName(), ioe); \n\n      // Inform the shuffle-scheduler\n      metrics.failedFetch();\n      return new TaskAttemptID[] {mapId};\n    }\n\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.Fetcher.checkTimeoutOrRetry": "  private void checkTimeoutOrRetry(MapHost host, IOException ioe)\n      throws IOException {\n    // First time to retry.\n    long currentTime = Time.monotonicNow();\n    if (retryStartTime == 0) {\n       retryStartTime = currentTime;\n    }\n  \n    // Retry is not timeout, let's do retry with throwing an exception.\n    if (currentTime - retryStartTime < this.fetchRetryTimeout) {\n      LOG.warn(\"Shuffle output from \" + host.getHostName() +\n          \" failed, retry it.\", ioe);\n      throw ioe;\n    } else {\n      // timeout, prepare to be failed.\n      LOG.warn(\"Timeout for copying MapOutput with retry on host \" + host \n          + \"after \" + fetchRetryTimeout + \" milliseconds.\");\n      \n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.Fetcher.verifySanity": "  private boolean verifySanity(long compressedLength, long decompressedLength,\n      int forReduce, Set<TaskAttemptID> remaining, TaskAttemptID mapId) {\n    if (compressedLength < 0 || decompressedLength < 0) {\n      wrongLengthErrs.increment(1);\n      LOG.warn(getName() + \" invalid lengths in map output header: id: \" +\n               mapId + \" len: \" + compressedLength + \", decomp len: \" + \n               decompressedLength);\n      return false;\n    }\n    \n    if (forReduce != reduce) {\n      wrongReduceErrs.increment(1);\n      LOG.warn(getName() + \" data for the wrong reduce map: \" +\n               mapId + \" len: \" + compressedLength + \" decomp len: \" +\n               decompressedLength + \" for reduce \" + forReduce);\n      return false;\n    }\n\n    // Sanity check\n    if (!remaining.contains(mapId)) {\n      wrongMapErrs.increment(1);\n      LOG.warn(\"Invalid map-output! Received output for \" + mapId);\n      return false;\n    }\n    \n    return true;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost": "  protected void copyFromHost(MapHost host) throws IOException {\n    // reset retryStartTime for a new host\n    retryStartTime = 0;\n    // Get completed maps on 'host'\n    List<TaskAttemptID> maps = scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only 'OBSOLETE' maps, \n    // especially at the tail of large jobs\n    if (maps.size() == 0) {\n      return;\n    }\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n        + maps);\n    }\n    \n    // List of maps to be fetched yet\n    Set<TaskAttemptID> remaining = new HashSet<TaskAttemptID>(maps);\n    \n    // Construct the url and connect\n    URL url = getMapOutputURL(host, maps);\n    DataInputStream input = openShuffleUrl(host, remaining, url);\n    if (input == null) {\n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, faildTasks is not null and we exit\n      // after putting back the remaining maps to the \n      // yet_to_be_fetched list and marking the failed tasks.\n      TaskAttemptID[] failedTasks = null;\n      while (!remaining.isEmpty() && failedTasks == null) {\n        try {\n          failedTasks = copyMapOutput(host, input, remaining, fetchRetryEnabled);\n        } catch (IOException e) {\n          IOUtils.cleanup(LOG, input);\n          //\n          // Setup connection again if disconnected by NM\n          connection.disconnect();\n          // Get map output from remaining tasks only.\n          url = getMapOutputURL(host, remaining);\n          input = openShuffleUrl(host, remaining, url);\n          if (input == null) {\n            return;\n          }\n        }\n      }\n      \n      if(failedTasks != null && failedTasks.length > 0) {\n        LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n        scheduler.hostFailed(host.getHostName());\n        for(TaskAttemptID left: failedTasks) {\n          scheduler.copyFailed(left, host, true, false);\n        }\n      }\n\n      // Sanity check\n      if (failedTasks == null && !remaining.isEmpty()) {\n        throw new IOException(\"server didn't return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n      input.close();\n      input = null;\n    } finally {\n      if (input != null) {\n        IOUtils.cleanup(LOG, input);\n        input = null;\n      }\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.Fetcher.openShuffleUrl": "  private DataInputStream openShuffleUrl(MapHost host,\n      Set<TaskAttemptID> remaining, URL url) {\n    DataInputStream input = null;\n\n    try {\n      setupConnectionsWithRetry(host, remaining, url);\n      if (stopped) {\n        abortConnect(host, remaining);\n      } else {\n        input = new DataInputStream(connection.getInputStream());\n      }\n    } catch (IOException ie) {\n      boolean connectExcpt = ie instanceof ConnectException;\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() +\n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      scheduler.hostFailed(host.getHostName());\n      for(TaskAttemptID left: remaining) {\n        scheduler.copyFailed(left, host, false, connectExcpt);\n      }\n\n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n\n    return input;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.Fetcher.getMapOutputURL": "  private URL getMapOutputURL(MapHost host, Collection<TaskAttemptID> maps\n                              )  throws MalformedURLException {\n    // Get the base url\n    StringBuffer url = new StringBuffer(host.getBaseUrl());\n    \n    boolean first = true;\n    for (TaskAttemptID mapId : maps) {\n      if (!first) {\n        url.append(\",\");\n      }\n      url.append(mapId);\n      first = false;\n    }\n   \n    LOG.debug(\"MapOutput URL for \" + host + \" -> \" + url.toString());\n    return new URL(url.toString());\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.Fetcher.run": "  public void run() {\n    try {\n      while (!stopped && !Thread.currentThread().isInterrupted()) {\n        MapHost host = null;\n        try {\n          // If merge is on, block\n          merger.waitForResource();\n\n          // Get a host to shuffle from\n          host = scheduler.getHost();\n          metrics.threadBusy();\n\n          // Shuffle\n          copyFromHost(host);\n        } finally {\n          if (host != null) {\n            scheduler.freeHost(host);\n            metrics.threadFree();            \n          }\n        }\n      }\n    } catch (InterruptedException ie) {\n      return;\n    } catch (Throwable t) {\n      exceptionReporter.reportException(t);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.fetcher.shutDown": "  public void shutDown() throws InterruptedException {\n    this.stopped = true;\n    interrupt();\n    try {\n      join(5000);\n    } catch (InterruptedException ie) {\n      LOG.warn(\"Got interrupt while joining \" + getName(), ie);\n    }\n    if (sslFactory != null) {\n      sslFactory.destroy();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.fetcher.interrupt": "  public void interrupt() {\n    try {\n      closeConnection();\n    } finally {\n      super.interrupt();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.EventFetcher.shutDown": "  public void shutDown() {\n    this.stopped = true;\n    interrupt();\n    try {\n      join(5000);\n    } catch(InterruptedException ie) {\n      LOG.warn(\"Got interrupted while joining \" + getName(), ie);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ShuffleConsumerPlugin.init": "  public void init(Context<K, V> context);\n\n  public RawKeyValueIterator run() throws IOException, InterruptedException;\n\n  public void close();\n\n  @InterfaceAudience.LimitedPrivate(\"mapreduce\")\n  @InterfaceStability.Unstable\n  public static class Context<K,V> {\n    private final org.apache.hadoop.mapreduce.TaskAttemptID reduceId;\n    private final JobConf jobConf;\n    private final FileSystem localFS;\n    private final TaskUmbilicalProtocol umbilical;\n    private final LocalDirAllocator localDirAllocator;\n    private final Reporter reporter;\n    private final CompressionCodec codec;\n    private final Class<? extends Reducer> combinerClass;\n    private final CombineOutputCollector<K, V> combineCollector;\n    private final Counters.Counter spilledRecordsCounter;\n    private final Counters.Counter reduceCombineInputCounter;\n    private final Counters.Counter shuffledMapsCounter;\n    private final Counters.Counter reduceShuffleBytes;\n    private final Counters.Counter failedShuffleCounter;\n    private final Counters.Counter mergedMapOutputsCounter;\n    private final TaskStatus status;\n    private final Progress copyPhase;\n    private final Progress mergePhase;\n    private final Task reduceTask;\n    private final MapOutputFile mapOutputFile;\n    private final Map<TaskAttemptID, MapOutputFile> localMapFiles;\n\n    public Context(org.apache.hadoop.mapreduce.TaskAttemptID reduceId,\n                   JobConf jobConf, FileSystem localFS,\n                   TaskUmbilicalProtocol umbilical,\n                   LocalDirAllocator localDirAllocator,\n                   Reporter reporter, CompressionCodec codec,\n                   Class<? extends Reducer> combinerClass,\n                   CombineOutputCollector<K,V> combineCollector,\n                   Counters.Counter spilledRecordsCounter,\n                   Counters.Counter reduceCombineInputCounter,\n                   Counters.Counter shuffledMapsCounter,\n                   Counters.Counter reduceShuffleBytes,\n                   Counters.Counter failedShuffleCounter,\n                   Counters.Counter mergedMapOutputsCounter,\n                   TaskStatus status, Progress copyPhase, Progress mergePhase,\n                   Task reduceTask, MapOutputFile mapOutputFile,\n                   Map<TaskAttemptID, MapOutputFile> localMapFiles) {\n      this.reduceId = reduceId;\n      this.jobConf = jobConf;\n      this.localFS = localFS;\n      this. umbilical = umbilical;\n      this.localDirAllocator = localDirAllocator;\n      this.reporter = reporter;\n      this.codec = codec;\n      this.combinerClass = combinerClass;\n      this.combineCollector = combineCollector;\n      this.spilledRecordsCounter = spilledRecordsCounter;\n      this.reduceCombineInputCounter = reduceCombineInputCounter;\n      this.shuffledMapsCounter = shuffledMapsCounter;\n      this.reduceShuffleBytes = reduceShuffleBytes;\n      this.failedShuffleCounter = failedShuffleCounter;\n      this.mergedMapOutputsCounter = mergedMapOutputsCounter;\n      this.status = status;\n      this.copyPhase = copyPhase;\n      this.mergePhase = mergePhase;\n      this.reduceTask = reduceTask;\n      this.mapOutputFile = mapOutputFile;\n      this.localMapFiles = localMapFiles;\n    }\n\n    public org.apache.hadoop.mapreduce.TaskAttemptID getReduceId() {\n      return reduceId;\n    }\n    public JobConf getJobConf() {\n      return jobConf;\n    }\n    public FileSystem getLocalFS() {\n      return localFS;\n    }\n    public TaskUmbilicalProtocol getUmbilical() {\n      return umbilical;\n    }\n    public LocalDirAllocator getLocalDirAllocator() {\n      return localDirAllocator;\n    }\n    public Reporter getReporter() {\n      return reporter;\n    }\n    public CompressionCodec getCodec() {\n      return codec;\n    }\n    public Class<? extends Reducer> getCombinerClass() {\n      return combinerClass;\n    }\n    public CombineOutputCollector<K, V> getCombineCollector() {\n      return combineCollector;\n    }\n    public Counters.Counter getSpilledRecordsCounter() {\n      return spilledRecordsCounter;\n    }\n    public Counters.Counter getReduceCombineInputCounter() {\n      return reduceCombineInputCounter;\n    }\n    public Counters.Counter getShuffledMapsCounter() {\n      return shuffledMapsCounter;\n    }\n    public Counters.Counter getReduceShuffleBytes() {\n      return reduceShuffleBytes;\n    }\n    public Counters.Counter getFailedShuffleCounter() {\n      return failedShuffleCounter;\n    }\n    public Counters.Counter getMergedMapOutputsCounter() {\n      return mergedMapOutputsCounter;\n    }\n    public TaskStatus getStatus() {\n      return status;\n    }\n    public Progress getCopyPhase() {\n      return copyPhase;\n    }\n    public Progress getMergePhase() {\n      return mergePhase;\n    }\n    public Task getReduceTask() {\n      return reduceTask;\n    }\n    public MapOutputFile getMapOutputFile() {\n      return mapOutputFile;\n    }\n    public Map<TaskAttemptID, MapOutputFile> getLocalMapFiles() {\n      return localMapFiles;\n    }\n  } // end of public static class Context<K,V>",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.taskCleanup": "  void taskCleanup(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    // set phase for this task\n    setPhase(TaskStatus.Phase.CLEANUP);\n    getProgress().setStatus(\"cleanup\");\n    statusUpdate(umbilical);\n    LOG.info(\"Runnning cleanup for the task\");\n    // do the cleanup\n    committer.abortTask(taskContext);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.statusUpdate": "  public void statusUpdate(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    int retries = MAX_RETRIES;\n    while (true) {\n      try {\n        if (!umbilical.statusUpdate(getTaskID(), taskStatus).getTaskFound()) {\n          LOG.warn(\"Parent died.  Exiting \"+taskId);\n          System.exit(66);\n        }\n        taskStatus.clearStatus();\n        return;\n      } catch (InterruptedException ie) {\n        Thread.currentThread().interrupt(); // interrupt ourself\n      } catch (IOException ie) {\n        LOG.warn(\"Failure sending status update: \" + \n                  StringUtils.stringifyException(ie));\n        if (--retries == 0) {\n          throw ie;\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setPhase": "  protected synchronized void setPhase(TaskStatus.Phase phase){\n    this.taskStatus.setPhase(phase); \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setStatus": "    public void setStatus(String status) {\n      taskProgress.setStatus(normalizeStatus(status, conf));\n      // indicate that progress update needs to be sent\n      setProgressFlag();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getProgress": "    public float getProgress() {\n      return taskProgress.getProgress();\n    };",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getWorkingDirectory": "  public Path getWorkingDirectory() {\n    String name = get(JobContext.WORKING_DIR);\n    if (name != null) {\n      return new Path(name);\n    } else {\n      try {\n        Path dir = FileSystem.get(this).getWorkingDirectory();\n        set(JobContext.WORKING_DIR, dir.toString());\n        return dir;\n      } catch (IOException e) {\n        throw new RuntimeException(e);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.toString": "  public String toString() { \n    return appendTo(new StringBuilder(JVM)).toString();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.appendTo": "  protected StringBuilder appendTo(StringBuilder builder) {\n    return jobId.appendTo(builder).\n                 append(SEPARATOR).\n                 append(isMap ? 'm' : 'r').\n                 append(SEPARATOR).\n                 append(idFormat.format(jvmId));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getSessionId": "  public String getSessionId() {\n      return get(\"session.id\", \"\");\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.createLogSyncer": "  public static ScheduledExecutorService createLogSyncer() {\n    final ScheduledExecutorService scheduler =\n      Executors.newSingleThreadScheduledExecutor(\n        new ThreadFactory() {\n          @Override\n          public Thread newThread(Runnable r) {\n            final Thread t = Executors.defaultThreadFactory().newThread(r);\n            t.setDaemon(true);\n            t.setName(\"Thread for syncLogs\");\n            return t;\n          }\n        });\n    ShutdownHookManager.get().addShutdownHook(new Runnable() {\n        @Override\n        public void run() {\n          TaskLog.syncLogsShutdown(scheduler);\n        }\n      }, 50);\n    scheduler.scheduleWithFixedDelay(\n        new Runnable() {\n          @Override\n          public void run() {\n            TaskLog.syncLogs();\n          }\n        }, 0L, 5L, TimeUnit.SECONDS);\n    return scheduler;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.newThread": "          public Thread newThread(Runnable r) {\n            final Thread t = Executors.defaultThreadFactory().newThread(r);\n            t.setDaemon(true);\n            t.setName(\"Thread for syncLogs\");\n            return t;\n          }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.syncLogsShutdown": "  public static synchronized void syncLogsShutdown(\n    ScheduledExecutorService scheduler) \n  {\n    // flush standard streams\n    //\n    System.out.flush();\n    System.err.flush();\n\n    if (scheduler != null) {\n      scheduler.shutdownNow();\n    }\n\n    // flush & close all appenders\n    LogManager.shutdown(); \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.syncLogs": "  public static synchronized void syncLogs() {\n    // flush standard streams\n    //\n    System.out.flush();\n    System.err.flush();\n\n    // flush flushable appenders\n    //\n    final Logger rootLogger = Logger.getRootLogger();\n    flushAppenders(rootLogger);\n    final Enumeration<Logger> allLoggers = rootLogger.getLoggerRepository().\n      getCurrentLoggers();\n    while (allLoggers.hasMoreElements()) {\n      final Logger l = allLoggers.nextElement();\n      flushAppenders(l);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fsError": "  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.shouldDie": "  public boolean shouldDie() {\n    return shouldDie;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.getTask": "  public Task getTask() {\n    return t;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.getTask": "  JvmTask getTask(JvmContext context) throws IOException;\n  \n  /**\n   * Report child's progress to parent. Also invoked to report still alive (used\n   * to be in ping). It reports an AMFeedback used to propagate preemption requests.\n   * \n   * @param taskId task-id of the child\n   * @param taskStatus status of the child\n   * @throws IOException\n   * @throws InterruptedException\n   * @return True if the task is known\n   */\n  AMFeedback statusUpdate(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;\n  \n  /** Report error messages back to parent.  Calls should be sparing, since all\n   *  such messages are held in the job tracker.\n   *  @param taskid the id of the task involved\n   *  @param trace the text to report\n   */\n  void reportDiagnosticInfo(TaskAttemptID taskid, String trace) throws IOException;\n  \n  /**\n   * Report the record range which is going to process next by the Task.\n   * @param taskid the id of the task involved\n   * @param range the range of record sequence nos\n   * @throws IOException\n   */\n  void reportNextRecordRange(TaskAttemptID taskid, SortedRanges.Range range) \n    throws IOException;\n\n  /** Report that the task is successfully completed.  Failure is assumed if\n   * the task process exits without calling this.\n   * @param taskid task's id\n   */\n  void done(TaskAttemptID taskid) throws IOException;\n  \n  /** \n   * Report that the task is complete, but its commit is pending.\n   * \n   * @param taskId task's id\n   * @param taskStatus status of the child\n   * @throws IOException\n   */\n  void commitPending(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;  \n\n  /**\n   * Polling to know whether the task can go-ahead with commit \n   * @param taskid\n   * @return true/false \n   * @throws IOException\n   */\n  boolean canCommit(TaskAttemptID taskid) throws IOException;\n\n  /** Report that a reduce-task couldn't shuffle map-outputs.*/\n  void shuffleError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Report that the task encounted a local filesystem error.*/\n  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.getJobID": "  public JobID getJobID() {\n    return (JobID) super.getJobID();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fatalError": "  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.forName": "  public static TaskAttemptID forName(String str\n                                      ) throws IllegalArgumentException {\n    return (TaskAttemptID) \n             org.apache.hadoop.mapreduce.TaskAttemptID.forName(str);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getTaskID": "  public TaskAttemptID getTaskID() { return taskId; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.decompressor.finished": "  public boolean finished();\n  \n  /**\n   * Fills specified buffer with uncompressed data. Returns actual number\n   * of bytes of uncompressed data. A return value of 0 indicates that\n   * {@link #needsInput()} should be called in order to determine if more",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.decompressor.needsDictionary": "  public boolean needsDictionary();\n\n  /**\n   * Returns <code>true</code> if the end of the decompressed \n   * data output stream has been reached. Indicates a concatenated data stream\n   * when finished() returns <code>true</code> and {@link #getRemaining()}",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.decompressor.setInput": "  public void setInput(byte[] b, int off, int len);\n  \n  /**\n   * Returns <code>true</code> if the input data buffer is empty and \n   * {@link #setInput(byte[], int, int)} should be called to",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.decompressor.needsInput": "  public boolean needsInput();\n  \n  /**\n   * Sets preset dictionary for compression. A preset dictionary\n   * is used when the history buffer can be predetermined. \n   *\n   * @param b Dictionary data bytes\n   * @param off Start offset\n   * @param len Length\n   */\n  public void setDictionary(byte[] b, int off, int len);\n  \n  /**\n   * Returns <code>true</code> if a preset dictionary is needed for decompression.\n   * @return <code>true</code> if a preset dictionary is needed for decompression\n   */\n  public boolean needsDictionary();\n\n  /**\n   * Returns <code>true</code> if the end of the decompressed \n   * data output stream has been reached. Indicates a concatenated data stream\n   * when finished() returns <code>true</code> and {@link #getRemaining()}",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.MapOutput.getDescription": "  public abstract String getDescription();\n\n  public String toString() {\n    return \"MapOutput(\" + mapId + \", \" + getDescription() + \")\";\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.MapOutput.getMapId": "  public TaskAttemptID getMapId() {\n    return mapId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.ShuffleHeader.readFields": "  public void readFields(DataInput in) throws IOException {\n    mapId = WritableUtils.readStringSafely(in, MAX_ID_LENGTH);\n    compressedLength = WritableUtils.readVLong(in);\n    uncompressedLength = WritableUtils.readVLong(in);\n    forReduce = WritableUtils.readVInt(in);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.MapOutput.shuffle": "  public abstract void shuffle(MapHost host, InputStream input,\n                               long compressedLength,\n                               long decompressedLength,\n                               ShuffleClientMetrics metrics,\n                               Reporter reporter) throws IOException;\n\n  public abstract void commit() throws IOException;\n  \n  public abstract void abort();\n\n  public abstract String getDescription();\n\n  public String toString() {\n    return \"MapOutput(\" + mapId + \", \" + getDescription() + \")\";\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.MapOutput.abort": "  public abstract void abort();\n\n  public abstract String getDescription();\n\n  public String toString() {\n    return \"MapOutput(\" + mapId + \", \" + getDescription() + \")\";\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.MapHost.getHostName": "  public String getHostName() {\n    return hostName;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.exceptionReporter.reportException": "  void reportException(Throwable t);\n}"
        },
        "bug_report": {
            "Title": "AM should retry map attempts if the reduce task encounters commpression related errors.",
            "Description": "When reduce task encounters compression related errors, AM  doesn't retry the corresponding map task.\nIn one of the case we encountered, here is the stack trace.\n{noformat}\n2016-01-27 13:44:28,915 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#29\n\tat org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n\tat com.hadoop.compression.lzo.LzoDecompressor.setInput(LzoDecompressor.java:196)\n\tat org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:104)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)\n\tat org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:192)\n\tat org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.shuffle(InMemoryMapOutput.java:97)\n\tat org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:537)\n\tat org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:336)\n\tat org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)\n{noformat}\nIn this case, the node on which the map task ran had a bad drive.\nIf the AM had retried running that map task somewhere else, the job definitely would have succeeded."
        }
    },
    {
        "filename": "MAPREDUCE-6577.json",
        "creation_time": "2015-12-17T06:46:30.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: native lz4 library not available\n\tat org.apache.hadoop.io.compress.Lz4Codec.getCompressorType(Lz4Codec.java:125)\n\tat org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:148)\n\tat org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:163)\n\tat org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:114)\n\tat org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:97)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1602)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1482)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:457)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:391)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:309)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:195)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:238)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.Lz4Codec.getCompressorType": "  public Class<? extends Compressor> getCompressorType() {\n    if (!isNativeCodeLoaded()) {\n      throw new RuntimeException(\"native lz4 library not available\");\n    }\n\n    return Lz4Compressor.class;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.Lz4Codec.isNativeCodeLoaded": "  public static boolean isNativeCodeLoaded() {\n    return NativeCodeLoader.isNativeCodeLoaded();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.CodecPool.getCompressor": "  public static Compressor getCompressor(CompressionCodec codec) {\n    return getCompressor(codec, null);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.CodecPool.updateLeaseCount": "  private static <T> void updateLeaseCount(\n      LoadingCache<Class<T>, AtomicInteger> usageCounts, T codec, int delta) {\n    if (codec != null) {\n      Class<T> codecClass = ReflectionUtils.getClass(codec);\n      usageCounts.getUnchecked(codecClass).addAndGet(delta);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.CodecPool.borrow": "  private static <T> T borrow(Map<Class<T>, Set<T>> pool,\n                             Class<? extends T> codecClass) {\n    T codec = null;\n    \n    // Check if an appropriate codec is available\n    Set<T> codecSet;\n    synchronized (pool) {\n      codecSet = pool.get(codecClass);\n    }\n\n    if (codecSet != null) {\n      synchronized (codecSet) {\n        if (!codecSet.isEmpty()) {\n          codec = codecSet.iterator().next();\n          codecSet.remove(codec);\n        }\n      }\n    }\n    \n    return codec;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.sortAndSpill": "    private void sortAndSpill() throws IOException, ClassNotFoundException,\n                                       InterruptedException {\n      //approximate the length of the output file to be the length of the\n      //buffer + header lengths for the partitions\n      final long size = distanceTo(bufstart, bufend, bufvoid) +\n                  partitions * APPROX_HEADER_LENGTH;\n      FSDataOutputStream out = null;\n      try {\n        // create spill file\n        final SpillRecord spillRec = new SpillRecord(partitions);\n        final Path filename =\n            mapOutputFile.getSpillFileForWrite(numSpills, size);\n        out = rfs.create(filename);\n\n        final int mstart = kvend / NMETA;\n        final int mend = 1 + // kvend is a valid record\n          (kvstart >= kvend\n          ? kvstart\n          : kvmeta.capacity() + kvstart) / NMETA;\n        sorter.sort(MapOutputBuffer.this, mstart, mend, reporter);\n        int spindex = mstart;\n        final IndexRecord rec = new IndexRecord();\n        final InMemValBytes value = new InMemValBytes();\n        for (int i = 0; i < partitions; ++i) {\n          IFile.Writer<K, V> writer = null;\n          try {\n            long segmentStart = out.getPos();\n            FSDataOutputStream partitionOut = CryptoUtils.wrapIfNecessary(job, out);\n            writer = new Writer<K, V>(job, partitionOut, keyClass, valClass, codec,\n                                      spilledRecordsCounter);\n            if (combinerRunner == null) {\n              // spill directly\n              DataInputBuffer key = new DataInputBuffer();\n              while (spindex < mend &&\n                  kvmeta.get(offsetFor(spindex % maxRec) + PARTITION) == i) {\n                final int kvoff = offsetFor(spindex % maxRec);\n                int keystart = kvmeta.get(kvoff + KEYSTART);\n                int valstart = kvmeta.get(kvoff + VALSTART);\n                key.reset(kvbuffer, keystart, valstart - keystart);\n                getVBytesForOffset(kvoff, value);\n                writer.append(key, value);\n                ++spindex;\n              }\n            } else {\n              int spstart = spindex;\n              while (spindex < mend &&\n                  kvmeta.get(offsetFor(spindex % maxRec)\n                            + PARTITION) == i) {\n                ++spindex;\n              }\n              // Note: we would like to avoid the combiner if we've fewer\n              // than some threshold of records for a partition\n              if (spstart != spindex) {\n                combineCollector.setWriter(writer);\n                RawKeyValueIterator kvIter =\n                  new MRResultIterator(spstart, spindex);\n                combinerRunner.combine(kvIter, combineCollector);\n              }\n            }\n\n            // close the writer\n            writer.close();\n\n            // record offsets\n            rec.startOffset = segmentStart;\n            rec.rawLength = writer.getRawLength() + CryptoUtils.cryptoPadding(job);\n            rec.partLength = writer.getCompressedLength() + CryptoUtils.cryptoPadding(job);\n            spillRec.putIndex(rec, i);\n\n            writer = null;\n          } finally {\n            if (null != writer) writer.close();\n          }\n        }\n\n        if (totalIndexCacheMemory >= indexCacheMemoryLimit) {\n          // create spill index file\n          Path indexFilename =\n              mapOutputFile.getSpillIndexFileForWrite(numSpills, partitions\n                  * MAP_OUTPUT_INDEX_RECORD_LENGTH);\n          spillRec.writeToFile(indexFilename, job);\n        } else {\n          indexCacheList.add(spillRec);\n          totalIndexCacheMemory +=\n            spillRec.size() * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n        }\n        LOG.info(\"Finished spill \" + numSpills);\n        ++numSpills;\n      } finally {\n        if (out != null) out.close();\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.distanceTo": "    int distanceTo(final int i, final int j, final int mod) {\n      return i <= j\n        ? j - i\n        : mod - i + j;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.offsetFor": "    int offsetFor(int metapos) {\n      return metapos * NMETA;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getPos": "    public long getPos() throws IOException { return rawIn.getPos(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.close": "      public void close() { }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.reset": "      public void reset(byte[] buffer, int start, int length) {\n        this.buffer = buffer;\n        this.start = start;\n        this.length = length;\n\n        if (start + length > bufvoid) {\n          this.buffer = new byte[this.length];\n          final int taillen = bufvoid - start;\n          System.arraycopy(buffer, start, this.buffer, 0, taillen);\n          System.arraycopy(buffer, 0, this.buffer, taillen, length-taillen);\n          this.start = 0;\n        }\n\n        super.reset(this.buffer, this.start, this.length);\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getVBytesForOffset": "    private void getVBytesForOffset(int kvoff, InMemValBytes vbytes) {\n      // get the keystart for the next serialized value to be the end\n      // of this value. If this is the last value in the buffer, use bufend\n      final int vallen = kvmeta.get(kvoff + VALLEN);\n      assert vallen >= 0;\n      vbytes.reset(kvbuffer, kvmeta.get(kvoff + VALSTART), vallen);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.flush": "    public void flush() throws IOException, ClassNotFoundException,\n           InterruptedException {\n      LOG.info(\"Starting flush of map output\");\n      if (kvbuffer == null) {\n        LOG.info(\"kvbuffer is null. Skipping flush.\");\n        return;\n      }\n      spillLock.lock();\n      try {\n        while (spillInProgress) {\n          reporter.progress();\n          spillDone.await();\n        }\n        checkSpillException();\n\n        final int kvbend = 4 * kvend;\n        if ((kvbend + METASIZE) % kvbuffer.length !=\n            equator - (equator % METASIZE)) {\n          // spill finished\n          resetSpill();\n        }\n        if (kvindex != kvend) {\n          kvend = (kvindex + NMETA) % kvmeta.capacity();\n          bufend = bufmark;\n          LOG.info(\"Spilling map output\");\n          LOG.info(\"bufstart = \" + bufstart + \"; bufend = \" + bufmark +\n                   \"; bufvoid = \" + bufvoid);\n          LOG.info(\"kvstart = \" + kvstart + \"(\" + (kvstart * 4) +\n                   \"); kvend = \" + kvend + \"(\" + (kvend * 4) +\n                   \"); length = \" + (distanceTo(kvend, kvstart,\n                         kvmeta.capacity()) + 1) + \"/\" + maxRec);\n          sortAndSpill();\n        }\n      } catch (InterruptedException e) {\n        throw new IOException(\"Interrupted while waiting for the writer\", e);\n      } finally {\n        spillLock.unlock();\n      }\n      assert !spillLock.isHeldByCurrentThread();\n      // shut down spill thread and wait for it to exit. Since the preceding\n      // ensures that it is finished with its work (and sortAndSpill did not\n      // throw), we elect to use an interrupt instead of setting a flag.\n      // Spilling simultaneously from this thread while the spill thread\n      // finishes its work might be both a useful way to extend this and also\n      // sufficient motivation for the latter approach.\n      try {\n        spillThread.interrupt();\n        spillThread.join();\n      } catch (InterruptedException e) {\n        throw new IOException(\"Spill failed\", e);\n      }\n      // release sort buffer before the merge\n      kvbuffer = null;\n      mergeParts();\n      Path outputPath = mapOutputFile.getOutputFile();\n      fileOutputByteCounter.increment(rfs.getFileStatus(outputPath).getLen());\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.mergeParts": "    private void mergeParts() throws IOException, InterruptedException, \n                                     ClassNotFoundException {\n      // get the approximate size of the final output/index files\n      long finalOutFileSize = 0;\n      long finalIndexFileSize = 0;\n      final Path[] filename = new Path[numSpills];\n      final TaskAttemptID mapId = getTaskID();\n\n      for(int i = 0; i < numSpills; i++) {\n        filename[i] = mapOutputFile.getSpillFile(i);\n        finalOutFileSize += rfs.getFileStatus(filename[i]).getLen();\n      }\n      if (numSpills == 1) { //the spill is the final output\n        sameVolRename(filename[0],\n            mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n        if (indexCacheList.size() == 0) {\n          sameVolRename(mapOutputFile.getSpillIndexFile(0),\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n        } else {\n          indexCacheList.get(0).writeToFile(\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n        }\n        sortPhase.complete();\n        return;\n      }\n\n      // read in paged indices\n      for (int i = indexCacheList.size(); i < numSpills; ++i) {\n        Path indexFileName = mapOutputFile.getSpillIndexFile(i);\n        indexCacheList.add(new SpillRecord(indexFileName, job));\n      }\n\n      //make correction in the length to include the sequence file header\n      //lengths for each partition\n      finalOutFileSize += partitions * APPROX_HEADER_LENGTH;\n      finalIndexFileSize = partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n      Path finalOutputFile =\n          mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n      Path finalIndexFile =\n          mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n\n      //The output stream for the final single output file\n      FSDataOutputStream finalOut = rfs.create(finalOutputFile, true, 4096);\n\n      if (numSpills == 0) {\n        //create dummy files\n        IndexRecord rec = new IndexRecord();\n        SpillRecord sr = new SpillRecord(partitions);\n        try {\n          for (int i = 0; i < partitions; i++) {\n            long segmentStart = finalOut.getPos();\n            FSDataOutputStream finalPartitionOut = CryptoUtils.wrapIfNecessary(job, finalOut);\n            Writer<K, V> writer =\n              new Writer<K, V>(job, finalPartitionOut, keyClass, valClass, codec, null);\n            writer.close();\n            rec.startOffset = segmentStart;\n            rec.rawLength = writer.getRawLength() + CryptoUtils.cryptoPadding(job);\n            rec.partLength = writer.getCompressedLength() + CryptoUtils.cryptoPadding(job);\n            sr.putIndex(rec, i);\n          }\n          sr.writeToFile(finalIndexFile, job);\n        } finally {\n          finalOut.close();\n        }\n        sortPhase.complete();\n        return;\n      }\n      {\n        sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n        \n        IndexRecord rec = new IndexRecord();\n        final SpillRecord spillRec = new SpillRecord(partitions);\n        for (int parts = 0; parts < partitions; parts++) {\n          //create the segments to be merged\n          List<Segment<K,V>> segmentList =\n            new ArrayList<Segment<K, V>>(numSpills);\n          for(int i = 0; i < numSpills; i++) {\n            IndexRecord indexRecord = indexCacheList.get(i).getIndex(parts);\n\n            Segment<K,V> s =\n              new Segment<K,V>(job, rfs, filename[i], indexRecord.startOffset,\n                               indexRecord.partLength, codec, true);\n            segmentList.add(i, s);\n\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"MapId=\" + mapId + \" Reducer=\" + parts +\n                  \"Spill =\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                  indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n            }\n          }\n\n          int mergeFactor = job.getInt(JobContext.IO_SORT_FACTOR, 100);\n          // sort the segments only if there are intermediate merges\n          boolean sortSegments = segmentList.size() > mergeFactor;\n          //merge\n          @SuppressWarnings(\"unchecked\")\n          RawKeyValueIterator kvIter = Merger.merge(job, rfs,\n                         keyClass, valClass, codec,\n                         segmentList, mergeFactor,\n                         new Path(mapId.toString()),\n                         job.getOutputKeyComparator(), reporter, sortSegments,\n                         null, spilledRecordsCounter, sortPhase.phase(),\n                         TaskType.MAP);\n\n          //write merged output to disk\n          long segmentStart = finalOut.getPos();\n          FSDataOutputStream finalPartitionOut = CryptoUtils.wrapIfNecessary(job, finalOut);\n          Writer<K, V> writer =\n              new Writer<K, V>(job, finalPartitionOut, keyClass, valClass, codec,\n                               spilledRecordsCounter);\n          if (combinerRunner == null || numSpills < minSpillsForCombine) {\n            Merger.writeFile(kvIter, writer, reporter, job);\n          } else {\n            combineCollector.setWriter(writer);\n            combinerRunner.combine(kvIter, combineCollector);\n          }\n\n          //close\n          writer.close();\n\n          sortPhase.startNextPhase();\n          \n          // record offsets\n          rec.startOffset = segmentStart;\n          rec.rawLength = writer.getRawLength() + CryptoUtils.cryptoPadding(job);\n          rec.partLength = writer.getCompressedLength() + CryptoUtils.cryptoPadding(job);\n          spillRec.putIndex(rec, parts);\n        }\n        spillRec.writeToFile(finalIndexFile, job);\n        finalOut.close();\n        for(int i = 0; i < numSpills; i++) {\n          rfs.delete(filename[i],true);\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.resetSpill": "    private void resetSpill() {\n      final int e = equator;\n      bufstart = bufend = e;\n      final int aligned = e - (e % METASIZE);\n      // set start/end to point to first meta record\n      // Cast one of the operands to long to avoid integer overflow\n      kvstart = kvend = (int)\n        (((long)aligned - METASIZE + kvbuffer.length) % kvbuffer.length) / 4;\n      LOG.info(\"(RESET) equator \" + e + \" kv \" + kvstart + \"(\" +\n        (kvstart * 4) + \")\" + \" kvi \" + kvindex + \"(\" + (kvindex * 4) + \")\");\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.checkSpillException": "    private void checkSpillException() throws IOException {\n      final Throwable lspillException = sortSpillException;\n      if (lspillException != null) {\n        if (lspillException instanceof Error) {\n          final String logMsg = \"Task \" + getTaskID() + \" failed : \" +\n            StringUtils.stringifyException(lspillException);\n          mapTask.reportFatalError(getTaskID(), lspillException, logMsg);\n        }\n        throw new IOException(\"Spill failed\", lspillException);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.runOldMapper": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runOldMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, InterruptedException,\n                             ClassNotFoundException {\n    InputSplit inputSplit = getSplitDetails(new Path(splitIndex.getSplitLocation()),\n           splitIndex.getStartOffset());\n\n    updateJobWithSplit(job, inputSplit);\n    reporter.setInputSplit(inputSplit);\n\n    RecordReader<INKEY,INVALUE> in = isSkipping() ? \n        new SkippingRecordReader<INKEY,INVALUE>(umbilical, reporter, job) :\n          new TrackedRecordReader<INKEY,INVALUE>(reporter, job);\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n\n\n    int numReduceTasks = conf.getNumReduceTasks();\n    LOG.info(\"numReduceTasks: \" + numReduceTasks);\n    MapOutputCollector<OUTKEY, OUTVALUE> collector = null;\n    if (numReduceTasks > 0) {\n      collector = createSortingCollector(job, reporter);\n    } else { \n      collector = new DirectMapOutputCollector<OUTKEY, OUTVALUE>();\n       MapOutputCollector.Context context =\n                           new MapOutputCollector.Context(this, job, reporter);\n      collector.init(context);\n    }\n    MapRunnable<INKEY,INVALUE,OUTKEY,OUTVALUE> runner =\n      ReflectionUtils.newInstance(job.getMapRunnerClass(), job);\n\n    try {\n      runner.run(in, new OldOutputCollector(collector, conf), reporter);\n      mapPhase.complete();\n      // start the sort phase only if there are reducers\n      if (numReduceTasks > 0) {\n        setPhase(TaskStatus.Phase.SORT);\n      }\n      statusUpdate(umbilical);\n      collector.flush();\n      \n      in.close();\n      in = null;\n      \n      collector.close();\n      collector = null;\n    } finally {\n      closeQuietly(in);\n      closeQuietly(collector);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.createSortingCollector": "  private <KEY, VALUE> MapOutputCollector<KEY, VALUE>\n          createSortingCollector(JobConf job, TaskReporter reporter)\n    throws IOException, ClassNotFoundException {\n    MapOutputCollector.Context context =\n      new MapOutputCollector.Context(this, job, reporter);\n\n    Class<?>[] collectorClasses = job.getClasses(\n      JobContext.MAP_OUTPUT_COLLECTOR_CLASS_ATTR, MapOutputBuffer.class);\n    int remainingCollectors = collectorClasses.length;\n    Exception lastException = null;\n    for (Class clazz : collectorClasses) {\n      try {\n        if (!MapOutputCollector.class.isAssignableFrom(clazz)) {\n          throw new IOException(\"Invalid output collector class: \" + clazz.getName() +\n            \" (does not implement MapOutputCollector)\");\n        }\n        Class<? extends MapOutputCollector> subclazz =\n          clazz.asSubclass(MapOutputCollector.class);\n        LOG.debug(\"Trying map output collector class: \" + subclazz.getName());\n        MapOutputCollector<KEY, VALUE> collector =\n          ReflectionUtils.newInstance(subclazz, job);\n        collector.init(context);\n        LOG.info(\"Map output collector class = \" + collector.getClass().getName());\n        return collector;\n      } catch (Exception e) {\n        String msg = \"Unable to initialize MapOutputCollector \" + clazz.getName();\n        if (--remainingCollectors > 0) {\n          msg += \" (\" + remainingCollectors + \" more collector(s) to try)\";\n        }\n        lastException = e;\n        LOG.warn(msg, e);\n      }\n    }\n    throw new IOException(\"Initialization of all the collectors failed. \" +\n      \"Error in last collector was :\" + lastException.getMessage(), lastException);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.updateJobWithSplit": "  private void updateJobWithSplit(final JobConf job, InputSplit inputSplit) {\n    if (inputSplit instanceof FileSplit) {\n      FileSplit fileSplit = (FileSplit) inputSplit;\n      job.set(JobContext.MAP_INPUT_FILE, fileSplit.getPath().toString());\n      job.setLong(JobContext.MAP_INPUT_START, fileSplit.getStart());\n      job.setLong(JobContext.MAP_INPUT_PATH, fileSplit.getLength());\n    }\n    LOG.info(\"Processing split: \" + inputSplit);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.run": "      public void run() {\n        spillLock.lock();\n        spillThreadRunning = true;\n        try {\n          while (true) {\n            spillDone.signal();\n            while (!spillInProgress) {\n              spillReady.await();\n            }\n            try {\n              spillLock.unlock();\n              sortAndSpill();\n            } catch (Throwable t) {\n              sortSpillException = t;\n            } finally {\n              spillLock.lock();\n              if (bufend < bufstart) {\n                bufvoid = kvbuffer.length;\n              }\n              kvstart = kvend;\n              bufstart = bufend;\n              spillInProgress = false;\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        } finally {\n          spillLock.unlock();\n          spillThreadRunning = false;\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getSplitDetails": " private <T> T getSplitDetails(Path file, long offset) \n  throws IOException {\n   FileSystem fs = file.getFileSystem(conf);\n   FSDataInputStream inFile = fs.open(file);\n   inFile.seek(offset);\n   String className = StringInterner.weakIntern(Text.readString(inFile));\n   Class<T> cls;\n   try {\n     cls = (Class<T>) conf.getClassByName(className);\n   } catch (ClassNotFoundException ce) {\n     IOException wrap = new IOException(\"Split class \" + className + \n                                         \" not found\");\n     wrap.initCause(ce);\n     throw wrap;\n   }\n   SerializationFactory factory = new SerializationFactory(conf);\n   Deserializer<T> deserializer = \n     (Deserializer<T>) factory.getDeserializer(cls);\n   deserializer.open(inFile);\n   T split = deserializer.deserialize(null);\n   long pos = inFile.getPos();\n   getCounters().findCounter(\n       TaskCounter.SPLIT_RAW_BYTES).increment(pos - offset);\n   inFile.close();\n   return split;\n }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.init": "    public void init(MapOutputCollector.Context context\n                    ) throws IOException, ClassNotFoundException {\n      job = context.getJobConf();\n      reporter = context.getReporter();\n      mapTask = context.getMapTask();\n      mapOutputFile = mapTask.getMapOutputFile();\n      sortPhase = mapTask.getSortPhase();\n      spilledRecordsCounter = reporter.getCounter(TaskCounter.SPILLED_RECORDS);\n      partitions = job.getNumReduceTasks();\n      rfs = ((LocalFileSystem)FileSystem.getLocal(job)).getRaw();\n\n      //sanity checks\n      final float spillper =\n        job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);\n      final int sortmb = job.getInt(JobContext.IO_SORT_MB, 100);\n      indexCacheMemoryLimit = job.getInt(JobContext.INDEX_CACHE_MEMORY_LIMIT,\n                                         INDEX_CACHE_MEMORY_LIMIT_DEFAULT);\n      if (spillper > (float)1.0 || spillper <= (float)0.0) {\n        throw new IOException(\"Invalid \\\"\" + JobContext.MAP_SORT_SPILL_PERCENT +\n            \"\\\": \" + spillper);\n      }\n      if ((sortmb & 0x7FF) != sortmb) {\n        throw new IOException(\n            \"Invalid \\\"\" + JobContext.IO_SORT_MB + \"\\\": \" + sortmb);\n      }\n      sorter = ReflectionUtils.newInstance(job.getClass(\n                   MRJobConfig.MAP_SORT_CLASS, QuickSort.class,\n                   IndexedSorter.class), job);\n      // buffers and accounting\n      int maxMemUsage = sortmb << 20;\n      maxMemUsage -= maxMemUsage % METASIZE;\n      kvbuffer = new byte[maxMemUsage];\n      bufvoid = kvbuffer.length;\n      kvmeta = ByteBuffer.wrap(kvbuffer)\n         .order(ByteOrder.nativeOrder())\n         .asIntBuffer();\n      setEquator(0);\n      bufstart = bufend = bufindex = equator;\n      kvstart = kvend = kvindex;\n\n      maxRec = kvmeta.capacity() / NMETA;\n      softLimit = (int)(kvbuffer.length * spillper);\n      bufferRemaining = softLimit;\n      LOG.info(JobContext.IO_SORT_MB + \": \" + sortmb);\n      LOG.info(\"soft limit at \" + softLimit);\n      LOG.info(\"bufstart = \" + bufstart + \"; bufvoid = \" + bufvoid);\n      LOG.info(\"kvstart = \" + kvstart + \"; length = \" + maxRec);\n\n      // k/v serialization\n      comparator = job.getOutputKeyComparator();\n      keyClass = (Class<K>)job.getMapOutputKeyClass();\n      valClass = (Class<V>)job.getMapOutputValueClass();\n      serializationFactory = new SerializationFactory(job);\n      keySerializer = serializationFactory.getSerializer(keyClass);\n      keySerializer.open(bb);\n      valSerializer = serializationFactory.getSerializer(valClass);\n      valSerializer.open(bb);\n\n      // output counters\n      mapOutputByteCounter = reporter.getCounter(TaskCounter.MAP_OUTPUT_BYTES);\n      mapOutputRecordCounter =\n        reporter.getCounter(TaskCounter.MAP_OUTPUT_RECORDS);\n      fileOutputByteCounter = reporter\n          .getCounter(TaskCounter.MAP_OUTPUT_MATERIALIZED_BYTES);\n\n      // compression\n      if (job.getCompressMapOutput()) {\n        Class<? extends CompressionCodec> codecClass =\n          job.getMapOutputCompressorClass(DefaultCodec.class);\n        codec = ReflectionUtils.newInstance(codecClass, job);\n      } else {\n        codec = null;\n      }\n\n      // combiner\n      final Counters.Counter combineInputCounter =\n        reporter.getCounter(TaskCounter.COMBINE_INPUT_RECORDS);\n      combinerRunner = CombinerRunner.create(job, getTaskID(), \n                                             combineInputCounter,\n                                             reporter, null);\n      if (combinerRunner != null) {\n        final Counters.Counter combineOutputCounter =\n          reporter.getCounter(TaskCounter.COMBINE_OUTPUT_RECORDS);\n        combineCollector= new CombineOutputCollector<K,V>(combineOutputCounter, reporter, job);\n      } else {\n        combineCollector = null;\n      }\n      spillInProgress = false;\n      minSpillsForCombine = job.getInt(JobContext.MAP_COMBINE_MIN_SPILLS, 3);\n      spillThread.setDaemon(true);\n      spillThread.setName(\"SpillThread\");\n      spillLock.lock();\n      try {\n        spillThread.start();\n        while (!spillThreadRunning) {\n          spillDone.await();\n        }\n      } catch (InterruptedException e) {\n        throw new IOException(\"Spill thread failed to initialize\", e);\n      } finally {\n        spillLock.unlock();\n      }\n      if (sortSpillException != null) {\n        throw new IOException(\"Spill thread failed to initialize\",\n            sortSpillException);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.closeQuietly": "  private <INKEY, INVALUE, OUTKEY, OUTVALUE>\n  void closeQuietly(\n      org.apache.hadoop.mapreduce.RecordWriter<OUTKEY, OUTVALUE> c,\n      org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context\n          mapperContext) {\n    if (c != null) {\n      try {\n        c.close(mapperContext);\n      } catch (Exception ie) {\n        // Ignore\n        LOG.info(\"Ignoring exception during close for \" + c, ie);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.LocalContainerLauncher.runSubtask": "    private void runSubtask(org.apache.hadoop.mapred.Task task,\n                            final TaskType taskType,\n                            TaskAttemptId attemptID,\n                            final int numMapTasks,\n                            boolean renameOutputs,\n                            Map<TaskAttemptID, MapOutputFile> localMapFiles)\n    throws RuntimeException, IOException {\n      org.apache.hadoop.mapred.TaskAttemptID classicAttemptID =\n          TypeConverter.fromYarn(attemptID);\n\n      try {\n        JobConf conf = new JobConf(getConfig());\n        conf.set(JobContext.TASK_ID, task.getTaskID().toString());\n        conf.set(JobContext.TASK_ATTEMPT_ID, classicAttemptID.toString());\n        conf.setBoolean(JobContext.TASK_ISMAP, (taskType == TaskType.MAP));\n        conf.setInt(JobContext.TASK_PARTITION, task.getPartition());\n        conf.set(JobContext.ID, task.getJobID().toString());\n\n        // Use the AM's local dir env to generate the intermediate step \n        // output files\n        String[] localSysDirs = StringUtils.getTrimmedStrings(\n            System.getenv(Environment.LOCAL_DIRS.name()));\n        conf.setStrings(MRConfig.LOCAL_DIR, localSysDirs);\n        LOG.info(MRConfig.LOCAL_DIR + \" for uber task: \"\n            + conf.get(MRConfig.LOCAL_DIR));\n\n        // mark this as an uberized subtask so it can set task counter\n        // (longer-term/FIXME:  could redefine as job counter and send\n        // \"JobCounterEvent\" to JobImpl on [successful] completion of subtask;\n        // will need new Job state-machine transition and JobImpl jobCounters\n        // map to handle)\n        conf.setBoolean(\"mapreduce.task.uberized\", true);\n\n        // Check and handle Encrypted spill key\n        task.setEncryptedSpillKey(encryptedSpillKey);\n        YarnChild.setEncryptedSpillKeyIfRequired(task);\n\n        // META-FIXME: do we want the extra sanity-checking (doneWithMaps,\n        // etc.), or just assume/hope the state machine(s) and uber-AM work\n        // as expected?\n        if (taskType == TaskType.MAP) {\n          if (doneWithMaps) {\n            LOG.error(\"CONTAINER_REMOTE_LAUNCH contains a map task (\"\n                      + attemptID + \"), but should be finished with maps\");\n            throw new RuntimeException();\n          }\n\n          MapTask map = (MapTask)task;\n          map.setConf(conf);\n\n          map.run(conf, umbilical);\n\n          if (renameOutputs) {\n            MapOutputFile renamed = renameMapOutputForReduce(conf, attemptID,\n                map.getMapOutputFile());\n            localMapFiles.put(classicAttemptID, renamed);\n          }\n          relocalize();\n\n          if (++finishedSubMaps == numMapTasks) {\n            doneWithMaps = true;\n          }\n\n        } else /* TaskType.REDUCE */ {\n\n          if (!doneWithMaps) {\n            // check if event-queue empty?  whole idea of counting maps vs. \n            // checking event queue is a tad wacky...but could enforce ordering\n            // (assuming no \"lost events\") at LocalMRAppMaster [CURRENT BUG(?): \n            // doesn't send reduce event until maps all done]\n            LOG.error(\"CONTAINER_REMOTE_LAUNCH contains a reduce task (\"\n                      + attemptID + \"), but not yet finished with maps\");\n            throw new RuntimeException();\n          }\n\n          // a.k.a. \"mapreduce.jobtracker.address\" in LocalJobRunner:\n          // set framework name to local to make task local\n          conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\n          conf.set(MRConfig.MASTER_ADDRESS, \"local\");  // bypass shuffle\n\n          ReduceTask reduce = (ReduceTask)task;\n          reduce.setLocalMapFiles(localMapFiles);\n          reduce.setConf(conf);          \n\n          reduce.run(conf, umbilical);\n          relocalize();\n        }\n\n      } catch (FSError e) {\n        LOG.fatal(\"FSError from child\", e);\n        // umbilical:  MRAppMaster creates (taskAttemptListener), passes to us\n        if (!ShutdownHookManager.get().isShutdownInProgress()) {\n          umbilical.fsError(classicAttemptID, e.getMessage());\n        }\n        throw new RuntimeException();\n\n      } catch (Exception exception) {\n        LOG.warn(\"Exception running local (uberized) 'child' : \"\n            + StringUtils.stringifyException(exception));\n        try {\n          if (task != null) {\n            // do cleanup for the task\n            task.taskCleanup(umbilical);\n          }\n        } catch (Exception e) {\n          LOG.info(\"Exception cleaning up: \"\n              + StringUtils.stringifyException(e));\n        }\n        // Report back any failures, for diagnostic purposes\n        umbilical.reportDiagnosticInfo(classicAttemptID, \n            StringUtils.stringifyException(exception));\n        throw new RuntimeException();\n\n      } catch (Throwable throwable) {\n        LOG.fatal(\"Error running local (uberized) 'child' : \"\n            + StringUtils.stringifyException(throwable));\n        if (!ShutdownHookManager.get().isShutdownInProgress()) {\n          Throwable tCause = throwable.getCause();\n          String cause =\n              (tCause == null) ? throwable.getMessage() : StringUtils\n                  .stringifyException(tCause);\n          umbilical.fatalError(classicAttemptID, cause);\n        }\n        throw new RuntimeException();\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.LocalContainerLauncher.run": "            public void run() {\n              runTask(launchEv, localMapFiles);\n            }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.LocalContainerLauncher.renameMapOutputForReduce": "  protected static MapOutputFile renameMapOutputForReduce(JobConf conf,\n      TaskAttemptId mapId, MapOutputFile subMapOutputFile) throws IOException {\n    FileSystem localFs = FileSystem.getLocal(conf);\n    // move map output to reduce input\n    Path mapOut = subMapOutputFile.getOutputFile();\n    FileStatus mStatus = localFs.getFileStatus(mapOut);\n    Path reduceIn = subMapOutputFile.getInputFileForWrite(\n        TypeConverter.fromYarn(mapId).getTaskID(), mStatus.getLen());\n    Path mapOutIndex = subMapOutputFile.getOutputIndexFile();\n    Path reduceInIndex = new Path(reduceIn.toString() + \".index\");\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Renaming map output file for task attempt \"\n          + mapId.toString() + \" from original location \" + mapOut.toString()\n          + \" to destination \" + reduceIn.toString());\n    }\n    if (!localFs.mkdirs(reduceIn.getParent())) {\n      throw new IOException(\"Mkdirs failed to create \"\n          + reduceIn.getParent().toString());\n    }\n    if (!localFs.rename(mapOut, reduceIn))\n      throw new IOException(\"Couldn't rename \" + mapOut);\n    if (!localFs.rename(mapOutIndex, reduceInIndex))\n      throw new IOException(\"Couldn't rename \" + mapOutIndex);\n\n    return new RenamedMapOutputFile(reduceIn);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.LocalContainerLauncher.relocalize": "    private void relocalize() {\n      File[] curLocalFiles = curDir.listFiles();\n      for (int j = 0; j < curLocalFiles.length; ++j) {\n        if (!localizedFiles.contains(curLocalFiles[j])) {\n          // found one that wasn't there before:  delete it\n          boolean deleted = false;\n          try {\n            if (curFC != null) {\n              // this is recursive, unlike File delete():\n              deleted = curFC.delete(new Path(curLocalFiles[j].getName()),true);\n            }\n          } catch (IOException e) {\n            deleted = false;\n          }\n          if (!deleted) {\n            LOG.warn(\"Unable to delete unexpected local file/dir \"\n                + curLocalFiles[j].getName() + \": insufficient permissions?\");\n          }\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.LocalContainerLauncher.setEncryptedSpillKey": "  public void setEncryptedSpillKey(byte[] encryptedSpillKey) {\n    if (encryptedSpillKey != null) {\n      this.encryptedSpillKey = encryptedSpillKey;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.LocalContainerLauncher.runTask": "    private void runTask(ContainerRemoteLaunchEvent launchEv,\n        Map<TaskAttemptID, MapOutputFile> localMapFiles) {\n      TaskAttemptId attemptID = launchEv.getTaskAttemptID(); \n\n      Job job = context.getAllJobs().get(attemptID.getTaskId().getJobId());\n      int numMapTasks = job.getTotalMaps();\n      int numReduceTasks = job.getTotalReduces();\n\n      // YARN (tracking) Task:\n      org.apache.hadoop.mapreduce.v2.app.job.Task ytask =\n          job.getTask(attemptID.getTaskId());\n      // classic mapred Task:\n      org.apache.hadoop.mapred.Task remoteTask = launchEv.getRemoteTask();\n\n      // after \"launching,\" send launched event to task attempt to move\n      // state from ASSIGNED to RUNNING (also nukes \"remoteTask\", so must\n      // do getRemoteTask() call first)\n      \n      //There is no port number because we are not really talking to a task\n      // tracker.  The shuffle is just done through local files.  So the\n      // port number is set to -1 in this case.\n      context.getEventHandler().handle(\n          new TaskAttemptContainerLaunchedEvent(attemptID, -1));\n\n      if (numMapTasks == 0) {\n        doneWithMaps = true;\n      }\n\n      try {\n        if (remoteTask.isMapOrReduce()) {\n          JobCounterUpdateEvent jce = new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n          jce.addCounterUpdate(JobCounter.TOTAL_LAUNCHED_UBERTASKS, 1);\n          if (remoteTask.isMapTask()) {\n            jce.addCounterUpdate(JobCounter.NUM_UBER_SUBMAPS, 1);\n          } else {\n            jce.addCounterUpdate(JobCounter.NUM_UBER_SUBREDUCES, 1);\n          }\n          context.getEventHandler().handle(jce);\n        }\n        runSubtask(remoteTask, ytask.getType(), attemptID, numMapTasks,\n                   (numReduceTasks > 0), localMapFiles);\n\n        // In non-uber mode, TA gets TA_CONTAINER_COMPLETED from MRAppMaster\n        // as part of NM -> RM -> AM notification route.\n        // In uber mode, given the task run inside the MRAppMaster container,\n        // we have to simulate the notification.\n        context.getEventHandler().handle(new TaskAttemptEvent(attemptID,\n            TaskAttemptEventType.TA_CONTAINER_COMPLETED));\n\n      } catch (RuntimeException re) {\n        JobCounterUpdateEvent jce = new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n        jce.addCounterUpdate(JobCounter.NUM_FAILED_UBERTASKS, 1);\n        context.getEventHandler().handle(jce);\n        // this is our signal that the subtask failed in some way, so\n        // simulate a failed JVM/container and send a container-completed\n        // event to task attempt (i.e., move state machine from RUNNING\n        // to FAIL_CONTAINER_CLEANUP [and ultimately to FAILED])\n        context.getEventHandler().handle(new TaskAttemptEvent(attemptID,\n            TaskAttemptEventType.TA_CONTAINER_COMPLETED));\n      } catch (IOException ioe) {\n        // if umbilical itself barfs (in error-handler of runSubMap()),\n        // we're pretty much hosed, so do what YarnChild main() does\n        // (i.e., exit clumsily--but can never happen, so no worries!)\n        LOG.fatal(\"oopsie...  this can never happen: \"\n            + StringUtils.stringifyException(ioe));\n        ExitUtil.terminate(-1);\n      } finally {\n        // remove my future\n        if (futures.remove(attemptID) != null) {\n          LOG.info(\"removed attempt \" + attemptID +\n              \" from the futures to keep track of\");\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.LocalContainerLauncher.handle": "  public void handle(ContainerLauncherEvent event) {\n    try {\n      eventQueue.put(event);\n    } catch (InterruptedException e) {\n      throw new YarnRuntimeException(e);  // FIXME? YarnRuntimeException is \"for runtime exceptions only\"\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.Compressor.reinit": "  public void reinit(Configuration conf);\n}",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.getCompressedLength": "    public long getCompressedLength() {\n      return compressedBytesWritten;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.getRawLength": "    public long getRawLength() {\n      return decompressedBytesWritten;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.SpillRecord.putIndex": "  public void putIndex(IndexRecord rec, int partition) {\n    final int pos = partition * MapTask.MAP_OUTPUT_INDEX_RECORD_LENGTH / 8;\n    entries.put(pos, rec.startOffset);\n    entries.put(pos + 1, rec.rawLength);\n    entries.put(pos + 2, rec.partLength);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.SpillRecord.writeToFile": "  public void writeToFile(Path loc, JobConf job, Checksum crc)\n      throws IOException {\n    final FileSystem rfs = FileSystem.getLocal(job).getRaw();\n    CheckedOutputStream chk = null;\n    final FSDataOutputStream out = rfs.create(loc);\n    try {\n      if (crc != null) {\n        crc.reset();\n        chk = new CheckedOutputStream(out, crc);\n        chk.write(buf.array());\n        out.writeLong(chk.getChecksum().getValue());\n      } else {\n        out.write(buf.array());\n      }\n    } finally {\n      if (chk != null) {\n        chk.close();\n      } else {\n        out.close();\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.SpillRecord.size": "  public int size() {\n    return entries.capacity() / (MapTask.MAP_OUTPUT_INDEX_RECORD_LENGTH / 8);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.append": "    public void append(DataInputBuffer key, DataInputBuffer value)\n    throws IOException {\n      int keyLength = key.getLength() - key.getPosition();\n      if (keyLength < 0) {\n        throw new IOException(\"Negative key-length not allowed: \" + keyLength + \n                              \" for \" + key);\n      }\n      \n      int valueLength = value.getLength() - value.getPosition();\n      if (valueLength < 0) {\n        throw new IOException(\"Negative value-length not allowed: \" + \n                              valueLength + \" for \" + value);\n      }\n\n      WritableUtils.writeVInt(out, keyLength);\n      WritableUtils.writeVInt(out, valueLength);\n      out.write(key.getData(), key.getPosition(), keyLength); \n      out.write(value.getData(), value.getPosition(), valueLength); \n\n      // Update bytes written\n      decompressedBytesWritten += keyLength + valueLength + \n                      WritableUtils.getVIntSize(keyLength) + \n                      WritableUtils.getVIntSize(valueLength);\n      ++numRecordsWritten;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.reset": "    public void reset(int offset) {\n      return;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.getPosition": "    public long getPosition() throws IOException {    \n      return checksumIn.getPosition(); \n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.getLength": "    public long getLength() { \n      return fileLength - checksumIn.getSize();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.mapOutputFile.getSpillFileForWrite": "  public abstract Path getSpillFileForWrite(int spillNumber, long size)\n      throws IOException;\n\n  /**\n   * Return a local map spill index file created earlier\n   *\n   * @param spillNumber the number\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillIndexFile(int spillNumber) throws IOException;\n\n  /**\n   * Create a local map spill index file name.\n   *\n   * @param spillNumber the number\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillIndexFileForWrite(int spillNumber, long size)\n      throws IOException;\n\n  /**\n   * Return a local reduce input file created earlier\n   *\n   * @param mapId a map task id\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFile(int mapId) throws IOException;\n\n  /**\n   * Create a local reduce input file name.\n   *\n   * @param mapId a map task id\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFileForWrite(\n      org.apache.hadoop.mapreduce.TaskID mapId, long size) throws IOException;\n\n  /** Removes all of the files related to a task. */\n  public abstract void removeAll() throws IOException;\n\n  @Override\n  public void setConf(Configuration conf) {\n    this.conf = conf;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.mapOutputFile.getSpillIndexFileForWrite": "  public abstract Path getSpillIndexFileForWrite(int spillNumber, long size)\n      throws IOException;\n\n  /**\n   * Return a local reduce input file created earlier\n   *\n   * @param mapId a map task id\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFile(int mapId) throws IOException;\n\n  /**\n   * Create a local reduce input file name.\n   *\n   * @param mapId a map task id\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFileForWrite(\n      org.apache.hadoop.mapreduce.TaskID mapId, long size) throws IOException;\n\n  /** Removes all of the files related to a task. */\n  public abstract void removeAll() throws IOException;\n\n  @Override\n  public void setConf(Configuration conf) {\n    this.conf = conf;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.mapOutputFile.getOutputFile": "  public abstract Path getOutputFile() throws IOException;\n\n  /**\n   * Create a local map output file name.\n   *\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getOutputFileForWrite(long size) throws IOException;\n\n  /**\n   * Create a local map output file name on the same volume.\n   */\n  public abstract Path getOutputFileForWriteInVolume(Path existing);\n\n  /**\n   * Return the path to a local map output index file created earlier\n   *\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getOutputIndexFile() throws IOException;\n\n  /**\n   * Create a local map output index file name.\n   *\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getOutputIndexFileForWrite(long size) throws IOException;\n\n  /**\n   * Create a local map output index file name on the same volume.\n   */\n  public abstract Path getOutputIndexFileForWriteInVolume(Path existing);\n\n  /**\n   * Return a local map spill file created earlier.\n   *\n   * @param spillNumber the number\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillFile(int spillNumber) throws IOException;\n\n  /**\n   * Create a local map spill file name.\n   *\n   * @param spillNumber the number\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillFileForWrite(int spillNumber, long size)\n      throws IOException;\n\n  /**\n   * Return a local map spill index file created earlier\n   *\n   * @param spillNumber the number\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillIndexFile(int spillNumber) throws IOException;\n\n  /**\n   * Create a local map spill index file name.\n   *\n   * @param spillNumber the number\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillIndexFileForWrite(int spillNumber, long size)\n      throws IOException;\n\n  /**\n   * Return a local reduce input file created earlier\n   *\n   * @param mapId a map task id\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFile(int mapId) throws IOException;\n\n  /**\n   * Create a local reduce input file name.\n   *\n   * @param mapId a map task id\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFileForWrite(\n      org.apache.hadoop.mapreduce.TaskID mapId, long size) throws IOException;\n\n  /** Removes all of the files related to a task. */\n  public abstract void removeAll() throws IOException;\n\n  @Override\n  public void setConf(Configuration conf) {\n    this.conf = conf;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getMapRunnerClass": "  public Class<? extends MapRunnable> getMapRunnerClass() {\n    return getClass(\"mapred.map.runner.class\",\n                    MapRunner.class, MapRunnable.class);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.task.getPartition": "  public int getPartition() {\n    return partition;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.setLocalMapFiles": "  public void setLocalMapFiles(Map<TaskAttemptID, MapOutputFile> mapFiles) {\n    this.localMapFiles = mapFiles;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.task.getJobID": "  public JobID getJobID() {\n    return taskId.getJobID();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.task.taskCleanup": "  void taskCleanup(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    // set phase for this task\n    setPhase(TaskStatus.Phase.CLEANUP);\n    getProgress().setStatus(\"cleanup\");\n    statusUpdate(umbilical);\n    LOG.info(\"Runnning cleanup for the task\");\n    // do the cleanup\n    committer.abortTask(taskContext);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.task.statusUpdate": "  public void statusUpdate(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    int retries = MAX_RETRIES;\n    while (true) {\n      try {\n        if (!umbilical.statusUpdate(getTaskID(), taskStatus).getTaskFound()) {\n          LOG.warn(\"Parent died.  Exiting \"+taskId);\n          System.exit(66);\n        }\n        taskStatus.clearStatus();\n        return;\n      } catch (InterruptedException ie) {\n        Thread.currentThread().interrupt(); // interrupt ourself\n      } catch (IOException ie) {\n        LOG.warn(\"Failure sending status update: \" + \n                  StringUtils.stringifyException(ie));\n        if (--retries == 0) {\n          throw ie;\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.task.setPhase": "  protected synchronized void setPhase(TaskStatus.Phase phase){\n    this.taskStatus.setPhase(phase); \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.task.setStatus": "    public void setStatus(String status) {\n      taskProgress.setStatus(normalizeStatus(status, conf));\n      // indicate that progress update needs to be sent\n      setProgressFlag();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.task.getProgress": "    public float getProgress() {\n      return taskProgress.getProgress();\n    };",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.setEncryptedSpillKeyIfRequired": "  public static void setEncryptedSpillKeyIfRequired(Task task) throws\n          Exception {\n    if ((task != null) && (task.getEncryptedSpillKey() != null) && (task\n            .getEncryptedSpillKey().length > 1)) {\n      Credentials creds =\n              UserGroupInformation.getCurrentUser().getCredentials();\n      TokenCache.setEncryptedSpillKey(task.getEncryptedSpillKey(), creds);\n      UserGroupInformation.getCurrentUser().addCredentials(creds);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.task.getTaskID": "  public TaskAttemptID getTaskID() { return taskId; }"
        },
        "bug_report": {
            "Title": "MR AM unable to load native library without MR_AM_ADMIN_USER_ENV set",
            "Description": "If yarn.app.mapreduce.am.admin.user.env (or yarn.app.mapreduce.am.env) is not configured to set LD_LIBRARY_PATH, MR AM will fail to load the native library:\n\n{noformat}\n2015-12-15 21:29:22,473 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n{noformat}\n\nAs a result, any code that needs the hadoop native library in the MR AM will fail. For example, an uber-AM with lz4 compression for the mapper task will fail:\n{noformat}\n2015-12-15 21:30:17,575 WARN [uber-SubtaskRunner] org.apache.hadoop.mapred.LocalContainerLauncher: Exception running local (uberized) 'child' : java.lang.RuntimeException: native lz4 library not available\n\tat org.apache.hadoop.io.compress.Lz4Codec.getCompressorType(Lz4Codec.java:125)\n\tat org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:148)\n\tat org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:163)\n\tat org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:114)\n\tat org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:97)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1602)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1482)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:457)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:391)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:309)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:195)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:238)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n{noformat}"
        }
    },
    {
        "filename": "MAPREDUCE-5137.json",
        "creation_time": "2013-04-09T15:14:34.000+0000",
        "stack_trace": "```\nException in the log looks like:\n\ncom.sun.jersey.api.NotFoundException: null for uri: http://host.com:38158/mapreduce/task/task_1365457322543_0004_m_000000\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1470)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)\n\tat com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:123)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1069)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer.doFilter": "    public void doFilter(ServletRequest request, \n                         ServletResponse response,\n                         FilterChain chain\n                         ) throws IOException, ServletException {\n      HttpServletRequestWrapper quoted = \n        new RequestQuoter((HttpServletRequest) request);\n      HttpServletResponse httpResponse = (HttpServletResponse) response;\n\n      String mime = inferMimeType(request);\n      if (mime == null) {\n        httpResponse.setContentType(\"text/plain; charset=utf-8\");\n      } else if (mime.startsWith(\"text/html\")) {\n        // HTML with unspecified encoding, we want to\n        // force HTML with utf-8 encoding\n        // This is to avoid the following security issue:\n        // http://openmya.hacker.jp/hasegawa/security/utf7cs.html\n        httpResponse.setContentType(\"text/html; charset=utf-8\");\n      } else if (mime.startsWith(\"application/xml\")) {\n        httpResponse.setContentType(\"text/xml; charset=utf-8\");\n      }\n      chain.doFilter(quoted, httpResponse);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer.inferMimeType": "    private String inferMimeType(ServletRequest request) {\n      String path = ((HttpServletRequest)request).getRequestURI();\n      ContextHandler.SContext sContext = (ContextHandler.SContext)config.getServletContext();\n      MimeTypes mimes = sContext.getContextHandler().getMimeTypes();\n      Buffer mimeBuffer = mimes.getMimeByExtension(path);\n      return (mimeBuffer == null) ? null : mimeBuffer.toString();\n    }"
        },
        "bug_report": {
            "Title": "AM web UI: clicking on Map Task results in 500 error",
            "Description": "Go to a running mapreduce app master web UI. Click on the job, then click on the MAP task type to bring up the list of maps, then try to click on a particular map task.  It fails with a 500 error.  Note this doesn't exist in 0.23.6.\n\n\nException in the log looks like:\n\n2013-04-09 13:53:01,587 DEBUG [1088374@qtp-13877033-2 - /mapreduce/task/task_1365457322543_0004_m_000000] org.apache.hadoop.yarn.webapp.GenericExceptionHandler: GOT EXCEPITION\ncom.sun.jersey.api.NotFoundException: null for uri: http://host.com:38158/mapreduce/task/task_1365457322543_0004_m_000000\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1470)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)\n\tat com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:123)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1069)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n...\n...\n..."
        }
    },
    {
        "filename": "MAPREDUCE-4008.json",
        "creation_time": "2012-03-14T10:08:39.000+0000",
        "stack_trace": "```\norg.apache.hadoop.metrics2.MetricsException: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default already exists!\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:117)\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newMBeanName(DefaultMetricsSystem.java:102)\n\tat org.apache.hadoop.metrics2.util.MBeans.getMBeanName(MBeans.java:91)\n\tat org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:55)\n\tat org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:218)\n\tat org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:93)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:243)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1.postStart(MetricsSystemImpl.java:227)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3.invoke(MetricsSystemImpl.java:288)\n\tat $Proxy6.postStart(Unknown Source)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:183)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:155)\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:54)\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start(ResourceManager.java:454)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:588)\nCaused by: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default already exists!\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:113)\n\t... 19 more\n\njavax.management.RuntimeOperationsException: Exception occurred trying to register the MBean\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:969)\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:917)\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:312)\n\tat com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)\n\tat org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:57)\n\tat org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:218)\n\tat org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:93)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:243)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1.postStart(MetricsSystemImpl.java:227)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3.invoke(MetricsSystemImpl.java:288)\n\tat $Proxy6.postStart(Unknown Source)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:183)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:155)\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:54)\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start(ResourceManager.java:454)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:588)\nCaused by: java.lang.IllegalArgumentException: No object name specified\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:967)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName": "  synchronized ObjectName newObjectName(String name) {\n    try {\n      if (mBeanNames.map.containsKey(name) && !miniClusterMode) {\n        throw new MetricsException(name +\" already exists!\");\n      }\n      return new ObjectName(mBeanNames.uniqueName(name));\n    } catch (Exception e) {\n      throw new MetricsException(e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newMBeanName": "  public static ObjectName newMBeanName(String name) {\n    return INSTANCE.newObjectName(name);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.util.MBeans.getMBeanName": "  static private ObjectName getMBeanName(String serviceName, String nameName) {\n    ObjectName name = null;\n    String nameStr = \"Hadoop:service=\"+ serviceName +\",name=\"+ nameName;\n    try {\n      name = DefaultMetricsSystem.newMBeanName(nameStr);\n    } catch (Exception e) {\n      LOG.warn(\"Error creating MBean object name: \"+ nameStr, e);\n    }\n    return name;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.util.MBeans.register": "  static public ObjectName register(String serviceName, String nameName,\n                                    Object theMbean) {\n    final MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();\n    ObjectName name = getMBeanName(serviceName, nameName);\n    try {\n      mbs.registerMBean(theMbean, name);\n      LOG.debug(\"Registered \"+ name);\n      return name;\n    } catch (InstanceAlreadyExistsException iaee) {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"Failed to register MBean \\\"\"+ name + \"\\\"\", iaee);\n      } else {\n        LOG.warn(\"Failed to register MBean \\\"\"+ name\n            + \"\\\": Instance already exists.\");\n      }\n    } catch (Exception e) {\n      LOG.warn(\"Failed to register MBean \\\"\"+ name + \"\\\"\", e);\n    }\n    return null;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans": "  synchronized void startMBeans() {\n    if (mbeanName != null) {\n      LOG.warn(\"MBean \"+ name +\" already initialized!\");\n      LOG.debug(\"Stacktrace: \", new Throwable());\n      return;\n    }\n    mbeanName = MBeans.register(prefix, name, this);\n    LOG.debug(\"MBean for source \"+ name +\" registered.\");\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start": "  void start() {\n    if (startMBeans) startMBeans();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource": "  void registerSource(String name, String desc, MetricsSource source) {\n    checkNotNull(config, \"config\");\n    MetricsConfig conf = sourceConfigs.get(name);\n    MetricsSourceAdapter sa = conf != null\n        ? new MetricsSourceAdapter(prefix, name, desc, source,\n                                   injectedTags, period, conf)\n        : new MetricsSourceAdapter(prefix, name, desc, source,\n          injectedTags, period, config.subset(SOURCE_KEY));\n    sources.put(name, sa);\n    sa.start();\n    LOG.debug(\"Registered source \"+ name);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start": "  public synchronized void start() {\n    checkNotNull(prefix, \"prefix\");\n    if (monitoring) {\n      LOG.warn(prefix +\" metrics system already started!\",\n               new MetricsException(\"Illegal start\"));\n      return;\n    }\n    for (Callback cb : callbacks) cb.preStart();\n    configure(prefix);\n    startTimer();\n    monitoring = true;\n    LOG.info(prefix +\" metrics system started\");\n    for (Callback cb : callbacks) cb.postStart();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSystemImpl.postStart": "      @Override public void postStart() {\n        register(name, description, sink);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register": "  public synchronized void register(final Callback callback) {\n    callbacks.add((Callback) Proxy.newProxyInstance(\n        callback.getClass().getClassLoader(), new Class<?>[] { Callback.class },\n        new InvocationHandler() {\n          @Override\n          public Object invoke(Object proxy, Method method, Object[] args)\n              throws Throwable {\n            try {\n              return method.invoke(callback, args);\n            }\n            catch (Exception e) {\n              // These are not considered fatal.\n              LOG.warn(\"Caught exception in callback \"+ method.getName(), e);\n            }\n            return null;\n          }\n        }));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSystemImpl.invoke": "          public Object invoke(Object proxy, Method method, Object[] args)\n              throws Throwable {\n            try {\n              return method.invoke(callback, args);\n            }\n            catch (Exception e) {\n              // These are not considered fatal.\n              LOG.warn(\"Caught exception in callback \"+ method.getName(), e);\n            }\n            return null;\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init": "  public synchronized MetricsSystem init(String prefix) {\n    if (monitoring && !DefaultMetricsSystem.inMiniClusterMode()) {\n      LOG.warn(this.prefix +\" metrics system already initialized!\");\n      return this;\n    }\n    this.prefix = checkNotNull(prefix, \"prefix\");\n    ++refCount;\n    if (monitoring) {\n      // in mini cluster mode\n      LOG.info(this.prefix +\" metrics system started (again)\");\n      return this;\n    }\n    switch (initMode()) {\n      case NORMAL:\n        try { start(); }\n        catch (MetricsConfigException e) {\n          // Configuration errors (e.g., typos) should not be fatal.\n          // We can always start the metrics system later via JMX.\n          LOG.warn(\"Metrics system not started: \"+ e.getMessage());\n          LOG.debug(\"Stacktrace: \", e);\n        }\n        break;\n      case STANDBY:\n        LOG.info(prefix +\" metrics system started in standby mode\");\n    }\n    initSystemMBean();\n    return this;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initMode": "  private InitMode initMode() {\n    LOG.debug(\"from system property: \"+ System.getProperty(MS_INIT_MODE_KEY));\n    LOG.debug(\"from environment variable: \"+ System.getenv(MS_INIT_MODE_KEY));\n    String m = System.getProperty(MS_INIT_MODE_KEY);\n    String m2 = m == null ? System.getenv(MS_INIT_MODE_KEY) : m;\n    return InitMode.valueOf((m2 == null ? InitMode.NORMAL.name() : m2)\n                            .toUpperCase(Locale.US));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initSystemMBean": "  private void initSystemMBean() {\n    checkNotNull(prefix, \"prefix should not be null here!\");\n    if (mbeanName == null) {\n      mbeanName = MBeans.register(prefix, MS_CONTROL_NAME, this);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init": "  MetricsSystem init(String prefix) {\n    return impl.get().init(prefix);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize": "  public static MetricsSystem initialize(String prefix) {\n    return INSTANCE.init(prefix);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start": "  public void start() {\n    try {\n      doSecureLogin();\n    } catch(IOException ie) {\n      throw new YarnException(\"Failed to login\", ie);\n    }\n\n    startWepApp();\n    DefaultMetricsSystem.initialize(\"ResourceManager\");\n    JvmMetrics.initSingleton(\"ResourceManager\", null);\n    try {\n      rmDTSecretManager.startThreads();\n    } catch(IOException ie) {\n      throw new YarnException(\"Failed to start secret manager threads\", ie);\n    }\n    \n    super.start();\n\n    /*synchronized(shutdown) {\n      try {\n        while(!shutdown.get()) {\n          shutdown.wait();\n        }\n      } catch(InterruptedException ie) {\n        LOG.info(\"Interrupted while waiting\", ie);\n      }\n    }*/\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp": "  protected void startWepApp() {\n    Builder<ApplicationMasterService> builder = \n      WebApps.$for(\"cluster\", ApplicationMasterService.class, masterService, \"ws\").at(\n          this.conf.get(YarnConfiguration.RM_WEBAPP_ADDRESS,\n          YarnConfiguration.DEFAULT_RM_WEBAPP_ADDRESS)); \n    if(YarnConfiguration.getRMWebAppHostAndPort(conf).\n        equals(YarnConfiguration.getProxyHostAndPort(conf))) {\n      AppReportFetcher fetcher = new AppReportFetcher(conf, getClientRMService());\n      builder.withServlet(ProxyUriUtils.PROXY_SERVLET_NAME, \n          ProxyUriUtils.PROXY_PATH_SPEC, WebAppProxyServlet.class);\n      builder.withAttribute(WebAppProxy.FETCHER_ATTRIBUTE, fetcher);\n      String proxy = YarnConfiguration.getProxyHostAndPort(conf);\n      String[] proxyParts = proxy.split(\":\");\n      builder.withAttribute(WebAppProxy.PROXY_HOST_ATTRIBUTE, proxyParts[0]);\n\n    }\n    webApp = builder.start(new RMWebApp(this));\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.doSecureLogin": "  protected void doSecureLogin() throws IOException {\n    SecurityUtil.login(this.conf, YarnConfiguration.RM_KEYTAB,\n        YarnConfiguration.RM_PRINCIPAL);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main": "  public static void main(String argv[]) {\n    StringUtils.startupShutdownMessage(ResourceManager.class, argv, LOG);\n    try {\n      Configuration conf = new YarnConfiguration();\n      Store store =  StoreFactory.getStore(conf);\n      ResourceManager resourceManager = new ResourceManager(store);\n      Runtime.getRuntime().addShutdownHook(\n          new CompositeServiceShutdownHook(resourceManager));\n      resourceManager.init(conf);\n      //resourceManager.recover(store.restore());\n      //store.doneWithRecovery();\n      resourceManager.start();\n    } catch (Throwable t) {\n      LOG.fatal(\"Error starting ResourceManager\", t);\n      System.exit(-1);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.init": "  public synchronized void init(Configuration conf) {\n\n    this.conf = conf;\n\n    this.conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    this.rmDispatcher = createDispatcher();\n    addIfService(this.rmDispatcher);\n\n    this.containerAllocationExpirer = new ContainerAllocationExpirer(\n        this.rmDispatcher);\n    addService(this.containerAllocationExpirer);\n\n    AMLivelinessMonitor amLivelinessMonitor = createAMLivelinessMonitor();\n    addService(amLivelinessMonitor);\n\n    DelegationTokenRenewer tokenRenewer = createDelegationTokenRenewer();\n    addService(tokenRenewer);\n    \n    this.rmContext = new RMContextImpl(this.store, this.rmDispatcher,\n        this.containerAllocationExpirer, amLivelinessMonitor, tokenRenewer);\n\n    addService(nodesListManager);\n\n    // Initialize the scheduler\n    this.scheduler = createScheduler();\n    this.schedulerDispatcher = createSchedulerEventDispatcher();\n    addIfService(this.schedulerDispatcher);\n    this.rmDispatcher.register(SchedulerEventType.class,\n        this.schedulerDispatcher);\n\n    // Register event handler for RmAppEvents\n    this.rmDispatcher.register(RMAppEventType.class,\n        new ApplicationEventDispatcher(this.rmContext));\n\n    // Register event handler for RmAppAttemptEvents\n    this.rmDispatcher.register(RMAppAttemptEventType.class,\n        new ApplicationAttemptEventDispatcher(this.rmContext));\n\n    // Register event handler for RmNodes\n    this.rmDispatcher.register(RMNodeEventType.class,\n        new NodeEventDispatcher(this.rmContext));\n\n    //TODO change this to be random\n    this.appTokenSecretManager.setMasterKey(ApplicationTokenSecretManager\n        .createSecretKey(\"Dummy\".getBytes()));\n\n    this.nmLivelinessMonitor = createNMLivelinessMonitor();\n    addService(this.nmLivelinessMonitor);\n\n    this.resourceTracker = createResourceTrackerService();\n    addService(resourceTracker);\n  \n    try {\n      this.scheduler.reinitialize(conf,\n          this.containerTokenSecretManager, this.rmContext);\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Failed to initialize scheduler\", ioe);\n    }\n\n    masterService = createApplicationMasterService();\n    addService(masterService) ;\n\n    this.applicationACLsManager = new ApplicationACLsManager(conf);\n\n    this.rmAppManager = createRMAppManager();\n    // Register event handler for RMAppManagerEvents\n    this.rmDispatcher.register(RMAppManagerEventType.class,\n        this.rmAppManager);\n    this.rmDTSecretManager = createRMDelegationTokenSecretManager();\n    clientRM = createClientRMService();\n    addService(clientRM);\n    \n    adminService = createAdminService(clientRM, masterService, resourceTracker);\n    addService(adminService);\n\n    this.applicationMasterLauncher = createAMLauncher();\n    this.rmDispatcher.register(AMLauncherEventType.class, \n        this.applicationMasterLauncher);\n\n    addService(applicationMasterLauncher);\n\n    new RMNMInfo(this.rmContext, this.scheduler);\n\n    super.init(conf);\n  }"
        },
        "bug_report": {
            "Title": "ResourceManager throws MetricsException on start up saying QueueMetrics MBean already exists",
            "Description": "{code:xml}\n2012-03-14 15:22:23,089 WARN org.apache.hadoop.metrics2.util.MBeans: Error creating MBean object name: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default\norg.apache.hadoop.metrics2.MetricsException: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default already exists!\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:117)\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newMBeanName(DefaultMetricsSystem.java:102)\n\tat org.apache.hadoop.metrics2.util.MBeans.getMBeanName(MBeans.java:91)\n\tat org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:55)\n\tat org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:218)\n\tat org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:93)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:243)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1.postStart(MetricsSystemImpl.java:227)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3.invoke(MetricsSystemImpl.java:288)\n\tat $Proxy6.postStart(Unknown Source)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:183)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:155)\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:54)\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start(ResourceManager.java:454)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:588)\nCaused by: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default already exists!\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:113)\n\t... 19 more\n2012-03-14 15:22:23,090 WARN org.apache.hadoop.metrics2.util.MBeans: Failed to register MBean \"null\"\njavax.management.RuntimeOperationsException: Exception occurred trying to register the MBean\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:969)\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:917)\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:312)\n\tat com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)\n\tat org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:57)\n\tat org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:218)\n\tat org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:93)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:243)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1.postStart(MetricsSystemImpl.java:227)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3.invoke(MetricsSystemImpl.java:288)\n\tat $Proxy6.postStart(Unknown Source)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:183)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:155)\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:54)\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start(ResourceManager.java:454)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:588)\nCaused by: java.lang.IllegalArgumentException: No object name specified\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:967)\n\t... 20 more\n{code}"
        }
    },
    {
        "filename": "MAPREDUCE-6259.json",
        "creation_time": "2015-02-13T03:20:41.000+0000",
        "stack_trace": "```\njava.lang.IllegalArgumentException: No enum constant org.apache.hadoop.mapreduce.v2.api.records.JobState.0\n\tat java.lang.Enum.valueOf(Enum.java:236)\n\tat org.apache.hadoop.mapreduce.v2.api.records.JobState.valueOf(JobState.java:21)\n\tat org.apache.hadoop.mapreduce.v2.hs.PartialJob.getState(PartialJob.java:82)\n\tat org.apache.hadoop.mapreduce.v2.hs.PartialJob.<init>(PartialJob.java:59)\n\tat org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getAllPartialJobs(CachedHistoryStorage.java:159)\n\tat org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getPartialJobs(CachedHistoryStorage.java:173)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistory.getPartialJobs(JobHistory.java:284)\n\tat org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices.getJobs(HsWebServices.java:212)\n\tat sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)\n\tat com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1223)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:767)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n\tat org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n\tat org.mortbay.jetty.Server.handle(Server.java:326)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n\tat org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.PartialJob.getState": "  public JobState getState() {\n    JobState js = null;\n    try {\n      js = JobState.valueOf(jobIndexInfo.getJobStatus());\n    } catch (Exception e) {\n      // Meant for use by the display UI. Exception would prevent it from being\n      // rendered.e Defaulting to KILLED\n      LOG.warn(\"Exception while parsing job state. Defaulting to KILLED\", e);\n      js = JobState.KILLED;\n    }\n    return js;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getAllPartialJobs": "  public Map<JobId, Job> getAllPartialJobs() {\n    LOG.debug(\"Called getAllPartialJobs()\");\n    SortedMap<JobId, Job> result = new TreeMap<JobId, Job>();\n    try {\n      for (HistoryFileInfo mi : hsManager.getAllFileInfo()) {\n        if (mi != null) {\n          JobId id = mi.getJobId();\n          result.put(id, new PartialJob(mi.getJobIndexInfo(), id));\n        }\n      }\n    } catch (IOException e) {\n      LOG.warn(\"Error trying to scan for all FileInfos\", e);\n      throw new YarnRuntimeException(e);\n    }\n    return result;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getPartialJobs": "  public static JobsInfo getPartialJobs(Collection<Job> jobs, Long offset,\n      Long count, String user, String queue, Long sBegin, Long sEnd,\n      Long fBegin, Long fEnd, JobState jobState) {\n    JobsInfo allJobs = new JobsInfo();\n\n    if (sBegin == null || sBegin < 0)\n      sBegin = 0l;\n    if (sEnd == null)\n      sEnd = Long.MAX_VALUE;\n    if (fBegin == null || fBegin < 0)\n      fBegin = 0l;\n    if (fEnd == null)\n      fEnd = Long.MAX_VALUE;\n    if (offset == null || offset < 0)\n      offset = 0l;\n    if (count == null)\n      count = Long.MAX_VALUE;\n\n    if (offset > jobs.size()) {\n      return allJobs;\n    }\n\n    long at = 0;\n    long end = offset + count - 1;\n    if (end < 0) { // due to overflow\n      end = Long.MAX_VALUE;\n    }\n\n    for (Job job : jobs) {\n      if (at > end) {\n        break;\n      }\n\n      // can't really validate queue is a valid one since queues could change\n      if (queue != null && !queue.isEmpty()) {\n        if (!job.getQueueName().equals(queue)) {\n          continue;\n        }\n      }\n\n      if (user != null && !user.isEmpty()) {\n        if (!job.getUserName().equals(user)) {\n          continue;\n        }\n      }\n\n      JobReport report = job.getReport();\n\n      if (report.getStartTime() < sBegin || report.getStartTime() > sEnd) {\n        continue;\n      }\n      if (report.getFinishTime() < fBegin || report.getFinishTime() > fEnd) {\n        continue;\n      }\n      if (jobState != null && jobState != report.getJobState()) {\n        continue;\n      }\n\n      at++;\n      if ((at - 1) < offset) {\n        continue;\n      }\n\n      JobInfo jobInfo = new JobInfo(job);\n\n      allJobs.add(jobInfo);\n    }\n    return allJobs;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.JobHistory.getPartialJobs": "  public JobsInfo getPartialJobs(Long offset, Long count, String user,\n      String queue, Long sBegin, Long sEnd, Long fBegin, Long fEnd,\n      JobState jobState) {\n    return storage.getPartialJobs(offset, count, user, queue, sBegin, sEnd,\n        fBegin, fEnd, jobState);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices.getJobs": "  public JobsInfo getJobs(@QueryParam(\"user\") String userQuery,\n      @QueryParam(\"limit\") String count,\n      @QueryParam(\"state\") String stateQuery,\n      @QueryParam(\"queue\") String queueQuery,\n      @QueryParam(\"startedTimeBegin\") String startedBegin,\n      @QueryParam(\"startedTimeEnd\") String startedEnd,\n      @QueryParam(\"finishedTimeBegin\") String finishBegin,\n      @QueryParam(\"finishedTimeEnd\") String finishEnd) {\n\n    Long countParam = null;\n    init();\n    \n    if (count != null && !count.isEmpty()) {\n      try {\n        countParam = Long.parseLong(count);\n      } catch (NumberFormatException e) {\n        throw new BadRequestException(e.getMessage());\n      }\n      if (countParam <= 0) {\n        throw new BadRequestException(\"limit value must be greater then 0\");\n      }\n    }\n\n    Long sBegin = null;\n    if (startedBegin != null && !startedBegin.isEmpty()) {\n      try {\n        sBegin = Long.parseLong(startedBegin);\n      } catch (NumberFormatException e) {\n        throw new BadRequestException(\"Invalid number format: \" + e.getMessage());\n      }\n      if (sBegin < 0) {\n        throw new BadRequestException(\"startedTimeBegin must be greater than 0\");\n      }\n    }\n    \n    Long sEnd = null;\n    if (startedEnd != null && !startedEnd.isEmpty()) {\n      try {\n        sEnd = Long.parseLong(startedEnd);\n      } catch (NumberFormatException e) {\n        throw new BadRequestException(\"Invalid number format: \" + e.getMessage());\n      }\n      if (sEnd < 0) {\n        throw new BadRequestException(\"startedTimeEnd must be greater than 0\");\n      }\n    }\n    if (sBegin != null && sEnd != null && sBegin > sEnd) {\n      throw new BadRequestException(\n          \"startedTimeEnd must be greater than startTimeBegin\");\n    }\n\n    Long fBegin = null;\n    if (finishBegin != null && !finishBegin.isEmpty()) {\n      try {\n        fBegin = Long.parseLong(finishBegin);\n      } catch (NumberFormatException e) {\n        throw new BadRequestException(\"Invalid number format: \" + e.getMessage());\n      }\n      if (fBegin < 0) {\n        throw new BadRequestException(\"finishedTimeBegin must be greater than 0\");\n      }\n    }\n    Long fEnd = null;\n    if (finishEnd != null && !finishEnd.isEmpty()) {\n      try {\n        fEnd = Long.parseLong(finishEnd);\n      } catch (NumberFormatException e) {\n        throw new BadRequestException(\"Invalid number format: \" + e.getMessage());\n      }\n      if (fEnd < 0) {\n        throw new BadRequestException(\"finishedTimeEnd must be greater than 0\");\n      }\n    }\n    if (fBegin != null && fEnd != null && fBegin > fEnd) {\n      throw new BadRequestException(\n          \"finishedTimeEnd must be greater than finishedTimeBegin\");\n    }\n    \n    JobState jobState = null;\n    if (stateQuery != null) {\n      jobState = JobState.valueOf(stateQuery);\n    }\n\n    return ctx.getPartialJobs(0l, countParam, userQuery, queueQuery, \n        sBegin, sEnd, fBegin, fEnd, jobState);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices.init": "  private void init() {\n    //clear content type\n    response.setContentType(null);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.lib.StaticUserWebFilter.doFilter": "    public void doFilter(ServletRequest request, ServletResponse response,\n                         FilterChain chain\n                         ) throws IOException, ServletException {\n      HttpServletRequest httpRequest = (HttpServletRequest) request;\n      // if the user is already authenticated, don't override it\n      if (httpRequest.getRemoteUser() != null) {\n        chain.doFilter(request, response);\n      } else {\n        HttpServletRequestWrapper wrapper = \n            new HttpServletRequestWrapper(httpRequest) {\n          @Override\n          public Principal getUserPrincipal() {\n            return user;\n          }\n          @Override\n          public String getRemoteUser() {\n            return username;\n          }\n        };\n        chain.doFilter(wrapper, response);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.lib.StaticUserWebFilter.getRemoteUser": "          public String getRemoteUser() {\n            return username;\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.doFilter": "    public void doFilter(ServletRequest request,\n                         ServletResponse response,\n                         FilterChain chain\n                         ) throws IOException, ServletException {\n      HttpServletRequestWrapper quoted =\n        new RequestQuoter((HttpServletRequest) request);\n      HttpServletResponse httpResponse = (HttpServletResponse) response;\n\n      String mime = inferMimeType(request);\n      if (mime == null) {\n        httpResponse.setContentType(\"text/plain; charset=utf-8\");\n      } else if (mime.startsWith(\"text/html\")) {\n        // HTML with unspecified encoding, we want to\n        // force HTML with utf-8 encoding\n        // This is to avoid the following security issue:\n        // http://openmya.hacker.jp/hasegawa/security/utf7cs.html\n        httpResponse.setContentType(\"text/html; charset=utf-8\");\n      } else if (mime.startsWith(\"application/xml\")) {\n        httpResponse.setContentType(\"text/xml; charset=utf-8\");\n      }\n      chain.doFilter(quoted, httpResponse);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.inferMimeType": "    private String inferMimeType(ServletRequest request) {\n      String path = ((HttpServletRequest)request).getRequestURI();\n      ContextHandler.SContext sContext = (ContextHandler.SContext)config.getServletContext();\n      MimeTypes mimes = sContext.getContextHandler().getMimeTypes();\n      Buffer mimeBuffer = mimes.getMimeByExtension(path);\n      return (mimeBuffer == null) ? null : mimeBuffer.toString();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.NoCacheFilter.doFilter": "  public void doFilter(ServletRequest req, ServletResponse res,\n                       FilterChain chain)\n    throws IOException, ServletException {\n    HttpServletResponse httpRes = (HttpServletResponse) res;\n    httpRes.setHeader(\"Cache-Control\", \"no-cache\");\n    long now = System.currentTimeMillis();\n    httpRes.addDateHeader(\"Expires\", now);\n    httpRes.addDateHeader(\"Date\", now);\n    httpRes.addHeader(\"Pragma\", \"no-cache\");\n    chain.doFilter(req, res);\n  }"
        },
        "bug_report": {
            "Title": "IllegalArgumentException due to missing job submit time",
            "Description": "-1 job submit time cause IllegalArgumentException when parse the Job history file name and JOB_INIT_FAILED cause -1 job submit time in JobIndexInfo.\nWe found the following job history file name which cause IllegalArgumentException when parse the job status in the job history file name.\n{code}\njob_1418398645407_115853--1-worun-kafka%2Dto%2Dhdfs%5Btwo%5D%5B15+topic%28s%29%5D-1423572836007-0-0-FAILED-root.journaling-1423572836007.jhist\n{code}\nThe stack trace for the IllegalArgumentException is\n{code}\n2015-02-10 04:54:01,863 WARN org.apache.hadoop.mapreduce.v2.hs.PartialJob: Exception while parsing job state. Defaulting to KILLED\njava.lang.IllegalArgumentException: No enum constant org.apache.hadoop.mapreduce.v2.api.records.JobState.0\n\tat java.lang.Enum.valueOf(Enum.java:236)\n\tat org.apache.hadoop.mapreduce.v2.api.records.JobState.valueOf(JobState.java:21)\n\tat org.apache.hadoop.mapreduce.v2.hs.PartialJob.getState(PartialJob.java:82)\n\tat org.apache.hadoop.mapreduce.v2.hs.PartialJob.<init>(PartialJob.java:59)\n\tat org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getAllPartialJobs(CachedHistoryStorage.java:159)\n\tat org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getPartialJobs(CachedHistoryStorage.java:173)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistory.getPartialJobs(JobHistory.java:284)\n\tat org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices.getJobs(HsWebServices.java:212)\n\tat sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)\n\tat com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1223)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:767)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n\tat org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n\tat org.mortbay.jetty.Server.handle(Server.java:326)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n\tat org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n{code}\n\n\nwhen IOException happened in JobImpl#setup, the Job submit time in JobHistoryEventHandler#MetaInfo#JobIndexInfo will not be changed and the Job submit time will be its [initial value -1|https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java#L1185].\n{code}\n      this.jobIndexInfo =\n          new JobIndexInfo(-1, -1, user, jobName, jobId, -1, -1, null,\n                           queueName);\n{code}\n\nThe following is the sequences to get -1 job submit time:\n1. \na job is created at MRAppMaster#serviceStart and  the new job is at state JobStateInternal.NEW after created\n{code}\n    job = createJob(getConfig(), forcedState, shutDownMessage);\n{code}\n\n2.\nJobEventType.JOB_INIT is sent to JobImpl from MRAppMaster#serviceStart\n{code}\n      JobEvent initJobEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT);\n      // Send init to the job (this does NOT trigger job execution)\n      // This is a synchronous call, not an event through dispatcher. We want\n      // job-init to be done completely here.\n      jobEventDispatcher.handle(initJobEvent);\n{code}\n\n3.\nafter JobImpl received JobEventType.JOB_INIT, it call InitTransition#transition\n{code}\n          .addTransition\n              (JobStateInternal.NEW,\n              EnumSet.of(JobStateInternal.INITED, JobStateInternal.NEW),\n              JobEventType.JOB_INIT,\n              new InitTransition())\n{code}\n\n4.\nthen the exception happen from setup(job) in InitTransition#transition before JobSubmittedEvent is handled.\nJobSubmittedEvent will update the job submit time. Due to the exception, the submit time is still the initial value -1.\nThis is the code InitTransition#transition\n{code}\npublic JobStateInternal transition(JobImpl job, JobEvent event) {\n      job.metrics.submittedJob(job);\n      job.metrics.preparingJob(job);\n      if (job.newApiCommitter) {\n        job.jobContext = new JobContextImpl(job.conf, job.oldJobId);\n      } else {\n        job.jobContext = new org.apache.hadoop.mapred.JobContextImpl(job.conf, job.oldJobId);\n      }\n      try {\n        setup(job);\n        job.fs = job.getFileSystem(job.conf);\n        //log to job history\n        JobSubmittedEvent jse = new JobSubmittedEvent(job.oldJobId,\n              job.conf.get(MRJobConfig.JOB_NAME, \"test\"), \n            job.conf.get(MRJobConfig.USER_NAME, \"mapred\"),\n            job.appSubmitTime,\n            job.remoteJobConfFile.toString(),\n            job.jobACLs, job.queueName,\n            job.conf.get(MRJobConfig.WORKFLOW_ID, \"\"),\n            job.conf.get(MRJobConfig.WORKFLOW_NAME, \"\"),\n            job.conf.get(MRJobConfig.WORKFLOW_NODE_NAME, \"\"),\n            getWorkflowAdjacencies(job.conf),\n            job.conf.get(MRJobConfig.WORKFLOW_TAGS, \"\"));\n        job.eventHandler.handle(new JobHistoryEvent(job.jobId, jse));\n        //TODO JH Verify jobACLs, UserName via UGI?\n\n        TaskSplitMetaInfo[] taskSplitMetaInfo = createSplits(job, job.jobId);\n        job.numMapTasks = taskSplitMetaInfo.length;\n        job.numReduceTasks = job.conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n\n        if (job.numMapTasks == 0 && job.numReduceTasks == 0) {\n          job.addDiagnostic(\"No of maps and reduces are 0 \" + job.jobId);\n        } else if (job.numMapTasks == 0) {\n          job.reduceWeight = 0.9f;\n        } else if (job.numReduceTasks == 0) {\n          job.mapWeight = 0.9f;\n        } else {\n          job.mapWeight = job.reduceWeight = 0.45f;\n        }\n\n        checkTaskLimits();\n\n        long inputLength = 0;\n        for (int i = 0; i < job.numMapTasks; ++i) {\n          inputLength += taskSplitMetaInfo[i].getInputDataLength();\n        }\n\n        job.makeUberDecision(inputLength);\n        \n        job.taskAttemptCompletionEvents =\n            new ArrayList<TaskAttemptCompletionEvent>(\n                job.numMapTasks + job.numReduceTasks + 10);\n        job.mapAttemptCompletionEvents =\n            new ArrayList<TaskCompletionEvent>(job.numMapTasks + 10);\n        job.taskCompletionIdxToMapCompletionIdx = new ArrayList<Integer>(\n            job.numMapTasks + job.numReduceTasks + 10);\n\n        job.allowedMapFailuresPercent =\n            job.conf.getInt(MRJobConfig.MAP_FAILURES_MAX_PERCENT, 0);\n        job.allowedReduceFailuresPercent =\n            job.conf.getInt(MRJobConfig.REDUCE_FAILURES_MAXPERCENT, 0);\n\n        // create the Tasks but don't start them yet\n        createMapTasks(job, inputLength, taskSplitMetaInfo);\n        createReduceTasks(job);\n\n        job.metrics.endPreparingJob(job);\n        return JobStateInternal.INITED;\n      } catch (Exception e) {\n        LOG.warn(\"Job init failed\", e);\n        job.metrics.endPreparingJob(job);\n        job.addDiagnostic(\"Job init failed : \"\n            + StringUtils.stringifyException(e));\n        // Leave job in the NEW state. The MR AM will detect that the state is\n        // not INITED and send a JOB_INIT_FAILED event.\n        return JobStateInternal.NEW;\n      }\n    }\n{code}\n\nThis is the code JobImpl#setup\n{code}\n    protected void setup(JobImpl job) throws IOException {\n\n      String oldJobIDString = job.oldJobId.toString();\n      String user = \n        UserGroupInformation.getCurrentUser().getShortUserName();\n      Path path = MRApps.getStagingAreaDir(job.conf, user);\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"startJobs: parent=\" + path + \" child=\" + oldJobIDString);\n      }\n\n      job.remoteJobSubmitDir =\n          FileSystem.get(job.conf).makeQualified(\n              new Path(path, oldJobIDString));\n      job.remoteJobConfFile =\n          new Path(job.remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n\n      // Prepare the TaskAttemptListener server for authentication of Containers\n      // TaskAttemptListener gets the information via jobTokenSecretManager.\n      JobTokenIdentifier identifier =\n          new JobTokenIdentifier(new Text(oldJobIDString));\n      job.jobToken =\n          new Token<JobTokenIdentifier>(identifier, job.jobTokenSecretManager);\n      job.jobToken.setService(identifier.getJobId());\n      // Add it to the jobTokenSecretManager so that TaskAttemptListener server\n      // can authenticate containers(tasks)\n      job.jobTokenSecretManager.addTokenForJob(oldJobIDString, job.jobToken);\n      LOG.info(\"Adding job token for \" + oldJobIDString\n          + \" to jobTokenSecretManager\");\n\n      // If the job client did not setup the shuffle secret then reuse\n      // the job token secret for the shuffle.\n      if (TokenCache.getShuffleSecretKey(job.jobCredentials) == null) {\n        LOG.warn(\"Shuffle secret key missing from job credentials.\"\n            + \" Using job token secret as shuffle secret.\");\n        TokenCache.setShuffleSecretKey(job.jobToken.getPassword(),\n            job.jobCredentials);\n      }\n    }\n{code}\n\n5.\nDue to the IOException from  JobImpl#setup, the new job is still at state JobStateInternal.NEW\n{code}\n      } catch (Exception e) {\n        LOG.warn(\"Job init failed\", e);\n        job.metrics.endPreparingJob(job);\n        job.addDiagnostic(\"Job init failed : \"\n            + StringUtils.stringifyException(e));\n        // Leave job in the NEW state. The MR AM will detect that the state is\n        // not INITED and send a JOB_INIT_FAILED event.\n        return JobStateInternal.NEW;\n      }\n{code}\nAt the following code of MRAppMaster#serviceStart, The MR AM detect the state is not INITED and send a JOB_INIT_FAILED event.\n{code}\n      // If job is still not initialized, an error happened during\n      // initialization. Must complete starting all of the services so failure\n      // events can be processed.\n      initFailed = (((JobImpl)job).getInternalState() != JobStateInternal.INITED);\n    if (initFailed) {\n      JobEvent initFailedEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT_FAILED);\n      jobEventDispatcher.handle(initFailedEvent);\n    } else {\n      // All components have started, start the job.\n      startJobs();\n    }\n{code}\n\n6.\nAfter JobImpl receives the JOB_INIT_FAILED, it will call InitFailedTransition#transition and enter state JobStateInternal.FAIL_ABORT\n{code}\n          .addTransition(JobStateInternal.NEW, JobStateInternal.FAIL_ABORT,\n              JobEventType.JOB_INIT_FAILED,\n              new InitFailedTransition())\n{code}\n\n7.\nJobImpl will send CommitterJobAbortEvent in  InitFailedTransition#transition \n{code}\n    public void transition(JobImpl job, JobEvent event) {\n        job.eventHandler.handle(new CommitterJobAbortEvent(job.jobId,\n                job.jobContext,\n                org.apache.hadoop.mapreduce.JobStatus.State.FAILED));\n    }\n{code}\n\n8.\nCommitterJobAbortEvent will be handled by CommitterEventHandler#handleJobAbort which will send JobAbortCompletedEvent(JobEventType.JOB_ABORT_COMPLETED)\n{code}\n    protected void handleJobAbort(CommitterJobAbortEvent event) {\n      cancelJobCommit();\n      try {\n        committer.abortJob(event.getJobContext(), event.getFinalState());\n      } catch (Exception e) {\n        LOG.warn(\"Could not abort job\", e);\n      }\n      context.getEventHandler().handle(new JobAbortCompletedEvent(\n          event.getJobID(), event.getFinalState()));\n    }\n{code}\n\n9.\nAfter JobImpl receives the JOB_ABORT_COMPLETED, it will call JobAbortCompletedTransition#transition and enter state JobStateInternal.FAILED\n{code}\n          .addTransition(JobStateInternal.FAIL_ABORT, JobStateInternal.FAILED,\n              JobEventType.JOB_ABORT_COMPLETED,\n              new JobAbortCompletedTransition())\n{code}\n\n10.\nJobAbortCompletedTransition#transition will call JobImpl#unsuccessfulFinish which will send JobUnsuccessfulCompletionEvent with finish time.\n{code}\n    public void transition(JobImpl job, JobEvent event) {\n      JobStateInternal finalState = JobStateInternal.valueOf(\n          ((JobAbortCompletedEvent) event).getFinalState().name());\n      job.unsuccessfulFinish(finalState);\n    }\n  private void unsuccessfulFinish(JobStateInternal finalState) {\n      if (finishTime == 0) setFinishTime();\n      cleanupProgress = 1.0f;\n      JobUnsuccessfulCompletionEvent unsuccessfulJobEvent =\n          new JobUnsuccessfulCompletionEvent(oldJobId,\n              finishTime,\n              succeededMapTaskCount,\n              succeededReduceTaskCount,\n              finalState.toString(),\n              diagnostics);\n      eventHandler.handle(new JobHistoryEvent(jobId,\n          unsuccessfulJobEvent));\n      finished(finalState);\n  }\n{code}\n\n11.\nJobUnsuccessfulCompletionEvent will be handled by JobHistoryEventHandler#handleEvent with type EventType.JOB_FAILED\nBased on the following code, you can see the JobIndexInfo#finishTime is set correctly but JobIndexInfo#submitTime and  JobIndexInfo#jobStartTime are still -1.\n{code}\n      if (event.getHistoryEvent().getEventType() == EventType.JOB_FAILED\n          || event.getHistoryEvent().getEventType() == EventType.JOB_KILLED) {\n        try {\n          JobUnsuccessfulCompletionEvent jucEvent = \n              (JobUnsuccessfulCompletionEvent) event\n              .getHistoryEvent();\n          mi.getJobIndexInfo().setFinishTime(jucEvent.getFinishTime());\n          mi.getJobIndexInfo().setNumMaps(jucEvent.getFinishedMaps());\n          mi.getJobIndexInfo().setNumReduces(jucEvent.getFinishedReduces());\n          mi.getJobIndexInfo().setJobStatus(jucEvent.getStatus());\n          closeEventWriter(event.getJobID());\n          processDoneFiles(event.getJobID());\n        } catch (IOException e) {\n          throw new YarnRuntimeException(e);\n        }\n      }\n{code}\n\nThe error job history file name in our log is \"job_1418398645407_115853--1-worun-kafka%2Dto%2Dhdfs%5Btwo%5D%5B15+topic%28s%29%5D-1423572836007-0-0-FAILED-root.journaling-1423572836007.jhist\"\nBased on the filename, you can see submitTime is -1, finishTime is 1423572836007 and jobStartTime is 1423572836007.\nThe jobStartTime is not -1, and  jobStartTime is the same as  finishTime.\nIt is because jobStartTime is handled specially in FileNameIndexUtils#getDoneFileName:\n{code}\n    //JobStartTime\n    if (indexInfo.getJobStartTime() >= 0) {\n      sb.append(indexInfo.getJobStartTime());\n    } else {\n      sb.append(indexInfo.getFinishTime());\n    }\n{code}\n\n"
        }
    },
    {
        "filename": "MAPREDUCE-3333.json",
        "creation_time": "2011-11-02T14:00:54.000+0000",
        "stack_trace": "```\njava.lang.reflect.UndeclaredThrowableException\n        at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:88)\n        at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:290)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\nCaused by: com.google.protobuf.ServiceException: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"gsbl91281.blue.ygrid.yahoo.com/98.137.101.189\"; destination host is: \"\"gsbl91525.blue.ygrid.yahoo.com\":45450; \n        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:139)\n        at $Proxy20.startContainer(Unknown Source)\n        at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:81)\n        ... 4 more\nCaused by: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"gsbl91281.blue.ygrid.yahoo.com/98.137.101.189\"; destination host is: \"\"gsbl91525.blue.ygrid.yahoo.com\":45450; \n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:655)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1089)\n        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:136)\n        ... 6 more\nCaused by: java.io.IOException: Couldn't set up IO streams\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:621)\n        at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:205)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1195)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1065)\n        ... 7 more\nCaused by: java.lang.OutOfMemoryError: unable to create new native thread\n        at java.lang.Thread.start0(Native Method)\n        at java.lang.Thread.start(Thread.java:597)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:614)```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer": "  public StartContainerResponse startContainer(StartContainerRequest request)\n      throws YarnRemoteException {\n    StartContainerRequestProto requestProto = ((StartContainerRequestPBImpl)request).getProto();\n    try {\n      return new StartContainerResponsePBImpl(proxy.startContainer(null, requestProto));\n    } catch (ServiceException e) {\n      if (e.getCause() instanceof YarnRemoteException) {\n        throw (YarnRemoteException)e.getCause();\n      } else if (e.getCause() instanceof UndeclaredThrowableException) {\n        throw (UndeclaredThrowableException)e.getCause();\n      } else {\n        throw new UndeclaredThrowableException(e);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.run": "    public void run() {\n      LOG.info(\"Processing the event \" + event.toString());\n\n      // Load ContainerManager tokens before creating a connection.\n      // TODO: Do it only once per NodeManager.\n      final String containerManagerBindAddr = event.getContainerMgrAddress();\n      ContainerId containerID = event.getContainerID();\n      ContainerToken containerToken = event.getContainerToken();\n      TaskAttemptId taskAttemptID = event.getTaskAttemptID();\n\n      Timer timer = new Timer(true);\n\n      switch(event.getType()) {\n\n      case CONTAINER_REMOTE_LAUNCH:\n        ContainerRemoteLaunchEvent launchEvent\n            = (ContainerRemoteLaunchEvent) event;\n\n        try {\n          timer.schedule(new CommandTimer(Thread.currentThread(), event),\n              nmTimeOut);\n\n          ContainerManager proxy = getCMProxy(containerID,\n              containerManagerBindAddr, containerToken);\n\n          // Interruped during getProxy, but that didn't throw exception\n          if (Thread.currentThread().isInterrupted()) {\n            // The timer cancelled the command in the mean while.\n            String message = \"Start-container for \" + event.getContainerID()\n                + \" got interrupted. Returning.\";\n            sendContainerLaunchFailedMsg(taskAttemptID, message);\n            return;\n          }\n\n          // Construct the actual Container\n          ContainerLaunchContext containerLaunchContext =\n              launchEvent.getContainer();\n\n          // Now launch the actual container\n          StartContainerRequest startRequest = recordFactory\n              .newRecordInstance(StartContainerRequest.class);\n          startRequest.setContainerLaunchContext(containerLaunchContext);\n          StartContainerResponse response = proxy.startContainer(startRequest);\n\n          // container started properly. Stop the timer\n          timer.cancel();\n          if (Thread.currentThread().isInterrupted()) {\n            // The timer cancelled the command in the mean while, but\n            // startContainer didn't throw exception\n            String message = \"Start-container for \" + event.getContainerID()\n                + \" got interrupted. Returning.\";\n            sendContainerLaunchFailedMsg(taskAttemptID, message);\n            return;\n          }\n\n          ByteBuffer portInfo = response\n              .getServiceResponse(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID);\n          int port = -1;\n          if(portInfo != null) {\n            port = ShuffleHandler.deserializeMetaData(portInfo);\n          }\n          LOG.info(\"Shuffle port returned by ContainerManager for \"\n              + taskAttemptID + \" : \" + port);\n          \n          if(port < 0) {\n            throw new IllegalStateException(\"Invalid shuffle port number \"\n                + port + \" returned for \" + taskAttemptID);\n          }\n\n          // after launching, send launched event to task attempt to move\n          // it from ASSIGNED to RUNNING state\n          context.getEventHandler().handle(\n              new TaskAttemptContainerLaunchedEvent(taskAttemptID, port));\n        } catch (Throwable t) {\n          String message = \"Container launch failed for \" + containerID\n              + \" : \" + StringUtils.stringifyException(t);\n          sendContainerLaunchFailedMsg(taskAttemptID, message);\n        } finally {\n          timer.cancel();\n        }\n\n        break;\n\n      case CONTAINER_REMOTE_CLEANUP:\n        // We will have to remove the launch (meant \"cleanup\"? FIXME) event if it is still in eventQueue\n        // and not yet processed\n        if (eventQueue.contains(event)) {\n          eventQueue.remove(event); // TODO: Any synchro needed?\n          //deallocate the container\n          context.getEventHandler().handle(\n              new ContainerAllocatorEvent(taskAttemptID,\n                  ContainerAllocator.EventType.CONTAINER_DEALLOCATE));\n        } else {\n\n          try {\n            timer.schedule(new CommandTimer(Thread.currentThread(), event),\n                nmTimeOut);\n\n            ContainerManager proxy = getCMProxy(containerID,\n                containerManagerBindAddr, containerToken);\n\n            if (Thread.currentThread().isInterrupted()) {\n              // The timer cancelled the command in the mean while. No need to\n              // return, send cleanedup event anyways.\n              LOG.info(\"Stop-container for \" + event.getContainerID()\n                  + \" got interrupted.\");\n            } else {\n\n              // TODO:check whether container is launched\n\n              // kill the remote container if already launched\n              StopContainerRequest stopRequest = recordFactory\n                  .newRecordInstance(StopContainerRequest.class);\n              stopRequest.setContainerId(event.getContainerID());\n              proxy.stopContainer(stopRequest);\n            }\n          } catch (Throwable t) {\n            // ignore the cleanup failure\n            String message = \"cleanup failed for container \"\n                + event.getContainerID() + \" : \"\n                + StringUtils.stringifyException(t);\n            context.getEventHandler()\n                .handle(\n                    new TaskAttemptDiagnosticsUpdateEvent(taskAttemptID,\n                        message));\n            LOG.warn(message);\n          } finally {\n            timer.cancel();\n          }\n\n          // after killing, send killed event to taskattempt\n          context.getEventHandler().handle(\n              new TaskAttemptEvent(event.getTaskAttemptID(),\n                  TaskAttemptEventType.TA_CONTAINER_CLEANED));\n        }\n        break;\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.sendContainerLaunchFailedMsg": "  void sendContainerLaunchFailedMsg(TaskAttemptId taskAttemptID,\n      String message) {\n    LOG.error(message);\n    context.getEventHandler().handle(\n        new TaskAttemptDiagnosticsUpdateEvent(taskAttemptID, message));\n    context.getEventHandler().handle(\n        new TaskAttemptEvent(taskAttemptID,\n            TaskAttemptEventType.TA_CONTAINER_LAUNCH_FAILED));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.handle": "  public void handle(ContainerLauncherEvent event) {\n    try {\n      eventQueue.put(event);\n    } catch (InterruptedException e) {\n      throw new YarnException(e);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.getCMProxy": "  protected ContainerManager getCMProxy(ContainerId containerID,\n      final String containerManagerBindAddr, ContainerToken containerToken)\n      throws IOException {\n\n    UserGroupInformation user = UserGroupInformation.getCurrentUser();\n\n    synchronized (this.clientCache) {\n\n      if (this.clientCache.containsKey(containerID)) {\n        return this.clientCache.get(containerID);\n      }\n\n      this.allNodes.add(containerManagerBindAddr);\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        Token<ContainerTokenIdentifier> token = new Token<ContainerTokenIdentifier>(\n            containerToken.getIdentifier().array(), containerToken\n                .getPassword().array(), new Text(containerToken.getKind()),\n            new Text(containerToken.getService()));\n        // the user in createRemoteUser in this context has to be ContainerID\n        user = UserGroupInformation.createRemoteUser(containerID.toString());\n        user.addToken(token);\n      }\n\n      ContainerManager proxy = user\n          .doAs(new PrivilegedAction<ContainerManager>() {\n            @Override\n            public ContainerManager run() {\n              YarnRPC rpc = YarnRPC.create(getConfig());\n              return (ContainerManager) rpc.getProxy(ContainerManager.class,\n                  NetUtils.createSocketAddr(containerManagerBindAddr),\n                  getConfig());\n            }\n          });\n      this.clientCache.put(containerID, proxy);\n      return proxy;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.invoke": "    public Object invoke(Object proxy, Method method, Object[] args)\n        throws Throwable {\n      long startTime = 0;\n      if (LOG.isDebugEnabled()) {\n        startTime = System.currentTimeMillis();\n      }\n\n      ProtoSpecificRpcRequest rpcRequest = constructRpcRequest(method, args);\n      ProtoSpecificResponseWritable val = null;\n      try {\n        val = (ProtoSpecificResponseWritable) client.call(\n            new ProtoSpecificRequestWritable(rpcRequest), remoteId);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      \n      ProtoSpecificRpcResponse response = val.message;\n   \n      if (LOG.isDebugEnabled()) {\n        long callTime = System.currentTimeMillis() - startTime;\n        LOG.debug(\"Call: \" + method.getName() + \" \" + callTime);\n      }\n \n      if (response.hasIsError() && response.getIsError() == true) {\n        YarnRemoteExceptionPBImpl exception = new YarnRemoteExceptionPBImpl(response.getException());\n        exception.fillInStackTrace();\n        ServiceException se = new ServiceException(exception);\n        throw se;\n      }\n      \n      Message prototype = null;\n      try {\n        prototype = getReturnProtoType(method);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      Message actualReturnMessage = prototype.newBuilderForType()\n          .mergeFrom(response.getResponseProto()).build();\n      return actualReturnMessage;\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.constructRpcRequest": "    private ProtoSpecificRpcRequest constructRpcRequest(Method method,\n        Object[] params) throws ServiceException {\n      ProtoSpecificRpcRequest rpcRequest;\n      ProtoSpecificRpcRequest.Builder builder;\n\n      builder = ProtoSpecificRpcRequest.newBuilder();\n      builder.setMethodName(method.getName());\n\n      if (params.length != 2) { // RpcController + Message\n        throw new ServiceException(\"Too many parameters for request. Method: [\"\n            + method.getName() + \"]\" + \", Expected: 2, Actual: \"\n            + params.length);\n      }\n      if (params[1] == null) {\n        throw new ServiceException(\"null param while calling Method: [\"\n            + method.getName() + \"]\");\n      }\n\n      Message param = (Message) params[1];\n      builder.setRequestProto(param.toByteString());\n\n      rpcRequest = builder.build();\n      return rpcRequest;\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.call": "    public Writable call(String protocol, Writable writableRequest,\n        long receiveTime) throws IOException {\n      ProtoSpecificRequestWritable request = (ProtoSpecificRequestWritable) writableRequest;\n      ProtoSpecificRpcRequest rpcRequest = request.message;\n      String methodName = rpcRequest.getMethodName();\n      System.out.println(\"Call: protocol=\" + protocol + \", method=\"\n          + methodName);\n      if (verbose)\n        log(\"Call: protocol=\" + protocol + \", method=\"\n            + methodName);\n      MethodDescriptor methodDescriptor = service.getDescriptorForType()\n          .findMethodByName(methodName);\n      if (methodDescriptor == null) {\n        String msg = \"Unknown method \" + methodName + \" called on \"\n            + protocol + \" protocol.\";\n        LOG.warn(msg);\n        return handleException(new IOException(msg));\n      }\n      Message prototype = service.getRequestPrototype(methodDescriptor);\n      Message param = prototype.newBuilderForType()\n          .mergeFrom(rpcRequest.getRequestProto()).build();\n      Message result;\n      try {\n        result = service.callBlockingMethod(methodDescriptor, null, param);\n      } catch (ServiceException e) {\n        e.printStackTrace();\n        return handleException(e);\n      } catch (Exception e) {\n        return handleException(e);\n      }\n\n      ProtoSpecificRpcResponse response = constructProtoSpecificRpcSuccessResponse(result);\n      return new ProtoSpecificResponseWritable(response);\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.getReturnProtoType": "    private Message getReturnProtoType(Method method) throws Exception {\n      if (returnTypes.containsKey(method.getName())) {\n        return returnTypes.get(method.getName());\n      } else {\n        Class<?> returnType = method.getReturnType();\n\n        Method newInstMethod = returnType.getMethod(\"getDefaultInstance\");\n        newInstMethod.setAccessible(true);\n        Message prototype = (Message) newInstMethod.invoke(null,\n            (Object[]) null);\n        returnTypes.put(method.getName(), prototype);\n        return prototype;\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.wrapException": "  public static IOException wrapException(final String destHost,\n                                          final int destPort,\n                                          final String localHost,\n                                          final int localPort,\n                                          final IOException exception) {\n    if (exception instanceof BindException) {\n      return new BindException(\n          \"Problem binding to [\"\n              + localHost\n              + \":\"\n              + localPort\n              + \"] \"\n              + exception\n              + \";\"\n              + see(\"BindException\"));\n    } else if (exception instanceof ConnectException) {\n      // connection refused; include the host:port in the error\n      return (ConnectException) new ConnectException(\n          \"Call From \"\n              + localHost\n              + \" to \"\n              + destHost\n              + \":\"\n              + destPort\n              + \" failed on connection exception: \"\n              + exception\n              + \";\"\n              + see(\"ConnectionRefused\"))\n          .initCause(exception);\n    } else if (exception instanceof UnknownHostException) {\n      return (UnknownHostException) new UnknownHostException(\n          \"Invalid host name: \"\n              + getHostDetailsAsString(destHost, destPort, localHost)\n              + exception\n              + \";\"\n              + see(\"UnknownHost\"))\n          .initCause(exception);\n    } else if (exception instanceof SocketTimeoutException) {\n      return (SocketTimeoutException) new SocketTimeoutException(\n          \"Call From \"\n              + localHost + \" to \" + destHost + \":\" + destPort\n              + \" failed on socket timeout exception: \" + exception\n              + \";\"\n              + see(\"SocketTimeout\"))\n          .initCause(exception);\n    } else if (exception instanceof NoRouteToHostException) {\n      return (NoRouteToHostException) new NoRouteToHostException(\n          \"No Route to Host from  \"\n              + localHost + \" to \" + destHost + \":\" + destPort\n              + \" failed on socket timeout exception: \" + exception\n              + \";\"\n              + see(\"NoRouteToHost\"))\n          .initCause(exception);\n    }\n    else {\n      return (IOException) new IOException(\"Failed on local exception: \"\n                                               + exception\n                                               + \"; Host Details : \"\n                                               + getHostDetailsAsString(destHost, destPort, localHost))\n          .initCause(exception);\n\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.getHostDetailsAsString": "  private static String getHostDetailsAsString(final String destHost,\n                                               final int destPort,\n                                               final String localHost) {\n    StringBuilder hostDetails = new StringBuilder(27);\n    hostDetails.append(\"local host is: \")\n        .append(quoteHost(localHost))\n        .append(\"; \");\n    hostDetails.append(\"destination host is: \\\"\").append(quoteHost(destHost))\n        .append(\":\")\n        .append(destPort).append(\"; \");\n    return hostDetails.toString();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.see": "  private static String see(final String entry) {\n    return FOR_MORE_DETAILS_SEE + HADOOP_WIKI + entry;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.call": "  public Writable[] call(Writable[] params, InetSocketAddress[] addresses,\n      Class<?> protocol, UserGroupInformation ticket, Configuration conf)\n      throws IOException, InterruptedException {\n    if (addresses.length == 0) return new Writable[0];\n\n    ParallelResults results = new ParallelResults(params.length);\n    synchronized (results) {\n      for (int i = 0; i < params.length; i++) {\n        ParallelCall call = new ParallelCall(params[i], results, i);\n        try {\n          ConnectionId remoteId = ConnectionId.getConnectionId(addresses[i],\n              protocol, ticket, 0, conf);\n          Connection connection = getConnection(remoteId, call);\n          connection.sendParam(call);             // send each parameter\n        } catch (IOException e) {\n          // log errors\n          LOG.info(\"Calling \"+addresses[i]+\" caught: \" + \n                   e.getMessage(),e);\n          results.size--;                         //  wait for one fewer result\n        }\n      }\n      while (results.count != results.size) {\n        try {\n          results.wait();                    // wait for all results\n        } catch (InterruptedException e) {}\n      }\n\n      return results.values;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnectionId": "    public static ConnectionId getConnectionId(InetSocketAddress addr,\n        Class<?> protocol, UserGroupInformation ticket, int rpcTimeout,\n        Configuration conf) throws IOException {\n      String remotePrincipal = getRemotePrincipal(conf, addr, protocol);\n      boolean doPing = conf.getBoolean(\"ipc.client.ping\", true);\n      return new ConnectionId(addr, protocol, ticket,\n          rpcTimeout, remotePrincipal,\n          conf.getInt(\"ipc.client.connection.maxidletime\", 10000), // 10s\n          conf.getInt(\"ipc.client.connect.max.retries\", 10),\n          conf.getBoolean(\"ipc.client.tcpnodelay\", false),\n          doPing, \n          (doPing ? Client.getPingInterval(conf) : 0));\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.sendParam": "    public void sendParam(Call call) {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n\n      DataOutputBuffer d=null;\n      try {\n        synchronized (this.out) {\n          if (LOG.isDebugEnabled())\n            LOG.debug(getName() + \" sending #\" + call.id);\n          \n          //for serializing the\n          //data to be written\n          d = new DataOutputBuffer();\n          d.writeInt(call.id);\n          call.param.write(d);\n          byte[] data = d.getData();\n          int dataLength = d.getLength();\n          out.writeInt(dataLength);      //first put the data length\n          out.write(data, 0, dataLength);//write the data\n          out.flush();\n        }\n      } catch(IOException e) {\n        markClosed(e);\n      } finally {\n        //the buffer is just an in-memory buffer, but it is still polite to\n        // close early\n        IOUtils.closeStream(d);\n      }\n    }  ",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getAddress": "    InetSocketAddress getAddress() {\n      return address;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnection": "  private Connection getConnection(ConnectionId remoteId,\n                                   Call call)\n                                   throws IOException, InterruptedException {\n    if (!running.get()) {\n      // the client is stopped\n      throw new IOException(\"The client is stopped\");\n    }\n    Connection connection;\n    /* we could avoid this allocation for each RPC by having a  \n     * connectionsId object and with set() method. We need to manage the\n     * refs for keys in HashMap properly. For now its ok.\n     */\n    do {\n      synchronized (connections) {\n        connection = connections.get(remoteId);\n        if (connection == null) {\n          connection = new Connection(remoteId);\n          connections.put(remoteId, connection);\n        }\n      }\n    } while (!connection.addCall(call));\n    \n    //we don't invoke the method below inside \"synchronized (connections)\"\n    //block above. The reason for that is if the server happens to be slow,\n    //it will take longer to establish a connection and that will slow the\n    //entire system down.\n    connection.setupIOstreams();\n    return connection;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupIOstreams": "    private synchronized void setupIOstreams() throws InterruptedException {\n      if (socket != null || shouldCloseConnection.get()) {\n        return;\n      } \n      try {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Connecting to \"+server);\n        }\n        short numRetries = 0;\n        final short MAX_RETRIES = 5;\n        Random rand = null;\n        while (true) {\n          setupConnection();\n          InputStream inStream = NetUtils.getInputStream(socket);\n          OutputStream outStream = NetUtils.getOutputStream(socket);\n          writeRpcHeader(outStream);\n          if (useSasl) {\n            final InputStream in2 = inStream;\n            final OutputStream out2 = outStream;\n            UserGroupInformation ticket = remoteId.getTicket();\n            if (authMethod == AuthMethod.KERBEROS) {\n              if (ticket.getRealUser() != null) {\n                ticket = ticket.getRealUser();\n              }\n            }\n            boolean continueSasl = false;\n            try {\n              continueSasl = ticket\n                  .doAs(new PrivilegedExceptionAction<Boolean>() {\n                    @Override\n                    public Boolean run() throws IOException {\n                      return setupSaslConnection(in2, out2);\n                    }\n                  });\n            } catch (Exception ex) {\n              if (rand == null) {\n                rand = new Random();\n              }\n              handleSaslConnectionFailure(numRetries++, MAX_RETRIES, ex, rand,\n                  ticket);\n              continue;\n            }\n            if (continueSasl) {\n              // Sasl connect is successful. Let's set up Sasl i/o streams.\n              inStream = saslRpcClient.getInputStream(inStream);\n              outStream = saslRpcClient.getOutputStream(outStream);\n            } else {\n              // fall back to simple auth because server told us so.\n              authMethod = AuthMethod.SIMPLE;\n              header = new ConnectionHeader(header.getProtocol(), header\n                  .getUgi(), authMethod);\n              useSasl = false;\n            }\n          }\n        \n          if (doPing) {\n            this.in = new DataInputStream(new BufferedInputStream(\n                new PingInputStream(inStream)));\n          } else {\n            this.in = new DataInputStream(new BufferedInputStream(inStream));\n          }\n          this.out = new DataOutputStream(new BufferedOutputStream(outStream));\n          writeHeader();\n\n          // update last activity time\n          touch();\n\n          // start the receiver thread after the socket connection has been set\n          // up\n          start();\n          return;\n        }\n      } catch (Throwable t) {\n        if (t instanceof IOException) {\n          markClosed((IOException)t);\n        } else {\n          markClosed(new IOException(\"Couldn't set up IO streams\", t));\n        }\n        close();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupConnection": "    private synchronized void setupConnection() throws IOException {\n      short ioFailures = 0;\n      short timeoutFailures = 0;\n      while (true) {\n        try {\n          this.socket = socketFactory.createSocket();\n          this.socket.setTcpNoDelay(tcpNoDelay);\n          \n          /*\n           * Bind the socket to the host specified in the principal name of the\n           * client, to ensure Server matching address of the client connection\n           * to host name in principal passed.\n           */\n          if (UserGroupInformation.isSecurityEnabled()) {\n            KerberosInfo krbInfo = \n              remoteId.getProtocol().getAnnotation(KerberosInfo.class);\n            if (krbInfo != null && krbInfo.clientPrincipal() != null) {\n              String host = \n                SecurityUtil.getHostFromPrincipal(remoteId.getTicket().getUserName());\n              \n              // If host name is a valid local address then bind socket to it\n              InetAddress localAddr = NetUtils.getLocalInetAddress(host);\n              if (localAddr != null) {\n                this.socket.bind(new InetSocketAddress(localAddr, 0));\n              }\n            }\n          }\n          \n          // connection time out is 20s\n          NetUtils.connect(this.socket, server, 20000);\n          if (rpcTimeout > 0) {\n            pingInterval = rpcTimeout;  // rpcTimeout overwrites pingInterval\n          }\n          this.socket.setSoTimeout(pingInterval);\n          return;\n        } catch (SocketTimeoutException toe) {\n          /* Check for an address change and update the local reference.\n           * Reset the failure counter if the address was changed\n           */\n          if (updateAddress()) {\n            timeoutFailures = ioFailures = 0;\n          }\n          /*\n           * The max number of retries is 45, which amounts to 20s*45 = 15\n           * minutes retries.\n           */\n          handleConnectionFailure(timeoutFailures++, 45, toe);\n        } catch (IOException ie) {\n          if (updateAddress()) {\n            timeoutFailures = ioFailures = 0;\n          }\n          handleConnectionFailure(ioFailures++, maxRetries, ie);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.writeHeader": "    private void writeHeader() throws IOException {\n      // Write out the ConnectionHeader\n      DataOutputBuffer buf = new DataOutputBuffer();\n      header.write(buf);\n      \n      // Write out the payload length\n      int bufLen = buf.getLength();\n      out.writeInt(bufLen);\n      out.write(buf.getData(), 0, bufLen);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.close": "    private synchronized void close() {\n      if (!shouldCloseConnection.get()) {\n        LOG.error(\"The connection is not in the closed state\");\n        return;\n      }\n\n      // release the resources\n      // first thing to do;take the connection out of the connection list\n      synchronized (connections) {\n        if (connections.get(remoteId) == this) {\n          connections.remove(remoteId);\n        }\n      }\n\n      // close the streams and therefore the socket\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(in);\n      disposeSasl();\n\n      // clean up all calls\n      if (closeException == null) {\n        if (!calls.isEmpty()) {\n          LOG.warn(\n              \"A connection is closed for no cause and calls are not empty\");\n\n          // clean up calls anyway\n          closeException = new IOException(\"Unexpected closed connection\");\n          cleanupCalls();\n        }\n      } else {\n        // log the info\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"closing ipc connection to \" + server + \": \" +\n              closeException.getMessage(),closeException);\n        }\n\n        // cleanup calls\n        cleanupCalls();\n      }\n      if (LOG.isDebugEnabled())\n        LOG.debug(getName() + \": closed\");\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupSaslConnection": "    private synchronized boolean setupSaslConnection(final InputStream in2, \n        final OutputStream out2) \n        throws IOException {\n      saslRpcClient = new SaslRpcClient(authMethod, token, serverPrincipal);\n      return saslRpcClient.saslConnect(in2, out2);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.touch": "    private void touch() {\n      lastActivity.set(System.currentTimeMillis());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.writeRpcHeader": "    private void writeRpcHeader(OutputStream outStream) throws IOException {\n      DataOutputStream out = new DataOutputStream(new BufferedOutputStream(outStream));\n      // Write out the header, version and authentication method\n      out.write(Server.HEADER.array());\n      out.write(Server.CURRENT_VERSION);\n      authMethod.write(out);\n      out.flush();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getProtocol": "    Class<?> getProtocol() {\n      return protocol;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.handleSaslConnectionFailure": "    private synchronized void handleSaslConnectionFailure(\n        final int currRetries, final int maxRetries, final Exception ex,\n        final Random rand, final UserGroupInformation ugi) throws IOException,\n        InterruptedException {\n      ugi.doAs(new PrivilegedExceptionAction<Object>() {\n        public Object run() throws IOException, InterruptedException {\n          final short MAX_BACKOFF = 5000;\n          closeConnection();\n          disposeSasl();\n          if (shouldAuthenticateOverKrb()) {\n            if (currRetries < maxRetries) {\n              if(LOG.isDebugEnabled()) {\n                LOG.debug(\"Exception encountered while connecting to \"\n                    + \"the server : \" + ex);\n              }\n              // try re-login\n              if (UserGroupInformation.isLoginKeytabBased()) {\n                UserGroupInformation.getLoginUser().reloginFromKeytab();\n              } else {\n                UserGroupInformation.getLoginUser().reloginFromTicketCache();\n              }\n              // have granularity of milliseconds\n              //we are sleeping with the Connection lock held but since this\n              //connection instance is being used for connecting to the server\n              //in question, it is okay\n              Thread.sleep((rand.nextInt(MAX_BACKOFF) + 1));\n              return null;\n            } else {\n              String msg = \"Couldn't setup connection for \"\n                  + UserGroupInformation.getLoginUser().getUserName() + \" to \"\n                  + serverPrincipal;\n              LOG.warn(msg);\n              throw (IOException) new IOException(msg).initCause(ex);\n            }\n          } else {\n            LOG.warn(\"Exception encountered while connecting to \"\n                + \"the server : \" + ex);\n          }\n          if (ex instanceof RemoteException)\n            throw (RemoteException) ex;\n          throw new IOException(ex);\n        }\n      });\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getTicket": "    UserGroupInformation getTicket() {\n      return ticket;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.markClosed": "    private synchronized void markClosed(IOException e) {\n      if (shouldCloseConnection.compareAndSet(false, true)) {\n        closeException = e;\n        notifyAll();\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherEvent.toString": "  public String toString() {\n    return super.toString() + \" for taskAttempt \" + taskAttemptID;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherEvent.getContainerMgrAddress": "  public String getContainerMgrAddress() {\n    return containerMgrAddress;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherEvent.getContainerID": "  public ContainerId getContainerID() {\n    return containerID;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherEvent.getContainerToken": "  public ContainerToken getContainerToken() {\n    return containerToken;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherEvent.getTaskAttemptID": "  public TaskAttemptId getTaskAttemptID() {\n    return this.taskAttemptID;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.launcher.ContainerRemoteLaunchEvent.getContainer": "  public abstract ContainerLaunchContext getContainer();\n\n  public abstract Task getRemoteTask();\n\n}"
        },
        "bug_report": {
            "Title": "MR AM for sort-job going out of memory",
            "Description": "[~Karams] just found this. The usual sort job on a 350 node cluster hung due to OutOfMemory and eventually failed after an hour instead of the usual odd 20 minutes.\n{code}\n2011-11-02 11:40:36,438 ERROR [ContainerLauncher #258] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Container launch failed for container_1320233407485_0002\n_01_001434 : java.lang.reflect.UndeclaredThrowableException\n        at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:88)\n        at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:290)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\nCaused by: com.google.protobuf.ServiceException: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"gsbl91281.blue.ygrid.yahoo.com/98.137.101.189\"; destination host is: \"\"gsbl91525.blue.ygrid.yahoo.com\":45450; \n        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:139)\n        at $Proxy20.startContainer(Unknown Source)\n        at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:81)\n        ... 4 more\nCaused by: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"gsbl91281.blue.ygrid.yahoo.com/98.137.101.189\"; destination host is: \"\"gsbl91525.blue.ygrid.yahoo.com\":45450; \n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:655)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1089)\n        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:136)\n        ... 6 more\nCaused by: java.io.IOException: Couldn't set up IO streams\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:621)\n        at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:205)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1195)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1065)\n        ... 7 more\nCaused by: java.lang.OutOfMemoryError: unable to create new native thread\n        at java.lang.Thread.start0(Native Method)\n        at java.lang.Thread.start(Thread.java:597)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:614)\n        ... 10 more\n{code}"
        }
    },
    {
        "filename": "MAPREDUCE-7059.json",
        "creation_time": "2018-02-26T09:38:39.000+0000",
        "stack_trace": "```\norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RpcNoSuchMethodException): Unknown method setErasureCodingPolicy called on org.apache.hadoop.hdfs.protocol.ClientProtocol.\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:436)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:846)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:789)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:180)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2457)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1437)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1347)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy11.setErasureCodingPolicy(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setErasureCodingPolicy(ClientNamenodeProtocolTranslatorPB.java:1583)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy12.setErasureCodingPolicy(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.setErasureCodingPolicy(DFSClient.java:2678)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$63.doCall(DistributedFileSystem.java:2665)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$63.doCall(DistributedFileSystem.java:2662)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.setErasureCodingPolicy(DistributedFileSystem.java:2680)\n\tat org.apache.hadoop.mapreduce.JobResourceUploader.disableErasureCodingForPath(JobResourceUploader.java:882)\n\tat org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:174)\n\tat org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:131)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:102)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:197)\n\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)\n\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)\n\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)\n\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)\n\tat org.apache.hadoop.examples.terasort.TeraGen.run(TeraGen.java:304)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n\tat org.apache.hadoop.examples.terasort.TeraGen.main(TeraGen.java:308)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\n\tat org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n\tat org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:304)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:218)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcProtobufRequest request = (RpcProtobufRequest) writableRequest;\n        RequestHeaderProto rpcRequest = request.getRequestHeader();\n        String methodName = rpcRequest.getMethodName();\n\n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = request.getValue(prototype);\n\n        Message result;\n        long startTime = Time.now();\n        int qTime = (int) (startTime - receiveTime);\n        Exception exception = null;\n        boolean isDeferred = false;\n        try {\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          currentCallInfo.set(new CallInfo(server, methodName));\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n          // Check if this needs to be a deferred response,\n          // by checking the ThreadLocal callback being set\n          if (currentCallback.get() != null) {\n            Server.getCurCall().get().deferResponse();\n            isDeferred = true;\n            currentCallback.set(null);\n            return null;\n          }\n        } catch (ServiceException e) {\n          exception = (Exception) e.getCause();\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          exception = e;\n          throw e;\n        } finally {\n          currentCallInfo.set(null);\n          int processingTime = (int) (Time.now() - startTime);\n          if (LOG.isDebugEnabled()) {\n            String msg =\n                \"Served: \" + methodName + (isDeferred ? \", deferred\" : \"\") +\n                    \", queueTime= \" + qTime +\n                    \" procesingTime= \" + processingTime;\n            if (exception != null) {\n              msg += \" exception= \" + exception.getClass().getSimpleName();\n            }\n            LOG.debug(msg);\n          }\n          String detailedMetricsName = (exception == null) ?\n              methodName :\n              exception.getClass().getSimpleName();\n          server.updateMetrics(detailedMetricsName, qTime, processingTime,\n              isDeferred);\n        }\n        return RpcWritable.wrap(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.get": "          public Message get(long timeout, TimeUnit unit) throws Exception {\n            return getReturnMessage(method, arr.get(timeout, unit));\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getRequestHeader": "    RequestHeaderProto getRequestHeader() throws IOException {\n      if (getByteBuffer() != null && requestHeader == null) {\n        requestHeader = getValue(RequestHeaderProto.getDefaultInstance());\n      }\n      return requestHeader;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getProtocolImpl": "      private static ProtoClassProtoImpl getProtocolImpl(RPC.Server server,\n          String protoName, long clientVersion) throws RpcServerException {\n        ProtoNameVer pv = new ProtoNameVer(protoName, clientVersion);\n        ProtoClassProtoImpl impl = \n            server.getProtocolImplMap(RPC.RpcKind.RPC_PROTOCOL_BUFFER).get(pv);\n        if (impl == null) { // no match for Protocol AND Version\n          VerProtocolImpl highest = \n              server.getHighestSupportedProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER, \n                  protoName);\n          if (highest == null) {\n            throw new RpcNoSuchProtocolException(\n                \"Unknown protocol: \" + protoName);\n          }\n          // protocol supported but not the version that client wants\n          throw new RPC.VersionMismatch(protoName, clientVersion,\n              highest.version);\n        }\n        return impl;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.call": "    public Writable call(RPC.RpcKind rpcKind, String protocol,\n        Writable rpcRequest, long receiveTime) throws Exception {\n      return getRpcInvoker(rpcKind).call(this, protocol, rpcRequest,\n          receiveTime);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    boolean close(Connection connection) {\n      boolean exists = remove(connection);\n      if (exists) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(Thread.currentThread().getName() +\n              \": disconnecting client \" + connection +\n              \". Number of active connections: \"+ size());\n        }\n        // only close if actually removed to avoid double-closing due\n        // to possible races\n        connection.close();\n        // Remove authorized users only\n        if (connection.user != null && connection.connectionContextRead) {\n          decrUserConnections(connection.user.getShortUserName());\n        }\n      }\n      return exists;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.sendResponse": "    private void sendResponse(RpcCall call) throws IOException {\n      responder.doRespond(call);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.isResponseDeferred": "    public boolean isResponseDeferred() {\n      return this.deferredResponse;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.populateResponseParamsOnError": "    private void populateResponseParamsOnError(Throwable t,\n                                               ResponseParams responseParams) {\n      if (t instanceof UndeclaredThrowableException) {\n        t = t.getCause();\n      }\n      logException(Server.LOG, t, this);\n      if (t instanceof RpcServerException) {\n        RpcServerException rse = ((RpcServerException) t);\n        responseParams.returnStatus = rse.getRpcStatusProto();\n        responseParams.detailedErr = rse.getRpcErrorCodeProto();\n      } else {\n        responseParams.returnStatus = RpcStatusProto.ERROR;\n        responseParams.detailedErr = RpcErrorCodeProto.ERROR_APPLICATION;\n      }\n      responseParams.errorClass = t.getClass().getName();\n      responseParams.error = StringUtils.stringifyException(t);\n      // Remove redundant error class name from the beginning of the\n      // stack trace\n      String exceptionHdr = responseParams.errorClass + \": \";\n      if (responseParams.error.startsWith(exceptionHdr)) {\n        responseParams.error =\n            responseParams.error.substring(exceptionHdr.length());\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws InterruptedException, IOException,  OutOfMemoryError {\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        channel.socket().setKeepAlive(true);\n        \n        Reader reader = getReader();\n        Connection c = connectionManager.register(channel);\n        // If the connectionManager can't take it, close the connection.\n        if (c == null) {\n          if (channel.isOpen()) {\n            IOUtils.cleanup(null, channel);\n          }\n          connectionManager.droppedConnections.getAndIncrement();\n          continue;\n        }\n        key.attach(c);  // so closeCurrentConnection can get the object\n        reader.addConnection(c);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(RpcCall call,\n      RpcResponseHeaderProto header, Writable rv) throws IOException {\n    final byte[] response;\n    if (rv == null || (rv instanceof RpcWritable.ProtobufWrapper)) {\n      response = setupResponseForProtobuf(header, rv);\n    } else {\n      response = setupResponseForWritable(header, rv);\n    }\n    if (response.length > maxRespSize) {\n      LOG.warn(\"Large response size \" + response.length + \" for call \"\n          + call.toString());\n    }\n    call.setResponse(ByteBuffer.wrap(response));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.stopIdleScan": "    void stopIdleScan() {\n      idleScanTimer.cancel();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeIdle": "    synchronized void closeIdle(boolean scanAll) {\n      long minLastContact = Time.now() - maxIdleTime;\n      // concurrent iterator might miss new connections added\n      // during the iteration, but that's ok because they won't\n      // be idle yet anyway and will be caught on next scan\n      int closed = 0;\n      for (Connection connection : connections) {\n        // stop if connections dropped below threshold unless scanning all\n        if (!scanAll && size() < idleScanThreshold) {\n          break;\n        }\n        // stop if not scanning all and max connections are closed\n        if (connection.isIdle() &&\n            connection.getLastContact() < minLastContact &&\n            close(connection) &&\n            !scanAll && (++closed == maxIdleToClose)) {\n          break;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(RPC.RpcKind rpcKind, String protocol,\n      Writable param, long receiveTime) throws Exception;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param protocolName - the protocol\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  private void authorize(UserGroupInformation user, String protocolName,\n      InetAddress addr) throws AuthorizationException {\n    if (authorize) {\n      if (protocolName == null) {\n        throw new AuthorizationException(\"Null protocol not authorized\");\n      }\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(protocolName, getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         protocolName);\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeAll": "    void closeAll() {\n      // use a copy of the connections to be absolutely sure the concurrent\n      // iterator doesn't miss a connection\n      for (Connection connection : toArray()) {\n        close(connection);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.remove": "    private boolean remove(Connection connection) {\n      boolean removed = connections.remove(connection);\n      if (removed) {\n        count.getAndDecrement();\n      }\n      return removed;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getRemoteUser": "    public UserGroupInformation getRemoteUser() {\n      return connection.user;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.startIdleScan": "    void startIdleScan() {\n      scheduleIdleScanTask();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isWritable()) {\n                doAsyncWrite(key);\n              }\n            } catch (CancelledKeyException cke) {\n              // something else closed the connection, ex. reader or the\n              // listener doing an idle scan.  ignore it and let them clean\n              // up\n              RpcCall call = (RpcCall)key.attachment();\n              if (call != null) {\n                LOG.info(Thread.currentThread().getName() +\n                    \": connection aborted from \" + call.connection);\n              }\n            } catch (IOException e) {\n              LOG.info(Thread.currentThread().getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = Time.now();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<RpcCall> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<RpcCall>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              RpcCall call = (RpcCall)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n\n          for (RpcCall call : calls) {\n            doPurge(call, now);\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.scheduleIdleScanTask": "    private void scheduleIdleScanTask() {\n      if (!running) {\n        return;\n      }\n      TimerTask idleScanTask = new TimerTask(){\n        @Override\n        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }\n      };\n      idleScanTimer.schedule(idleScanTask, idleScanInterval);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause == null) {\n        throw new RuntimeException(\"PrivilegedActionException with no \" +\n                \"underlying cause. UGI [\" + this + \"]\" +\": \" + pae, pae);\n      } else if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRpcResponse": "  private Writable getRpcResponse(final Call call, final Connection connection,\n      final long timeout, final TimeUnit unit) throws IOException {\n    synchronized (call) {\n      while (!call.done) {\n        try {\n          AsyncGet.Util.wait(call, timeout, unit);\n          if (timeout >= 0 && !call.done) {\n            return null;\n          }\n        } catch (InterruptedException ie) {\n          Thread.currentThread().interrupt();\n          throw new InterruptedIOException(\"Call interrupted\");\n        }\n      }\n\n      if (call.error != null) {\n        if (call.error instanceof RemoteException) {\n          call.error.fillInStackTrace();\n          throw call.error;\n        } else { // local exception\n          InetSocketAddress address = connection.getRemoteAddress();\n          throw NetUtils.wrapException(address.getHostName(),\n                  address.getPort(),\n                  NetUtils.getHostname(),\n                  0,\n                  call.error);\n        }\n      } else {\n        return call.getRpcResponse();\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRemoteAddress": "    public InetSocketAddress getRemoteAddress() {\n      return server;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.call": "  Writable call(RPC.RpcKind rpcKind, Writable rpcRequest,\n      ConnectionId remoteId, int serviceClass,\n      AtomicBoolean fallbackToSimpleAuth) throws IOException {\n    final Call call = createCall(rpcKind, rpcRequest);\n    final Connection connection = getConnection(remoteId, call, serviceClass,\n        fallbackToSimpleAuth);\n\n    try {\n      checkAsyncCall();\n      try {\n        connection.sendRpcRequest(call);                 // send the rpc request\n      } catch (RejectedExecutionException e) {\n        throw new IOException(\"connection has been closed\", e);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n        LOG.warn(\"interrupted waiting to send rpc request to server\", e);\n        throw new IOException(e);\n      }\n    } catch(Exception e) {\n      if (isAsynchronousMode()) {\n        releaseAsyncCall();\n      }\n      throw e;\n    }\n\n    if (isAsynchronousMode()) {\n      final AsyncGet<Writable, IOException> asyncGet\n          = new AsyncGet<Writable, IOException>() {\n        @Override\n        public Writable get(long timeout, TimeUnit unit)\n            throws IOException, TimeoutException{\n          boolean done = true;\n          try {\n            final Writable w = getRpcResponse(call, connection, timeout, unit);\n            if (w == null) {\n              done = false;\n              throw new TimeoutException(call + \" timed out \"\n                  + timeout + \" \" + unit);\n            }\n            return w;\n          } finally {\n            if (done) {\n              releaseAsyncCall();\n            }\n          }\n        }\n\n        @Override\n        public boolean isDone() {\n          synchronized (call) {\n            return call.done;\n          }\n        }\n      };\n\n      ASYNC_RPC_RESPONSE.set(asyncGet);\n      return null;\n    } else {\n      return getRpcResponse(call, connection, -1, null);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.isAsynchronousMode": "  public static boolean isAsynchronousMode() {\n    return asynchronousMode.get();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.releaseAsyncCall": "  private void releaseAsyncCall() {\n    asyncCallCounter.decrementAndGet();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.checkAsyncCall": "  private void checkAsyncCall() throws IOException {\n    if (isAsynchronousMode()) {\n      if (asyncCallCounter.incrementAndGet() > maxAsyncCalls) {\n        asyncCallCounter.decrementAndGet();\n        String errMsg = String.format(\n            \"Exceeded limit of max asynchronous calls: %d, \" +\n            \"please configure %s to adjust it.\",\n            maxAsyncCalls,\n            CommonConfigurationKeys.IPC_CLIENT_ASYNC_CALLS_MAX_KEY);\n        throw new AsyncCallLimitExceededException(errMsg);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.sendRpcRequest": "    public void sendRpcRequest(final Call call)\n        throws InterruptedException, IOException {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n\n      // Serialize the call to be sent. This is done from the actual\n      // caller thread, rather than the sendParamsExecutor thread,\n      \n      // so that if the serialization throws an error, it is reported\n      // properly. This also parallelizes the serialization.\n      //\n      // Format of a call on the wire:\n      // 0) Length of rest below (1 + 2)\n      // 1) RpcRequestHeader  - is serialized Delimited hence contains length\n      // 2) RpcRequest\n      //\n      // Items '1' and '2' are prepared here. \n      RpcRequestHeaderProto header = ProtoUtil.makeRpcRequestHeader(\n          call.rpcKind, OperationProto.RPC_FINAL_PACKET, call.id, call.retry,\n          clientId);\n\n      final ResponseBuffer buf = new ResponseBuffer();\n      header.writeDelimitedTo(buf);\n      RpcWritable.wrap(call.rpcRequest).writeTo(buf);\n\n      synchronized (sendRpcRequestLock) {\n        Future<?> senderFuture = sendParamsExecutor.submit(new Runnable() {\n          @Override\n          public void run() {\n            try {\n              synchronized (ipcStreams.out) {\n                if (shouldCloseConnection.get()) {\n                  return;\n                }\n                if (LOG.isDebugEnabled()) {\n                  LOG.debug(getName() + \" sending #\" + call.id\n                      + \" \" + call.rpcRequest);\n                }\n                // RpcRequestHeader + RpcRequest\n                ipcStreams.sendRequest(buf.toByteArray());\n                ipcStreams.flush();\n              }\n            } catch (IOException e) {\n              // exception at this point would leave the connection in an\n              // unrecoverable state (eg half a call left on the wire).\n              // So, close the connection, killing any outstanding calls\n              markClosed(e);\n            } finally {\n              //the buffer is just an in-memory buffer, but it is still polite to\n              // close early\n              IOUtils.closeStream(buf);\n            }\n          }\n        });\n      \n        try {\n          senderFuture.get();\n        } catch (ExecutionException e) {\n          Throwable cause = e.getCause();\n          \n          // cause should only be a RuntimeException as the Runnable above\n          // catches IOException\n          if (cause instanceof RuntimeException) {\n            throw (RuntimeException) cause;\n          } else {\n            throw new RuntimeException(\"unexpected checked exception\", cause);\n          }\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.createCall": "  Call createCall(RPC.RpcKind rpcKind, Writable rpcRequest) {\n    return new Call(rpcKind, rpcRequest);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnection": "  private Connection getConnection(ConnectionId remoteId,\n      Call call, int serviceClass, AtomicBoolean fallbackToSimpleAuth)\n      throws IOException {\n    if (!running.get()) {\n      // the client is stopped\n      throw new IOException(\"The client is stopped\");\n    }\n    Connection connection;\n    /* we could avoid this allocation for each RPC by having a  \n     * connectionsId object and with set() method. We need to manage the\n     * refs for keys in HashMap properly. For now its ok.\n     */\n    while (true) {\n      // These lines below can be shorten with computeIfAbsent in Java8\n      connection = connections.get(remoteId);\n      if (connection == null) {\n        connection = new Connection(remoteId, serviceClass);\n        Connection existing = connections.putIfAbsent(remoteId, connection);\n        if (existing != null) {\n          connection = existing;\n        }\n      }\n\n      if (connection.addCall(call)) {\n        break;\n      } else {\n        // This connection is closed, should be removed. But other thread could\n        // have already known this closedConnection, and replace it with a new\n        // connection. So we should call conditional remove to make sure we only\n        // remove this closedConnection.\n        connections.remove(remoteId, connection);\n      }\n    }\n\n    // If the server happens to be slow, the method below will take longer to\n    // establish a connection.\n    connection.setupIOstreams(fallbackToSimpleAuth);\n    return connection;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.invoke": "    public Message invoke(Object proxy, final Method method, Object[] args)\n        throws ServiceException {\n      long startTime = 0;\n      if (LOG.isDebugEnabled()) {\n        startTime = Time.now();\n      }\n      \n      if (args.length != 2) { // RpcController + Message\n        throw new ServiceException(\n            \"Too many or few parameters for request. Method: [\"\n            + method.getName() + \"]\" + \", Expected: 2, Actual: \"\n            + args.length);\n      }\n      if (args[1] == null) {\n        throw new ServiceException(\"null param while calling Method: [\"\n            + method.getName() + \"]\");\n      }\n\n      // if Tracing is on then start a new span for this rpc.\n      // guard it in the if statement to make sure there isn't\n      // any extra string manipulation.\n      Tracer tracer = Tracer.curThreadTracer();\n      TraceScope traceScope = null;\n      if (tracer != null) {\n        traceScope = tracer.newScope(RpcClientUtil.methodToTraceString(method));\n      }\n\n      RequestHeaderProto rpcRequestHeader = constructRpcRequestHeader(method);\n      \n      if (LOG.isTraceEnabled()) {\n        LOG.trace(Thread.currentThread().getId() + \": Call -> \" +\n            remoteId + \": \" + method.getName() +\n            \" {\" + TextFormat.shortDebugString((Message) args[1]) + \"}\");\n      }\n\n\n      final Message theRequest = (Message) args[1];\n      final RpcWritable.Buffer val;\n      try {\n        val = (RpcWritable.Buffer) client.call(RPC.RpcKind.RPC_PROTOCOL_BUFFER,\n            new RpcProtobufRequest(rpcRequestHeader, theRequest), remoteId,\n            fallbackToSimpleAuth);\n\n      } catch (Throwable e) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Exception <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + e + \"}\");\n        }\n        if (traceScope != null) {\n          traceScope.addTimelineAnnotation(\"Call got exception: \" +\n              e.toString());\n        }\n        throw new ServiceException(e);\n      } finally {\n        if (traceScope != null) traceScope.close();\n      }\n\n      if (LOG.isDebugEnabled()) {\n        long callTime = Time.now() - startTime;\n        LOG.debug(\"Call: \" + method.getName() + \" took \" + callTime + \"ms\");\n      }\n      \n      if (Client.isAsynchronousMode()) {\n        final AsyncGet<RpcWritable.Buffer, IOException> arr\n            = Client.getAsyncRpcResponse();\n        final AsyncGet<Message, Exception> asyncGet\n            = new AsyncGet<Message, Exception>() {\n          @Override\n          public Message get(long timeout, TimeUnit unit) throws Exception {\n            return getReturnMessage(method, arr.get(timeout, unit));\n          }\n\n          @Override\n          public boolean isDone() {\n            return arr.isDone();\n          }\n        };\n        ASYNC_RETURN_MESSAGE.set(asyncGet);\n        return null;\n      } else {\n        return getReturnMessage(method, val);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.close": "    public void close() throws IOException {\n      if (!isClosed) {\n        isClosed = true;\n        CLIENTS.stopClient(client);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.isDone": "          public boolean isDone() {\n            return arr.isDone();\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getReturnMessage": "    private Message getReturnMessage(final Method method,\n        final RpcWritable.Buffer buf) throws ServiceException {\n      Message prototype = null;\n      try {\n        prototype = getReturnProtoType(method);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      Message returnMessage;\n      try {\n        returnMessage = buf.getValue(prototype.getDefaultInstanceForType());\n\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Response <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + TextFormat.shortDebugString(returnMessage) + \"}\");\n        }\n\n      } catch (Throwable e) {\n        throw new ServiceException(e);\n      }\n      return returnMessage;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.constructRpcRequestHeader": "    private RequestHeaderProto constructRpcRequestHeader(Method method) {\n      RequestHeaderProto.Builder builder = RequestHeaderProto\n          .newBuilder();\n      builder.setMethodName(method.getName());\n     \n\n      // For protobuf, {@code protocol} used when creating client side proxy is\n      // the interface extending BlockingInterface, which has the annotations \n      // such as ProtocolName etc.\n      //\n      // Using Method.getDeclaringClass(), as in WritableEngine to get at\n      // the protocol interface will return BlockingInterface, from where \n      // the annotation ProtocolName and Version cannot be\n      // obtained.\n      //\n      // Hence we simply use the protocol class used to create the proxy.\n      // For PB this may limit the use of mixins on client side.\n      builder.setDeclaringClassProtocolName(protocolName);\n      builder.setClientProtocolVersion(clientProtocolVersion);\n      return builder.build();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.toString": "    public String toString() {\n      try {\n        RequestHeaderProto header = getRequestHeader();\n        return header.getDeclaringClassProtocolName() + \".\" +\n               header.getMethodName();\n      } catch (IOException e) {\n        throw new IllegalArgumentException(e);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod": "  protected Object invokeMethod(Method method, Object[] args) throws Throwable {\n    try {\n      if (!method.isAccessible()) {\n        method.setAccessible(true);\n      }\n      final Object r = method.invoke(proxyDescriptor.getProxy(), args);\n      hasSuccessfulCall = true;\n      return r;\n    } catch (InvocationTargetException e) {\n      throw e.getCause();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.getProxy": "    synchronized T getProxy() {\n      return proxyInfo.proxy;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invoke": "  public Object invoke(Object proxy, Method method, Object[] args)\n      throws Throwable {\n    final boolean isRpc = isRpcInvocation(proxyDescriptor.getProxy());\n    final int callId = isRpc? Client.nextCallId(): RpcConstants.INVALID_CALL_ID;\n\n    final Call call = newCall(method, args, isRpc, callId);\n    while (true) {\n      final CallReturn c = call.invokeOnce();\n      final CallReturn.State state = c.getState();\n      if (state == CallReturn.State.ASYNC_INVOKED) {\n        return null; // return null for async calls\n      } else if (c.getState() != CallReturn.State.RETRY) {\n        return c.getReturnValue();\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invokeOnce": "    synchronized CallReturn invokeOnce() {\n      try {\n        if (retryInfo != null) {\n          return processWaitTimeAndRetryInfo();\n        }\n\n        // The number of times this invocation handler has ever been failed over\n        // before this method invocation attempt. Used to prevent concurrent\n        // failed method invocations from triggering multiple failover attempts.\n        final long failoverCount = retryInvocationHandler.getFailoverCount();\n        try {\n          return invoke();\n        } catch (Exception e) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(toString(), e);\n          }\n          if (Thread.currentThread().isInterrupted()) {\n            // If interrupted, do not retry.\n            throw e;\n          }\n\n          retryInfo = retryInvocationHandler.handleException(\n              method, callId, retryPolicy, counters, failoverCount, e);\n          return processWaitTimeAndRetryInfo();\n        }\n      } catch(Throwable t) {\n        return new CallReturn(t);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.getFailoverCount": "  private long getFailoverCount() {\n    return proxyDescriptor.getFailoverCount();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.handleException": "  private RetryInfo handleException(final Method method, final int callId,\n      final RetryPolicy policy, final Counters counters,\n      final long expectFailoverCount, final Exception e) throws Exception {\n    final RetryInfo retryInfo = RetryInfo.newRetryInfo(policy, e,\n        counters, proxyDescriptor.idempotentOrAtMostOnce(method),\n        expectFailoverCount);\n    if (retryInfo.isFail()) {\n      // fail.\n      if (retryInfo.action.reason != null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Exception while invoking call #\" + callId + \" \"\n              + proxyDescriptor.getProxyInfo().getString(method.getName())\n              + \". Not retrying because \" + retryInfo.action.reason, e);\n        }\n      }\n      throw retryInfo.getFailException();\n    }\n\n    log(method, retryInfo.isFailover(), counters.failovers, retryInfo.delay, e);\n    return retryInfo;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.processWaitTimeAndRetryInfo": "    CallReturn processWaitTimeAndRetryInfo() throws InterruptedIOException {\n      final Long waitTime = getWaitTime(Time.monotonicNow());\n      LOG.trace(\"#{} processRetryInfo: retryInfo={}, waitTime={}\",\n          callId, retryInfo, waitTime);\n      if (waitTime != null && waitTime > 0) {\n        try {\n          Thread.sleep(retryInfo.delay);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Interrupted while waiting to retry\", e);\n          }\n          InterruptedIOException intIOE = new InterruptedIOException(\n              \"Retry interrupted\");\n          intIOE.initCause(e);\n          throw intIOE;\n        }\n      }\n      processRetryInfo();\n      return CallReturn.RETRY;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.toString": "    public String toString() {\n      return \"RetryInfo{\" +\n              \"retryTime=\" + retryTime +\n              \", delay=\" + delay +\n              \", action=\" + action +\n              \", expectedFailoverCount=\" + expectedFailoverCount +\n              \", failException=\" + failException +\n              '}';\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystemLinkResolver.resolve": "  public T resolve(final FileSystem filesys, final Path path)\n      throws IOException {\n    int count = 0;\n    T in = null;\n    Path p = path;\n    // Assumes path belongs to this FileSystem.\n    // Callers validate this by passing paths through FileSystem#checkPath\n    FileSystem fs = filesys;\n    for (boolean isLink = true; isLink;) {\n      try {\n        in = doCall(p);\n        isLink = false;\n      } catch (UnresolvedLinkException e) {\n        if (!filesys.resolveSymlinks) {\n          throw new IOException(\"Path \" + path + \" contains a symlink\"\n              + \" and symlink resolution is disabled (\"\n              + CommonConfigurationKeys.FS_CLIENT_RESOLVE_REMOTE_SYMLINKS_KEY\n              + \").\", e);\n        }\n        if (!FileSystem.areSymlinksEnabled()) {\n          throw new IOException(\"Symlink resolution is disabled in\" +\n              \" this version of Hadoop.\");\n        }\n        if (count++ > FsConstants.MAX_PATH_LINKS) {\n          throw new IOException(\"Possible cyclic loop while \" +\n                                \"following symbolic link \" + path);\n        }\n        // Resolve the first unresolved path component\n        p = FSLinkResolver.qualifySymlinkTarget(fs.getUri(), p,\n            filesys.resolveLink(p));\n        fs = FileSystem.getFSofPath(p, filesys.getConf());\n        // Have to call next if it's a new FS\n        if (!fs.equals(filesys)) {\n          return next(fs, p);\n        }\n        // Else, we keep resolving with this filesystem\n      }\n    }\n    // Successful call, path was fully resolved\n    return in;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystemLinkResolver.next": "  abstract public T next(final FileSystem fs, final Path p) throws IOException;\n\n  /**\n   * Attempt calling overridden {@link #doCall(Path)} method with",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystemLinkResolver.doCall": "  abstract public T doCall(final Path p) throws IOException,\n      UnresolvedLinkException;\n\n  /**\n   * Calls the abstract FileSystem call equivalent to the specialized subclass\n   * implementation in {@link #doCall(Path)}. This is used when retrying the",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobResourceUploader.disableErasureCodingForPath": "  private void disableErasureCodingForPath(FileSystem fs, Path path)\n      throws IOException {\n    if (jtFs instanceof DistributedFileSystem) {\n      LOG.info(\"Disabling Erasure Coding for path: \" + path);\n      DistributedFileSystem dfs = (DistributedFileSystem) jtFs;\n      dfs.setErasureCodingPolicy(path,\n          SystemErasureCodingPolicies.getReplicationPolicy().getName());\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal": "  private void uploadResourcesInternal(Job job, Path submitJobDir)\n      throws IOException {\n    Configuration conf = job.getConfiguration();\n    short replication =\n        (short) conf.getInt(Job.SUBMIT_REPLICATION,\n            Job.DEFAULT_SUBMIT_REPLICATION);\n\n    if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n      LOG.warn(\"Hadoop command-line option parsing not performed. \"\n          + \"Implement the Tool interface and execute your application \"\n          + \"with ToolRunner to remedy this.\");\n    }\n\n    //\n    // Figure out what fs the JobTracker is using. Copy the\n    // job to it, under a temporary name. This allows DFS to work,\n    // and under the local fs also provides UNIX-like object loading\n    // semantics. (that is, if the job file is deleted right after\n    // submission, we can still run the submission to completion)\n    //\n\n    // Create a number of filenames in the JobTracker's fs namespace\n    LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n    if (jtFs.exists(submitJobDir)) {\n      throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n          + \" already exists!! This is unexpected.Please check what's there in\"\n          + \" that directory\");\n    }\n    // Create the submission directory for the MapReduce job.\n    submitJobDir = jtFs.makeQualified(submitJobDir);\n    submitJobDir = new Path(submitJobDir.toUri().getPath());\n    FsPermission mapredSysPerms =\n        new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    mkdirs(jtFs, submitJobDir, mapredSysPerms);\n\n    if (!conf.getBoolean(MRJobConfig.MR_AM_STAGING_DIR_ERASURECODING_ENABLED,\n        MRJobConfig.DEFAULT_MR_AM_STAGING_ERASURECODING_ENABLED)) {\n      disableErasureCodingForPath(jtFs, submitJobDir);\n    }\n\n    // Get the resources that have been added via command line arguments in the\n    // GenericOptionsParser (i.e. files, libjars, archives).\n    Collection<String> files = conf.getStringCollection(\"tmpfiles\");\n    Collection<String> libjars = conf.getStringCollection(\"tmpjars\");\n    Collection<String> archives = conf.getStringCollection(\"tmparchives\");\n    String jobJar = job.getJar();\n\n    // Merge resources that have been programmatically specified for the shared\n    // cache via the Job API.\n    files.addAll(conf.getStringCollection(MRJobConfig.FILES_FOR_SHARED_CACHE));\n    libjars.addAll(conf.getStringCollection(\n            MRJobConfig.FILES_FOR_CLASSPATH_AND_SHARED_CACHE));\n    archives.addAll(conf\n        .getStringCollection(MRJobConfig.ARCHIVES_FOR_SHARED_CACHE));\n\n\n    Map<URI, FileStatus> statCache = new HashMap<URI, FileStatus>();\n    checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n\n    Map<String, Boolean> fileSCUploadPolicies =\n        new LinkedHashMap<String, Boolean>();\n    Map<String, Boolean> archiveSCUploadPolicies =\n        new LinkedHashMap<String, Boolean>();\n\n    uploadFiles(job, files, submitJobDir, mapredSysPerms, replication,\n        fileSCUploadPolicies, statCache);\n    uploadLibJars(job, libjars, submitJobDir, mapredSysPerms, replication,\n        fileSCUploadPolicies, statCache);\n    uploadArchives(job, archives, submitJobDir, mapredSysPerms, replication,\n        archiveSCUploadPolicies, statCache);\n    uploadJobJar(job, jobJar, submitJobDir, replication, statCache);\n    addLog4jToDistributedCache(job, submitJobDir);\n\n    // Note, we do not consider resources in the distributed cache for the\n    // shared cache at this time. Only resources specified via the\n    // GenericOptionsParser or the jobjar.\n    Job.setFileSharedCacheUploadPolicies(conf, fileSCUploadPolicies);\n    Job.setArchiveSharedCacheUploadPolicies(conf, archiveSCUploadPolicies);\n\n    // set the timestamps of the archives and files\n    // set the public/private visibility of the archives and files\n    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n        statCache);\n    // get DelegationToken for cached file\n    ClientDistributedCacheManager.getDelegationTokens(conf,\n        job.getCredentials());\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobResourceUploader.uploadFiles": "  void uploadFiles(Job job, Collection<String> files,\n      Path submitJobDir, FsPermission mapredSysPerms, short submitReplication,\n      Map<String, Boolean> fileSCUploadPolicies, Map<URI, FileStatus> statCache)\n      throws IOException {\n    Configuration conf = job.getConfiguration();\n    Path filesDir = JobSubmissionFiles.getJobDistCacheFiles(submitJobDir);\n    if (!files.isEmpty()) {\n      mkdirs(jtFs, filesDir, mapredSysPerms);\n      for (String tmpFile : files) {\n        URI tmpURI = null;\n        try {\n          tmpURI = new URI(tmpFile);\n        } catch (URISyntaxException e) {\n          throw new IllegalArgumentException(\"Error parsing files argument.\"\n              + \" Argument must be a valid URI: \" + tmpFile, e);\n        }\n        Path tmp = new Path(tmpURI);\n        URI newURI = null;\n        boolean uploadToSharedCache = false;\n        if (scConfig.isSharedCacheFilesEnabled()) {\n          newURI = useSharedCache(tmpURI, tmp.getName(), statCache, conf, true);\n          if (newURI == null) {\n            uploadToSharedCache = true;\n          }\n        }\n\n        if (newURI == null) {\n          Path newPath =\n              copyRemoteFiles(filesDir, tmp, conf, submitReplication);\n          try {\n            newURI = getPathURI(newPath, tmpURI.getFragment());\n          } catch (URISyntaxException ue) {\n            // should not throw a uri exception\n            throw new IOException(\n                \"Failed to create a URI (URISyntaxException) for the\"\n                    + \" remote path \" + newPath\n                    + \". This was based on the files parameter: \" + tmpFile,\n                ue);\n          }\n        }\n\n        job.addCacheFile(newURI);\n        if (scConfig.isSharedCacheFilesEnabled()) {\n          fileSCUploadPolicies.put(newURI.toString(), uploadToSharedCache);\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobResourceUploader.uploadJobJar": "  void uploadJobJar(Job job, String jobJar, Path submitJobDir,\n      short submitReplication, Map<URI, FileStatus> statCache)\n      throws IOException {\n    Configuration conf = job.getConfiguration();\n    if (jobJar != null) { // copy jar to JobTracker's fs\n      // use jar name if job is not named.\n      if (\"\".equals(job.getJobName())) {\n        job.setJobName(new Path(jobJar).getName());\n      }\n      Path jobJarPath = new Path(jobJar);\n      URI jobJarURI = jobJarPath.toUri();\n      Path newJarPath = null;\n      boolean uploadToSharedCache = false;\n      if (jobJarURI.getScheme() == null ||\n          jobJarURI.getScheme().equals(\"file\")) {\n        // job jar is on the local file system\n        if (scConfig.isSharedCacheJobjarEnabled()) {\n          // We must have a qualified path for the shared cache client. We can\n          // assume this is for the local filesystem\n          jobJarPath = FileSystem.getLocal(conf).makeQualified(jobJarPath);\n          // Don't add a resource name here because the resource name (i.e.\n          // job.jar directory symlink) will always be hard coded to job.jar for\n          // the job.jar\n          URI newURI =\n              useSharedCache(jobJarPath.toUri(), null, statCache, conf, false);\n          if (newURI == null) {\n            uploadToSharedCache = true;\n          } else {\n            newJarPath = stringToPath(newURI.toString());\n            // The job jar is coming from the shared cache (i.e. a public\n            // place), so we want the job.jar to have a public visibility.\n            conf.setBoolean(MRJobConfig.JOBJAR_VISIBILITY, true);\n          }\n        }\n        if (newJarPath == null) {\n          newJarPath = JobSubmissionFiles.getJobJar(submitJobDir);\n          copyJar(jobJarPath, newJarPath, submitReplication);\n        }\n      } else {\n        // job jar is in a remote file system\n        if (scConfig.isSharedCacheJobjarEnabled()) {\n          // Don't add a resource name here because the resource name (i.e.\n          // job.jar directory symlink) will always be hard coded to job.jar for\n          // the job.jar\n          URI newURI = useSharedCache(jobJarURI, null, statCache, conf, false);\n          if (newURI == null) {\n            uploadToSharedCache = true;\n            newJarPath = jobJarPath;\n          } else {\n            newJarPath = stringToPath(newURI.toString());\n            // The job jar is coming from the shared cache (i.e. a public\n            // place), so we want the job.jar to have a public visibility.\n            conf.setBoolean(MRJobConfig.JOBJAR_VISIBILITY, true);\n          }\n        } else {\n          // we don't need to upload the jobjar to the staging directory because\n          // it is already in an accessible place\n          newJarPath = jobJarPath;\n        }\n      }\n      job.setJar(newJarPath.toString());\n      if (scConfig.isSharedCacheJobjarEnabled()) {\n        conf.setBoolean(MRJobConfig.JOBJAR_SHARED_CACHE_UPLOAD_POLICY,\n            uploadToSharedCache);\n      }\n    } else {\n      LOG.warn(\"No job jar file set.  User classes may not be found. \"\n          + \"See Job or Job#setJar(String).\");\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobResourceUploader.uploadArchives": "  void uploadArchives(Job job, Collection<String> archives,\n      Path submitJobDir, FsPermission mapredSysPerms, short submitReplication,\n      Map<String, Boolean> archiveSCUploadPolicies,\n      Map<URI, FileStatus> statCache) throws IOException {\n    Configuration conf = job.getConfiguration();\n    Path archivesDir = JobSubmissionFiles.getJobDistCacheArchives(submitJobDir);\n    if (!archives.isEmpty()) {\n      mkdirs(jtFs, archivesDir, mapredSysPerms);\n      for (String tmpArchives : archives) {\n        URI tmpURI;\n        try {\n          tmpURI = new URI(tmpArchives);\n        } catch (URISyntaxException e) {\n          throw new IllegalArgumentException(\"Error parsing archives argument.\"\n              + \" Argument must be a valid URI: \" + tmpArchives, e);\n        }\n        Path tmp = new Path(tmpURI);\n        URI newURI = null;\n        boolean uploadToSharedCache = false;\n        if (scConfig.isSharedCacheArchivesEnabled()) {\n          newURI = useSharedCache(tmpURI, tmp.getName(), statCache, conf, true);\n          if (newURI == null) {\n            uploadToSharedCache = true;\n          }\n        }\n\n        if (newURI == null) {\n          Path newPath =\n              copyRemoteFiles(archivesDir, tmp, conf, submitReplication);\n          try {\n            newURI = getPathURI(newPath, tmpURI.getFragment());\n          } catch (URISyntaxException ue) {\n            // should not throw a uri exception\n            throw new IOException(\n                \"Failed to create a URI (URISyntaxException) for the\"\n                    + \" remote path \" + newPath\n                    + \". This was based on the archive parameter: \"\n                    + tmpArchives,\n                ue);\n          }\n        }\n\n        job.addCacheArchive(newURI);\n        if (scConfig.isSharedCacheArchivesEnabled()) {\n          archiveSCUploadPolicies.put(newURI.toString(), uploadToSharedCache);\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobResourceUploader.mkdirs": "  boolean mkdirs(FileSystem fs, Path dir, FsPermission permission)\n      throws IOException {\n    return FileSystem.mkdirs(fs, dir, permission);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobResourceUploader.checkLocalizationLimits": "  void checkLocalizationLimits(Configuration conf, Collection<String> files,\n      Collection<String> libjars, Collection<String> archives, String jobJar,\n      Map<URI, FileStatus> statCache) throws IOException {\n\n    LimitChecker limitChecker = new LimitChecker(conf);\n    if (!limitChecker.hasLimits()) {\n      // there are no limits set, so we are done.\n      return;\n    }\n\n    // Get the files and archives that are already in the distributed cache\n    Collection<String> dcFiles =\n        conf.getStringCollection(MRJobConfig.CACHE_FILES);\n    Collection<String> dcArchives =\n        conf.getStringCollection(MRJobConfig.CACHE_ARCHIVES);\n\n    for (String uri : dcFiles) {\n      explorePath(conf, stringToPath(uri), limitChecker, statCache);\n    }\n\n    for (String uri : dcArchives) {\n      explorePath(conf, stringToPath(uri), limitChecker, statCache);\n    }\n\n    for (String uri : files) {\n      explorePath(conf, stringToPath(uri), limitChecker, statCache);\n    }\n\n    for (String uri : libjars) {\n      explorePath(conf, stringToPath(uri), limitChecker, statCache);\n    }\n\n    for (String uri : archives) {\n      explorePath(conf, stringToPath(uri), limitChecker, statCache);\n    }\n\n    if (jobJar != null) {\n      explorePath(conf, stringToPath(jobJar), limitChecker, statCache);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobResourceUploader.addLog4jToDistributedCache": "  private void addLog4jToDistributedCache(Job job, Path jobSubmitDir)\n      throws IOException {\n    Configuration conf = job.getConfiguration();\n    String log4jPropertyFile =\n        conf.get(MRJobConfig.MAPREDUCE_JOB_LOG4J_PROPERTIES_FILE, \"\");\n    if (!log4jPropertyFile.isEmpty()) {\n      short replication = (short) conf.getInt(Job.SUBMIT_REPLICATION, 10);\n      copyLog4jPropertyFile(job, jobSubmitDir, replication);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobResourceUploader.uploadLibJars": "  void uploadLibJars(Job job, Collection<String> libjars, Path submitJobDir,\n      FsPermission mapredSysPerms, short submitReplication,\n      Map<String, Boolean> fileSCUploadPolicies, Map<URI, FileStatus> statCache)\n      throws IOException {\n    Configuration conf = job.getConfiguration();\n    Path libjarsDir = JobSubmissionFiles.getJobDistCacheLibjars(submitJobDir);\n    if (!libjars.isEmpty()) {\n      mkdirs(jtFs, libjarsDir, mapredSysPerms);\n      Collection<URI> libjarURIs = new LinkedList<>();\n      boolean foundFragment = false;\n      for (String tmpjars : libjars) {\n        URI tmpURI = null;\n        try {\n          tmpURI = new URI(tmpjars);\n        } catch (URISyntaxException e) {\n          throw new IllegalArgumentException(\"Error parsing libjars argument.\"\n              + \" Argument must be a valid URI: \" + tmpjars, e);\n        }\n        Path tmp = new Path(tmpURI);\n        URI newURI = null;\n        boolean uploadToSharedCache = false;\n        boolean fromSharedCache = false;\n        if (scConfig.isSharedCacheLibjarsEnabled()) {\n          newURI = useSharedCache(tmpURI, tmp.getName(), statCache, conf, true);\n          if (newURI == null) {\n            uploadToSharedCache = true;\n          } else {\n            fromSharedCache = true;\n          }\n        }\n\n        if (newURI == null) {\n          Path newPath =\n              copyRemoteFiles(libjarsDir, tmp, conf, submitReplication);\n          try {\n            newURI = getPathURI(newPath, tmpURI.getFragment());\n          } catch (URISyntaxException ue) {\n            // should not throw a uri exception\n            throw new IOException(\n                \"Failed to create a URI (URISyntaxException) for the\"\n                    + \" remote path \" + newPath\n                    + \". This was based on the libjar parameter: \" + tmpjars,\n                ue);\n          }\n        }\n\n        if (!foundFragment) {\n          // We do not count shared cache paths containing fragments as a\n          // \"foundFragment.\" This is because these resources are not in the\n          // staging directory and will be added to the distributed cache\n          // separately.\n          foundFragment = (newURI.getFragment() != null) && !fromSharedCache;\n        }\n        DistributedCache.addFileToClassPath(new Path(newURI.getPath()), conf,\n            jtFs, false);\n        if (fromSharedCache) {\n          // We simply add this URI to the distributed cache. It will not come\n          // from the staging directory (it is in the shared cache), so we\n          // must add it to the cache regardless of the wildcard feature.\n          DistributedCache.addCacheFile(newURI, conf);\n        } else {\n          libjarURIs.add(newURI);\n        }\n\n        if (scConfig.isSharedCacheLibjarsEnabled()) {\n          fileSCUploadPolicies.put(newURI.toString(), uploadToSharedCache);\n        }\n      }\n\n      if (useWildcard && !foundFragment) {\n        // Add the whole directory to the cache using a wild card\n        Path libJarsDirWildcard =\n            jtFs.makeQualified(new Path(libjarsDir, DistributedCache.WILDCARD));\n        DistributedCache.addCacheFile(libJarsDirWildcard.toUri(), conf);\n      } else {\n        for (URI uri : libjarURIs) {\n          DistributedCache.addCacheFile(uri, conf);\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources": "  public void uploadResources(Job job, Path submitJobDir) throws IOException {\n    try {\n      initSharedCache(job.getJobID(), job.getConfiguration());\n      uploadResourcesInternal(job, submitJobDir);\n    } finally {\n      stopSharedCache();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobResourceUploader.initSharedCache": "  private void initSharedCache(JobID jobid, Configuration conf) {\n    this.scConfig.init(conf);\n    if (this.scConfig.isSharedCacheEnabled()) {\n      this.scClient = createSharedCacheClient(conf);\n      appId = jobIDToAppId(jobid);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobResourceUploader.stopSharedCache": "  private void stopSharedCache() {\n    if (scClient != null) {\n      scClient.stop();\n      scClient = null;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles": "  private void copyAndConfigureFiles(Job job, Path jobSubmitDir) \n  throws IOException {\n    Configuration conf = job.getConfiguration();\n    boolean useWildcards = conf.getBoolean(Job.USE_WILDCARD_FOR_LIBJARS,\n        Job.DEFAULT_USE_WILDCARD_FOR_LIBJARS);\n    JobResourceUploader rUploader = new JobResourceUploader(jtFs, useWildcards);\n\n    rUploader.uploadResources(job, jobSubmitDir);\n\n    // Get the working directory. If not set, sets it to filesystem working dir\n    // This code has been added so that working directory reset before running\n    // the job. This is necessary for backward compatibility as other systems\n    // might use the public API JobConf#setWorkingDirectory to reset the working\n    // directory.\n    job.getWorkingDirectory();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    //validate the jobs output specs \n    checkSpecs(job);\n\n    Configuration conf = job.getConfiguration();\n    addMRFrameworkToDistributedCache(conf);\n\n    Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);\n    //configure the command line options correctly on the submitting dfs\n    InetAddress ip = InetAddress.getLocalHost();\n    if (ip != null) {\n      submitHostAddress = ip.getHostAddress();\n      submitHostName = ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId = submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir = new Path(jobStagingArea, jobId.toString());\n    JobStatus status = null;\n    try {\n      conf.set(MRJobConfig.USER_NAME,\n          UserGroupInformation.getCurrentUser().getShortUserName());\n      conf.set(\"hadoop.http.filter.initializers\", \n          \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n      conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      // generate a secret to authenticate shuffle transfers\n      if (TokenCache.getShuffleSecretKey(job.getCredentials()) == null) {\n        KeyGenerator keyGen;\n        try {\n          keyGen = KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n          keyGen.init(SHUFFLE_KEY_LENGTH);\n        } catch (NoSuchAlgorithmException e) {\n          throw new IOException(\"Error generating shuffle secret key\", e);\n        }\n        SecretKey shuffleKey = keyGen.generateKey();\n        TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n            job.getCredentials());\n      }\n      if (CryptoUtils.isEncryptedSpillEnabled(conf)) {\n        conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS, 1);\n        LOG.warn(\"Max job attempts set to 1 since encrypted intermediate\" +\n                \"data spill is enabled\");\n      }\n\n      copyAndConfigureFiles(job, submitJobDir);\n\n      Path submitJobFile = JobSubmissionFiles.getJobConfPath(submitJobDir);\n      \n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps = writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      int maxMaps = conf.getInt(MRJobConfig.JOB_MAX_MAP,\n          MRJobConfig.DEFAULT_JOB_MAX_MAP);\n      if (maxMaps >= 0 && maxMaps < maps) {\n        throw new IllegalArgumentException(\"The number of map tasks \" + maps +\n            \" exceeded limit \" + maxMaps);\n      }\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue = conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl = submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // removing jobtoken referrals before copying the jobconf to HDFS\n      // as the tasks don't need this setting, actually they may break\n      // because of it if present as the referral will point to a\n      // different job.\n      TokenCache.cleanUpTokenReferral(conf);\n\n      if (conf.getBoolean(\n          MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,\n          MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {\n        // Add HDFS tracking ids\n        ArrayList<String> trackingIds = new ArrayList<String>();\n        for (Token<? extends TokenIdentifier> t :\n            job.getCredentials().getAllTokens()) {\n          trackingIds.add(t.decodeIdentifier().getTrackingId());\n        }\n        conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,\n            trackingIds.toArray(new String[trackingIds.size()]));\n      }\n\n      // Set reservation info if it exists\n      ReservationId reservationId = job.getReservationId();\n      if (reservationId != null) {\n        conf.set(MRJobConfig.RESERVATION_ID, reservationId.toString());\n      }\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status = submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status != null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status == null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs != null && submitJobDir != null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmitter.writeConf": "  private void writeConf(Configuration conf, Path jobFile) \n      throws IOException {\n    // Write job file to JobTracker's fs        \n    FSDataOutputStream out = \n      FileSystem.create(jtFs, jobFile, \n                        new FsPermission(JobSubmissionFiles.JOB_FILE_PERMISSION));\n    try {\n      conf.writeXml(out);\n    } finally {\n      out.close();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmitter.addMRFrameworkToDistributedCache": "  private static void addMRFrameworkToDistributedCache(Configuration conf)\n      throws IOException {\n    String framework =\n        conf.get(MRJobConfig.MAPREDUCE_APPLICATION_FRAMEWORK_PATH, \"\");\n    if (!framework.isEmpty()) {\n      URI uri;\n      try {\n        uri = new URI(framework);\n      } catch (URISyntaxException e) {\n        throw new IllegalArgumentException(\"Unable to parse '\" + framework\n            + \"' as a URI, check the setting for \"\n            + MRJobConfig.MAPREDUCE_APPLICATION_FRAMEWORK_PATH, e);\n      }\n\n      String linkedName = uri.getFragment();\n\n      // resolve any symlinks in the URI path so using a \"current\" symlink\n      // to point to a specific version shows the specific version\n      // in the distributed cache configuration\n      FileSystem fs = FileSystem.get(uri, conf);\n      Path frameworkPath = fs.makeQualified(\n          new Path(uri.getScheme(), uri.getAuthority(), uri.getPath()));\n      FileContext fc = FileContext.getFileContext(frameworkPath.toUri(), conf);\n      frameworkPath = fc.resolvePath(frameworkPath);\n      uri = frameworkPath.toUri();\n      try {\n        uri = new URI(uri.getScheme(), uri.getAuthority(), uri.getPath(),\n            null, linkedName);\n      } catch (URISyntaxException e) {\n        throw new IllegalArgumentException(e);\n      }\n\n      DistributedCache.addCacheArchive(uri, conf);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmitter.printTokens": "  private void printTokens(JobID jobId,\n      Credentials credentials) throws IOException {\n    LOG.info(\"Submitting tokens for job: \" + jobId);\n    LOG.info(\"Executing with tokens: {}\", credentials.getAllTokens());\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmitter.writeSplits": "  private int writeSplits(org.apache.hadoop.mapreduce.JobContext job,\n      Path jobSubmitDir) throws IOException,\n      InterruptedException, ClassNotFoundException {\n    JobConf jConf = (JobConf)job.getConfiguration();\n    int maps;\n    if (jConf.getUseNewMapper()) {\n      maps = writeNewSplits(job, jobSubmitDir);\n    } else {\n      maps = writeOldSplits(jConf, jobSubmitDir);\n    }\n    return maps;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs": "  private void checkSpecs(Job job) throws ClassNotFoundException, \n      InterruptedException, IOException {\n    JobConf jConf = (JobConf)job.getConfiguration();\n    // Check the output specification\n    if (jConf.getNumReduceTasks() == 0 ? \n        jConf.getUseNewMapper() : jConf.getUseNewReducer()) {\n      org.apache.hadoop.mapreduce.OutputFormat<?, ?> output =\n        ReflectionUtils.newInstance(job.getOutputFormatClass(),\n          job.getConfiguration());\n      output.checkOutputSpecs(job);\n    } else {\n      jConf.getOutputFormat().checkOutputSpecs(jtFs, jConf);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmitter.populateTokenCache": "  private void populateTokenCache(Configuration conf, Credentials credentials) \n  throws IOException{\n    readTokensFromFiles(conf, credentials);\n    // add the delegation tokens from configuration\n    String [] nameNodes = conf.getStrings(MRJobConfig.JOB_NAMENODES);\n    LOG.debug(\"adding the following namenodes' delegation tokens:\" + \n        Arrays.toString(nameNodes));\n    if(nameNodes != null) {\n      Path [] ps = new Path[nameNodes.length];\n      for(int i=0; i< nameNodes.length; i++) {\n        ps[i] = new Path(nameNodes[i]);\n      }\n      TokenCache.obtainTokensForNamenodes(credentials, ps, conf);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ToolRunner.run": "  public static int run(Tool tool, String[] args) \n    throws Exception{\n    return run(tool.getConf(), tool, args);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ProgramDriver.invoke": "    public void invoke(String[] args)\n      throws Throwable {\n      try {\n        main.invoke(null, new Object[]{args});\n      } catch (InvocationTargetException except) {\n        throw except.getCause();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ProgramDriver.run": "  public int run(String[] args)\n    throws Throwable \n  {\n    // Make sure they gave us a program name.\n    if (args.length == 0) {\n      System.out.println(\"An example program must be given as the\" + \n                         \" first argument.\");\n      printUsage(programs);\n      return -1;\n    }\n\t\n    // And that it is good.\n    ProgramDescription pgm = programs.get(args[0]);\n    if (pgm == null) {\n      System.out.println(\"Unknown program '\" + args[0] + \"' chosen.\");\n      printUsage(programs);\n      return -1;\n    }\n\t\n    // Remove the leading argument and call main\n    String[] new_args = new String[args.length - 1];\n    for(int i=1; i < args.length; ++i) {\n      new_args[i-1] = args[i];\n    }\n    pgm.invoke(new_args);\n    return 0;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ProgramDriver.printUsage": "  private static void printUsage(Map<String, ProgramDescription> programs) {\n    System.out.println(\"Valid program names are:\");\n    for(Map.Entry<String, ProgramDescription> item : programs.entrySet()) {\n      System.out.println(\"  \" + item.getKey() + \": \" +\n                         item.getValue().getDescription());         \n    } \n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.RunJar.run": "          public void run() {\n            FileUtil.fullyDelete(workDir);\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.RunJar.createClassLoader": "  private ClassLoader createClassLoader(File file, final File workDir)\n      throws MalformedURLException {\n    ClassLoader loader;\n    // see if the client classloader is enabled\n    if (useClientClassLoader()) {\n      StringBuilder sb = new StringBuilder();\n      sb.append(workDir).append(\"/\").\n          append(File.pathSeparator).append(file).\n          append(File.pathSeparator).append(workDir).append(\"/classes/\").\n          append(File.pathSeparator).append(workDir).append(\"/lib/*\");\n      // HADOOP_CLASSPATH is added to the client classpath\n      String hadoopClasspath = getHadoopClasspath();\n      if (hadoopClasspath != null && !hadoopClasspath.isEmpty()) {\n        sb.append(File.pathSeparator).append(hadoopClasspath);\n      }\n      String clientClasspath = sb.toString();\n      // get the system classes\n      String systemClasses = getSystemClasses();\n      List<String> systemClassesList = systemClasses == null ?\n          null :\n          Arrays.asList(StringUtils.getTrimmedStrings(systemClasses));\n      // create an application classloader that isolates the user classes\n      loader = new ApplicationClassLoader(clientClasspath,\n          getClass().getClassLoader(), systemClassesList);\n    } else {\n      List<URL> classPath = new ArrayList<>();\n      classPath.add(new File(workDir + \"/\").toURI().toURL());\n      classPath.add(file.toURI().toURL());\n      classPath.add(new File(workDir, \"classes/\").toURI().toURL());\n      File[] libs = new File(workDir, \"lib\").listFiles();\n      if (libs != null) {\n        for (File lib : libs) {\n          classPath.add(lib.toURI().toURL());\n        }\n      }\n      // create a normal parent-delegating classloader\n      loader = new URLClassLoader(classPath.toArray(new URL[classPath.size()]));\n    }\n    return loader;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.RunJar.ensureDirectory": "  private static void ensureDirectory(File dir) throws IOException {\n    if (!dir.mkdirs() && !dir.isDirectory()) {\n      throw new IOException(\"Mkdirs failed to create \" +\n                            dir.toString());\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.RunJar.unJar": "  public static void unJar(File jarFile, File toDir, Pattern unpackRegex)\n      throws IOException {\n    try (JarFile jar = new JarFile(jarFile)) {\n      int numOfFailedLastModifiedSet = 0;\n      Enumeration<JarEntry> entries = jar.entries();\n      while (entries.hasMoreElements()) {\n        final JarEntry entry = entries.nextElement();\n        if (!entry.isDirectory() &&\n            unpackRegex.matcher(entry.getName()).matches()) {\n          try (InputStream in = jar.getInputStream(entry)) {\n            File file = new File(toDir, entry.getName());\n            ensureDirectory(file.getParentFile());\n            try (OutputStream out = new FileOutputStream(file)) {\n              IOUtils.copyBytes(in, out, BUFFER_SIZE);\n            }\n            if (!file.setLastModified(entry.getTime())) {\n              numOfFailedLastModifiedSet++;\n            }\n          }\n        }\n      }\n      if (numOfFailedLastModifiedSet > 0) {\n        LOG.warn(\"Could not set last modfied time for {} file(s)\",\n            numOfFailedLastModifiedSet);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.RunJar.main": "  public static void main(String[] args) throws Throwable {\n    new RunJar().run(args);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getCurCall": "  public static ThreadLocal<Call> getCurCall() {\n    return CurCall;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcWritable.wrap": "    public static Buffer wrap(ByteBuffer bb) {\n      return new Buffer(bb);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.updateMetrics": "  void updateMetrics(String name, int queueTime, int processingTime,\n                     boolean deferredCall) {\n    rpcMetrics.addRpcQueueTime(queueTime);\n    if (!deferredCall) {\n      rpcMetrics.addRpcProcessingTime(processingTime);\n      rpcDetailedMetrics.addProcessingTime(name, processingTime);\n      callQueue.addResponseTime(name, getPriorityLevel(), queueTime,\n          processingTime);\n      if (isLogSlowRPC()) {\n        logSlowRpcCalls(name, processingTime);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.isLogSlowRPC": "  protected boolean isLogSlowRPC() {\n    return logSlowRPC;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.getPriorityLevel": "    public int getPriorityLevel() {\n      return this.priorityLevel;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.logSlowRpcCalls": "  void logSlowRpcCalls(String methodName, int processingTime) {\n    final int deviation = 3;\n\n    // 1024 for minSampleSize just a guess -- not a number computed based on\n    // sample size analysis. It is chosen with the hope that this\n    // number is high enough to avoid spurious logging, yet useful\n    // in practice.\n    final int minSampleSize = 1024;\n    final double threeSigma = rpcMetrics.getProcessingMean() +\n        (rpcMetrics.getProcessingStdDev() * deviation);\n\n    if ((rpcMetrics.getProcessingSampleCount() > minSampleSize) &&\n        (processingTime > threeSigma)) {\n      if(LOG.isWarnEnabled()) {\n        String client = CurCall.get().toString();\n        LOG.warn(\n            \"Slow RPC : \" + methodName + \" took \" + processingTime +\n                \" milliseconds to process from client \" + client);\n      }\n      rpcMetrics.incrSlowRpc();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.CallerContext.setCurrent": "  public static void setCurrent(CallerContext callerContext) {\n    CurrentCallerContextHolder.CALLER_CONTEXT.set(callerContext);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcClientUtil.methodToTraceString": "  public static String methodToTraceString(Method method) {\n    Class<?> clazz = method.getDeclaringClass();\n    while (true) {\n      Class<?> next = clazz.getEnclosingClass();\n      if (next == null || next.getEnclosingClass() == null) break;\n      clazz = next;\n    }\n    return clazz.getSimpleName() + \"#\" + method.getName();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getAsyncRpcResponse": "  public static <T extends Writable> AsyncGet<T, IOException>\n      getAsyncRpcResponse() {\n    return (AsyncGet<T, IOException>) ASYNC_RPC_RESPONSE.get();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.get": "        public Writable get(long timeout, TimeUnit unit)\n            throws IOException, TimeoutException{\n          boolean done = true;\n          try {\n            final Writable w = getRpcResponse(call, connection, timeout, unit);\n            if (w == null) {\n              done = false;\n              throw new TimeoutException(call + \" timed out \"\n                  + timeout + \" \" + unit);\n            }\n            return w;\n          } finally {\n            if (done) {\n              releaseAsyncCall();\n            }\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.equals": "      public boolean equals(Object obj) {\n        if (obj == this) {\n          return true;\n        }\n        if (obj instanceof Key) {\n          Key that = (Key)obj;\n          return isEqual(this.scheme, that.scheme)\n                 && isEqual(this.authority, that.authority)\n                 && isEqual(this.ugi, that.ugi)\n                 && (this.unique == that.unique);\n        }\n        return false;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.isEqual": "      static boolean isEqual(Object a, Object b) {\n        return a == b || (a != null && a.equals(b));\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getFSofPath": "  protected static FileSystem getFSofPath(final Path absOrFqPath,\n      final Configuration conf)\n      throws UnsupportedFileSystemException, IOException {\n    absOrFqPath.checkNotSchemeWithRelative();\n    absOrFqPath.checkNotRelative();\n\n    // Uses the default FileSystem if not fully qualified\n    return get(absOrFqPath.toUri(), conf);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.get": "    FileSystem get(URI uri, Configuration conf) throws IOException{\n      Key key = new Key(uri, conf);\n      return getInternal(uri, conf, key);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FSLinkResolver.qualifySymlinkTarget": "  public static Path qualifySymlinkTarget(final URI pathURI,\n      Path pathWithLink, Path target) {\n    // NB: makeQualified uses the target's scheme and authority, if\n    // specified, and the scheme and authority of pathURI, if not.\n    final URI targetUri = target.toUri();\n    final String scheme = targetUri.getScheme();\n    final String auth = targetUri.getAuthority();\n    return (scheme == null && auth == null) ? target.makeQualified(pathURI,\n        pathWithLink.getParent()) : target;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.areSymlinksEnabled": "  public static boolean areSymlinksEnabled() {\n    return symlinksEnabled;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getUri": "  public abstract URI getUri();\n\n  /**\n   * Return a canonicalized form of this FileSystem's URI.\n   *\n   * The default implementation simply calls {@link #canonicalizeUri(URI)}",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.Job.setFileSharedCacheUploadPolicies": "  public static void setFileSharedCacheUploadPolicies(Configuration conf,\n      Map<String, Boolean> policies) {\n    setSharedCacheUploadPolicies(conf, policies, true);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.Job.setSharedCacheUploadPolicies": "  private static void setSharedCacheUploadPolicies(Configuration conf,\n      Map<String, Boolean> policies, boolean areFiles) {\n    if (policies != null) {\n      StringBuilder sb = new StringBuilder();\n      Iterator<Map.Entry<String, Boolean>> it = policies.entrySet().iterator();\n      Map.Entry<String, Boolean> e;\n      if (it.hasNext()) {\n        e = it.next();\n        sb.append(e.getKey() + DELIM + e.getValue());\n      } else {\n        // policies is an empty map, just skip setting the parameter\n        return;\n      }\n      while (it.hasNext()) {\n        e = it.next();\n        sb.append(\",\" + e.getKey() + DELIM + e.getValue());\n      }\n      String confParam =\n          areFiles ? MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES\n              : MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n      conf.set(confParam, sb.toString());\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.Job.setArchiveSharedCacheUploadPolicies": "  public static void setArchiveSharedCacheUploadPolicies(Configuration conf,\n      Map<String, Boolean> policies) {\n    setSharedCacheUploadPolicies(conf, policies, false);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.job.getReservationId": "  public ReservationId getReservationId() {\n    return reservationId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.CryptoUtils.isEncryptedSpillEnabled": "  public static boolean isEncryptedSpillEnabled(Configuration conf) {\n    return conf.getBoolean(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA,\n        MRJobConfig.DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmissionFiles.getJobConfPath": "  public static Path getJobConfPath(Path jobSubmitDir) {\n    return new Path(jobSubmitDir, \"job.xml\");\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobID.toString": "  public String toString() {\n    return appendTo(new StringBuilder(JOB)).toString();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobID.appendTo": "  public StringBuilder appendTo(StringBuilder builder) {\n    builder.append(SEPARATOR);\n    builder.append(jtIdentifier);\n    builder.append(SEPARATOR);\n    builder.append(idFormat.format(id));\n    return builder;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir": "  public static Path getStagingDir(Cluster cluster, Configuration conf,\n      UserGroupInformation realUser) throws IOException, InterruptedException {\n    Path stagingArea = cluster.getStagingAreaDir();\n    FileSystem fs = stagingArea.getFileSystem(conf);\n    UserGroupInformation currentUser = realUser.getCurrentUser();\n    try {\n      FileStatus fsStatus = fs.getFileStatus(stagingArea);\n      String fileOwner = fsStatus.getOwner();\n      if (!(fileOwner.equals(currentUser.getShortUserName()) || fileOwner\n          .equalsIgnoreCase(currentUser.getUserName()) || fileOwner\n          .equals(realUser.getShortUserName()) || fileOwner\n          .equalsIgnoreCase(realUser.getUserName()))) {\n        String errorMessage = \"The ownership on the staging directory \" +\n            stagingArea + \" is not as expected. \" +\n            \"It is owned by \" + fileOwner + \". The directory must \" +\n            \"be owned by the submitter \" + currentUser.getShortUserName()\n            + \" or \" + currentUser.getUserName();\n        if (!realUser.getUserName().equals(currentUser.getUserName())) {\n          throw new IOException(\n              errorMessage + \" or \" + realUser.getShortUserName() + \" or \"\n                  + realUser.getUserName());\n        } else {\n          throw new IOException(errorMessage);\n        }\n      }\n      if (!fsStatus.getPermission().equals(JOB_DIR_PERMISSION)) {\n        LOG.info(\"Permissions on staging directory \" + stagingArea + \" are \" +\n            \"incorrect: \" + fsStatus.getPermission() + \". Fixing permissions \" +\n            \"to correct value \" + JOB_DIR_PERMISSION);\n        fs.setPermission(stagingArea, JOB_DIR_PERMISSION);\n      }\n    } catch (FileNotFoundException e) {\n      fs.mkdirs(stagingArea, new FsPermission(JOB_DIR_PERMISSION));\n    }\n    return stagingArea;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs": "  public String[] getRemainingArgs() {\n    return (commandLine == null) ? new String[]{} : commandLine.getArgs();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ShutdownHookManager.get": "  public static ShutdownHookManager get() {\n    return MGR;\n  }"
        },
        "bug_report": {
            "Title": "Downward Compatibility issue: MR job fails because of unknown setErasureCodingPolicy method from 3.x client to HDFS 2.x cluster",
            "Description": "Running teragen failed in the version of hadoop-3.1, and hdfs server is 2.8.\r\n{code:java}\r\nbin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0-SNAPSHOT.jar \u00a0teragen \u00a0100000 /teragen\r\n{code}\r\n\r\nThe reason of failing is 2.8 HDFS does not have setErasureCodingPolicy.\r\n\r\none  solution is parsing RemoteException in JobResourceUploader#disableErasure like this:\r\n{code:java}\r\nprivate void disableErasureCodingForPath(FileSystem fs, Path path)\r\n      throws IOException {\r\n    try {\r\n      if (jtFs instanceof DistributedFileSystem) {\r\n        LOG.info(\"Disabling Erasure Coding for path: \" + path);\r\n        DistributedFileSystem dfs = (DistributedFileSystem) jtFs;\r\n        dfs.setErasureCodingPolicy(path,\r\n            SystemErasureCodingPolicies.getReplicationPolicy().getName());\r\n      }\r\n    } catch (RemoteException e) {\r\n      if (!e.getClassName().equals(RpcNoSuchMethodException.class.getName())) {\r\n        throw e;\r\n      } else {\r\n        LOG.warn(\r\n            \"hdfs server does not have method disableErasureCodingForPath,\" \r\n                + \" and skip disableErasureCodingForPath\", e);\r\n      }\r\n    }\r\n  }\r\n{code}\r\n\r\nDoes anyone have better solution?\r\n\r\nThe detailed exception trace is:\r\n{code:java}\r\n2018-02-26 11:22:53,178 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1518615699369_0006\r\norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RpcNoSuchMethodException): Unknown method setErasureCodingPolicy called on org.apache.hadoop.hdfs.protocol.ClientProtocol protocol.\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:436)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\r\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:846)\r\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:789)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1804)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2457)\r\n\r\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1437)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1347)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\r\n\tat com.sun.proxy.$Proxy11.setErasureCodingPolicy(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setErasureCodingPolicy(ClientNamenodeProtocolTranslatorPB.java:1583)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\r\n\tat com.sun.proxy.$Proxy12.setErasureCodingPolicy(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.DFSClient.setErasureCodingPolicy(DFSClient.java:2678)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$63.doCall(DistributedFileSystem.java:2665)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$63.doCall(DistributedFileSystem.java:2662)\r\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.setErasureCodingPolicy(DistributedFileSystem.java:2680)\r\n\tat org.apache.hadoop.mapreduce.JobResourceUploader.disableErasureCodingForPath(JobResourceUploader.java:882)\r\n\tat org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:174)\r\n\tat org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:131)\r\n\tat org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:102)\r\n\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:197)\r\n\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)\r\n\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)\r\n\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)\r\n\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)\r\n\tat org.apache.hadoop.examples.terasort.TeraGen.run(TeraGen.java:304)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\r\n\tat org.apache.hadoop.examples.terasort.TeraGen.main(TeraGen.java:308)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\r\n\tat org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\r\n\tat org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:304)\r\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:218)\r\n{code}\r\n"
        }
    },
    {
        "filename": "MAPREDUCE-4843.json",
        "creation_time": "2012-12-04T13:39:40.000+0000",
        "stack_trace": "```\norg.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/$username/jobcache/job_201212031626_1115/job.xml in any of the configured local directories\n\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:424)\n\tat org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:160)\n\tat org.apache.hadoop.mapred.TaskTracker.initializeJob(TaskTracker.java:1175)\n\tat org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:1058)\n\tat org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:2213)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead": "    public synchronized Path getLocalPathToRead(String pathStr, \n        Configuration conf) throws IOException {\n      confChanged(conf);\n      int numDirs = localDirs.length;\n      int numDirsSearched = 0;\n      //remove the leading slash from the path (to make sure that the uri\n      //resolution results in a valid path on the dir being checked)\n      if (pathStr.startsWith(\"/\")) {\n        pathStr = pathStr.substring(1);\n      }\n      while (numDirsSearched < numDirs) {\n        Path file = new Path(localDirs[numDirsSearched], pathStr);\n        if (localFS.exists(file)) {\n          return file;\n        }\n        numDirsSearched++;\n      }\n\n      //no path found\n      throw new DiskErrorException (\"Could not find \" + pathStr +\" in any of\" +\n      \" the configured local directories\");\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.LocalDirAllocator.obtainContext": "  private AllocatorPerContext obtainContext(String contextCfgItemName) {\n    synchronized (contexts) {\n      AllocatorPerContext l = contexts.get(contextCfgItemName);\n      if (l == null) {\n        contexts.put(contextCfgItemName, \n                    (l = new AllocatorPerContext(contextCfgItemName)));\n      }\n      return l;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.LocalDirAllocator.confChanged": "    private synchronized void confChanged(Configuration conf) \n        throws IOException {\n      String newLocalDirs = conf.get(contextCfgItemName);\n      if (null == newLocalDirs) {\n        throw new IOException(contextCfgItemName + \" not configured\");\n      }\n      if (!newLocalDirs.equals(savedLocalDirs)) {\n        localDirs = StringUtils.getTrimmedStrings(newLocalDirs);\n        localFS = FileSystem.getLocal(conf);\n        int numDirs = localDirs.length;\n        ArrayList<String> dirs = new ArrayList<String>(numDirs);\n        ArrayList<DF> dfList = new ArrayList<DF>(numDirs);\n        for (int i = 0; i < numDirs; i++) {\n          try {\n            // filter problematic directories\n            Path tmpDir = new Path(localDirs[i]);\n            if(localFS.mkdirs(tmpDir)|| localFS.exists(tmpDir)) {\n              try {\n\n                File tmpFile = tmpDir.isAbsolute()\n                  ? new File(localFS.makeQualified(tmpDir).toUri())\n                  : new File(localDirs[i]);\n\n                DiskChecker.checkDir(tmpFile);\n                dirs.add(tmpFile.getPath());\n                dfList.add(new DF(tmpFile, 30000));\n\n              } catch (DiskErrorException de) {\n                LOG.warn( localDirs[i] + \" is not writable\\n\", de);\n              }\n            } else {\n              LOG.warn( \"Failed to create \" + localDirs[i]);\n            }\n          } catch (IOException ie) { \n            LOG.warn( \"Failed to create \" + localDirs[i] + \": \" +\n                ie.getMessage() + \"\\n\", ie);\n          } //ignore\n        }\n        localDirs = dirs.toArray(new String[dirs.size()]);\n        dirDF = dfList.toArray(new DF[dirs.size()]);\n        savedLocalDirs = newLocalDirs;\n        \n        // randomize the first disk picked in the round-robin selection \n        dirNumLastAccessed = dirIndexRandomizer.nextInt(dirs.size());\n      }\n    }"
        },
        "bug_report": {
            "Title": "When using DefaultTaskController, JobLocalizer not thread safe",
            "Description": "In our cluster, some times job will failed due to below exception:\n2012-12-03 23:11:54,811 WARN org.apache.hadoop.mapred.TaskTracker: Error initializing attempt_201212031626_1115_r_000023_0:\norg.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/$username/jobcache/job_201212031626_1115/job.xml in any of the configured local directories\n\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:424)\n\tat org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:160)\n\tat org.apache.hadoop.mapred.TaskTracker.initializeJob(TaskTracker.java:1175)\n\tat org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:1058)\n\tat org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:2213)\n\nThe root cause is JobLocalizer is not thread safe.\nIn DefaultTaskController.initializeJob method:\n     JobLocalizer localizer = new JobLocalizer((JobConf)getConf(), user, jobid);\nbut in JobLocalizer, it just simply keep the reference of the conf.\nWhen two TaskLauncher threads(mapLauncher and reduceLauncher) try to initializeJob at same time, it will have two JobLocalizer, but only one conf instance.\nSo some times ttConf.setStrings(JOB_LOCAL_CTXT, localDirs) will reset previous job's conf.\nThen it will cause the previous job's job.xml stored at another user's dir."
        }
    },
    {
        "filename": "MAPREDUCE-5028.json",
        "creation_time": "2013-02-26T03:54:25.000+0000",
        "stack_trace": "```\njava.io.IOException: Spill failed\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1031)\n\tat org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:692)\n\tat org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)\n\tat org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:45)\n\tat org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:34)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1149)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:375)\n\tat org.apache.hadoop.io.IntWritable.readFields(IntWritable.java:38)\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:40)\n\tat org.apache.hadoop.mapreduce.ReduceContext.nextKeyValue(ReduceContext.java:116)\n\tat org.apache.hadoop.mapreduce.ReduceContext.nextKey(ReduceContext.java:92)\n\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:175)\n\tat org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1505)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1438)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1800(MapTask.java:855)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1346)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.collect": "    public synchronized void collect(K key, V value, final int partition\n                                     ) throws IOException {\n      reporter.progress();\n      if (key.getClass() != keyClass) {\n        throw new IOException(\"Type mismatch in key from map: expected \"\n                              + keyClass.getName() + \", received \"\n                              + key.getClass().getName());\n      }\n      if (value.getClass() != valClass) {\n        throw new IOException(\"Type mismatch in value from map: expected \"\n                              + valClass.getName() + \", received \"\n                              + value.getClass().getName());\n      }\n      if (partition < 0 || partition >= partitions) {\n        throw new IOException(\"Illegal partition for \" + key + \" (\" +\n            partition + \")\");\n      }\n      checkSpillException();\n      bufferRemaining -= METASIZE;\n      if (bufferRemaining <= 0) {\n        // start spill if the thread is not running and the soft limit has been\n        // reached\n        spillLock.lock();\n        try {\n          do {\n            if (!spillInProgress) {\n              final int kvbidx = 4 * kvindex;\n              final int kvbend = 4 * kvend;\n              // serialized, unspilled bytes always lie between kvindex and\n              // bufindex, crossing the equator. Note that any void space\n              // created by a reset must be included in \"used\" bytes\n              final int bUsed = distanceTo(kvbidx, bufindex);\n              final boolean bufsoftlimit = bUsed >= softLimit;\n              if ((kvbend + METASIZE) % kvbuffer.length !=\n                  equator - (equator % METASIZE)) {\n                // spill finished, reclaim space\n                resetSpill();\n                bufferRemaining = Math.min(\n                    distanceTo(bufindex, kvbidx) - 2 * METASIZE,\n                    softLimit - bUsed) - METASIZE;\n                continue;\n              } else if (bufsoftlimit && kvindex != kvend) {\n                // spill records, if any collected; check latter, as it may\n                // be possible for metadata alignment to hit spill pcnt\n                startSpill();\n                final int avgRec = (int)\n                  (mapOutputByteCounter.getCounter() /\n                  mapOutputRecordCounter.getCounter());\n                // leave at least half the split buffer for serialization data\n                // ensure that kvindex >= bufindex\n                final int distkvi = distanceTo(bufindex, kvbidx);\n                final int newPos = (bufindex +\n                  Math.max(2 * METASIZE - 1,\n                          Math.min(distkvi / 2,\n                                   distkvi / (METASIZE + avgRec) * METASIZE)))\n                  % kvbuffer.length;\n                setEquator(newPos);\n                bufmark = bufindex = newPos;\n                final int serBound = 4 * kvend;\n                // bytes remaining before the lock must be held and limits\n                // checked is the minimum of three arcs: the metadata space, the\n                // serialization space, and the soft limit\n                bufferRemaining = Math.min(\n                    // metadata max\n                    distanceTo(bufend, newPos),\n                    Math.min(\n                      // serialization max\n                      distanceTo(newPos, serBound),\n                      // soft limit\n                      softLimit)) - 2 * METASIZE;\n              }\n            }\n          } while (false);\n        } finally {\n          spillLock.unlock();\n        }\n      }\n\n      try {\n        // serialize key bytes into buffer\n        int keystart = bufindex;\n        keySerializer.serialize(key);\n        if (bufindex < keystart) {\n          // wrapped the key; must make contiguous\n          bb.shiftBufferedKey();\n          keystart = 0;\n        }\n        // serialize value bytes into buffer\n        final int valstart = bufindex;\n        valSerializer.serialize(value);\n        // It's possible for records to have zero length, i.e. the serializer\n        // will perform no writes. To ensure that the boundary conditions are\n        // checked and that the kvindex invariant is maintained, perform a\n        // zero-length write into the buffer. The logic monitoring this could be\n        // moved into collect, but this is cleaner and inexpensive. For now, it\n        // is acceptable.\n        bb.write(b0, 0, 0);\n\n        // the record must be marked after the preceding write, as the metadata\n        // for this record are not yet written\n        int valend = bb.markRecord();\n\n        mapOutputRecordCounter.increment(1);\n        mapOutputByteCounter.increment(\n            distanceTo(keystart, valend, bufvoid));\n\n        // write accounting info\n        kvmeta.put(kvindex + INDEX, kvindex);\n        kvmeta.put(kvindex + PARTITION, partition);\n        kvmeta.put(kvindex + KEYSTART, keystart);\n        kvmeta.put(kvindex + VALSTART, valstart);\n        // advance kvindex\n        kvindex = (kvindex - NMETA + kvmeta.capacity()) % kvmeta.capacity();\n      } catch (MapBufferTooSmallException e) {\n        LOG.info(\"Record too large for in-memory buffer: \" + e.getMessage());\n        spillSingleRecord(key, value, partition);\n        mapOutputRecordCounter.increment(1);\n        return;\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.setEquator": "    private void setEquator(int pos) {\n      equator = pos;\n      // set index prior to first entry, aligned at meta boundary\n      final int aligned = pos - (pos % METASIZE);\n      kvindex =\n        ((aligned - METASIZE + kvbuffer.length) % kvbuffer.length) / 4;\n      if (LOG.isInfoEnabled()) {\n        LOG.info(\"(EQUATOR) \" + pos + \" kvi \" + kvindex +\n            \"(\" + (kvindex * 4) + \")\");\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.distanceTo": "    int distanceTo(final int i, final int j, final int mod) {\n      return i <= j\n        ? j - i\n        : mod - i + j;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.write": "      public void write(byte b[], int off, int len)\n          throws IOException {\n        // must always verify the invariant that at least METASIZE bytes are\n        // available beyond kvindex, even when len == 0\n        bufferRemaining -= len;\n        if (bufferRemaining <= 0) {\n          // writing these bytes could exhaust available buffer space or fill\n          // the buffer to soft limit. check if spill or blocking are necessary\n          boolean blockwrite = false;\n          spillLock.lock();\n          try {\n            do {\n              checkSpillException();\n\n              final int kvbidx = 4 * kvindex;\n              final int kvbend = 4 * kvend;\n              // ser distance to key index\n              final int distkvi = distanceTo(bufindex, kvbidx);\n              // ser distance to spill end index\n              final int distkve = distanceTo(bufindex, kvbend);\n\n              // if kvindex is closer than kvend, then a spill is neither in\n              // progress nor complete and reset since the lock was held. The\n              // write should block only if there is insufficient space to\n              // complete the current write, write the metadata for this record,\n              // and write the metadata for the next record. If kvend is closer,\n              // then the write should block if there is too little space for\n              // either the metadata or the current write. Note that collect\n              // ensures its metadata requirement with a zero-length write\n              blockwrite = distkvi <= distkve\n                ? distkvi <= len + 2 * METASIZE\n                : distkve <= len || distanceTo(bufend, kvbidx) < 2 * METASIZE;\n\n              if (!spillInProgress) {\n                if (blockwrite) {\n                  if ((kvbend + METASIZE) % kvbuffer.length !=\n                      equator - (equator % METASIZE)) {\n                    // spill finished, reclaim space\n                    // need to use meta exclusively; zero-len rec & 100% spill\n                    // pcnt would fail\n                    resetSpill(); // resetSpill doesn't move bufindex, kvindex\n                    bufferRemaining = Math.min(\n                        distkvi - 2 * METASIZE,\n                        softLimit - distanceTo(kvbidx, bufindex)) - len;\n                    continue;\n                  }\n                  // we have records we can spill; only spill if blocked\n                  if (kvindex != kvend) {\n                    startSpill();\n                    // Blocked on this write, waiting for the spill just\n                    // initiated to finish. Instead of repositioning the marker\n                    // and copying the partial record, we set the record start\n                    // to be the new equator\n                    setEquator(bufmark);\n                  } else {\n                    // We have no buffered records, and this record is too large\n                    // to write into kvbuffer. We must spill it directly from\n                    // collect\n                    final int size = distanceTo(bufstart, bufindex) + len;\n                    setEquator(0);\n                    bufstart = bufend = bufindex = equator;\n                    kvstart = kvend = kvindex;\n                    bufvoid = kvbuffer.length;\n                    throw new MapBufferTooSmallException(size + \" bytes\");\n                  }\n                }\n              }\n\n              if (blockwrite) {\n                // wait for spill\n                try {\n                  while (spillInProgress) {\n                    reporter.progress();\n                    spillDone.await();\n                  }\n                } catch (InterruptedException e) {\n                    throw new IOException(\n                        \"Buffer interrupted while waiting for the writer\", e);\n                }\n              }\n            } while (blockwrite);\n          } finally {\n            spillLock.unlock();\n          }\n        }\n        // here, we know that we have sufficient space to write\n        if (bufindex + len > bufvoid) {\n          final int gaplen = bufvoid - bufindex;\n          System.arraycopy(b, off, kvbuffer, bufindex, gaplen);\n          len -= gaplen;\n          off += gaplen;\n          bufindex = 0;\n        }\n        System.arraycopy(b, off, kvbuffer, bufindex, len);\n        bufindex += len;\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.resetSpill": "    private void resetSpill() {\n      final int e = equator;\n      bufstart = bufend = e;\n      final int aligned = e - (e % METASIZE);\n      // set start/end to point to first meta record\n      kvstart = kvend =\n        ((aligned - METASIZE + kvbuffer.length) % kvbuffer.length) / 4;\n      if (LOG.isInfoEnabled()) {\n        LOG.info(\"(RESET) equator \" + e + \" kv \" + kvstart + \"(\" +\n          (kvstart * 4) + \")\" + \" kvi \" + kvindex + \"(\" + (kvindex * 4) + \")\");\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.shiftBufferedKey": "      protected void shiftBufferedKey() throws IOException {\n        // spillLock unnecessary; both kvend and kvindex are current\n        int headbytelen = bufvoid - bufmark;\n        bufvoid = bufmark;\n        final int kvbidx = 4 * kvindex;\n        final int kvbend = 4 * kvend;\n        final int avail =\n          Math.min(distanceTo(0, kvbidx), distanceTo(0, kvbend));\n        if (bufindex + headbytelen < avail) {\n          System.arraycopy(kvbuffer, 0, kvbuffer, headbytelen, bufindex);\n          System.arraycopy(kvbuffer, bufvoid, kvbuffer, 0, headbytelen);\n          bufindex += headbytelen;\n          bufferRemaining -= kvbuffer.length - bufvoid;\n        } else {\n          byte[] keytmp = new byte[bufindex];\n          System.arraycopy(kvbuffer, 0, keytmp, 0, bufindex);\n          bufindex = 0;\n          out.write(kvbuffer, bufmark, headbytelen);\n          out.write(keytmp);\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.markRecord": "      public int markRecord() {\n        bufmark = bufindex;\n        return bufindex;\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.checkSpillException": "    private void checkSpillException() throws IOException {\n      final Throwable lspillException = sortSpillException;\n      if (lspillException != null) {\n        if (lspillException instanceof Error) {\n          final String logMsg = \"Task \" + getTaskID() + \" failed : \" +\n            StringUtils.stringifyException(lspillException);\n          mapTask.reportFatalError(getTaskID(), lspillException, logMsg);\n        }\n        throw new IOException(\"Spill failed\", lspillException);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getPartition": "          public int getPartition(K key, V value, int numPartitions) {\n            return partitions - 1;\n          }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getOutputBytes": "    private long getOutputBytes(List<Statistics> stats) {\n      if (stats == null) return 0;\n      long bytesWritten = 0;\n      for (Statistics stat: stats) {\n        bytesWritten = bytesWritten + stat.getBytesWritten();\n      }\n      return bytesWritten;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.spillSingleRecord": "    private void spillSingleRecord(final K key, final V value,\n                                   int partition) throws IOException {\n      long size = kvbuffer.length + partitions * APPROX_HEADER_LENGTH;\n      FSDataOutputStream out = null;\n      try {\n        // create spill file\n        final SpillRecord spillRec = new SpillRecord(partitions);\n        final Path filename =\n            mapOutputFile.getSpillFileForWrite(numSpills, size);\n        out = rfs.create(filename);\n\n        // we don't run the combiner for a single record\n        IndexRecord rec = new IndexRecord();\n        for (int i = 0; i < partitions; ++i) {\n          IFile.Writer<K, V> writer = null;\n          try {\n            long segmentStart = out.getPos();\n            // Create a new codec, don't care!\n            writer = new IFile.Writer<K,V>(job, out, keyClass, valClass, codec,\n                                            spilledRecordsCounter);\n\n            if (i == partition) {\n              final long recordStart = out.getPos();\n              writer.append(key, value);\n              // Note that our map byte count will not be accurate with\n              // compression\n              mapOutputByteCounter.increment(out.getPos() - recordStart);\n            }\n            writer.close();\n\n            // record offsets\n            rec.startOffset = segmentStart;\n            rec.rawLength = writer.getRawLength();\n            rec.partLength = writer.getCompressedLength();\n            spillRec.putIndex(rec, i);\n\n            writer = null;\n          } catch (IOException e) {\n            if (null != writer) writer.close();\n            throw e;\n          }\n        }\n        if (totalIndexCacheMemory >= indexCacheMemoryLimit) {\n          // create spill index file\n          Path indexFilename =\n              mapOutputFile.getSpillIndexFileForWrite(numSpills, partitions\n                  * MAP_OUTPUT_INDEX_RECORD_LENGTH);\n          spillRec.writeToFile(indexFilename, job);\n        } else {\n          indexCacheList.add(spillRec);\n          totalIndexCacheMemory +=\n            spillRec.size() * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n        }\n        ++numSpills;\n      } finally {\n        if (out != null) out.close();\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.startSpill": "    private void startSpill() {\n      assert !spillInProgress;\n      kvend = (kvindex + NMETA) % kvmeta.capacity();\n      bufend = bufmark;\n      spillInProgress = true;\n      if (LOG.isInfoEnabled()) {\n        LOG.info(\"Spilling map output\");\n        LOG.info(\"bufstart = \" + bufstart + \"; bufend = \" + bufmark +\n                 \"; bufvoid = \" + bufvoid);\n        LOG.info(\"kvstart = \" + kvstart + \"(\" + (kvstart * 4) +\n                 \"); kvend = \" + kvend + \"(\" + (kvend * 4) +\n                 \"); length = \" + (distanceTo(kvend, kvstart,\n                       kvmeta.capacity()) + 1) + \"/\" + maxRec);\n      }\n      spillReady.signal();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.TaskInputOutputContext.write": "  public void write(KEYOUT key, VALUEOUT value) \n      throws IOException, InterruptedException;\n\n  /**\n   * Get the {@link OutputCommitter} for the task-attempt.",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.runNewMapper": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runNewMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, ClassNotFoundException,\n                             InterruptedException {\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, \n                                                                  getTaskID(),\n                                                                  reporter);\n    // make a mapper\n    org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE> mapper =\n      (org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>)\n        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);\n    // make the input format\n    org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE> inputFormat =\n      (org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE>)\n        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);\n    // rebuild the input split\n    org.apache.hadoop.mapreduce.InputSplit split = null;\n    split = getSplitDetails(new Path(splitIndex.getSplitLocation()),\n        splitIndex.getStartOffset());\n    LOG.info(\"Processing split: \" + split);\n\n    org.apache.hadoop.mapreduce.RecordReader<INKEY,INVALUE> input =\n      new NewTrackingRecordReader<INKEY,INVALUE>\n        (split, inputFormat, reporter, taskContext);\n    \n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.RecordWriter output = null;\n    \n    // get an output object\n    if (job.getNumReduceTasks() == 0) {\n      output = \n        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);\n    } else {\n      output = new NewOutputCollector(taskContext, job, umbilical, reporter);\n    }\n\n    org.apache.hadoop.mapreduce.MapContext<INKEY, INVALUE, OUTKEY, OUTVALUE> \n    mapContext = \n      new MapContextImpl<INKEY, INVALUE, OUTKEY, OUTVALUE>(job, getTaskID(), \n          input, output, \n          committer, \n          reporter, split);\n\n    org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context \n        mapperContext = \n          new WrappedMapper<INKEY, INVALUE, OUTKEY, OUTVALUE>().getMapContext(\n              mapContext);\n\n    input.initialize(split, mapperContext);\n    mapper.run(mapperContext);\n    mapPhase.complete();\n    setPhase(TaskStatus.Phase.SORT);\n    statusUpdate(umbilical);\n    input.close();\n    output.close(mapperContext);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.initialize": "    public void initialize(org.apache.hadoop.mapreduce.InputSplit split,\n                           org.apache.hadoop.mapreduce.TaskAttemptContext context\n                           ) throws IOException, InterruptedException {\n      long bytesInPrev = getInputBytes(fsStats);\n      real.initialize(split, context);\n      long bytesInCurr = getInputBytes(fsStats);\n      fileInputByteCounter.increment(bytesInCurr - bytesInPrev);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.close": "      public void close() { }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.run": "      public void run() {\n        spillLock.lock();\n        spillThreadRunning = true;\n        try {\n          while (true) {\n            spillDone.signal();\n            while (!spillInProgress) {\n              spillReady.await();\n            }\n            try {\n              spillLock.unlock();\n              sortAndSpill();\n            } catch (Throwable t) {\n              sortSpillException = t;\n            } finally {\n              spillLock.lock();\n              if (bufend < bufstart) {\n                bufvoid = kvbuffer.length;\n              }\n              kvstart = kvend;\n              bufstart = bufend;\n              spillInProgress = false;\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        } finally {\n          spillLock.unlock();\n          spillThreadRunning = false;\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getSplitDetails": " private <T> T getSplitDetails(Path file, long offset) \n  throws IOException {\n   FileSystem fs = file.getFileSystem(conf);\n   FSDataInputStream inFile = fs.open(file);\n   inFile.seek(offset);\n   String className = StringInterner.weakIntern(Text.readString(inFile));\n   Class<T> cls;\n   try {\n     cls = (Class<T>) conf.getClassByName(className);\n   } catch (ClassNotFoundException ce) {\n     IOException wrap = new IOException(\"Split class \" + className + \n                                         \" not found\");\n     wrap.initCause(ce);\n     throw wrap;\n   }\n   SerializationFactory factory = new SerializationFactory(conf);\n   Deserializer<T> deserializer = \n     (Deserializer<T>) factory.getDeserializer(cls);\n   deserializer.open(inFile);\n   T split = deserializer.deserialize(null);\n   long pos = inFile.getPos();\n   getCounters().findCounter(\n       TaskCounter.SPLIT_RAW_BYTES).increment(pos - offset);\n   inFile.close();\n   return split;\n }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getTaskID": "    private TaskAttemptID getTaskID() {\n      return mapTask.getTaskID();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      LOG.error(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(pae,\"Unknown exception in doAs\");\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.IntWritable.readFields": "  public void readFields(DataInput in) throws IOException {\n    value = in.readInt();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.serializer.WritableSerialization.deserialize": "    public Writable deserialize(Writable w) throws IOException {\n      Writable writable;\n      if (w == null) {\n        writable \n          = (Writable) ReflectionUtils.newInstance(writableClass, getConf());\n      } else {\n        writable = w;\n      }\n      writable.readFields(dataIn);\n      return writable;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.ReduceContext.nextKey": "  public boolean nextKey() throws IOException,InterruptedException;\n\n  /**\n   * Iterate through the values for the current key, reusing the same value \n   * object, which is stored in the context.\n   * @return the series of values associated with the current key. All of the \n   * objects returned directly and indirectly from this method are reused.\n   */\n  public Iterable<VALUEIN> getValues() throws IOException, InterruptedException;\n\n  /**\n   * {@link Iterator} to iterate over values for a given group of records.",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.sortAndSpill": "    private void sortAndSpill() throws IOException, ClassNotFoundException,\n                                       InterruptedException {\n      //approximate the length of the output file to be the length of the\n      //buffer + header lengths for the partitions\n      final long size = (bufend >= bufstart\n          ? bufend - bufstart\n          : (bufvoid - bufend) + bufstart) +\n                  partitions * APPROX_HEADER_LENGTH;\n      FSDataOutputStream out = null;\n      try {\n        // create spill file\n        final SpillRecord spillRec = new SpillRecord(partitions);\n        final Path filename =\n            mapOutputFile.getSpillFileForWrite(numSpills, size);\n        out = rfs.create(filename);\n\n        final int mstart = kvend / NMETA;\n        final int mend = 1 + // kvend is a valid record\n          (kvstart >= kvend\n          ? kvstart\n          : kvmeta.capacity() + kvstart) / NMETA;\n        sorter.sort(MapOutputBuffer.this, mstart, mend, reporter);\n        int spindex = mstart;\n        final IndexRecord rec = new IndexRecord();\n        final InMemValBytes value = new InMemValBytes();\n        for (int i = 0; i < partitions; ++i) {\n          IFile.Writer<K, V> writer = null;\n          try {\n            long segmentStart = out.getPos();\n            writer = new Writer<K, V>(job, out, keyClass, valClass, codec,\n                                      spilledRecordsCounter);\n            if (combinerRunner == null) {\n              // spill directly\n              DataInputBuffer key = new DataInputBuffer();\n              while (spindex < mend &&\n                  kvmeta.get(offsetFor(spindex % maxRec) + PARTITION) == i) {\n                final int kvoff = offsetFor(spindex % maxRec);\n                key.reset(kvbuffer, kvmeta.get(kvoff + KEYSTART),\n                          (kvmeta.get(kvoff + VALSTART) -\n                           kvmeta.get(kvoff + KEYSTART)));\n                getVBytesForOffset(kvoff, value);\n                writer.append(key, value);\n                ++spindex;\n              }\n            } else {\n              int spstart = spindex;\n              while (spindex < mend &&\n                  kvmeta.get(offsetFor(spindex % maxRec)\n                            + PARTITION) == i) {\n                ++spindex;\n              }\n              // Note: we would like to avoid the combiner if we've fewer\n              // than some threshold of records for a partition\n              if (spstart != spindex) {\n                combineCollector.setWriter(writer);\n                RawKeyValueIterator kvIter =\n                  new MRResultIterator(spstart, spindex);\n                combinerRunner.combine(kvIter, combineCollector);\n              }\n            }\n\n            // close the writer\n            writer.close();\n\n            // record offsets\n            rec.startOffset = segmentStart;\n            rec.rawLength = writer.getRawLength();\n            rec.partLength = writer.getCompressedLength();\n            spillRec.putIndex(rec, i);\n\n            writer = null;\n          } finally {\n            if (null != writer) writer.close();\n          }\n        }\n\n        if (totalIndexCacheMemory >= indexCacheMemoryLimit) {\n          // create spill index file\n          Path indexFilename =\n              mapOutputFile.getSpillIndexFileForWrite(numSpills, partitions\n                  * MAP_OUTPUT_INDEX_RECORD_LENGTH);\n          spillRec.writeToFile(indexFilename, job);\n        } else {\n          indexCacheList.add(spillRec);\n          totalIndexCacheMemory +=\n            spillRec.size() * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n        }\n        LOG.info(\"Finished spill \" + numSpills);\n        ++numSpills;\n      } finally {\n        if (out != null) out.close();\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.offsetFor": "    int offsetFor(int metapos) {\n      return kvmeta.get(metapos * NMETA + INDEX);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getPos": "    public long getPos() throws IOException { return rawIn.getPos(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.reset": "      public void reset(byte[] buffer, int start, int length) {\n        this.buffer = buffer;\n        this.start = start;\n        this.length = length;\n\n        if (start + length > bufvoid) {\n          this.buffer = new byte[this.length];\n          final int taillen = bufvoid - start;\n          System.arraycopy(buffer, start, this.buffer, 0, taillen);\n          System.arraycopy(buffer, 0, this.buffer, taillen, length-taillen);\n          this.start = 0;\n        }\n\n        super.reset(this.buffer, this.start, this.length);\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getVBytesForOffset": "    private void getVBytesForOffset(int kvoff, InMemValBytes vbytes) {\n      // get the keystart for the next serialized value to be the end\n      // of this value. If this is the last value in the buffer, use bufend\n      final int nextindex = kvoff == kvend\n        ? bufend\n        : kvmeta.get(\n            (kvoff - NMETA + kvmeta.capacity() + KEYSTART) % kvmeta.capacity());\n      // calculate the length of the value\n      int vallen = (nextindex >= kvmeta.get(kvoff + VALSTART))\n        ? nextindex - kvmeta.get(kvoff + VALSTART)\n        : (bufvoid - kvmeta.get(kvoff + VALSTART)) + nextindex;\n      vbytes.reset(kvbuffer, kvmeta.get(kvoff + VALSTART), vallen);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getNumReduceTasks": "  public int getNumReduceTasks() { return getInt(JobContext.NUM_REDUCES, 1); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.getCompressedLength": "    public long getCompressedLength() {\n      return compressedBytesWritten;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.getRawLength": "    public long getRawLength() {\n      return decompressedBytesWritten;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.SpillRecord.putIndex": "  public void putIndex(IndexRecord rec, int partition) {\n    final int pos = partition * MapTask.MAP_OUTPUT_INDEX_RECORD_LENGTH / 8;\n    entries.put(pos, rec.startOffset);\n    entries.put(pos + 1, rec.rawLength);\n    entries.put(pos + 2, rec.partLength);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.SpillRecord.writeToFile": "  public void writeToFile(Path loc, JobConf job, Checksum crc)\n      throws IOException {\n    final FileSystem rfs = FileSystem.getLocal(job).getRaw();\n    CheckedOutputStream chk = null;\n    final FSDataOutputStream out = rfs.create(loc);\n    try {\n      if (crc != null) {\n        crc.reset();\n        chk = new CheckedOutputStream(out, crc);\n        chk.write(buf.array());\n        out.writeLong(chk.getChecksum().getValue());\n      } else {\n        out.write(buf.array());\n      }\n    } finally {\n      if (chk != null) {\n        chk.close();\n      } else {\n        out.close();\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.SpillRecord.size": "  public int size() {\n    return entries.capacity() / (MapTask.MAP_OUTPUT_INDEX_RECORD_LENGTH / 8);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.append": "    public void append(DataInputBuffer key, DataInputBuffer value)\n    throws IOException {\n      int keyLength = key.getLength() - key.getPosition();\n      if (keyLength < 0) {\n        throw new IOException(\"Negative key-length not allowed: \" + keyLength + \n                              \" for \" + key);\n      }\n      \n      int valueLength = value.getLength() - value.getPosition();\n      if (valueLength < 0) {\n        throw new IOException(\"Negative value-length not allowed: \" + \n                              valueLength + \" for \" + value);\n      }\n\n      WritableUtils.writeVInt(out, keyLength);\n      WritableUtils.writeVInt(out, valueLength);\n      out.write(key.getData(), key.getPosition(), keyLength); \n      out.write(value.getData(), value.getPosition(), valueLength); \n\n      // Update bytes written\n      decompressedBytesWritten += keyLength + valueLength + \n                      WritableUtils.getVIntSize(keyLength) + \n                      WritableUtils.getVIntSize(valueLength);\n      ++numRecordsWritten;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.reset": "    public void reset(int offset) {\n      return;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.getPosition": "    public long getPosition() throws IOException {    \n      return checksumIn.getPosition(); \n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.getLength": "    public long getLength() { \n      return fileLength - checksumIn.getSize();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.mapOutputFile.getSpillFileForWrite": "  public abstract Path getSpillFileForWrite(int spillNumber, long size)\n      throws IOException;\n\n  /**\n   * Return a local map spill index file created earlier\n   *\n   * @param spillNumber the number\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillIndexFile(int spillNumber) throws IOException;\n\n  /**\n   * Create a local map spill index file name.\n   *\n   * @param spillNumber the number\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillIndexFileForWrite(int spillNumber, long size)\n      throws IOException;\n\n  /**\n   * Return a local reduce input file created earlier\n   *\n   * @param mapId a map task id\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFile(int mapId) throws IOException;\n\n  /**\n   * Create a local reduce input file name.\n   *\n   * @param mapId a map task id\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFileForWrite(\n      org.apache.hadoop.mapreduce.TaskID mapId, long size) throws IOException;\n\n  /** Removes all of the files related to a task. */\n  public abstract void removeAll() throws IOException;\n\n  @Override\n  public void setConf(Configuration conf) {\n    this.conf = conf;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.mapOutputFile.getSpillIndexFileForWrite": "  public abstract Path getSpillIndexFileForWrite(int spillNumber, long size)\n      throws IOException;\n\n  /**\n   * Return a local reduce input file created earlier\n   *\n   * @param mapId a map task id\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFile(int mapId) throws IOException;\n\n  /**\n   * Create a local reduce input file name.\n   *\n   * @param mapId a map task id\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFileForWrite(\n      org.apache.hadoop.mapreduce.TaskID mapId, long size) throws IOException;\n\n  /** Removes all of the files related to a task. */\n  public abstract void removeAll() throws IOException;\n\n  @Override\n  public void setConf(Configuration conf) {\n    this.conf = conf;\n  }"
        },
        "bug_report": {
            "Title": "Maps fail when io.sort.mb is set to high value",
            "Description": "Verified the problem exists on branch-1 with the following configuration:\n\nPseudo-dist mode: 2 maps/ 1 reduce, mapred.child.java.opts=-Xmx2048m, io.sort.mb=1280, dfs.block.size=2147483648\n\nRun teragen to generate 4 GB data\nMaps fail when you run wordcount on this configuration with the following error: \n{noformat}\njava.io.IOException: Spill failed\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1031)\n\tat org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:692)\n\tat org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)\n\tat org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:45)\n\tat org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:34)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1149)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:375)\n\tat org.apache.hadoop.io.IntWritable.readFields(IntWritable.java:38)\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:40)\n\tat org.apache.hadoop.mapreduce.ReduceContext.nextKeyValue(ReduceContext.java:116)\n\tat org.apache.hadoop.mapreduce.ReduceContext.nextKey(ReduceContext.java:92)\n\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:175)\n\tat org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1505)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1438)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1800(MapTask.java:855)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1346)\n{noformat}"
        }
    },
    {
        "filename": "MAPREDUCE-4300.json",
        "creation_time": "2012-05-31T20:44:00.000+0000",
        "stack_trace": "```\nException in thread \"ResponseProcessor for block BP-1114822160-<IP>-1322528669066:blk_-6528896407411719649_34227308\" java.lang.OutOfMemoryError: Java heap space\n\tat com.google.protobuf.CodedInputStream.(CodedInputStream.java:538)\n\tat com.google.protobuf.CodedInputStream.newInstance(CodedInputStream.java:55)\n\tat com.google.protobuf.AbstractMessageLite$Builder.mergeFrom(AbstractMessageLite.java:201)\n\tat com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:738)\n\tat org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto.parseFrom(DataTransferProtos.java:7287)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:95)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:656)\n\nException in thread \"DefaultSpeculator background processing\" java.lang.OutOfMemoryError: Java heap space\n\tat java.util.HashMap.resize(HashMap.java:462)\n\tat java.util.HashMap.addEntry(HashMap.java:755)\n\tat java.util.HashMap.put(HashMap.java:385)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getTasks(JobImpl.java:632)\n\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleASpeculation(DefaultSpeculator.java:465)\n\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleAMapSpeculation(DefaultSpeculator.java:433)\n\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.computeSpeculations(DefaultSpeculator.java:509)\n\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.access$100(DefaultSpeculator.java:56)\n\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator$1.run(DefaultSpeculator.java:176)\n\tat java.lang.Thread.run(Thread.java:619)\n\nException in thread \"Timer for 'MRAppMaster' metrics system\" java.lang.OutOfMemoryError: Java heap space\n\nException in thread \"Socket Reader #4 for port 50500\" java.lang.OutOfMemoryError: Java heap space\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getTasks": "  public Map<TaskId,Task> getTasks(TaskType taskType) {\n    Map<TaskId, Task> localTasksCopy = tasks;\n    Map<TaskId, Task> result = new HashMap<TaskId, Task>();\n    Set<TaskId> tasksOfGivenType = null;\n    readLock.lock();\n    try {\n      if (TaskType.MAP == taskType) {\n        tasksOfGivenType = mapTasks;\n      } else {\n        tasksOfGivenType = reduceTasks;\n      }\n      for (TaskId taskID : tasksOfGivenType)\n      result.put(taskID, localTasksCopy.get(taskID));\n      return result;\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleASpeculation": "  private int maybeScheduleASpeculation(TaskType type) {\n    int successes = 0;\n\n    long now = clock.getTime();\n\n    ConcurrentMap<JobId, AtomicInteger> containerNeeds\n        = type == TaskType.MAP ? mapContainerNeeds : reduceContainerNeeds;\n\n    for (ConcurrentMap.Entry<JobId, AtomicInteger> jobEntry : containerNeeds.entrySet()) {\n      // This race conditon is okay.  If we skip a speculation attempt we\n      //  should have tried because the event that lowers the number of\n      //  containers needed to zero hasn't come through, it will next time.\n      // Also, if we miss the fact that the number of containers needed was\n      //  zero but increased due to a failure it's not too bad to launch one\n      //  container prematurely.\n      if (jobEntry.getValue().get() > 0) {\n        continue;\n      }\n\n      int numberSpeculationsAlready = 0;\n      int numberRunningTasks = 0;\n\n      // loop through the tasks of the kind\n      Job job = context.getJob(jobEntry.getKey());\n\n      Map<TaskId, Task> tasks = job.getTasks(type);\n\n      int numberAllowedSpeculativeTasks\n          = (int) Math.max(MINIMUM_ALLOWED_SPECULATIVE_TASKS,\n                           PROPORTION_TOTAL_TASKS_SPECULATABLE * tasks.size());\n\n      TaskId bestTaskID = null;\n      long bestSpeculationValue = -1L;\n\n      // this loop is potentially pricey.\n      // TODO track the tasks that are potentially worth looking at\n      for (Map.Entry<TaskId, Task> taskEntry : tasks.entrySet()) {\n        long mySpeculationValue = speculationValue(taskEntry.getKey(), now);\n\n        if (mySpeculationValue == ALREADY_SPECULATING) {\n          ++numberSpeculationsAlready;\n        }\n\n        if (mySpeculationValue != NOT_RUNNING) {\n          ++numberRunningTasks;\n        }\n\n        if (mySpeculationValue > bestSpeculationValue) {\n          bestTaskID = taskEntry.getKey();\n          bestSpeculationValue = mySpeculationValue;\n        }\n      }\n      numberAllowedSpeculativeTasks\n          = (int) Math.max(numberAllowedSpeculativeTasks,\n                           PROPORTION_RUNNING_TASKS_SPECULATABLE * numberRunningTasks);\n\n      // If we found a speculation target, fire it off\n      if (bestTaskID != null\n          && numberAllowedSpeculativeTasks > numberSpeculationsAlready) {\n        addSpeculativeAttempt(bestTaskID);\n        ++successes;\n      }\n    }\n\n    return successes;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.speculationValue": "  private long speculationValue(TaskId taskID, long now) {\n    Job job = context.getJob(taskID.getJobId());\n    Task task = job.getTask(taskID);\n    Map<TaskAttemptId, TaskAttempt> attempts = task.getAttempts();\n    long acceptableRuntime = Long.MIN_VALUE;\n    long result = Long.MIN_VALUE;\n\n    if (!mayHaveSpeculated.contains(taskID)) {\n      acceptableRuntime = estimator.thresholdRuntime(taskID);\n      if (acceptableRuntime == Long.MAX_VALUE) {\n        return ON_SCHEDULE;\n      }\n    }\n\n    TaskAttemptId runningTaskAttemptID = null;\n\n    int numberRunningAttempts = 0;\n\n    for (TaskAttempt taskAttempt : attempts.values()) {\n      if (taskAttempt.getState() == TaskAttemptState.RUNNING\n          || taskAttempt.getState() == TaskAttemptState.ASSIGNED) {\n        if (++numberRunningAttempts > 1) {\n          return ALREADY_SPECULATING;\n        }\n        runningTaskAttemptID = taskAttempt.getID();\n\n        long estimatedRunTime = estimator.estimatedRuntime(runningTaskAttemptID);\n\n        long taskAttemptStartTime\n            = estimator.attemptEnrolledTime(runningTaskAttemptID);\n        if (taskAttemptStartTime > now) {\n          // This background process ran before we could process the task\n          //  attempt status change that chronicles the attempt start\n          return TOO_NEW;\n        }\n\n        long estimatedEndTime = estimatedRunTime + taskAttemptStartTime;\n\n        long estimatedReplacementEndTime\n            = now + estimator.estimatedNewAttemptRuntime(taskID);\n\n        if (estimatedEndTime < now) {\n          return PROGRESS_IS_GOOD;\n        }\n\n        if (estimatedReplacementEndTime >= estimatedEndTime) {\n          return TOO_LATE_TO_SPECULATE;\n        }\n\n        result = estimatedEndTime - estimatedReplacementEndTime;\n      }\n    }\n\n    // If we are here, there's at most one task attempt.\n    if (numberRunningAttempts == 0) {\n      return NOT_RUNNING;\n    }\n\n\n\n    if (acceptableRuntime == Long.MIN_VALUE) {\n      acceptableRuntime = estimator.thresholdRuntime(taskID);\n      if (acceptableRuntime == Long.MAX_VALUE) {\n        return ON_SCHEDULE;\n      }\n    }\n\n    return result;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.addSpeculativeAttempt": "  protected void addSpeculativeAttempt(TaskId taskID) {\n    LOG.info\n        (\"DefaultSpeculator.addSpeculativeAttempt -- we are speculating \" + taskID);\n    eventHandler.handle(new TaskEvent(taskID, TaskEventType.T_ADD_SPEC_ATTEMPT));\n    mayHaveSpeculated.add(taskID);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleAMapSpeculation": "  private int maybeScheduleAMapSpeculation() {\n    return maybeScheduleASpeculation(TaskType.MAP);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.computeSpeculations": "  private int computeSpeculations() {\n    // We'll try to issue one map and one reduce speculation per job per run\n    return maybeScheduleAMapSpeculation() + maybeScheduleAReduceSpeculation();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleAReduceSpeculation": "  private int maybeScheduleAReduceSpeculation() {\n    return maybeScheduleASpeculation(TaskType.REDUCE);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.run": "            public void run() {\n              while (!Thread.currentThread().isInterrupted()) {\n                long backgroundRunStartTime = clock.getTime();\n                try {\n                  int speculations = computeSpeculations();\n                  long mininumRecomp\n                      = speculations > 0 ? SOONEST_RETRY_AFTER_SPECULATE\n                                         : SOONEST_RETRY_AFTER_NO_SPECULATE;\n\n                  long wait = Math.max(mininumRecomp,\n                        clock.getTime() - backgroundRunStartTime);\n\n                  if (speculations > 0) {\n                    LOG.info(\"We launched \" + speculations\n                        + \" speculations.  Sleeping \" + wait + \" milliseconds.\");\n                  }\n\n                  Object pollResult\n                      = scanControl.poll(wait, TimeUnit.MILLISECONDS);\n                } catch (InterruptedException e) {\n                  LOG.error(\"Background thread returning, interrupted : \" + e);\n                  e.printStackTrace(System.out);\n                  return;\n                }\n              }\n            }"
        },
        "bug_report": {
            "Title": "OOM in AM can turn it into a zombie.",
            "Description": "It looks like 4 threads in the AM died with OOM but not the one pinging the RM.\n\nstderr for this AM\n{noformat}\nWARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.\nMay 30, 2012 4:49:55 AM com.google.inject.servlet.InternalServletModule$BackwardsCompatibleServletContextProvider get\nWARNING: You are attempting to use a deprecated API (specifically, attempting to @Inject ServletContext inside an eagerly created singleton. While we allow this for backwards compatibility, be warned that this MAY have unexpected behavior if you have more than one injector (with ServletModule) running in the same JVM. Please consult the Guice documentation at http://code.google.com/p/google-guice/wiki/Servlets for more information.\nMay 30, 2012 4:49:55 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\nINFO: Registering org.apache.hadoop.mapreduce.v2.app.webapp.JAXBContextResolver as a provider class\nMay 30, 2012 4:49:55 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\nINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\nMay 30, 2012 4:49:55 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\nINFO: Registering org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices as a root resource class\nMay 30, 2012 4:49:55 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\nINFO: Initiating Jersey application, version 'Jersey: 1.8 06/24/2011 12:17 PM'\nMay 30, 2012 4:49:55 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\nINFO: Binding org.apache.hadoop.mapreduce.v2.app.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\nMay 30, 2012 4:49:56 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\nINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\nMay 30, 2012 4:49:56 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\nINFO: Binding org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices to GuiceManagedComponentProvider with the scope \"PerRequest\"\nException in thread \"ResponseProcessor for block BP-1114822160-<IP>-1322528669066:blk_-6528896407411719649_34227308\" java.lang.OutOfMemoryError: Java heap space\n\tat com.google.protobuf.CodedInputStream.(CodedInputStream.java:538)\n\tat com.google.protobuf.CodedInputStream.newInstance(CodedInputStream.java:55)\n\tat com.google.protobuf.AbstractMessageLite$Builder.mergeFrom(AbstractMessageLite.java:201)\n\tat com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:738)\n\tat org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto.parseFrom(DataTransferProtos.java:7287)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:95)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:656)\nException in thread \"DefaultSpeculator background processing\" java.lang.OutOfMemoryError: Java heap space\n\tat java.util.HashMap.resize(HashMap.java:462)\n\tat java.util.HashMap.addEntry(HashMap.java:755)\n\tat java.util.HashMap.put(HashMap.java:385)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getTasks(JobImpl.java:632)\n\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleASpeculation(DefaultSpeculator.java:465)\n\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleAMapSpeculation(DefaultSpeculator.java:433)\n\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.computeSpeculations(DefaultSpeculator.java:509)\n\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.access$100(DefaultSpeculator.java:56)\n\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator$1.run(DefaultSpeculator.java:176)\n\tat java.lang.Thread.run(Thread.java:619)\nException in thread \"Timer for 'MRAppMaster' metrics system\" java.lang.OutOfMemoryError: Java heap space\nException in thread \"Socket Reader #4 for port 50500\" java.lang.OutOfMemoryError: Java heap space\n{noformat}"
        }
    },
    {
        "filename": "MAPREDUCE-3241.json",
        "creation_time": "2011-10-21T16:25:33.000+0000",
        "stack_trace": "```\njava.lang.IllegalArgumentException: JobBuilder.process(HistoryEvent): unknown event type\n        at org.apache.hadoop.tools.rumen.JobBuilder.process(JobBuilder.java:165)\n        at org.apache.hadoop.tools.rumen.TraceBuilder.processJobHistory(TraceBuilder.java:304)\n        at org.apache.hadoop.tools.rumen.TraceBuilder.run(TraceBuilder.java:258)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)\n        at org.apache.hadoop.tools.rumen.TraceBuilder.main(TraceBuilder.java:185)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:189)\n```",
        "source_code": {
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.process": "  public void process(Properties conf) {\n    if (finalized) {\n      throw new IllegalStateException(\n          \"JobBuilder.process(Properties conf) called after LoggedJob built\");\n    }\n\n    //TODO remove this once the deprecate APIs in LoggedJob are removed\n    result.setQueue(extract(conf, JobConfPropertyNames.QUEUE_NAMES\n        .getCandidates(), \"default\"));\n    result.setJobName(extract(conf, JobConfPropertyNames.JOB_NAMES\n        .getCandidates(), null));\n\n    maybeSetHeapMegabytes(extractMegabytes(conf,\n        JobConfPropertyNames.TASK_JAVA_OPTS_S.getCandidates()));\n    maybeSetJobMapMB(extractMegabytes(conf,\n        JobConfPropertyNames.MAP_JAVA_OPTS_S.getCandidates()));\n    maybeSetJobReduceMB(extractMegabytes(conf,\n        JobConfPropertyNames.REDUCE_JAVA_OPTS_S.getCandidates()));\n        \n    this.jobConfigurationParameters = conf;\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.maybeSetHeapMegabytes": "  private void maybeSetHeapMegabytes(Integer megabytes) {\n    if (megabytes != null) {\n      result.setHeapMegabytes(megabytes);\n    }\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.maybeSetJobReduceMB": "  private void maybeSetJobReduceMB(Integer megabytes) {\n    if (megabytes != null) {\n      result.setJobReduceMB(megabytes);\n    }\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.processTaskUpdatedEvent": "  private void processTaskUpdatedEvent(TaskUpdatedEvent event) {\n    LoggedTask task = getTask(event.getTaskId().toString());\n    if (task == null) {\n      return;\n    }\n    task.setFinishTime(event.getFinishTime());\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.processJobPriorityChangeEvent": "  private void processJobPriorityChangeEvent(JobPriorityChangeEvent event) {\n    result.setPriority(LoggedJob.JobPriority.valueOf(event.getPriority()\n        .toString()));\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.processJobFinishedEvent": "  private void processJobFinishedEvent(JobFinishedEvent event) {\n    result.setFinishTime(event.getFinishTime());\n    result.setJobID(jobID);\n    result.setOutcome(Values.SUCCESS);\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.processTaskAttemptUnsuccessfulCompletionEvent": "  private void processTaskAttemptUnsuccessfulCompletionEvent(\n      TaskAttemptUnsuccessfulCompletionEvent event) {\n    LoggedTaskAttempt attempt =\n        getOrMakeTaskAttempt(event.getTaskType(), event.getTaskId().toString(),\n            event.getTaskAttemptId().toString());\n\n    if (attempt == null) {\n      return;\n    }\n\n    attempt.setResult(getPre21Value(event.getTaskStatus()));\n    ParsedHost parsedHost = getAndRecordParsedHost(event.getHostname());\n\n    if (parsedHost != null) {\n      attempt.setLocation(parsedHost.makeLoggedLocation());\n    }\n\n    attempt.setFinishTime(event.getFinishTime());\n\n    attempt.arraySetClockSplits(event.getClockSplits());\n    attempt.arraySetCpuUsages(event.getCpuUsages());\n    attempt.arraySetVMemKbytes(event.getVMemKbytes());\n    attempt.arraySetPhysMemKbytes(event.getPhysMemKbytes());\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.processMapAttemptFinishedEvent": "  private void processMapAttemptFinishedEvent(MapAttemptFinishedEvent event) {\n    LoggedTaskAttempt attempt =\n        getOrMakeTaskAttempt(event.getTaskType(), event.getTaskId().toString(),\n            event.getAttemptId().toString());\n    if (attempt == null) {\n      return;\n    }\n    attempt.setResult(getPre21Value(event.getTaskStatus()));\n    attempt.setHostName(event.getHostname());\n    // XXX There may be redundant location info available in the event.\n    // We might consider extracting it from this event. Currently this\n    // is redundant, but making this will add future-proofing.\n    attempt.setFinishTime(event.getFinishTime());\n    attempt\n      .incorporateCounters(((MapAttemptFinished) event.getDatum()).counters);\n    attempt.arraySetClockSplits(event.getClockSplits());\n    attempt.arraySetCpuUsages(event.getCpuUsages());\n    attempt.arraySetVMemKbytes(event.getVMemKbytes());\n    attempt.arraySetPhysMemKbytes(event.getPhysMemKbytes());\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.processJobInitedEvent": "  private void processJobInitedEvent(JobInitedEvent event) {\n    result.setLaunchTime(event.getLaunchTime());\n    result.setTotalMaps(event.getTotalMaps());\n    result.setTotalReduces(event.getTotalReduces());\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.processReduceAttemptFinishedEvent": "  private void processReduceAttemptFinishedEvent(\n      ReduceAttemptFinishedEvent event) {\n    LoggedTaskAttempt attempt =\n        getOrMakeTaskAttempt(event.getTaskType(), event.getTaskId().toString(),\n            event.getAttemptId().toString());\n    if (attempt == null) {\n      return;\n    }\n    attempt.setResult(getPre21Value(event.getTaskStatus()));\n    attempt.setHostName(event.getHostname());\n    // XXX There may be redundant location info available in the event.\n    // We might consider extracting it from this event. Currently this\n    // is redundant, but making this will add future-proofing.\n    attempt.setFinishTime(event.getFinishTime());\n    attempt.setShuffleFinished(event.getShuffleFinishTime());\n    attempt.setSortFinished(event.getSortFinishTime());\n    attempt\n        .incorporateCounters(((ReduceAttemptFinished) event.getDatum()).counters);\n    attempt.arraySetClockSplits(event.getClockSplits());\n    attempt.arraySetCpuUsages(event.getCpuUsages());\n    attempt.arraySetVMemKbytes(event.getVMemKbytes());\n    attempt.arraySetPhysMemKbytes(event.getPhysMemKbytes());\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.extract": "  static String extract(Properties conf, String[] names, String defaultValue) {\n    for (String name : names) {\n      String result = conf.getProperty(name);\n\n      if (result != null) {\n        return result;\n      }\n    }\n\n    return defaultValue;\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.processJobInfoChangeEvent": "  private void processJobInfoChangeEvent(JobInfoChangeEvent event) {\n    result.setLaunchTime(event.getLaunchTime());\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.processJobStatusChangedEvent": "  private void processJobStatusChangedEvent(JobStatusChangedEvent event) {\n    result.setOutcome(Pre21JobHistoryConstants.Values\n        .valueOf(event.getStatus()));\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.processJobSubmittedEvent": "  private void processJobSubmittedEvent(JobSubmittedEvent event) {\n    result.setJobID(event.getJobId().toString());\n    result.setJobName(event.getJobName());\n    result.setUser(event.getUserName());\n    result.setSubmitTime(event.getSubmitTime());\n    // job queue name is set when conf file is processed.\n    // See JobBuilder.process(Properties) method for details.\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.processJobUnsuccessfulCompletionEvent": "  private void processJobUnsuccessfulCompletionEvent(\n      JobUnsuccessfulCompletionEvent event) {\n    result.setOutcome(Pre21JobHistoryConstants.Values\n        .valueOf(event.getStatus()));\n    result.setFinishTime(event.getFinishTime());\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.processTaskFailedEvent": "  private void processTaskFailedEvent(TaskFailedEvent event) {\n    LoggedTask task =\n        getOrMakeTask(event.getTaskType(), event.getTaskId().toString(), false);\n    if (task == null) {\n      return;\n    }\n    task.setFinishTime(event.getFinishTime());\n    task.setTaskStatus(getPre21Value(event.getTaskStatus()));\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.processTaskStartedEvent": "  private void processTaskStartedEvent(TaskStartedEvent event) {\n    LoggedTask task =\n        getOrMakeTask(event.getTaskType(), event.getTaskId().toString(), true);\n    task.setStartTime(event.getStartTime());\n    task.setPreferredLocations(preferredLocationForSplits(event\n        .getSplitLocations()));\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.processTaskAttemptStartedEvent": "  private void processTaskAttemptStartedEvent(TaskAttemptStartedEvent event) {\n    LoggedTaskAttempt attempt =\n        getOrMakeTaskAttempt(event.getTaskType(), event.getTaskId().toString(),\n            event.getTaskAttemptId().toString());\n    if (attempt == null) {\n      return;\n    }\n    attempt.setStartTime(event.getStartTime());\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.processTaskFinishedEvent": "  private void processTaskFinishedEvent(TaskFinishedEvent event) {\n    LoggedTask task =\n        getOrMakeTask(event.getTaskType(), event.getTaskId().toString(), false);\n    if (task == null) {\n      return;\n    }\n    task.setFinishTime(event.getFinishTime());\n    task.setTaskStatus(getPre21Value(event.getTaskStatus()));\n    task.incorporateCounters(((TaskFinished) event.getDatum()).counters);\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.maybeSetJobMapMB": "  private void maybeSetJobMapMB(Integer megabytes) {\n    if (megabytes != null) {\n      result.setJobMapMB(megabytes);\n    }\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.extractMegabytes": "  private Integer extractMegabytes(Properties conf, String[] names) {\n    String javaOptions = extract(conf, names, null);\n\n    if (javaOptions == null) {\n      return null;\n    }\n\n    Matcher matcher = heapPattern.matcher(javaOptions);\n\n    Integer heapMegabytes = null;\n\n    while (matcher.find()) {\n      String heapSize = matcher.group(1);\n      heapMegabytes =\n          ((int) (StringUtils.TraditionalBinaryPrefix.string2long(heapSize) / BYTES_IN_MEG));\n    }\n\n    return heapMegabytes;\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.processTaskAttemptFinishedEvent": "  private void processTaskAttemptFinishedEvent(TaskAttemptFinishedEvent event) {\n    LoggedTaskAttempt attempt =\n        getOrMakeTaskAttempt(event.getTaskType(), event.getTaskId().toString(),\n            event.getAttemptId().toString());\n    if (attempt == null) {\n      return;\n    }\n    attempt.setResult(getPre21Value(event.getTaskStatus()));\n    attempt.setLocation(getAndRecordParsedHost(event.getHostname())\n        .makeLoggedLocation());\n    attempt.setFinishTime(event.getFinishTime());\n    attempt\n        .incorporateCounters(((TaskAttemptFinished) event.getDatum()).counters);\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.TraceBuilder.processJobHistory": "  void processJobHistory(JobHistoryParser parser, JobBuilder jobBuilder)\n      throws IOException {\n    HistoryEvent e;\n    while ((e = parser.nextEvent()) != null) {\n      jobBuilder.process(e);\n      topologyBuilder.process(e);\n    }\n\n    parser.close();\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.TraceBuilder.run": "  public int run(String[] args) throws Exception {\n    MyOptions options = new MyOptions(args, getConf());\n    traceWriter = options.clazzTraceOutputter.newInstance();\n    traceWriter.init(options.traceOutput, getConf());\n    topologyWriter = new DefaultOutputter<LoggedNetworkTopology>();\n    topologyWriter.init(options.topologyOutput, getConf());\n\n    try {\n      JobBuilder jobBuilder = null;\n\n      for (Path p : options.inputs) {\n        InputDemuxer inputDemuxer = options.inputDemuxerClass.newInstance();\n\n        try {\n          inputDemuxer.bindTo(p, getConf());\n        } catch (IOException e) {\n          LOG.warn(\"Unable to bind Path \" + p + \" .  Skipping...\", e);\n\n          continue;\n        }\n\n        Pair<String, InputStream> filePair = null;\n\n        try {\n          while ((filePair = inputDemuxer.getNext()) != null) {\n            RewindableInputStream ris =\n                new RewindableInputStream(filePair.second());\n\n            JobHistoryParser parser = null;\n\n            try {\n              String jobID = JobHistoryUtils.extractJobID(filePair.first());\n              if (jobID == null) {\n                LOG.warn(\"File skipped: Invalid file name: \"\n                    + filePair.first());\n                continue;\n              }\n              if ((jobBuilder == null)\n                  || (!jobBuilder.getJobID().equals(jobID))) {\n                if (jobBuilder != null) {\n                  traceWriter.output(jobBuilder.build());\n                }\n                jobBuilder = new JobBuilder(jobID);\n              }\n\n              if (JobHistoryUtils.isJobConfXml(filePair.first())) {\n                processJobConf(JobConfigurationParser.parse(ris.rewind()),\n                               jobBuilder);\n              } else {\n                parser = JobHistoryParserFactory.getParser(ris);\n                if (parser == null) {\n                  LOG.warn(\"File skipped: Cannot find suitable parser: \"\n                      + filePair.first());\n                } else {\n                  processJobHistory(parser, jobBuilder);\n                }\n              }\n            } finally {\n              if (parser == null) {\n                ris.close();\n              } else {\n                parser.close();\n                parser = null;\n              }\n            }\n          }\n        } catch (Throwable t) {\n          if (filePair != null) {\n            LOG.warn(\"TraceBuilder got an error while processing the [possibly virtual] file \"\n                + filePair.first() + \" within Path \" + p , t);\n          }\n        } finally {\n          inputDemuxer.close();\n        }\n      }\n      if (jobBuilder != null) {\n        traceWriter.output(jobBuilder.build());\n        jobBuilder = null;\n      } else {\n        LOG.warn(\"No job found in traces: \");\n      }\n\n      topologyWriter.output(topologyBuilder.build());\n    } finally {\n      traceWriter.close();\n      topologyWriter.close();\n    }\n\n    return 0;\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.TraceBuilder.processJobConf": "  private void processJobConf(Properties properties, JobBuilder jobBuilder) {\n    jobBuilder.process(properties);\n    topologyBuilder.process(properties);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ToolRunner.run": "  public static int run(Tool tool, String[] args) \n    throws Exception{\n    return run(tool.getConf(), tool, args);\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.TraceBuilder.main": "  public static void main(String[] args) {\n    TraceBuilder builder = new TraceBuilder();\n    int result = RUN_METHOD_FAILED_EXIT_CODE;\n\n    try {\n      result = ToolRunner.run(builder, args); \n    } catch (Throwable t) {\n      t.printStackTrace(System.err);\n    } finally {\n      try {\n        builder.finish();\n      } finally {\n        if (result == 0) {\n          return;\n        }\n\n        System.exit(result);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.TraceBuilder.finish": "  void finish() {\n    IOUtils.cleanup(LOG, traceWriter, topologyWriter);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.RunJar.main": "  public static void main(String[] args) throws Throwable {\n    String usage = \"RunJar jarFile [mainClass] args...\";\n\n    if (args.length < 1) {\n      System.err.println(usage);\n      System.exit(-1);\n    }\n\n    int firstArg = 0;\n    String fileName = args[firstArg++];\n    File file = new File(fileName);\n    String mainClassName = null;\n\n    JarFile jarFile;\n    try {\n      jarFile = new JarFile(fileName);\n    } catch(IOException io) {\n      throw new IOException(\"Error opening job jar: \" + fileName)\n        .initCause(io);\n    }\n\n    Manifest manifest = jarFile.getManifest();\n    if (manifest != null) {\n      mainClassName = manifest.getMainAttributes().getValue(\"Main-Class\");\n    }\n    jarFile.close();\n\n    if (mainClassName == null) {\n      if (args.length < 2) {\n        System.err.println(usage);\n        System.exit(-1);\n      }\n      mainClassName = args[firstArg++];\n    }\n    mainClassName = mainClassName.replaceAll(\"/\", \".\");\n\n    File tmpDir = new File(new Configuration().get(\"hadoop.tmp.dir\"));\n    ensureDirectory(tmpDir);\n\n    final File workDir = File.createTempFile(\"hadoop-unjar\", \"\", tmpDir);\n    if (!workDir.delete()) {\n      System.err.println(\"Delete failed for \" + workDir);\n      System.exit(-1);\n    }\n    ensureDirectory(workDir);\n\n    Runtime.getRuntime().addShutdownHook(new Thread() {\n        public void run() {\n          FileUtil.fullyDelete(workDir);\n        }\n      });\n\n    unJar(file, workDir);\n\n    ArrayList<URL> classPath = new ArrayList<URL>();\n    classPath.add(new File(workDir+\"/\").toURI().toURL());\n    classPath.add(file.toURI().toURL());\n    classPath.add(new File(workDir, \"classes/\").toURI().toURL());\n    File[] libs = new File(workDir, \"lib\").listFiles();\n    if (libs != null) {\n      for (int i = 0; i < libs.length; i++) {\n        classPath.add(libs[i].toURI().toURL());\n      }\n    }\n    \n    ClassLoader loader =\n      new URLClassLoader(classPath.toArray(new URL[0]));\n\n    Thread.currentThread().setContextClassLoader(loader);\n    Class<?> mainClass = Class.forName(mainClassName, true, loader);\n    Method main = mainClass.getMethod(\"main\", new Class[] {\n      Array.newInstance(String.class, 0).getClass()\n    });\n    String[] newArgs = Arrays.asList(args)\n      .subList(firstArg, args.length).toArray(new String[0]);\n    try {\n      main.invoke(null, new Object[] { newArgs });\n    } catch (InvocationTargetException e) {\n      throw e.getTargetException();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.RunJar.ensureDirectory": "  private static void ensureDirectory(File dir) throws IOException {\n    if (!dir.mkdirs() && !dir.isDirectory()) {\n      throw new IOException(\"Mkdirs failed to create \" +\n                            dir.toString());\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.RunJar.unJar": "  public static void unJar(File jarFile, File toDir, Pattern unpackRegex)\n    throws IOException {\n    JarFile jar = new JarFile(jarFile);\n    try {\n      Enumeration<JarEntry> entries = jar.entries();\n      while (entries.hasMoreElements()) {\n        JarEntry entry = (JarEntry)entries.nextElement();\n        if (!entry.isDirectory() &&\n            unpackRegex.matcher(entry.getName()).matches()) {\n          InputStream in = jar.getInputStream(entry);\n          try {\n            File file = new File(toDir, entry.getName());\n            ensureDirectory(file.getParentFile());\n            OutputStream out = new FileOutputStream(file);\n            try {\n              IOUtils.copyBytes(in, out, 8192);\n            } finally {\n              out.close();\n            }\n          } finally {\n            in.close();\n          }\n        }\n      }\n    } finally {\n      jar.close();\n    }\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.topologyBuilder.process": "  public void process(Properties conf) {\n    // no code\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.topologyBuilder.processTaskAttemptFinishedEvent": "  private void processTaskAttemptFinishedEvent(TaskAttemptFinishedEvent event) {\n    recordParsedHost(event.getHostname());\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.topologyBuilder.processTaskStartedEvent": "  private void processTaskStartedEvent(TaskStartedEvent event) {\n    preferredLocationForSplits(event.getSplitLocations());\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.topologyBuilder.processTaskAttemptUnsuccessfulCompletionEvent": "  private void processTaskAttemptUnsuccessfulCompletionEvent(\n      TaskAttemptUnsuccessfulCompletionEvent event) {\n    recordParsedHost(event.getHostname());\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobHistoryParser.nextEvent": "  HistoryEvent nextEvent() throws IOException;\n  \n}",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.Pair.second": "  CdrType second() {\n    return cdr;\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobConfigurationParser.parse": "  static Properties parse(InputStream input) throws IOException {\n    Properties result = new Properties();\n\n    try {\n      DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\n\n      DocumentBuilder db = dbf.newDocumentBuilder();\n\n      Document doc = db.parse(input);\n\n      Element root = doc.getDocumentElement();\n\n      if (!\"configuration\".equals(root.getTagName())) {\n        System.out.print(\"root is not a configuration node\");\n        return null;\n      }\n\n      NodeList props = root.getChildNodes();\n\n      for (int i = 0; i < props.getLength(); ++i) {\n        Node propNode = props.item(i);\n        if (!(propNode instanceof Element))\n          continue;\n        Element prop = (Element) propNode;\n        if (!\"property\".equals(prop.getTagName())) {\n          System.out.print(\"bad conf file: element not <property>\");\n        }\n        NodeList fields = prop.getChildNodes();\n        String attr = null;\n        String value = null;\n        @SuppressWarnings(\"unused\")\n        boolean finalParameter = false;\n        for (int j = 0; j < fields.getLength(); j++) {\n          Node fieldNode = fields.item(j);\n          if (!(fieldNode instanceof Element)) {\n            continue;\n          }\n\n          Element field = (Element) fieldNode;\n          if (\"name\".equals(field.getTagName()) && field.hasChildNodes()) {\n            attr = ((Text) field.getFirstChild()).getData().trim();\n          }\n          if (\"value\".equals(field.getTagName()) && field.hasChildNodes()) {\n            value = ((Text) field.getFirstChild()).getData();\n          }\n          if (\"final\".equals(field.getTagName()) && field.hasChildNodes()) {\n            finalParameter =\n                \"true\".equals(((Text) field.getFirstChild()).getData());\n          }\n        }\n\n        if (attr != null && value != null) {\n          result.put(attr, value);\n        }\n      }\n    } catch (ParserConfigurationException e) {\n      return null;\n    } catch (SAXException e) {\n      return null;\n    }\n\n    return result;\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.InputDemuxer.bindTo": "  public void bindTo(Path path, Configuration conf) throws IOException;\n\n  /**\n   * Get the next <name, input> pair. The name should preserve the original job\n   * history file or job conf file name. The input object should be closed\n   * before calling getNext() again. The old input object would be invalid after\n   * calling getNext() again.\n   * \n   * @return the next <name, input> pair.\n   */\n  public Pair<String, InputStream> getNext() throws IOException;\n}",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.getJobID": "  public String getJobID() {\n    return jobID;\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.build": "  public LoggedJob build() {\n    // The main job here is to build CDFs and manage the conf\n    finalized = true;\n\n    // set the conf\n    result.setJobProperties(jobConfigurationParameters);\n    \n    // initialize all the per-job statistics gathering places\n    Histogram[] successfulMapAttemptTimes =\n        new Histogram[ParsedHost.numberOfDistances() + 1];\n    for (int i = 0; i < successfulMapAttemptTimes.length; ++i) {\n      successfulMapAttemptTimes[i] = new Histogram();\n    }\n\n    Histogram successfulReduceAttemptTimes = new Histogram();\n    Histogram[] failedMapAttemptTimes =\n        new Histogram[ParsedHost.numberOfDistances() + 1];\n    for (int i = 0; i < failedMapAttemptTimes.length; ++i) {\n      failedMapAttemptTimes[i] = new Histogram();\n    }\n    Histogram failedReduceAttemptTimes = new Histogram();\n\n    Histogram successfulNthMapperAttempts = new Histogram();\n    // Histogram successfulNthReducerAttempts = new Histogram();\n    // Histogram mapperLocality = new Histogram();\n\n    for (LoggedTask task : result.getMapTasks()) {\n      for (LoggedTaskAttempt attempt : task.getAttempts()) {\n        int distance = successfulMapAttemptTimes.length - 1;\n        Long runtime = null;\n\n        if (attempt.getFinishTime() > 0 && attempt.getStartTime() > 0) {\n          runtime = attempt.getFinishTime() - attempt.getStartTime();\n\n          if (attempt.getResult() == Values.SUCCESS) {\n            LoggedLocation host = attempt.getLocation();\n\n            List<LoggedLocation> locs = task.getPreferredLocations();\n\n            if (host != null && locs != null) {\n              for (LoggedLocation loc : locs) {\n                ParsedHost preferedLoc = new ParsedHost(loc);\n\n                distance =\n                    Math.min(distance, preferedLoc\n                        .distance(new ParsedHost(host)));\n              }\n\n              // mapperLocality.enter(distance);\n            }\n\n            if (attempt.getStartTime() > 0 && attempt.getFinishTime() > 0) {\n              if (runtime != null) {\n                successfulMapAttemptTimes[distance].enter(runtime);\n              }\n            }\n\n            String attemptID = attempt.getAttemptID();\n\n            if (attemptID != null) {\n              Matcher matcher = taskAttemptIDPattern.matcher(attemptID);\n\n              if (matcher.matches()) {\n                String attemptNumberString = matcher.group(1);\n\n                if (attemptNumberString != null) {\n                  int attemptNumber = Integer.parseInt(attemptNumberString);\n\n                  successfulNthMapperAttempts.enter(attemptNumber);\n                }\n              }\n            }\n          } else {\n            if (attempt.getResult() == Pre21JobHistoryConstants.Values.FAILED) {\n              if (runtime != null) {\n                failedMapAttemptTimes[distance].enter(runtime);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    for (LoggedTask task : result.getReduceTasks()) {\n      for (LoggedTaskAttempt attempt : task.getAttempts()) {\n        Long runtime = attempt.getFinishTime() - attempt.getStartTime();\n\n        if (attempt.getFinishTime() > 0 && attempt.getStartTime() > 0) {\n          runtime = attempt.getFinishTime() - attempt.getStartTime();\n        }\n        if (attempt.getResult() == Values.SUCCESS) {\n          if (runtime != null) {\n            successfulReduceAttemptTimes.enter(runtime);\n          }\n        } else if (attempt.getResult() == Pre21JobHistoryConstants.Values.FAILED) {\n          failedReduceAttemptTimes.enter(runtime);\n        }\n      }\n    }\n\n    result.setFailedMapAttemptCDFs(mapCDFArrayList(failedMapAttemptTimes));\n\n    LoggedDiscreteCDF failedReduce = new LoggedDiscreteCDF();\n    failedReduce.setCDF(failedReduceAttemptTimes, attemptTimesPercentiles, 100);\n    result.setFailedReduceAttemptCDF(failedReduce);\n\n    result\n        .setSuccessfulMapAttemptCDFs(mapCDFArrayList(successfulMapAttemptTimes));\n\n    LoggedDiscreteCDF succReduce = new LoggedDiscreteCDF();\n    succReduce.setCDF(successfulReduceAttemptTimes, attemptTimesPercentiles,\n        100);\n    result.setSuccessfulReduceAttemptCDF(succReduce);\n\n    long totalSuccessfulAttempts = 0L;\n    long maxTriesToSucceed = 0L;\n\n    for (Map.Entry<Long, Long> ent : successfulNthMapperAttempts) {\n      totalSuccessfulAttempts += ent.getValue();\n      maxTriesToSucceed = Math.max(maxTriesToSucceed, ent.getKey());\n    }\n\n    if (totalSuccessfulAttempts > 0L) {\n      double[] successAfterI = new double[(int) maxTriesToSucceed + 1];\n      for (int i = 0; i < successAfterI.length; ++i) {\n        successAfterI[i] = 0.0D;\n      }\n\n      for (Map.Entry<Long, Long> ent : successfulNthMapperAttempts) {\n        successAfterI[ent.getKey().intValue()] =\n            ((double) ent.getValue()) / totalSuccessfulAttempts;\n      }\n      result.setMapperTriesToSucceed(successAfterI);\n    } else {\n      result.setMapperTriesToSucceed(null);\n    }\n\n    return result;\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobBuilder.mapCDFArrayList": "  private ArrayList<LoggedDiscreteCDF> mapCDFArrayList(Histogram[] data) {\n    ArrayList<LoggedDiscreteCDF> result = new ArrayList<LoggedDiscreteCDF>();\n\n    for (Histogram hist : data) {\n      LoggedDiscreteCDF discCDF = new LoggedDiscreteCDF();\n      discCDF.setCDF(hist, attemptTimesPercentiles, 100);\n      result.add(discCDF);\n    }\n\n    return result;\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobHistoryUtils.isJobConfXml": "  static boolean isJobConfXml(String fileName) {\n    String jobId = extractJobIDFromConfFileName(fileName);\n    return jobId != null;\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobHistoryUtils.extractJobIDFromConfFileName": "  private static String extractJobIDFromConfFileName(String fileName) {\n    // History conf file name could be in one of the following formats\n    // (1) old pre21 job history file name format\n    // (2) new pre21 job history file name format\n    // (3) current job history file name format i.e. 0.22\n    String pre21JobID = applyParser(fileName,\n                          Pre21JobHistoryConstants.CONF_FILENAME_REGEX_V1);\n    if (pre21JobID == null) {\n      pre21JobID = applyParser(fileName,\n                     Pre21JobHistoryConstants.CONF_FILENAME_REGEX_V2);\n    }\n    if (pre21JobID != null) {\n      return pre21JobID;\n    }\n    return applyParser(fileName, JobHistory.CONF_FILENAME_REGEX);\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobHistoryUtils.extractJobID": "  static String extractJobID(String fileName) {\n    // Get jobID if fileName is a config file name.\n    String jobId = extractJobIDFromConfFileName(fileName);\n    if (jobId == null) {\n      // Get JobID if fileName is a job history file name\n      jobId = extractJobIDFromHistoryFileName(fileName);\n    }\n    return jobId;\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobHistoryUtils.extractJobIDFromHistoryFileName": "  private static String extractJobIDFromHistoryFileName(String fileName) {\n    // History file name could be in one of the following formats\n    // (1) old pre21 job history file name format\n    // (2) new pre21 job history file name format\n    // (3) current job history file name format i.e. 0.22\n\n    // Try to get the jobID assuming that the history file is from the current\n    // hadoop version\n    String jobID = extractJobIDFromCurrentHistoryFile(fileName);\n    if (jobID != null) {\n      return jobID;//history file is of current hadoop version\n    }\n\n    // History file could be of older hadoop versions\n    String pre21JobID = applyParser(fileName,\n        Pre21JobHistoryConstants.JOBHISTORY_FILENAME_REGEX_V1);\n    if (pre21JobID == null) {\n      pre21JobID = applyParser(fileName,\n          Pre21JobHistoryConstants.JOBHISTORY_FILENAME_REGEX_V2);\n    }\n    return pre21JobID;\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.RewindableInputStream.close": "  public void close() throws IOException {\n    input.close();\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.RewindableInputStream.rewind": "  public InputStream rewind() throws IOException {\n    try {\n      input.reset();\n      return this;\n    } catch (IOException e) {\n      throw new IOException(\"Unable to rewind the stream\", e);\n    }\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.InputDemuxer.getNext": "  public Pair<String, InputStream> getNext() throws IOException;\n}",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.topologyBuilder.build": "  public LoggedNetworkTopology build() {\n    return new LoggedNetworkTopology(allHosts);\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.Pair.first": "  CarType first() {\n    return car;\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobHistoryParserFactory.getParser": "  public static JobHistoryParser getParser(RewindableInputStream ris)\n      throws IOException {\n    for (VersionDetector vd : VersionDetector.values()) {\n      boolean canParse = vd.canParse(ris);\n      ris.rewind();\n      if (canParse) {\n        return vd.newInstance(ris);\n      }\n    }\n\n    throw new IOException(\"No suitable parser.\");\n  }",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobHistoryParserFactory.canParse": "    abstract boolean canParse(InputStream input) throws IOException;\n  }\n}",
            "hadoop-mapreduce-project.src.tools.org.apache.hadoop.tools.rumen.JobHistoryParserFactory.newInstance": "    abstract JobHistoryParser newInstance(InputStream input) throws IOException;\n\n    abstract boolean canParse(InputStream input) throws IOException;\n  }\n}",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs": "  public String[] getRemainingArgs() {\n    return (commandLine == null) ? new String[]{} : commandLine.getArgs();\n  }"
        },
        "bug_report": {
            "Title": "(Rumen)TraceBuilder throws IllegalArgumentException",
            "Description": "When we run the TraceBuilder, we get this exception. Output of the TraceBuilder doesn't contain the map and reduce task information.\n\n{code}\n2011-10-21 22:07:17,268 WARN  rumen.TraceBuilder (TraceBuilder.java:run(272)) - TraceBuilder got an error while processing the [possibly virtual] file job_1319214405771_0002-1319214846458-root-word+count-1319214871038-1-1-SUCCEEDED.jhist within Path hdfs://10.18.52.57:9000/user/root/null/history/done_intermediate/root/job_1319214405771_0002-1319214846458-root-word+count-1319214871038-1-1-SUCCEEDED.jhist\njava.lang.IllegalArgumentException: JobBuilder.process(HistoryEvent): unknown event type\n        at org.apache.hadoop.tools.rumen.JobBuilder.process(JobBuilder.java:165)\n        at org.apache.hadoop.tools.rumen.TraceBuilder.processJobHistory(TraceBuilder.java:304)\n        at org.apache.hadoop.tools.rumen.TraceBuilder.run(TraceBuilder.java:258)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)\n        at org.apache.hadoop.tools.rumen.TraceBuilder.main(TraceBuilder.java:185)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:189)\n\n{code}"
        }
    },
    {
        "filename": "MAPREDUCE-6002.json",
        "creation_time": "2014-07-24T02:49:25.000+0000",
        "stack_trace": "```\njava.io.IOException: Filesystem closed\n\tat org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:707)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:776)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:837)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:645)\n\tat java.io.DataInputStream.readByte(DataInputStream.java:265)\n\tat org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:308)\n\tat org.apache.hadoop.io.WritableUtils.readVIntInRange(WritableUtils.java:348)\n\tat org.apache.hadoop.io.Text.readString(Text.java:464)\n\tat org.apache.hadoop.io.Text.readString(Text.java:457)\n\tat org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:357)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:731)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.WritableUtils.readVLong": "  public static long readVLong(DataInput stream) throws IOException {\n    byte firstByte = stream.readByte();\n    int len = decodeVIntSize(firstByte);\n    if (len == 1) {\n      return firstByte;\n    }\n    long i = 0;\n    for (int idx = 0; idx < len-1; idx++) {\n      byte b = stream.readByte();\n      i = i << 8;\n      i = i | (b & 0xFF);\n    }\n    return (isNegativeVInt(firstByte) ? (i ^ -1L) : i);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.WritableUtils.decodeVIntSize": "  public static int decodeVIntSize(byte value) {\n    if (value >= -112) {\n      return 1;\n    } else if (value < -120) {\n      return -119 - value;\n    }\n    return -111 - value;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.WritableUtils.isNegativeVInt": "  public static boolean isNegativeVInt(byte value) {\n    return value < -120 || (value >= -112 && value < 0);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.WritableUtils.readVIntInRange": "  public static int readVIntInRange(DataInput stream, int lower, int upper)\n      throws IOException {\n    long n = readVLong(stream);\n    if (n < lower) {\n      if (lower == 0) {\n        throw new IOException(\"expected non-negative integer, got \" + n);\n      } else {\n        throw new IOException(\"expected integer greater than or equal to \" +\n            lower + \", got \" + n);\n      }\n    }\n    if (n > upper) {\n      throw new IOException(\"expected integer less or equal to \" + upper +\n          \", got \" + n);\n    }\n    return (int)n;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.Text.readString": "  public static String readString(DataInput in, int maxLength)\n      throws IOException {\n    int length = WritableUtils.readVIntInRange(in, 0, maxLength);\n    byte [] bytes = new byte[length];\n    in.readFully(bytes, 0, length);\n    return decode(bytes);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.Text.decode": "  private static String decode(ByteBuffer utf8, boolean replace) \n    throws CharacterCodingException {\n    CharsetDecoder decoder = DECODER_FACTORY.get();\n    if (replace) {\n      decoder.onMalformedInput(\n          java.nio.charset.CodingErrorAction.REPLACE);\n      decoder.onUnmappableCharacter(CodingErrorAction.REPLACE);\n    }\n    String str = decoder.decode(utf8).toString();\n    // set decoder back to its default value: REPORT\n    if (replace) {\n      decoder.onMalformedInput(CodingErrorAction.REPORT);\n      decoder.onUnmappableCharacter(CodingErrorAction.REPORT);\n    }\n    return str;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getSplitDetails": " private <T> T getSplitDetails(Path file, long offset) \n  throws IOException {\n   FileSystem fs = file.getFileSystem(conf);\n   FSDataInputStream inFile = fs.open(file);\n   inFile.seek(offset);\n   String className = StringInterner.weakIntern(Text.readString(inFile));\n   Class<T> cls;\n   try {\n     cls = (Class<T>) conf.getClassByName(className);\n   } catch (ClassNotFoundException ce) {\n     IOException wrap = new IOException(\"Split class \" + className + \n                                         \" not found\");\n     wrap.initCause(ce);\n     throw wrap;\n   }\n   SerializationFactory factory = new SerializationFactory(conf);\n   Deserializer<T> deserializer = \n     (Deserializer<T>) factory.getDeserializer(cls);\n   deserializer.open(inFile);\n   T split = deserializer.deserialize(null);\n   long pos = inFile.getPos();\n   getCounters().findCounter(\n       TaskCounter.SPLIT_RAW_BYTES).increment(pos - offset);\n   inFile.close();\n   return split;\n }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.close": "      public void close() { }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getPos": "    public long getPos() throws IOException { return rawIn.getPos(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.runNewMapper": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runNewMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, ClassNotFoundException,\n                             InterruptedException {\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, \n                                                                  getTaskID(),\n                                                                  reporter);\n    // make a mapper\n    org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE> mapper =\n      (org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>)\n        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);\n    // make the input format\n    org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE> inputFormat =\n      (org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE>)\n        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);\n    // rebuild the input split\n    org.apache.hadoop.mapreduce.InputSplit split = null;\n    split = getSplitDetails(new Path(splitIndex.getSplitLocation()),\n        splitIndex.getStartOffset());\n    LOG.info(\"Processing split: \" + split);\n\n    org.apache.hadoop.mapreduce.RecordReader<INKEY,INVALUE> input =\n      new NewTrackingRecordReader<INKEY,INVALUE>\n        (split, inputFormat, reporter, taskContext);\n    \n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.RecordWriter output = null;\n    \n    // get an output object\n    if (job.getNumReduceTasks() == 0) {\n      output = \n        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);\n    } else {\n      output = new NewOutputCollector(taskContext, job, umbilical, reporter);\n    }\n\n    org.apache.hadoop.mapreduce.MapContext<INKEY, INVALUE, OUTKEY, OUTVALUE> \n    mapContext = \n      new MapContextImpl<INKEY, INVALUE, OUTKEY, OUTVALUE>(job, getTaskID(), \n          input, output, \n          committer, \n          reporter, split);\n\n    org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context \n        mapperContext = \n          new WrappedMapper<INKEY, INVALUE, OUTKEY, OUTVALUE>().getMapContext(\n              mapContext);\n\n    try {\n      input.initialize(split, mapperContext);\n      mapper.run(mapperContext);\n      mapPhase.complete();\n      setPhase(TaskStatus.Phase.SORT);\n      statusUpdate(umbilical);\n      input.close();\n      input = null;\n      output.close(mapperContext);\n      output = null;\n    } finally {\n      closeQuietly(input);\n      closeQuietly(output, mapperContext);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getTaskID": "    private TaskAttemptID getTaskID() {\n      return mapTask.getTaskID();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.initialize": "    public void initialize(org.apache.hadoop.mapreduce.InputSplit split,\n                           org.apache.hadoop.mapreduce.TaskAttemptContext context\n                           ) throws IOException, InterruptedException {\n      long bytesInPrev = getInputBytes(fsStats);\n      real.initialize(split, context);\n      long bytesInCurr = getInputBytes(fsStats);\n      fileInputByteCounter.increment(bytesInCurr - bytesInPrev);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.run": "      public void run() {\n        spillLock.lock();\n        spillThreadRunning = true;\n        try {\n          while (true) {\n            spillDone.signal();\n            while (!spillInProgress) {\n              spillReady.await();\n            }\n            try {\n              spillLock.unlock();\n              sortAndSpill();\n            } catch (Throwable t) {\n              sortSpillException = t;\n            } finally {\n              spillLock.lock();\n              if (bufend < bufstart) {\n                bufvoid = kvbuffer.length;\n              }\n              kvstart = kvend;\n              bufstart = bufend;\n              spillInProgress = false;\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        } finally {\n          spillLock.unlock();\n          spillThreadRunning = false;\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.closeQuietly": "  private <INKEY, INVALUE, OUTKEY, OUTVALUE>\n  void closeQuietly(\n      org.apache.hadoop.mapreduce.RecordWriter<OUTKEY, OUTVALUE> c,\n      org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context\n          mapperContext) {\n    if (c != null) {\n      try {\n        c.close(mapperContext);\n      } catch (Exception ie) {\n        // Ignore\n        LOG.info(\"Ignoring exception during close for \" + c, ie);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.run": "              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.main": "  public static void main(String[] args) throws Throwable {\n    Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\n    LOG.debug(\"Child starting\");\n\n    final JobConf job = new JobConf(MRJobConfig.JOB_CONF_FILE);\n    // Initing with our JobConf allows us to avoid loading confs twice\n    Limits.init(job);\n    UserGroupInformation.setConfiguration(job);\n\n    String host = args[0];\n    int port = Integer.parseInt(args[1]);\n    final InetSocketAddress address =\n        NetUtils.createSocketAddrForHost(host, port);\n    final TaskAttemptID firstTaskid = TaskAttemptID.forName(args[2]);\n    int jvmIdInt = Integer.parseInt(args[3]);\n    JVMId jvmId = new JVMId(firstTaskid.getJobID(),\n        firstTaskid.getTaskType() == TaskType.MAP, jvmIdInt);\n\n    // initialize metrics\n    DefaultMetricsSystem.initialize(\n        StringUtils.camelize(firstTaskid.getTaskType().name()) +\"Task\");\n\n    // Security framework already loaded the tokens into current ugi\n    Credentials credentials =\n        UserGroupInformation.getCurrentUser().getCredentials();\n    LOG.info(\"Executing with tokens:\");\n    for (Token<?> token: credentials.getAllTokens()) {\n      LOG.info(token);\n    }\n\n    // Create TaskUmbilicalProtocol as actual task owner.\n    UserGroupInformation taskOwner =\n      UserGroupInformation.createRemoteUser(firstTaskid.getJobID().toString());\n    Token<JobTokenIdentifier> jt = TokenCache.getJobToken(credentials);\n    SecurityUtil.setTokenService(jt, address);\n    taskOwner.addToken(jt);\n    final TaskUmbilicalProtocol umbilical =\n      taskOwner.doAs(new PrivilegedExceptionAction<TaskUmbilicalProtocol>() {\n      @Override\n      public TaskUmbilicalProtocol run() throws Exception {\n        return (TaskUmbilicalProtocol)RPC.getProxy(TaskUmbilicalProtocol.class,\n            TaskUmbilicalProtocol.versionID, address, job);\n      }\n    });\n\n    // report non-pid to application master\n    JvmContext context = new JvmContext(jvmId, \"-1000\");\n    LOG.debug(\"PID: \" + System.getenv().get(\"JVM_PID\"));\n    Task task = null;\n    UserGroupInformation childUGI = null;\n    ScheduledExecutorService logSyncer = null;\n\n    try {\n      int idleLoopCount = 0;\n      JvmTask myTask = null;;\n      // poll for new task\n      for (int idle = 0; null == myTask; ++idle) {\n        long sleepTimeMilliSecs = Math.min(idle * 500, 1500);\n        LOG.info(\"Sleeping for \" + sleepTimeMilliSecs\n            + \"ms before retrying again. Got null now.\");\n        MILLISECONDS.sleep(sleepTimeMilliSecs);\n        myTask = umbilical.getTask(context);\n      }\n      if (myTask.shouldDie()) {\n        return;\n      }\n\n      task = myTask.getTask();\n      YarnChild.taskid = task.getTaskID();\n\n      // Create the job-conf and set credentials\n      configureTask(job, task, credentials, jt);\n\n      // log the system properties\n      String systemPropsToLog = MRApps.getSystemPropertiesToLog(job);\n      if (systemPropsToLog != null) {\n        LOG.info(systemPropsToLog);\n      }\n\n      // Initiate Java VM metrics\n      JvmMetrics.initSingleton(jvmId.toString(), job.getSessionId());\n      childUGI = UserGroupInformation.createRemoteUser(System\n          .getenv(ApplicationConstants.Environment.USER.toString()));\n      // Add tokens to new user so that it may execute its task correctly.\n      childUGI.addCredentials(credentials);\n\n      // set job classloader if configured before invoking the task\n      MRApps.setJobClassLoader(job);\n\n      logSyncer = TaskLog.createLogSyncer();\n\n      // Create a final reference to the task for the doAs block\n      final Task taskFinal = task;\n      childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n        @Override\n        public Object run() throws Exception {\n          // use job-specified working directory\n          FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory());\n          taskFinal.run(job, umbilical); // run the task\n          return null;\n        }\n      });\n    } catch (FSError e) {\n      LOG.fatal(\"FSError from child\", e);\n      umbilical.fsError(taskid, e.getMessage());\n    } catch (Exception exception) {\n      LOG.warn(\"Exception running child : \"\n          + StringUtils.stringifyException(exception));\n      try {\n        if (task != null) {\n          // do cleanup for the task\n          if (childUGI == null) { // no need to job into doAs block\n            task.taskCleanup(umbilical);\n          } else {\n            final Task taskFinal = task;\n            childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n              @Override\n              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }\n            });\n          }\n        }\n      } catch (Exception e) {\n        LOG.info(\"Exception cleaning up: \" + StringUtils.stringifyException(e));\n      }\n      // Report back any failures, for diagnostic purposes\n      if (taskid != null) {\n        umbilical.fatalError(taskid, StringUtils.stringifyException(exception));\n      }\n    } catch (Throwable throwable) {\n      LOG.fatal(\"Error running child : \"\n    \t        + StringUtils.stringifyException(throwable));\n      if (taskid != null) {\n        Throwable tCause = throwable.getCause();\n        String cause = tCause == null\n                                 ? throwable.getMessage()\n                                 : StringUtils.stringifyException(tCause);\n        umbilical.fatalError(taskid, cause);\n      }\n    } finally {\n      RPC.stopProxy(umbilical);\n      DefaultMetricsSystem.shutdown();\n      TaskLog.syncLogsShutdown(logSyncer);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.configureTask": "  private static void configureTask(JobConf job, Task task,\n      Credentials credentials, Token<JobTokenIdentifier> jt) throws IOException {\n    job.setCredentials(credentials);\n    \n    ApplicationAttemptId appAttemptId =\n        ConverterUtils.toContainerId(\n            System.getenv(Environment.CONTAINER_ID.name()))\n            .getApplicationAttemptId();\n    LOG.debug(\"APPLICATION_ATTEMPT_ID: \" + appAttemptId);\n    // Set it in conf, so as to be able to be used the the OutputCommitter.\n    job.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID,\n        appAttemptId.getAttemptId());\n\n    // set tcp nodelay\n    job.setBoolean(\"ipc.client.tcpnodelay\", true);\n    job.setClass(MRConfig.TASK_LOCAL_OUTPUT_CLASS,\n        YarnOutputFiles.class, MapOutputFile.class);\n    // set the jobToken and shuffle secrets into task\n    task.setJobTokenSecret(\n        JobTokenSecretManager.createSecretKey(jt.getPassword()));\n    byte[] shuffleSecret = TokenCache.getShuffleSecretKey(credentials);\n    if (shuffleSecret == null) {\n      LOG.warn(\"Shuffle secret missing from task credentials.\"\n          + \" Using job token secret as shuffle secret.\");\n      shuffleSecret = jt.getPassword();\n    }\n    task.setShuffleSecret(\n        JobTokenSecretManager.createSecretKey(shuffleSecret));\n\n    // setup the child's MRConfig.LOCAL_DIR.\n    configureLocalDirs(task, job);\n\n    // setup the child's attempt directories\n    // Do the task-type specific localization\n    task.localizeConfiguration(job);\n\n    // Set up the DistributedCache related configs\n    setupDistributedCacheConfig(job);\n\n    // Overwrite the localized task jobconf which is linked to in the current\n    // work-dir.\n    Path localTaskFile = new Path(MRJobConfig.JOB_CONF_FILE);\n    writeLocalJobFile(localTaskFile, job);\n    task.setJobFile(localTaskFile.toString());\n    task.setConf(job);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getNumReduceTasks": "  public int getNumReduceTasks() { return getInt(JobContext.NUM_REDUCES, 1); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.taskCleanup": "  void taskCleanup(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    // set phase for this task\n    setPhase(TaskStatus.Phase.CLEANUP);\n    getProgress().setStatus(\"cleanup\");\n    statusUpdate(umbilical);\n    LOG.info(\"Runnning cleanup for the task\");\n    // do the cleanup\n    committer.abortTask(taskContext);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.statusUpdate": "  public void statusUpdate(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    int retries = MAX_RETRIES;\n    while (true) {\n      try {\n        if (!umbilical.statusUpdate(getTaskID(), taskStatus).getTaskFound()) {\n          LOG.warn(\"Parent died.  Exiting \"+taskId);\n          System.exit(66);\n        }\n        taskStatus.clearStatus();\n        return;\n      } catch (InterruptedException ie) {\n        Thread.currentThread().interrupt(); // interrupt ourself\n      } catch (IOException ie) {\n        LOG.warn(\"Failure sending status update: \" + \n                  StringUtils.stringifyException(ie));\n        if (--retries == 0) {\n          throw ie;\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setPhase": "  protected synchronized void setPhase(TaskStatus.Phase phase){\n    this.taskStatus.setPhase(phase); \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setStatus": "    public void setStatus(String status) {\n      taskProgress.setStatus(normalizeStatus(status, conf));\n      // indicate that progress update needs to be sent\n      setProgressFlag();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getProgress": "    public float getProgress() {\n      return taskProgress.getProgress();\n    };",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getWorkingDirectory": "  public Path getWorkingDirectory() {\n    String name = get(JobContext.WORKING_DIR);\n    if (name != null) {\n      return new Path(name);\n    } else {\n      try {\n        Path dir = FileSystem.get(this).getWorkingDirectory();\n        set(JobContext.WORKING_DIR, dir.toString());\n        return dir;\n      } catch (IOException e) {\n        throw new RuntimeException(e);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.toString": "  public String toString() { \n    return appendTo(new StringBuilder(JVM)).toString();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.appendTo": "  protected StringBuilder appendTo(StringBuilder builder) {\n    return jobId.appendTo(builder).\n                 append(SEPARATOR).\n                 append(isMap ? 'm' : 'r').\n                 append(SEPARATOR).\n                 append(idFormat.format(id));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getSessionId": "  public String getSessionId() {\n      return get(\"session.id\", \"\");\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.createLogSyncer": "  public static ScheduledExecutorService createLogSyncer() {\n    final ScheduledExecutorService scheduler =\n      Executors.newSingleThreadScheduledExecutor(\n        new ThreadFactory() {\n          @Override\n          public Thread newThread(Runnable r) {\n            final Thread t = Executors.defaultThreadFactory().newThread(r);\n            t.setDaemon(true);\n            t.setName(\"Thread for syncLogs\");\n            return t;\n          }\n        });\n    ShutdownHookManager.get().addShutdownHook(new Runnable() {\n        @Override\n        public void run() {\n          TaskLog.syncLogsShutdown(scheduler);\n        }\n      }, 50);\n    scheduler.scheduleWithFixedDelay(\n        new Runnable() {\n          @Override\n          public void run() {\n            TaskLog.syncLogs();\n          }\n        }, 0L, 5L, TimeUnit.SECONDS);\n    return scheduler;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.newThread": "          public Thread newThread(Runnable r) {\n            final Thread t = Executors.defaultThreadFactory().newThread(r);\n            t.setDaemon(true);\n            t.setName(\"Thread for syncLogs\");\n            return t;\n          }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.syncLogsShutdown": "  public static synchronized void syncLogsShutdown(\n    ScheduledExecutorService scheduler) \n  {\n    // flush standard streams\n    //\n    System.out.flush();\n    System.err.flush();\n\n    if (scheduler != null) {\n      scheduler.shutdownNow();\n    }\n\n    // flush & close all appenders\n    LogManager.shutdown(); \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.syncLogs": "  public static synchronized void syncLogs() {\n    // flush standard streams\n    //\n    System.out.flush();\n    System.err.flush();\n\n    // flush flushable appenders\n    //\n    final Logger rootLogger = Logger.getRootLogger();\n    flushAppenders(rootLogger);\n    final Enumeration<Logger> allLoggers = rootLogger.getLoggerRepository().\n      getCurrentLoggers();\n    while (allLoggers.hasMoreElements()) {\n      final Logger l = allLoggers.nextElement();\n      flushAppenders(l);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fsError": "  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.shouldDie": "  public boolean shouldDie() {\n    return shouldDie;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.getTask": "  public Task getTask() {\n    return t;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.getTask": "  JvmTask getTask(JvmContext context) throws IOException;\n  \n  /**\n   * Report child's progress to parent. Also invoked to report still alive (used\n   * to be in ping). It reports an AMFeedback used to propagate preemption requests.\n   * \n   * @param taskId task-id of the child\n   * @param taskStatus status of the child\n   * @throws IOException\n   * @throws InterruptedException\n   * @return True if the task is known\n   */\n  AMFeedback statusUpdate(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;\n  \n  /** Report error messages back to parent.  Calls should be sparing, since all\n   *  such messages are held in the job tracker.\n   *  @param taskid the id of the task involved\n   *  @param trace the text to report\n   */\n  void reportDiagnosticInfo(TaskAttemptID taskid, String trace) throws IOException;\n  \n  /**\n   * Report the record range which is going to process next by the Task.\n   * @param taskid the id of the task involved\n   * @param range the range of record sequence nos\n   * @throws IOException\n   */\n  void reportNextRecordRange(TaskAttemptID taskid, SortedRanges.Range range) \n    throws IOException;\n\n  /** Report that the task is successfully completed.  Failure is assumed if\n   * the task process exits without calling this.\n   * @param taskid task's id\n   */\n  void done(TaskAttemptID taskid) throws IOException;\n  \n  /** \n   * Report that the task is complete, but its commit is pending.\n   * \n   * @param taskId task's id\n   * @param taskStatus status of the child\n   * @throws IOException\n   */\n  void commitPending(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;  \n\n  /**\n   * Polling to know whether the task can go-ahead with commit \n   * @param taskid\n   * @return true/false \n   * @throws IOException\n   */\n  boolean canCommit(TaskAttemptID taskid) throws IOException;\n\n  /** Report that a reduce-task couldn't shuffle map-outputs.*/\n  void shuffleError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Report that the task encounted a local filesystem error.*/\n  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.getJobID": "  public JobID getJobID() {\n    return (JobID) super.getJobID();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fatalError": "  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.forName": "  public static TaskAttemptID forName(String str\n                                      ) throws IllegalArgumentException {\n    return (TaskAttemptID) \n             org.apache.hadoop.mapreduce.TaskAttemptID.forName(str);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getTaskID": "  public TaskAttemptID getTaskID() { return taskId; }"
        },
        "bug_report": {
            "Title": "MR task should prevent report error to AM when process is shutting down",
            "Description": "With MAPREDUCE-5900, preempted MR task should not be treat as failed. \nBut it is still possible a MR task fail and report to AM when preemption take effect and the AM hasn't received completed container from RM yet. It will cause the task attempt marked failed instead of preempted.\n\nAn example is FileSystem has shutdown hook, it will close all FileSystem instance, if at the same time, the FileSystem is in-use (like reading split details from HDFS), MR task will fail and report the fatal error to MR AM. An exception will be raised:\n{code}\n2014-07-22 01:46:19,613 FATAL [IPC Server handler 10 on 56903] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1405985051088_0018_m_000025_0 - exited : java.io.IOException: Filesystem closed\n\tat org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:707)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:776)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:837)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:645)\n\tat java.io.DataInputStream.readByte(DataInputStream.java:265)\n\tat org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:308)\n\tat org.apache.hadoop.io.WritableUtils.readVIntInRange(WritableUtils.java:348)\n\tat org.apache.hadoop.io.Text.readString(Text.java:464)\n\tat org.apache.hadoop.io.Text.readString(Text.java:457)\n\tat org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:357)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:731)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\n{code}\n\nWe should prevent this, because it is possible other exceptions happen when shutting down, we shouldn't report any of such exceptions to AM."
        }
    },
    {
        "filename": "MAPREDUCE-6452.json",
        "creation_time": "2015-08-14T12:22:27.000+0000",
        "stack_trace": "```\njava.lang.Exception: java.lang.NullPointerException\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:463)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:523)\nCaused by: java.lang.NullPointerException\n        at org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:92)\n        at org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream.<init>(CryptoFSDataOutputStream.java:31)\n        at org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:112)\n        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1611)\n        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1492)\n        at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:723)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:793)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:244)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapred.LocalJobRunner.runTasks": "    private void runTasks(List<RunnableWithThrowable> runnables,\n        ExecutorService service, String taskType) throws Exception {\n      // Start populating the executor with work units.\n      // They may begin running immediately (in other threads).\n      for (Runnable r : runnables) {\n        service.submit(r);\n      }\n\n      try {\n        service.shutdown(); // Instructs queue to drain.\n\n        // Wait for tasks to finish; do not use a time-based timeout.\n        // (See http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6179024)\n        LOG.info(\"Waiting for \" + taskType + \" tasks\");\n        service.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);\n      } catch (InterruptedException ie) {\n        // Cancel all threads.\n        service.shutdownNow();\n        throw ie;\n      }\n\n      LOG.info(taskType + \" task executor complete.\");\n\n      // After waiting for the tasks to complete, if any of these\n      // have thrown an exception, rethrow it now in the main thread context.\n      for (RunnableWithThrowable r : runnables) {\n        if (r.storedException != null) {\n          throw new Exception(r.storedException);\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapred.LocalJobRunner.run": "    public void run() {\n      JobID jobId = profile.getJobID();\n      JobContext jContext = new JobContextImpl(job, jobId);\n      \n      org.apache.hadoop.mapreduce.OutputCommitter outputCommitter = null;\n      try {\n        outputCommitter = createOutputCommitter(conf.getUseNewMapper(), jobId, conf);\n      } catch (Exception e) {\n        LOG.info(\"Failed to createOutputCommitter\", e);\n        return;\n      }\n      \n      try {\n        TaskSplitMetaInfo[] taskSplitMetaInfos = \n          SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n\n        int numReduceTasks = job.getNumReduceTasks();\n        outputCommitter.setupJob(jContext);\n        status.setSetupProgress(1.0f);\n\n        Map<TaskAttemptID, MapOutputFile> mapOutputFiles =\n            Collections.synchronizedMap(new HashMap<TaskAttemptID, MapOutputFile>());\n        \n        List<RunnableWithThrowable> mapRunnables = getMapTaskRunnables(\n            taskSplitMetaInfos, jobId, mapOutputFiles);\n              \n        initCounters(mapRunnables.size(), numReduceTasks);\n        ExecutorService mapService = createMapExecutor();\n        runTasks(mapRunnables, mapService, \"map\");\n\n        try {\n          if (numReduceTasks > 0) {\n            List<RunnableWithThrowable> reduceRunnables = getReduceTaskRunnables(\n                jobId, mapOutputFiles);\n            ExecutorService reduceService = createReduceExecutor();\n            runTasks(reduceRunnables, reduceService, \"reduce\");\n          }\n        } finally {\n          for (MapOutputFile output : mapOutputFiles.values()) {\n            output.removeAll();\n          }\n        }\n        // delete the temporary directory in output directory\n        outputCommitter.commitJob(jContext);\n        status.setCleanupProgress(1.0f);\n\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.SUCCEEDED);\n        }\n\n        JobEndNotifier.localRunnerNotification(job, status);\n      } catch (Throwable t) {\n        try {\n          outputCommitter.abortJob(jContext, \n            org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n        } catch (IOException ioe) {\n          LOG.info(\"Error cleaning up job:\" + id);\n        }\n        status.setCleanupProgress(1.0f);\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.FAILED);\n        }\n        LOG.warn(id, t);\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } finally {\n        try {\n          fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n          localFs.delete(localJobFile, true);              // delete local copy\n          // Cleanup distributed cache\n          localDistributedCacheManager.close();\n        } catch (IOException e) {\n          LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapred.LocalJobRunner.initCounters": "    private synchronized void initCounters(int numMaps, int numReduces) {\n      // Initialize state trackers for all map tasks.\n      this.partialMapProgress = new float[numMaps];\n      this.mapCounters = new Counters[numMaps];\n      for (int i = 0; i < numMaps; i++) {\n        this.mapCounters[i] = new Counters();\n      }\n\n      this.partialReduceProgress = new float[numReduces];\n      this.reduceCounters = new Counters[numReduces];\n      for (int i = 0; i < numReduces; i++) {\n        this.reduceCounters[i] = new Counters();\n      }\n\n      this.numMapTasks = numMaps;\n      this.numReduceTasks = numReduces;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapred.LocalJobRunner.createOutputCommitter": "    private org.apache.hadoop.mapreduce.OutputCommitter \n    createOutputCommitter(boolean newApiCommitter, JobID jobId, Configuration conf) throws Exception {\n      org.apache.hadoop.mapreduce.OutputCommitter committer = null;\n\n      LOG.info(\"OutputCommitter set in config \"\n          + conf.get(\"mapred.output.committer.class\"));\n\n      if (newApiCommitter) {\n        org.apache.hadoop.mapreduce.TaskID taskId =\n            new org.apache.hadoop.mapreduce.TaskID(jobId, TaskType.MAP, 0);\n        org.apache.hadoop.mapreduce.TaskAttemptID taskAttemptID =\n            new org.apache.hadoop.mapreduce.TaskAttemptID(taskId, 0);\n        org.apache.hadoop.mapreduce.TaskAttemptContext taskContext = \n            new TaskAttemptContextImpl(conf, taskAttemptID);\n        OutputFormat outputFormat =\n          ReflectionUtils.newInstance(taskContext.getOutputFormatClass(), conf);\n        committer = outputFormat.getOutputCommitter(taskContext);\n      } else {\n        committer = ReflectionUtils.newInstance(conf.getClass(\n            \"mapred.output.committer.class\", FileOutputCommitter.class,\n            org.apache.hadoop.mapred.OutputCommitter.class), conf);\n      }\n      LOG.info(\"OutputCommitter is \" + committer.getClass().getName());\n      return committer;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapred.LocalJobRunner.setupChildMapredLocalDirs": "  static void setupChildMapredLocalDirs(Task t, JobConf conf) {\n    String[] localDirs = conf.getTrimmedStrings(MRConfig.LOCAL_DIR);\n    String jobId = t.getJobID().toString();\n    String taskId = t.getTaskID().toString();\n    boolean isCleanup = t.isTaskCleanupTask();\n    String user = t.getUser();\n    StringBuffer childMapredLocalDir =\n        new StringBuffer(localDirs[0] + Path.SEPARATOR\n            + getLocalTaskDir(user, jobId, taskId, isCleanup));\n    for (int i = 1; i < localDirs.length; i++) {\n      childMapredLocalDir.append(\",\" + localDirs[i] + Path.SEPARATOR\n          + getLocalTaskDir(user, jobId, taskId, isCleanup));\n    }\n    LOG.debug(MRConfig.LOCAL_DIR + \" for child : \" + childMapredLocalDir);\n    conf.set(MRConfig.LOCAL_DIR, childMapredLocalDir.toString());\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapred.LocalJobRunner.createMapExecutor": "    protected synchronized ExecutorService createMapExecutor() {\n\n      // Determine the size of the thread pool to use\n      int maxMapThreads = job.getInt(LOCAL_MAX_MAPS, 1);\n      if (maxMapThreads < 1) {\n        throw new IllegalArgumentException(\n            \"Configured \" + LOCAL_MAX_MAPS + \" must be >= 1\");\n      }\n      maxMapThreads = Math.min(maxMapThreads, this.numMapTasks);\n      maxMapThreads = Math.max(maxMapThreads, 1); // In case of no tasks.\n\n      LOG.debug(\"Starting mapper thread pool executor.\");\n      LOG.debug(\"Max local threads: \" + maxMapThreads);\n      LOG.debug(\"Map tasks to process: \" + this.numMapTasks);\n\n      // Create a new executor service to drain the work queue.\n      ThreadFactory tf = new ThreadFactoryBuilder()\n        .setNameFormat(\"LocalJobRunner Map Task Executor #%d\")\n        .build();\n      ExecutorService executor = Executors.newFixedThreadPool(maxMapThreads, tf);\n\n      return executor;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapred.LocalJobRunner.createReduceExecutor": "    protected synchronized ExecutorService createReduceExecutor() {\n\n      // Determine the size of the thread pool to use\n      int maxReduceThreads = job.getInt(LOCAL_MAX_REDUCES, 1);\n      if (maxReduceThreads < 1) {\n        throw new IllegalArgumentException(\n            \"Configured \" + LOCAL_MAX_REDUCES + \" must be >= 1\");\n      }\n      maxReduceThreads = Math.min(maxReduceThreads, this.numReduceTasks);\n      maxReduceThreads = Math.max(maxReduceThreads, 1); // In case of no tasks.\n\n      LOG.debug(\"Starting reduce thread pool executor.\");\n      LOG.debug(\"Max local threads: \" + maxReduceThreads);\n      LOG.debug(\"Reduce tasks to process: \" + this.numReduceTasks);\n\n      // Create a new executor service to drain the work queue.\n      ExecutorService executor = Executors.newFixedThreadPool(maxReduceThreads);\n\n      return executor;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapred.LocalJobRunner.getMapTaskRunnables": "    protected List<RunnableWithThrowable> getMapTaskRunnables(\n        TaskSplitMetaInfo [] taskInfo, JobID jobId,\n        Map<TaskAttemptID, MapOutputFile> mapOutputFiles) {\n\n      int numTasks = 0;\n      ArrayList<RunnableWithThrowable> list =\n          new ArrayList<RunnableWithThrowable>();\n      for (TaskSplitMetaInfo task : taskInfo) {\n        list.add(new MapTaskRunnable(task, numTasks++, jobId,\n            mapOutputFiles));\n      }\n\n      return list;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapred.LocalJobRunner.getReduceTaskRunnables": "    protected List<RunnableWithThrowable> getReduceTaskRunnables(\n        JobID jobId, Map<TaskAttemptID, MapOutputFile> mapOutputFiles) {\n\n      int taskId = 0;\n      ArrayList<RunnableWithThrowable> list =\n          new ArrayList<RunnableWithThrowable>();\n      for (int i = 0; i < this.numReduceTasks; i++) {\n        list.add(new ReduceTaskRunnable(taskId++, jobId, mapOutputFiles));\n      }\n\n      return list;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary": "  public static FSDataInputStream wrapIfNecessary(Configuration conf,\n      FSDataInputStream in) throws IOException {\n    if (isEncryptedSpillEnabled(conf)) {\n      CryptoCodec cryptoCodec = CryptoCodec.getInstance(conf);\n      int bufferSize = getBufferSize(conf);\n      // Not going to be used... but still has to be read...\n      // Since the O/P stream always writes it..\n      IOUtils.readFully(in, new byte[8], 0, 8);\n      byte[] iv = \n          new byte[cryptoCodec.getCipherSuite().getAlgorithmBlockSize()];\n      IOUtils.readFully(in, iv, 0, \n          cryptoCodec.getCipherSuite().getAlgorithmBlockSize());\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"IV read from Stream [\"\n            + Base64.encodeBase64URLSafeString(iv) + \"]\");\n      }\n      return new CryptoFSDataInputStream(in, cryptoCodec, bufferSize,\n          getEncryptionKey(), iv);\n    } else {\n      return in;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.CryptoUtils.isEncryptedSpillEnabled": "  public static boolean isEncryptedSpillEnabled(Configuration conf) {\n    return conf.getBoolean(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA,\n        MRJobConfig.DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.CryptoUtils.getBufferSize": "  private static int getBufferSize(Configuration conf) {\n    return conf.getInt(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA_BUFFER_KB,\n        MRJobConfig.DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA_BUFFER_KB) * 1024;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.CryptoUtils.getEncryptionKey": "  private static byte[] getEncryptionKey() throws IOException {\n    return TokenCache.getEncryptedSpillKey(UserGroupInformation.getCurrentUser()\n            .getCredentials());\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.CryptoUtils.createIV": "  public static byte[] createIV(Configuration conf) throws IOException {\n    CryptoCodec cryptoCodec = CryptoCodec.getInstance(conf);\n    if (isEncryptedSpillEnabled(conf)) {\n      byte[] iv = new byte[cryptoCodec.getCipherSuite().getAlgorithmBlockSize()];\n      cryptoCodec.generateSecureRandom(iv);\n      return iv;\n    } else {\n      return null;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.CryptoUtils.cryptoPadding": "  public static int cryptoPadding(Configuration conf) {\n    // Sizeof(IV) + long(start-offset)\n    return isEncryptedSpillEnabled(conf) ? CryptoCodec.getInstance(conf)\n        .getCipherSuite().getAlgorithmBlockSize() + 8 : 0;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.sortAndSpill": "    private void sortAndSpill() throws IOException, ClassNotFoundException,\n                                       InterruptedException {\n      //approximate the length of the output file to be the length of the\n      //buffer + header lengths for the partitions\n      final long size = distanceTo(bufstart, bufend, bufvoid) +\n                  partitions * APPROX_HEADER_LENGTH;\n      FSDataOutputStream out = null;\n      try {\n        // create spill file\n        final SpillRecord spillRec = new SpillRecord(partitions);\n        final Path filename =\n            mapOutputFile.getSpillFileForWrite(numSpills, size);\n        out = rfs.create(filename);\n\n        final int mstart = kvend / NMETA;\n        final int mend = 1 + // kvend is a valid record\n          (kvstart >= kvend\n          ? kvstart\n          : kvmeta.capacity() + kvstart) / NMETA;\n        sorter.sort(MapOutputBuffer.this, mstart, mend, reporter);\n        int spindex = mstart;\n        final IndexRecord rec = new IndexRecord();\n        final InMemValBytes value = new InMemValBytes();\n        for (int i = 0; i < partitions; ++i) {\n          IFile.Writer<K, V> writer = null;\n          try {\n            long segmentStart = out.getPos();\n            FSDataOutputStream partitionOut = CryptoUtils.wrapIfNecessary(job, out);\n            writer = new Writer<K, V>(job, partitionOut, keyClass, valClass, codec,\n                                      spilledRecordsCounter);\n            if (combinerRunner == null) {\n              // spill directly\n              DataInputBuffer key = new DataInputBuffer();\n              while (spindex < mend &&\n                  kvmeta.get(offsetFor(spindex % maxRec) + PARTITION) == i) {\n                final int kvoff = offsetFor(spindex % maxRec);\n                int keystart = kvmeta.get(kvoff + KEYSTART);\n                int valstart = kvmeta.get(kvoff + VALSTART);\n                key.reset(kvbuffer, keystart, valstart - keystart);\n                getVBytesForOffset(kvoff, value);\n                writer.append(key, value);\n                ++spindex;\n              }\n            } else {\n              int spstart = spindex;\n              while (spindex < mend &&\n                  kvmeta.get(offsetFor(spindex % maxRec)\n                            + PARTITION) == i) {\n                ++spindex;\n              }\n              // Note: we would like to avoid the combiner if we've fewer\n              // than some threshold of records for a partition\n              if (spstart != spindex) {\n                combineCollector.setWriter(writer);\n                RawKeyValueIterator kvIter =\n                  new MRResultIterator(spstart, spindex);\n                combinerRunner.combine(kvIter, combineCollector);\n              }\n            }\n\n            // close the writer\n            writer.close();\n\n            // record offsets\n            rec.startOffset = segmentStart;\n            rec.rawLength = writer.getRawLength() + CryptoUtils.cryptoPadding(job);\n            rec.partLength = writer.getCompressedLength() + CryptoUtils.cryptoPadding(job);\n            spillRec.putIndex(rec, i);\n\n            writer = null;\n          } finally {\n            if (null != writer) writer.close();\n          }\n        }\n\n        if (totalIndexCacheMemory >= indexCacheMemoryLimit) {\n          // create spill index file\n          Path indexFilename =\n              mapOutputFile.getSpillIndexFileForWrite(numSpills, partitions\n                  * MAP_OUTPUT_INDEX_RECORD_LENGTH);\n          spillRec.writeToFile(indexFilename, job);\n        } else {\n          indexCacheList.add(spillRec);\n          totalIndexCacheMemory +=\n            spillRec.size() * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n        }\n        LOG.info(\"Finished spill \" + numSpills);\n        ++numSpills;\n      } finally {\n        if (out != null) out.close();\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.distanceTo": "    int distanceTo(final int i, final int j, final int mod) {\n      return i <= j\n        ? j - i\n        : mod - i + j;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.offsetFor": "    int offsetFor(int metapos) {\n      return metapos * NMETA;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getPos": "    public long getPos() throws IOException { return rawIn.getPos(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.close": "      public void close() { }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.reset": "      public void reset(byte[] buffer, int start, int length) {\n        this.buffer = buffer;\n        this.start = start;\n        this.length = length;\n\n        if (start + length > bufvoid) {\n          this.buffer = new byte[this.length];\n          final int taillen = bufvoid - start;\n          System.arraycopy(buffer, start, this.buffer, 0, taillen);\n          System.arraycopy(buffer, 0, this.buffer, taillen, length-taillen);\n          this.start = 0;\n        }\n\n        super.reset(this.buffer, this.start, this.length);\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getVBytesForOffset": "    private void getVBytesForOffset(int kvoff, InMemValBytes vbytes) {\n      // get the keystart for the next serialized value to be the end\n      // of this value. If this is the last value in the buffer, use bufend\n      final int vallen = kvmeta.get(kvoff + VALLEN);\n      assert vallen >= 0;\n      vbytes.reset(kvbuffer, kvmeta.get(kvoff + VALSTART), vallen);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.flush": "    public void flush() throws IOException, ClassNotFoundException,\n           InterruptedException {\n      LOG.info(\"Starting flush of map output\");\n      if (kvbuffer == null) {\n        LOG.info(\"kvbuffer is null. Skipping flush.\");\n        return;\n      }\n      spillLock.lock();\n      try {\n        while (spillInProgress) {\n          reporter.progress();\n          spillDone.await();\n        }\n        checkSpillException();\n\n        final int kvbend = 4 * kvend;\n        if ((kvbend + METASIZE) % kvbuffer.length !=\n            equator - (equator % METASIZE)) {\n          // spill finished\n          resetSpill();\n        }\n        if (kvindex != kvend) {\n          kvend = (kvindex + NMETA) % kvmeta.capacity();\n          bufend = bufmark;\n          LOG.info(\"Spilling map output\");\n          LOG.info(\"bufstart = \" + bufstart + \"; bufend = \" + bufmark +\n                   \"; bufvoid = \" + bufvoid);\n          LOG.info(\"kvstart = \" + kvstart + \"(\" + (kvstart * 4) +\n                   \"); kvend = \" + kvend + \"(\" + (kvend * 4) +\n                   \"); length = \" + (distanceTo(kvend, kvstart,\n                         kvmeta.capacity()) + 1) + \"/\" + maxRec);\n          sortAndSpill();\n        }\n      } catch (InterruptedException e) {\n        throw new IOException(\"Interrupted while waiting for the writer\", e);\n      } finally {\n        spillLock.unlock();\n      }\n      assert !spillLock.isHeldByCurrentThread();\n      // shut down spill thread and wait for it to exit. Since the preceding\n      // ensures that it is finished with its work (and sortAndSpill did not\n      // throw), we elect to use an interrupt instead of setting a flag.\n      // Spilling simultaneously from this thread while the spill thread\n      // finishes its work might be both a useful way to extend this and also\n      // sufficient motivation for the latter approach.\n      try {\n        spillThread.interrupt();\n        spillThread.join();\n      } catch (InterruptedException e) {\n        throw new IOException(\"Spill failed\", e);\n      }\n      // release sort buffer before the merge\n      kvbuffer = null;\n      mergeParts();\n      Path outputPath = mapOutputFile.getOutputFile();\n      fileOutputByteCounter.increment(rfs.getFileStatus(outputPath).getLen());\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.mergeParts": "    private void mergeParts() throws IOException, InterruptedException, \n                                     ClassNotFoundException {\n      // get the approximate size of the final output/index files\n      long finalOutFileSize = 0;\n      long finalIndexFileSize = 0;\n      final Path[] filename = new Path[numSpills];\n      final TaskAttemptID mapId = getTaskID();\n\n      for(int i = 0; i < numSpills; i++) {\n        filename[i] = mapOutputFile.getSpillFile(i);\n        finalOutFileSize += rfs.getFileStatus(filename[i]).getLen();\n      }\n      if (numSpills == 1) { //the spill is the final output\n        sameVolRename(filename[0],\n            mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n        if (indexCacheList.size() == 0) {\n          sameVolRename(mapOutputFile.getSpillIndexFile(0),\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n        } else {\n          indexCacheList.get(0).writeToFile(\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n        }\n        sortPhase.complete();\n        return;\n      }\n\n      // read in paged indices\n      for (int i = indexCacheList.size(); i < numSpills; ++i) {\n        Path indexFileName = mapOutputFile.getSpillIndexFile(i);\n        indexCacheList.add(new SpillRecord(indexFileName, job));\n      }\n\n      //make correction in the length to include the sequence file header\n      //lengths for each partition\n      finalOutFileSize += partitions * APPROX_HEADER_LENGTH;\n      finalIndexFileSize = partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n      Path finalOutputFile =\n          mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n      Path finalIndexFile =\n          mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n\n      //The output stream for the final single output file\n      FSDataOutputStream finalOut = rfs.create(finalOutputFile, true, 4096);\n\n      if (numSpills == 0) {\n        //create dummy files\n        IndexRecord rec = new IndexRecord();\n        SpillRecord sr = new SpillRecord(partitions);\n        try {\n          for (int i = 0; i < partitions; i++) {\n            long segmentStart = finalOut.getPos();\n            FSDataOutputStream finalPartitionOut = CryptoUtils.wrapIfNecessary(job, finalOut);\n            Writer<K, V> writer =\n              new Writer<K, V>(job, finalPartitionOut, keyClass, valClass, codec, null);\n            writer.close();\n            rec.startOffset = segmentStart;\n            rec.rawLength = writer.getRawLength() + CryptoUtils.cryptoPadding(job);\n            rec.partLength = writer.getCompressedLength() + CryptoUtils.cryptoPadding(job);\n            sr.putIndex(rec, i);\n          }\n          sr.writeToFile(finalIndexFile, job);\n        } finally {\n          finalOut.close();\n        }\n        sortPhase.complete();\n        return;\n      }\n      {\n        sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n        \n        IndexRecord rec = new IndexRecord();\n        final SpillRecord spillRec = new SpillRecord(partitions);\n        for (int parts = 0; parts < partitions; parts++) {\n          //create the segments to be merged\n          List<Segment<K,V>> segmentList =\n            new ArrayList<Segment<K, V>>(numSpills);\n          for(int i = 0; i < numSpills; i++) {\n            IndexRecord indexRecord = indexCacheList.get(i).getIndex(parts);\n\n            Segment<K,V> s =\n              new Segment<K,V>(job, rfs, filename[i], indexRecord.startOffset,\n                               indexRecord.partLength, codec, true);\n            segmentList.add(i, s);\n\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"MapId=\" + mapId + \" Reducer=\" + parts +\n                  \"Spill =\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                  indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n            }\n          }\n\n          int mergeFactor = job.getInt(JobContext.IO_SORT_FACTOR, 100);\n          // sort the segments only if there are intermediate merges\n          boolean sortSegments = segmentList.size() > mergeFactor;\n          //merge\n          @SuppressWarnings(\"unchecked\")\n          RawKeyValueIterator kvIter = Merger.merge(job, rfs,\n                         keyClass, valClass, codec,\n                         segmentList, mergeFactor,\n                         new Path(mapId.toString()),\n                         job.getOutputKeyComparator(), reporter, sortSegments,\n                         null, spilledRecordsCounter, sortPhase.phase(),\n                         TaskType.MAP);\n\n          //write merged output to disk\n          long segmentStart = finalOut.getPos();\n          FSDataOutputStream finalPartitionOut = CryptoUtils.wrapIfNecessary(job, finalOut);\n          Writer<K, V> writer =\n              new Writer<K, V>(job, finalPartitionOut, keyClass, valClass, codec,\n                               spilledRecordsCounter);\n          if (combinerRunner == null || numSpills < minSpillsForCombine) {\n            Merger.writeFile(kvIter, writer, reporter, job);\n          } else {\n            combineCollector.setWriter(writer);\n            combinerRunner.combine(kvIter, combineCollector);\n          }\n\n          //close\n          writer.close();\n\n          sortPhase.startNextPhase();\n          \n          // record offsets\n          rec.startOffset = segmentStart;\n          rec.rawLength = writer.getRawLength() + CryptoUtils.cryptoPadding(job);\n          rec.partLength = writer.getCompressedLength() + CryptoUtils.cryptoPadding(job);\n          spillRec.putIndex(rec, parts);\n        }\n        spillRec.writeToFile(finalIndexFile, job);\n        finalOut.close();\n        for(int i = 0; i < numSpills; i++) {\n          rfs.delete(filename[i],true);\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.resetSpill": "    private void resetSpill() {\n      final int e = equator;\n      bufstart = bufend = e;\n      final int aligned = e - (e % METASIZE);\n      // set start/end to point to first meta record\n      // Cast one of the operands to long to avoid integer overflow\n      kvstart = kvend = (int)\n        (((long)aligned - METASIZE + kvbuffer.length) % kvbuffer.length) / 4;\n      LOG.info(\"(RESET) equator \" + e + \" kv \" + kvstart + \"(\" +\n        (kvstart * 4) + \")\" + \" kvi \" + kvindex + \"(\" + (kvindex * 4) + \")\");\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.checkSpillException": "    private void checkSpillException() throws IOException {\n      final Throwable lspillException = sortSpillException;\n      if (lspillException != null) {\n        if (lspillException instanceof Error) {\n          final String logMsg = \"Task \" + getTaskID() + \" failed : \" +\n            StringUtils.stringifyException(lspillException);\n          mapTask.reportFatalError(getTaskID(), lspillException, logMsg);\n        }\n        throw new IOException(\"Spill failed\", lspillException);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.runNewMapper": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runNewMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, ClassNotFoundException,\n                             InterruptedException {\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, \n                                                                  getTaskID(),\n                                                                  reporter);\n    // make a mapper\n    org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE> mapper =\n      (org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>)\n        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);\n    // make the input format\n    org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE> inputFormat =\n      (org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE>)\n        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);\n    // rebuild the input split\n    org.apache.hadoop.mapreduce.InputSplit split = null;\n    split = getSplitDetails(new Path(splitIndex.getSplitLocation()),\n        splitIndex.getStartOffset());\n    LOG.info(\"Processing split: \" + split);\n\n    org.apache.hadoop.mapreduce.RecordReader<INKEY,INVALUE> input =\n      new NewTrackingRecordReader<INKEY,INVALUE>\n        (split, inputFormat, reporter, taskContext);\n    \n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.RecordWriter output = null;\n    \n    // get an output object\n    if (job.getNumReduceTasks() == 0) {\n      output = \n        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);\n    } else {\n      output = new NewOutputCollector(taskContext, job, umbilical, reporter);\n    }\n\n    org.apache.hadoop.mapreduce.MapContext<INKEY, INVALUE, OUTKEY, OUTVALUE> \n    mapContext = \n      new MapContextImpl<INKEY, INVALUE, OUTKEY, OUTVALUE>(job, getTaskID(), \n          input, output, \n          committer, \n          reporter, split);\n\n    org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context \n        mapperContext = \n          new WrappedMapper<INKEY, INVALUE, OUTKEY, OUTVALUE>().getMapContext(\n              mapContext);\n\n    try {\n      input.initialize(split, mapperContext);\n      mapper.run(mapperContext);\n      mapPhase.complete();\n      setPhase(TaskStatus.Phase.SORT);\n      statusUpdate(umbilical);\n      input.close();\n      input = null;\n      output.close(mapperContext);\n      output = null;\n    } finally {\n      closeQuietly(input);\n      closeQuietly(output, mapperContext);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getSplitDetails": " private <T> T getSplitDetails(Path file, long offset) \n  throws IOException {\n   FileSystem fs = file.getFileSystem(conf);\n   FSDataInputStream inFile = fs.open(file);\n   inFile.seek(offset);\n   String className = StringInterner.weakIntern(Text.readString(inFile));\n   Class<T> cls;\n   try {\n     cls = (Class<T>) conf.getClassByName(className);\n   } catch (ClassNotFoundException ce) {\n     IOException wrap = new IOException(\"Split class \" + className + \n                                         \" not found\");\n     wrap.initCause(ce);\n     throw wrap;\n   }\n   SerializationFactory factory = new SerializationFactory(conf);\n   Deserializer<T> deserializer = \n     (Deserializer<T>) factory.getDeserializer(cls);\n   deserializer.open(inFile);\n   T split = deserializer.deserialize(null);\n   long pos = inFile.getPos();\n   getCounters().findCounter(\n       TaskCounter.SPLIT_RAW_BYTES).increment(pos - offset);\n   inFile.close();\n   return split;\n }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getTaskID": "    private TaskAttemptID getTaskID() {\n      return mapTask.getTaskID();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.initialize": "    public void initialize(org.apache.hadoop.mapreduce.InputSplit split,\n                           org.apache.hadoop.mapreduce.TaskAttemptContext context\n                           ) throws IOException, InterruptedException {\n      long bytesInPrev = getInputBytes(fsStats);\n      real.initialize(split, context);\n      long bytesInCurr = getInputBytes(fsStats);\n      fileInputByteCounter.increment(bytesInCurr - bytesInPrev);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.run": "      public void run() {\n        spillLock.lock();\n        spillThreadRunning = true;\n        try {\n          while (true) {\n            spillDone.signal();\n            while (!spillInProgress) {\n              spillReady.await();\n            }\n            try {\n              spillLock.unlock();\n              sortAndSpill();\n            } catch (Throwable t) {\n              sortSpillException = t;\n            } finally {\n              spillLock.lock();\n              if (bufend < bufstart) {\n                bufvoid = kvbuffer.length;\n              }\n              kvstart = kvend;\n              bufstart = bufend;\n              spillInProgress = false;\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        } finally {\n          spillLock.unlock();\n          spillThreadRunning = false;\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.closeQuietly": "  private <INKEY, INVALUE, OUTKEY, OUTVALUE>\n  void closeQuietly(\n      org.apache.hadoop.mapreduce.RecordWriter<OUTKEY, OUTVALUE> c,\n      org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context\n          mapperContext) {\n    if (c != null) {\n      try {\n        c.close(mapperContext);\n      } catch (Exception ie) {\n        // Ignore\n        LOG.info(\"Ignoring exception during close for \" + c, ie);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.localizeConfiguration": "  public void localizeConfiguration(JobConf conf)\n      throws IOException {\n    super.localizeConfiguration(conf);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapred.localDistributedCacheManager.close": "  public void close() throws IOException {\n    for (File symlink : symlinksCreated) {\n      if (!symlink.delete()) {\n        LOG.warn(\"Failed to delete symlink created by the local job runner: \" +\n            symlink);\n      }\n    }\n    FileContext localFSFileContext = FileContext.getLocalFSFileContext();\n    for (String archive : localArchives) {\n      localFSFileContext.delete(new Path(archive), true);\n    }\n    for (String file : localFiles) {\n      localFSFileContext.delete(new Path(file), true);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.localizeConfiguration": "  public void localizeConfiguration(JobConf conf) throws IOException {\n    super.localizeConfiguration(conf);\n    conf.setNumMapTasks(numMaps);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getUseNewMapper": "  public boolean getUseNewMapper() {\n    return getBoolean(\"mapred.mapper.new-api\", false);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapOutputFile.setConf": "  public void setConf(Configuration conf) {\n    this.conf = conf;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobEndNotifier.localRunnerNotification": "  public static void localRunnerNotification(JobConf conf, JobStatus status) {\n    JobEndStatusInfo notification = createNotification(conf, status);\n    if (notification != null) {\n      do {\n        try {\n          int code = httpNotification(notification.getUri(),\n              notification.getTimeout());\n          if (code != 200) {\n            throw new IOException(\"Invalid response status code: \" + code);\n          }\n          else {\n            break;\n          }\n        }\n        catch (IOException ioex) {\n          LOG.error(\"Notification error [\" + notification.getUri() + \"]\", ioex);\n        }\n        catch (Exception ex) {\n          LOG.error(\"Notification error [\" + notification.getUri() + \"]\", ex);\n        }\n        try {\n          Thread.sleep(notification.getRetryInterval());\n        }\n        catch (InterruptedException iex) {\n          LOG.error(\"Notification retry error [\" + notification + \"]\", iex);\n        }\n      } while (notification.configureForRetry());\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobEndNotifier.getRetryInterval": "    public long getRetryInterval() {\n      return retryInterval;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobEndNotifier.configureForRetry": "    public boolean configureForRetry() {\n      boolean retry = false;\n      if (getRetryAttempts() > 0) {\n        retry = true;\n        delayTime = System.currentTimeMillis() + retryInterval;\n      }\n      retryAttempts--;\n      return retry;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobEndNotifier.getTimeout": "    public int getTimeout() {\n      return timeout;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobEndNotifier.httpNotification": "  private static int httpNotification(String uri, int timeout)\n      throws IOException, URISyntaxException {\n    DefaultHttpClient client = new DefaultHttpClient();\n    client.getParams()\n        .setIntParameter(CoreConnectionPNames.SO_TIMEOUT, timeout)\n        .setLongParameter(ClientPNames.CONN_MANAGER_TIMEOUT, (long) timeout);\n    HttpGet httpGet = new HttpGet(new URI(uri));\n    httpGet.setHeader(\"Accept\", \"*/*\");\n    return client.execute(httpGet).getStatusLine().getStatusCode();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobEndNotifier.getUri": "    public String getUri() {\n      return uri;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobEndNotifier.createNotification": "  private static JobEndStatusInfo createNotification(JobConf conf,\n                                                     JobStatus status) {\n    JobEndStatusInfo notification = null;\n    String uri = conf.getJobEndNotificationURI();\n    if (uri != null) {\n      int retryAttempts = conf.getInt(JobContext.MR_JOB_END_RETRY_ATTEMPTS, 0);\n      long retryInterval = conf.getInt(JobContext.MR_JOB_END_RETRY_INTERVAL, 30000);\n      int timeout = conf.getInt(JobContext.MR_JOB_END_NOTIFICATION_TIMEOUT,\n          JobContext.DEFAULT_MR_JOB_END_NOTIFICATION_TIMEOUT);\n      if (uri.contains(\"$jobId\")) {\n        uri = uri.replace(\"$jobId\", status.getJobID().toString());\n      }\n      if (uri.contains(\"$jobStatus\")) {\n        String statusStr =\n          (status.getRunState() == JobStatus.SUCCEEDED) ? \"SUCCEEDED\" : \n            (status.getRunState() == JobStatus.FAILED) ? \"FAILED\" : \"KILLED\";\n        uri = uri.replace(\"$jobStatus\", statusStr);\n      }\n      notification = new JobEndStatusInfo(\n          uri, retryAttempts, retryInterval, timeout);\n    }\n    return notification;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.setLocalMapFiles": "  public void setLocalMapFiles(Map<TaskAttemptID, MapOutputFile> mapFiles) {\n    this.localMapFiles = mapFiles;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.getCompressedLength": "    public long getCompressedLength() {\n      return compressedBytesWritten;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.getRawLength": "    public long getRawLength() {\n      return decompressedBytesWritten;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.SpillRecord.putIndex": "  public void putIndex(IndexRecord rec, int partition) {\n    final int pos = partition * MapTask.MAP_OUTPUT_INDEX_RECORD_LENGTH / 8;\n    entries.put(pos, rec.startOffset);\n    entries.put(pos + 1, rec.rawLength);\n    entries.put(pos + 2, rec.partLength);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.SpillRecord.writeToFile": "  public void writeToFile(Path loc, JobConf job, Checksum crc)\n      throws IOException {\n    final FileSystem rfs = FileSystem.getLocal(job).getRaw();\n    CheckedOutputStream chk = null;\n    final FSDataOutputStream out = rfs.create(loc);\n    try {\n      if (crc != null) {\n        crc.reset();\n        chk = new CheckedOutputStream(out, crc);\n        chk.write(buf.array());\n        out.writeLong(chk.getChecksum().getValue());\n      } else {\n        out.write(buf.array());\n      }\n    } finally {\n      if (chk != null) {\n        chk.close();\n      } else {\n        out.close();\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.SpillRecord.size": "  public int size() {\n    return entries.capacity() / (MapTask.MAP_OUTPUT_INDEX_RECORD_LENGTH / 8);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.append": "    public void append(DataInputBuffer key, DataInputBuffer value)\n    throws IOException {\n      int keyLength = key.getLength() - key.getPosition();\n      if (keyLength < 0) {\n        throw new IOException(\"Negative key-length not allowed: \" + keyLength + \n                              \" for \" + key);\n      }\n      \n      int valueLength = value.getLength() - value.getPosition();\n      if (valueLength < 0) {\n        throw new IOException(\"Negative value-length not allowed: \" + \n                              valueLength + \" for \" + value);\n      }\n\n      WritableUtils.writeVInt(out, keyLength);\n      WritableUtils.writeVInt(out, valueLength);\n      out.write(key.getData(), key.getPosition(), keyLength); \n      out.write(value.getData(), value.getPosition(), valueLength); \n\n      // Update bytes written\n      decompressedBytesWritten += keyLength + valueLength + \n                      WritableUtils.getVIntSize(keyLength) + \n                      WritableUtils.getVIntSize(valueLength);\n      ++numRecordsWritten;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.reset": "    public void reset(int offset) {\n      return;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.getPosition": "    public long getPosition() throws IOException {    \n      return checksumIn.getPosition(); \n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.getLength": "    public long getLength() { \n      return fileLength - checksumIn.getSize();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.mapOutputFile.getSpillFileForWrite": "  public abstract Path getSpillFileForWrite(int spillNumber, long size)\n      throws IOException;\n\n  /**\n   * Return a local map spill index file created earlier\n   *\n   * @param spillNumber the number\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillIndexFile(int spillNumber) throws IOException;\n\n  /**\n   * Create a local map spill index file name.\n   *\n   * @param spillNumber the number\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillIndexFileForWrite(int spillNumber, long size)\n      throws IOException;\n\n  /**\n   * Return a local reduce input file created earlier\n   *\n   * @param mapId a map task id\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFile(int mapId) throws IOException;\n\n  /**\n   * Create a local reduce input file name.\n   *\n   * @param mapId a map task id\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFileForWrite(\n      org.apache.hadoop.mapreduce.TaskID mapId, long size) throws IOException;\n\n  /** Removes all of the files related to a task. */\n  public abstract void removeAll() throws IOException;\n\n  @Override\n  public void setConf(Configuration conf) {\n    this.conf = conf;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.mapOutputFile.getSpillIndexFileForWrite": "  public abstract Path getSpillIndexFileForWrite(int spillNumber, long size)\n      throws IOException;\n\n  /**\n   * Return a local reduce input file created earlier\n   *\n   * @param mapId a map task id\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFile(int mapId) throws IOException;\n\n  /**\n   * Create a local reduce input file name.\n   *\n   * @param mapId a map task id\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFileForWrite(\n      org.apache.hadoop.mapreduce.TaskID mapId, long size) throws IOException;\n\n  /** Removes all of the files related to a task. */\n  public abstract void removeAll() throws IOException;\n\n  @Override\n  public void setConf(Configuration conf) {\n    this.conf = conf;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.mapOutputFile.getOutputFile": "  public abstract Path getOutputFile() throws IOException;\n\n  /**\n   * Create a local map output file name.\n   *\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getOutputFileForWrite(long size) throws IOException;\n\n  /**\n   * Create a local map output file name on the same volume.\n   */\n  public abstract Path getOutputFileForWriteInVolume(Path existing);\n\n  /**\n   * Return the path to a local map output index file created earlier\n   *\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getOutputIndexFile() throws IOException;\n\n  /**\n   * Create a local map output index file name.\n   *\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getOutputIndexFileForWrite(long size) throws IOException;\n\n  /**\n   * Create a local map output index file name on the same volume.\n   */\n  public abstract Path getOutputIndexFileForWriteInVolume(Path existing);\n\n  /**\n   * Return a local map spill file created earlier.\n   *\n   * @param spillNumber the number\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillFile(int spillNumber) throws IOException;\n\n  /**\n   * Create a local map spill file name.\n   *\n   * @param spillNumber the number\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillFileForWrite(int spillNumber, long size)\n      throws IOException;\n\n  /**\n   * Return a local map spill index file created earlier\n   *\n   * @param spillNumber the number\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillIndexFile(int spillNumber) throws IOException;\n\n  /**\n   * Create a local map spill index file name.\n   *\n   * @param spillNumber the number\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillIndexFileForWrite(int spillNumber, long size)\n      throws IOException;\n\n  /**\n   * Return a local reduce input file created earlier\n   *\n   * @param mapId a map task id\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFile(int mapId) throws IOException;\n\n  /**\n   * Create a local reduce input file name.\n   *\n   * @param mapId a map task id\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFileForWrite(\n      org.apache.hadoop.mapreduce.TaskID mapId, long size) throws IOException;\n\n  /** Removes all of the files related to a task. */\n  public abstract void removeAll() throws IOException;\n\n  @Override\n  public void setConf(Configuration conf) {\n    this.conf = conf;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getNumReduceTasks": "  public int getNumReduceTasks() { return getInt(JobContext.NUM_REDUCES, 1); }"
        },
        "bug_report": {
            "Title": "NPE when intermediate encrypt enabled for LocalRunner",
            "Description": "Enable the below properties try running mapreduce job\n\nmapreduce.framework.name=local\nmapreduce.job.encrypted-intermediate-data=true\n\n{code}\n2015-08-14 16:27:25,248 WARN  [Thread-21] mapred.LocalJobRunner (LocalJobRunner.java:run(561)) - job_local473843898_0001\njava.lang.Exception: java.lang.NullPointerException\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:463)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:523)\nCaused by: java.lang.NullPointerException\n        at org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:92)\n        at org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream.<init>(CryptoFSDataOutputStream.java:31)\n        at org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:112)\n        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1611)\n        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1492)\n        at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:723)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:793)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:244)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\n{code}\n\nJobs are failing always"
        }
    },
    {
        "filename": "MAPREDUCE-6649.json",
        "creation_time": "2016-03-07T16:37:13.000+0000",
        "stack_trace": "```\nStack trace: ExitCodeException exitCode=1: \n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:927)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:838)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1117)\n\tat org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:227)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:319)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:88)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.runCommand": "  private void runCommand() throws IOException { \n    ProcessBuilder builder = new ProcessBuilder(getExecString());\n    Timer timeOutTimer = null;\n    ShellTimeoutTimerTask timeoutTimerTask = null;\n    timedOut.set(false);\n    completed.set(false);\n\n    if (environment != null) {\n      builder.environment().putAll(this.environment);\n    }\n    if (dir != null) {\n      builder.directory(this.dir);\n    }\n\n    builder.redirectErrorStream(redirectErrorStream);\n    \n    if (Shell.WINDOWS) {\n      synchronized (WindowsProcessLaunchLock) {\n        // To workaround the race condition issue with child processes\n        // inheriting unintended handles during process launch that can\n        // lead to hangs on reading output and error streams, we\n        // serialize process creation. More info available at:\n        // http://support.microsoft.com/kb/315939\n        process = builder.start();\n      }\n    } else {\n      process = builder.start();\n    }\n\n    if (timeOutInterval > 0) {\n      timeOutTimer = new Timer(\"Shell command timeout\");\n      timeoutTimerTask = new ShellTimeoutTimerTask(\n          this);\n      //One time scheduling.\n      timeOutTimer.schedule(timeoutTimerTask, timeOutInterval);\n    }\n    final BufferedReader errReader = \n            new BufferedReader(new InputStreamReader(\n                process.getErrorStream(), Charset.defaultCharset()));\n    BufferedReader inReader =\n            new BufferedReader(new InputStreamReader(\n                process.getInputStream(), Charset.defaultCharset()));\n    final StringBuffer errMsg = new StringBuffer();\n\n    // read error and input streams as this would free up the buffers\n    // free the error stream buffer\n    Thread errThread = new Thread() {\n      @Override\n      public void run() {\n        try {\n          String line = errReader.readLine();\n          while((line != null) && !isInterrupted()) {\n            errMsg.append(line);\n            errMsg.append(System.getProperty(\"line.separator\"));\n            line = errReader.readLine();\n          }\n        } catch(IOException ioe) {\n          LOG.warn(\"Error reading the error stream\", ioe);\n        }\n      }\n    };\n    try {\n      errThread.start();\n    } catch (IllegalStateException ise) {\n    } catch (OutOfMemoryError oe) {\n      LOG.error(\"Caught \" + oe + \". One possible reason is that ulimit\"\n          + \" setting of 'max user processes' is too low. If so, do\"\n          + \" 'ulimit -u <largerNum>' and try again.\");\n      throw oe;\n    }\n    try {\n      parseExecResult(inReader); // parse the output\n      // clear the input stream buffer\n      String line = inReader.readLine();\n      while(line != null) { \n        line = inReader.readLine();\n      }\n      // wait for the process to finish and check the exit code\n      exitCode  = process.waitFor();\n      // make sure that the error thread exits\n      joinThread(errThread);\n      completed.set(true);\n      //the timeout thread handling\n      //taken care in finally block\n      if (exitCode != 0) {\n        throw new ExitCodeException(exitCode, errMsg.toString());\n      }\n    } catch (InterruptedException ie) {\n      InterruptedIOException iie = new InterruptedIOException(ie.toString());\n      iie.initCause(ie);\n      throw iie;\n    } finally {\n      if (timeOutTimer != null) {\n        timeOutTimer.cancel();\n      }\n      // close the input stream\n      try {\n        // JDK 7 tries to automatically drain the input streams for us\n        // when the process exits, but since close is not synchronized,\n        // it creates a race if we close the stream first and the same\n        // fd is recycled.  the stream draining thread will attempt to\n        // drain that fd!!  it may block, OOM, or cause bizarre behavior\n        // see: https://bugs.openjdk.java.net/browse/JDK-8024521\n        //      issue is fixed in build 7u60\n        InputStream stdout = process.getInputStream();\n        synchronized (stdout) {\n          inReader.close();\n        }\n      } catch (IOException ioe) {\n        LOG.warn(\"Error while closing the input stream\", ioe);\n      }\n      if (!completed.get()) {\n        errThread.interrupt();\n        joinThread(errThread);\n      }\n      try {\n        InputStream stderr = process.getErrorStream();\n        synchronized (stderr) {\n          errReader.close();\n        }\n      } catch (IOException ioe) {\n        LOG.warn(\"Error while closing the error stream\", ioe);\n      }\n      process.destroy();\n      lastTime = Time.monotonicNow();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.toString": "    public String toString() {\n      StringBuilder builder = new StringBuilder();\n      String[] args = getExecString();\n      for (String s : args) {\n        if (s.indexOf(' ') >= 0) {\n          builder.append('\"').append(s).append('\"');\n        } else {\n          builder.append(s);\n        }\n        builder.append(' ');\n      }\n      return builder.toString();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.parseExecResult": "    protected void parseExecResult(BufferedReader lines) throws IOException {\n      output = new StringBuffer();\n      char[] buf = new char[512];\n      int nRead;\n      while ( (nRead = lines.read(buf, 0, buf.length)) > 0 ) {\n        output.append(buf, 0, nRead);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.joinThread": "  private static void joinThread(Thread t) {\n    while (t.isAlive()) {\n      try {\n        t.join();\n      } catch (InterruptedException ie) {\n        if (LOG.isWarnEnabled()) {\n          LOG.warn(\"Interrupted while joining on: \" + t, ie);\n        }\n        t.interrupt(); // propagate interrupt\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.close": "    public void close() {\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.getExecString": "    public String[] getExecString() {\n      return command;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.run": "    public void run() {\n      Process p = shell.getProcess();\n      try {\n        p.exitValue();\n      } catch (Exception e) {\n        //Process has not terminated.\n        //So check if it has completed \n        //if not just destroy it.\n        if (p != null && !shell.completed.get()) {\n          shell.setTimedOut();\n          p.destroy();\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.getProcess": "  public Process getProcess() {\n    return process;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.setTimedOut": "  private void setTimedOut() {\n    this.timedOut.set(true);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.execute": "    public void execute() throws IOException {\n      for (String s : command) {\n        if (s == null) {\n          throw new IOException(\"(null) entry in command string: \"\n              + StringUtils.join(\" \", command));\n        }\n      }\n      this.run();    \n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Time.monotonicNow": "  public static long monotonicNow() {\n    return System.nanoTime() / NANOSECONDS_PER_MILLISECOND;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.StringUtils.join": "  public static String join(char separator, String[] strings) {\n    return join(separator + \"\", strings);\n  }"
        },
        "bug_report": {
            "Title": "getFailureInfo not returning any failure info",
            "Description": "The following command does not produce any failure info as to why the job failed. \n\n{noformat}\n$HADOOP_PREFIX/bin/hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-${HADOOP_VERSION}-tests.jar sleep -Dmapreduce.jobtracker.split.metainfo.maxsize=10 -Dmapreduce.job.queuename=default -m 1 -r 1 -mt 1 -rt 1\n{noformat}\n\n{noformat}\n2016-03-07 10:34:58,112 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1431)) - Job job_1457364518683_0004 failed with state FAILED due to: \n{noformat}\n\nTo contrast, here is a command and associated command line output to show a failed job that gives the correct failiure info. \n\n{noformat}\n$HADOOP_PREFIX/bin/hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-${HADOOP_VERSION}-tests.jar sleep -Dyarn.app.mapreduce.am.command-opts=-goober -Dmapreduce.job.queuename=default -m 20 -r 0 -mt 30000\n{noformat}\n\n{noformat}\n2016-03-07 10:30:13,103 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1431)) - Job job_1457364518683_0003 failed with state FAILED due to: Application application_1457364518683_0003 failed 3 times due to AM Container for appattempt_1457364518683_0003_000003 exited with  exitCode: 1\nFailing this attempt.Diagnostics: Exception from container-launch.\nContainer id: container_1457364518683_0003_03_000001\nExit code: 1\nStack trace: ExitCodeException exitCode=1: \n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:927)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:838)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1117)\n\tat org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:227)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:319)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:88)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n{noformat}"
        }
    },
    {
        "filename": "MAPREDUCE-4048.json",
        "creation_time": "2012-03-21T10:01:09.000+0000",
        "stack_trace": "```\njava.lang.reflect.InvocationTargetException\n        at sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:150)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)\n        at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)\n        at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)\n        .......\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\nCaused by: java.lang.NullPointerException\n        at com.google.common.base.Joiner.toString(Joiner.java:317)\n        at com.google.common.base.Joiner.appendTo(Joiner.java:97)\n        at com.google.common.base.Joiner.appendTo(Joiner.java:127)\n        at com.google.common.base.Joiner.join(Joiner.java:158)\n        at com.google.common.base.Joiner.join(Joiner.java:166)\n        at org.apache.hadoop.yarn.util.StringHelper.join(StringHelper.java:102)\n        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.badRequest(AppController.java:319)\n        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts(AppController.java:286)```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.Dispatcher.service": "  public void service(HttpServletRequest req, HttpServletResponse res)\n      throws ServletException, IOException {\n    res.setCharacterEncoding(\"UTF-8\");\n    String uri = HtmlQuoting.quoteHtmlChars(req.getRequestURI());\n\n    if (uri == null) {\n      uri = \"/\";\n    }\n    if (devMode && uri.equals(\"/__stop\")) {\n      // quick hack to restart servers in dev mode without OS commands\n      res.setStatus(res.SC_NO_CONTENT);\n      LOG.info(\"dev mode restart requested\");\n      prepareToExit();\n      return;\n    }\n    // if they provide a redirectPath go there instead of going to\n    // \"/\" so that filters can differentiate the webapps.\n    if (uri.equals(\"/\")) {\n      String redirectPath = webApp.getRedirectPath();\n      if (redirectPath != null && !redirectPath.isEmpty()) {\n        res.sendRedirect(redirectPath);\n        return;\n      }\n    }\n    String method = req.getMethod();\n    if (method.equals(\"OPTIONS\")) {\n      doOptions(req, res);\n      return;\n    }\n    if (method.equals(\"TRACE\")) {\n      doTrace(req, res);\n      return;\n    }\n    if (method.equals(\"HEAD\")) {\n      doGet(req, res); // default to bad request\n      return;\n    }\n    String pathInfo = req.getPathInfo();\n    if (pathInfo == null) {\n      pathInfo = \"/\";\n    }\n    Controller.RequestContext rc =\n        injector.getInstance(Controller.RequestContext.class);\n    if (setCookieParams(rc, req) > 0) {\n      Cookie ec = rc.cookies().get(ERROR_COOKIE);\n      if (ec != null) {\n        rc.setStatus(Integer.parseInt(rc.cookies().\n            get(STATUS_COOKIE).getValue()));\n        removeErrorCookies(res, uri);\n        rc.set(Params.ERROR_DETAILS, ec.getValue());\n        render(ErrorPage.class);\n        return;\n      }\n    }\n    rc.prefix = webApp.name();\n    Router.Dest dest = null;\n    try {\n      dest = router.resolve(method, pathInfo);\n    } catch (WebAppException e) {\n      rc.error = e;\n      if (!e.getMessage().contains(\"not found\")) {\n        rc.setStatus(res.SC_INTERNAL_SERVER_ERROR);\n        render(ErrorPage.class);\n        return;\n      }\n    }\n    if (dest == null) {\n      rc.setStatus(res.SC_NOT_FOUND);\n      render(ErrorPage.class);\n      return;\n    }\n    rc.devMode = devMode;\n    setMoreParams(rc, pathInfo, dest);\n    Controller controller = injector.getInstance(dest.controllerClass);\n    try {\n      // TODO: support args converted from /path/:arg1/...\n      dest.action.invoke(controller, (Object[]) null);\n      if (!rc.rendered) {\n        if (dest.defaultViewClass != null) {\n          render(dest.defaultViewClass);\n        } else if (rc.status == 200) {\n          throw new IllegalStateException(\"No view rendered for 200\");\n        }\n      }\n    } catch (Exception e) {\n      LOG.error(\"error handling URI: \"+ uri, e);\n      // Page could be half rendered (but still not flushed). So redirect.\n      redirectToErrorPage(res, e, uri, devMode);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.Dispatcher.doOptions": "  public void doOptions(HttpServletRequest req, HttpServletResponse res) {\n    // for simplicity\n    res.setHeader(\"Allow\", \"GET, POST\");\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.Dispatcher.render": "  private void render(Class<? extends View> cls) {\n    injector.getInstance(cls).render();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.Dispatcher.setCookieParams": "  private int setCookieParams(RequestContext rc, HttpServletRequest req) {\n    Cookie[] cookies = req.getCookies();\n    if (cookies != null) {\n      for (Cookie cookie : cookies) {\n        rc.cookies().put(cookie.getName(), cookie);\n      }\n      return cookies.length;\n    }\n    return 0;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.Dispatcher.redirectToErrorPage": "  public static void redirectToErrorPage(HttpServletResponse res, Throwable e,\n                                         String path, boolean devMode) {\n    String st = devMode ? ErrorPage.toStackTrace(e, 1024 * 3) // spec: min 4KB\n                        : \"See logs for stack trace\";\n    res.setStatus(res.SC_FOUND);\n    Cookie cookie = new Cookie(STATUS_COOKIE, String.valueOf(500));\n    cookie.setPath(path);\n    res.addCookie(cookie);\n    cookie = new Cookie(ERROR_COOKIE, st);\n    cookie.setPath(path);\n    res.addCookie(cookie);\n    res.setHeader(\"Location\", path);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.Dispatcher.prepareToExit": "  private void prepareToExit() {\n    checkState(devMode, \"only in dev mode\");\n    new Timer(\"webapp exit\", true).schedule(new TimerTask() {\n      @Override public void run() {\n        LOG.info(\"WebAppp /{} exiting...\", webApp.name());\n        webApp.stop();\n        System.exit(0); // FINDBUG: this is intended in dev mode\n      }\n    }, 18); // enough time for the last local request to complete\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.Dispatcher.removeErrorCookies": "  public static void removeErrorCookies(HttpServletResponse res, String path) {\n    removeCookie(res, ERROR_COOKIE, path);\n    removeCookie(res, STATUS_COOKIE, path);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.Dispatcher.setMoreParams": "  private void setMoreParams(RequestContext rc, String pathInfo, Dest dest) {\n    checkState(pathInfo.startsWith(dest.prefix), \"prefix should match\");\n    if (dest.pathParams.size() == 0 ||\n        dest.prefix.length() == pathInfo.length()) {\n      return;\n    }\n    String[] parts = Iterables.toArray(WebApp.pathSplitter.split(\n        pathInfo.substring(dest.prefix.length())), String.class);\n    LOG.debug(\"parts={}, params={}\", parts, dest.pathParams);\n    for (int i = 0; i < dest.pathParams.size() && i < parts.length; ++i) {\n      String key = dest.pathParams.get(i);\n      if (key.charAt(0) == ':') {\n        rc.moreParams().put(key.substring(1), parts[i]);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.util.StringHelper.join": "  public static String join(Object... args) {\n    return JOINER.join(args);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.webapp.AppController.badRequest": "  void badRequest(String s) {\n    setStatus(HttpServletResponse.SC_BAD_REQUEST);\n    setTitle(join(\"Bad request: \", s));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts": "  public void attempts() {\n    try {\n      requireJob();\n    }\n    catch (Exception e) {\n      renderText(e.getMessage());\n      return;\n    }\n    if (app.getJob() != null) {\n      try {\n        String taskType = $(TASK_TYPE);\n        if (taskType.isEmpty()) {\n          throw new RuntimeException(\"missing task-type.\");\n        }\n        String attemptState = $(ATTEMPT_STATE);\n        if (attemptState.isEmpty()) {\n          throw new RuntimeException(\"missing attempt-state.\");\n        }\n        setTitle(join(attemptState, \" \",\n            MRApps.taskType(taskType).toString(), \" attempts in \", $(JOB_ID)));\n\n        render(attemptsPage());\n      } catch (Exception e) {\n        badRequest(e.getMessage());\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attemptsPage": "  protected Class<? extends View> attemptsPage() {\n    return AttemptsPage.class;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.webapp.AppController.requireJob": "  public void requireJob() {\n    if ($(JOB_ID).isEmpty()) {\n      badRequest(\"missing job ID\");\n      throw new RuntimeException(\"Bad Request: Missing job ID\");\n    }\n\n    JobId jobID = MRApps.toJobID($(JOB_ID));\n    app.setJob(app.context.getJob(jobID));\n    if (app.getJob() == null) {\n      notFound($(JOB_ID));\n      throw new RuntimeException(\"Not Found: \" + $(JOB_ID));\n    }\n\n    /* check for acl access */\n    Job job = app.context.getJob(jobID);\n    if (!checkAccess(job)) {\n      accessDenied(\"User \" + request().getRemoteUser() + \" does not have \" +\n          \" permission to view job \" + $(JOB_ID));\n      throw new RuntimeException(\"Access denied: User \" +\n          request().getRemoteUser() + \" does not have permission to view job \" +\n          $(JOB_ID));\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.webApp.getRedirectPath": "  public String getRedirectPath() { return this.redirectPath; }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.Controller.set": "  public void set(String key, String value) {\n    context().set(key, value);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.Controller.context": "  public RequestContext context() {\n    if (context == null) {\n      if (injector == null) {\n        // One of the downsides of making injection in subclasses optional.\n        throw new WebAppException(join(\"Error accessing RequestContext from\\n\",\n            \"a child constructor, either move the usage of the Controller\\n\",\n            \"methods out of the constructor or inject the RequestContext\\n\",\n            \"into the constructor\"));\n      }\n      context = injector.getInstance(RequestContext.class);\n    }\n    return context;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.Controller.moreParams": "    public Map<String, String> moreParams() {\n      if (moreParams == null) {\n        moreParams = Maps.newHashMap();\n      }\n      return moreParams; // OK\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.Controller.setStatus": "  public void setStatus(int status) {\n    context().setStatus(status);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.router.resolve": "  synchronized Dest resolve(String httpMethod, String path) {\n    WebApp.HTTP method = WebApp.HTTP.valueOf(httpMethod); // can throw\n    Dest dest = lookupRoute(method, path);\n    if (dest == null) {\n      return resolveDefault(method, path);\n    }\n    return dest;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.router.resolveDefault": "  private Dest resolveDefault(WebApp.HTTP method, String path) {\n    List<String> parts = WebApp.parseRoute(path);\n    String controller = parts.get(WebApp.R_CONTROLLER);\n    String action = parts.get(WebApp.R_ACTION);\n    // NameController is encouraged default\n    Class<? extends Controller> cls = find(Controller.class,\n                                           join(controller, \"Controller\"));\n    if (cls == null) {\n      cls = find(Controller.class, controller);\n    }\n    if (cls == null) {\n      throw new WebAppException(join(path, \": controller for \", controller,\n                                \" not found\"));\n    }\n    return add(method, defaultPrefix(controller, action), cls, action, null);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.router.lookupRoute": "  private Dest lookupRoute(WebApp.HTTP method, String path) {\n    String key = path;\n    do {\n      Dest dest = routes.get(key);\n      if (dest != null && methodAllowed(method, dest)) {\n        if ((Object)key == path) { // shut up warnings\n          LOG.debug(\"exact match for {}: {}\", key, dest.action);\n          return dest;\n        } else if (isGoodMatch(dest, path)) {\n          LOG.debug(\"prefix match2 for {}: {}\", key, dest.action);\n          return dest;\n        }\n        return resolveAction(method, dest, path);\n      }\n      Map.Entry<String, Dest> lower = routes.lowerEntry(key);\n      if (lower == null) {\n        return null;\n      }\n      dest = lower.getValue();\n      if (prefixMatches(dest, path)) {\n        if (methodAllowed(method, dest)) {\n          if (isGoodMatch(dest, path)) {\n            LOG.debug(\"prefix match for {}: {}\", lower.getKey(), dest.action);\n            return dest;\n          }\n          return resolveAction(method, dest, path);\n        }\n        // check other candidates\n        int slashPos = key.lastIndexOf('/');\n        key = slashPos > 0 ? path.substring(0, slashPos) : \"/\";\n      } else {\n        key = \"/\";\n      }\n    } while (true);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.webApp.name": "  public String name() { return this.name; }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.Controller.cookies": "  public Map<String, Cookie> cookies() {\n    return context().cookies();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.webapp.app.getJob": "  public Job getJob() {\n    return job;\n  }"
        },
        "bug_report": {
            "Title": "NullPointerException exception while accessing the Application Master UI",
            "Description": "{code:xml}\n2012-03-21 10:21:31,838 ERROR [2145015588@qtp-957250718-801] org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /mapreduce/attempts/job_1332261815858_2_8/m/KILLED\njava.lang.reflect.InvocationTargetException\n        at sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:150)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)\n        at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)\n        at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)\n        .......\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\nCaused by: java.lang.NullPointerException\n        at com.google.common.base.Joiner.toString(Joiner.java:317)\n        at com.google.common.base.Joiner.appendTo(Joiner.java:97)\n        at com.google.common.base.Joiner.appendTo(Joiner.java:127)\n        at com.google.common.base.Joiner.join(Joiner.java:158)\n        at com.google.common.base.Joiner.join(Joiner.java:166)\n        at org.apache.hadoop.yarn.util.StringHelper.join(StringHelper.java:102)\n        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.badRequest(AppController.java:319)\n        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts(AppController.java:286)\n        ... 36 more\n{code}"
        }
    },
    {
        "filename": "MAPREDUCE-5198.json",
        "creation_time": "2013-04-30T22:22:33.000+0000",
        "stack_trace": "```\njava.nio.channels.ClosedChannelException\n\tat sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:133)\n\tat sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)\n\tat org.apache.hadoop.ipc.Server.channelWrite(Server.java:1717)\n\tat org.apache.hadoop.ipc.Server.access$2000(Server.java:98)\n\tat org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:744)\n\tat org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:808)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1433)\n\norg.apache.hadoop.util.Shell$ExitCodeException: \n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:255)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:182)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:375)\n\tat org.apache.hadoop.mapred.LinuxTaskController.deleteAsUser(LinuxTaskController.java:281)\n\tat org.apache.hadoop.mapred.TaskTracker.deleteUserDirectories(TaskTracker.java:779)\n\tat org.apache.hadoop.mapred.TaskTracker.initialize(TaskTracker.java:816)\n\tat org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:2704)\n\tat org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:3934)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.channelWrite": "  private int channelWrite(WritableByteChannel channel, \n                           ByteBuffer buffer) throws IOException {\n    \n    int count =  (buffer.remaining() <= NIO_BUFFER_LIMIT) ?\n                 channel.write(buffer) : channelIO(null, channel, buffer);\n    if (count > 0) {\n      rpcMetrics.incrSentBytes(count);\n    }\n    return count;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.channelIO": "  private static int channelIO(ReadableByteChannel readCh, \n                               WritableByteChannel writeCh,\n                               ByteBuffer buf) throws IOException {\n    \n    int originalLimit = buf.limit();\n    int initialRemaining = buf.remaining();\n    int ret = 0;\n    \n    while (buf.remaining() > 0) {\n      try {\n        int ioSize = Math.min(buf.remaining(), NIO_BUFFER_LIMIT);\n        buf.limit(buf.position() + ioSize);\n        \n        ret = (readCh == null) ? writeCh.write(buf) : readCh.read(buf); \n        \n        if (ret < ioSize) {\n          break;\n        }\n\n      } finally {\n        buf.limit(originalLimit);        \n      }\n    }\n\n    int nBytes = initialRemaining - buf.remaining(); \n    return (nBytes > 0) ? nBytes : ret;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.write": "    void write(DataOutput out) throws IOException {\n      out.writeByte(this.ordinal());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.processResponse": "    private boolean processResponse(LinkedList<Call> responseQueue,\n                                    boolean inHandler) throws IOException {\n      boolean error = true;\n      boolean done = false;       // there is more data for this channel.\n      int numElements = 0;\n      Call call = null;\n      try {\n        synchronized (responseQueue) {\n          //\n          // If there are no items for this channel, then we are done\n          //\n          numElements = responseQueue.size();\n          if (numElements == 0) {\n            error = false;\n            return true;              // no more data for this channel.\n          }\n          //\n          // Extract the first call\n          //\n          call = responseQueue.removeFirst();\n          SocketChannel channel = call.connection.channel;\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(getName() + \": responding to #\" + call.callId + \" from \" +\n                      call.connection);\n          }\n          //\n          // Send as much data as we can in the non-blocking fashion\n          //\n          int numBytes = channelWrite(channel, call.rpcResponse);\n          if (numBytes < 0) {\n            return true;\n          }\n          if (!call.rpcResponse.hasRemaining()) {\n            //Clear out the response buffer so it can be collected\n            call.rpcResponse = null;\n            call.connection.decRpcCount();\n            if (numElements == 1) {    // last call fully processes.\n              done = true;             // no more data for this channel.\n            } else {\n              done = false;            // more calls pending to be sent.\n            }\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(getName() + \": responding to #\" + call.callId + \" from \" +\n                        call.connection + \" Wrote \" + numBytes + \" bytes.\");\n            }\n          } else {\n            //\n            // If we were unable to write the entire response out, then \n            // insert in Selector queue. \n            //\n            call.connection.responseQueue.addFirst(call);\n            \n            if (inHandler) {\n              // set the serve time when the response has to be sent later\n              call.timestamp = Time.now();\n              \n              incPending();\n              try {\n                // Wakeup the thread blocked on select, only then can the call \n                // to channel.register() complete.\n                writeSelector.wakeup();\n                channel.register(writeSelector, SelectionKey.OP_WRITE, call);\n              } catch (ClosedChannelException e) {\n                //Its ok. channel might be closed else where.\n                done = true;\n              } finally {\n                decPending();\n              }\n            }\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(getName() + \": responding to #\" + call.callId + \" from \" +\n                        call.connection + \" Wrote partial \" + numBytes + \n                        \" bytes.\");\n            }\n          }\n          error = false;              // everything went off well\n        }\n      } finally {\n        if (error && call != null) {\n          LOG.warn(getName()+\", call \" + call + \": output error\");\n          done = true;               // error. no more data for this channel.\n          closeConnection(call.connection);\n        }\n      }\n      return done;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.incPending": "    private synchronized void incPending() {   // call waiting to be enqueued.\n      pending++;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeConnection": "  private void closeConnection(Connection connection) {\n    synchronized (connectionList) {\n      if (connectionList.remove(connection))\n        numConnections--;\n    }\n    try {\n      connection.close();\n    } catch (IOException e) {\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.decPending": "    private synchronized void decPending() { // call done enqueueing.\n      pending--;\n      notify();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRespond": "    void doRespond(Call call) throws IOException {\n      synchronized (call.connection.responseQueue) {\n        call.connection.responseQueue.addLast(call);\n        if (call.connection.responseQueue.size() == 1) {\n          processResponse(call.connection.responseQueue, true);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "                     public Writable run() throws Exception {\n                       // make the call\n                       return call(call.rpcKind, call.connection.protocolName, \n                                   call.rpcRequest, call.timestamp);\n\n                     }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.toString": "    public String toString() {\n      return getHostAddress() + \":\" + remotePort; \n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    private synchronized void close() throws IOException {\n      disposeSasl();\n      data = null;\n      dataLengthBuffer = null;\n      if (!channel.isOpen())\n        return;\n      try {socket.shutdownOutput();} catch(Exception e) {\n        LOG.debug(\"Ignoring socket shutdown exception\", e);\n      }\n      if (channel.isOpen()) {\n        try {channel.close();} catch(Exception e) {}\n      }\n      try {socket.close();} catch(Exception e) {}\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          if (LOG.isDebugEnabled())\n            LOG.debug(getName() + \": disconnecting client \" + c.getHostAddress());\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.isTerse": "    boolean isTerse(Class<?> t) {\n      return terseExceptions.contains(t.toString());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws IOException,  OutOfMemoryError {\n      Connection c = null;\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        \n        Reader reader = getReader();\n        try {\n          reader.startAdd();\n          SelectionKey readKey = reader.registerChannel(channel);\n          c = new Connection(readKey, channel, Time.now());\n          readKey.attach(c);\n          synchronized (connectionList) {\n            connectionList.add(numConnections, c);\n            numConnections++;\n          }\n          if (LOG.isDebugEnabled())\n            LOG.debug(\"Server connection from \" + c.toString() +\n                \"; # active connections: \" + numConnections +\n                \"; # queued calls: \" + callQueue.size());          \n        } finally {\n          reader.finishAdd(); \n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.cleanupConnections": "    private void cleanupConnections(boolean force) {\n      if (force || numConnections > thresholdIdleConnections) {\n        long currentTime = Time.now();\n        if (!force && (currentTime - lastCleanupRunTime) < cleanupInterval) {\n          return;\n        }\n        int start = 0;\n        int end = numConnections - 1;\n        if (!force) {\n          start = rand.nextInt() % numConnections;\n          end = rand.nextInt() % numConnections;\n          int temp;\n          if (end < start) {\n            temp = start;\n            start = end;\n            end = temp;\n          }\n        }\n        int i = start;\n        int numNuked = 0;\n        while (i <= end) {\n          Connection c;\n          synchronized (connectionList) {\n            try {\n              c = connectionList.get(i);\n            } catch (Exception e) {return;}\n          }\n          if (c.timedOut(currentTime)) {\n            if (LOG.isDebugEnabled())\n              LOG.debug(getName() + \": disconnecting client \" + c.getHostAddress());\n            closeConnection(c);\n            numNuked++;\n            end--;\n            c = null;\n            if (!force && numNuked == maxConnectionsToNuke) break;\n          }\n          else i++;\n        }\n        lastCleanupRunTime = Time.now();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(ByteArrayOutputStream responseBuf,\n                             Call call, RpcStatusProto status, \n                             Writable rv, String errorClass, String error) \n  throws IOException {\n    responseBuf.reset();\n    DataOutputStream out = new DataOutputStream(responseBuf);\n    RpcResponseHeaderProto.Builder headerBuilder =  \n        RpcResponseHeaderProto.newBuilder();\n    headerBuilder.setCallId(call.callId);\n    headerBuilder.setStatus(status);\n    headerBuilder.setServerIpcVersionNum(Server.CURRENT_VERSION);\n\n    if (status == RpcStatusProto.SUCCESS) {\n      RpcResponseHeaderProto header = headerBuilder.build();\n      final int headerLen = header.getSerializedSize();\n      int fullLength  = CodedOutputStream.computeRawVarint32Size(headerLen) +\n          headerLen;\n      try {\n        if (rv instanceof ProtobufRpcEngine.RpcWrapper) {\n          ProtobufRpcEngine.RpcWrapper resWrapper = \n              (ProtobufRpcEngine.RpcWrapper) rv;\n          fullLength += resWrapper.getLength();\n          out.writeInt(fullLength);\n          header.writeDelimitedTo(out);\n          rv.write(out);\n        } else { // Have to serialize to buffer to get len\n          final DataOutputBuffer buf = new DataOutputBuffer();\n          rv.write(buf);\n          byte[] data = buf.getData();\n          fullLength += buf.getLength();\n          out.writeInt(fullLength);\n          header.writeDelimitedTo(out);\n          out.write(data, 0, buf.getLength());\n        }\n      } catch (Throwable t) {\n        LOG.warn(\"Error serializing call response for call \" + call, t);\n        // Call back to same function - this is OK since the\n        // buffer is reset at the top, and since status is changed\n        // to ERROR it won't infinite loop.\n        setupResponse(responseBuf, call, RpcStatusProto.ERROR,\n            null, t.getClass().getName(),\n            StringUtils.stringifyException(t));\n        return;\n      }\n    } else { // Rpc Failure\n      headerBuilder.setExceptionClassName(errorClass);\n      headerBuilder.setErrorMsg(error);\n      RpcResponseHeaderProto header = headerBuilder.build();\n      int headerLen = header.getSerializedSize();\n      final int fullLength  = \n          CodedOutputStream.computeRawVarint32Size(headerLen) + headerLen;\n      out.writeInt(fullLength);\n      header.writeDelimitedTo(out);\n    }\n    if (call.connection.useWrap) {\n      wrapWithSasl(responseBuf, call);\n    }\n    call.setResponse(ByteBuffer.wrap(responseBuf.toByteArray()));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.createSaslServer": "    private SaslServer createSaslServer(final String mechanism,\n                                        final String protocol,\n                                        final String hostname,\n                                        final CallbackHandler callback\n        ) throws IOException, InterruptedException {\n      SaslServer saslServer = UserGroupInformation.getCurrentUser().doAs(\n          new PrivilegedExceptionAction<SaslServer>() {\n            @Override\n            public SaslServer run() throws SaslException  {\n              return Sasl.createSaslServer(mechanism, protocol, hostname,\n                                           SaslRpcServer.SASL_PROPS, callback);\n            }\n          });\n      if (saslServer == null) {\n        throw new AccessControlException(\n            \"Unable to find SASL server implementation for \" + mechanism);\n      }\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Created SASL server with mechanism = \" + mechanism);\n      }\n      return saslServer;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(RPC.RpcKind rpcKind, String protocol,\n      Writable param, long receiveTime) throws Exception;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param protocolName - the protocol\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  private void authorize(UserGroupInformation user, String protocolName,\n      InetAddress addr) throws AuthorizationException {\n    if (authorize) {\n      if (protocolName == null) {\n        throw new AuthorizationException(\"Null protocol not authorized\");\n      }\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(protocolName, getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         protocolName);\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isValid() && key.isWritable()) {\n                  doAsyncWrite(key);\n              }\n            } catch (IOException e) {\n              LOG.info(getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = Time.now();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<Call> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<Call>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              Call call = (Call)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n          \n          for(Call call : calls) {\n            try {\n              doPurge(call, now);\n            } catch (IOException e) {\n              LOG.warn(\"Error in purging old calls \" + e);\n            }\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.runCommand": "  private void runCommand() throws IOException { \n    ProcessBuilder builder = new ProcessBuilder(getExecString());\n    Timer timeOutTimer = null;\n    ShellTimeoutTimerTask timeoutTimerTask = null;\n    timedOut = new AtomicBoolean(false);\n    completed = new AtomicBoolean(false);\n    \n    if (environment != null) {\n      builder.environment().putAll(this.environment);\n    }\n    if (dir != null) {\n      builder.directory(this.dir);\n    }\n    \n    if (Shell.WINDOWS) {\n      synchronized (WindowsProcessLaunchLock) {\n        // To workaround the race condition issue with child processes\n        // inheriting unintended handles during process launch that can\n        // lead to hangs on reading output and error streams, we\n        // serialize process creation. More info available at:\n        // http://support.microsoft.com/kb/315939\n        process = builder.start();\n      }\n    } else {\n      process = builder.start();\n    }\n\n    if (timeOutInterval > 0) {\n      timeOutTimer = new Timer(\"Shell command timeout\");\n      timeoutTimerTask = new ShellTimeoutTimerTask(\n          this);\n      //One time scheduling.\n      timeOutTimer.schedule(timeoutTimerTask, timeOutInterval);\n    }\n    final BufferedReader errReader = \n            new BufferedReader(new InputStreamReader(process\n                                                     .getErrorStream()));\n    BufferedReader inReader = \n            new BufferedReader(new InputStreamReader(process\n                                                     .getInputStream()));\n    final StringBuffer errMsg = new StringBuffer();\n    \n    // read error and input streams as this would free up the buffers\n    // free the error stream buffer\n    Thread errThread = new Thread() {\n      @Override\n      public void run() {\n        try {\n          String line = errReader.readLine();\n          while((line != null) && !isInterrupted()) {\n            errMsg.append(line);\n            errMsg.append(System.getProperty(\"line.separator\"));\n            line = errReader.readLine();\n          }\n        } catch(IOException ioe) {\n          LOG.warn(\"Error reading the error stream\", ioe);\n        }\n      }\n    };\n    try {\n      errThread.start();\n    } catch (IllegalStateException ise) { }\n    try {\n      parseExecResult(inReader); // parse the output\n      // clear the input stream buffer\n      String line = inReader.readLine();\n      while(line != null) { \n        line = inReader.readLine();\n      }\n      // wait for the process to finish and check the exit code\n      exitCode  = process.waitFor();\n      try {\n        // make sure that the error thread exits\n        errThread.join();\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Interrupted while reading the error stream\", ie);\n      }\n      completed.set(true);\n      //the timeout thread handling\n      //taken care in finally block\n      if (exitCode != 0) {\n        throw new ExitCodeException(exitCode, errMsg.toString());\n      }\n    } catch (InterruptedException ie) {\n      throw new IOException(ie.toString());\n    } finally {\n      if (timeOutTimer != null) {\n        timeOutTimer.cancel();\n      }\n      // close the input stream\n      try {\n        inReader.close();\n      } catch (IOException ioe) {\n        LOG.warn(\"Error while closing the input stream\", ioe);\n      }\n      if (!completed.get()) {\n        errThread.interrupt();\n      }\n      try {\n        errReader.close();\n      } catch (IOException ioe) {\n        LOG.warn(\"Error while closing the error stream\", ioe);\n      }\n      process.destroy();\n      lastTime = Time.now();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.toString": "    public String toString() {\n      StringBuilder builder = new StringBuilder();\n      String[] args = getExecString();\n      for (String s : args) {\n        if (s.indexOf(' ') >= 0) {\n          builder.append('\"').append(s).append('\"');\n        } else {\n          builder.append(s);\n        }\n        builder.append(' ');\n      }\n      return builder.toString();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.parseExecResult": "    protected void parseExecResult(BufferedReader lines) throws IOException {\n      output = new StringBuffer();\n      char[] buf = new char[512];\n      int nRead;\n      while ( (nRead = lines.read(buf, 0, buf.length)) > 0 ) {\n        output.append(buf, 0, nRead);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.getExecString": "    public String[] getExecString() {\n      return command;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.run": "    public void run() {\n      Process p = shell.getProcess();\n      try {\n        p.exitValue();\n      } catch (Exception e) {\n        //Process has not terminated.\n        //So check if it has completed \n        //if not just destroy it.\n        if (p != null && !shell.completed.get()) {\n          shell.setTimedOut();\n          p.destroy();\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.getProcess": "  public Process getProcess() {\n    return process;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.setTimedOut": "  private void setTimedOut() {\n    this.timedOut.set(true);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.execute": "    public void execute() throws IOException {\n      this.run();    \n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Time.now": "  public static long now() {\n    return System.currentTimeMillis();\n  }"
        },
        "bug_report": {
            "Title": "Race condition in cleanup during task tracker renint with LinuxTaskController",
            "Description": "This was noticed when job tracker would be restarted while jobs were running and would ask the task tracker to reinitialize. \n\nTasktracker would fail with an error like\n\n{code}\n013-04-27 20:19:09,627 INFO org.apache.hadoop.mapred.TaskTracker: Good mapred local directories are: /grid/0/hdp/mapred/local,/grid/1/hdp/mapred/local,/grid/2/hdp/mapred/local,/grid/3/hdp/mapred/local,/grid/4/hdp/mapred/local,/grid/5/hdp/mapred/local\n2013-04-27 20:19:09,628 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 42075 caught: java.nio.channels.ClosedChannelException\n\tat sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:133)\n\tat sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)\n\tat org.apache.hadoop.ipc.Server.channelWrite(Server.java:1717)\n\tat org.apache.hadoop.ipc.Server.access$2000(Server.java:98)\n\tat org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:744)\n\tat org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:808)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1433)\n\n2013-04-27 20:19:09,628 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 42075: exiting\n2013-04-27 20:19:10,414 ERROR org.apache.hadoop.mapred.TaskTracker: Got fatal exception while reinitializing TaskTracker: org.apache.hadoop.util.Shell$ExitCodeException: \n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:255)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:182)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:375)\n\tat org.apache.hadoop.mapred.LinuxTaskController.deleteAsUser(LinuxTaskController.java:281)\n\tat org.apache.hadoop.mapred.TaskTracker.deleteUserDirectories(TaskTracker.java:779)\n\tat org.apache.hadoop.mapred.TaskTracker.initialize(TaskTracker.java:816)\n\tat org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:2704)\n\tat org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:3934)\n{code} "
        }
    },
    {
        "filename": "MAPREDUCE-7028.json",
        "creation_time": "2017-12-20T15:33:20.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2450)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2433)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1362)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:154)\n        at org.apache.hadoop.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1543)\n        at org.apache.hadoop.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1535)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)\n        at java.lang.Thread.run(Thread.java:748)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.transition": "    public void transition(TaskAttemptImpl taskAttempt, \n        TaskAttemptEvent event) {\n      TaskAttemptDiagnosticsUpdateEvent diagEvent =\n          (TaskAttemptDiagnosticsUpdateEvent) event;\n      LOG.info(\"Diagnostics report from \" + taskAttempt.attemptId + \": \"\n          + diagEvent.getDiagnosticInfo());\n      taskAttempt.addDiagnosticInfo(diagEvent.getDiagnosticInfo());\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getAssignedContainerMgrAddress": "  public String getAssignedContainerMgrAddress() {\n    readLock.lock();\n    try {\n      return container == null ? null : StringInterner.weakIntern(container\n        .getNodeId().toString());\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createJobCounterUpdateEventTASucceeded": "  private static JobCounterUpdateEvent createJobCounterUpdateEventTASucceeded(\n      TaskAttemptImpl taskAttempt) {\n    TaskId taskId = taskAttempt.attemptId.getTaskId();\n    JobCounterUpdateEvent jce = new JobCounterUpdateEvent(taskId.getJobId());\n    updateMillisCounters(jce, taskAttempt);\n    return jce;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.addDiagnosticInfo": "  private void addDiagnosticInfo(String diag) {\n    if (diag != null && !diag.equals(\"\")) {\n      diagnostics.add(diag);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getAssignedContainerID": "  public ContainerId getAssignedContainerID() {\n    readLock.lock();\n    try {\n      return container == null ? null : container.getId();\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.notifyTaskAttemptFailed": "  private static void notifyTaskAttemptFailed(TaskAttemptImpl taskAttempt) {\n    if (taskAttempt.getLaunchTime() == 0) {\n      sendJHStartEventForAssignedFailTask(taskAttempt);\n    }\n    // set the finish time\n    taskAttempt.setFinishTime();\n    taskAttempt.eventHandler\n        .handle(createJobCounterUpdateEventTAFailed(taskAttempt, false));\n    TaskAttemptUnsuccessfulCompletionEvent tauce =\n        createTaskAttemptUnsuccessfulCompletionEvent(taskAttempt,\n            TaskAttemptStateInternal.FAILED);\n    taskAttempt.eventHandler.handle(new JobHistoryEvent(\n        taskAttempt.attemptId.getTaskId().getJobId(), tauce));\n\n    taskAttempt.eventHandler.handle(new TaskTAttemptEvent(\n        taskAttempt.attemptId, TaskEventType.T_ATTEMPT_FAILED));\n\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.setRescheduleNextAttempt": "  private void setRescheduleNextAttempt(boolean reschedule) {\n    rescheduleNextAttempt = reschedule;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createRemoteTask": "  protected abstract org.apache.hadoop.mapred.Task createRemoteTask();\n\n  @Override\n  public TaskAttemptId getID() {\n    return attemptId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.setFinishTime": "  private void setFinishTime() {\n    //set the finish time only if launch time is set\n    if (launchTime != 0) {\n      finishTime = clock.getTime();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getFinishTime": "  public long getFinishTime() {\n    readLock.lock();\n    try {\n      return finishTime;\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.finalizeProgress": "  private static void finalizeProgress(TaskAttemptImpl taskAttempt) {\n    // unregister it to TaskAttemptListener so that it stops listening\n    taskAttempt.taskAttemptListener.unregister(\n        taskAttempt.attemptId, taskAttempt.jvmID);\n    taskAttempt.reportedStatus.progress = 1.0f;\n    taskAttempt.updateProgressSplits();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createTaskAttemptUnsuccessfulCompletionEvent": "      TaskAttemptUnsuccessfulCompletionEvent\n      createTaskAttemptUnsuccessfulCompletionEvent(TaskAttemptImpl taskAttempt,\n          TaskAttemptStateInternal attemptState) {\n    TaskAttemptUnsuccessfulCompletionEvent tauce =\n        new TaskAttemptUnsuccessfulCompletionEvent(\n            TypeConverter.fromYarn(taskAttempt.attemptId),\n            TypeConverter.fromYarn(taskAttempt.attemptId.getTaskId()\n                .getTaskType()), attemptState.toString(),\n            taskAttempt.finishTime,\n            taskAttempt.container == null ? \"UNKNOWN\"\n                : taskAttempt.container.getNodeId().getHost(),\n            taskAttempt.container == null ? -1 \n                : taskAttempt.container.getNodeId().getPort(),    \n            taskAttempt.nodeRackName == null ? \"UNKNOWN\" \n                : taskAttempt.nodeRackName,\n            StringUtils.join(\n                LINE_SEPARATOR, taskAttempt.getDiagnostics()),\n                taskAttempt.getCounters(), taskAttempt\n                .getProgressSplitBlock().burst(), taskAttempt.launchTime);\n    return tauce;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getShufflePort": "  public int getShufflePort() {\n    readLock.lock();\n    try {\n      return shufflePort;\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getLaunchTime": "  public long getLaunchTime() {\n    readLock.lock();\n    try {\n      return launchTime;\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getID": "  public TaskAttemptId getID() {\n    return attemptId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.logAttemptFinishedEvent": "  private void logAttemptFinishedEvent(TaskAttemptStateInternal state) {\n    //Log finished events only if an attempt started.\n    if (getLaunchTime() == 0) return; \n    String containerHostName = this.container == null ? \"UNKNOWN\"\n         : this.container.getNodeId().getHost();\n    int containerNodePort =\n        this.container == null ? -1 : this.container.getNodeId().getPort();\n    if (attemptId.getTaskId().getTaskType() == TaskType.MAP) {\n      MapAttemptFinishedEvent mfe =\n          new MapAttemptFinishedEvent(TypeConverter.fromYarn(attemptId),\n          TypeConverter.fromYarn(attemptId.getTaskId().getTaskType()),\n          state.toString(),\n          this.reportedStatus.mapFinishTime,\n          finishTime,\n          containerHostName,\n          containerNodePort,\n          this.nodeRackName == null ? \"UNKNOWN\" : this.nodeRackName,\n          this.reportedStatus.stateString,\n          getCounters(),\n          getProgressSplitBlock().burst(), launchTime);\n      eventHandler.handle(\n          new JobHistoryEvent(attemptId.getTaskId().getJobId(), mfe));\n    } else {\n      ReduceAttemptFinishedEvent rfe =\n          new ReduceAttemptFinishedEvent(TypeConverter.fromYarn(attemptId),\n          TypeConverter.fromYarn(attemptId.getTaskId().getTaskType()),\n          state.toString(),\n          this.reportedStatus.shuffleFinishTime,\n          this.reportedStatus.sortFinishTime,\n          finishTime,\n          containerHostName,\n          containerNodePort,\n          this.nodeRackName == null ? \"UNKNOWN\" : this.nodeRackName,\n          this.reportedStatus.stateString,\n          getCounters(),\n          getProgressSplitBlock().burst(), launchTime);\n      eventHandler.handle(\n          new JobHistoryEvent(attemptId.getTaskId().getJobId(), rfe));\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createJobCounterUpdateEventTAFailed": "  private static JobCounterUpdateEvent createJobCounterUpdateEventTAFailed(\n      TaskAttemptImpl taskAttempt, boolean taskAlreadyCompleted) {\n    TaskType taskType = taskAttempt.getID().getTaskId().getTaskType();\n    JobCounterUpdateEvent jce = new JobCounterUpdateEvent(taskAttempt.getID().getTaskId().getJobId());\n    \n    if (taskType == TaskType.MAP) {\n      jce.addCounterUpdate(JobCounter.NUM_FAILED_MAPS, 1);\n    } else {\n      jce.addCounterUpdate(JobCounter.NUM_FAILED_REDUCES, 1);\n    }\n    if (!taskAlreadyCompleted) {\n      updateMillisCounters(jce, taskAttempt);\n    }\n    return jce;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.updateProgressSplits": "  private void updateProgressSplits() {\n    double newProgress = reportedStatus.progress;\n    newProgress = Math.max(Math.min(newProgress, 1.0D), 0.0D);\n    Counters counters = reportedStatus.counters;\n    if (counters == null)\n      return;\n\n    WrappedProgressSplitsBlock splitsBlock = getProgressSplitBlock();\n    if (splitsBlock != null) {\n      long now = clock.getTime();\n      long start = getLaunchTime(); // TODO Ensure not 0\n\n      if (start != 0 && now - start <= Integer.MAX_VALUE) {\n        splitsBlock.getProgressWallclockTime().extend(newProgress,\n            (int) (now - start));\n      }\n\n      Counter cpuCounter = counters.findCounter(TaskCounter.CPU_MILLISECONDS);\n      if (cpuCounter != null && cpuCounter.getValue() <= Integer.MAX_VALUE) {\n        splitsBlock.getProgressCPUTime().extend(newProgress,\n            (int) cpuCounter.getValue()); // long to int? TODO: FIX. Same below\n      }\n\n      Counter virtualBytes = counters\n        .findCounter(TaskCounter.VIRTUAL_MEMORY_BYTES);\n      if (virtualBytes != null) {\n        splitsBlock.getProgressVirtualMemoryKbytes().extend(newProgress,\n            (int) (virtualBytes.getValue() / (MEMORY_SPLITS_RESOLUTION)));\n      }\n\n      Counter physicalBytes = counters\n        .findCounter(TaskCounter.PHYSICAL_MEMORY_BYTES);\n      if (physicalBytes != null) {\n        splitsBlock.getProgressPhysicalMemoryKbytes().extend(newProgress,\n            (int) (physicalBytes.getValue() / (MEMORY_SPLITS_RESOLUTION)));\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendContainerCleanup": "  private static void sendContainerCleanup(TaskAttemptImpl taskAttempt,\n      TaskAttemptEvent event) {\n    if (event instanceof TaskAttemptKillEvent) {\n      taskAttempt.addDiagnosticInfo(\n          ((TaskAttemptKillEvent) event).getMessage());\n    }\n    //send the cleanup event to containerLauncher\n    taskAttempt.eventHandler.handle(new ContainerLauncherEvent(\n        taskAttempt.attemptId,\n        taskAttempt.container.getId(), StringInterner\n        .weakIntern(taskAttempt.container.getNodeId().toString()),\n        taskAttempt.container.getContainerToken(),\n        ContainerLauncher.EventType.CONTAINER_REMOTE_CLEANUP,\n        event.getType() == TaskAttemptEventType.TA_TIMED_OUT));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendJHStartEventForAssignedFailTask": "  private static void\n      sendJHStartEventForAssignedFailTask(TaskAttemptImpl taskAttempt) {\n    if (null == taskAttempt.container) {\n      return;\n    }\n    taskAttempt.launchTime = taskAttempt.clock.getTime();\n\n    InetSocketAddress nodeHttpInetAddr =\n        NetUtils.createSocketAddr(taskAttempt.container.getNodeHttpAddress());\n    taskAttempt.trackerName = nodeHttpInetAddr.getHostName();\n    taskAttempt.httpPort = nodeHttpInetAddr.getPort();\n    taskAttempt.sendLaunchedEvents();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getRescheduleNextAttempt": "  private boolean getRescheduleNextAttempt() {\n    return rescheduleNextAttempt;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.recover": "  public TaskAttemptStateInternal recover(TaskAttemptInfo taInfo,\n      OutputCommitter committer, boolean recoverOutput) {\n    ContainerId containerId = taInfo.getContainerId();\n    NodeId containerNodeId = NodeId.fromString(\n        taInfo.getHostname() + \":\" + taInfo.getPort());\n    String nodeHttpAddress = StringInterner.weakIntern(taInfo.getHostname() + \":\"\n        + taInfo.getHttpPort());\n    // Resource/Priority/Tokens are only needed while launching the container on\n    // an NM, these are already completed tasks, so setting them to null\n    container =\n        Container.newInstance(containerId, containerNodeId,\n          nodeHttpAddress, null, null, null);\n    computeRackAndLocality();\n    launchTime = taInfo.getStartTime();\n    finishTime = (taInfo.getFinishTime() != -1) ?\n        taInfo.getFinishTime() : clock.getTime();\n    shufflePort = taInfo.getShufflePort();\n    trackerName = taInfo.getHostname();\n    httpPort = taInfo.getHttpPort();\n    sendLaunchedEvents();\n\n    reportedStatus.id = attemptId;\n    reportedStatus.progress = 1.0f;\n    reportedStatus.counters = taInfo.getCounters();\n    reportedStatus.stateString = taInfo.getState();\n    reportedStatus.phase = Phase.CLEANUP;\n    reportedStatus.mapFinishTime = taInfo.getMapFinishTime();\n    reportedStatus.shuffleFinishTime = taInfo.getShuffleFinishTime();\n    reportedStatus.sortFinishTime = taInfo.getSortFinishTime();\n    addDiagnosticInfo(taInfo.getError());\n\n    boolean needToClean = false;\n    String recoveredState = taInfo.getTaskStatus();\n    if (recoverOutput\n        && TaskAttemptState.SUCCEEDED.toString().equals(recoveredState)) {\n      TaskAttemptContext tac = new TaskAttemptContextImpl(conf,\n          TypeConverter.fromYarn(attemptId));\n      try {\n        committer.recoverTask(tac);\n        LOG.info(\"Recovered output from task attempt \" + attemptId);\n      } catch (Exception e) {\n        LOG.error(\"Unable to recover task attempt \" + attemptId, e);\n        LOG.info(\"Task attempt \" + attemptId + \" will be recovered as KILLED\");\n        recoveredState = TaskAttemptState.KILLED.toString();\n        needToClean = true;\n      }\n    }\n\n    TaskAttemptStateInternal attemptState;\n    if (TaskAttemptState.SUCCEEDED.toString().equals(recoveredState)) {\n      attemptState = TaskAttemptStateInternal.SUCCEEDED;\n      reportedStatus.taskState = TaskAttemptState.SUCCEEDED;\n      eventHandler.handle(createJobCounterUpdateEventTASucceeded(this));\n      logAttemptFinishedEvent(attemptState);\n    } else if (TaskAttemptState.FAILED.toString().equals(recoveredState)) {\n      attemptState = TaskAttemptStateInternal.FAILED;\n      reportedStatus.taskState = TaskAttemptState.FAILED;\n      eventHandler.handle(createJobCounterUpdateEventTAFailed(this, false));\n      TaskAttemptUnsuccessfulCompletionEvent tauce =\n          createTaskAttemptUnsuccessfulCompletionEvent(this,\n              TaskAttemptStateInternal.FAILED);\n      eventHandler.handle(\n          new JobHistoryEvent(attemptId.getTaskId().getJobId(), tauce));\n    } else {\n      if (!TaskAttemptState.KILLED.toString().equals(recoveredState)) {\n        if (String.valueOf(recoveredState).isEmpty()) {\n          LOG.info(\"TaskAttempt\" + attemptId\n              + \" had not completed, recovering as KILLED\");\n        } else {\n          LOG.warn(\"TaskAttempt \" + attemptId + \" found in unexpected state \"\n              + recoveredState + \", recovering as KILLED\");\n        }\n        addDiagnosticInfo(\"Killed during application recovery\");\n        needToClean = true;\n      }\n      attemptState = TaskAttemptStateInternal.KILLED;\n      reportedStatus.taskState = TaskAttemptState.KILLED;\n      eventHandler.handle(createJobCounterUpdateEventTAKilled(this, false));\n      TaskAttemptUnsuccessfulCompletionEvent tauce =\n          createTaskAttemptUnsuccessfulCompletionEvent(this,\n              TaskAttemptStateInternal.KILLED);\n      eventHandler.handle(\n          new JobHistoryEvent(attemptId.getTaskId().getJobId(), tauce));\n    }\n\n    if (needToClean) {\n      TaskAttemptContext tac = new TaskAttemptContextImpl(conf,\n          TypeConverter.fromYarn(attemptId));\n      try {\n        committer.abortTask(tac);\n      } catch (Exception e) {\n        LOG.warn(\"Task cleanup failed for attempt \" + attemptId, e);\n      }\n    }\n\n    return attemptState;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createContainerLaunchContext": "  static ContainerLaunchContext createContainerLaunchContext(\n      Map<ApplicationAccessType, String> applicationACLs,\n      Configuration conf, Token<JobTokenIdentifier> jobToken, Task remoteTask,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      WrappedJvmID jvmID,\n      TaskAttemptListener taskAttemptListener,\n      Credentials credentials) {\n\n    synchronized (commonContainerSpecLock) {\n      if (commonContainerSpec == null) {\n        commonContainerSpec = createCommonContainerLaunchContext(\n            applicationACLs, conf, jobToken, oldJobId, credentials);\n      }\n    }\n\n    // Fill in the fields needed per-container that are missing in the common\n    // spec.\n\n    boolean userClassesTakesPrecedence =\n      conf.getBoolean(MRJobConfig.MAPREDUCE_JOB_USER_CLASSPATH_FIRST, false);\n\n    // Setup environment by cloning from common env.\n    Map<String, String> env = commonContainerSpec.getEnvironment();\n    Map<String, String> myEnv = new HashMap<String, String>(env.size());\n    myEnv.putAll(env);\n    if (userClassesTakesPrecedence) {\n      myEnv.put(Environment.CLASSPATH_PREPEND_DISTCACHE.name(), \"true\");\n    }\n    MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n\n    // Set up the launch command\n    List<String> commands = MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, jvmID);\n\n    // Duplicate the ByteBuffers for access by multiple containers.\n    Map<String, ByteBuffer> myServiceData = new HashMap<String, ByteBuffer>();\n    for (Entry<String, ByteBuffer> entry : commonContainerSpec\n                .getServiceData().entrySet()) {\n      myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n    }\n\n    // Construct the actual Container\n    ContainerLaunchContext container = ContainerLaunchContext.newInstance(\n        commonContainerSpec.getLocalResources(), myEnv, commands,\n        myServiceData, commonContainerSpec.getTokens().duplicate(),\n        applicationACLs);\n\n    return container;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendContainerCompleted": "  private static void sendContainerCompleted(TaskAttemptImpl taskAttempt) {\n    taskAttempt.eventHandler.handle(new ContainerLauncherEvent(\n        taskAttempt.attemptId,\n        taskAttempt.container.getId(), StringInterner\n        .weakIntern(taskAttempt.container.getNodeId().toString()),\n        taskAttempt.container.getContainerToken(),\n        ContainerLauncher.EventType.CONTAINER_COMPLETED));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendLaunchedEvents": "  private void sendLaunchedEvents() {\n    JobCounterUpdateEvent jce = new JobCounterUpdateEvent(attemptId.getTaskId()\n        .getJobId());\n    jce.addCounterUpdate(attemptId.getTaskId().getTaskType() == TaskType.MAP ?\n        JobCounter.TOTAL_LAUNCHED_MAPS : JobCounter.TOTAL_LAUNCHED_REDUCES, 1);\n    eventHandler.handle(jce);\n\n    LOG.info(\"TaskAttempt: [\" + attemptId\n        + \"] using containerId: [\" + container.getId() + \" on NM: [\"\n        + StringInterner.weakIntern(container.getNodeId().toString()) + \"]\");\n    TaskAttemptStartedEvent tase =\n      new TaskAttemptStartedEvent(TypeConverter.fromYarn(attemptId),\n          TypeConverter.fromYarn(attemptId.getTaskId().getTaskType()),\n          launchTime, trackerName, httpPort, shufflePort, container.getId(),\n          locality.toString(), avataar.toString());\n    eventHandler.handle(\n        new JobHistoryEvent(attemptId.getTaskId().getJobId(), tase));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createJobCounterUpdateEventTAKilled": "  private static JobCounterUpdateEvent createJobCounterUpdateEventTAKilled(\n      TaskAttemptImpl taskAttempt, boolean taskAlreadyCompleted) {\n    TaskType taskType = taskAttempt.getID().getTaskId().getTaskType();\n    JobCounterUpdateEvent jce = new JobCounterUpdateEvent(taskAttempt.getID().getTaskId().getJobId());\n    \n    if (taskType == TaskType.MAP) {\n      jce.addCounterUpdate(JobCounter.NUM_KILLED_MAPS, 1);\n    } else {\n      jce.addCounterUpdate(JobCounter.NUM_KILLED_REDUCES, 1);\n    }\n    if (!taskAlreadyCompleted) {\n      updateMillisCounters(jce, taskAttempt);\n    }\n    return jce;\n  }  ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getState": "  public TaskAttemptState getState() {\n    readLock.lock();\n    try {\n      return getExternalState(stateMachine.getCurrentState());\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.computeRackAndLocality": "  private void computeRackAndLocality() {\n    NodeId containerNodeId = container.getNodeId();\n    nodeRackName = RackResolver.resolve(\n        containerNodeId.getHost()).getNetworkLocation();\n\n    locality = Locality.OFF_SWITCH;\n    if (dataLocalHosts.size() > 0) {\n      String cHost = resolveHost(containerNodeId.getHost());\n      if (dataLocalHosts.contains(cHost)) {\n        locality = Locality.NODE_LOCAL;\n      }\n    }\n    if (locality == Locality.OFF_SWITCH) {\n      if (dataLocalRacks.contains(nodeRackName)) {\n        locality = Locality.RACK_LOCAL;\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle": "  public void handle(TaskAttemptEvent event) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Processing \" + event.getTaskAttemptID() + \" of type \"\n          + event.getType());\n    }\n    writeLock.lock();\n    try {\n      final TaskAttemptStateInternal oldState = getInternalState()  ;\n      try {\n        stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitionException e) {\n        LOG.error(\"Can't handle this event at current state for \"\n            + this.attemptId, e);\n        eventHandler.handle(new JobDiagnosticsUpdateEvent(\n            this.attemptId.getTaskId().getJobId(), \"Invalid event \" + event.getType() + \n            \" on TaskAttempt \" + this.attemptId));\n        eventHandler.handle(new JobEvent(this.attemptId.getTaskId().getJobId(),\n            JobEventType.INTERNAL_ERROR));\n      }\n      if (oldState != getInternalState()) {\n        if (getInternalState() == TaskAttemptStateInternal.FAILED) {\n          String nodeId = null == this.container ? \"Not-assigned\"\n              : this.container.getNodeId().toString();\n          LOG.info(attemptId + \" transitioned from state \" + oldState + \" to \"\n              + getInternalState() + \", event type is \" + event.getType()\n              + \" and nodeId=\" + nodeId);\n        } else {\n          LOG.info(attemptId + \" TaskAttempt Transitioned from \" + oldState\n              + \" to \" + getInternalState());\n        }\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getNodeId": "  public NodeId getNodeId() {\n    readLock.lock();\n    try {\n      return container == null ? null : container.getNodeId();\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getInternalState": "  public TaskAttemptStateInternal getInternalState() {\n    readLock.lock();\n    try {\n      return stateMachine.getCurrentState();\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.handle": "    public void handle(Event event) {\n      //Empty\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJob": "    public Job getJob(JobId jobID) {\n      return jobs.get(jobID);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob": "  public void shutDownJob() {\n    // job has finished\n    // this is the only job, so shut down the Appmaster\n    // note in a workflow scenario, this may lead to creation of a new\n    // job (FIXME?)\n\n    JobEndNotifier notifier = null;\n    if (getConfig().get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL) != null) {\n      notifier = new JobEndNotifier();\n      notifier.setConf(getConfig());\n    }\n\n    try {\n      //if isLastAMRetry comes as true, should never set it to false\n      if ( !isLastAMRetry){\n        if (((JobImpl)job).getInternalState() != JobStateInternal.REBOOT) {\n          LOG.info(\"Job finished cleanly, recording last MRAppMaster retry\");\n          isLastAMRetry = true;\n        }\n      }\n      notifyIsLastAMRetry(isLastAMRetry);\n      // Stop all services\n      // This will also send the final report to the ResourceManager\n      LOG.info(\"Calling stop for all the services\");\n      MRAppMaster.this.stop();\n\n      if (isLastAMRetry && notifier != null) {\n        // Send job-end notification when it is safe to report termination to\n        // users and it is the last AM retry\n        sendJobEndNotify(notifier);\n        notifier = null;\n      }\n\n      try {\n        Thread.sleep(5000);\n      } catch (InterruptedException e) {\n        e.printStackTrace();\n      }\n      clientService.stop();\n    } catch (Throwable t) {\n      LOG.warn(\"Graceful stop failed. Exiting.. \", t);\n      exitMRAppMaster(1, t);\n    } finally {\n      if (isLastAMRetry && notifier != null) {\n        sendJobEndNotify(notifier);\n      }\n    }\n    exitMRAppMaster(0, null);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.callWithJobClassLoader": "  <T> T callWithJobClassLoader(Configuration conf, ExceptionAction<T> action)\n      throws IOException {\n    // if the job classloader is enabled, we may need it to load the (custom)\n    // classes; we make the job classloader available and unset it once it is\n    // done\n    ClassLoader currentClassLoader = conf.getClassLoader();\n    boolean setJobClassLoader =\n        jobClassLoader != null && currentClassLoader != jobClassLoader;\n    if (setJobClassLoader) {\n      MRApps.setClassLoader(jobClassLoader, conf);\n    }\n    try {\n      return action.call(conf);\n    } catch (IOException e) {\n      throw e;\n    } catch (YarnRuntimeException e) {\n      throw e;\n    } catch (Exception e) {\n      // wrap it with a YarnRuntimeException\n      throw new YarnRuntimeException(e);\n    } finally {\n      if (setJobClassLoader) {\n        // restore the original classloader\n        MRApps.setClassLoader(currentClassLoader, conf);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJobId": "  public JobId getJobId() {\n    return jobId;\n  }"
        },
        "bug_report": {
            "Title": "Concurrent task progress updates causing NPE in Application Master",
            "Description": "Concurrent task progress updates can cause a NullPointerException in the Application Master (stack trace is with code at current trunk):\r\n\r\n{quote}\r\n2017-12-20 06:49:42,369 INFO [IPC Server handler 9 on 39501] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1513780867907_0001_m_000002_0 is : 0.02677883\r\n2017-12-20 06:49:42,369 INFO [IPC Server handler 13 on 39501] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1513780867907_0001_m_000002_0 is : 0.02677883\r\n2017-12-20 06:49:42,383 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread\r\njava.lang.NullPointerException\r\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2450)\r\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2433)\r\n        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)\r\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\r\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)\r\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)\r\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1362)\r\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:154)\r\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1543)\r\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1535)\r\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)\r\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n2017-12-20 06:49:42,385 INFO [IPC Server handler 13 on 39501] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1513780867907_0001_m_000002_0 is : 0.02677883\r\n2017-12-20 06:49:42,386 INFO [AsyncDispatcher ShutDown handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye..\r\n{quote}\r\n\r\nThis happened naturally in several big wordcount runs, and I could reproduce this reliably by artificially making task updates more frequent."
        }
    },
    {
        "filename": "MAPREDUCE-3790.json",
        "creation_time": "2012-02-02T18:04:23.000+0000",
        "stack_trace": "```\nError: java.io.IOException: Broken pipe\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:282)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:90)\n\tat org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)\n\tat org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)\n\tat org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:394)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:329)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)\n```",
        "source_code": {
            "hadoop-tools.hadoop-streaming.src.main.java.org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8": "  private void writeUTF8(Object object) throws IOException {\n    byte[] bval;\n    int valSize;\n    if (object instanceof BytesWritable) {\n      BytesWritable val = (BytesWritable) object;\n      bval = val.getBytes();\n      valSize = val.getLength();\n    } else if (object instanceof Text) {\n      Text val = (Text) object;\n      bval = val.getBytes();\n      valSize = val.getLength();\n    } else {\n      String sval = object.toString();\n      bval = sval.getBytes(\"UTF-8\");\n      valSize = bval.length;\n    }\n    clientOut.write(bval, 0, valSize);\n  }",
            "hadoop-tools.hadoop-streaming.src.main.java.org.apache.hadoop.streaming.io.TextInputWriter.writeValue": "  public void writeValue(Object value) throws IOException {\n    writeUTF8(value);\n    clientOut.write('\\n');\n  }",
            "hadoop-tools.hadoop-streaming.src.main.java.org.apache.hadoop.streaming.PipeMapper.map": "  public void map(Object key, Object value, OutputCollector output, Reporter reporter) throws IOException {\n    if (outerrThreadsThrowable != null) {\n      mapRedFinished();\n      throw new IOException(\"MROutput/MRErrThread failed:\",\n          outerrThreadsThrowable);\n    }\n    try {\n      // 1/4 Hadoop in\n      numRecRead_++;\n      maybeLogRecord();\n\n      // 2/4 Hadoop to Tool\n      if (numExceptions_ == 0) {\n        if (!this.ignoreKey) {\n          inWriter_.writeKey(key);\n        }\n        inWriter_.writeValue(value);\n        if(skipping) {\n          //flush the streams on every record input if running in skip mode\n          //so that we don't buffer other records surrounding a bad record. \n          clientOut_.flush();\n        }\n      } else {\n        numRecSkipped_++;\n      }\n    } catch (IOException io) {\n      numExceptions_++;\n      if (numExceptions_ > 1 || numRecWritten_ < minRecWrittenToEnableSkip_) {\n        // terminate with failure\n        LOG.info(getContext() , io);\n        mapRedFinished();\n        throw io;\n      } else {\n        // terminate with success:\n        // swallow input records although the stream processor failed/closed\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapRunner.run": "  public void run(RecordReader<K1, V1> input, OutputCollector<K2, V2> output,\n                  Reporter reporter)\n    throws IOException {\n    try {\n      // allocate key & value instances that are re-used for all entries\n      K1 key = input.createKey();\n      V1 value = input.createValue();\n      \n      while (input.next(key, value)) {\n        // map pair to output\n        mapper.map(key, value, output, reporter);\n        if(incrProcCount) {\n          reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n              SkipBadRecords.COUNTER_MAP_PROCESSED_RECORDS, 1);\n        }\n      }\n    } finally {\n      mapper.close();\n    }\n  }",
            "hadoop-tools.hadoop-streaming.src.main.java.org.apache.hadoop.streaming.PipeMapRunner.run": "  public void run(RecordReader<K1, V1> input, OutputCollector<K2, V2> output,\n                  Reporter reporter)\n         throws IOException {\n    PipeMapper pipeMapper = (PipeMapper)getMapper();\n    pipeMapper.startOutputThreads(output, reporter);\n    super.run(input, output, reporter);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.runOldMapper": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runOldMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, InterruptedException,\n                             ClassNotFoundException {\n    InputSplit inputSplit = getSplitDetails(new Path(splitIndex.getSplitLocation()),\n           splitIndex.getStartOffset());\n\n    updateJobWithSplit(job, inputSplit);\n    reporter.setInputSplit(inputSplit);\n\n    RecordReader<INKEY,INVALUE> in = isSkipping() ? \n        new SkippingRecordReader<INKEY,INVALUE>(umbilical, reporter, job) :\n          new TrackedRecordReader<INKEY,INVALUE>(reporter, job);\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n\n\n    int numReduceTasks = conf.getNumReduceTasks();\n    LOG.info(\"numReduceTasks: \" + numReduceTasks);\n    MapOutputCollector collector = null;\n    if (numReduceTasks > 0) {\n      collector = new MapOutputBuffer(umbilical, job, reporter);\n    } else { \n      collector = new DirectMapOutputCollector(umbilical, job, reporter);\n    }\n    MapRunnable<INKEY,INVALUE,OUTKEY,OUTVALUE> runner =\n      ReflectionUtils.newInstance(job.getMapRunnerClass(), job);\n\n    try {\n      runner.run(in, new OldOutputCollector(collector, conf), reporter);\n      mapPhase.complete();\n      // start the sort phase only if there are reducers\n      if (numReduceTasks > 0) {\n        setPhase(TaskStatus.Phase.SORT);\n      }\n      statusUpdate(umbilical);\n      collector.flush();\n    } finally {\n      //close\n      in.close();                               // close input\n      collector.close();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.run": "      public void run() {\n        spillLock.lock();\n        spillThreadRunning = true;\n        try {\n          while (true) {\n            spillDone.signal();\n            while (!spillInProgress) {\n              spillReady.await();\n            }\n            try {\n              spillLock.unlock();\n              sortAndSpill();\n            } catch (Throwable t) {\n              sortSpillException = t;\n            } finally {\n              spillLock.lock();\n              if (bufend < bufstart) {\n                bufvoid = kvbuffer.length;\n              }\n              kvstart = kvend;\n              bufstart = bufend;\n              spillInProgress = false;\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        } finally {\n          spillLock.unlock();\n          spillThreadRunning = false;\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.updateJobWithSplit": "  private void updateJobWithSplit(final JobConf job, InputSplit inputSplit) {\n    if (inputSplit instanceof FileSplit) {\n      FileSplit fileSplit = (FileSplit) inputSplit;\n      job.set(JobContext.MAP_INPUT_FILE, fileSplit.getPath().toString());\n      job.setLong(JobContext.MAP_INPUT_START, fileSplit.getStart());\n      job.setLong(JobContext.MAP_INPUT_PATH, fileSplit.getLength());\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.close": "      public void close() { }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.flush": "    public void flush() throws IOException, ClassNotFoundException,\n           InterruptedException {\n      LOG.info(\"Starting flush of map output\");\n      spillLock.lock();\n      try {\n        while (spillInProgress) {\n          reporter.progress();\n          spillDone.await();\n        }\n        checkSpillException();\n\n        final int kvbend = 4 * kvend;\n        if ((kvbend + METASIZE) % kvbuffer.length !=\n            equator - (equator % METASIZE)) {\n          // spill finished\n          resetSpill();\n        }\n        if (kvindex != kvend) {\n          kvend = (kvindex + NMETA) % kvmeta.capacity();\n          bufend = bufmark;\n          if (LOG.isInfoEnabled()) {\n            LOG.info(\"Spilling map output\");\n            LOG.info(\"bufstart = \" + bufstart + \"; bufend = \" + bufmark +\n                     \"; bufvoid = \" + bufvoid);\n            LOG.info(\"kvstart = \" + kvstart + \"(\" + (kvstart * 4) +\n                     \"); kvend = \" + kvend + \"(\" + (kvend * 4) +\n                     \"); length = \" + (distanceTo(kvend, kvstart,\n                           kvmeta.capacity()) + 1) + \"/\" + maxRec);\n          }\n          sortAndSpill();\n        }\n      } catch (InterruptedException e) {\n        throw new IOException(\"Interrupted while waiting for the writer\", e);\n      } finally {\n        spillLock.unlock();\n      }\n      assert !spillLock.isHeldByCurrentThread();\n      // shut down spill thread and wait for it to exit. Since the preceding\n      // ensures that it is finished with its work (and sortAndSpill did not\n      // throw), we elect to use an interrupt instead of setting a flag.\n      // Spilling simultaneously from this thread while the spill thread\n      // finishes its work might be both a useful way to extend this and also\n      // sufficient motivation for the latter approach.\n      try {\n        spillThread.interrupt();\n        spillThread.join();\n      } catch (InterruptedException e) {\n        throw new IOException(\"Spill failed\", e);\n      }\n      // release sort buffer before the merge\n      kvbuffer = null;\n      mergeParts();\n      Path outputPath = mapOutputFile.getOutputFile();\n      fileOutputByteCounter.increment(rfs.getFileStatus(outputPath).getLen());\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getSplitDetails": " private <T> T getSplitDetails(Path file, long offset) \n  throws IOException {\n   FileSystem fs = file.getFileSystem(conf);\n   FSDataInputStream inFile = fs.open(file);\n   inFile.seek(offset);\n   String className = Text.readString(inFile);\n   Class<T> cls;\n   try {\n     cls = (Class<T>) conf.getClassByName(className);\n   } catch (ClassNotFoundException ce) {\n     IOException wrap = new IOException(\"Split class \" + className + \n                                         \" not found\");\n     wrap.initCause(ce);\n     throw wrap;\n   }\n   SerializationFactory factory = new SerializationFactory(conf);\n   Deserializer<T> deserializer = \n     (Deserializer<T>) factory.getDeserializer(cls);\n   deserializer.open(inFile);\n   T split = deserializer.deserialize(null);\n   long pos = inFile.getPos();\n   getCounters().findCounter(\n       TaskCounter.SPLIT_RAW_BYTES).increment(pos - offset);\n   inFile.close();\n   return split;\n }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.run": "              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      LOG.error(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(pae,\"Unknown exception in doAs\");\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.main": "  public static void main(String[] args) throws Throwable {\n    LOG.debug(\"Child starting\");\n\n    final JobConf defaultConf = new JobConf();\n    defaultConf.addResource(MRJobConfig.JOB_CONF_FILE);\n    UserGroupInformation.setConfiguration(defaultConf);\n\n    String host = args[0];\n    int port = Integer.parseInt(args[1]);\n    final InetSocketAddress address = new InetSocketAddress(host, port);\n    final TaskAttemptID firstTaskid = TaskAttemptID.forName(args[2]);\n    int jvmIdInt = Integer.parseInt(args[3]);\n    JVMId jvmId = new JVMId(firstTaskid.getJobID(),\n        firstTaskid.getTaskType() == TaskType.MAP, jvmIdInt);\n\n    // initialize metrics\n    DefaultMetricsSystem.initialize(\n        StringUtils.camelize(firstTaskid.getTaskType().name()) +\"Task\");\n\n    Token<JobTokenIdentifier> jt = loadCredentials(defaultConf, address);\n\n    // Create TaskUmbilicalProtocol as actual task owner.\n    UserGroupInformation taskOwner =\n      UserGroupInformation.createRemoteUser(firstTaskid.getJobID().toString());\n    taskOwner.addToken(jt);\n    final TaskUmbilicalProtocol umbilical =\n      taskOwner.doAs(new PrivilegedExceptionAction<TaskUmbilicalProtocol>() {\n      @Override\n      public TaskUmbilicalProtocol run() throws Exception {\n        return (TaskUmbilicalProtocol)RPC.getProxy(TaskUmbilicalProtocol.class,\n            TaskUmbilicalProtocol.versionID, address, defaultConf);\n      }\n    });\n\n    // report non-pid to application master\n    JvmContext context = new JvmContext(jvmId, \"-1000\");\n    LOG.debug(\"PID: \" + System.getenv().get(\"JVM_PID\"));\n    Task task = null;\n    UserGroupInformation childUGI = null;\n\n    try {\n      int idleLoopCount = 0;\n      JvmTask myTask = null;;\n      // poll for new task\n      for (int idle = 0; null == myTask; ++idle) {\n        long sleepTimeMilliSecs = Math.min(idle * 500, 1500);\n        LOG.info(\"Sleeping for \" + sleepTimeMilliSecs\n            + \"ms before retrying again. Got null now.\");\n        MILLISECONDS.sleep(sleepTimeMilliSecs);\n        myTask = umbilical.getTask(context);\n      }\n      if (myTask.shouldDie()) {\n        return;\n      }\n\n      task = myTask.getTask();\n      YarnChild.taskid = task.getTaskID();\n\n      // Create the job-conf and set credentials\n      final JobConf job =\n        configureTask(task, defaultConf.getCredentials(), jt);\n\n      // Initiate Java VM metrics\n      JvmMetrics.initSingleton(jvmId.toString(), job.getSessionId());\n      childUGI = UserGroupInformation.createRemoteUser(System\n          .getenv(ApplicationConstants.Environment.USER.toString()));\n      // Add tokens to new user so that it may execute its task correctly.\n      for(Token<?> token : UserGroupInformation.getCurrentUser().getTokens()) {\n        childUGI.addToken(token);\n      }\n\n      // Create a final reference to the task for the doAs block\n      final Task taskFinal = task;\n      childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n        @Override\n        public Object run() throws Exception {\n          // use job-specified working directory\n          FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory());\n          taskFinal.run(job, umbilical); // run the task\n          return null;\n        }\n      });\n    } catch (FSError e) {\n      LOG.fatal(\"FSError from child\", e);\n      umbilical.fsError(taskid, e.getMessage());\n    } catch (Exception exception) {\n      LOG.warn(\"Exception running child : \"\n          + StringUtils.stringifyException(exception));\n      try {\n        if (task != null) {\n          // do cleanup for the task\n          if (childUGI == null) { // no need to job into doAs block\n            task.taskCleanup(umbilical);\n          } else {\n            final Task taskFinal = task;\n            childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n              @Override\n              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }\n            });\n          }\n        }\n      } catch (Exception e) {\n        LOG.info(\"Exception cleaning up: \" + StringUtils.stringifyException(e));\n      }\n      // Report back any failures, for diagnostic purposes\n      ByteArrayOutputStream baos = new ByteArrayOutputStream();\n      exception.printStackTrace(new PrintStream(baos));\n      if (taskid != null) {\n        umbilical.fatalError(taskid, baos.toString());\n      }\n    } catch (Throwable throwable) {\n      LOG.fatal(\"Error running child : \"\n    \t        + StringUtils.stringifyException(throwable));\n      if (taskid != null) {\n        Throwable tCause = throwable.getCause();\n        String cause = tCause == null\n                                 ? throwable.getMessage()\n                                 : StringUtils.stringifyException(tCause);\n        umbilical.fatalError(taskid, cause);\n      }\n    } finally {\n      RPC.stopProxy(umbilical);\n      DefaultMetricsSystem.shutdown();\n      // Shutting down log4j of the child-vm...\n      // This assumes that on return from Task.run()\n      // there is no more logging done.\n      LogManager.shutdown();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.configureTask": "  private static JobConf configureTask(Task task, Credentials credentials,\n      Token<JobTokenIdentifier> jt) throws IOException {\n    final JobConf job = new JobConf(MRJobConfig.JOB_CONF_FILE);\n    job.setCredentials(credentials);\n    \n    String appAttemptIdEnv = System\n        .getenv(MRJobConfig.APPLICATION_ATTEMPT_ID_ENV);\n    LOG.debug(\"APPLICATION_ATTEMPT_ID: \" + appAttemptIdEnv);\n    // Set it in conf, so as to be able to be used the the OutputCommitter.\n    job.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, Integer\n        .parseInt(appAttemptIdEnv));\n\n    // set tcp nodelay\n    job.setBoolean(\"ipc.client.tcpnodelay\", true);\n    job.setClass(MRConfig.TASK_LOCAL_OUTPUT_CLASS,\n        YarnOutputFiles.class, MapOutputFile.class);\n    // set the jobTokenFile into task\n    task.setJobTokenSecret(\n        JobTokenSecretManager.createSecretKey(jt.getPassword()));\n\n    // setup the child's MRConfig.LOCAL_DIR.\n    configureLocalDirs(task, job);\n\n    // setup the child's attempt directories\n    // Do the task-type specific localization\n    task.localizeConfiguration(job);\n\n    // Set up the DistributedCache related configs\n    setupDistributedCacheConfig(job);\n\n    // Overwrite the localized task jobconf which is linked to in the current\n    // work-dir.\n    Path localTaskFile = new Path(MRJobConfig.JOB_CONF_FILE);\n    writeLocalJobFile(localTaskFile, job);\n    task.setJobFile(localTaskFile.toString());\n    task.setConf(job);\n    return job;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.loadCredentials": "  private static Token<JobTokenIdentifier> loadCredentials(JobConf conf,\n      InetSocketAddress address) throws IOException {\n    //load token cache storage\n    String tokenFileLocation =\n        System.getenv(ApplicationConstants.CONTAINER_TOKEN_FILE_ENV_NAME);\n    String jobTokenFile =\n        new Path(tokenFileLocation).makeQualified(FileSystem.getLocal(conf))\n            .toUri().getPath();\n    Credentials credentials =\n      TokenCache.loadTokens(jobTokenFile, conf);\n    LOG.debug(\"loading token. # keys =\" +credentials.numberOfSecretKeys() +\n        \"; from file=\" + jobTokenFile);\n    Token<JobTokenIdentifier> jt = TokenCache.getJobToken(credentials);\n    jt.setService(new Text(address.getAddress().getHostAddress() + \":\"\n        + address.getPort()));\n    UserGroupInformation current = UserGroupInformation.getCurrentUser();\n    current.addToken(jt);\n    for (Token<? extends TokenIdentifier> tok : credentials.getAllTokens()) {\n      current.addToken(tok);\n    }\n    // Set the credentials\n    conf.setCredentials(credentials);\n    return jt;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.reporter.incrCounter": "  public abstract void incrCounter(String group, String counter, long amount);\n  \n  /**\n   * Get the {@link InputSplit} object for a map.",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.mapper.map": "  void map(K1 key, V1 value, OutputCollector<K2, V2> output, Reporter reporter)\n  throws IOException;\n}",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.taskCleanup": "  void taskCleanup(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    // set phase for this task\n    setPhase(TaskStatus.Phase.CLEANUP);\n    getProgress().setStatus(\"cleanup\");\n    statusUpdate(umbilical);\n    LOG.info(\"Runnning cleanup for the task\");\n    // do the cleanup\n    committer.abortTask(taskContext);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.statusUpdate": "  public void statusUpdate(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    int retries = MAX_RETRIES;\n    while (true) {\n      try {\n        if (!umbilical.statusUpdate(getTaskID(), taskStatus)) {\n          LOG.warn(\"Parent died.  Exiting \"+taskId);\n          System.exit(66);\n        }\n        taskStatus.clearStatus();\n        return;\n      } catch (InterruptedException ie) {\n        Thread.currentThread().interrupt(); // interrupt ourself\n      } catch (IOException ie) {\n        LOG.warn(\"Failure sending status update: \" + \n                  StringUtils.stringifyException(ie));\n        if (--retries == 0) {\n          throw ie;\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setPhase": "  protected synchronized void setPhase(TaskStatus.Phase phase){\n    this.taskStatus.setPhase(phase); \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setStatus": "    public void setStatus(String status) {\n      taskProgress.setStatus(status);\n      // indicate that progress update needs to be sent\n      setProgressFlag();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getProgress": "    public float getProgress() {\n      return taskProgress.getProgress();\n    };",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getWorkingDirectory": "  public Path getWorkingDirectory() {\n    String name = get(JobContext.WORKING_DIR);\n    if (name != null) {\n      return new Path(name);\n    } else {\n      try {\n        Path dir = FileSystem.get(this).getWorkingDirectory();\n        set(JobContext.WORKING_DIR, dir.toString());\n        return dir;\n      } catch (IOException e) {\n        throw new RuntimeException(e);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.toString": "  public String toString() { \n    return appendTo(new StringBuilder(JVM)).toString();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.appendTo": "  protected StringBuilder appendTo(StringBuilder builder) {\n    return jobId.appendTo(builder).\n                 append(SEPARATOR).\n                 append(isMap ? 'm' : 'r').\n                 append(SEPARATOR).\n                 append(idFormat.format(id));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getSessionId": "  public String getSessionId() {\n      return get(\"session.id\", \"\");\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fsError": "  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.shouldDie": "  public boolean shouldDie() {\n    return shouldDie;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.getTask": "  public Task getTask() {\n    return t;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getCredentials": "  public Credentials getCredentials() {\n    return credentials;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.getTask": "  JvmTask getTask(JvmContext context) throws IOException;\n  \n  /**\n   * Report child's progress to parent.\n   * \n   * @param taskId task-id of the child\n   * @param taskStatus status of the child\n   * @throws IOException\n   * @throws InterruptedException\n   * @return True if the task is known\n   */\n  boolean statusUpdate(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;\n  \n  /** Report error messages back to parent.  Calls should be sparing, since all\n   *  such messages are held in the job tracker.\n   *  @param taskid the id of the task involved\n   *  @param trace the text to report\n   */\n  void reportDiagnosticInfo(TaskAttemptID taskid, String trace) throws IOException;\n  \n  /**\n   * Report the record range which is going to process next by the Task.\n   * @param taskid the id of the task involved\n   * @param range the range of record sequence nos\n   * @throws IOException\n   */\n  void reportNextRecordRange(TaskAttemptID taskid, SortedRanges.Range range) \n    throws IOException;\n\n  /** Periodically called by child to check if parent is still alive. \n   * @return True if the task is known\n   */\n  boolean ping(TaskAttemptID taskid) throws IOException;\n\n  /** Report that the task is successfully completed.  Failure is assumed if\n   * the task process exits without calling this.\n   * @param taskid task's id\n   */\n  void done(TaskAttemptID taskid) throws IOException;\n  \n  /** \n   * Report that the task is complete, but its commit is pending.\n   * \n   * @param taskId task's id\n   * @param taskStatus status of the child\n   * @throws IOException\n   */\n  void commitPending(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;  \n\n  /**\n   * Polling to know whether the task can go-ahead with commit \n   * @param taskid\n   * @return true/false \n   * @throws IOException\n   */\n  boolean canCommit(TaskAttemptID taskid) throws IOException;\n\n  /** Report that a reduce-task couldn't shuffle map-outputs.*/\n  void shuffleError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Report that the task encounted a local filesystem error.*/\n  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.getJobID": "  public JobID getJobID() {\n    return (JobID) super.getJobID();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fatalError": "  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.forName": "  public static TaskAttemptID forName(String str\n                                      ) throws IllegalArgumentException {\n    return (TaskAttemptID) \n             org.apache.hadoop.mapreduce.TaskAttemptID.forName(str);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getTaskID": "  public TaskAttemptID getTaskID() { return taskId; }"
        },
        "bug_report": {
            "Title": "Broken pipe on streaming job can lead to truncated output for a successful job",
            "Description": "If a streaming job doesn't consume all of its input then the job can be marked successful even though the job's output is truncated.\n\nHere's a simple setup that can exhibit the problem.  Note that the job output will most likely be truncated compared to the same job run with a zero-length input file.\n\n{code}\n$ hdfs dfs -cat in\nfoo\n$ yarn jar ./share/hadoop/tools/lib/hadoop-streaming-0.24.0-SNAPSHOT.jar -Dmapred.map.tasks=1 -Dmapred.reduce.tasks=1 -mapper /bin/env -reducer NONE -input in -output out\n{code}\n\nExamining the map task log shows this:\n\n{code:title=Excerpt from map task stdout log}\n2012-02-02 11:27:25,054 WARN [main] org.apache.hadoop.streaming.PipeMapRed: java.io.IOException: Broken pipe\n2012-02-02 11:27:25,054 INFO [main] org.apache.hadoop.streaming.PipeMapRed: mapRedFinished\n2012-02-02 11:27:25,056 WARN [Thread-12] org.apache.hadoop.streaming.PipeMapRed: java.io.IOException: Bad file descriptor\n2012-02-02 11:27:25,124 INFO [main] org.apache.hadoop.mapred.Task: Task:attempt_1328203555769_0001_m_000000_0 is done. And is in the process of commiting\n2012-02-02 11:27:25,127 WARN [Thread-11] org.apache.hadoop.streaming.PipeMapRed: java.io.IOException: DFSOutputStream is closed\n2012-02-02 11:27:25,199 INFO [main] org.apache.hadoop.mapred.Task: Task attempt_1328203555769_0001_m_000000_0 is allowed to commit now\n2012-02-02 11:27:25,225 INFO [main] org.apache.hadoop.mapred.FileOutputCommitter: Saved output of task 'attempt_1328203555769_0001_m_000000_0' to hdfs://localhost:9000/user/somebody/out/_temporary/1\n2012-02-02 11:27:27,834 INFO [main] org.apache.hadoop.mapred.Task: Task 'attempt_1328203555769_0001_m_000000_0' done.\n{code}\n\nIn PipeMapRed.mapRedFinished() we can see it will eat IOExceptions and return without waiting for the output threads or throwing a runtime exception to fail the job.  Net result is that the DFS streams could be shutdown too early if the output threads are still busy and we could lose job output.\n\nFixing this brings up the bigger question of what *should* happen when a streaming job doesn't consume all of its input.  Should we have grabbed all of the output from the job and still marked it successful or should we have failed the job?  If the former then we need to fix some other places in the code as well, since feeding a much larger input file (e.g.: 600K) to the same sample streaming job results in the job failing with the exception below.  It wouldn't be consistent to fail the job that doesn't consume a lot of input but pass the job that leaves just a few leftovers.\n\n{code}\n2012-02-02 10:29:37,220 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1270)) - Running job: job_1328200108174_0001\n2012-02-02 10:29:44,354 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1291)) - Job job_1328200108174_0001 running in uber mode : false\n2012-02-02 10:29:44,355 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1298)) -  map 0% reduce 0%\n2012-02-02 10:29:46,394 INFO  mapreduce.Job (Job.java:printTaskEvents(1386)) - Task Id : attempt_1328200108174_0001_m_000000_0, Status : FAILED\nError: java.io.IOException: Broken pipe\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:282)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:90)\n\tat org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)\n\tat org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)\n\tat org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:394)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:329)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)\n{code}\n\nAssuming the job returns a successful exit code, I think we should allow the job to complete successfully even though it doesn't consume all of its inputs.  Part of the reasoning is that there's already this comment in PipeMapper.java that implies we desire that behavior:\n\n{code:title=PipeMapper.java}\n        // terminate with success:\n        // swallow input records although the stream processor failed/closed\n{code}"
        }
    },
    {
        "filename": "MAPREDUCE-6357.json",
        "creation_time": "2015-05-05T18:11:38.000+0000",
        "stack_trace": "```\njava.io.IOException: File already exists:wasb://full20150320@bgtstoragefull.blob.core.windows.net/user/hadoop/some/path/block-r-00299.bz2\n       at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1354)\n       at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1195)\n       at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n       at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n       at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786)\n       at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:135)\n       at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.getRecordWriter(MultipleOutputs.java:475)\n       at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.write(MultipleOutputs.java:433)\n       at com.ancestry.bigtree.hadoop.LevelReducer.processValue(LevelReducer.java:91)\n       at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:69)\n       at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:14)\n       at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)\n       at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)\n       at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)\n       at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n       at java.security.AccessController.doPrivileged(Native Method)\n       at javax.security.auth.Subject.doAs(Subject.java:415)\n       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n       at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.create": "  public FSDataOutputStream create(Path f,\n      FsPermission permission,\n      EnumSet<CreateFlag> flags,\n      int bufferSize,\n      short replication,\n      long blockSize,\n      Progressable progress,\n      ChecksumOpt checksumOpt) throws IOException {\n    // Checksum options are ignored by default. The file systems that\n    // implement checksum need to override this method. The full\n    // support is currently only available in DFS.\n    return create(f, permission, flags.contains(CreateFlag.OVERWRITE), \n        bufferSize, replication, blockSize, progress);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.setPermission": "  public void setPermission(Path p, FsPermission permission\n      ) throws IOException {\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getDefaultReplication": "  public short getDefaultReplication(Path path) {\n    return getDefaultReplication();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getDefaultBlockSize": "  public long getDefaultBlockSize(Path f) {\n    return getDefaultBlockSize();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter": "  public RecordWriter<K, V> getRecordWriter(FileSystem ignored,\n                                                  JobConf job,\n                                                  String name,\n                                                  Progressable progress)\n    throws IOException {\n    boolean isCompressed = getCompressOutput(job);\n    String keyValueSeparator = job.get(\"mapreduce.output.textoutputformat.separator\", \n                                       \"\\t\");\n    if (!isCompressed) {\n      Path file = FileOutputFormat.getTaskOutputPath(job, name);\n      FileSystem fs = file.getFileSystem(job);\n      FSDataOutputStream fileOut = fs.create(file, progress);\n      return new LineRecordWriter<K, V>(fileOut, keyValueSeparator);\n    } else {\n      Class<? extends CompressionCodec> codecClass =\n        getOutputCompressorClass(job, GzipCodec.class);\n      // create the named codec\n      CompressionCodec codec = ReflectionUtils.newInstance(codecClass, job);\n      // build the filename including the extension\n      Path file = \n        FileOutputFormat.getTaskOutputPath(job, \n                                           name + codec.getDefaultExtension());\n      FileSystem fs = file.getFileSystem(job);\n      FSDataOutputStream fileOut = fs.create(file, progress);\n      return new LineRecordWriter<K, V>(new DataOutputStream\n                                        (codec.createOutputStream(fileOut)),\n                                        keyValueSeparator);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.lib.MultipleOutputs.getRecordWriter": "    public RecordWriter<Object, Object> getRecordWriter(\n      FileSystem fs, JobConf job, String baseFileName, Progressable progress)\n      throws IOException {\n\n      String nameOutput = job.get(CONFIG_NAMED_OUTPUT, null);\n      String fileName = getUniqueName(job, baseFileName);\n\n      // The following trick leverages the instantiation of a record writer via\n      // the job conf thus supporting arbitrary output formats.\n      JobConf outputConf = new JobConf(job);\n      outputConf.setOutputFormat(getNamedOutputFormatClass(job, nameOutput));\n      outputConf.setOutputKeyClass(getNamedOutputKeyClass(job, nameOutput));\n      outputConf.setOutputValueClass(getNamedOutputValueClass(job, nameOutput));\n      OutputFormat outputFormat = outputConf.getOutputFormat();\n      return outputFormat.getRecordWriter(fs, outputConf, fileName, progress);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.lib.MultipleOutputs.getNamedOutputKeyClass": "  public static Class<?> getNamedOutputKeyClass(JobConf conf,\n                                                String namedOutput) {\n    checkNamedOutput(conf, namedOutput, false);\n    return conf.getClass(MO_PREFIX + namedOutput + KEY, null,\n\tObject.class);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.lib.MultipleOutputs.getNamedOutputFormatClass": "  public static Class<? extends OutputFormat> getNamedOutputFormatClass(\n    JobConf conf, String namedOutput) {\n    checkNamedOutput(conf, namedOutput, false);\n    return conf.getClass(MO_PREFIX + namedOutput + FORMAT, null,\n      OutputFormat.class);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.lib.MultipleOutputs.getNamedOutputValueClass": "  public static Class<?> getNamedOutputValueClass(JobConf conf,\n                                                  String namedOutput) {\n    checkNamedOutput(conf, namedOutput, false);\n    return conf.getClass(MO_PREFIX + namedOutput + VALUE, null,\n      Object.class);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.lib.MultipleOutputs.write": "    public void write(Object key, Object value) throws IOException {\n      reporter.incrCounter(COUNTERS_GROUP, counterName, 1);\n      writer.write(key, value);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.runNewReducer": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runNewReducer(JobConf job,\n                     final TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator<INKEY> comparator,\n                     Class<INKEY> keyClass,\n                     Class<INVALUE> valueClass\n                     ) throws IOException,InterruptedException, \n                              ClassNotFoundException {\n    // wrap value iterator to report progress.\n    final RawKeyValueIterator rawIter = rIter;\n    rIter = new RawKeyValueIterator() {\n      public void close() throws IOException {\n        rawIter.close();\n      }\n      public DataInputBuffer getKey() throws IOException {\n        return rawIter.getKey();\n      }\n      public Progress getProgress() {\n        return rawIter.getProgress();\n      }\n      public DataInputBuffer getValue() throws IOException {\n        return rawIter.getValue();\n      }\n      public boolean next() throws IOException {\n        boolean ret = rawIter.next();\n        reporter.setProgress(rawIter.getProgress().getProgress());\n        return ret;\n      }\n    };\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,\n          getTaskID(), reporter);\n    // make a reducer\n    org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer =\n      (org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE>)\n        ReflectionUtils.newInstance(taskContext.getReducerClass(), job);\n    org.apache.hadoop.mapreduce.RecordWriter<OUTKEY,OUTVALUE> trackedRW = \n      new NewTrackingRecordWriter<OUTKEY, OUTVALUE>(this, taskContext);\n    job.setBoolean(\"mapred.skip.on\", isSkipping());\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.Reducer.Context \n         reducerContext = createReduceContext(reducer, job, getTaskID(),\n                                               rIter, reduceInputKeyCounter, \n                                               reduceInputValueCounter, \n                                               trackedRW,\n                                               committer,\n                                               reporter, comparator, keyClass,\n                                               valueClass);\n    try {\n      reducer.run(reducerContext);\n    } finally {\n      trackedRW.close(reducerContext);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.newInstance": "         public Writable newInstance() { return new ReduceTask(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.getValue": "      public DataInputBuffer getValue() throws IOException {\n        return rawIter.getValue();\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.close": "      public void close() throws IOException {\n        rawIter.close();\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.getKey": "      public DataInputBuffer getKey() throws IOException {\n        return rawIter.getKey();\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.next": "      public boolean next() throws IOException {\n        boolean ret = rawIter.next();\n        reporter.setProgress(rawIter.getProgress().getProgress());\n        return ret;\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.run": "  public void run(JobConf job, final TaskUmbilicalProtocol umbilical)\n    throws IOException, InterruptedException, ClassNotFoundException {\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n\n    if (isMapOrReduce()) {\n      copyPhase = getProgress().addPhase(\"copy\");\n      sortPhase  = getProgress().addPhase(\"sort\");\n      reducePhase = getProgress().addPhase(\"reduce\");\n    }\n    // start thread that will handle communication with parent\n    TaskReporter reporter = startReporter(umbilical);\n    \n    boolean useNewApi = job.getUseNewReducer();\n    initialize(job, getJobID(), reporter, useNewApi);\n\n    // check if it is a cleanupJobTask\n    if (jobCleanup) {\n      runJobCleanupTask(umbilical, reporter);\n      return;\n    }\n    if (jobSetup) {\n      runJobSetupTask(umbilical, reporter);\n      return;\n    }\n    if (taskCleanup) {\n      runTaskCleanupTask(umbilical, reporter);\n      return;\n    }\n    \n    // Initialize the codec\n    codec = initCodec();\n    RawKeyValueIterator rIter = null;\n    ShuffleConsumerPlugin shuffleConsumerPlugin = null;\n    \n    Class combinerClass = conf.getCombinerClass();\n    CombineOutputCollector combineCollector = \n      (null != combinerClass) ? \n     new CombineOutputCollector(reduceCombineOutputCounter, reporter, conf) : null;\n\n    Class<? extends ShuffleConsumerPlugin> clazz =\n          job.getClass(MRConfig.SHUFFLE_CONSUMER_PLUGIN, Shuffle.class, ShuffleConsumerPlugin.class);\n\t\t\t\t\t\n    shuffleConsumerPlugin = ReflectionUtils.newInstance(clazz, job);\n    LOG.info(\"Using ShuffleConsumerPlugin: \" + shuffleConsumerPlugin);\n\n    ShuffleConsumerPlugin.Context shuffleContext = \n      new ShuffleConsumerPlugin.Context(getTaskID(), job, FileSystem.getLocal(job), umbilical, \n                  super.lDirAlloc, reporter, codec, \n                  combinerClass, combineCollector, \n                  spilledRecordsCounter, reduceCombineInputCounter,\n                  shuffledMapsCounter,\n                  reduceShuffleBytes, failedShuffleCounter,\n                  mergedMapOutputsCounter,\n                  taskStatus, copyPhase, sortPhase, this,\n                  mapOutputFile, localMapFiles);\n    shuffleConsumerPlugin.init(shuffleContext);\n\n    rIter = shuffleConsumerPlugin.run();\n\n    // free up the data structures\n    mapOutputFilesOnDisk.clear();\n    \n    sortPhase.complete();                         // sort is complete\n    setPhase(TaskStatus.Phase.REDUCE); \n    statusUpdate(umbilical);\n    Class keyClass = job.getMapOutputKeyClass();\n    Class valueClass = job.getMapOutputValueClass();\n    RawComparator comparator = job.getOutputValueGroupingComparator();\n\n    if (useNewApi) {\n      runNewReducer(job, umbilical, reporter, rIter, comparator, \n                    keyClass, valueClass);\n    } else {\n      runOldReducer(job, umbilical, reporter, rIter, comparator, \n                    keyClass, valueClass);\n    }\n\n    shuffleConsumerPlugin.close();\n    done(umbilical, reporter);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.getProgress": "      public Progress getProgress() {\n        return rawIter.getProgress();\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.run": "              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.main": "  public static void main(String[] args) throws Throwable {\n    Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\n    LOG.debug(\"Child starting\");\n\n    final JobConf job = new JobConf(MRJobConfig.JOB_CONF_FILE);\n    // Initing with our JobConf allows us to avoid loading confs twice\n    Limits.init(job);\n    UserGroupInformation.setConfiguration(job);\n\n    String host = args[0];\n    int port = Integer.parseInt(args[1]);\n    final InetSocketAddress address =\n        NetUtils.createSocketAddrForHost(host, port);\n    final TaskAttemptID firstTaskid = TaskAttemptID.forName(args[2]);\n    long jvmIdLong = Long.parseLong(args[3]);\n    JVMId jvmId = new JVMId(firstTaskid.getJobID(),\n        firstTaskid.getTaskType() == TaskType.MAP, jvmIdLong);\n\n    // initialize metrics\n    DefaultMetricsSystem.initialize(\n        StringUtils.camelize(firstTaskid.getTaskType().name()) +\"Task\");\n\n    // Security framework already loaded the tokens into current ugi\n    Credentials credentials =\n        UserGroupInformation.getCurrentUser().getCredentials();\n    LOG.info(\"Executing with tokens:\");\n    for (Token<?> token: credentials.getAllTokens()) {\n      LOG.info(token);\n    }\n\n    // Create TaskUmbilicalProtocol as actual task owner.\n    UserGroupInformation taskOwner =\n      UserGroupInformation.createRemoteUser(firstTaskid.getJobID().toString());\n    Token<JobTokenIdentifier> jt = TokenCache.getJobToken(credentials);\n    SecurityUtil.setTokenService(jt, address);\n    taskOwner.addToken(jt);\n    final TaskUmbilicalProtocol umbilical =\n      taskOwner.doAs(new PrivilegedExceptionAction<TaskUmbilicalProtocol>() {\n      @Override\n      public TaskUmbilicalProtocol run() throws Exception {\n        return (TaskUmbilicalProtocol)RPC.getProxy(TaskUmbilicalProtocol.class,\n            TaskUmbilicalProtocol.versionID, address, job);\n      }\n    });\n\n    // report non-pid to application master\n    JvmContext context = new JvmContext(jvmId, \"-1000\");\n    LOG.debug(\"PID: \" + System.getenv().get(\"JVM_PID\"));\n    Task task = null;\n    UserGroupInformation childUGI = null;\n    ScheduledExecutorService logSyncer = null;\n\n    try {\n      int idleLoopCount = 0;\n      JvmTask myTask = null;;\n      // poll for new task\n      for (int idle = 0; null == myTask; ++idle) {\n        long sleepTimeMilliSecs = Math.min(idle * 500, 1500);\n        LOG.info(\"Sleeping for \" + sleepTimeMilliSecs\n            + \"ms before retrying again. Got null now.\");\n        MILLISECONDS.sleep(sleepTimeMilliSecs);\n        myTask = umbilical.getTask(context);\n      }\n      if (myTask.shouldDie()) {\n        return;\n      }\n\n      task = myTask.getTask();\n      YarnChild.taskid = task.getTaskID();\n\n      // Create the job-conf and set credentials\n      configureTask(job, task, credentials, jt);\n\n      // log the system properties\n      String systemPropsToLog = MRApps.getSystemPropertiesToLog(job);\n      if (systemPropsToLog != null) {\n        LOG.info(systemPropsToLog);\n      }\n\n      // Initiate Java VM metrics\n      JvmMetrics.initSingleton(jvmId.toString(), job.getSessionId());\n      childUGI = UserGroupInformation.createRemoteUser(System\n          .getenv(ApplicationConstants.Environment.USER.toString()));\n      // Add tokens to new user so that it may execute its task correctly.\n      childUGI.addCredentials(credentials);\n\n      // set job classloader if configured before invoking the task\n      MRApps.setJobClassLoader(job);\n\n      logSyncer = TaskLog.createLogSyncer();\n\n      // Create a final reference to the task for the doAs block\n      final Task taskFinal = task;\n      childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n        @Override\n        public Object run() throws Exception {\n          // use job-specified working directory\n          FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory());\n          taskFinal.run(job, umbilical); // run the task\n          return null;\n        }\n      });\n    } catch (FSError e) {\n      LOG.fatal(\"FSError from child\", e);\n      if (!ShutdownHookManager.get().isShutdownInProgress()) {\n        umbilical.fsError(taskid, e.getMessage());\n      }\n    } catch (Exception exception) {\n      LOG.warn(\"Exception running child : \"\n          + StringUtils.stringifyException(exception));\n      try {\n        if (task != null) {\n          // do cleanup for the task\n          if (childUGI == null) { // no need to job into doAs block\n            task.taskCleanup(umbilical);\n          } else {\n            final Task taskFinal = task;\n            childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n              @Override\n              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }\n            });\n          }\n        }\n      } catch (Exception e) {\n        LOG.info(\"Exception cleaning up: \" + StringUtils.stringifyException(e));\n      }\n      // Report back any failures, for diagnostic purposes\n      if (taskid != null) {\n        if (!ShutdownHookManager.get().isShutdownInProgress()) {\n          umbilical.fatalError(taskid,\n              StringUtils.stringifyException(exception));\n        }\n      }\n    } catch (Throwable throwable) {\n      LOG.fatal(\"Error running child : \"\n    \t        + StringUtils.stringifyException(throwable));\n      if (taskid != null) {\n        if (!ShutdownHookManager.get().isShutdownInProgress()) {\n          Throwable tCause = throwable.getCause();\n          String cause =\n              tCause == null ? throwable.getMessage() : StringUtils\n                  .stringifyException(tCause);\n          umbilical.fatalError(taskid, cause);\n        }\n      }\n    } finally {\n      RPC.stopProxy(umbilical);\n      DefaultMetricsSystem.shutdown();\n      TaskLog.syncLogsShutdown(logSyncer);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.configureTask": "  private static void configureTask(JobConf job, Task task,\n      Credentials credentials, Token<JobTokenIdentifier> jt) throws IOException {\n    job.setCredentials(credentials);\n    \n    ApplicationAttemptId appAttemptId =\n        ConverterUtils.toContainerId(\n            System.getenv(Environment.CONTAINER_ID.name()))\n            .getApplicationAttemptId();\n    LOG.debug(\"APPLICATION_ATTEMPT_ID: \" + appAttemptId);\n    // Set it in conf, so as to be able to be used the the OutputCommitter.\n    job.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID,\n        appAttemptId.getAttemptId());\n\n    // set tcp nodelay\n    job.setBoolean(\"ipc.client.tcpnodelay\", true);\n    job.setClass(MRConfig.TASK_LOCAL_OUTPUT_CLASS,\n        YarnOutputFiles.class, MapOutputFile.class);\n    // set the jobToken and shuffle secrets into task\n    task.setJobTokenSecret(\n        JobTokenSecretManager.createSecretKey(jt.getPassword()));\n    byte[] shuffleSecret = TokenCache.getShuffleSecretKey(credentials);\n    if (shuffleSecret == null) {\n      LOG.warn(\"Shuffle secret missing from task credentials.\"\n          + \" Using job token secret as shuffle secret.\");\n      shuffleSecret = jt.getPassword();\n    }\n    task.setShuffleSecret(\n        JobTokenSecretManager.createSecretKey(shuffleSecret));\n\n    // setup the child's MRConfig.LOCAL_DIR.\n    configureLocalDirs(task, job);\n\n    // setup the child's attempt directories\n    // Do the task-type specific localization\n    task.localizeConfiguration(job);\n\n    // Set up the DistributedCache related configs\n    MRApps.setupDistributedCacheLocal(job);\n\n    // Overwrite the localized task jobconf which is linked to in the current\n    // work-dir.\n    Path localTaskFile = new Path(MRJobConfig.JOB_CONF_FILE);\n    writeLocalJobFile(localTaskFile, job);\n    task.setJobFile(localTaskFile.toString());\n    task.setConf(job);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.taskCleanup": "  void taskCleanup(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    // set phase for this task\n    setPhase(TaskStatus.Phase.CLEANUP);\n    getProgress().setStatus(\"cleanup\");\n    statusUpdate(umbilical);\n    LOG.info(\"Runnning cleanup for the task\");\n    // do the cleanup\n    committer.abortTask(taskContext);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.statusUpdate": "  public void statusUpdate(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    int retries = MAX_RETRIES;\n    while (true) {\n      try {\n        if (!umbilical.statusUpdate(getTaskID(), taskStatus).getTaskFound()) {\n          LOG.warn(\"Parent died.  Exiting \"+taskId);\n          System.exit(66);\n        }\n        taskStatus.clearStatus();\n        return;\n      } catch (InterruptedException ie) {\n        Thread.currentThread().interrupt(); // interrupt ourself\n      } catch (IOException ie) {\n        LOG.warn(\"Failure sending status update: \" + \n                  StringUtils.stringifyException(ie));\n        if (--retries == 0) {\n          throw ie;\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setPhase": "  protected synchronized void setPhase(TaskStatus.Phase phase){\n    this.taskStatus.setPhase(phase); \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setStatus": "    public void setStatus(String status) {\n      taskProgress.setStatus(normalizeStatus(status, conf));\n      // indicate that progress update needs to be sent\n      setProgressFlag();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getProgress": "    public float getProgress() {\n      return taskProgress.getProgress();\n    };",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getWorkingDirectory": "  public Path getWorkingDirectory() {\n    String name = get(JobContext.WORKING_DIR);\n    if (name != null) {\n      return new Path(name);\n    } else {\n      try {\n        Path dir = FileSystem.get(this).getWorkingDirectory();\n        set(JobContext.WORKING_DIR, dir.toString());\n        return dir;\n      } catch (IOException e) {\n        throw new RuntimeException(e);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.toString": "  public String toString() { \n    return appendTo(new StringBuilder(JVM)).toString();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.appendTo": "  protected StringBuilder appendTo(StringBuilder builder) {\n    return jobId.appendTo(builder).\n                 append(SEPARATOR).\n                 append(isMap ? 'm' : 'r').\n                 append(SEPARATOR).\n                 append(idFormat.format(jvmId));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getSessionId": "  public String getSessionId() {\n      return get(\"session.id\", \"\");\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.createLogSyncer": "  public static ScheduledExecutorService createLogSyncer() {\n    final ScheduledExecutorService scheduler =\n      Executors.newSingleThreadScheduledExecutor(\n        new ThreadFactory() {\n          @Override\n          public Thread newThread(Runnable r) {\n            final Thread t = Executors.defaultThreadFactory().newThread(r);\n            t.setDaemon(true);\n            t.setName(\"Thread for syncLogs\");\n            return t;\n          }\n        });\n    ShutdownHookManager.get().addShutdownHook(new Runnable() {\n        @Override\n        public void run() {\n          TaskLog.syncLogsShutdown(scheduler);\n        }\n      }, 50);\n    scheduler.scheduleWithFixedDelay(\n        new Runnable() {\n          @Override\n          public void run() {\n            TaskLog.syncLogs();\n          }\n        }, 0L, 5L, TimeUnit.SECONDS);\n    return scheduler;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.newThread": "          public Thread newThread(Runnable r) {\n            final Thread t = Executors.defaultThreadFactory().newThread(r);\n            t.setDaemon(true);\n            t.setName(\"Thread for syncLogs\");\n            return t;\n          }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.syncLogsShutdown": "  public static synchronized void syncLogsShutdown(\n    ScheduledExecutorService scheduler) \n  {\n    // flush standard streams\n    //\n    System.out.flush();\n    System.err.flush();\n\n    if (scheduler != null) {\n      scheduler.shutdownNow();\n    }\n\n    // flush & close all appenders\n    LogManager.shutdown(); \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.syncLogs": "  public static synchronized void syncLogs() {\n    // flush standard streams\n    //\n    System.out.flush();\n    System.err.flush();\n\n    // flush flushable appenders\n    //\n    final Logger rootLogger = Logger.getRootLogger();\n    flushAppenders(rootLogger);\n    final Enumeration<Logger> allLoggers = rootLogger.getLoggerRepository().\n      getCurrentLoggers();\n    while (allLoggers.hasMoreElements()) {\n      final Logger l = allLoggers.nextElement();\n      flushAppenders(l);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fsError": "  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.shouldDie": "  public boolean shouldDie() {\n    return shouldDie;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.getTask": "  public Task getTask() {\n    return t;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.getTask": "  JvmTask getTask(JvmContext context) throws IOException;\n  \n  /**\n   * Report child's progress to parent. Also invoked to report still alive (used\n   * to be in ping). It reports an AMFeedback used to propagate preemption requests.\n   * \n   * @param taskId task-id of the child\n   * @param taskStatus status of the child\n   * @throws IOException\n   * @throws InterruptedException\n   * @return True if the task is known\n   */\n  AMFeedback statusUpdate(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;\n  \n  /** Report error messages back to parent.  Calls should be sparing, since all\n   *  such messages are held in the job tracker.\n   *  @param taskid the id of the task involved\n   *  @param trace the text to report\n   */\n  void reportDiagnosticInfo(TaskAttemptID taskid, String trace) throws IOException;\n  \n  /**\n   * Report the record range which is going to process next by the Task.\n   * @param taskid the id of the task involved\n   * @param range the range of record sequence nos\n   * @throws IOException\n   */\n  void reportNextRecordRange(TaskAttemptID taskid, SortedRanges.Range range) \n    throws IOException;\n\n  /** Report that the task is successfully completed.  Failure is assumed if\n   * the task process exits without calling this.\n   * @param taskid task's id\n   */\n  void done(TaskAttemptID taskid) throws IOException;\n  \n  /** \n   * Report that the task is complete, but its commit is pending.\n   * \n   * @param taskId task's id\n   * @param taskStatus status of the child\n   * @throws IOException\n   */\n  void commitPending(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;  \n\n  /**\n   * Polling to know whether the task can go-ahead with commit \n   * @param taskid\n   * @return true/false \n   * @throws IOException\n   */\n  boolean canCommit(TaskAttemptID taskid) throws IOException;\n\n  /** Report that a reduce-task couldn't shuffle map-outputs.*/\n  void shuffleError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Report that the task encounted a local filesystem error.*/\n  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.getJobID": "  public JobID getJobID() {\n    return (JobID) super.getJobID();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fatalError": "  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.forName": "  public static TaskAttemptID forName(String str\n                                      ) throws IllegalArgumentException {\n    return (TaskAttemptID) \n             org.apache.hadoop.mapreduce.TaskAttemptID.forName(str);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getTaskID": "  public TaskAttemptID getTaskID() { return taskId; }"
        },
        "bug_report": {
            "Title": "MultipleOutputs.write() API should document that output committing is not utilized when input path is absolute",
            "Description": "After spending the afternoon debugging a user job where reduce tasks were failing on retry with the below exception, I think it would be worthwhile to add a note in the MultipleOutputs.write() documentation, saying that absolute paths may cause improper execution of tasks on retry or when MR speculative execution is enabled. \n\n{code}\n2015-04-28 23:13:10,452 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: File already exists:wasb://full20150320@bgtstoragefull.blob.core.windows.net/user/hadoop/some/path/block-r-00299.bz2\n       at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1354)\n       at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1195)\n       at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n       at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n       at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786)\n       at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:135)\n       at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.getRecordWriter(MultipleOutputs.java:475)\n       at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.write(MultipleOutputs.java:433)\n       at com.ancestry.bigtree.hadoop.LevelReducer.processValue(LevelReducer.java:91)\n       at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:69)\n       at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:14)\n       at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)\n       at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)\n       at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)\n       at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n       at java.security.AccessController.doPrivileged(Native Method)\n       at javax.security.auth.Subject.doAs(Subject.java:415)\n       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n       at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n{code}\n\nAs discussed in MAPREDUCE-3772, when the baseOutputPath passed to MultipleOutputs.write() is an absolute path (or more precisely a path that resolves outside of the job output-dir), the concept of output committing is not utilized. \n\nIn this case, the user read thru the MultipleOutputs docs and was assuming that everything will be working fine, as there are blog posts saying that MultipleOutputs does handle output commit. "
        }
    },
    {
        "filename": "MAPREDUCE-6091.json",
        "creation_time": "2014-09-16T01:19:58.000+0000",
        "stack_trace": "```\njava.io.IOException: org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException: Application with id 'application_1410289045532_90542' doesn't exist in RM.\n\tat org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplicationReport(ClientRMService.java:288)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplicationReport(ApplicationClientProtocolPBServiceImpl.java:150)\n\tat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:337)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2058)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2054)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2052)\n\n\tat org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:348)\n\tat org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:419)\n\tat org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:559)\n\tat org.apache.hadoop.mapreduce.Job$1.run(Job.java:314)\n\tat org.apache.hadoop.mapreduce.Job$1.run(Job.java:311)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)\n\tat org.apache.hadoop.mapreduce.Job.updateStatus(Job.java:311)\n\tat org.apache.hadoop.mapreduce.Job.isComplete(Job.java:599)\n\tat org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkRunningState(ControlledJob.java:257)\n\tat org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkState(ControlledJob.java:282)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.pig.backend.hadoop23.PigJobControl.checkState(PigJobControl.java:120)\n\tat org.apache.pig.backend.hadoop23.PigJobControl.run(PigJobControl.java:180)\n\tat java.lang.Thread.run(Thread.java:662)\n\tat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:279)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcRequestWrapper request = (RpcRequestWrapper) writableRequest;\n        RequestHeaderProto rpcRequest = request.requestHeader;\n        String methodName = rpcRequest.getMethodName();\n        \n        \n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = prototype.newBuilderForType()\n            .mergeFrom(request.theRequestRead).build();\n        \n        Message result;\n        long startTime = Time.now();\n        int qTime = (int) (startTime - receiveTime);\n        Exception exception = null;\n        try {\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n        } catch (ServiceException e) {\n          exception = (Exception) e.getCause();\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          exception = e;\n          throw e;\n        } finally {\n          int processingTime = (int) (Time.now() - startTime);\n          if (LOG.isDebugEnabled()) {\n            String msg = \"Served: \" + methodName + \" queueTime= \" + qTime +\n                \" procesingTime= \" + processingTime;\n            if (exception != null) {\n              msg += \" exception= \" + exception.getClass().getSimpleName();\n            }\n            LOG.debug(msg);\n          }\n          String detailedMetricsName = (exception == null) ?\n              methodName :\n              exception.getClass().getSimpleName();\n          server.rpcMetrics.addRpcQueueTime(qTime);\n          server.rpcMetrics.addRpcProcessingTime(processingTime);\n          server.rpcDetailedMetrics.addProcessingTime(detailedMetricsName,\n              processingTime);\n        }\n        return new RpcResponseWrapper(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getProtocolImpl": "      private static ProtoClassProtoImpl getProtocolImpl(RPC.Server server,\n          String protoName, long clientVersion) throws RpcServerException {\n        ProtoNameVer pv = new ProtoNameVer(protoName, clientVersion);\n        ProtoClassProtoImpl impl = \n            server.getProtocolImplMap(RPC.RpcKind.RPC_PROTOCOL_BUFFER).get(pv);\n        if (impl == null) { // no match for Protocol AND Version\n          VerProtocolImpl highest = \n              server.getHighestSupportedProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER, \n                  protoName);\n          if (highest == null) {\n            throw new RpcNoSuchProtocolException(\n                \"Unknown protocol: \" + protoName);\n          }\n          // protocol supported but not the version that client wants\n          throw new RPC.VersionMismatch(protoName, clientVersion,\n              highest.version);\n        }\n        return impl;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.call": "    public Writable call(RPC.RpcKind rpcKind, String protocol,\n        Writable rpcRequest, long receiveTime) throws Exception {\n      return getRpcInvoker(rpcKind).call(this, protocol, rpcRequest,\n          receiveTime);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRespond": "    void doRespond(Call call) throws IOException {\n      synchronized (call.connection.responseQueue) {\n        call.connection.responseQueue.addLast(call);\n        if (call.connection.responseQueue.size() == 1) {\n          processResponse(call.connection.responseQueue, true);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.toString": "    public String toString() {\n      return getHostAddress() + \":\" + remotePort; \n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getRpcErrorCodeProto": "    public RpcErrorCodeProto getRpcErrorCodeProto() {\n      return errCode;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    boolean close(Connection connection) {\n      boolean exists = remove(connection);\n      if (exists) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(Thread.currentThread().getName() +\n              \": disconnecting client \" + connection +\n              \". Number of active connections: \"+ size());\n        }\n        // only close if actually removed to avoid double-closing due\n        // to possible races\n        connection.close();\n      }\n      return exists;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.isTerse": "    boolean isTerse(Class<?> t) {\n      return terseExceptions.contains(t.toString());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.size": "    int size() {\n      return count.get();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws InterruptedException, IOException,  OutOfMemoryError {\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        channel.socket().setKeepAlive(true);\n        \n        Reader reader = getReader();\n        Connection c = connectionManager.register(channel);\n        key.attach(c);  // so closeCurrentConnection can get the object\n        reader.addConnection(c);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(ByteArrayOutputStream responseBuf,\n                             Call call, RpcStatusProto status, RpcErrorCodeProto erCode,\n                             Writable rv, String errorClass, String error) \n  throws IOException {\n    responseBuf.reset();\n    DataOutputStream out = new DataOutputStream(responseBuf);\n    RpcResponseHeaderProto.Builder headerBuilder =  \n        RpcResponseHeaderProto.newBuilder();\n    headerBuilder.setClientId(ByteString.copyFrom(call.clientId));\n    headerBuilder.setCallId(call.callId);\n    headerBuilder.setRetryCount(call.retryCount);\n    headerBuilder.setStatus(status);\n    headerBuilder.setServerIpcVersionNum(CURRENT_VERSION);\n\n    if (status == RpcStatusProto.SUCCESS) {\n      RpcResponseHeaderProto header = headerBuilder.build();\n      final int headerLen = header.getSerializedSize();\n      int fullLength  = CodedOutputStream.computeRawVarint32Size(headerLen) +\n          headerLen;\n      try {\n        if (rv instanceof ProtobufRpcEngine.RpcWrapper) {\n          ProtobufRpcEngine.RpcWrapper resWrapper = \n              (ProtobufRpcEngine.RpcWrapper) rv;\n          fullLength += resWrapper.getLength();\n          out.writeInt(fullLength);\n          header.writeDelimitedTo(out);\n          rv.write(out);\n        } else { // Have to serialize to buffer to get len\n          final DataOutputBuffer buf = new DataOutputBuffer();\n          rv.write(buf);\n          byte[] data = buf.getData();\n          fullLength += buf.getLength();\n          out.writeInt(fullLength);\n          header.writeDelimitedTo(out);\n          out.write(data, 0, buf.getLength());\n        }\n      } catch (Throwable t) {\n        LOG.warn(\"Error serializing call response for call \" + call, t);\n        // Call back to same function - this is OK since the\n        // buffer is reset at the top, and since status is changed\n        // to ERROR it won't infinite loop.\n        setupResponse(responseBuf, call, RpcStatusProto.ERROR,\n            RpcErrorCodeProto.ERROR_SERIALIZING_RESPONSE,\n            null, t.getClass().getName(),\n            StringUtils.stringifyException(t));\n        return;\n      }\n    } else { // Rpc Failure\n      headerBuilder.setExceptionClassName(errorClass);\n      headerBuilder.setErrorMsg(error);\n      headerBuilder.setErrorDetail(erCode);\n      RpcResponseHeaderProto header = headerBuilder.build();\n      int headerLen = header.getSerializedSize();\n      final int fullLength  = \n          CodedOutputStream.computeRawVarint32Size(headerLen) + headerLen;\n      out.writeInt(fullLength);\n      header.writeDelimitedTo(out);\n    }\n    if (call.connection.useWrap) {\n      wrapWithSasl(responseBuf, call);\n    }\n    call.setResponse(ByteBuffer.wrap(responseBuf.toByteArray()));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.stopIdleScan": "    void stopIdleScan() {\n      idleScanTimer.cancel();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeIdle": "    synchronized void closeIdle(boolean scanAll) {\n      long minLastContact = Time.now() - maxIdleTime;\n      // concurrent iterator might miss new connections added\n      // during the iteration, but that's ok because they won't\n      // be idle yet anyway and will be caught on next scan\n      int closed = 0;\n      for (Connection connection : connections) {\n        // stop if connections dropped below threshold unless scanning all\n        if (!scanAll && size() < idleScanThreshold) {\n          break;\n        }\n        // stop if not scanning all and max connections are closed\n        if (connection.isIdle() &&\n            connection.getLastContact() < minLastContact &&\n            close(connection) &&\n            !scanAll && (++closed == maxIdleToClose)) {\n          break;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(RPC.RpcKind rpcKind, String protocol,\n      Writable param, long receiveTime) throws Exception;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param protocolName - the protocol\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  private void authorize(UserGroupInformation user, String protocolName,\n      InetAddress addr) throws AuthorizationException {\n    if (authorize) {\n      if (protocolName == null) {\n        throw new AuthorizationException(\"Null protocol not authorized\");\n      }\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(protocolName, getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         protocolName);\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeAll": "    void closeAll() {\n      // use a copy of the connections to be absolutely sure the concurrent\n      // iterator doesn't miss a connection\n      for (Connection connection : toArray()) {\n        close(connection);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.remove": "    private boolean remove(Connection connection) {\n      boolean removed = connections.remove(connection);\n      if (removed) {\n        count.getAndDecrement();\n      }\n      return removed;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.startIdleScan": "    void startIdleScan() {\n      scheduleIdleScanTask();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isValid() && key.isWritable()) {\n                  doAsyncWrite(key);\n              }\n            } catch (IOException e) {\n              LOG.info(Thread.currentThread().getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = Time.now();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<Call> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<Call>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              Call call = (Call)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n          \n          for(Call call : calls) {\n            doPurge(call, now);\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.scheduleIdleScanTask": "    private void scheduleIdleScanTask() {\n      if (!running) {\n        return;\n      }\n      TimerTask idleScanTask = new TimerTask(){\n        @Override\n        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }\n      };\n      idleScanTimer.schedule(idleScanTask, idleScanInterval);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.ClientServiceDelegate.invoke": "  private synchronized Object invoke(String method, Class argClass,\n      Object args) throws IOException {\n    Method methodOb = null;\n    try {\n      methodOb = MRClientProtocol.class.getMethod(method, argClass);\n    } catch (SecurityException e) {\n      throw new YarnRuntimeException(e);\n    } catch (NoSuchMethodException e) {\n      throw new YarnRuntimeException(\"Method name mismatch\", e);\n    }\n    maxClientRetry = this.conf.getInt(\n        MRJobConfig.MR_CLIENT_MAX_RETRIES,\n        MRJobConfig.DEFAULT_MR_CLIENT_MAX_RETRIES);\n    IOException lastException = null;\n    while (maxClientRetry > 0) {\n      MRClientProtocol MRClientProxy = null;\n      try {\n        MRClientProxy = getProxy();\n        return methodOb.invoke(MRClientProxy, args);\n      } catch (InvocationTargetException e) {\n        // Will not throw out YarnException anymore\n        LOG.debug(\"Failed to contact AM/History for job \" + jobId + \n            \" retrying..\", e.getTargetException());\n        // Force reconnection by setting the proxy to null.\n        realProxy = null;\n        // HS/AMS shut down\n        // if it's AM shut down, do not decrement maxClientRetry as we wait for\n        // AM to be restarted.\n        if (!usingAMProxy.get()) {\n          maxClientRetry--;\n        }\n        usingAMProxy.set(false);\n        lastException = new IOException(e.getTargetException());\n        try {\n          Thread.sleep(100);\n        } catch (InterruptedException ie) {\n          LOG.warn(\"ClientServiceDelegate invoke call interrupted\", ie);\n          throw new YarnRuntimeException(ie);\n        }\n      } catch (Exception e) {\n        LOG.debug(\"Failed to contact AM/History for job \" + jobId\n            + \"  Will retry..\", e);\n        // Force reconnection by setting the proxy to null.\n        realProxy = null;\n        // RM shutdown\n        maxClientRetry--;\n        lastException = new IOException(e.getMessage());\n        try {\n          Thread.sleep(100);\n        } catch (InterruptedException ie) {\n          LOG.warn(\"ClientServiceDelegate invoke call interrupted\", ie);\n          throw new YarnRuntimeException(ie);\n        }\n      }\n    }\n    throw lastException;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.ClientServiceDelegate.getProxy": "  private MRClientProtocol getProxy() throws IOException {\n    if (realProxy != null) {\n      return realProxy;\n    }\n    \n    // Possibly allow nulls through the PB tunnel, otherwise deal with an exception\n    // and redirect to the history server.\n    ApplicationReport application = null;\n    try {\n      application = rm.getApplicationReport(appId);\n    } catch (YarnException e2) {\n      throw new IOException(e2);\n    }\n    if (application != null) {\n      trackingUrl = application.getTrackingUrl();\n    }\n    InetSocketAddress serviceAddr = null;\n    while (application == null\n        || YarnApplicationState.RUNNING == application\n            .getYarnApplicationState()) {\n      if (application == null) {\n        LOG.info(\"Could not get Job info from RM for job \" + jobId\n            + \". Redirecting to job history server.\");\n        return checkAndGetHSProxy(null, JobState.NEW);\n      }\n      try {\n        if (application.getHost() == null || \"\".equals(application.getHost())) {\n          LOG.debug(\"AM not assigned to Job. Waiting to get the AM ...\");\n          Thread.sleep(2000);\n\n          LOG.debug(\"Application state is \" + application.getYarnApplicationState());\n          application = rm.getApplicationReport(appId);\n          continue;\n        } else if (UNAVAILABLE.equals(application.getHost())) {\n          if (!amAclDisabledStatusLogged) {\n            LOG.info(\"Job \" + jobId + \" is running, but the host is unknown.\"\n                + \" Verify user has VIEW_JOB access.\");\n            amAclDisabledStatusLogged = true;\n          }\n          return getNotRunningJob(application, JobState.RUNNING);\n        }\n        if(!conf.getBoolean(MRJobConfig.JOB_AM_ACCESS_DISABLED, false)) {\n          UserGroupInformation newUgi = UserGroupInformation.createRemoteUser(\n              UserGroupInformation.getCurrentUser().getUserName());\n          serviceAddr = NetUtils.createSocketAddrForHost(\n              application.getHost(), application.getRpcPort());\n          if (UserGroupInformation.isSecurityEnabled()) {\n            org.apache.hadoop.yarn.api.records.Token clientToAMToken =\n                application.getClientToAMToken();\n            Token<ClientToAMTokenIdentifier> token =\n                ConverterUtils.convertFromYarn(clientToAMToken, serviceAddr);\n            newUgi.addToken(token);\n          }\n          LOG.debug(\"Connecting to \" + serviceAddr);\n          final InetSocketAddress finalServiceAddr = serviceAddr;\n          realProxy = newUgi.doAs(new PrivilegedExceptionAction<MRClientProtocol>() {\n            @Override\n            public MRClientProtocol run() throws IOException {\n              return instantiateAMProxy(finalServiceAddr);\n            }\n          });\n        } else {\n          if (!amAclDisabledStatusLogged) {\n            LOG.info(\"Network ACL closed to AM for job \" + jobId\n                + \". Not going to try to reach the AM.\");\n            amAclDisabledStatusLogged = true;\n          }\n          return getNotRunningJob(null, JobState.RUNNING);\n        }\n        return realProxy;\n      } catch (IOException e) {\n        //possibly the AM has crashed\n        //there may be some time before AM is restarted\n        //keep retrying by getting the address from RM\n        LOG.info(\"Could not connect to \" + serviceAddr +\n        \". Waiting for getting the latest AM address...\");\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e1) {\n          LOG.warn(\"getProxy() call interruped\", e1);\n          throw new YarnRuntimeException(e1);\n        }\n        try {\n          application = rm.getApplicationReport(appId);\n        } catch (YarnException e1) {\n          throw new IOException(e1);\n        }\n        if (application == null) {\n          LOG.info(\"Could not get Job info from RM for job \" + jobId\n              + \". Redirecting to job history server.\");\n          return checkAndGetHSProxy(null, JobState.RUNNING);\n        }\n      } catch (InterruptedException e) {\n        LOG.warn(\"getProxy() call interruped\", e);\n        throw new YarnRuntimeException(e);\n      } catch (YarnException e) {\n        throw new IOException(e);\n      }\n    }\n\n    /** we just want to return if its allocating, so that we don't\n     * block on it. This is to be able to return job status\n     * on an allocating Application.\n     */\n    String user = application.getUser();\n    if (user == null) {\n      throw new IOException(\"User is not set in the application report\");\n    }\n    if (application.getYarnApplicationState() == YarnApplicationState.NEW\n        || application.getYarnApplicationState() ==\n            YarnApplicationState.NEW_SAVING\n        || application.getYarnApplicationState() == YarnApplicationState.SUBMITTED\n        || application.getYarnApplicationState() == YarnApplicationState.ACCEPTED) {\n      realProxy = null;\n      return getNotRunningJob(application, JobState.NEW);\n    }\n\n    if (application.getYarnApplicationState() == YarnApplicationState.FAILED) {\n      realProxy = null;\n      return getNotRunningJob(application, JobState.FAILED);\n    }\n\n    if (application.getYarnApplicationState() == YarnApplicationState.KILLED) {\n      realProxy = null;\n      return getNotRunningJob(application, JobState.KILLED);\n    }\n\n    //History server can serve a job only if application\n    //succeeded.\n    if (application.getYarnApplicationState() == YarnApplicationState.FINISHED) {\n      LOG.info(\"Application state is completed. FinalApplicationStatus=\"\n          + application.getFinalApplicationStatus().toString()\n          + \". Redirecting to job history server\");\n      realProxy = checkAndGetHSProxy(application, JobState.SUCCEEDED);\n    }\n    return realProxy;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus": "  public JobStatus getJobStatus(JobID oldJobID) throws IOException {\n    org.apache.hadoop.mapreduce.v2.api.records.JobId jobId =\n      TypeConverter.toYarn(oldJobID);\n    GetJobReportRequest request =\n        recordFactory.newRecordInstance(GetJobReportRequest.class);\n    request.setJobId(jobId);\n    JobReport report = ((GetJobReportResponse) invoke(\"getJobReport\",\n        GetJobReportRequest.class, request)).getJobReport();\n    JobStatus jobStatus = null;\n    if (report != null) {\n      if (StringUtils.isEmpty(report.getJobFile())) {\n        String jobFile = MRApps.getJobFile(conf, report.getUser(), oldJobID);\n        report.setJobFile(jobFile);\n      }\n      String historyTrackingUrl = report.getTrackingUrl();\n      String url = StringUtils.isNotEmpty(historyTrackingUrl)\n          ? historyTrackingUrl : trackingUrl;\n      jobStatus = TypeConverter.fromYarn(report, url);\n    }\n    return jobStatus;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.YARNRunner.getJobStatus": "  public JobStatus getJobStatus(JobID jobID) throws IOException,\n      InterruptedException {\n    JobStatus status = clientCache.getClient(jobID).getJobStatus(jobID);\n    return status;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkRunningState": "  private void checkRunningState() throws IOException, InterruptedException {\n    try {\n      if (job.isComplete()) {\n        if (job.isSuccessful()) {\n          this.state = State.SUCCESS;\n        } else {\n          this.state = State.FAILED;\n          this.message = \"Job failed!\";\n        }\n      }\n    } catch (IOException ioe) {\n      this.state = State.FAILED;\n      this.message = StringUtils.stringifyException(ioe);\n      try {\n        if (job != null) {\n          job.killJob();\n        }\n      } catch (IOException e) {}\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.killJob": "  public void killJob() throws IOException, InterruptedException {\n    job.killJob();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkState": "   synchronized State checkState() throws IOException, InterruptedException {\n    if (this.state == State.RUNNING) {\n      checkRunningState();\n    }\n    if (this.state != State.WAITING) {\n      return this.state;\n    }\n    if (this.dependingJobs == null || this.dependingJobs.size() == 0) {\n      this.state = State.READY;\n      return this.state;\n    }\n    ControlledJob pred = null;\n    int n = this.dependingJobs.size();\n    for (int i = 0; i < n; i++) {\n      pred = this.dependingJobs.get(i);\n      State s = pred.checkState();\n      if (s == State.WAITING || s == State.READY || s == State.RUNNING) {\n        break; // a pred is still not completed, continue in WAITING\n        // state\n      }\n      if (s == State.FAILED || s == State.DEPENDENT_FAILED) {\n        this.state = State.DEPENDENT_FAILED;\n        this.message = \"depending job \" + i + \" with jobID \"\n          + pred.getJobID() + \" failed. \" + pred.getMessage();\n        break;\n      }\n      // pred must be in success state\n      if (i == n - 1) {\n        this.state = State.READY;\n      }\n    }\n\n    return this.state;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.getJobID": "  public String getJobID() {\n    return this.controlID;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.getMessage": "  public synchronized String getMessage() {\n    return this.message;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcServerException.getRpcStatusProto": "  public RpcStatusProto getRpcStatusProto() {\n    return RpcStatusProto.ERROR;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.clientCache.getClient": "  public synchronized ClientServiceDelegate getClient(JobID jobId) {\n    if (hsProxy == null) {\n      try {\n        hsProxy = instantiateHistoryProxy();\n      } catch (IOException e) {\n        LOG.warn(\"Could not connect to History server.\", e);\n        throw new YarnRuntimeException(\"Could not connect to History server.\", e);\n      }\n    }\n    ClientServiceDelegate client = cache.get(jobId);\n    if (client == null) {\n      client = new ClientServiceDelegate(conf, rm, jobId, hsProxy);\n      cache.put(jobId, client);\n    }\n    return client;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.clientCache.instantiateHistoryProxy": "  protected MRClientProtocol instantiateHistoryProxy()\n      throws IOException {\n    final String serviceAddr = conf.get(JHAdminConfig.MR_HISTORY_ADDRESS);\n    if (StringUtils.isEmpty(serviceAddr)) {\n      return null;\n    }\n    LOG.debug(\"Connecting to HistoryServer at: \" + serviceAddr);\n    final YarnRPC rpc = YarnRPC.create(conf);\n    LOG.debug(\"Connected to HistoryServer at: \" + serviceAddr);\n    UserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\n    return currentUser.doAs(new PrivilegedAction<MRClientProtocol>() {\n      @Override\n      public MRClientProtocol run() {\n        return (MRClientProtocol) rpc.getProxy(HSClientProtocol.class,\n            NetUtils.createSocketAddr(serviceAddr), conf);\n      }\n    });\n  }"
        },
        "bug_report": {
            "Title": "YARNRunner.getJobStatus() fails with ApplicationNotFoundException if the job rolled off the RM view",
            "Description": "If you query the job status of a job that rolled off the RM view via YARNRunner.getJobStatus(), it fails with an ApplicationNotFoundException. For example,\n\n{noformat}\n2014-09-15 07:09:51,084 ERROR org.apache.pig.tools.grunt.Grunt: ERROR 6017: JobID: job_1410289045532_90542 Reason: java.io.IOException: org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException: Application with id 'application_1410289045532_90542' doesn't exist in RM.\n\tat org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplicationReport(ClientRMService.java:288)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplicationReport(ApplicationClientProtocolPBServiceImpl.java:150)\n\tat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:337)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2058)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2054)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2052)\n\n\tat org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:348)\n\tat org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:419)\n\tat org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:559)\n\tat org.apache.hadoop.mapreduce.Job$1.run(Job.java:314)\n\tat org.apache.hadoop.mapreduce.Job$1.run(Job.java:311)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)\n\tat org.apache.hadoop.mapreduce.Job.updateStatus(Job.java:311)\n\tat org.apache.hadoop.mapreduce.Job.isComplete(Job.java:599)\n\tat org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkRunningState(ControlledJob.java:257)\n\tat org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkState(ControlledJob.java:282)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.pig.backend.hadoop23.PigJobControl.checkState(PigJobControl.java:120)\n\tat org.apache.pig.backend.hadoop23.PigJobControl.run(PigJobControl.java:180)\n\tat java.lang.Thread.run(Thread.java:662)\n\tat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:279)\n{noformat}\n\nPrior to 2.1.0, it used to be able to fall back onto the job history server and get the status.\n\nThis appears to be introduced by YARN-873. YARN-873 changed ClientRMService to throw an ApplicationNotFoundException on an unknown app id (from returning null). But MR's ClientServiceDelegate was never modified to change its behavior."
        }
    },
    {
        "filename": "MAPREDUCE-4825.json",
        "creation_time": "2012-11-27T19:01:45.000+0000",
        "stack_trace": "```\njava.lang.IllegalArgumentException: Illegal job state: ERROR\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.finished(JobImpl.java:838)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1622)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:359)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:299)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:287)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:445)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:723)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:974)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)\n\tat java.lang.Thread.run(Thread.java:662)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.finished": "  JobStateInternal finished(JobStateInternal finalState) {\n    if (getInternalState() == JobStateInternal.RUNNING) {\n      metrics.endRunningJob(this);\n    }\n    if (finishTime == 0) setFinishTime();\n    eventHandler.handle(new JobFinishEvent(jobId));\n\n    switch (finalState) {\n      case KILLED:\n        metrics.killedJob(this);\n        break;\n      case FAILED:\n        metrics.failedJob(this);\n        break;\n      case SUCCEEDED:\n        metrics.completedJob(this);\n        break;\n      default:\n        throw new IllegalArgumentException(\"Illegal job state: \" + finalState);\n    }\n    return finalState;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle": "  public void handle(JobEvent event) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Processing \" + event.getJobId() + \" of type \"\n          + event.getType());\n    }\n    try {\n      writeLock.lock();\n      JobStateInternal oldState = getInternalState();\n      try {\n         getStateMachine().doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        addDiagnostic(\"Invalid event \" + event.getType() + \n            \" on Job \" + this.jobId);\n        eventHandler.handle(new JobEvent(this.jobId,\n            JobEventType.INTERNAL_ERROR));\n      }\n      //notify the eventhandler of state change\n      if (oldState != getInternalState()) {\n        LOG.info(jobId + \"Job Transitioned from \" + oldState + \" to \"\n                 + getInternalState());\n      }\n    }\n    \n    finally {\n      writeLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getInternalState": "  public JobStateInternal getInternalState() {\n    readLock.lock();\n    try {\n     return getStateMachine().getCurrentState();\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.setFinishTime": "  void setFinishTime() {\n    finishTime = clock.getTime();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.transition": "    public void transition(JobImpl job, JobEvent event) {\n      //TODO Is this JH event required.\n      job.setFinishTime();\n      JobUnsuccessfulCompletionEvent failedEvent =\n          new JobUnsuccessfulCompletionEvent(job.oldJobId,\n              job.finishTime, 0, 0,\n              JobStateInternal.ERROR.toString());\n      job.eventHandler.handle(new JobHistoryEvent(job.jobId, failedEvent));\n      job.finished(JobStateInternal.ERROR);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.taskKilled": "    private void taskKilled(JobImpl job, Task task) {\n      if (task.getType() == TaskType.MAP) {\n        job.killedMapTaskCount++;\n      } else if (task.getType() == TaskType.REDUCE) {\n        job.killedReduceTaskCount++;\n      }\n      job.metrics.killedTask(task);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.actOnUnusableNode": "  private void actOnUnusableNode(NodeId nodeId, NodeState nodeState) {\n    // rerun previously successful map tasks\n    List<TaskAttemptId> taskAttemptIdList = nodesToSucceededTaskAttempts.get(nodeId);\n    if(taskAttemptIdList != null) {\n      String mesg = \"TaskAttempt killed because it ran on unusable node \"\n          + nodeId;\n      for(TaskAttemptId id : taskAttemptIdList) {\n        if(TaskType.MAP == id.getTaskId().getTaskType()) {\n          // reschedule only map tasks because their outputs maybe unusable\n          LOG.info(mesg + \". AttemptId:\" + id);\n          eventHandler.handle(new TaskAttemptKillEvent(id, mesg));\n        }\n      }\n    }\n    // currently running task attempts on unusable nodes are handled in\n    // RMContainerAllocator\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getState": "  public JobState getState() {\n    readLock.lock();\n    try {\n      return getExternalState(getStateMachine().getCurrentState());\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.scheduleTasks": "  protected void scheduleTasks(Set<TaskId> taskIDs) {\n    for (TaskId taskID : taskIDs) {\n      eventHandler.handle(new TaskEvent(taskID, \n          TaskEventType.T_SCHEDULE));\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getReport": "  public JobReport getReport() {\n    readLock.lock();\n    try {\n      JobState state = getState();\n\n      // jobFile can be null if the job is not yet inited.\n      String jobFile =\n          remoteJobConfFile == null ? \"\" : remoteJobConfFile.toString();\n\n      StringBuilder diagsb = new StringBuilder();\n      for (String s : getDiagnostics()) {\n        diagsb.append(s).append(\"\\n\");\n      }\n\n      if (getInternalState() == JobStateInternal.NEW) {\n        return MRBuilderUtils.newJobReport(jobId, jobName, username, state,\n            appSubmitTime, startTime, finishTime, setupProgress, 0.0f, 0.0f,\n            cleanupProgress, jobFile, amInfos, isUber, diagsb.toString());\n      }\n\n      computeProgress();\n      JobReport report = MRBuilderUtils.newJobReport(jobId, jobName, username,\n          state, appSubmitTime, startTime, finishTime, setupProgress,\n          this.mapProgress, this.reduceProgress,\n          cleanupProgress, jobFile, amInfos, isUber, diagsb.toString());\n      return report;\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.setup": "    protected void setup(JobImpl job) throws IOException {\n\n      String oldJobIDString = job.oldJobId.toString();\n      String user = \n        UserGroupInformation.getCurrentUser().getShortUserName();\n      Path path = MRApps.getStagingAreaDir(job.conf, user);\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"startJobs: parent=\" + path + \" child=\" + oldJobIDString);\n      }\n\n      job.remoteJobSubmitDir =\n          FileSystem.get(job.conf).makeQualified(\n              new Path(path, oldJobIDString));\n      job.remoteJobConfFile =\n          new Path(job.remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n\n      // Prepare the TaskAttemptListener server for authentication of Containers\n      // TaskAttemptListener gets the information via jobTokenSecretManager.\n      JobTokenIdentifier identifier =\n          new JobTokenIdentifier(new Text(oldJobIDString));\n      job.jobToken =\n          new Token<JobTokenIdentifier>(identifier, job.jobTokenSecretManager);\n      job.jobToken.setService(identifier.getJobId());\n      // Add it to the jobTokenSecretManager so that TaskAttemptListener server\n      // can authenticate containers(tasks)\n      job.jobTokenSecretManager.addTokenForJob(oldJobIDString, job.jobToken);\n      LOG.info(\"Adding job token for \" + oldJobIDString\n          + \" to jobTokenSecretManager\");\n\n      // Upload the jobTokens onto the remote FS so that ContainerManager can\n      // localize it to be used by the Containers(tasks)\n      Credentials tokenStorage = new Credentials();\n      TokenCache.setJobToken(job.jobToken, tokenStorage);\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        tokenStorage.addAll(job.fsTokens);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getFileSystem": "  protected FileSystem getFileSystem(Configuration conf) throws IOException {\n    return FileSystem.get(conf);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.makeUberDecision": "  private void makeUberDecision(long dataInputLength) {\n    //FIXME:  need new memory criterion for uber-decision (oops, too late here;\n    // until AM-resizing supported,\n    // must depend on job client to pass fat-slot needs)\n    // these are no longer \"system\" settings, necessarily; user may override\n    int sysMaxMaps = conf.getInt(MRJobConfig.JOB_UBERTASK_MAXMAPS, 9);\n\n    //FIXME: handling multiple reduces within a single AM does not seem to\n    //work.\n    int sysMaxReduces = conf.getInt(MRJobConfig.JOB_UBERTASK_MAXREDUCES, 1);\n    boolean isValidUberMaxReduces = (sysMaxReduces == 0)\n        || (sysMaxReduces == 1);\n\n    long sysMaxBytes = conf.getLong(MRJobConfig.JOB_UBERTASK_MAXBYTES,\n        fs.getDefaultBlockSize(this.remoteJobSubmitDir)); // FIXME: this is wrong; get FS from\n                                   // [File?]InputFormat and default block size\n                                   // from that\n\n    long sysMemSizeForUberSlot =\n        conf.getInt(MRJobConfig.MR_AM_VMEM_MB,\n            MRJobConfig.DEFAULT_MR_AM_VMEM_MB);\n\n    boolean uberEnabled =\n        conf.getBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\n    boolean smallNumMapTasks = (numMapTasks <= sysMaxMaps);\n    boolean smallNumReduceTasks = (numReduceTasks <= sysMaxReduces);\n    boolean smallInput = (dataInputLength <= sysMaxBytes);\n    // ignoring overhead due to UberAM and statics as negligible here:\n    boolean smallMemory =\n        ( (Math.max(conf.getLong(MRJobConfig.MAP_MEMORY_MB, 0),\n            conf.getLong(MRJobConfig.REDUCE_MEMORY_MB, 0))\n            <= sysMemSizeForUberSlot)\n            || (sysMemSizeForUberSlot == JobConf.DISABLED_MEMORY_LIMIT));\n    boolean notChainJob = !isChainJob(conf);\n\n    // User has overall veto power over uberization, or user can modify\n    // limits (overriding system settings and potentially shooting\n    // themselves in the head).  Note that ChainMapper/Reducer are\n    // fundamentally incompatible with MR-1220; they employ a blocking\n    // queue between the maps/reduces and thus require parallel execution,\n    // while \"uber-AM\" (MR AM + LocalContainerLauncher) loops over tasks\n    // and thus requires sequential execution.\n    isUber = uberEnabled && smallNumMapTasks && smallNumReduceTasks\n        && smallInput && smallMemory && notChainJob && isValidUberMaxReduces;\n\n    if (isUber) {\n      LOG.info(\"Uberizing job \" + jobId + \": \" + numMapTasks + \"m+\"\n          + numReduceTasks + \"r tasks (\" + dataInputLength\n          + \" input bytes) will run sequentially on single node.\");\n\n      // make sure reduces are scheduled only after all map are completed\n      conf.setFloat(MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART,\n                        1.0f);\n      // uber-subtask attempts all get launched on same node; if one fails,\n      // probably should retry elsewhere, i.e., move entire uber-AM:  ergo,\n      // limit attempts to 1 (or at most 2?  probably not...)\n      conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS, 1);\n      conf.setInt(MRJobConfig.REDUCE_MAX_ATTEMPTS, 1);\n\n      // disable speculation\n      conf.setBoolean(MRJobConfig.MAP_SPECULATIVE, false);\n      conf.setBoolean(MRJobConfig.REDUCE_SPECULATIVE, false);\n    } else {\n      StringBuilder msg = new StringBuilder();\n      msg.append(\"Not uberizing \").append(jobId).append(\" because:\");\n      if (!uberEnabled)\n        msg.append(\" not enabled;\");\n      if (!smallNumMapTasks)\n        msg.append(\" too many maps;\");\n      if (!smallNumReduceTasks)\n        msg.append(\" too many reduces;\");\n      if (!smallInput)\n        msg.append(\" too much input;\");\n      if (!smallMemory)\n        msg.append(\" too much RAM;\");\n      if (!notChainJob)\n        msg.append(\" chainjob;\");\n      if (!isValidUberMaxReduces)\n        msg.append(\" not supported uber max reduces\");\n      LOG.info(msg.toString());\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.checkJobCompleteSuccess": "  static JobStateInternal checkJobCompleteSuccess(JobImpl job) {\n    // check for Job success\n    if (job.completedTaskCount == job.tasks.size()) {\n      try {\n        // Commit job & do cleanup\n        job.getCommitter().commitJob(job.getJobContext());\n      } catch (IOException e) {\n        LOG.error(\"Could not do commit for Job\", e);\n        job.addDiagnostic(\"Job commit failed: \" + e.getMessage());\n        job.abortJob(org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n        return job.finished(JobStateInternal.FAILED);\n      }\n      job.logJobHistoryFinishedEvent();\n      return job.finished(JobStateInternal.SUCCEEDED);\n    }\n    return null;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.taskSucceeded": "    private void taskSucceeded(JobImpl job, Task task) {\n      if (task.getType() == TaskType.MAP) {\n        job.succeededMapTaskCount++;\n      } else {\n        job.succeededReduceTaskCount++;\n      }\n      job.metrics.completedTask(task);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.checkJobForCompletion": "    protected JobStateInternal checkJobForCompletion(JobImpl job) {\n      if (job.completedTaskCount == job.tasks.size()) {\n        job.setFinishTime();\n        job.abortJob(org.apache.hadoop.mapreduce.JobStatus.State.KILLED);\n        return job.finished(JobStateInternal.KILLED);\n      }\n      //return the current state, Job not finished yet\n      return job.getInternalState();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.abortJob": "  protected void abortJob(\n      org.apache.hadoop.mapreduce.JobStatus.State finalState) {\n    try {\n      committer.abortJob(jobContext, finalState);\n    } catch (IOException e) {\n      LOG.warn(\"Could not abortJob\", e);\n    }\n    if (finishTime == 0) setFinishTime();\n    cleanupProgress = 1.0f;\n    JobUnsuccessfulCompletionEvent unsuccessfulJobEvent =\n      new JobUnsuccessfulCompletionEvent(oldJobId,\n          finishTime,\n          succeededMapTaskCount,\n          succeededReduceTaskCount,\n          finalState.toString());\n    eventHandler.handle(new JobHistoryEvent(jobId, unsuccessfulJobEvent));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.createMapTasks": "    private void createMapTasks(JobImpl job, long inputLength,\n                                TaskSplitMetaInfo[] splits) {\n      for (int i=0; i < job.numMapTasks; ++i) {\n        TaskImpl task =\n            new MapTaskImpl(job.jobId, i,\n                job.eventHandler, \n                job.remoteJobConfFile, \n                job.conf, splits[i], \n                job.taskAttemptListener, \n                job.committer, job.jobToken, job.fsTokens,\n                job.clock, job.completedTasksFromPreviousRun, \n                job.applicationAttemptId.getAttemptId(),\n                job.metrics, job.appContext);\n        job.addTask(task);\n      }\n      LOG.info(\"Input size for job \" + job.jobId + \" = \" + inputLength\n          + \". Number of splits = \" + splits.length);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getID": "  public JobId getID() {\n    return jobId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.taskFailed": "    private void taskFailed(JobImpl job, Task task) {\n      if (task.getType() == TaskType.MAP) {\n        job.failedMapTaskCount++;\n      } else if (task.getType() == TaskType.REDUCE) {\n        job.failedReduceTaskCount++;\n      }\n      job.addDiagnostic(\"Task failed \" + task.getID());\n      job.metrics.failedTask(task);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.checkTaskLimits": "    private void checkTaskLimits() {\n      // no code, for now\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.isUber": "  public boolean isUber() {\n    return isUber;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.addDiagnostic": "  protected void addDiagnostic(String diag) {\n    diagnostics.add(diag);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.createSplits": "    protected TaskSplitMetaInfo[] createSplits(JobImpl job, JobId jobId) {\n      TaskSplitMetaInfo[] allTaskSplitMetaInfo;\n      try {\n        allTaskSplitMetaInfo = SplitMetaInfoReader.readSplitMetaInfo(\n            job.oldJobId, job.fs, \n            job.conf, \n            job.remoteJobSubmitDir);\n      } catch (IOException e) {\n        throw new YarnException(e);\n      }\n      return allTaskSplitMetaInfo;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.createReduceTasks": "    private void createReduceTasks(JobImpl job) {\n      for (int i = 0; i < job.numReduceTasks; i++) {\n        TaskImpl task =\n            new ReduceTaskImpl(job.jobId, i,\n                job.eventHandler, \n                job.remoteJobConfFile, \n                job.conf, job.numMapTasks, \n                job.taskAttemptListener, job.committer, job.jobToken,\n                job.fsTokens, job.clock,\n                job.completedTasksFromPreviousRun, \n                job.applicationAttemptId.getAttemptId(),\n                job.metrics, job.appContext);\n        job.addTask(task);\n      }\n      LOG.info(\"Number of reduces for job \" + job.jobId + \" = \"\n          + job.numReduceTasks);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.handle": "    public void handle(SpeculatorEvent event) {\n      if (disabled) {\n        return;\n      }\n\n      TaskId tId = event.getTaskID();\n      TaskType tType = null;\n      /* event's TaskId will be null if the event type is JOB_CREATE or\n       * ATTEMPT_STATUS_UPDATE\n       */\n      if (tId != null) {\n        tType = tId.getTaskType(); \n      }\n      boolean shouldMapSpec =\n              conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false);\n      boolean shouldReduceSpec =\n              conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false);\n\n      /* The point of the following is to allow the MAP and REDUCE speculative\n       * config values to be independent:\n       * IF spec-exec is turned on for maps AND the task is a map task\n       * OR IF spec-exec is turned on for reduces AND the task is a reduce task\n       * THEN call the speculator to handle the event.\n       */\n      if ( (shouldMapSpec && (tType == null || tType == TaskType.MAP))\n        || (shouldReduceSpec && (tType == null || tType == TaskType.REDUCE))) {\n        // Speculator IS enabled, direct the event to there.\n        speculator.handle(event);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJob": "    public Job getJob(JobId jobID) {\n      return jobs.get(jobID);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob": "  public void shutDownJob() {\n    // job has finished\n    // this is the only job, so shut down the Appmaster\n    // note in a workflow scenario, this may lead to creation of a new\n    // job (FIXME?)\n    // Send job-end notification\n    if (getConfig().get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL) != null) {\n      try {\n        LOG.info(\"Job end notification started for jobID : \"\n            + job.getReport().getJobId());\n        JobEndNotifier notifier = new JobEndNotifier();\n        notifier.setConf(getConfig());\n        notifier.notify(job.getReport());\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Job end notification interrupted for jobID : \"\n            + job.getReport().getJobId(), ie);\n      }\n    }\n\n    // TODO:currently just wait for some time so clients can know the\n    // final states. Will be removed once RM come on.\n    try {\n      Thread.sleep(5000);\n    } catch (InterruptedException e) {\n      e.printStackTrace();\n    }\n\n    try {\n      //We are finishing cleanly so this is the last retry\n      isLastAMRetry = true;\n      // Stop all services\n      // This will also send the final report to the ResourceManager\n      LOG.info(\"Calling stop for all the services\");\n      MRAppMaster.this.stop();\n\n    } catch (Throwable t) {\n      LOG.warn(\"Graceful stop failed \", t);\n    }\n\n    //Bring the process down by force.\n    //Not needed after HADOOP-7140\n    LOG.info(\"Exiting MR AppMaster..GoodBye!\");\n    sysexit();   \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.start": "  public void start() {\n\n    amInfos = new LinkedList<AMInfo>();\n\n    // Pull completedTasks etc from recovery\n    if (inRecovery) {\n      completedTasksFromPreviousRun = recoveryServ.getCompletedTasks();\n      amInfos = recoveryServ.getAMInfos();\n    } else {\n      // Get the amInfos anyways irrespective of whether recovery is enabled or\n      // not IF this is not the first AM generation\n      if (appAttemptID.getAttemptId() != 1) {\n        amInfos.addAll(readJustAMInfos());\n      }\n    }\n\n    // Current an AMInfo for the current AM generation.\n    AMInfo amInfo =\n        MRBuilderUtils.newAMInfo(appAttemptID, startTime, containerID, nmHost,\n            nmPort, nmHttpPort);\n    amInfos.add(amInfo);\n\n    // /////////////////// Create the job itself.\n    job = createJob(getConfig());\n\n    // End of creating the job.\n\n    // Send out an MR AM inited event for this AM and all previous AMs.\n    for (AMInfo info : amInfos) {\n      dispatcher.getEventHandler().handle(\n          new JobHistoryEvent(job.getID(), new AMStartedEvent(info\n              .getAppAttemptId(), info.getStartTime(), info.getContainerId(),\n              info.getNodeManagerHost(), info.getNodeManagerPort(), info\n                  .getNodeManagerHttpPort())));\n    }\n\n    // metrics system init is really init & start.\n    // It's more test friendly to put it here.\n    DefaultMetricsSystem.initialize(\"MRAppMaster\");\n\n    // create a job event for job intialization\n    JobEvent initJobEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT);\n    // Send init to the job (this does NOT trigger job execution)\n    // This is a synchronous call, not an event through dispatcher. We want\n    // job-init to be done completely here.\n    jobEventDispatcher.handle(initJobEvent);\n\n\n    // JobImpl's InitTransition is done (call above is synchronous), so the\n    // \"uber-decision\" (MR-1220) has been made.  Query job and switch to\n    // ubermode if appropriate (by registering different container-allocator\n    // and container-launcher services/event-handlers).\n\n    if (job.isUber()) {\n      speculatorEventDispatcher.disableSpeculation();\n      LOG.info(\"MRAppMaster uberizing job \" + job.getID()\n               + \" in local container (\\\"uber-AM\\\") on node \"\n               + nmHost + \":\" + nmPort + \".\");\n    } else {\n      // send init to speculator only for non-uber jobs. \n      // This won't yet start as dispatcher isn't started yet.\n      dispatcher.getEventHandler().handle(\n          new SpeculatorEvent(job.getID(), clock.getTime()));\n      LOG.info(\"MRAppMaster launching normal, non-uberized, multi-container \"\n               + \"job \" + job.getID() + \".\");\n    }\n\n    //start all the components\n    super.start();\n\n    // All components have started, start the job.\n    startJobs();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJobId": "  public JobId getJobId() {\n    return jobId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.getAttempt": "  public TaskAttempt getAttempt(TaskAttemptId attemptID) {\n    readLock.lock();\n    try {\n      return attempts.get(attemptID);\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.getAttempts": "  public Map<TaskAttemptId, TaskAttempt> getAttempts() {\n    readLock.lock();\n\n    try {\n      if (attempts.size() <= 1) {\n        return attempts;\n      }\n      \n      Map<TaskAttemptId, TaskAttempt> result\n          = new LinkedHashMap<TaskAttemptId, TaskAttempt>();\n      result.putAll(attempts);\n\n      return result;\n    } finally {\n      readLock.unlock();\n    }\n  }"
        },
        "bug_report": {
            "Title": "JobImpl.finished doesn't expect ERROR as a final job state",
            "Description": "TestMRApp.testJobError is causing AsyncDispatcher to exit with System.exit due to an exception being thrown.  From the console output from testJobError:\n\n{noformat}\n2012-11-27 18:46:15,240 ERROR [AsyncDispatcher event handler] impl.TaskImpl (TaskImpl.java:internalError(665)) - Invalid event T_SCHEDULE on Task task_0_0000_m_000000\n2012-11-27 18:46:15,242 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(132)) - Error in dispatcher thread\njava.lang.IllegalArgumentException: Illegal job state: ERROR\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.finished(JobImpl.java:838)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1622)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:359)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:299)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:287)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:445)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:723)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:974)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)\n\tat java.lang.Thread.run(Thread.java:662)\n2012-11-27 18:46:15,242 INFO  [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(135)) - Exiting, bbye..\n{noformat}\n"
        }
    },
    {
        "filename": "MAPREDUCE-3319.json",
        "creation_time": "2011-10-31T21:41:36.000+0000",
        "stack_trace": "```\njava.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to org.apache.hadoop.io.LongWritable\n\tat org.apache.hadoop.mapred.lib.LongSumReducer.reduce(LongSumReducer.java:44)\n\tat org.apache.hadoop.mapred.Task$OldCombinerRunner.combine(Task.java:1431)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1436)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1298)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:437)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.lib.LongSumReducer.reduce": "  public void reduce(K key, Iterator<LongWritable> values,\n                     OutputCollector<K, LongWritable> output,\n                     Reporter reporter)\n    throws IOException {\n\n    // sum all values for this key\n    long sum = 0;\n    while (values.hasNext()) {\n      sum += values.next().get();\n    }\n\n    // output sum\n    output.collect(key, new LongWritable(sum));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.sortAndSpill": "    private void sortAndSpill() throws IOException, ClassNotFoundException,\n                                       InterruptedException {\n      //approximate the length of the output file to be the length of the\n      //buffer + header lengths for the partitions\n      final long size = (bufend >= bufstart\n          ? bufend - bufstart\n          : (bufvoid - bufend) + bufstart) +\n                  partitions * APPROX_HEADER_LENGTH;\n      FSDataOutputStream out = null;\n      try {\n        // create spill file\n        final SpillRecord spillRec = new SpillRecord(partitions);\n        final Path filename =\n            mapOutputFile.getSpillFileForWrite(numSpills, size);\n        out = rfs.create(filename);\n\n        final int mstart = kvend / NMETA;\n        final int mend = 1 + // kvend is a valid record\n          (kvstart >= kvend\n          ? kvstart\n          : kvmeta.capacity() + kvstart) / NMETA;\n        sorter.sort(MapOutputBuffer.this, mstart, mend, reporter);\n        int spindex = mstart;\n        final IndexRecord rec = new IndexRecord();\n        final InMemValBytes value = new InMemValBytes();\n        for (int i = 0; i < partitions; ++i) {\n          IFile.Writer<K, V> writer = null;\n          try {\n            long segmentStart = out.getPos();\n            writer = new Writer<K, V>(job, out, keyClass, valClass, codec,\n                                      spilledRecordsCounter);\n            if (combinerRunner == null) {\n              // spill directly\n              DataInputBuffer key = new DataInputBuffer();\n              while (spindex < mend &&\n                  kvmeta.get(offsetFor(spindex % maxRec) + PARTITION) == i) {\n                final int kvoff = offsetFor(spindex % maxRec);\n                key.reset(kvbuffer, kvmeta.get(kvoff + KEYSTART),\n                          (kvmeta.get(kvoff + VALSTART) -\n                           kvmeta.get(kvoff + KEYSTART)));\n                getVBytesForOffset(kvoff, value);\n                writer.append(key, value);\n                ++spindex;\n              }\n            } else {\n              int spstart = spindex;\n              while (spindex < mend &&\n                  kvmeta.get(offsetFor(spindex % maxRec)\n                            + PARTITION) == i) {\n                ++spindex;\n              }\n              // Note: we would like to avoid the combiner if we've fewer\n              // than some threshold of records for a partition\n              if (spstart != spindex) {\n                combineCollector.setWriter(writer);\n                RawKeyValueIterator kvIter =\n                  new MRResultIterator(spstart, spindex);\n                combinerRunner.combine(kvIter, combineCollector);\n              }\n            }\n\n            // close the writer\n            writer.close();\n\n            // record offsets\n            rec.startOffset = segmentStart;\n            rec.rawLength = writer.getRawLength();\n            rec.partLength = writer.getCompressedLength();\n            spillRec.putIndex(rec, i);\n\n            writer = null;\n          } finally {\n            if (null != writer) writer.close();\n          }\n        }\n\n        if (totalIndexCacheMemory >= indexCacheMemoryLimit) {\n          // create spill index file\n          Path indexFilename =\n              mapOutputFile.getSpillIndexFileForWrite(numSpills, partitions\n                  * MAP_OUTPUT_INDEX_RECORD_LENGTH);\n          spillRec.writeToFile(indexFilename, job);\n        } else {\n          indexCacheList.add(spillRec);\n          totalIndexCacheMemory +=\n            spillRec.size() * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n        }\n        LOG.info(\"Finished spill \" + numSpills);\n        ++numSpills;\n      } finally {\n        if (out != null) out.close();\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.offsetFor": "    int offsetFor(int metapos) {\n      return kvmeta.get(metapos * NMETA + INDEX);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getPos": "    public long getPos() throws IOException { return rawIn.getPos(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.close": "      public void close() { }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.reset": "      public void reset(byte[] buffer, int start, int length) {\n        this.buffer = buffer;\n        this.start = start;\n        this.length = length;\n\n        if (start + length > bufvoid) {\n          this.buffer = new byte[this.length];\n          final int taillen = bufvoid - start;\n          System.arraycopy(buffer, start, this.buffer, 0, taillen);\n          System.arraycopy(buffer, 0, this.buffer, taillen, length-taillen);\n          this.start = 0;\n        }\n\n        super.reset(this.buffer, this.start, this.length);\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getVBytesForOffset": "    private void getVBytesForOffset(int kvoff, InMemValBytes vbytes) {\n      // get the keystart for the next serialized value to be the end\n      // of this value. If this is the last value in the buffer, use bufend\n      final int nextindex = kvoff == kvend\n        ? bufend\n        : kvmeta.get(\n            (kvoff - NMETA + kvmeta.capacity() + KEYSTART) % kvmeta.capacity());\n      // calculate the length of the value\n      int vallen = (nextindex >= kvmeta.get(kvoff + VALSTART))\n        ? nextindex - kvmeta.get(kvoff + VALSTART)\n        : (bufvoid - kvmeta.get(kvoff + VALSTART)) + nextindex;\n      vbytes.reset(kvbuffer, kvmeta.get(kvoff + VALSTART), vallen);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.flush": "    public void flush() throws IOException, ClassNotFoundException,\n           InterruptedException {\n      LOG.info(\"Starting flush of map output\");\n      spillLock.lock();\n      try {\n        while (spillInProgress) {\n          reporter.progress();\n          spillDone.await();\n        }\n        checkSpillException();\n\n        final int kvbend = 4 * kvend;\n        if ((kvbend + METASIZE) % kvbuffer.length !=\n            equator - (equator % METASIZE)) {\n          // spill finished\n          resetSpill();\n        }\n        if (kvindex != kvend) {\n          kvend = (kvindex + NMETA) % kvmeta.capacity();\n          bufend = bufmark;\n          if (LOG.isInfoEnabled()) {\n            LOG.info(\"Spilling map output\");\n            LOG.info(\"bufstart = \" + bufstart + \"; bufend = \" + bufmark +\n                     \"; bufvoid = \" + bufvoid);\n            LOG.info(\"kvstart = \" + kvstart + \"(\" + (kvstart * 4) +\n                     \"); kvend = \" + kvend + \"(\" + (kvend * 4) +\n                     \"); length = \" + (distanceTo(kvend, kvstart,\n                           kvmeta.capacity()) + 1) + \"/\" + maxRec);\n          }\n          sortAndSpill();\n        }\n      } catch (InterruptedException e) {\n        throw new IOException(\"Interrupted while waiting for the writer\", e);\n      } finally {\n        spillLock.unlock();\n      }\n      assert !spillLock.isHeldByCurrentThread();\n      // shut down spill thread and wait for it to exit. Since the preceding\n      // ensures that it is finished with its work (and sortAndSpill did not\n      // throw), we elect to use an interrupt instead of setting a flag.\n      // Spilling simultaneously from this thread while the spill thread\n      // finishes its work might be both a useful way to extend this and also\n      // sufficient motivation for the latter approach.\n      try {\n        spillThread.interrupt();\n        spillThread.join();\n      } catch (InterruptedException e) {\n        throw new IOException(\"Spill failed\", e);\n      }\n      // release sort buffer before the merge\n      kvbuffer = null;\n      mergeParts();\n      Path outputPath = mapOutputFile.getOutputFile();\n      fileOutputByteCounter.increment(rfs.getFileStatus(outputPath).getLen());\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.distanceTo": "    int distanceTo(final int i, final int j, final int mod) {\n      return i <= j\n        ? j - i\n        : mod - i + j;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.resetSpill": "    private void resetSpill() {\n      final int e = equator;\n      bufstart = bufend = e;\n      final int aligned = e - (e % METASIZE);\n      // set start/end to point to first meta record\n      kvstart = kvend =\n        ((aligned - METASIZE + kvbuffer.length) % kvbuffer.length) / 4;\n      if (LOG.isInfoEnabled()) {\n        LOG.info(\"(RESET) equator \" + e + \" kv \" + kvstart + \"(\" +\n          (kvstart * 4) + \")\" + \" kvi \" + kvindex + \"(\" + (kvindex * 4) + \")\");\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.checkSpillException": "    private void checkSpillException() throws IOException {\n      final Throwable lspillException = sortSpillException;\n      if (lspillException != null) {\n        if (lspillException instanceof Error) {\n          final String logMsg = \"Task \" + getTaskID() + \" failed : \" +\n            StringUtils.stringifyException(lspillException);\n          reportFatalError(getTaskID(), lspillException, logMsg);\n        }\n        throw new IOException(\"Spill failed\", lspillException);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.mergeParts": "    private void mergeParts() throws IOException, InterruptedException, \n                                     ClassNotFoundException {\n      // get the approximate size of the final output/index files\n      long finalOutFileSize = 0;\n      long finalIndexFileSize = 0;\n      final Path[] filename = new Path[numSpills];\n      final TaskAttemptID mapId = getTaskID();\n\n      for(int i = 0; i < numSpills; i++) {\n        filename[i] = mapOutputFile.getSpillFile(i);\n        finalOutFileSize += rfs.getFileStatus(filename[i]).getLen();\n      }\n      if (numSpills == 1) { //the spill is the final output\n        sameVolRename(filename[0],\n            mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n        if (indexCacheList.size() == 0) {\n          sameVolRename(mapOutputFile.getSpillIndexFile(0),\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n        } else {\n          indexCacheList.get(0).writeToFile(\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n        }\n        sortPhase.complete();\n        return;\n      }\n\n      // read in paged indices\n      for (int i = indexCacheList.size(); i < numSpills; ++i) {\n        Path indexFileName = mapOutputFile.getSpillIndexFile(i);\n        indexCacheList.add(new SpillRecord(indexFileName, job));\n      }\n\n      //make correction in the length to include the sequence file header\n      //lengths for each partition\n      finalOutFileSize += partitions * APPROX_HEADER_LENGTH;\n      finalIndexFileSize = partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n      Path finalOutputFile =\n          mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n      Path finalIndexFile =\n          mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n\n      //The output stream for the final single output file\n      FSDataOutputStream finalOut = rfs.create(finalOutputFile, true, 4096);\n\n      if (numSpills == 0) {\n        //create dummy files\n        IndexRecord rec = new IndexRecord();\n        SpillRecord sr = new SpillRecord(partitions);\n        try {\n          for (int i = 0; i < partitions; i++) {\n            long segmentStart = finalOut.getPos();\n            Writer<K, V> writer =\n              new Writer<K, V>(job, finalOut, keyClass, valClass, codec, null);\n            writer.close();\n            rec.startOffset = segmentStart;\n            rec.rawLength = writer.getRawLength();\n            rec.partLength = writer.getCompressedLength();\n            sr.putIndex(rec, i);\n          }\n          sr.writeToFile(finalIndexFile, job);\n        } finally {\n          finalOut.close();\n        }\n        sortPhase.complete();\n        return;\n      }\n      {\n        sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n        Merger.considerFinalMergeForProgress();\n        \n        IndexRecord rec = new IndexRecord();\n        final SpillRecord spillRec = new SpillRecord(partitions);\n        for (int parts = 0; parts < partitions; parts++) {\n          //create the segments to be merged\n          List<Segment<K,V>> segmentList =\n            new ArrayList<Segment<K, V>>(numSpills);\n          for(int i = 0; i < numSpills; i++) {\n            IndexRecord indexRecord = indexCacheList.get(i).getIndex(parts);\n\n            Segment<K,V> s =\n              new Segment<K,V>(job, rfs, filename[i], indexRecord.startOffset,\n                               indexRecord.partLength, codec, true);\n            segmentList.add(i, s);\n\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"MapId=\" + mapId + \" Reducer=\" + parts +\n                  \"Spill =\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                  indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n            }\n          }\n\n          int mergeFactor = job.getInt(JobContext.IO_SORT_FACTOR, 100);\n          // sort the segments only if there are intermediate merges\n          boolean sortSegments = segmentList.size() > mergeFactor;\n          //merge\n          @SuppressWarnings(\"unchecked\")\n          RawKeyValueIterator kvIter = Merger.merge(job, rfs,\n                         keyClass, valClass, codec,\n                         segmentList, mergeFactor,\n                         new Path(mapId.toString()),\n                         job.getOutputKeyComparator(), reporter, sortSegments,\n                         null, spilledRecordsCounter, sortPhase.phase());\n\n          //write merged output to disk\n          long segmentStart = finalOut.getPos();\n          Writer<K, V> writer =\n              new Writer<K, V>(job, finalOut, keyClass, valClass, codec,\n                               spilledRecordsCounter);\n          if (combinerRunner == null || numSpills < minSpillsForCombine) {\n            Merger.writeFile(kvIter, writer, reporter, job);\n          } else {\n            combineCollector.setWriter(writer);\n            combinerRunner.combine(kvIter, combineCollector);\n          }\n\n          //close\n          writer.close();\n\n          sortPhase.startNextPhase();\n          \n          // record offsets\n          rec.startOffset = segmentStart;\n          rec.rawLength = writer.getRawLength();\n          rec.partLength = writer.getCompressedLength();\n          spillRec.putIndex(rec, parts);\n        }\n        spillRec.writeToFile(finalIndexFile, job);\n        finalOut.close();\n        for(int i = 0; i < numSpills; i++) {\n          rfs.delete(filename[i],true);\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.runOldMapper": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runOldMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, InterruptedException,\n                             ClassNotFoundException {\n    InputSplit inputSplit = getSplitDetails(new Path(splitIndex.getSplitLocation()),\n           splitIndex.getStartOffset());\n\n    updateJobWithSplit(job, inputSplit);\n    reporter.setInputSplit(inputSplit);\n\n    RecordReader<INKEY,INVALUE> in = isSkipping() ? \n        new SkippingRecordReader<INKEY,INVALUE>(umbilical, reporter, job) :\n          new TrackedRecordReader<INKEY,INVALUE>(reporter, job);\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n\n\n    int numReduceTasks = conf.getNumReduceTasks();\n    LOG.info(\"numReduceTasks: \" + numReduceTasks);\n    MapOutputCollector collector = null;\n    if (numReduceTasks > 0) {\n      collector = new MapOutputBuffer(umbilical, job, reporter);\n    } else { \n      collector = new DirectMapOutputCollector(umbilical, job, reporter);\n    }\n    MapRunnable<INKEY,INVALUE,OUTKEY,OUTVALUE> runner =\n      ReflectionUtils.newInstance(job.getMapRunnerClass(), job);\n\n    try {\n      runner.run(in, new OldOutputCollector(collector, conf), reporter);\n      mapPhase.complete();\n      // start the sort phase only if there are reducers\n      if (numReduceTasks > 0) {\n        setPhase(TaskStatus.Phase.SORT);\n      }\n      statusUpdate(umbilical);\n      collector.flush();\n    } finally {\n      //close\n      in.close();                               // close input\n      collector.close();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.run": "      public void run() {\n        spillLock.lock();\n        spillThreadRunning = true;\n        try {\n          while (true) {\n            spillDone.signal();\n            while (!spillInProgress) {\n              spillReady.await();\n            }\n            try {\n              spillLock.unlock();\n              sortAndSpill();\n            } catch (Throwable t) {\n              sortSpillException = t;\n            } finally {\n              spillLock.lock();\n              if (bufend < bufstart) {\n                bufvoid = kvbuffer.length;\n              }\n              kvstart = kvend;\n              bufstart = bufend;\n              spillInProgress = false;\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        } finally {\n          spillLock.unlock();\n          spillThreadRunning = false;\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.updateJobWithSplit": "  private void updateJobWithSplit(final JobConf job, InputSplit inputSplit) {\n    if (inputSplit instanceof FileSplit) {\n      FileSplit fileSplit = (FileSplit) inputSplit;\n      job.set(JobContext.MAP_INPUT_FILE, fileSplit.getPath().toString());\n      job.setLong(JobContext.MAP_INPUT_START, fileSplit.getStart());\n      job.setLong(JobContext.MAP_INPUT_PATH, fileSplit.getLength());\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getSplitDetails": " private <T> T getSplitDetails(Path file, long offset) \n  throws IOException {\n   FileSystem fs = file.getFileSystem(conf);\n   FSDataInputStream inFile = fs.open(file);\n   inFile.seek(offset);\n   String className = Text.readString(inFile);\n   Class<T> cls;\n   try {\n     cls = (Class<T>) conf.getClassByName(className);\n   } catch (ClassNotFoundException ce) {\n     IOException wrap = new IOException(\"Split class \" + className + \n                                         \" not found\");\n     wrap.initCause(ce);\n     throw wrap;\n   }\n   SerializationFactory factory = new SerializationFactory(conf);\n   Deserializer<T> deserializer = \n     (Deserializer<T>) factory.getDeserializer(cls);\n   deserializer.open(inFile);\n   T split = deserializer.deserialize(null);\n   long pos = inFile.getPos();\n   getCounters().findCounter(\n       TaskCounter.SPLIT_RAW_BYTES).increment(pos - offset);\n   inFile.close();\n   return split;\n }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(pae,\"Unknown exception in doAs\");\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.getCompressedLength": "    public long getCompressedLength() {\n      return compressedBytesWritten;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.getRawLength": "    public long getRawLength() {\n      return decompressedBytesWritten;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.SpillRecord.putIndex": "  public void putIndex(IndexRecord rec, int partition) {\n    final int pos = partition * MapTask.MAP_OUTPUT_INDEX_RECORD_LENGTH / 8;\n    entries.put(pos, rec.startOffset);\n    entries.put(pos + 1, rec.rawLength);\n    entries.put(pos + 2, rec.partLength);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.SpillRecord.writeToFile": "  public void writeToFile(Path loc, JobConf job, Checksum crc)\n      throws IOException {\n    final FileSystem rfs = FileSystem.getLocal(job).getRaw();\n    CheckedOutputStream chk = null;\n    final FSDataOutputStream out = rfs.create(loc);\n    try {\n      if (crc != null) {\n        crc.reset();\n        chk = new CheckedOutputStream(out, crc);\n        chk.write(buf.array());\n        out.writeLong(chk.getChecksum().getValue());\n      } else {\n        out.write(buf.array());\n      }\n    } finally {\n      if (chk != null) {\n        chk.close();\n      } else {\n        out.close();\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.SpillRecord.size": "  public int size() {\n    return entries.capacity() / (MapTask.MAP_OUTPUT_INDEX_RECORD_LENGTH / 8);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.append": "    public void append(DataInputBuffer key, DataInputBuffer value)\n    throws IOException {\n      int keyLength = key.getLength() - key.getPosition();\n      if (keyLength < 0) {\n        throw new IOException(\"Negative key-length not allowed: \" + keyLength + \n                              \" for \" + key);\n      }\n      \n      int valueLength = value.getLength() - value.getPosition();\n      if (valueLength < 0) {\n        throw new IOException(\"Negative value-length not allowed: \" + \n                              valueLength + \" for \" + value);\n      }\n\n      WritableUtils.writeVInt(out, keyLength);\n      WritableUtils.writeVInt(out, valueLength);\n      out.write(key.getData(), key.getPosition(), keyLength); \n      out.write(value.getData(), value.getPosition(), valueLength); \n\n      // Update bytes written\n      decompressedBytesWritten += keyLength + valueLength + \n                      WritableUtils.getVIntSize(keyLength) + \n                      WritableUtils.getVIntSize(valueLength);\n      ++numRecordsWritten;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.reset": "    public void reset(int offset) {\n      return;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.getPosition": "    public long getPosition() throws IOException {    \n      return checksumIn.getPosition(); \n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IFile.getLength": "    public long getLength() { \n      return fileLength - checksumIn.getSize();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.mapOutputFile.getSpillFileForWrite": "  public abstract Path getSpillFileForWrite(int spillNumber, long size)\n      throws IOException;\n\n  /**\n   * Return a local map spill index file created earlier\n   *\n   * @param spillNumber the number\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillIndexFile(int spillNumber) throws IOException;\n\n  /**\n   * Create a local map spill index file name.\n   *\n   * @param spillNumber the number\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillIndexFileForWrite(int spillNumber, long size)\n      throws IOException;\n\n  /**\n   * Return a local reduce input file created earlier\n   *\n   * @param mapId a map task id\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFile(int mapId) throws IOException;\n\n  /**\n   * Create a local reduce input file name.\n   *\n   * @param mapId a map task id\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFileForWrite(\n      org.apache.hadoop.mapreduce.TaskID mapId, long size) throws IOException;\n\n  /** Removes all of the files related to a task. */\n  public abstract void removeAll() throws IOException;\n\n  @Override\n  public void setConf(Configuration conf) {\n    this.conf = conf;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.mapOutputFile.getSpillIndexFileForWrite": "  public abstract Path getSpillIndexFileForWrite(int spillNumber, long size)\n      throws IOException;\n\n  /**\n   * Return a local reduce input file created earlier\n   *\n   * @param mapId a map task id\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFile(int mapId) throws IOException;\n\n  /**\n   * Create a local reduce input file name.\n   *\n   * @param mapId a map task id\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFileForWrite(\n      org.apache.hadoop.mapreduce.TaskID mapId, long size) throws IOException;\n\n  /** Removes all of the files related to a task. */\n  public abstract void removeAll() throws IOException;\n\n  @Override\n  public void setConf(Configuration conf) {\n    this.conf = conf;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.mapOutputFile.getOutputFile": "  public abstract Path getOutputFile() throws IOException;\n\n  /**\n   * Create a local map output file name.\n   *\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getOutputFileForWrite(long size) throws IOException;\n\n  /**\n   * Create a local map output file name on the same volume.\n   */\n  public abstract Path getOutputFileForWriteInVolume(Path existing);\n\n  /**\n   * Return the path to a local map output index file created earlier\n   *\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getOutputIndexFile() throws IOException;\n\n  /**\n   * Create a local map output index file name.\n   *\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getOutputIndexFileForWrite(long size) throws IOException;\n\n  /**\n   * Create a local map output index file name on the same volume.\n   */\n  public abstract Path getOutputIndexFileForWriteInVolume(Path existing);\n\n  /**\n   * Return a local map spill file created earlier.\n   *\n   * @param spillNumber the number\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillFile(int spillNumber) throws IOException;\n\n  /**\n   * Create a local map spill file name.\n   *\n   * @param spillNumber the number\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillFileForWrite(int spillNumber, long size)\n      throws IOException;\n\n  /**\n   * Return a local map spill index file created earlier\n   *\n   * @param spillNumber the number\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillIndexFile(int spillNumber) throws IOException;\n\n  /**\n   * Create a local map spill index file name.\n   *\n   * @param spillNumber the number\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getSpillIndexFileForWrite(int spillNumber, long size)\n      throws IOException;\n\n  /**\n   * Return a local reduce input file created earlier\n   *\n   * @param mapId a map task id\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFile(int mapId) throws IOException;\n\n  /**\n   * Create a local reduce input file name.\n   *\n   * @param mapId a map task id\n   * @param size the size of the file\n   * @return path\n   * @throws IOException\n   */\n  public abstract Path getInputFileForWrite(\n      org.apache.hadoop.mapreduce.TaskID mapId, long size) throws IOException;\n\n  /** Removes all of the files related to a task. */\n  public abstract void removeAll() throws IOException;\n\n  @Override\n  public void setConf(Configuration conf) {\n    this.conf = conf;\n  }"
        },
        "bug_report": {
            "Title": "multifilewc from hadoop examples seems to be broken in 0.20.205.0",
            "Description": "{noformat}\n/usr/lib/hadoop/bin/hadoop jar /usr/lib/hadoop/hadoop-examples-0.20.205.0.22.jar multifilewc  examples/text examples-output/multifilewc\n11/10/31 16:50:26 INFO mapred.FileInputFormat: Total input paths to process : 2\n11/10/31 16:50:26 INFO mapred.JobClient: Running job: job_201110311350_0220\n11/10/31 16:50:27 INFO mapred.JobClient:  map 0% reduce 0%\n11/10/31 16:50:42 INFO mapred.JobClient: Task Id : attempt_201110311350_0220_m_000000_0, Status : FAILED\njava.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to org.apache.hadoop.io.LongWritable\n\tat org.apache.hadoop.mapred.lib.LongSumReducer.reduce(LongSumReducer.java:44)\n\tat org.apache.hadoop.mapred.Task$OldCombinerRunner.combine(Task.java:1431)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1436)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1298)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:437)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\n{noformat}"
        }
    },
    {
        "filename": "MAPREDUCE-6361.json",
        "creation_time": "2015-05-11T15:36:10.000+0000",
        "stack_trace": "```\nException running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#25\n         at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)\n         at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)\n         at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n         at java.security.AccessController.doPrivileged(Native Method)\n         at javax.security.auth.Subject.doAs(Subject.java:415)\n         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n         at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.lang.NullPointerException\n         at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:267)\n         at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:308)\n         at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.Shuffle.run": "  public RawKeyValueIterator run() throws IOException, InterruptedException {\n    // Scale the maximum events we fetch per RPC call to mitigate OOM issues\n    // on the ApplicationMaster when a thundering herd of reducers fetch events\n    // TODO: This should not be necessary after HADOOP-8942\n    int eventsPerReducer = Math.max(MIN_EVENTS_TO_FETCH,\n        MAX_RPC_OUTSTANDING_EVENTS / jobConf.getNumReduceTasks());\n    int maxEventsToFetch = Math.min(MAX_EVENTS_TO_FETCH, eventsPerReducer);\n\n    // Start the map-completion events fetcher thread\n    final EventFetcher<K,V> eventFetcher = \n      new EventFetcher<K,V>(reduceId, umbilical, scheduler, this,\n          maxEventsToFetch);\n    eventFetcher.start();\n    \n    // Start the map-output fetcher threads\n    boolean isLocal = localMapFiles != null;\n    final int numFetchers = isLocal ? 1 :\n      jobConf.getInt(MRJobConfig.SHUFFLE_PARALLEL_COPIES, 5);\n    Fetcher<K,V>[] fetchers = new Fetcher[numFetchers];\n    if (isLocal) {\n      fetchers[0] = new LocalFetcher<K, V>(jobConf, reduceId, scheduler,\n          merger, reporter, metrics, this, reduceTask.getShuffleSecret(),\n          localMapFiles);\n      fetchers[0].start();\n    } else {\n      for (int i=0; i < numFetchers; ++i) {\n        fetchers[i] = new Fetcher<K,V>(jobConf, reduceId, scheduler, merger, \n                                       reporter, metrics, this, \n                                       reduceTask.getShuffleSecret());\n        fetchers[i].start();\n      }\n    }\n    \n    // Wait for shuffle to complete successfully\n    while (!scheduler.waitUntilDone(PROGRESS_FREQUENCY)) {\n      reporter.progress();\n      \n      synchronized (this) {\n        if (throwable != null) {\n          throw new ShuffleError(\"error in shuffle in \" + throwingThreadName,\n                                 throwable);\n        }\n      }\n    }\n\n    // Stop the event-fetcher thread\n    eventFetcher.shutDown();\n    \n    // Stop the map-output fetcher threads\n    for (Fetcher<K,V> fetcher : fetchers) {\n      fetcher.shutDown();\n    }\n    \n    // stop the scheduler\n    scheduler.close();\n\n    copyPhase.complete(); // copy is already complete\n    taskStatus.setPhase(TaskStatus.Phase.SORT);\n    reduceTask.statusUpdate(umbilical);\n\n    // Finish the on-going merges...\n    RawKeyValueIterator kvIter = null;\n    try {\n      kvIter = merger.close();\n    } catch (Throwable e) {\n      throw new ShuffleError(\"Error while doing final merge \" , e);\n    }\n\n    // Sanity check\n    synchronized (this) {\n      if (throwable != null) {\n        throw new ShuffleError(\"error in shuffle in \" + throwingThreadName,\n                               throwable);\n      }\n    }\n    \n    return kvIter;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.Shuffle.close": "  public void close(){\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.run": "  public void run(JobConf job, final TaskUmbilicalProtocol umbilical)\n    throws IOException, InterruptedException, ClassNotFoundException {\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n\n    if (isMapOrReduce()) {\n      copyPhase = getProgress().addPhase(\"copy\");\n      sortPhase  = getProgress().addPhase(\"sort\");\n      reducePhase = getProgress().addPhase(\"reduce\");\n    }\n    // start thread that will handle communication with parent\n    TaskReporter reporter = startReporter(umbilical);\n    \n    boolean useNewApi = job.getUseNewReducer();\n    initialize(job, getJobID(), reporter, useNewApi);\n\n    // check if it is a cleanupJobTask\n    if (jobCleanup) {\n      runJobCleanupTask(umbilical, reporter);\n      return;\n    }\n    if (jobSetup) {\n      runJobSetupTask(umbilical, reporter);\n      return;\n    }\n    if (taskCleanup) {\n      runTaskCleanupTask(umbilical, reporter);\n      return;\n    }\n    \n    // Initialize the codec\n    codec = initCodec();\n    RawKeyValueIterator rIter = null;\n    ShuffleConsumerPlugin shuffleConsumerPlugin = null;\n    \n    Class combinerClass = conf.getCombinerClass();\n    CombineOutputCollector combineCollector = \n      (null != combinerClass) ? \n     new CombineOutputCollector(reduceCombineOutputCounter, reporter, conf) : null;\n\n    Class<? extends ShuffleConsumerPlugin> clazz =\n          job.getClass(MRConfig.SHUFFLE_CONSUMER_PLUGIN, Shuffle.class, ShuffleConsumerPlugin.class);\n\t\t\t\t\t\n    shuffleConsumerPlugin = ReflectionUtils.newInstance(clazz, job);\n    LOG.info(\"Using ShuffleConsumerPlugin: \" + shuffleConsumerPlugin);\n\n    ShuffleConsumerPlugin.Context shuffleContext = \n      new ShuffleConsumerPlugin.Context(getTaskID(), job, FileSystem.getLocal(job), umbilical, \n                  super.lDirAlloc, reporter, codec, \n                  combinerClass, combineCollector, \n                  spilledRecordsCounter, reduceCombineInputCounter,\n                  shuffledMapsCounter,\n                  reduceShuffleBytes, failedShuffleCounter,\n                  mergedMapOutputsCounter,\n                  taskStatus, copyPhase, sortPhase, this,\n                  mapOutputFile, localMapFiles);\n    shuffleConsumerPlugin.init(shuffleContext);\n\n    rIter = shuffleConsumerPlugin.run();\n\n    // free up the data structures\n    mapOutputFilesOnDisk.clear();\n    \n    sortPhase.complete();                         // sort is complete\n    setPhase(TaskStatus.Phase.REDUCE); \n    statusUpdate(umbilical);\n    Class keyClass = job.getMapOutputKeyClass();\n    Class valueClass = job.getMapOutputValueClass();\n    RawComparator comparator = job.getOutputValueGroupingComparator();\n\n    if (useNewApi) {\n      runNewReducer(job, umbilical, reporter, rIter, comparator, \n                    keyClass, valueClass);\n    } else {\n      runOldReducer(job, umbilical, reporter, rIter, comparator, \n                    keyClass, valueClass);\n    }\n\n    shuffleConsumerPlugin.close();\n    done(umbilical, reporter);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.newInstance": "         public Writable newInstance() { return new ReduceTask(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.runOldReducer": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runOldReducer(JobConf job,\n                     TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator<INKEY> comparator,\n                     Class<INKEY> keyClass,\n                     Class<INVALUE> valueClass) throws IOException {\n    Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer = \n      ReflectionUtils.newInstance(job.getReducerClass(), job);\n    // make output collector\n    String finalName = getOutputName(getPartition());\n\n    RecordWriter<OUTKEY, OUTVALUE> out = new OldTrackingRecordWriter<OUTKEY, OUTVALUE>(\n        this, job, reporter, finalName);\n    final RecordWriter<OUTKEY, OUTVALUE> finalOut = out;\n    \n    OutputCollector<OUTKEY,OUTVALUE> collector = \n      new OutputCollector<OUTKEY,OUTVALUE>() {\n        public void collect(OUTKEY key, OUTVALUE value)\n          throws IOException {\n          finalOut.write(key, value);\n          // indicate that progress update needs to be sent\n          reporter.progress();\n        }\n      };\n    \n    // apply reduce function\n    try {\n      //increment processed counter only if skipping feature is enabled\n      boolean incrProcCount = SkipBadRecords.getReducerMaxSkipGroups(job)>0 &&\n        SkipBadRecords.getAutoIncrReducerProcCount(job);\n      \n      ReduceValuesIterator<INKEY,INVALUE> values = isSkipping() ? \n          new SkippingReduceValuesIterator<INKEY,INVALUE>(rIter, \n              comparator, keyClass, valueClass, \n              job, reporter, umbilical) :\n          new ReduceValuesIterator<INKEY,INVALUE>(rIter, \n          comparator, keyClass, valueClass,\n          job, reporter);\n      values.informReduceProgress();\n      while (values.more()) {\n        reduceInputKeyCounter.increment(1);\n        reducer.reduce(values.getKey(), values, collector, reporter);\n        if(incrProcCount) {\n          reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n              SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);\n        }\n        values.nextKey();\n        values.informReduceProgress();\n      }\n\n      reducer.close();\n      reducer = null;\n      \n      out.close(reporter);\n      out = null;\n    } finally {\n      IOUtils.cleanup(LOG, reducer);\n      closeQuietly(out, reporter);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.close": "      public void close() throws IOException {\n        rawIter.close();\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.getProgress": "      public Progress getProgress() {\n        return rawIter.getProgress();\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.runNewReducer": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runNewReducer(JobConf job,\n                     final TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator<INKEY> comparator,\n                     Class<INKEY> keyClass,\n                     Class<INVALUE> valueClass\n                     ) throws IOException,InterruptedException, \n                              ClassNotFoundException {\n    // wrap value iterator to report progress.\n    final RawKeyValueIterator rawIter = rIter;\n    rIter = new RawKeyValueIterator() {\n      public void close() throws IOException {\n        rawIter.close();\n      }\n      public DataInputBuffer getKey() throws IOException {\n        return rawIter.getKey();\n      }\n      public Progress getProgress() {\n        return rawIter.getProgress();\n      }\n      public DataInputBuffer getValue() throws IOException {\n        return rawIter.getValue();\n      }\n      public boolean next() throws IOException {\n        boolean ret = rawIter.next();\n        reporter.setProgress(rawIter.getProgress().getProgress());\n        return ret;\n      }\n    };\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,\n          getTaskID(), reporter);\n    // make a reducer\n    org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer =\n      (org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE>)\n        ReflectionUtils.newInstance(taskContext.getReducerClass(), job);\n    org.apache.hadoop.mapreduce.RecordWriter<OUTKEY,OUTVALUE> trackedRW = \n      new NewTrackingRecordWriter<OUTKEY, OUTVALUE>(this, taskContext);\n    job.setBoolean(\"mapred.skip.on\", isSkipping());\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.Reducer.Context \n         reducerContext = createReduceContext(reducer, job, getTaskID(),\n                                               rIter, reduceInputKeyCounter, \n                                               reduceInputValueCounter, \n                                               trackedRW,\n                                               committer,\n                                               reporter, comparator, keyClass,\n                                               valueClass);\n    try {\n      reducer.run(reducerContext);\n    } finally {\n      trackedRW.close(reducerContext);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.initCodec": "  private CompressionCodec initCodec() {\n    // check if map-outputs are to be compressed\n    if (conf.getCompressMapOutput()) {\n      Class<? extends CompressionCodec> codecClass =\n        conf.getMapOutputCompressorClass(DefaultCodec.class);\n      return ReflectionUtils.newInstance(codecClass, conf);\n    } \n\n    return null;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.run": "              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.main": "  public static void main(String[] args) throws Throwable {\n    Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\n    LOG.debug(\"Child starting\");\n\n    final JobConf job = new JobConf(MRJobConfig.JOB_CONF_FILE);\n    // Initing with our JobConf allows us to avoid loading confs twice\n    Limits.init(job);\n    UserGroupInformation.setConfiguration(job);\n\n    String host = args[0];\n    int port = Integer.parseInt(args[1]);\n    final InetSocketAddress address =\n        NetUtils.createSocketAddrForHost(host, port);\n    final TaskAttemptID firstTaskid = TaskAttemptID.forName(args[2]);\n    long jvmIdLong = Long.parseLong(args[3]);\n    JVMId jvmId = new JVMId(firstTaskid.getJobID(),\n        firstTaskid.getTaskType() == TaskType.MAP, jvmIdLong);\n\n    // initialize metrics\n    DefaultMetricsSystem.initialize(\n        StringUtils.camelize(firstTaskid.getTaskType().name()) +\"Task\");\n\n    // Security framework already loaded the tokens into current ugi\n    Credentials credentials =\n        UserGroupInformation.getCurrentUser().getCredentials();\n    LOG.info(\"Executing with tokens:\");\n    for (Token<?> token: credentials.getAllTokens()) {\n      LOG.info(token);\n    }\n\n    // Create TaskUmbilicalProtocol as actual task owner.\n    UserGroupInformation taskOwner =\n      UserGroupInformation.createRemoteUser(firstTaskid.getJobID().toString());\n    Token<JobTokenIdentifier> jt = TokenCache.getJobToken(credentials);\n    SecurityUtil.setTokenService(jt, address);\n    taskOwner.addToken(jt);\n    final TaskUmbilicalProtocol umbilical =\n      taskOwner.doAs(new PrivilegedExceptionAction<TaskUmbilicalProtocol>() {\n      @Override\n      public TaskUmbilicalProtocol run() throws Exception {\n        return (TaskUmbilicalProtocol)RPC.getProxy(TaskUmbilicalProtocol.class,\n            TaskUmbilicalProtocol.versionID, address, job);\n      }\n    });\n\n    // report non-pid to application master\n    JvmContext context = new JvmContext(jvmId, \"-1000\");\n    LOG.debug(\"PID: \" + System.getenv().get(\"JVM_PID\"));\n    Task task = null;\n    UserGroupInformation childUGI = null;\n    ScheduledExecutorService logSyncer = null;\n\n    try {\n      int idleLoopCount = 0;\n      JvmTask myTask = null;;\n      // poll for new task\n      for (int idle = 0; null == myTask; ++idle) {\n        long sleepTimeMilliSecs = Math.min(idle * 500, 1500);\n        LOG.info(\"Sleeping for \" + sleepTimeMilliSecs\n            + \"ms before retrying again. Got null now.\");\n        MILLISECONDS.sleep(sleepTimeMilliSecs);\n        myTask = umbilical.getTask(context);\n      }\n      if (myTask.shouldDie()) {\n        return;\n      }\n\n      task = myTask.getTask();\n      YarnChild.taskid = task.getTaskID();\n\n      // Create the job-conf and set credentials\n      configureTask(job, task, credentials, jt);\n\n      // log the system properties\n      String systemPropsToLog = MRApps.getSystemPropertiesToLog(job);\n      if (systemPropsToLog != null) {\n        LOG.info(systemPropsToLog);\n      }\n\n      // Initiate Java VM metrics\n      JvmMetrics.initSingleton(jvmId.toString(), job.getSessionId());\n      childUGI = UserGroupInformation.createRemoteUser(System\n          .getenv(ApplicationConstants.Environment.USER.toString()));\n      // Add tokens to new user so that it may execute its task correctly.\n      childUGI.addCredentials(credentials);\n\n      // set job classloader if configured before invoking the task\n      MRApps.setJobClassLoader(job);\n\n      logSyncer = TaskLog.createLogSyncer();\n\n      // Create a final reference to the task for the doAs block\n      final Task taskFinal = task;\n      childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n        @Override\n        public Object run() throws Exception {\n          // use job-specified working directory\n          FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory());\n          taskFinal.run(job, umbilical); // run the task\n          return null;\n        }\n      });\n    } catch (FSError e) {\n      LOG.fatal(\"FSError from child\", e);\n      if (!ShutdownHookManager.get().isShutdownInProgress()) {\n        umbilical.fsError(taskid, e.getMessage());\n      }\n    } catch (Exception exception) {\n      LOG.warn(\"Exception running child : \"\n          + StringUtils.stringifyException(exception));\n      try {\n        if (task != null) {\n          // do cleanup for the task\n          if (childUGI == null) { // no need to job into doAs block\n            task.taskCleanup(umbilical);\n          } else {\n            final Task taskFinal = task;\n            childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n              @Override\n              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }\n            });\n          }\n        }\n      } catch (Exception e) {\n        LOG.info(\"Exception cleaning up: \" + StringUtils.stringifyException(e));\n      }\n      // Report back any failures, for diagnostic purposes\n      if (taskid != null) {\n        if (!ShutdownHookManager.get().isShutdownInProgress()) {\n          umbilical.fatalError(taskid,\n              StringUtils.stringifyException(exception));\n        }\n      }\n    } catch (Throwable throwable) {\n      LOG.fatal(\"Error running child : \"\n    \t        + StringUtils.stringifyException(throwable));\n      if (taskid != null) {\n        if (!ShutdownHookManager.get().isShutdownInProgress()) {\n          Throwable tCause = throwable.getCause();\n          String cause =\n              tCause == null ? throwable.getMessage() : StringUtils\n                  .stringifyException(tCause);\n          umbilical.fatalError(taskid, cause);\n        }\n      }\n    } finally {\n      RPC.stopProxy(umbilical);\n      DefaultMetricsSystem.shutdown();\n      TaskLog.syncLogsShutdown(logSyncer);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.configureTask": "  private static void configureTask(JobConf job, Task task,\n      Credentials credentials, Token<JobTokenIdentifier> jt) throws IOException {\n    job.setCredentials(credentials);\n    \n    ApplicationAttemptId appAttemptId =\n        ConverterUtils.toContainerId(\n            System.getenv(Environment.CONTAINER_ID.name()))\n            .getApplicationAttemptId();\n    LOG.debug(\"APPLICATION_ATTEMPT_ID: \" + appAttemptId);\n    // Set it in conf, so as to be able to be used the the OutputCommitter.\n    job.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID,\n        appAttemptId.getAttemptId());\n\n    // set tcp nodelay\n    job.setBoolean(\"ipc.client.tcpnodelay\", true);\n    job.setClass(MRConfig.TASK_LOCAL_OUTPUT_CLASS,\n        YarnOutputFiles.class, MapOutputFile.class);\n    // set the jobToken and shuffle secrets into task\n    task.setJobTokenSecret(\n        JobTokenSecretManager.createSecretKey(jt.getPassword()));\n    byte[] shuffleSecret = TokenCache.getShuffleSecretKey(credentials);\n    if (shuffleSecret == null) {\n      LOG.warn(\"Shuffle secret missing from task credentials.\"\n          + \" Using job token secret as shuffle secret.\");\n      shuffleSecret = jt.getPassword();\n    }\n    task.setShuffleSecret(\n        JobTokenSecretManager.createSecretKey(shuffleSecret));\n\n    // setup the child's MRConfig.LOCAL_DIR.\n    configureLocalDirs(task, job);\n\n    // setup the child's attempt directories\n    // Do the task-type specific localization\n    task.localizeConfiguration(job);\n\n    // Set up the DistributedCache related configs\n    MRApps.setupDistributedCacheLocal(job);\n\n    // Overwrite the localized task jobconf which is linked to in the current\n    // work-dir.\n    Path localTaskFile = new Path(MRJobConfig.JOB_CONF_FILE);\n    writeLocalJobFile(localTaskFile, job);\n    task.setJobFile(localTaskFile.toString());\n    task.setConf(job);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed": "  public synchronized void copyFailed(TaskAttemptID mapId, MapHost host,\n      boolean readError, boolean connectExcpt) {\n    host.penalize();\n    int failures = 1;\n    if (failureCounts.containsKey(mapId)) {\n      IntWritable x = failureCounts.get(mapId);\n      x.set(x.get() + 1);\n      failures = x.get();\n    } else {\n      failureCounts.put(mapId, new IntWritable(1));\n    }\n    String hostname = host.getHostName();\n    //report failure if already retried maxHostFailures times\n    boolean hostFail = hostFailures.get(hostname).get() > getMaxHostFailures() ? true : false;\n    \n    if (failures >= abortFailureLimit) {\n      try {\n        throw new IOException(failures + \" failures downloading \" + mapId);\n      } catch (IOException ie) {\n        reporter.reportException(ie);\n      }\n    }\n\n    checkAndInformMRAppMaster(failures, mapId, readError, connectExcpt,\n        hostFail);\n\n    checkReducerHealth();\n\n    long delay = (long) (INITIAL_PENALTY *\n        Math.pow(PENALTY_GROWTH_RATE, failures));\n    if (delay > maxDelay) {\n      delay = maxDelay;\n    }\n\n    penalties.add(new Penalty(host, delay));\n\n    failedShuffleCounter.increment(1);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.checkReducerHealth": "  private void checkReducerHealth() {\n    final float MAX_ALLOWED_FAILED_FETCH_ATTEMPT_PERCENT = 0.5f;\n    final float MIN_REQUIRED_PROGRESS_PERCENT = 0.5f;\n    final float MAX_ALLOWED_STALL_TIME_PERCENT = 0.5f;\n\n    long totalFailures = failedShuffleCounter.getValue();\n    int doneMaps = totalMaps - remainingMaps;\n\n    boolean reducerHealthy =\n      (((float)totalFailures / (totalFailures + doneMaps))\n          < MAX_ALLOWED_FAILED_FETCH_ATTEMPT_PERCENT);\n\n    // check if the reducer has progressed enough\n    boolean reducerProgressedEnough =\n      (((float)doneMaps / totalMaps)\n          >= MIN_REQUIRED_PROGRESS_PERCENT);\n\n    // check if the reducer is stalled for a long time\n    // duration for which the reducer is stalled\n    int stallDuration =\n      (int)(Time.monotonicNow() - lastProgressTime);\n\n    // duration for which the reducer ran with progress\n    int shuffleProgressDuration =\n      (int)(lastProgressTime - startTime);\n\n    // min time the reducer should run without getting killed\n    int minShuffleRunDuration =\n      Math.max(shuffleProgressDuration, maxMapRuntime);\n\n    boolean reducerStalled =\n      (((float)stallDuration / minShuffleRunDuration)\n          >= MAX_ALLOWED_STALL_TIME_PERCENT);\n\n    // kill if not healthy and has insufficient progress\n    if ((failureCounts.size() >= maxFailedUniqueFetches ||\n        failureCounts.size() == (totalMaps - doneMaps))\n        && !reducerHealthy\n        && (!reducerProgressedEnough || reducerStalled)) {\n      LOG.fatal(\"Shuffle failed with too many fetch failures \" +\n      \"and insufficient progress!\");\n      String errorMsg = \"Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out.\";\n      reporter.reportException(new IOException(errorMsg));\n    }\n\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.getMaxHostFailures": "  public int getMaxHostFailures() {\n    return maxHostFailures;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.checkAndInformMRAppMaster": "  private void checkAndInformMRAppMaster(\n      int failures, TaskAttemptID mapId, boolean readError,\n      boolean connectExcpt, boolean hostFailed) {\n    if (connectExcpt || (reportReadErrorImmediately && readError)\n        || ((failures % maxFetchFailuresBeforeReporting) == 0) || hostFailed) {\n      LOG.info(\"Reporting fetch failure for \" + mapId + \" to MRAppMaster.\");\n      status.addFetchFailedMap((org.apache.hadoop.mapred.TaskAttemptID) mapId);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.add": "    public void add(long s, long e) {\n      Interval interval = new Interval(s, e);\n      copyMillis = getTotalCopyMillis(interval);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost": "  protected void copyFromHost(MapHost host) throws IOException {\n    // reset retryStartTime for a new host\n    retryStartTime = 0;\n    // Get completed maps on 'host'\n    List<TaskAttemptID> maps = scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only 'OBSOLETE' maps, \n    // especially at the tail of large jobs\n    if (maps.size() == 0) {\n      return;\n    }\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n        + maps);\n    }\n    \n    // List of maps to be fetched yet\n    Set<TaskAttemptID> remaining = new HashSet<TaskAttemptID>(maps);\n    \n    // Construct the url and connect\n    URL url = getMapOutputURL(host, maps);\n    DataInputStream input = openShuffleUrl(host, remaining, url);\n    if (input == null) {\n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, faildTasks is not null and we exit\n      // after putting back the remaining maps to the \n      // yet_to_be_fetched list and marking the failed tasks.\n      TaskAttemptID[] failedTasks = null;\n      while (!remaining.isEmpty() && failedTasks == null) {\n        try {\n          failedTasks = copyMapOutput(host, input, remaining, fetchRetryEnabled);\n        } catch (IOException e) {\n          //\n          // Setup connection again if disconnected by NM\n          connection.disconnect();\n          // Get map output from remaining tasks only.\n          url = getMapOutputURL(host, remaining);\n          input = openShuffleUrl(host, remaining, url);\n          if (input == null) {\n            return;\n          }\n        }\n      }\n      \n      if(failedTasks != null && failedTasks.length > 0) {\n        LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n        scheduler.hostFailed(host.getHostName());\n        for(TaskAttemptID left: failedTasks) {\n          scheduler.copyFailed(left, host, true, false);\n        }\n      }\n\n      // Sanity check\n      if (failedTasks == null && !remaining.isEmpty()) {\n        throw new IOException(\"server didn't return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n      input.close();\n      input = null;\n    } finally {\n      if (input != null) {\n        IOUtils.cleanup(LOG, input);\n        input = null;\n      }\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.Fetcher.openShuffleUrl": "  private DataInputStream openShuffleUrl(MapHost host,\n      Set<TaskAttemptID> remaining, URL url) {\n    DataInputStream input = null;\n\n    try {\n      setupConnectionsWithRetry(host, remaining, url);\n      if (stopped) {\n        abortConnect(host, remaining);\n      } else {\n        input = new DataInputStream(connection.getInputStream());\n      }\n    } catch (IOException ie) {\n      boolean connectExcpt = ie instanceof ConnectException;\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() +\n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      scheduler.hostFailed(host.getHostName());\n      for(TaskAttemptID left: remaining) {\n        scheduler.copyFailed(left, host, false, connectExcpt);\n      }\n\n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n\n    return input;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput": "  private TaskAttemptID[] copyMapOutput(MapHost host,\n                                DataInputStream input,\n                                Set<TaskAttemptID> remaining,\n                                boolean canRetry) throws IOException {\n    MapOutput<K,V> mapOutput = null;\n    TaskAttemptID mapId = null;\n    long decompressedLength = -1;\n    long compressedLength = -1;\n    \n    try {\n      long startTime = Time.monotonicNow();\n      int forReduce = -1;\n      //Read the shuffle header\n      try {\n        ShuffleHeader header = new ShuffleHeader();\n        header.readFields(input);\n        mapId = TaskAttemptID.forName(header.mapId);\n        compressedLength = header.compressedLength;\n        decompressedLength = header.uncompressedLength;\n        forReduce = header.forReduce;\n      } catch (IllegalArgumentException e) {\n        badIdErrs.increment(1);\n        LOG.warn(\"Invalid map id \", e);\n        //Don't know which one was bad, so consider all of them as bad\n        return remaining.toArray(new TaskAttemptID[remaining.size()]);\n      }\n\n      InputStream is = input;\n      is = CryptoUtils.wrapIfNecessary(jobConf, is, compressedLength);\n      compressedLength -= CryptoUtils.cryptoPadding(jobConf);\n      decompressedLength -= CryptoUtils.cryptoPadding(jobConf);\n      \n      // Do some basic sanity verification\n      if (!verifySanity(compressedLength, decompressedLength, forReduce,\n          remaining, mapId)) {\n        return new TaskAttemptID[] {mapId};\n      }\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"header: \" + mapId + \", len: \" + compressedLength + \n            \", decomp len: \" + decompressedLength);\n      }\n      \n      // Get the location for the map output - either in-memory or on-disk\n      try {\n        mapOutput = merger.reserve(mapId, decompressedLength, id);\n      } catch (IOException ioe) {\n        // kill this reduce attempt\n        ioErrs.increment(1);\n        scheduler.reportLocalError(ioe);\n        return EMPTY_ATTEMPT_ID_ARRAY;\n      }\n      \n      // Check if we can shuffle *now* ...\n      if (mapOutput == null) {\n        LOG.info(\"fetcher#\" + id + \" - MergeManager returned status WAIT ...\");\n        //Not an error but wait to process data.\n        return EMPTY_ATTEMPT_ID_ARRAY;\n      } \n      \n      // The codec for lz0,lz4,snappy,bz2,etc. throw java.lang.InternalError\n      // on decompression failures. Catching and re-throwing as IOException\n      // to allow fetch failure logic to be processed\n      try {\n        // Go!\n        LOG.info(\"fetcher#\" + id + \" about to shuffle output of map \"\n            + mapOutput.getMapId() + \" decomp: \" + decompressedLength\n            + \" len: \" + compressedLength + \" to \" + mapOutput.getDescription());\n        mapOutput.shuffle(host, is, compressedLength, decompressedLength,\n            metrics, reporter);\n      } catch (java.lang.InternalError e) {\n        LOG.warn(\"Failed to shuffle for fetcher#\"+id, e);\n        throw new IOException(e);\n      }\n      \n      // Inform the shuffle scheduler\n      long endTime = Time.monotonicNow();\n      // Reset retryStartTime as map task make progress if retried before.\n      retryStartTime = 0;\n      \n      scheduler.copySucceeded(mapId, host, compressedLength, \n                              startTime, endTime, mapOutput);\n      // Note successful shuffle\n      remaining.remove(mapId);\n      metrics.successFetch();\n      return null;\n    } catch (IOException ioe) {\n      if (mapOutput != null) {\n        mapOutput.abort();\n      }\n\n      if (canRetry) {\n        checkTimeoutOrRetry(host, ioe);\n      } \n      \n      ioErrs.increment(1);\n      if (mapId == null || mapOutput == null) {\n        LOG.warn(\"fetcher#\" + id + \" failed to read map header\" + \n                 mapId + \" decomp: \" + \n                 decompressedLength + \", \" + compressedLength, ioe);\n        if(mapId == null) {\n          return remaining.toArray(new TaskAttemptID[remaining.size()]);\n        } else {\n          return new TaskAttemptID[] {mapId};\n        }\n      }\n        \n      LOG.warn(\"Failed to shuffle output of \" + mapId + \n               \" from \" + host.getHostName(), ioe); \n\n      // Inform the shuffle-scheduler\n      metrics.failedFetch();\n      return new TaskAttemptID[] {mapId};\n    }\n\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.Fetcher.getMapOutputURL": "  private URL getMapOutputURL(MapHost host, Collection<TaskAttemptID> maps\n                              )  throws MalformedURLException {\n    // Get the base url\n    StringBuffer url = new StringBuffer(host.getBaseUrl());\n    \n    boolean first = true;\n    for (TaskAttemptID mapId : maps) {\n      if (!first) {\n        url.append(\",\");\n      }\n      url.append(mapId);\n      first = false;\n    }\n   \n    LOG.debug(\"MapOutput URL for \" + host + \" -> \" + url.toString());\n    return new URL(url.toString());\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.Fetcher.run": "  public void run() {\n    try {\n      while (!stopped && !Thread.currentThread().isInterrupted()) {\n        MapHost host = null;\n        try {\n          // If merge is on, block\n          merger.waitForResource();\n\n          // Get a host to shuffle from\n          host = scheduler.getHost();\n          metrics.threadBusy();\n\n          // Shuffle\n          copyFromHost(host);\n        } finally {\n          if (host != null) {\n            scheduler.freeHost(host);\n            metrics.threadFree();            \n          }\n        }\n      }\n    } catch (InterruptedException ie) {\n      return;\n    } catch (Throwable t) {\n      exceptionReporter.reportException(t);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.fetcher.shutDown": "  public void shutDown() throws InterruptedException {\n    this.stopped = true;\n    interrupt();\n    try {\n      join(5000);\n    } catch (InterruptedException ie) {\n      LOG.warn(\"Got interrupt while joining \" + getName(), ie);\n    }\n    if (sslFactory != null) {\n      sslFactory.destroy();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.fetcher.interrupt": "  public void interrupt() {\n    try {\n      closeConnection();\n    } finally {\n      super.interrupt();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.EventFetcher.shutDown": "  public void shutDown() {\n    this.stopped = true;\n    interrupt();\n    try {\n      join(5000);\n    } catch(InterruptedException ie) {\n      LOG.warn(\"Got interrupted while joining \" + getName(), ie);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ShuffleConsumerPlugin.init": "  public void init(Context<K, V> context);\n\n  public RawKeyValueIterator run() throws IOException, InterruptedException;\n\n  public void close();\n\n  @InterfaceAudience.LimitedPrivate(\"mapreduce\")\n  @InterfaceStability.Unstable\n  public static class Context<K,V> {\n    private final org.apache.hadoop.mapreduce.TaskAttemptID reduceId;\n    private final JobConf jobConf;\n    private final FileSystem localFS;\n    private final TaskUmbilicalProtocol umbilical;\n    private final LocalDirAllocator localDirAllocator;\n    private final Reporter reporter;\n    private final CompressionCodec codec;\n    private final Class<? extends Reducer> combinerClass;\n    private final CombineOutputCollector<K, V> combineCollector;\n    private final Counters.Counter spilledRecordsCounter;\n    private final Counters.Counter reduceCombineInputCounter;\n    private final Counters.Counter shuffledMapsCounter;\n    private final Counters.Counter reduceShuffleBytes;\n    private final Counters.Counter failedShuffleCounter;\n    private final Counters.Counter mergedMapOutputsCounter;\n    private final TaskStatus status;\n    private final Progress copyPhase;\n    private final Progress mergePhase;\n    private final Task reduceTask;\n    private final MapOutputFile mapOutputFile;\n    private final Map<TaskAttemptID, MapOutputFile> localMapFiles;\n\n    public Context(org.apache.hadoop.mapreduce.TaskAttemptID reduceId,\n                   JobConf jobConf, FileSystem localFS,\n                   TaskUmbilicalProtocol umbilical,\n                   LocalDirAllocator localDirAllocator,\n                   Reporter reporter, CompressionCodec codec,\n                   Class<? extends Reducer> combinerClass,\n                   CombineOutputCollector<K,V> combineCollector,\n                   Counters.Counter spilledRecordsCounter,\n                   Counters.Counter reduceCombineInputCounter,\n                   Counters.Counter shuffledMapsCounter,\n                   Counters.Counter reduceShuffleBytes,\n                   Counters.Counter failedShuffleCounter,\n                   Counters.Counter mergedMapOutputsCounter,\n                   TaskStatus status, Progress copyPhase, Progress mergePhase,\n                   Task reduceTask, MapOutputFile mapOutputFile,\n                   Map<TaskAttemptID, MapOutputFile> localMapFiles) {\n      this.reduceId = reduceId;\n      this.jobConf = jobConf;\n      this.localFS = localFS;\n      this. umbilical = umbilical;\n      this.localDirAllocator = localDirAllocator;\n      this.reporter = reporter;\n      this.codec = codec;\n      this.combinerClass = combinerClass;\n      this.combineCollector = combineCollector;\n      this.spilledRecordsCounter = spilledRecordsCounter;\n      this.reduceCombineInputCounter = reduceCombineInputCounter;\n      this.shuffledMapsCounter = shuffledMapsCounter;\n      this.reduceShuffleBytes = reduceShuffleBytes;\n      this.failedShuffleCounter = failedShuffleCounter;\n      this.mergedMapOutputsCounter = mergedMapOutputsCounter;\n      this.status = status;\n      this.copyPhase = copyPhase;\n      this.mergePhase = mergePhase;\n      this.reduceTask = reduceTask;\n      this.mapOutputFile = mapOutputFile;\n      this.localMapFiles = localMapFiles;\n    }\n\n    public org.apache.hadoop.mapreduce.TaskAttemptID getReduceId() {\n      return reduceId;\n    }\n    public JobConf getJobConf() {\n      return jobConf;\n    }\n    public FileSystem getLocalFS() {\n      return localFS;\n    }\n    public TaskUmbilicalProtocol getUmbilical() {\n      return umbilical;\n    }\n    public LocalDirAllocator getLocalDirAllocator() {\n      return localDirAllocator;\n    }\n    public Reporter getReporter() {\n      return reporter;\n    }\n    public CompressionCodec getCodec() {\n      return codec;\n    }\n    public Class<? extends Reducer> getCombinerClass() {\n      return combinerClass;\n    }\n    public CombineOutputCollector<K, V> getCombineCollector() {\n      return combineCollector;\n    }\n    public Counters.Counter getSpilledRecordsCounter() {\n      return spilledRecordsCounter;\n    }\n    public Counters.Counter getReduceCombineInputCounter() {\n      return reduceCombineInputCounter;\n    }\n    public Counters.Counter getShuffledMapsCounter() {\n      return shuffledMapsCounter;\n    }\n    public Counters.Counter getReduceShuffleBytes() {\n      return reduceShuffleBytes;\n    }\n    public Counters.Counter getFailedShuffleCounter() {\n      return failedShuffleCounter;\n    }\n    public Counters.Counter getMergedMapOutputsCounter() {\n      return mergedMapOutputsCounter;\n    }\n    public TaskStatus getStatus() {\n      return status;\n    }\n    public Progress getCopyPhase() {\n      return copyPhase;\n    }\n    public Progress getMergePhase() {\n      return mergePhase;\n    }\n    public Task getReduceTask() {\n      return reduceTask;\n    }\n    public MapOutputFile getMapOutputFile() {\n      return mapOutputFile;\n    }\n    public Map<TaskAttemptID, MapOutputFile> getLocalMapFiles() {\n      return localMapFiles;\n    }\n  } // end of public static class Context<K,V>",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.taskCleanup": "  void taskCleanup(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    // set phase for this task\n    setPhase(TaskStatus.Phase.CLEANUP);\n    getProgress().setStatus(\"cleanup\");\n    statusUpdate(umbilical);\n    LOG.info(\"Runnning cleanup for the task\");\n    // do the cleanup\n    committer.abortTask(taskContext);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.statusUpdate": "  public void statusUpdate(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    int retries = MAX_RETRIES;\n    while (true) {\n      try {\n        if (!umbilical.statusUpdate(getTaskID(), taskStatus).getTaskFound()) {\n          LOG.warn(\"Parent died.  Exiting \"+taskId);\n          System.exit(66);\n        }\n        taskStatus.clearStatus();\n        return;\n      } catch (InterruptedException ie) {\n        Thread.currentThread().interrupt(); // interrupt ourself\n      } catch (IOException ie) {\n        LOG.warn(\"Failure sending status update: \" + \n                  StringUtils.stringifyException(ie));\n        if (--retries == 0) {\n          throw ie;\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setPhase": "  protected synchronized void setPhase(TaskStatus.Phase phase){\n    this.taskStatus.setPhase(phase); \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setStatus": "    public void setStatus(String status) {\n      taskProgress.setStatus(normalizeStatus(status, conf));\n      // indicate that progress update needs to be sent\n      setProgressFlag();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getProgress": "    public float getProgress() {\n      return taskProgress.getProgress();\n    };",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getWorkingDirectory": "  public Path getWorkingDirectory() {\n    String name = get(JobContext.WORKING_DIR);\n    if (name != null) {\n      return new Path(name);\n    } else {\n      try {\n        Path dir = FileSystem.get(this).getWorkingDirectory();\n        set(JobContext.WORKING_DIR, dir.toString());\n        return dir;\n      } catch (IOException e) {\n        throw new RuntimeException(e);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.toString": "  public String toString() { \n    return appendTo(new StringBuilder(JVM)).toString();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.appendTo": "  protected StringBuilder appendTo(StringBuilder builder) {\n    return jobId.appendTo(builder).\n                 append(SEPARATOR).\n                 append(isMap ? 'm' : 'r').\n                 append(SEPARATOR).\n                 append(idFormat.format(jvmId));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getSessionId": "  public String getSessionId() {\n      return get(\"session.id\", \"\");\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.createLogSyncer": "  public static ScheduledExecutorService createLogSyncer() {\n    final ScheduledExecutorService scheduler =\n      Executors.newSingleThreadScheduledExecutor(\n        new ThreadFactory() {\n          @Override\n          public Thread newThread(Runnable r) {\n            final Thread t = Executors.defaultThreadFactory().newThread(r);\n            t.setDaemon(true);\n            t.setName(\"Thread for syncLogs\");\n            return t;\n          }\n        });\n    ShutdownHookManager.get().addShutdownHook(new Runnable() {\n        @Override\n        public void run() {\n          TaskLog.syncLogsShutdown(scheduler);\n        }\n      }, 50);\n    scheduler.scheduleWithFixedDelay(\n        new Runnable() {\n          @Override\n          public void run() {\n            TaskLog.syncLogs();\n          }\n        }, 0L, 5L, TimeUnit.SECONDS);\n    return scheduler;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.newThread": "          public Thread newThread(Runnable r) {\n            final Thread t = Executors.defaultThreadFactory().newThread(r);\n            t.setDaemon(true);\n            t.setName(\"Thread for syncLogs\");\n            return t;\n          }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.syncLogsShutdown": "  public static synchronized void syncLogsShutdown(\n    ScheduledExecutorService scheduler) \n  {\n    // flush standard streams\n    //\n    System.out.flush();\n    System.err.flush();\n\n    if (scheduler != null) {\n      scheduler.shutdownNow();\n    }\n\n    // flush & close all appenders\n    LogManager.shutdown(); \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.syncLogs": "  public static synchronized void syncLogs() {\n    // flush standard streams\n    //\n    System.out.flush();\n    System.err.flush();\n\n    // flush flushable appenders\n    //\n    final Logger rootLogger = Logger.getRootLogger();\n    flushAppenders(rootLogger);\n    final Enumeration<Logger> allLoggers = rootLogger.getLoggerRepository().\n      getCurrentLoggers();\n    while (allLoggers.hasMoreElements()) {\n      final Logger l = allLoggers.nextElement();\n      flushAppenders(l);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fsError": "  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.shouldDie": "  public boolean shouldDie() {\n    return shouldDie;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.getTask": "  public Task getTask() {\n    return t;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.getTask": "  JvmTask getTask(JvmContext context) throws IOException;\n  \n  /**\n   * Report child's progress to parent. Also invoked to report still alive (used\n   * to be in ping). It reports an AMFeedback used to propagate preemption requests.\n   * \n   * @param taskId task-id of the child\n   * @param taskStatus status of the child\n   * @throws IOException\n   * @throws InterruptedException\n   * @return True if the task is known\n   */\n  AMFeedback statusUpdate(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;\n  \n  /** Report error messages back to parent.  Calls should be sparing, since all\n   *  such messages are held in the job tracker.\n   *  @param taskid the id of the task involved\n   *  @param trace the text to report\n   */\n  void reportDiagnosticInfo(TaskAttemptID taskid, String trace) throws IOException;\n  \n  /**\n   * Report the record range which is going to process next by the Task.\n   * @param taskid the id of the task involved\n   * @param range the range of record sequence nos\n   * @throws IOException\n   */\n  void reportNextRecordRange(TaskAttemptID taskid, SortedRanges.Range range) \n    throws IOException;\n\n  /** Report that the task is successfully completed.  Failure is assumed if\n   * the task process exits without calling this.\n   * @param taskid task's id\n   */\n  void done(TaskAttemptID taskid) throws IOException;\n  \n  /** \n   * Report that the task is complete, but its commit is pending.\n   * \n   * @param taskId task's id\n   * @param taskStatus status of the child\n   * @throws IOException\n   */\n  void commitPending(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;  \n\n  /**\n   * Polling to know whether the task can go-ahead with commit \n   * @param taskid\n   * @return true/false \n   * @throws IOException\n   */\n  boolean canCommit(TaskAttemptID taskid) throws IOException;\n\n  /** Report that a reduce-task couldn't shuffle map-outputs.*/\n  void shuffleError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Report that the task encounted a local filesystem error.*/\n  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.getJobID": "  public JobID getJobID() {\n    return (JobID) super.getJobID();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fatalError": "  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.forName": "  public static TaskAttemptID forName(String str\n                                      ) throws IllegalArgumentException {\n    return (TaskAttemptID) \n             org.apache.hadoop.mapreduce.TaskAttemptID.forName(str);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getTaskID": "  public TaskAttemptID getTaskID() { return taskId; }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.MapHost.penalize": "  public synchronized void penalize() {\n    state = State.PENALIZED;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.MapHost.getHostName": "  public String getHostName() {\n    return hostName;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.exceptionReporter.reportException": "  void reportException(Throwable t);\n}"
        },
        "bug_report": {
            "Title": "NPE issue in shuffle caused by concurrent issue between copySucceeded() in one thread and copyFailed() in another thread on the same host",
            "Description": "The failure in log:\n2015-05-08 21:00:00,513 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#25\n         at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)\n         at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)\n         at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n         at java.security.AccessController.doPrivileged(Native Method)\n         at javax.security.auth.Subject.doAs(Subject.java:415)\n         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n         at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.lang.NullPointerException\n         at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:267)\n         at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:308)\n         at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)"
        }
    },
    {
        "filename": "MAPREDUCE-4848.json",
        "creation_time": "2012-12-05T04:01:47.000+0000",
        "stack_trace": "```\njava.lang.ClassCastException: org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl cannot be cast to org.apache.hadoop.mapred.TaskAttemptContext\n\tat org.apache.hadoop.mapred.OutputCommitter.recoverTask(OutputCommitter.java:284)\n\tat org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:361)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1211)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1177)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:958)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:926)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:918)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)\n\tat org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryService.java:285)\n\tat org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:281)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n\tat java.lang.Thread.run(Thread.java:619)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.OutputCommitter.recoverTask": "  void recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext taskContext\n      ) throws IOException {\n    recoverTask((TaskAttemptContext) taskContext);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService.handle": "    public void handle(Event event) {\n      if (!recoveryMode) {\n        // delegate to the dispatcher one\n        actualHandler.handle(event);\n        return;\n      }\n\n      else if (event.getType() == TaskEventType.T_SCHEDULE) {\n        TaskEvent taskEvent = (TaskEvent) event;\n        // delay the scheduling of new tasks till previous ones are recovered\n        if (completedTasks.get(taskEvent.getTaskID()) == null) {\n          LOG.debug(\"Adding to pending task events \"\n              + taskEvent.getTaskID());\n          pendingTaskScheduleEvents.add(taskEvent);\n          return;\n        }\n      }\n\n      else if (event.getType() == ContainerAllocator.EventType.CONTAINER_REQ) {\n        TaskAttemptId aId = ((ContainerAllocatorEvent) event).getAttemptID();\n        TaskAttemptInfo attInfo = getTaskAttemptInfo(aId);\n        LOG.debug(\"CONTAINER_REQ \" + aId);\n        sendAssignedEvent(aId, attInfo);\n        return;\n      }\n\n      else if (event.getType() == TaskCleaner.EventType.TASK_CLEAN) {\n        TaskAttemptId aId = ((TaskCleanupEvent) event).getAttemptID();\n        LOG.debug(\"TASK_CLEAN\");\n        actualHandler.handle(new TaskAttemptEvent(aId,\n            TaskAttemptEventType.TA_CLEANUP_DONE));\n        return;\n      }\n\n      else if (event.getType() == ContainerLauncher.EventType.CONTAINER_REMOTE_LAUNCH) {\n        TaskAttemptId aId = ((ContainerRemoteLaunchEvent) event)\n            .getTaskAttemptID();\n        TaskAttemptInfo attInfo = getTaskAttemptInfo(aId);\n        actualHandler.handle(new TaskAttemptContainerLaunchedEvent(aId,\n            attInfo.getShufflePort()));\n        // send the status update event\n        sendStatusUpdateEvent(aId, attInfo);\n\n        TaskAttemptState state = TaskAttemptState.valueOf(attInfo.getTaskStatus());\n        switch (state) {\n        case SUCCEEDED:\n          //recover the task output\n          TaskAttemptContext taskContext = new TaskAttemptContextImpl(getConfig(),\n              attInfo.getAttemptId());\n          try { \n            TaskType type = taskContext.getTaskAttemptID().getTaskID().getTaskType();\n            int numReducers = taskContext.getConfiguration().getInt(MRJobConfig.NUM_REDUCES, 1); \n            if(type == TaskType.REDUCE || (type == TaskType.MAP && numReducers <= 0)) {\n              committer.recoverTask(taskContext);\n              LOG.info(\"Recovered output from task attempt \" + attInfo.getAttemptId());\n            } else {\n              LOG.info(\"Will not try to recover output for \"\n                  + taskContext.getTaskAttemptID());\n            }\n          } catch (IOException e) {\n            LOG.error(\"Caught an exception while trying to recover task \"+aId, e);\n            actualHandler.handle(new JobDiagnosticsUpdateEvent(\n                aId.getTaskId().getJobId(), \"Error in recovering task output \" + \n                e.getMessage()));\n            actualHandler.handle(new JobEvent(aId.getTaskId().getJobId(),\n                JobEventType.INTERNAL_ERROR));\n          }\n          \n          // send the done event\n          LOG.info(\"Sending done event to recovered attempt \" + aId);\n          actualHandler.handle(new TaskAttemptEvent(aId,\n              TaskAttemptEventType.TA_DONE));\n          break;\n        case KILLED:\n          LOG.info(\"Sending kill event to recovered attempt \" + aId);\n          actualHandler.handle(new TaskAttemptEvent(aId,\n              TaskAttemptEventType.TA_KILL));\n          break;\n        default:\n          LOG.info(\"Sending fail event to recovered attempt \" + aId);\n          actualHandler.handle(new TaskAttemptEvent(aId,\n              TaskAttemptEventType.TA_FAILMSG));\n          break;\n        }\n        return;\n      }\n\n      else if (event.getType() == \n        ContainerLauncher.EventType.CONTAINER_REMOTE_CLEANUP) {\n        TaskAttemptId aId = ((ContainerLauncherEvent) event)\n          .getTaskAttemptID();\n        actualHandler.handle(\n           new TaskAttemptEvent(aId,\n                TaskAttemptEventType.TA_CONTAINER_CLEANED));\n        return;\n      }\n\n      // delegate to the actual handler\n      actualHandler.handle(event);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService.sendStatusUpdateEvent": "    private void sendStatusUpdateEvent(TaskAttemptId yarnAttemptID,\n        TaskAttemptInfo attemptInfo) {\n      LOG.info(\"Sending status update event to \" + yarnAttemptID);\n      TaskAttemptStatus taskAttemptStatus = new TaskAttemptStatus();\n      taskAttemptStatus.id = yarnAttemptID;\n      taskAttemptStatus.progress = 1.0f;\n      taskAttemptStatus.stateString = attemptInfo.getTaskStatus(); \n      // taskAttemptStatus.outputSize = attemptInfo.getOutputSize();\n      taskAttemptStatus.phase = Phase.CLEANUP;\n      org.apache.hadoop.mapreduce.Counters cntrs = attemptInfo.getCounters();\n      if (cntrs == null) {\n        taskAttemptStatus.counters = null;\n      } else {\n        taskAttemptStatus.counters = cntrs;\n      }\n      actualHandler.handle(new TaskAttemptStatusUpdateEvent(\n          taskAttemptStatus.id, taskAttemptStatus));\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService.sendAssignedEvent": "    private void sendAssignedEvent(TaskAttemptId yarnAttemptID,\n        TaskAttemptInfo attemptInfo) {\n      LOG.info(\"Sending assigned event to \" + yarnAttemptID);\n      ContainerId cId = attemptInfo.getContainerId();\n\n      NodeId nodeId =\n          ConverterUtils.toNodeId(attemptInfo.getHostname() + \":\"\n              + attemptInfo.getPort());\n      // Resource/Priority/ApplicationACLs are only needed while launching the\n      // container on an NM, these are already completed tasks, so setting them\n      // to null\n      Container container = BuilderUtils.newContainer(cId, nodeId,\n          attemptInfo.getTrackerName() + \":\" + attemptInfo.getHttpPort(),\n          null, null, null);\n      actualHandler.handle(new TaskAttemptContainerAssignedEvent(yarnAttemptID,\n          container, null));\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService.getTaskAttemptInfo": "  private TaskAttemptInfo getTaskAttemptInfo(TaskAttemptId id) {\n    TaskInfo taskInfo = completedTasks.get(id.getTaskId());\n    return taskInfo.getAllTaskAttempts().get(TypeConverter.fromYarn(id));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.transition": "    public void transition(TaskAttemptImpl taskAttempt, \n        TaskAttemptEvent event) {\n      TaskAttemptDiagnosticsUpdateEvent diagEvent =\n          (TaskAttemptDiagnosticsUpdateEvent) event;\n      LOG.info(\"Diagnostics report from \" + taskAttempt.attemptId + \": \"\n          + diagEvent.getDiagnosticInfo());\n      taskAttempt.addDiagnosticInfo(diagEvent.getDiagnosticInfo());\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.addDiagnosticInfo": "  private void addDiagnosticInfo(String diag) {\n    if (diag != null && !diag.equals(\"\")) {\n      diagnostics.add(diag);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.resolveHosts": "  protected String[] resolveHosts(String[] src) {\n    String[] result = new String[src.length];\n    for (int i = 0; i < src.length; i++) {\n      if (isIP(src[i])) {\n        result[i] = resolveHost(src[i]);\n      } else {\n        result[i] = src[i];\n      }\n    }\n    return result;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getNodeHttpAddress": "  public String getNodeHttpAddress() {\n    readLock.lock();\n    try {\n      return nodeHttpAddress;\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createRemoteTask": "  protected abstract org.apache.hadoop.mapred.Task createRemoteTask();\n\n  @Override\n  public TaskAttemptId getID() {\n    return attemptId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.setFinishTime": "  private void setFinishTime() {\n    //set the finish time only if launch time is set\n    if (launchTime != 0) {\n      finishTime = clock.getTime();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createTaskAttemptUnsuccessfulCompletionEvent": "      TaskAttemptUnsuccessfulCompletionEvent\n      createTaskAttemptUnsuccessfulCompletionEvent(TaskAttemptImpl taskAttempt,\n          TaskAttemptStateInternal attemptState) {\n    TaskAttemptUnsuccessfulCompletionEvent tauce =\n        new TaskAttemptUnsuccessfulCompletionEvent(\n            TypeConverter.fromYarn(taskAttempt.attemptId),\n            TypeConverter.fromYarn(taskAttempt.attemptId.getTaskId()\n                .getTaskType()), attemptState.toString(),\n            taskAttempt.finishTime,\n            taskAttempt.containerNodeId == null ? \"UNKNOWN\"\n                : taskAttempt.containerNodeId.getHost(),\n            taskAttempt.containerNodeId == null ? -1 \n                : taskAttempt.containerNodeId.getPort(),    \n            taskAttempt.nodeRackName == null ? \"UNKNOWN\" \n                : taskAttempt.nodeRackName,\n            StringUtils.join(\n                LINE_SEPARATOR, taskAttempt.getDiagnostics()), taskAttempt\n                .getProgressSplitBlock().burst());\n    return tauce;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.computeSlotMillis": "  private static long computeSlotMillis(TaskAttemptImpl taskAttempt) {\n    TaskType taskType = taskAttempt.getID().getTaskId().getTaskType();\n    int slotMemoryReq =\n        taskAttempt.getMemoryRequired(taskAttempt.conf, taskType);\n\n    int minSlotMemSize =\n        taskAttempt.appContext.getClusterInfo().getMinContainerCapability()\n            .getMemory();\n\n    int simSlotsRequired =\n        minSlotMemSize == 0 ? 0 : (int) Math.ceil((float) slotMemoryReq\n            / minSlotMemSize);\n\n    long slotMillisIncrement =\n        simSlotsRequired\n            * (taskAttempt.getFinishTime() - taskAttempt.getLaunchTime());\n    return slotMillisIncrement;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getShufflePort": "  public int getShufflePort() {\n    readLock.lock();\n    try {\n      return shufflePort;\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getLaunchTime": "  public long getLaunchTime() {\n    readLock.lock();\n    try {\n      return launchTime;\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getID": "  public TaskAttemptId getID() {\n    return attemptId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.logAttemptFinishedEvent": "  private void logAttemptFinishedEvent(TaskAttemptStateInternal state) {\n    //Log finished events only if an attempt started.\n    if (getLaunchTime() == 0) return; \n    if (attemptId.getTaskId().getTaskType() == TaskType.MAP) {\n      MapAttemptFinishedEvent mfe =\n         new MapAttemptFinishedEvent(TypeConverter.fromYarn(attemptId),\n         TypeConverter.fromYarn(attemptId.getTaskId().getTaskType()),\n         state.toString(),\n         this.reportedStatus.mapFinishTime,\n         finishTime,\n         this.containerNodeId == null ? \"UNKNOWN\"\n             : this.containerNodeId.getHost(),\n         this.containerNodeId == null ? -1 : this.containerNodeId.getPort(),\n         this.nodeRackName == null ? \"UNKNOWN\" : this.nodeRackName,\n         this.reportedStatus.stateString,\n         getCounters(),\n         getProgressSplitBlock().burst());\n         eventHandler.handle(\n           new JobHistoryEvent(attemptId.getTaskId().getJobId(), mfe));\n    } else {\n       ReduceAttemptFinishedEvent rfe =\n         new ReduceAttemptFinishedEvent(TypeConverter.fromYarn(attemptId),\n         TypeConverter.fromYarn(attemptId.getTaskId().getTaskType()),\n         state.toString(),\n         this.reportedStatus.shuffleFinishTime,\n         this.reportedStatus.sortFinishTime,\n         finishTime,\n         this.containerNodeId == null ? \"UNKNOWN\"\n             : this.containerNodeId.getHost(),\n         this.containerNodeId == null ? -1 : this.containerNodeId.getPort(),\n         this.nodeRackName == null ? \"UNKNOWN\" : this.nodeRackName,\n         this.reportedStatus.stateString,\n         getCounters(),\n         getProgressSplitBlock().burst());\n         eventHandler.handle(\n           new JobHistoryEvent(attemptId.getTaskId().getJobId(), rfe));\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createJobCounterUpdateEventTAFailed": "  private static JobCounterUpdateEvent createJobCounterUpdateEventTAFailed(\n      TaskAttemptImpl taskAttempt, boolean taskAlreadyCompleted) {\n    TaskType taskType = taskAttempt.getID().getTaskId().getTaskType();\n    JobCounterUpdateEvent jce = new JobCounterUpdateEvent(taskAttempt.getID().getTaskId().getJobId());\n    \n    long slotMillisIncrement = computeSlotMillis(taskAttempt);\n    \n    if (taskType == TaskType.MAP) {\n      jce.addCounterUpdate(JobCounter.NUM_FAILED_MAPS, 1);\n      if(!taskAlreadyCompleted) {\n        // dont double count the elapsed time\n        jce.addCounterUpdate(JobCounter.SLOTS_MILLIS_MAPS, slotMillisIncrement);\n      }\n    } else {\n      jce.addCounterUpdate(JobCounter.NUM_FAILED_REDUCES, 1);\n      if(!taskAlreadyCompleted) {\n        // dont double count the elapsed time\n        jce.addCounterUpdate(JobCounter.SLOTS_MILLIS_REDUCES, slotMillisIncrement);\n      }\n    }\n    return jce;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.updateProgressSplits": "  private void updateProgressSplits() {\n    double newProgress = reportedStatus.progress;\n    newProgress = Math.max(Math.min(newProgress, 1.0D), 0.0D);\n    Counters counters = reportedStatus.counters;\n    if (counters == null)\n      return;\n\n    WrappedProgressSplitsBlock splitsBlock = getProgressSplitBlock();\n    if (splitsBlock != null) {\n      long now = clock.getTime();\n      long start = getLaunchTime(); // TODO Ensure not 0\n\n      if (start != 0 && now - start <= Integer.MAX_VALUE) {\n        splitsBlock.getProgressWallclockTime().extend(newProgress,\n            (int) (now - start));\n      }\n\n      Counter cpuCounter = counters.findCounter(TaskCounter.CPU_MILLISECONDS);\n      if (cpuCounter != null && cpuCounter.getValue() <= Integer.MAX_VALUE) {\n        splitsBlock.getProgressCPUTime().extend(newProgress,\n            (int) cpuCounter.getValue()); // long to int? TODO: FIX. Same below\n      }\n\n      Counter virtualBytes = counters\n        .findCounter(TaskCounter.VIRTUAL_MEMORY_BYTES);\n      if (virtualBytes != null) {\n        splitsBlock.getProgressVirtualMemoryKbytes().extend(newProgress,\n            (int) (virtualBytes.getValue() / (MEMORY_SPLITS_RESOLUTION)));\n      }\n\n      Counter physicalBytes = counters\n        .findCounter(TaskCounter.PHYSICAL_MEMORY_BYTES);\n      if (physicalBytes != null) {\n        splitsBlock.getProgressPhysicalMemoryKbytes().extend(newProgress,\n            (int) (physicalBytes.getValue() / (MEMORY_SPLITS_RESOLUTION)));\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getNodeId": "  public NodeId getNodeId() {\n    readLock.lock();\n    try {\n      return containerNodeId;\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getFinishTime": "  public long getFinishTime() {\n    readLock.lock();\n    try {\n      return finishTime;\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createContainerLaunchContext": "  static ContainerLaunchContext createContainerLaunchContext(\n      Map<ApplicationAccessType, String> applicationACLs,\n      ContainerId containerID, Configuration conf,\n      Token<JobTokenIdentifier> jobToken, Task remoteTask,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Resource assignedCapability, WrappedJvmID jvmID,\n      TaskAttemptListener taskAttemptListener,\n      Credentials credentials) {\n\n    synchronized (commonContainerSpecLock) {\n      if (commonContainerSpec == null) {\n        commonContainerSpec = createCommonContainerLaunchContext(\n            applicationACLs, conf, jobToken, oldJobId, credentials);\n      }\n    }\n\n    // Fill in the fields needed per-container that are missing in the common\n    // spec.\n\n    // Setup environment by cloning from common env.\n    Map<String, String> env = commonContainerSpec.getEnvironment();\n    Map<String, String> myEnv = new HashMap<String, String>(env.size());\n    myEnv.putAll(env);\n    MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n\n    // Set up the launch command\n    List<String> commands = MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, jvmID);\n\n    // Duplicate the ByteBuffers for access by multiple containers.\n    Map<String, ByteBuffer> myServiceData = new HashMap<String, ByteBuffer>();\n    for (Entry<String, ByteBuffer> entry : commonContainerSpec\n                .getServiceData().entrySet()) {\n      myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n    }\n\n    // Construct the actual Container\n    ContainerLaunchContext container = BuilderUtils.newContainerLaunchContext(\n        containerID, commonContainerSpec.getUser(), assignedCapability,\n        commonContainerSpec.getLocalResources(), myEnv, commands,\n        myServiceData, commonContainerSpec.getContainerTokens().duplicate(),\n        applicationACLs);\n\n    return container;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createJobCounterUpdateEventTAKilled": "  private static JobCounterUpdateEvent createJobCounterUpdateEventTAKilled(\n      TaskAttemptImpl taskAttempt, boolean taskAlreadyCompleted) {\n    TaskType taskType = taskAttempt.getID().getTaskId().getTaskType();\n    JobCounterUpdateEvent jce = new JobCounterUpdateEvent(taskAttempt.getID().getTaskId().getJobId());\n    \n    long slotMillisIncrement = computeSlotMillis(taskAttempt);\n    \n    if (taskType == TaskType.MAP) {\n      jce.addCounterUpdate(JobCounter.NUM_KILLED_MAPS, 1);\n      if(!taskAlreadyCompleted) {\n        // dont double count the elapsed time\n        jce.addCounterUpdate(JobCounter.SLOTS_MILLIS_MAPS, slotMillisIncrement);\n      }\n    } else {\n      jce.addCounterUpdate(JobCounter.NUM_KILLED_REDUCES, 1);\n      if(!taskAlreadyCompleted) {\n        // dont double count the elapsed time\n        jce.addCounterUpdate(JobCounter.SLOTS_MILLIS_REDUCES, slotMillisIncrement);\n      }\n    }\n    return jce;\n  }  ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getState": "  public TaskAttemptState getState() {\n    readLock.lock();\n    try {\n      return getExternalState(stateMachine.getCurrentState());\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle": "  public void handle(TaskAttemptEvent event) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Processing \" + event.getTaskAttemptID() + \" of type \"\n          + event.getType());\n    }\n    writeLock.lock();\n    try {\n      final TaskAttemptStateInternal oldState = getInternalState()  ;\n      try {\n        stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state for \"\n            + this.attemptId, e);\n        eventHandler.handle(new JobDiagnosticsUpdateEvent(\n            this.attemptId.getTaskId().getJobId(), \"Invalid event \" + event.getType() + \n            \" on TaskAttempt \" + this.attemptId));\n        eventHandler.handle(new JobEvent(this.attemptId.getTaskId().getJobId(),\n            JobEventType.INTERNAL_ERROR));\n      }\n      if (oldState != getInternalState()) {\n          LOG.info(attemptId + \" TaskAttempt Transitioned from \" \n           + oldState + \" to \"\n           + getInternalState());\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getInternalState": "  public TaskAttemptStateInternal getInternalState() {\n    readLock.lock();\n    try {\n      return stateMachine.getCurrentState();\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.handle": "    public void handle(SpeculatorEvent event) {\n      if (disabled) {\n        return;\n      }\n\n      TaskId tId = event.getTaskID();\n      TaskType tType = null;\n      /* event's TaskId will be null if the event type is JOB_CREATE or\n       * ATTEMPT_STATUS_UPDATE\n       */\n      if (tId != null) {\n        tType = tId.getTaskType(); \n      }\n      boolean shouldMapSpec =\n              conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false);\n      boolean shouldReduceSpec =\n              conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false);\n\n      /* The point of the following is to allow the MAP and REDUCE speculative\n       * config values to be independent:\n       * IF spec-exec is turned on for maps AND the task is a map task\n       * OR IF spec-exec is turned on for reduces AND the task is a reduce task\n       * THEN call the speculator to handle the event.\n       */\n      if ( (shouldMapSpec && (tType == null || tType == TaskType.MAP))\n        || (shouldReduceSpec && (tType == null || tType == TaskType.REDUCE))) {\n        // Speculator IS enabled, direct the event to there.\n        speculator.handle(event);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJob": "    public Job getJob(JobId jobID) {\n      return jobs.get(jobID);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob": "  public void shutDownJob() {\n    // job has finished\n    // this is the only job, so shut down the Appmaster\n    // note in a workflow scenario, this may lead to creation of a new\n    // job (FIXME?)\n    // Send job-end notification\n    if (getConfig().get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL) != null) {\n      try {\n        LOG.info(\"Job end notification started for jobID : \"\n            + job.getReport().getJobId());\n        JobEndNotifier notifier = new JobEndNotifier();\n        notifier.setConf(getConfig());\n        notifier.notify(job.getReport());\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Job end notification interrupted for jobID : \"\n            + job.getReport().getJobId(), ie);\n      }\n    }\n\n    // TODO:currently just wait for some time so clients can know the\n    // final states. Will be removed once RM come on.\n    try {\n      Thread.sleep(5000);\n    } catch (InterruptedException e) {\n      e.printStackTrace();\n    }\n\n    try {\n      //We are finishing cleanly so this is the last retry\n      isLastAMRetry = true;\n      // Stop all services\n      // This will also send the final report to the ResourceManager\n      LOG.info(\"Calling stop for all the services\");\n      MRAppMaster.this.stop();\n\n    } catch (Throwable t) {\n      LOG.warn(\"Graceful stop failed \", t);\n    }\n\n    //Bring the process down by force.\n    //Not needed after HADOOP-7140\n    LOG.info(\"Exiting MR AppMaster..GoodBye!\");\n    sysexit();   \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.start": "  public void start() {\n\n    amInfos = new LinkedList<AMInfo>();\n\n    // Pull completedTasks etc from recovery\n    if (inRecovery) {\n      completedTasksFromPreviousRun = recoveryServ.getCompletedTasks();\n      amInfos = recoveryServ.getAMInfos();\n    } else {\n      // Get the amInfos anyways irrespective of whether recovery is enabled or\n      // not IF this is not the first AM generation\n      if (appAttemptID.getAttemptId() != 1) {\n        amInfos.addAll(readJustAMInfos());\n      }\n    }\n\n    // Current an AMInfo for the current AM generation.\n    AMInfo amInfo =\n        MRBuilderUtils.newAMInfo(appAttemptID, startTime, containerID, nmHost,\n            nmPort, nmHttpPort);\n    amInfos.add(amInfo);\n\n    // /////////////////// Create the job itself.\n    job = createJob(getConfig());\n\n    // End of creating the job.\n\n    // Send out an MR AM inited event for this AM and all previous AMs.\n    for (AMInfo info : amInfos) {\n      dispatcher.getEventHandler().handle(\n          new JobHistoryEvent(job.getID(), new AMStartedEvent(info\n              .getAppAttemptId(), info.getStartTime(), info.getContainerId(),\n              info.getNodeManagerHost(), info.getNodeManagerPort(), info\n                  .getNodeManagerHttpPort())));\n    }\n\n    // metrics system init is really init & start.\n    // It's more test friendly to put it here.\n    DefaultMetricsSystem.initialize(\"MRAppMaster\");\n\n    // create a job event for job intialization\n    JobEvent initJobEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT);\n    // Send init to the job (this does NOT trigger job execution)\n    // This is a synchronous call, not an event through dispatcher. We want\n    // job-init to be done completely here.\n    jobEventDispatcher.handle(initJobEvent);\n\n\n    // JobImpl's InitTransition is done (call above is synchronous), so the\n    // \"uber-decision\" (MR-1220) has been made.  Query job and switch to\n    // ubermode if appropriate (by registering different container-allocator\n    // and container-launcher services/event-handlers).\n\n    if (job.isUber()) {\n      speculatorEventDispatcher.disableSpeculation();\n      LOG.info(\"MRAppMaster uberizing job \" + job.getID()\n               + \" in local container (\\\"uber-AM\\\") on node \"\n               + nmHost + \":\" + nmPort + \".\");\n    } else {\n      // send init to speculator only for non-uber jobs. \n      // This won't yet start as dispatcher isn't started yet.\n      dispatcher.getEventHandler().handle(\n          new SpeculatorEvent(job.getID(), clock.getTime()));\n      LOG.info(\"MRAppMaster launching normal, non-uberized, multi-container \"\n               + \"job \" + job.getID() + \".\");\n    }\n\n    //start all the components\n    super.start();\n\n    // All components have started, start the job.\n    startJobs();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJobId": "  public JobId getJobId() {\n    return jobId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService.realDispatch": "    public void realDispatch(Event event) {\n      super.dispatch(event);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService.dispatch": "    public void dispatch(Event event) {\n      if (recoveryMode) {\n        if (event.getType() == TaskAttemptEventType.TA_CONTAINER_LAUNCHED) {\n          TaskAttemptInfo attInfo = getTaskAttemptInfo(((TaskAttemptEvent) event)\n              .getTaskAttemptID());\n          LOG.info(\"Recovered Attempt start time \" + attInfo.getStartTime());\n          clock.setTime(attInfo.getStartTime());\n\n        } else if (event.getType() == TaskAttemptEventType.TA_DONE\n            || event.getType() == TaskAttemptEventType.TA_FAILMSG\n            || event.getType() == TaskAttemptEventType.TA_KILL) {\n          TaskAttemptInfo attInfo = getTaskAttemptInfo(((TaskAttemptEvent) event)\n              .getTaskAttemptID());\n          LOG.info(\"Recovered Attempt finish time \" + attInfo.getFinishTime());\n          clock.setTime(attInfo.getFinishTime());\n        }\n\n        else if (event.getType() == TaskEventType.T_ATTEMPT_FAILED\n            || event.getType() == TaskEventType.T_ATTEMPT_KILLED\n            || event.getType() == TaskEventType.T_ATTEMPT_SUCCEEDED) {\n          TaskTAttemptEvent tEvent = (TaskTAttemptEvent) event;\n          LOG.info(\"Recovered Task attempt \" + tEvent.getTaskAttemptID());\n          TaskInfo taskInfo = completedTasks.get(tEvent.getTaskAttemptID()\n              .getTaskId());\n          taskInfo.getAllTaskAttempts().remove(\n              TypeConverter.fromYarn(tEvent.getTaskAttemptID()));\n          // remove the task info from completed tasks if all attempts are\n          // recovered\n          if (taskInfo.getAllTaskAttempts().size() == 0) {\n            completedTasks.remove(tEvent.getTaskAttemptID().getTaskId());\n            // checkForRecoveryComplete\n            LOG.info(\"CompletedTasks() \" + completedTasks.size());\n            if (completedTasks.size() == 0) {\n              recoveryMode = false;\n              clock.reset();\n              LOG.info(\"Setting the recovery mode to false. \" +\n                 \"Recovery is complete!\");\n\n              // send all pending tasks schedule events\n              for (TaskEvent tEv : pendingTaskScheduleEvents) {\n                actualHandler.handle(tEv);\n              }\n\n            }\n          }\n        }\n      }\n      realDispatch(event);\n    }"
        },
        "bug_report": {
            "Title": "TaskAttemptContext cast error during AM recovery",
            "Description": "Recently saw an AM that failed and tried to recover, but the subsequent attempt quickly exited with its own failure during recovery:\n\n{noformat}\n2012-12-05 02:33:36,752 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread\njava.lang.ClassCastException: org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl cannot be cast to org.apache.hadoop.mapred.TaskAttemptContext\n\tat org.apache.hadoop.mapred.OutputCommitter.recoverTask(OutputCommitter.java:284)\n\tat org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:361)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1211)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1177)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:958)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:926)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:918)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)\n\tat org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryService.java:285)\n\tat org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:281)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n\tat java.lang.Thread.run(Thread.java:619)\n2012-12-05 02:33:36,752 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye..\n{noformat}\n\nThe RM then launched a third AM attempt which succeeded. The third attempt saw basically no progress after parsing the history file from the second attempt and ran the job again from scratch."
        }
    },
    {
        "filename": "MAPREDUCE-3226.json",
        "creation_time": "2011-10-20T06:38:04.000+0000",
        "stack_trace": "```\njava.lang.Thread.State: TIMED_WAITING (sleeping)\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.mapreduce.task.reduce.EventFetcher.run(EventFetcher.java:71)\n\njava.lang.Thread.State: WAITING (on object monitor)\n        at java.lang.Object.wait(Native Method)\n        - waiting on <0xa94b23d8> (a org.apache.hadoop.mapreduce.task.reduce.EventFetcher)\n        at java.lang.Thread.join(Thread.java:1143)\n        - locked <0xa94b23d8> (a org.apache.hadoop.mapreduce.task.reduce.EventFetcher)\n        at java.lang.Thread.join(Thread.java:1196)\n        at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:135)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:367)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.EventFetcher.run": "  public void run() {\n    int failures = 0;\n    LOG.info(reduce + \" Thread started: \" + getName());\n    \n    try {\n      while (true) {\n        try {\n          int numNewMaps = getMapCompletionEvents();\n          failures = 0;\n          if (numNewMaps > 0) {\n            LOG.info(reduce + \": \" + \"Got \" + numNewMaps + \" new map-outputs\");\n          }\n          LOG.debug(\"GetMapEventsThread about to sleep for \" + SLEEP_TIME);\n          Thread.sleep(SLEEP_TIME);\n        } catch (IOException ie) {\n          LOG.info(\"Exception in getting events\", ie);\n          // check to see whether to abort\n          if (++failures >= MAX_RETRIES) {\n            throw new IOException(\"too many failures downloading events\", ie);\n          }\n          // sleep for a bit\n          Thread.sleep(RETRY_PERIOD);\n        }\n      }\n    } catch (InterruptedException e) {\n      return;\n    } catch (Throwable t) {\n      exceptionReporter.reportException(t);\n      return;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.EventFetcher.getMapCompletionEvents": "  private int getMapCompletionEvents() throws IOException {\n    \n    int numNewMaps = 0;\n    \n    MapTaskCompletionEventsUpdate update = \n      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n                                       reduce.getJobID(), \n                                       fromEventId, \n                                       MAX_EVENTS_TO_FETCH,\n                                       (org.apache.hadoop.mapred.TaskAttemptID)\n                                         reduce);\n    TaskCompletionEvent events[] = update.getMapTaskCompletionEvents();\n    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n             fromEventId);\n      \n    // Check if the reset is required.\n    // Since there is no ordering of the task completion events at the \n    // reducer, the only option to sync with the new jobtracker is to reset \n    // the events index\n    if (update.shouldReset()) {\n      fromEventId = 0;\n      scheduler.resetKnownMaps();\n    }\n    \n    // Update the last seen event ID\n    fromEventId += events.length;\n    \n    // Process the TaskCompletionEvents:\n    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n    //    fetching from those maps.\n    // 3. Remove TIPFAILED maps from neededOutputs since we don't need their\n    //    outputs at all.\n    for (TaskCompletionEvent event : events) {\n      switch (event.getTaskStatus()) {\n        case SUCCEEDED:\n          URI u = getBaseURI(event.getTaskTrackerHttp());\n          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n                                      u.toString(),\n                                      event.getTaskAttemptId());\n          numNewMaps ++;\n          int duration = event.getTaskRunTime();\n          if (duration > maxMapRuntime) {\n            maxMapRuntime = duration;\n            scheduler.informMaxMapRunTime(maxMapRuntime);\n          }\n          break;\n        case FAILED:\n        case KILLED:\n        case OBSOLETE:\n          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n                   \" map-task: '\" + event.getTaskAttemptId() + \"'\");\n          break;\n        case TIPFAILED:\n          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n          LOG.info(\"Ignoring output of failed map TIP: '\" +  \n               event.getTaskAttemptId() + \"'\");\n          break;\n      }\n    }\n    return numNewMaps;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.Shuffle.run": "  public RawKeyValueIterator run() throws IOException, InterruptedException {\n    // Start the map-completion events fetcher thread\n    final EventFetcher<K,V> eventFetcher = \n      new EventFetcher<K,V>(reduceId, umbilical, scheduler, this);\n    eventFetcher.start();\n    \n    // Start the map-output fetcher threads\n    final int numFetchers = jobConf.getInt(MRJobConfig.SHUFFLE_PARALLEL_COPIES, 5);\n    Fetcher<K,V>[] fetchers = new Fetcher[numFetchers];\n    for (int i=0; i < numFetchers; ++i) {\n      fetchers[i] = new Fetcher<K,V>(jobConf, reduceId, scheduler, merger, \n                                     reporter, metrics, this, \n                                     reduceTask.getJobTokenSecret());\n      fetchers[i].start();\n    }\n    \n    // Wait for shuffle to complete successfully\n    while (!scheduler.waitUntilDone(PROGRESS_FREQUENCY)) {\n      reporter.progress();\n      \n      synchronized (this) {\n        if (throwable != null) {\n          throw new ShuffleError(\"error in shuffle in \" + throwingThreadName,\n                                 throwable);\n        }\n      }\n    }\n\n    // Stop the event-fetcher thread\n    eventFetcher.interrupt();\n    try {\n      eventFetcher.join();\n    } catch(Throwable t) {\n      LOG.info(\"Failed to stop \" + eventFetcher.getName(), t);\n    }\n    \n    // Stop the map-output fetcher threads\n    for (Fetcher<K,V> fetcher : fetchers) {\n      fetcher.interrupt();\n    }\n    for (Fetcher<K,V> fetcher : fetchers) {\n      fetcher.join();\n    }\n    fetchers = null;\n    \n    // stop the scheduler\n    scheduler.close();\n\n    copyPhase.complete(); // copy is already complete\n    taskStatus.setPhase(TaskStatus.Phase.SORT);\n    reduceTask.statusUpdate(umbilical);\n\n    // Finish the on-going merges...\n    RawKeyValueIterator kvIter = null;\n    try {\n      kvIter = merger.close();\n    } catch (Throwable e) {\n      throw new ShuffleError(\"Error while doing final merge \" , e);\n    }\n\n    // Sanity check\n    synchronized (this) {\n      if (throwable != null) {\n        throw new ShuffleError(\"error in shuffle in \" + throwingThreadName,\n                               throwable);\n      }\n    }\n    \n    return kvIter;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.run": "  public void run(JobConf job, final TaskUmbilicalProtocol umbilical)\n    throws IOException, InterruptedException, ClassNotFoundException {\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n\n    if (isMapOrReduce()) {\n      copyPhase = getProgress().addPhase(\"copy\");\n      sortPhase  = getProgress().addPhase(\"sort\");\n      reducePhase = getProgress().addPhase(\"reduce\");\n    }\n    // start thread that will handle communication with parent\n    TaskReporter reporter = startReporter(umbilical);\n    \n    boolean useNewApi = job.getUseNewReducer();\n    initialize(job, getJobID(), reporter, useNewApi);\n\n    // check if it is a cleanupJobTask\n    if (jobCleanup) {\n      runJobCleanupTask(umbilical, reporter);\n      return;\n    }\n    if (jobSetup) {\n      runJobSetupTask(umbilical, reporter);\n      return;\n    }\n    if (taskCleanup) {\n      runTaskCleanupTask(umbilical, reporter);\n      return;\n    }\n    \n    // Initialize the codec\n    codec = initCodec();\n    RawKeyValueIterator rIter = null;\n    \n    boolean isLocal = false; \n    // local if\n    // 1) framework == local or\n    // 2) framework == null and job tracker address == local\n    String framework = job.get(MRConfig.FRAMEWORK_NAME);\n    String masterAddr = job.get(MRConfig.MASTER_ADDRESS, \"local\");\n    if ((framework == null && masterAddr.equals(\"local\"))\n        || (framework != null && framework.equals(MRConfig.LOCAL_FRAMEWORK_NAME))) {\n      isLocal = true;\n    }\n    \n    if (!isLocal) {\n      Class combinerClass = conf.getCombinerClass();\n      CombineOutputCollector combineCollector = \n        (null != combinerClass) ? \n \t     new CombineOutputCollector(reduceCombineOutputCounter, reporter, conf) : null;\n\n      Shuffle shuffle = \n        new Shuffle(getTaskID(), job, FileSystem.getLocal(job), umbilical, \n                    super.lDirAlloc, reporter, codec, \n                    combinerClass, combineCollector, \n                    spilledRecordsCounter, reduceCombineInputCounter,\n                    shuffledMapsCounter,\n                    reduceShuffleBytes, failedShuffleCounter,\n                    mergedMapOutputsCounter,\n                    taskStatus, copyPhase, sortPhase, this,\n                    mapOutputFile);\n      rIter = shuffle.run();\n    } else {\n      // local job runner doesn't have a copy phase\n      copyPhase.complete();\n      final FileSystem rfs = FileSystem.getLocal(job).getRaw();\n      rIter = Merger.merge(job, rfs, job.getMapOutputKeyClass(),\n                           job.getMapOutputValueClass(), codec, \n                           getMapFiles(rfs, true),\n                           !conf.getKeepFailedTaskFiles(), \n                           job.getInt(JobContext.IO_SORT_FACTOR, 100),\n                           new Path(getTaskID().toString()), \n                           job.getOutputKeyComparator(),\n                           reporter, spilledRecordsCounter, null, null);\n    }\n    // free up the data structures\n    mapOutputFilesOnDisk.clear();\n    \n    sortPhase.complete();                         // sort is complete\n    setPhase(TaskStatus.Phase.REDUCE); \n    statusUpdate(umbilical);\n    Class keyClass = job.getMapOutputKeyClass();\n    Class valueClass = job.getMapOutputValueClass();\n    RawComparator comparator = job.getOutputValueGroupingComparator();\n\n    if (useNewApi) {\n      runNewReducer(job, umbilical, reporter, rIter, comparator, \n                    keyClass, valueClass);\n    } else {\n      runOldReducer(job, umbilical, reporter, rIter, comparator, \n                    keyClass, valueClass);\n    }\n    done(umbilical, reporter);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.runOldReducer": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runOldReducer(JobConf job,\n                     TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator<INKEY> comparator,\n                     Class<INKEY> keyClass,\n                     Class<INVALUE> valueClass) throws IOException {\n    Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer = \n      ReflectionUtils.newInstance(job.getReducerClass(), job);\n    // make output collector\n    String finalName = getOutputName(getPartition());\n\n    final RecordWriter<OUTKEY, OUTVALUE> out = new OldTrackingRecordWriter<OUTKEY, OUTVALUE>(\n        this, job, reporter, finalName);\n\n    OutputCollector<OUTKEY,OUTVALUE> collector = \n      new OutputCollector<OUTKEY,OUTVALUE>() {\n        public void collect(OUTKEY key, OUTVALUE value)\n          throws IOException {\n          out.write(key, value);\n          // indicate that progress update needs to be sent\n          reporter.progress();\n        }\n      };\n    \n    // apply reduce function\n    try {\n      //increment processed counter only if skipping feature is enabled\n      boolean incrProcCount = SkipBadRecords.getReducerMaxSkipGroups(job)>0 &&\n        SkipBadRecords.getAutoIncrReducerProcCount(job);\n      \n      ReduceValuesIterator<INKEY,INVALUE> values = isSkipping() ? \n          new SkippingReduceValuesIterator<INKEY,INVALUE>(rIter, \n              comparator, keyClass, valueClass, \n              job, reporter, umbilical) :\n          new ReduceValuesIterator<INKEY,INVALUE>(rIter, \n          job.getOutputValueGroupingComparator(), keyClass, valueClass, \n          job, reporter);\n      values.informReduceProgress();\n      while (values.more()) {\n        reduceInputKeyCounter.increment(1);\n        reducer.reduce(values.getKey(), values, collector, reporter);\n        if(incrProcCount) {\n          reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n              SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);\n        }\n        values.nextKey();\n        values.informReduceProgress();\n      }\n\n      //Clean up: repeated in catch block below\n      reducer.close();\n      out.close(reporter);\n      //End of clean up.\n    } catch (IOException ioe) {\n      try {\n        reducer.close();\n      } catch (IOException ignored) {}\n        \n      try {\n        out.close(reporter);\n      } catch (IOException ignored) {}\n      \n      throw ioe;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.getMapFiles": "  private Path[] getMapFiles(FileSystem fs, boolean isLocal) \n  throws IOException {\n    List<Path> fileList = new ArrayList<Path>();\n    if (isLocal) {\n      // for local jobs\n      for(int i = 0; i < numMaps; ++i) {\n        fileList.add(mapOutputFile.getInputFile(i));\n      }\n    } else {\n      // for non local jobs\n      for (FileStatus filestatus : mapOutputFilesOnDisk) {\n        fileList.add(filestatus.getPath());\n      }\n    }\n    return fileList.toArray(new Path[0]);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.getProgress": "      public Progress getProgress() {\n        return rawIter.getProgress();\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.runNewReducer": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runNewReducer(JobConf job,\n                     final TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator<INKEY> comparator,\n                     Class<INKEY> keyClass,\n                     Class<INVALUE> valueClass\n                     ) throws IOException,InterruptedException, \n                              ClassNotFoundException {\n    // wrap value iterator to report progress.\n    final RawKeyValueIterator rawIter = rIter;\n    rIter = new RawKeyValueIterator() {\n      public void close() throws IOException {\n        rawIter.close();\n      }\n      public DataInputBuffer getKey() throws IOException {\n        return rawIter.getKey();\n      }\n      public Progress getProgress() {\n        return rawIter.getProgress();\n      }\n      public DataInputBuffer getValue() throws IOException {\n        return rawIter.getValue();\n      }\n      public boolean next() throws IOException {\n        boolean ret = rawIter.next();\n        reporter.setProgress(rawIter.getProgress().getProgress());\n        return ret;\n      }\n    };\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,\n          getTaskID(), reporter);\n    // make a reducer\n    org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer =\n      (org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE>)\n        ReflectionUtils.newInstance(taskContext.getReducerClass(), job);\n    org.apache.hadoop.mapreduce.RecordWriter<OUTKEY,OUTVALUE> trackedRW = \n      new NewTrackingRecordWriter<OUTKEY, OUTVALUE>(this, taskContext);\n    job.setBoolean(\"mapred.skip.on\", isSkipping());\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.Reducer.Context \n         reducerContext = createReduceContext(reducer, job, getTaskID(),\n                                               rIter, reduceInputKeyCounter, \n                                               reduceInputValueCounter, \n                                               trackedRW,\n                                               committer,\n                                               reporter, comparator, keyClass,\n                                               valueClass);\n    reducer.run(reducerContext);\n    trackedRW.close(reducerContext);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.initCodec": "  private CompressionCodec initCodec() {\n    // check if map-outputs are to be compressed\n    if (conf.getCompressMapOutput()) {\n      Class<? extends CompressionCodec> codecClass =\n        conf.getMapOutputCompressorClass(DefaultCodec.class);\n      return ReflectionUtils.newInstance(codecClass, conf);\n    } \n\n    return null;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.run": "              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(pae,\"Unknown exception in doAs\");\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.main": "  public static void main(String[] args) throws Throwable {\n    LOG.debug(\"Child starting\");\n\n    final JobConf defaultConf = new JobConf();\n    defaultConf.addResource(MRJobConfig.JOB_CONF_FILE);\n    UserGroupInformation.setConfiguration(defaultConf);\n\n    String host = args[0];\n    int port = Integer.parseInt(args[1]);\n    final InetSocketAddress address = new InetSocketAddress(host, port);\n    final TaskAttemptID firstTaskid = TaskAttemptID.forName(args[2]);\n    int jvmIdInt = Integer.parseInt(args[3]);\n    JVMId jvmId = new JVMId(firstTaskid.getJobID(),\n        firstTaskid.getTaskType() == TaskType.MAP, jvmIdInt);\n\n    // initialize metrics\n    DefaultMetricsSystem.initialize(\n        StringUtils.camelize(firstTaskid.getTaskType().name()) +\"Task\");\n\n    Token<JobTokenIdentifier> jt = loadCredentials(defaultConf, address);\n\n    // Create TaskUmbilicalProtocol as actual task owner.\n    UserGroupInformation taskOwner =\n      UserGroupInformation.createRemoteUser(firstTaskid.getJobID().toString());\n    taskOwner.addToken(jt);\n    final TaskUmbilicalProtocol umbilical =\n      taskOwner.doAs(new PrivilegedExceptionAction<TaskUmbilicalProtocol>() {\n      @Override\n      public TaskUmbilicalProtocol run() throws Exception {\n        return (TaskUmbilicalProtocol)RPC.getProxy(TaskUmbilicalProtocol.class,\n            TaskUmbilicalProtocol.versionID, address, defaultConf);\n      }\n    });\n\n    // report non-pid to application master\n    JvmContext context = new JvmContext(jvmId, \"-1000\");\n    LOG.debug(\"PID: \" + System.getenv().get(\"JVM_PID\"));\n    Task task = null;\n    UserGroupInformation childUGI = null;\n\n    try {\n      int idleLoopCount = 0;\n      JvmTask myTask = null;;\n      // poll for new task\n      for (int idle = 0; null == myTask; ++idle) {\n        long sleepTimeMilliSecs = Math.min(idle * 500, 1500);\n        LOG.info(\"Sleeping for \" + sleepTimeMilliSecs\n            + \"ms before retrying again. Got null now.\");\n        MILLISECONDS.sleep(sleepTimeMilliSecs);\n        myTask = umbilical.getTask(context);\n      }\n      if (myTask.shouldDie()) {\n        return;\n      }\n\n      task = myTask.getTask();\n      YarnChild.taskid = task.getTaskID();\n\n      // Create the job-conf and set credentials\n      final JobConf job =\n        configureTask(task, defaultConf.getCredentials(), jt);\n\n      // Initiate Java VM metrics\n      JvmMetrics.initSingleton(jvmId.toString(), job.getSessionId());\n      LOG.debug(\"Remote user: \" + job.get(\"user.name\"));\n      childUGI = UserGroupInformation.createRemoteUser(job.get(\"user.name\"));\n      // Add tokens to new user so that it may execute its task correctly.\n      for(Token<?> token : UserGroupInformation.getCurrentUser().getTokens()) {\n        childUGI.addToken(token);\n      }\n\n      // Create a final reference to the task for the doAs block\n      final Task taskFinal = task;\n      childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n        @Override\n        public Object run() throws Exception {\n          // use job-specified working directory\n          FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory());\n          taskFinal.run(job, umbilical); // run the task\n          return null;\n        }\n      });\n    } catch (FSError e) {\n      LOG.fatal(\"FSError from child\", e);\n      umbilical.fsError(taskid, e.getMessage());\n    } catch (Exception exception) {\n      LOG.warn(\"Exception running child : \"\n          + StringUtils.stringifyException(exception));\n      try {\n        if (task != null) {\n          // do cleanup for the task\n          if (childUGI == null) { // no need to job into doAs block\n            task.taskCleanup(umbilical);\n          } else {\n            final Task taskFinal = task;\n            childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n              @Override\n              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }\n            });\n          }\n        }\n      } catch (Exception e) {\n        LOG.info(\"Exception cleaning up: \" + StringUtils.stringifyException(e));\n      }\n      // Report back any failures, for diagnostic purposes\n      ByteArrayOutputStream baos = new ByteArrayOutputStream();\n      exception.printStackTrace(new PrintStream(baos));\n      if (taskid != null) {\n        umbilical.reportDiagnosticInfo(taskid, baos.toString());\n      }\n    } catch (Throwable throwable) {\n      LOG.fatal(\"Error running child : \"\n    \t        + StringUtils.stringifyException(throwable));\n      if (taskid != null) {\n        Throwable tCause = throwable.getCause();\n        String cause = tCause == null\n                                 ? throwable.getMessage()\n                                 : StringUtils.stringifyException(tCause);\n        umbilical.fatalError(taskid, cause);\n      }\n    } finally {\n      RPC.stopProxy(umbilical);\n      DefaultMetricsSystem.shutdown();\n      // Shutting down log4j of the child-vm...\n      // This assumes that on return from Task.run()\n      // there is no more logging done.\n      LogManager.shutdown();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.configureTask": "  private static JobConf configureTask(Task task, Credentials credentials,\n      Token<JobTokenIdentifier> jt) throws IOException {\n    final JobConf job = new JobConf(MRJobConfig.JOB_CONF_FILE);\n    job.setCredentials(credentials);\n    // set tcp nodelay\n    job.setBoolean(\"ipc.client.tcpnodelay\", true);\n    job.setClass(MRConfig.TASK_LOCAL_OUTPUT_CLASS,\n        YarnOutputFiles.class, MapOutputFile.class);\n    // set the jobTokenFile into task\n    task.setJobTokenSecret(\n        JobTokenSecretManager.createSecretKey(jt.getPassword()));\n\n    // setup the child's MRConfig.LOCAL_DIR.\n    configureLocalDirs(task, job);\n\n    // setup the child's attempt directories\n    // Do the task-type specific localization\n    task.localizeConfiguration(job);\n\n    // Set up the DistributedCache related configs\n    setupDistributedCacheConfig(job);\n\n    // Overwrite the localized task jobconf which is linked to in the current\n    // work-dir.\n    Path localTaskFile = new Path(MRJobConfig.JOB_CONF_FILE);\n    writeLocalJobFile(localTaskFile, job);\n    task.setJobFile(localTaskFile.toString());\n    task.setConf(job);\n    return job;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.loadCredentials": "  private static Token<JobTokenIdentifier> loadCredentials(JobConf conf,\n      InetSocketAddress address) throws IOException {\n    //load token cache storage\n    String tokenFileLocation =\n        System.getenv(ApplicationConstants.CONTAINER_TOKEN_FILE_ENV_NAME);\n    String jobTokenFile =\n        new Path(tokenFileLocation).makeQualified(FileSystem.getLocal(conf))\n            .toUri().getPath();\n    Credentials credentials =\n      TokenCache.loadTokens(jobTokenFile, conf);\n    LOG.debug(\"loading token. # keys =\" +credentials.numberOfSecretKeys() +\n        \"; from file=\" + jobTokenFile);\n    Token<JobTokenIdentifier> jt = TokenCache.getJobToken(credentials);\n    jt.setService(new Text(address.getAddress().getHostAddress() + \":\"\n        + address.getPort()));\n    UserGroupInformation current = UserGroupInformation.getCurrentUser();\n    current.addToken(jt);\n    for (Token<? extends TokenIdentifier> tok : credentials.getAllTokens()) {\n      current.addToken(tok);\n    }\n    // Set the credentials\n    conf.setCredentials(credentials);\n    return jt;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.task.reduce.exceptionReporter.reportException": "  void reportException(Throwable t);\n}",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Merger.merge": "    RawKeyValueIterator merge(Class<K> keyClass, Class<V> valueClass,\n                                     int factor, int inMem, Path tmpDir,\n                                     Counters.Counter readsCounter,\n                                     Counters.Counter writesCounter,\n                                     Progress mergePhase)\n        throws IOException {\n      LOG.info(\"Merging \" + segments.size() + \" sorted segments\");\n\n      /*\n       * If there are inMemory segments, then they come first in the segments\n       * list and then the sorted disk segments. Otherwise(if there are only\n       * disk segments), then they are sorted segments if there are more than\n       * factor segments in the segments list.\n       */\n      int numSegments = segments.size();\n      int origFactor = factor;\n      int passNo = 1;\n      if (mergePhase != null) {\n        mergeProgress = mergePhase;\n      }\n\n      long totalBytes = computeBytesInMerges(factor, inMem);\n      if (totalBytes != 0) {\n        progPerByte = 1.0f / (float)totalBytes;\n      }\n      \n      //create the MergeStreams from the sorted map created in the constructor\n      //and dump the final output to a file\n      do {\n        //get the factor for this pass of merge. We assume in-memory segments\n        //are the first entries in the segment list and that the pass factor\n        //doesn't apply to them\n        factor = getPassFactor(factor, passNo, numSegments - inMem);\n        if (1 == passNo) {\n          factor += inMem;\n        }\n        List<Segment<K, V>> segmentsToMerge =\n          new ArrayList<Segment<K, V>>();\n        int segmentsConsidered = 0;\n        int numSegmentsToConsider = factor;\n        long startBytes = 0; // starting bytes of segments of this merge\n        while (true) {\n          //extract the smallest 'factor' number of segments  \n          //Call cleanup on the empty segments (no key/value data)\n          List<Segment<K, V>> mStream = \n            getSegmentDescriptors(numSegmentsToConsider);\n          for (Segment<K, V> segment : mStream) {\n            // Initialize the segment at the last possible moment;\n            // this helps in ensuring we don't use buffers until we need them\n            segment.init(readsCounter);\n            long startPos = segment.getPosition();\n            boolean hasNext = segment.nextRawKey();\n            long endPos = segment.getPosition();\n            \n            if (hasNext) {\n              startBytes += endPos - startPos;\n              segmentsToMerge.add(segment);\n              segmentsConsidered++;\n            }\n            else {\n              segment.close();\n              numSegments--; //we ignore this segment for the merge\n            }\n          }\n          //if we have the desired number of segments\n          //or looked at all available segments, we break\n          if (segmentsConsidered == factor || \n              segments.size() == 0) {\n            break;\n          }\n            \n          numSegmentsToConsider = factor - segmentsConsidered;\n        }\n        \n        //feed the streams to the priority queue\n        initialize(segmentsToMerge.size());\n        clear();\n        for (Segment<K, V> segment : segmentsToMerge) {\n          put(segment);\n        }\n        \n        //if we have lesser number of segments remaining, then just return the\n        //iterator, else do another single level merge\n        if (numSegments <= factor) {\n          if (!includeFinalMerge) { // for reduce task\n\n            // Reset totalBytesProcessed and recalculate totalBytes from the\n            // remaining segments to track the progress of the final merge.\n            // Final merge is considered as the progress of the reducePhase,\n            // the 3rd phase of reduce task.\n            totalBytesProcessed = 0;\n            totalBytes = 0;\n            for (int i = 0; i < segmentsToMerge.size(); i++) {\n              totalBytes += segmentsToMerge.get(i).getLength();\n            }\n          }\n          if (totalBytes != 0) //being paranoid\n            progPerByte = 1.0f / (float)totalBytes;\n          \n          totalBytesProcessed += startBytes;         \n          if (totalBytes != 0)\n            mergeProgress.set(totalBytesProcessed * progPerByte);\n          else\n            mergeProgress.set(1.0f); // Last pass and no segments left - we're done\n          \n          LOG.info(\"Down to the last merge-pass, with \" + numSegments + \n                   \" segments left of total size: \" +\n                   (totalBytes - totalBytesProcessed) + \" bytes\");\n          return this;\n        } else {\n          LOG.info(\"Merging \" + segmentsToMerge.size() + \n                   \" intermediate segments out of a total of \" + \n                   (segments.size()+segmentsToMerge.size()));\n          \n          long bytesProcessedInPrevMerges = totalBytesProcessed;\n          totalBytesProcessed += startBytes;\n\n          //we want to spread the creation of temp files on multiple disks if \n          //available under the space constraints\n          long approxOutputSize = 0; \n          for (Segment<K, V> s : segmentsToMerge) {\n            approxOutputSize += s.getLength() + \n                                ChecksumFileSystem.getApproxChkSumLength(\n                                s.getLength());\n          }\n          Path tmpFilename = \n            new Path(tmpDir, \"intermediate\").suffix(\".\" + passNo);\n\n          Path outputFile =  lDirAlloc.getLocalPathForWrite(\n                                              tmpFilename.toString(),\n                                              approxOutputSize, conf);\n\n          Writer<K, V> writer = \n            new Writer<K, V>(conf, fs, outputFile, keyClass, valueClass, codec,\n                             writesCounter);\n          writeFile(this, writer, reporter, conf);\n          writer.close();\n          \n          //we finished one single level merge; now clean up the priority \n          //queue\n          this.close();\n\n          // Add the newly create segment to the list of segments to be merged\n          Segment<K, V> tempSegment = \n            new Segment<K, V>(conf, fs, outputFile, codec, false);\n\n          // Insert new merged segment into the sorted list\n          int pos = Collections.binarySearch(segments, tempSegment,\n                                             segmentComparator);\n          if (pos < 0) {\n            // binary search failed. So position to be inserted at is -pos-1\n            pos = -pos-1;\n          }\n          segments.add(pos, tempSegment);\n          numSegments = segments.size();\n          \n          // Subtract the difference between expected size of new segment and \n          // actual size of new segment(Expected size of new segment is\n          // inputBytesOfThisMerge) from totalBytes. Expected size and actual\n          // size will match(almost) if combiner is not called in merge.\n          long inputBytesOfThisMerge = totalBytesProcessed -\n                                       bytesProcessedInPrevMerges;\n          totalBytes -= inputBytesOfThisMerge - tempSegment.getLength();\n          if (totalBytes != 0) {\n            progPerByte = 1.0f / (float)totalBytes;\n          }\n          \n          passNo++;\n        }\n        //we are worried about only the first pass merge factor. So reset the \n        //factor to what it originally was\n        factor = origFactor;\n      } while(true);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Merger.writeFile": "  public static <K extends Object, V extends Object>\n  void writeFile(RawKeyValueIterator records, Writer<K, V> writer, \n                 Progressable progressable, Configuration conf) \n  throws IOException {\n    long progressBar = conf.getLong(JobContext.RECORDS_BEFORE_PROGRESS,\n        10000);\n    long recordCtr = 0;\n    while(records.next()) {\n      writer.append(records.getKey(), records.getValue());\n      \n      if (((recordCtr++) % progressBar) == 0) {\n        progressable.progress();\n      }\n    }\n}",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Merger.nextRawKey": "    boolean nextRawKey() throws IOException {\n      return reader.nextRawKey(key);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Merger.init": "    void init(Counters.Counter readsCounter) throws IOException {\n      if (reader == null) {\n        FSDataInputStream in = fs.open(file);\n        in.seek(segmentOffset);\n        reader = new Reader<K, V>(conf, in, segmentLength, codec, readsCounter);\n      }\n      \n      if (mapOutputsCounter != null) {\n        mapOutputsCounter.increment(1);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Merger.computeBytesInMerges": "    long computeBytesInMerges(int factor, int inMem) {\n      int numSegments = segments.size();\n      List<Long> segmentSizes = new ArrayList<Long>(numSegments);\n      long totalBytes = 0;\n      int n = numSegments - inMem;\n      // factor for 1st pass\n      int f = getPassFactor(factor, 1, n) + inMem;\n      n = numSegments;\n \n      for (int i = 0; i < numSegments; i++) {\n        // Not handling empty segments here assuming that it would not affect\n        // much in calculation of mergeProgress.\n        segmentSizes.add(segments.get(i).getLength());\n      }\n      \n      // If includeFinalMerge is true, allow the following while loop iterate\n      // for 1 more iteration. This is to include final merge as part of the\n      // computation of expected input bytes of merges\n      boolean considerFinalMerge = includeFinalMerge;\n      \n      while (n > f || considerFinalMerge) {\n        if (n <=f ) {\n          considerFinalMerge = false;\n        }\n        long mergedSize = 0;\n        f = Math.min(f, segmentSizes.size());\n        for (int j = 0; j < f; j++) {\n          mergedSize += segmentSizes.remove(0);\n        }\n        totalBytes += mergedSize;\n        \n        // insert new size into the sorted list\n        int pos = Collections.binarySearch(segmentSizes, mergedSize);\n        if (pos < 0) {\n          pos = -pos-1;\n        }\n        segmentSizes.add(pos, mergedSize);\n        \n        n -= (f-1);\n        f = factor;\n      }\n\n      return totalBytes;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Merger.getLength": "    public long getLength() { \n      return (reader == null) ?\n        segmentLength : reader.getLength();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Merger.getPosition": "    public long getPosition() throws IOException {\n      return reader.getPosition();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Merger.close": "    public void close() throws IOException {\n      Segment<K, V> segment;\n      while((segment = pop()) != null) {\n        segment.close();\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Merger.getSegmentDescriptors": "    private List<Segment<K, V>> getSegmentDescriptors(int numDescriptors) {\n      if (numDescriptors > segments.size()) {\n        List<Segment<K, V>> subList = new ArrayList<Segment<K,V>>(segments);\n        segments.clear();\n        return subList;\n      }\n      \n      List<Segment<K, V>> subList = \n        new ArrayList<Segment<K,V>>(segments.subList(0, numDescriptors));\n      for (int i=0; i < numDescriptors; ++i) {\n        segments.remove(0);\n      }\n      return subList;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Merger.getPassFactor": "    private int getPassFactor(int factor, int passNo, int numSegments) {\n      if (passNo > 1 || numSegments <= factor || factor == 1) \n        return factor;\n      int mod = (numSegments - 1) % (factor - 1);\n      if (mod == 0)\n        return factor;\n      return mod + 1;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.taskCleanup": "  void taskCleanup(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    // set phase for this task\n    setPhase(TaskStatus.Phase.CLEANUP);\n    getProgress().setStatus(\"cleanup\");\n    statusUpdate(umbilical);\n    LOG.info(\"Runnning cleanup for the task\");\n    // do the cleanup\n    committer.abortTask(taskContext);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.statusUpdate": "  public void statusUpdate(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    int retries = MAX_RETRIES;\n    while (true) {\n      try {\n        if (!umbilical.statusUpdate(getTaskID(), taskStatus)) {\n          LOG.warn(\"Parent died.  Exiting \"+taskId);\n          System.exit(66);\n        }\n        taskStatus.clearStatus();\n        return;\n      } catch (InterruptedException ie) {\n        Thread.currentThread().interrupt(); // interrupt ourself\n      } catch (IOException ie) {\n        LOG.warn(\"Failure sending status update: \" + \n                  StringUtils.stringifyException(ie));\n        if (--retries == 0) {\n          throw ie;\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setPhase": "  protected synchronized void setPhase(TaskStatus.Phase phase){\n    this.taskStatus.setPhase(phase); \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setStatus": "    public void setStatus(String status) {\n      taskProgress.setStatus(status);\n      // indicate that progress update needs to be sent\n      setProgressFlag();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getProgress": "    public float getProgress() {\n      return taskProgress.getProgress();\n    };",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getWorkingDirectory": "  public Path getWorkingDirectory() {\n    String name = get(JobContext.WORKING_DIR);\n    if (name != null) {\n      return new Path(name);\n    } else {\n      try {\n        Path dir = FileSystem.get(this).getWorkingDirectory();\n        set(JobContext.WORKING_DIR, dir.toString());\n        return dir;\n      } catch (IOException e) {\n        throw new RuntimeException(e);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.toString": "  public String toString() { \n    return appendTo(new StringBuilder(JVM)).toString();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.appendTo": "  protected StringBuilder appendTo(StringBuilder builder) {\n    return jobId.appendTo(builder).\n                 append(SEPARATOR).\n                 append(isMap ? 'm' : 'r').\n                 append(SEPARATOR).\n                 append(idFormat.format(id));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getSessionId": "  public String getSessionId() {\n      return get(\"session.id\", \"\");\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fsError": "  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.shouldDie": "  public boolean shouldDie() {\n    return shouldDie;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.reportDiagnosticInfo": "  void reportDiagnosticInfo(TaskAttemptID taskid, String trace) throws IOException;\n  \n  /**\n   * Report the record range which is going to process next by the Task.\n   * @param taskid the id of the task involved\n   * @param range the range of record sequence nos\n   * @throws IOException\n   */\n  void reportNextRecordRange(TaskAttemptID taskid, SortedRanges.Range range) \n    throws IOException;\n\n  /** Periodically called by child to check if parent is still alive. \n   * @return True if the task is known\n   */\n  boolean ping(TaskAttemptID taskid) throws IOException;\n\n  /** Report that the task is successfully completed.  Failure is assumed if\n   * the task process exits without calling this.\n   * @param taskid task's id\n   */\n  void done(TaskAttemptID taskid) throws IOException;\n  \n  /** \n   * Report that the task is complete, but its commit is pending.\n   * \n   * @param taskId task's id\n   * @param taskStatus status of the child\n   * @throws IOException\n   */\n  void commitPending(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;  \n\n  /**\n   * Polling to know whether the task can go-ahead with commit \n   * @param taskid\n   * @return true/false \n   * @throws IOException\n   */\n  boolean canCommit(TaskAttemptID taskid) throws IOException;\n\n  /** Report that a reduce-task couldn't shuffle map-outputs.*/\n  void shuffleError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Report that the task encounted a local filesystem error.*/\n  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.getTask": "  public Task getTask() {\n    return t;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getCredentials": "  public Credentials getCredentials() {\n    return credentials;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.getTask": "  JvmTask getTask(JvmContext context) throws IOException;\n  \n  /**\n   * Report child's progress to parent.\n   * \n   * @param taskId task-id of the child\n   * @param taskStatus status of the child\n   * @throws IOException\n   * @throws InterruptedException\n   * @return True if the task is known\n   */\n  boolean statusUpdate(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;\n  \n  /** Report error messages back to parent.  Calls should be sparing, since all\n   *  such messages are held in the job tracker.\n   *  @param taskid the id of the task involved\n   *  @param trace the text to report\n   */\n  void reportDiagnosticInfo(TaskAttemptID taskid, String trace) throws IOException;\n  \n  /**\n   * Report the record range which is going to process next by the Task.\n   * @param taskid the id of the task involved\n   * @param range the range of record sequence nos\n   * @throws IOException\n   */\n  void reportNextRecordRange(TaskAttemptID taskid, SortedRanges.Range range) \n    throws IOException;\n\n  /** Periodically called by child to check if parent is still alive. \n   * @return True if the task is known\n   */\n  boolean ping(TaskAttemptID taskid) throws IOException;\n\n  /** Report that the task is successfully completed.  Failure is assumed if\n   * the task process exits without calling this.\n   * @param taskid task's id\n   */\n  void done(TaskAttemptID taskid) throws IOException;\n  \n  /** \n   * Report that the task is complete, but its commit is pending.\n   * \n   * @param taskId task's id\n   * @param taskStatus status of the child\n   * @throws IOException\n   */\n  void commitPending(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;  \n\n  /**\n   * Polling to know whether the task can go-ahead with commit \n   * @param taskid\n   * @return true/false \n   * @throws IOException\n   */\n  boolean canCommit(TaskAttemptID taskid) throws IOException;\n\n  /** Report that a reduce-task couldn't shuffle map-outputs.*/\n  void shuffleError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Report that the task encounted a local filesystem error.*/\n  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.getJobID": "  public JobID getJobID() {\n    return (JobID) super.getJobID();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fatalError": "  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.forName": "  public static TaskAttemptID forName(String str\n                                      ) throws IllegalArgumentException {\n    return (TaskAttemptID) \n             org.apache.hadoop.mapreduce.TaskAttemptID.forName(str);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getTaskID": "  public TaskAttemptID getTaskID() { return taskId; }"
        },
        "bug_report": {
            "Title": "Few reduce tasks hanging in a gridmix-run",
            "Description": "In a gridmix run with ~1000 jobs, one job is getting stuck because of 2-3 hanging reducers. All of the them are stuck after downloading all map outputs and have the following thread dump.\n\n{code}\n\"EventFetcher for fetching Map Completion Events\" daemon prio=10 tid=0xa325fc00 nid=0x1ca4 waiting on condition [0xa315c000]\n   java.lang.Thread.State: TIMED_WAITING (sleeping)\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.mapreduce.task.reduce.EventFetcher.run(EventFetcher.java:71)\n\n\"main\" prio=10 tid=0x080ed400 nid=0x1c71 in Object.wait() [0xf73a2000]\n   java.lang.Thread.State: WAITING (on object monitor)\n        at java.lang.Object.wait(Native Method)\n        - waiting on <0xa94b23d8> (a org.apache.hadoop.mapreduce.task.reduce.EventFetcher)\n        at java.lang.Thread.join(Thread.java:1143)\n        - locked <0xa94b23d8> (a org.apache.hadoop.mapreduce.task.reduce.EventFetcher)\n        at java.lang.Thread.join(Thread.java:1196)\n        at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:135)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:367)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)\n{code}\n\nThanks to [~karams] for helping track this down."
        }
    },
    {
        "filename": "MAPREDUCE-3070.json",
        "creation_time": "2011-09-22T14:33:58.000+0000",
        "stack_trace": "```\norg.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager\n\tat org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)\nCaused by: org.apache.avro.AvroRuntimeException: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)\n\tat org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)\n\t... 2 more\nCaused by: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:142)\n\tat $Proxy13.registerNodeManager(Unknown Source)\n\tat org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:175)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.service.CompositeService.start": "  public synchronized void start() {\n    int i = 0;\n    try {\n      for (int n = serviceList.size(); i < n; i++) {\n        Service service = serviceList.get(i);\n        service.start();\n      }\n      super.start();\n    } catch (Throwable e) {\n      LOG.error(\"Error starting services \" + getName(), e);\n      // Note that the state of the failed service is still INITED and not\n      // STARTED. Even though the last service is not started completely, still\n      // call stop() on all services including failed service to make sure cleanup\n      // happens.\n      stop(i);\n      throw new YarnException(\"Failed to Start \" + getName(), e);\n    }\n\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.service.CompositeService.stop": "  private synchronized void stop(int numOfServicesStarted) {\n    // stop in reserve order of start\n    for (int i = numOfServicesStarted; i >= 0; i--) {\n      Service service = serviceList.get(i);\n      try {\n        service.stop();\n      } catch (Throwable t) {\n        LOG.info(\"Error stopping \" + service.getName(), t);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.start": "  public void start() {\n    try {\n      doSecureLogin();\n    } catch (IOException e) {\n      throw new YarnException(\"Failed NodeManager login\", e);\n    }\n    super.start();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.doSecureLogin": "  protected void doSecureLogin() throws IOException {\n    SecurityUtil.login(getConfig(), YarnConfiguration.NM_KEYTAB,\n        YarnConfiguration.NM_PRINCIPAL);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.main": "  public static void main(String[] args) {\n    StringUtils.startupShutdownMessage(NodeManager.class, args, LOG);\n    try {\n      NodeManager nodeManager = new NodeManager();\n      Runtime.getRuntime().addShutdownHook(\n          new CompositeServiceShutdownHook(nodeManager));\n      YarnConfiguration conf = new YarnConfiguration();\n      nodeManager.init(conf);\n      nodeManager.start();\n    } catch (Throwable t) {\n      LOG.fatal(\"Error starting NodeManager\", t);\n      System.exit(-1);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.init": "  public void init(Configuration conf) {\n\n    Context context = new NMContext();\n\n    // Create the secretManager if need be.\n    if (UserGroupInformation.isSecurityEnabled()) {\n      LOG.info(\"Security is enabled on NodeManager. \"\n          + \"Creating ContainerTokenSecretManager\");\n      this.containerTokenSecretManager = new ContainerTokenSecretManager();\n    }\n\n    ContainerExecutor exec = ReflectionUtils.newInstance(\n        conf.getClass(YarnConfiguration.NM_CONTAINER_EXECUTOR,\n          DefaultContainerExecutor.class, ContainerExecutor.class), conf);\n    DeletionService del = new DeletionService(exec);\n    addService(del);\n\n    // NodeManager level dispatcher\n    AsyncDispatcher dispatcher = new AsyncDispatcher();\n\n    NodeHealthCheckerService healthChecker = null;\n    if (NodeHealthCheckerService.shouldRun(conf)) {\n      healthChecker = new NodeHealthCheckerService();\n      addService(healthChecker);\n    }\n\n    NodeStatusUpdater nodeStatusUpdater =\n        createNodeStatusUpdater(context, dispatcher, healthChecker, \n        this.containerTokenSecretManager);\n\n    NodeResourceMonitor nodeResourceMonitor = createNodeResourceMonitor();\n    addService(nodeResourceMonitor);\n\n    ContainerManagerImpl containerManager =\n        createContainerManager(context, exec, del, nodeStatusUpdater,\n        this.containerTokenSecretManager);\n    addService(containerManager);\n\n    Service webServer =\n        createWebServer(context, containerManager.getContainersMonitor());\n    addService(webServer);\n\n    dispatcher.register(ContainerManagerEventType.class, containerManager);\n    addService(dispatcher);\n\n    DefaultMetricsSystem.initialize(\"NodeManager\");\n\n    // StatusUpdater should be added last so that it get started last \n    // so that we make sure everything is up before registering with RM. \n    addService(nodeStatusUpdater);\n\n    super.init(conf);\n    // TODO add local dirs to del\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start": "  public void start() {\n    String cmBindAddressStr =\n        getConfig().get(YarnConfiguration.NM_ADDRESS,\n            YarnConfiguration.DEFAULT_NM_ADDRESS);\n    InetSocketAddress cmBindAddress =\n        NetUtils.createSocketAddr(cmBindAddressStr);\n    String httpBindAddressStr =\n      getConfig().get(YarnConfiguration.NM_WEBAPP_ADDRESS,\n          YarnConfiguration.DEFAULT_NM_WEBAPP_ADDRESS);\n    InetSocketAddress httpBindAddress =\n      NetUtils.createSocketAddr(httpBindAddressStr);\n    try {\n      this.hostName = InetAddress.getLocalHost().getHostAddress();\n      this.containerManagerPort = cmBindAddress.getPort();\n      this.httpPort = httpBindAddress.getPort();\n      this.containerManagerBindAddress =\n          this.hostName + \":\" + this.containerManagerPort;\n      LOG.info(\"Configured ContainerManager Address is \"\n          + this.containerManagerBindAddress);\n      // Registration has to be in start so that ContainerManager can get the\n      // perNM tokens needed to authenticate ContainerTokens.\n      registerWithRM();\n      super.start();\n      startStatusUpdater();\n    } catch (Exception e) {\n      throw new AvroRuntimeException(e);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM": "  private void registerWithRM() throws YarnRemoteException {\n    this.resourceTracker = getRMClient();\n    LOG.info(\"Connected to ResourceManager at \" + this.rmAddress);\n    \n    RegisterNodeManagerRequest request = recordFactory.newRecordInstance(RegisterNodeManagerRequest.class);\n    this.nodeId = Records.newRecord(NodeId.class);\n    this.nodeId.setHost(this.hostName);\n    this.nodeId.setPort(this.containerManagerPort);\n    request.setHttpPort(this.httpPort);\n    request.setResource(this.totalResource);\n    request.setNodeId(this.nodeId);\n    RegistrationResponse regResponse =\n        this.resourceTracker.registerNodeManager(request).getRegistrationResponse();\n    if (UserGroupInformation.isSecurityEnabled()) {\n      this.secretKeyBytes = regResponse.getSecretKey().array();\n    }\n\n    // do this now so that its set before we start heartbeating to RM\n    if (UserGroupInformation.isSecurityEnabled()) {\n      LOG.info(\"Security enabled - updating secret keys now\");\n      // It is expected that status updater is started by this point and\n      // RM gives the shared secret in registration during StatusUpdater#start().\n      this.containerTokenSecretManager.setSecretKey(\n          this.getContainerManagerBindAddress(),\n          this.getRMNMSharedSecret());\n    }\n    LOG.info(\"Registered with ResourceManager as \" + this.containerManagerBindAddress\n        + \" with total resource of \" + this.totalResource);\n\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.startStatusUpdater": "  protected void startStatusUpdater() {\n\n    new Thread() {\n      @Override\n      public void run() {\n        int lastHeartBeatID = 0;\n        while (!isStopped) {\n          // Send heartbeat\n          try {\n            synchronized (heartbeatMonitor) {\n              heartbeatMonitor.wait(heartBeatInterval);\n            }\n            NodeStatus nodeStatus = getNodeStatus();\n            nodeStatus.setResponseId(lastHeartBeatID);\n            \n            NodeHeartbeatRequest request = recordFactory.newRecordInstance(NodeHeartbeatRequest.class);\n            request.setNodeStatus(nodeStatus);            \n            HeartbeatResponse response =\n              resourceTracker.nodeHeartbeat(request).getHeartbeatResponse();\n            lastHeartBeatID = response.getResponseId();\n            List<ContainerId> containersToCleanup = response\n                .getContainersToCleanupList();\n            if (containersToCleanup.size() != 0) {\n              dispatcher.getEventHandler().handle(\n                  new CMgrCompletedContainersEvent(containersToCleanup));\n            }\n            List<ApplicationId> appsToCleanup =\n                response.getApplicationsToCleanupList();\n            if (appsToCleanup.size() != 0) {\n              dispatcher.getEventHandler().handle(\n                  new CMgrCompletedAppsEvent(appsToCleanup));\n            }\n          } catch (Throwable e) {\n            LOG.error(\"Caught exception in status-updater\", e);\n            break;\n          }\n        }\n      }\n    }.start();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.invoke": "    public Object invoke(Object proxy, Method method, Object[] args)\n        throws Throwable {\n      long startTime = 0;\n      if (LOG.isDebugEnabled()) {\n        startTime = System.currentTimeMillis();\n      }\n\n      ProtoSpecificRpcRequest rpcRequest = constructRpcRequest(method, args);\n      ProtoSpecificResponseWritable val = null;\n      try {\n        val = (ProtoSpecificResponseWritable) client.call(\n            new ProtoSpecificRequestWritable(rpcRequest), remoteId);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      \n      ProtoSpecificRpcResponse response = val.message;\n   \n      if (LOG.isDebugEnabled()) {\n        long callTime = System.currentTimeMillis() - startTime;\n        LOG.debug(\"Call: \" + method.getName() + \" \" + callTime);\n      }\n \n      if (response.hasIsError() && response.getIsError() == true) {\n        YarnRemoteExceptionPBImpl exception = new YarnRemoteExceptionPBImpl(response.getException());\n        exception.fillInStackTrace();\n        ServiceException se = new ServiceException(exception);\n        throw se;\n      }\n      \n      Message prototype = null;\n      try {\n        prototype = getReturnProtoType(method);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      Message actualReturnMessage = prototype.newBuilderForType()\n          .mergeFrom(response.getResponseProto()).build();\n      return actualReturnMessage;\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.constructRpcRequest": "    private ProtoSpecificRpcRequest constructRpcRequest(Method method,\n        Object[] params) throws ServiceException {\n      ProtoSpecificRpcRequest rpcRequest;\n      ProtoSpecificRpcRequest.Builder builder;\n\n      builder = ProtoSpecificRpcRequest.newBuilder();\n      builder.setMethodName(method.getName());\n\n      if (params.length != 2) { // RpcController + Message\n        throw new ServiceException(\"Too many parameters for request. Method: [\"\n            + method.getName() + \"]\" + \", Expected: 2, Actual: \"\n            + params.length);\n      }\n      if (params[1] == null) {\n        throw new ServiceException(\"null param while calling Method: [\"\n            + method.getName() + \"]\");\n      }\n\n      Message param = (Message) params[1];\n      builder.setRequestProto(param.toByteString());\n\n      rpcRequest = builder.build();\n      return rpcRequest;\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.call": "    public Writable call(String protocol, Writable writableRequest,\n        long receiveTime) throws IOException {\n      ProtoSpecificRequestWritable request = (ProtoSpecificRequestWritable) writableRequest;\n      ProtoSpecificRpcRequest rpcRequest = request.message;\n      String methodName = rpcRequest.getMethodName();\n      System.out.println(\"Call: protocol=\" + protocol + \", method=\"\n          + methodName);\n      if (verbose)\n        log(\"Call: protocol=\" + protocol + \", method=\"\n            + methodName);\n      MethodDescriptor methodDescriptor = service.getDescriptorForType()\n          .findMethodByName(methodName);\n      Message prototype = service.getRequestPrototype(methodDescriptor);\n      Message param = prototype.newBuilderForType()\n          .mergeFrom(rpcRequest.getRequestProto()).build();\n      Message result;\n      try {\n        result = service.callBlockingMethod(methodDescriptor, null, param);\n      } catch (ServiceException e) {\n        e.printStackTrace();\n        return handleException(e);\n      } catch (Exception e) {\n        return handleException(e);\n      }\n\n      ProtoSpecificRpcResponse response = constructProtoSpecificRpcSuccessResponse(result);\n      return new ProtoSpecificResponseWritable(response);\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.getReturnProtoType": "    private Message getReturnProtoType(Method method) throws Exception {\n      if (returnTypes.containsKey(method.getName())) {\n        return returnTypes.get(method.getName());\n      } else {\n        Class<?> returnType = method.getReturnType();\n\n        Method newInstMethod = returnType.getMethod(\"getDefaultInstance\");\n        newInstMethod.setAccessible(true);\n        Message prototype = (Message) newInstMethod.invoke(null,\n            (Object[]) null);\n        returnTypes.put(method.getName(), prototype);\n        return prototype;\n      }\n    }"
        },
        "bug_report": {
            "Title": "NM not able to register with RM after NM restart",
            "Description": "After stopping NM gracefully then starting NM, NM registration fails with RM with Duplicate registration from the node! error.\n\n\n{noformat} \n2011-09-23 01:50:46,705 FATAL nodemanager.NodeManager (NodeManager.java:main(204)) - Error starting NodeManager\norg.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager\n\tat org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)\nCaused by: org.apache.avro.AvroRuntimeException: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)\n\tat org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)\n\t... 2 more\nCaused by: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:142)\n\tat $Proxy13.registerNodeManager(Unknown Source)\n\tat org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:175)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)\n\t... 3 more\n{noformat} \n"
        }
    },
    {
        "filename": "MAPREDUCE-3932.json",
        "creation_time": "2012-02-27T22:39:43.000+0000",
        "stack_trace": "```\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_CONTAINER_LAUNCH_FAILED at KILL_TASK_CLEANUP\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:926)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:870)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:862)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)\n\tat java.lang.Thread.run(Thread.java:619)\n\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_COUNTER_UPDATE at ERROR\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:657)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:111)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:848)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:844)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)\n\tat java.lang.Thread.run(Thread.java:619)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.state.StateMachineFactory.doTransition": "    public synchronized STATE doTransition(EVENTTYPE eventType, EVENT event)\n         throws InvalidStateTransitonException  {\n      currentState = StateMachineFactory.this.doTransition\n          (operand, currentState, eventType, event);\n      return currentState;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle": "  public void handle(TaskAttemptEvent event) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Processing \" + event.getTaskAttemptID() + \" of type \"\n          + event.getType());\n    }\n    writeLock.lock();\n    try {\n      final TaskAttemptState oldState = getState();\n      try {\n        stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state for \"\n            + this.attemptId, e);\n        eventHandler.handle(new JobDiagnosticsUpdateEvent(\n            this.attemptId.getTaskId().getJobId(), \"Invalid event \" + event.getType() + \n            \" on TaskAttempt \" + this.attemptId));\n        eventHandler.handle(new JobEvent(this.attemptId.getTaskId().getJobId(),\n            JobEventType.INTERNAL_ERROR));\n      }\n      if (oldState != getState()) {\n          LOG.info(attemptId + \" TaskAttempt Transitioned from \" \n           + oldState + \" to \"\n           + getState());\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getState": "  public TaskAttemptState getState() {\n    readLock.lock();\n    try {\n      return stateMachine.getCurrentState();\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.handle": "    public void handle(SpeculatorEvent event) {\n      if (disabled) {\n        return;\n      }\n\n      TaskId tId = event.getTaskID();\n      TaskType tType = null;\n      /* event's TaskId will be null if the event type is JOB_CREATE or\n       * ATTEMPT_STATUS_UPDATE\n       */\n      if (tId != null) {\n        tType = tId.getTaskType(); \n      }\n      boolean shouldMapSpec =\n              conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false);\n      boolean shouldReduceSpec =\n              conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false);\n\n      /* The point of the following is to allow the MAP and REDUCE speculative\n       * config values to be independent:\n       * IF spec-exec is turned on for maps AND the task is a map task\n       * OR IF spec-exec is turned on for reduces AND the task is a reduce task\n       * THEN call the speculator to handle the event.\n       */\n      if ( (shouldMapSpec && (tType == null || tType == TaskType.MAP))\n        || (shouldReduceSpec && (tType == null || tType == TaskType.REDUCE))) {\n        // Speculator IS enabled, direct the event to there.\n        speculator.handle(event);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.cleanupStagingDir": "  public void cleanupStagingDir() throws IOException {\n    /* make sure we clean the staging files */\n    String jobTempDir = null;\n    FileSystem fs = getFileSystem(getConfig());\n    try {\n      if (!keepJobFiles(new JobConf(getConfig()))) {\n        jobTempDir = getConfig().get(MRJobConfig.MAPREDUCE_JOB_DIR);\n        if (jobTempDir == null) {\n          LOG.warn(\"Job Staging directory is null\");\n          return;\n        }\n        Path jobTempDirPath = new Path(jobTempDir);\n        LOG.info(\"Deleting staging directory \" + FileSystem.getDefaultUri(getConfig()) +\n            \" \" + jobTempDir);\n        fs.delete(jobTempDirPath, true);\n      }\n    } catch(IOException io) {\n      LOG.error(\"Failed to cleanup staging dir \" + jobTempDir, io);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop": "    public synchronized void stop() {\n      ((Service)this.containerLauncher).stop();\n      super.stop();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJobId": "  public JobId getJobId() {\n    return jobId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJob": "    public Job getJob(JobId jobID) {\n      return jobs.get(jobID);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.sysexit": "  protected void sysexit() {\n    System.exit(0);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch": "  protected void dispatch(Event event) {\n    //all events go thru this loop\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Dispatching the event \" + event.getClass().getName() + \".\"\n          + event.toString());\n    }\n\n    Class<? extends Enum> type = event.getType().getDeclaringClass();\n\n    try{\n      eventDispatchers.get(type).handle(event);\n    }\n    catch (Throwable t) {\n      //TODO Maybe log the state of the queue\n      LOG.fatal(\"Error in dispatcher thread\", t);\n      if (exitOnDispatchException) {\n        LOG.info(\"Exiting, bbye..\");\n        System.exit(-1);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.handle": "    public void handle(Event event) {\n      for (EventHandler<Event> handler: listofHandlers) {\n        handler.handle(event);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.run": "      public void run() {\n        while (!stopped && !Thread.currentThread().isInterrupted()) {\n          Event event;\n          try {\n            event = eventQueue.take();\n          } catch(InterruptedException ie) {\n            LOG.warn(\"AsyncDispatcher thread interrupted\", ie);\n            return;\n          }\n          if (event != null) {\n            dispatch(event);\n          }\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle": "  public void handle(JobEvent event) {\n    LOG.debug(\"Processing \" + event.getJobId() + \" of type \" + event.getType());\n    try {\n      writeLock.lock();\n      JobState oldState = getState();\n      try {\n         getStateMachine().doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        addDiagnostic(\"Invalid event \" + event.getType() + \n            \" on Job \" + this.jobId);\n        eventHandler.handle(new JobEvent(this.jobId,\n            JobEventType.INTERNAL_ERROR));\n      }\n      //notify the eventhandler of state change\n      if (oldState != getState()) {\n        LOG.info(jobId + \"Job Transitioned from \" + oldState + \" to \"\n                 + getState());\n      }\n    }\n    \n    finally {\n      writeLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getStateMachine": "  protected StateMachine<JobState, JobEventType, JobEvent> getStateMachine() {\n    return stateMachine;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.addDiagnostic": "  private void addDiagnostic(String diag) {\n    diagnostics.add(diag);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getState": "  public JobState getState() {\n    readLock.lock();\n    try {\n     return getStateMachine().getCurrentState();\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.setConf": "  public void setConf(Configuration conf) {\n    this.conf = conf;\n    \n    numTries = Math.min(\n      conf.getInt(MRJobConfig.MR_JOB_END_RETRY_ATTEMPTS, 0) + 1\n      , conf.getInt(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_ATTEMPTS, 1)\n    );\n    waitInterval = Math.min(\n    conf.getInt(MRJobConfig.MR_JOB_END_RETRY_INTERVAL, 5)\n    , conf.getInt(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_RETRY_INTERVAL, 5)\n    );\n    waitInterval = (waitInterval < 0) ? 5 : waitInterval;\n\n    userUrl = conf.get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL);\n\n    proxyConf = conf.get(MRJobConfig.MR_JOB_END_NOTIFICATION_PROXY);\n\n    //Configure the proxy to use if its set. It should be set like\n    //proxyType@proxyHostname:port\n    if(proxyConf != null && !proxyConf.equals(\"\") &&\n         proxyConf.lastIndexOf(\":\") != -1) {\n      int typeIndex = proxyConf.indexOf(\"@\");\n      Proxy.Type proxyType = Proxy.Type.HTTP;\n      if(typeIndex != -1 &&\n        proxyConf.substring(0, typeIndex).compareToIgnoreCase(\"socks\") == 0) {\n        proxyType = Proxy.Type.SOCKS;\n      }\n      String hostname = proxyConf.substring(typeIndex + 1,\n        proxyConf.lastIndexOf(\":\"));\n      String portConf = proxyConf.substring(proxyConf.lastIndexOf(\":\") + 1);\n      try {\n        int port = Integer.parseInt(portConf);\n        proxyToUse = new Proxy(proxyType,\n          new InetSocketAddress(hostname, port));\n        Log.info(\"Job end notification using proxy type \\\"\" + proxyType + \n        \"\\\" hostname \\\"\" + hostname + \"\\\" and port \\\"\" + port + \"\\\"\");\n      } catch(NumberFormatException nfe) {\n        Log.warn(\"Job end notification couldn't parse configured proxy's port \"\n          + portConf + \". Not going to use a proxy\");\n      }\n    }\n\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notify": "  public void notify(JobReport jobReport)\n    throws InterruptedException {\n    // Do we need job-end notification?\n    if (userUrl == null) {\n      Log.info(\"Job end notification URL not set, skipping.\");\n      return;\n    }\n\n    //Do string replacements for jobId and jobStatus\n    if (userUrl.contains(JOB_ID)) {\n      userUrl = userUrl.replace(JOB_ID, jobReport.getJobId().toString());\n    }\n    if (userUrl.contains(JOB_STATUS)) {\n      userUrl = userUrl.replace(JOB_STATUS, jobReport.getJobState().toString());\n    }\n\n    // Create the URL, ensure sanity\n    try {\n      urlToNotify = new URL(userUrl);\n    } catch (MalformedURLException mue) {\n      Log.warn(\"Job end notification couldn't parse \" + userUrl, mue);\n      return;\n    }\n\n    // Send notification\n    boolean success = false;\n    while (numTries-- > 0 && !success) {\n      Log.info(\"Job end notification attempts left \" + numTries);\n      success = notifyURLOnce();\n      if (!success) {\n        Thread.sleep(waitInterval);\n      }\n    }\n    if (!success) {\n      Log.warn(\"Job end notification failed to notify : \" + urlToNotify);\n    } else {\n      Log.info(\"Job end notification succeeded for \" + jobReport.getJobId());\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notifyURLOnce": "  protected boolean notifyURLOnce() {\n    boolean success = false;\n    try {\n      Log.info(\"Job end notification trying \" + urlToNotify);\n      HttpURLConnection conn =\n        (HttpURLConnection) urlToNotify.openConnection(proxyToUse);\n      conn.setConnectTimeout(5*1000);\n      conn.setReadTimeout(5*1000);\n      conn.setAllowUserInteraction(false);\n      if(conn.getResponseCode() != HttpURLConnection.HTTP_OK) {\n        Log.warn(\"Job end notification to \" + urlToNotify +\" failed with code: \"\n        + conn.getResponseCode() + \" and message \\\"\" + conn.getResponseMessage()\n        +\"\\\"\");\n      }\n      else {\n        success = true;\n        Log.info(\"Job end notification to \" + urlToNotify + \" succeeded\");\n      }\n    } catch(IOException ioe) {\n      Log.warn(\"Job end notification to \" + urlToNotify + \" failed\", ioe);\n    }\n    return success;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.Event.toString": "  String toString();\n}",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.Event.getType": "  TYPE getType();\n  long getTimestamp();\n  String toString();\n}"
        },
        "bug_report": {
            "Title": "MR tasks failing and crashing the AM when available-resources/headRoom becomes zero",
            "Description": "[~karams] reported this offline. One reduce task gets preempted because of zero headRoom and crashes the AM.\n{code}\n2012-02-23 11:30:15,956 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReduces:377 ScheduledMaps:6 ScheduledReduces:23 AssignedMaps:0 AssignedReduces:0 completedMaps:4 completedReduces:0 containersAllocated:4 containersReleased:0 hostLocalAssigned:0 rackLocalAssigned:4 availableResources(headroom):memory: 44544\n2012-02-23 11:30:16,959 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReduces:377 ScheduledMaps:6 ScheduledReduces:23 AssignedMaps:0 AssignedReduces:0 completedMaps:4 completedReduces:0 containersAllocated:4 containersReleased:0 hostLocalAssigned:0 rackLocalAssigned:4 availableResources(headroom):memory: 44544\n2012-02-23 11:30:16,965 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReduces:377 ScheduledMaps:6 ScheduledReduces:23 AssignedMaps:0 AssignedReduces:0 completedMaps:4 completedReduces:0 containersAllocated:4 containersReleased:0 hostLocalAssigned:0 rackLocalAssigned:4 availableResources(headroom):memory: 0\n2012-02-23 11:30:16,965 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Assign: PendingReduces:377 ScheduledMaps:6 ScheduledReduces:23 AssignedMaps:0 AssignedReduces:0 completedMaps:4 completedReduces:0 containersAllocated:4 containersReleased:0 hostLocalAssigned:0 rackLocalAssigned:4 availableResources(headroom):memory: 0\n2012-02-23 11:30:16,965 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 3\n2012-02-23 11:30:16,965 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned to reduce\n2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1329995034628_0983_01_000006 to attempt_1329995034628_0983_r_000000_0\n2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned to reduce\n2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1329995034628_0983_01_000007 to attempt_1329995034628_0983_r_000001_0\n2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned to reduce\n2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1329995034628_0983_01_000008 to attempt_1329995034628_0983_r_000002_0\n2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Assign: PendingReduces:377 ScheduledMaps:6 ScheduledReduces:20 AssignedMaps:0 AssignedReduces:3 completedMaps:4 completedReduces:0 containersAllocated:7 containersReleased:0 hostLocalAssigned:0 rackLocalAssigned:4 availableResources(headroom):memory: 0\n2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Ramping down all scheduled reduces:20\n2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Going to preempt 2\n2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Preempting attempt_1329995034628_0983_r_000002_0\n2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Preempting attempt_1329995034628_0983_r_000001_0\n2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule...\n2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: completedMapPercent 0.4 totalMemLimit:4608 finalMapMemLimit:2765 finalReduceMemLimit:1843 netScheduledMapMem:9216 netScheduledReduceMem:4608\n2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Ramping down 0\n2012-02-23 11:30:16,968 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved $host6 to /$rack6\n2012-02-23 11:30:16,976 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000000_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED\n2012-02-23 11:30:16,976 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved $host1 to /$rack1\n2012-02-23 11:30:16,977 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000001_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED\n2012-02-23 11:30:16,981 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved $host9 to /$rack9\n2012-02-23 11:30:16,982 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000002_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED\n2012-02-23 11:30:16,982 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000002_0 TaskAttempt Transitioned from ASSIGNED to KILL_CONTAINER_CLEANUP\n2012-02-23 11:30:16,983 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000001_0 TaskAttempt Transitioned from ASSIGNED to KILL_CONTAINER_CLEANUP\n2012-02-23 11:30:16,983 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for taskAttempt attempt_1329995034628_0983_r_000000_0\n2012-02-23 11:30:16,983 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for taskAttempt attempt_1329995034628_0983_r_000002_0\n2012-02-23 11:30:16,983 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1329995034628_0983_r_000000_0\n2012-02-23 11:30:16,984 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1329995034628_0983_r_000002_0\n2012-02-23 11:30:16,984 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for taskAttempt attempt_1329995034628_0983_r_000002_0\n2012-02-23 11:30:16,984 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for taskAttempt attempt_1329995034628_0983_r_000001_0\n2012-02-23 11:30:16,987 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for taskAttempt attempt_1329995034628_0983_r_000001_0\n2012-02-23 11:30:16,988 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1329995034628_0983_r_000001_0\n2012-02-23 11:30:16,988 ERROR [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Container was killed before it was launched\n2012-02-23 11:30:17,061 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1329995034628_0983_r_000000_0 : 53990\n2012-02-23 11:30:17,077 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000001_0 TaskAttempt Transitioned from KILL_CONTAINER_CLEANUP to KILL_TASK_CLEANUP\n2012-02-23 11:30:17,077 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1329995034628_0983_r_000001_0: Container was killed before it was launched\n2012-02-23 11:30:17,078 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Can't handle this event at current state for attempt_1329995034628_0983_r_000001_0\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_CONTAINER_LAUNCH_FAILED at KILL_TASK_CLEANUP\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:926)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:870)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:862)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)\n\tat java.lang.Thread.run(Thread.java:619)\n2012-02-23 11:30:17,080 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1329995034628_0983_r_000000_0] using containerId: [container_1329995034628_0983_01_000006 on NM: [$host6:51529]\n2012-02-23 11:30:17,081 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000000_0 TaskAttempt Transitioned from ASSIGNED to RUNNING\n2012-02-23 11:30:17,207 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1329995034628_0983_r_000002_0 : 47960\n2012-02-23 11:30:17,207 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1329995034628_0983_r_000002_0\n2012-02-23 11:30:17,215 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1329995034628_0983Job Transitioned from RUNNING to ERROR\n2012-02-23 11:30:17,216 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_COUNTER_UPDATE at ERROR\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:657)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:111)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:848)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:844)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)\n\tat java.lang.Thread.run(Thread.java:619)\n\n{code}"
        }
    },
    {
        "filename": "MAPREDUCE-4062.json",
        "creation_time": "2012-03-23T21:42:23.000+0000",
        "stack_trace": "```\njava.lang.Thread.State: WAITING (on object monitor)\n    at java.lang.Object.wait(Native Method)\n    at java.lang.Object.wait(Object.java:485)\n    at org.apache.hadoop.ipc.Client.call(Client.java:1076)\n    - locked <0x00002aab05a4f3f0> (a org.apache.hadoop.ipc.Client$Call)\n    at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:135)\n    at $Proxy76.startContainer(Unknown Source)\n    at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:87)\n    at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)\n    at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:265)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n    at java.lang.Thread.run(Thread.java:619)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.call": "  public Writable[] call(Writable[] params, InetSocketAddress[] addresses,\n      Class<?> protocol, UserGroupInformation ticket, Configuration conf)\n      throws IOException, InterruptedException {\n    if (addresses.length == 0) return new Writable[0];\n\n    ParallelResults results = new ParallelResults(params.length);\n    synchronized (results) {\n      for (int i = 0; i < params.length; i++) {\n        ParallelCall call = new ParallelCall(params[i], results, i);\n        try {\n          ConnectionId remoteId = ConnectionId.getConnectionId(addresses[i],\n              protocol, ticket, 0, conf);\n          Connection connection = getConnection(remoteId, call);\n          connection.sendParam(call);             // send each parameter\n        } catch (IOException e) {\n          // log errors\n          LOG.info(\"Calling \"+addresses[i]+\" caught: \" + \n                   e.getMessage(),e);\n          results.size--;                         //  wait for one fewer result\n        }\n      }\n      while (results.count != results.size) {\n        try {\n          results.wait();                    // wait for all results\n        } catch (InterruptedException e) {}\n      }\n\n      return results.values;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnectionId": "    public static ConnectionId getConnectionId(InetSocketAddress addr,\n        Class<?> protocol, UserGroupInformation ticket, int rpcTimeout,\n        Configuration conf) throws IOException {\n      String remotePrincipal = getRemotePrincipal(conf, addr, protocol);\n      boolean doPing =\n        conf.getBoolean(CommonConfigurationKeys.IPC_CLIENT_PING_KEY, true);\n      return new ConnectionId(addr, protocol, ticket,\n          rpcTimeout, remotePrincipal,\n          conf.getInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECTION_MAXIDLETIME_KEY,\n              CommonConfigurationKeysPublic.IPC_CLIENT_CONNECTION_MAXIDLETIME_DEFAULT),\n          conf.getInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY,\n              CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_DEFAULT),\n          conf.getInt(\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_KEY,\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_DEFAULT),\n          conf.getBoolean(CommonConfigurationKeysPublic.IPC_CLIENT_TCPNODELAY_KEY,\n              CommonConfigurationKeysPublic.IPC_CLIENT_TCPNODELAY_DEFAULT),\n          doPing, \n          (doPing ? Client.getPingInterval(conf) : 0));\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.sendParam": "    public void sendParam(Call call) {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n\n      DataOutputBuffer d=null;\n      try {\n        synchronized (this.out) {\n          if (LOG.isDebugEnabled())\n            LOG.debug(getName() + \" sending #\" + call.id);\n          \n          //for serializing the\n          //data to be written\n          d = new DataOutputBuffer();\n          d.writeInt(0); // placeholder for data length\n          RpcPayloadHeader header = new RpcPayloadHeader(\n              call.rpcKind, RpcPayloadOperation.RPC_FINAL_PAYLOAD, call.id);\n          header.write(d);\n          call.rpcRequest.write(d);\n          byte[] data = d.getData();\n          int dataLength = d.getLength() - 4;\n          data[0] = (byte)((dataLength >>> 24) & 0xff);\n          data[1] = (byte)((dataLength >>> 16) & 0xff);\n          data[2] = (byte)((dataLength >>> 8) & 0xff);\n          data[3] = (byte)(dataLength & 0xff);\n          out.write(data, 0, dataLength + 4);//write the data\n          out.flush();\n        }\n      } catch(IOException e) {\n        markClosed(e);\n      } finally {\n        //the buffer is just an in-memory buffer, but it is still polite to\n        // close early\n        IOUtils.closeStream(d);\n      }\n    }  ",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getAddress": "    InetSocketAddress getAddress() {\n      return address;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnection": "  private Connection getConnection(ConnectionId remoteId,\n                                   Call call)\n                                   throws IOException, InterruptedException {\n    if (!running.get()) {\n      // the client is stopped\n      throw new IOException(\"The client is stopped\");\n    }\n    Connection connection;\n    /* we could avoid this allocation for each RPC by having a  \n     * connectionsId object and with set() method. We need to manage the\n     * refs for keys in HashMap properly. For now its ok.\n     */\n    do {\n      synchronized (connections) {\n        connection = connections.get(remoteId);\n        if (connection == null) {\n          connection = new Connection(remoteId);\n          connections.put(remoteId, connection);\n        }\n      }\n    } while (!connection.addCall(call));\n    \n    //we don't invoke the method below inside \"synchronized (connections)\"\n    //block above. The reason for that is if the server happens to be slow,\n    //it will take longer to establish a connection and that will slow the\n    //entire system down.\n    connection.setupIOstreams();\n    return connection;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.invoke": "    public Object invoke(Object proxy, Method method, Object[] args)\n        throws Throwable {\n      long startTime = 0;\n      if (LOG.isDebugEnabled()) {\n        startTime = System.currentTimeMillis();\n      }\n\n      ProtoSpecificRpcRequest rpcRequest = constructRpcRequest(method, args);\n      ProtoSpecificResponseWritable val = null;\n      try {\n        val = (ProtoSpecificResponseWritable) client.call(\n            new ProtoSpecificRequestWritable(rpcRequest), remoteId);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      \n      ProtoSpecificRpcResponse response = val.message;\n   \n      if (LOG.isDebugEnabled()) {\n        long callTime = System.currentTimeMillis() - startTime;\n        LOG.debug(\"Call: \" + method.getName() + \" \" + callTime);\n      }\n \n      if (response.hasIsError() && response.getIsError() == true) {\n        YarnRemoteExceptionPBImpl exception = new YarnRemoteExceptionPBImpl(response.getException());\n        exception.fillInStackTrace();\n        ServiceException se = new ServiceException(exception);\n        throw se;\n      }\n      \n      Message prototype = null;\n      try {\n        prototype = getReturnProtoType(method);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      Message actualReturnMessage = prototype.newBuilderForType()\n          .mergeFrom(response.getResponseProto()).build();\n      return actualReturnMessage;\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.constructRpcRequest": "    private ProtoSpecificRpcRequest constructRpcRequest(Method method,\n        Object[] params) throws ServiceException {\n      ProtoSpecificRpcRequest rpcRequest;\n      ProtoSpecificRpcRequest.Builder builder;\n\n      builder = ProtoSpecificRpcRequest.newBuilder();\n      builder.setMethodName(method.getName());\n\n      if (params.length != 2) { // RpcController + Message\n        throw new ServiceException(\"Too many parameters for request. Method: [\"\n            + method.getName() + \"]\" + \", Expected: 2, Actual: \"\n            + params.length);\n      }\n      if (params[1] == null) {\n        throw new ServiceException(\"null param while calling Method: [\"\n            + method.getName() + \"]\");\n      }\n\n      Message param = (Message) params[1];\n      builder.setRequestProto(param.toByteString());\n\n      rpcRequest = builder.build();\n      return rpcRequest;\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.call": "    public Writable call(RpcKind rpcKind, String protocol, \n        Writable writableRequest, long receiveTime) throws IOException {\n      ProtoSpecificRequestWritable request = (ProtoSpecificRequestWritable) writableRequest;\n      ProtoSpecificRpcRequest rpcRequest = request.message;\n      String methodName = rpcRequest.getMethodName();\n      if (verbose) {\n        log(\"Call: protocol=\" + protocol + \", method=\"\n            + methodName);\n      }\n      MethodDescriptor methodDescriptor = service.getDescriptorForType()\n          .findMethodByName(methodName);\n      if (methodDescriptor == null) {\n        String msg = \"Unknown method \" + methodName + \" called on \"\n            + protocol + \" protocol.\";\n        LOG.warn(msg);\n        return handleException(new IOException(msg));\n      }\n      Message prototype = service.getRequestPrototype(methodDescriptor);\n      Message param = prototype.newBuilderForType()\n          .mergeFrom(rpcRequest.getRequestProto()).build();\n      Message result;\n      try {\n        result = service.callBlockingMethod(methodDescriptor, null, param);\n      } catch (ServiceException e) {\n        e.printStackTrace();\n        return handleException(e);\n      } catch (Exception e) {\n        return handleException(e);\n      }\n\n      ProtoSpecificRpcResponse response = constructProtoSpecificRpcSuccessResponse(result);\n      return new ProtoSpecificResponseWritable(response);\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.getReturnProtoType": "    private Message getReturnProtoType(Method method) throws Exception {\n      if (returnTypes.containsKey(method.getName())) {\n        return returnTypes.get(method.getName());\n      } else {\n        Class<?> returnType = method.getReturnType();\n\n        Method newInstMethod = returnType.getMethod(\"getDefaultInstance\");\n        newInstMethod.setAccessible(true);\n        Message prototype = (Message) newInstMethod.invoke(null,\n            (Object[]) null);\n        returnTypes.put(method.getName(), prototype);\n        return prototype;\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer": "  public StartContainerResponse startContainer(StartContainerRequest request)\n      throws YarnRemoteException {\n    StartContainerRequestProto requestProto = ((StartContainerRequestPBImpl)request).getProto();\n    try {\n      return new StartContainerResponsePBImpl(proxy.startContainer(null, requestProto));\n    } catch (ServiceException e) {\n      if (e.getCause() instanceof YarnRemoteException) {\n        throw (YarnRemoteException)e.getCause();\n      } else if (e.getCause() instanceof UndeclaredThrowableException) {\n        throw (UndeclaredThrowableException)e.getCause();\n      } else {\n        throw new UndeclaredThrowableException(e);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch": "  private void launch() throws IOException {\n    connect();\n    ContainerId masterContainerID = application.getMasterContainer().getId();\n    ApplicationSubmissionContext applicationContext =\n      application.getSubmissionContext();\n    LOG.info(\"Setting up container \" + application.getMasterContainer() \n        + \" for AM \" + application.getAppAttemptId());  \n    ContainerLaunchContext launchContext =\n        createAMContainerLaunchContext(applicationContext, masterContainerID);\n    StartContainerRequest request = \n        recordFactory.newRecordInstance(StartContainerRequest.class);\n    request.setContainerLaunchContext(launchContext);\n    containerMgrProxy.startContainer(request);\n    LOG.info(\"Done launching container \" + application.getMasterContainer() \n        + \" for AM \" + application.getAppAttemptId());\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.createAMContainerLaunchContext": "  private ContainerLaunchContext createAMContainerLaunchContext(\n      ApplicationSubmissionContext applicationMasterContext,\n      ContainerId containerID) throws IOException {\n\n    // Construct the actual Container\n    ContainerLaunchContext container = \n        applicationMasterContext.getAMContainerSpec();\n    LOG.info(\"Command to launch container \"\n        + containerID\n        + \" : \"\n        + StringUtils.arrayToString(container.getCommands().toArray(\n            new String[0])));\n    \n    // Finalize the container\n    container.setContainerId(containerID);\n    container.setUser(applicationMasterContext.getUser());\n    setupTokensAndEnv(container);\n    \n    return container;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.connect": "  private void connect() throws IOException {\n    ContainerId masterContainerID = application.getMasterContainer().getId();\n    \n    containerMgrProxy = getContainerMgrProxy(masterContainerID);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run": "  public void run() {\n    switch (eventType) {\n    case LAUNCH:\n      try {\n        LOG.info(\"Launching master\" + application.getAppAttemptId());\n        launch();\n        handler.handle(new RMAppAttemptEvent(application.getAppAttemptId(),\n            RMAppAttemptEventType.LAUNCHED));\n      } catch(Exception ie) {\n        String message = \"Error launching \" + application.getAppAttemptId()\n            + \". Got exception: \" + StringUtils.stringifyException(ie);\n        LOG.info(message);\n        handler.handle(new RMAppAttemptLaunchFailedEvent(application\n            .getAppAttemptId(), message));\n      }\n      break;\n    case CLEANUP:\n      try {\n        LOG.info(\"Cleaning master \" + application.getAppAttemptId());\n        cleanup();\n      } catch(IOException ie) {\n        LOG.info(\"Error cleaning master \", ie);\n      }\n      break;\n    default:\n      LOG.warn(\"Received unknown event-type \" + eventType + \". Ignoring.\");\n      break;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.cleanup": "  private void cleanup() throws IOException {\n    connect();\n    ContainerId containerId = application.getMasterContainer().getId();\n    StopContainerRequest stopRequest = \n        recordFactory.newRecordInstance(StopContainerRequest.class);\n    stopRequest.setContainerId(containerId);\n    containerMgrProxy.stopContainer(stopRequest);\n  }"
        },
        "bug_report": {
            "Title": "AM Launcher thread can hang forever",
            "Description": "We saw an instance where the RM stopped launch Application masters.  We found that the launcher thread was hung because something weird/bad happened to the NM node. Currently there is only 1 launcher thread (jira 4061 to fix that). We need this to not happen.  Even once we increase the number of threads  to > 1 if that many nodes go bad the RM would be stuck.  Note that this was stuck like this for approximately 9 hours.\n\nStack trace on hung AM launcher:\n\n\"pool-1-thread-1\" prio=10 tid=0x000000004343e800 nid=0x3a4c in Object.wait()\n[0x000000004fad2000]\n   java.lang.Thread.State: WAITING (on object monitor)\n    at java.lang.Object.wait(Native Method)\n    at java.lang.Object.wait(Object.java:485)\n    at org.apache.hadoop.ipc.Client.call(Client.java:1076)\n    - locked <0x00002aab05a4f3f0> (a org.apache.hadoop.ipc.Client$Call)\n    at\norg.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:135)\n    at $Proxy76.startContainer(Unknown Source)\n    at\norg.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:87)\n    at\norg.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)\n    at\norg.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:265)\n    at\njava.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n    at\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n    at java.lang.Thread.run(Thread.java:619)"
        }
    },
    {
        "filename": "MAPREDUCE-3066.json",
        "creation_time": "2011-09-21T22:42:00.000+0000",
        "stack_trace": "```\norg.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager\n\tat org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)\nCaused by: org.apache.avro.AvroRuntimeException: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)\n\tat org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)\n\t... 2 more\nCaused by: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address\n\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)\n\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.getRMClient(NodeStatusUpdaterImpl.java:154)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:164)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)\n\t... 3 more\n\njava.lang.IllegalStateException: For this operation, current State must be STARTED instead of INITED\n\tat org.apache.hadoop.yarn.service.AbstractService.ensureCurrentState(AbstractService.java:101)\n\tat org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:69)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.stop(NodeStatusUpdaterImpl.java:149)\n\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:95)\n\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:85)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:158)\n\tat org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:118)\n\njava.lang.IllegalStateException: For this operation, current State must be STARTED instead of INITED\n\tat org.apache.hadoop.yarn.service.AbstractService.ensureCurrentState(AbstractService.java:101)\n\tat org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:69)\n\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:87)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:158)\n\tat org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:118)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.service.CompositeService.start": "  public synchronized void start() {\n    int i = 0;\n    try {\n      for (int n = serviceList.size(); i < n; i++) {\n        Service service = serviceList.get(i);\n        service.start();\n      }\n      super.start();\n    } catch (Throwable e) {\n      LOG.error(\"Error starting services \" + getName(), e);\n      // Note that the state of the failed service is still INITED and not\n      // STARTED. Even though the last service is not started completely, still\n      // call stop() on all services including failed service to make sure cleanup\n      // happens.\n      stop(i);\n      throw new YarnException(\"Failed to Start \" + getName(), e);\n    }\n\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.service.CompositeService.stop": "  private synchronized void stop(int numOfServicesStarted) {\n    // stop in reserve order of start\n    for (int i = numOfServicesStarted; i >= 0; i--) {\n      Service service = serviceList.get(i);\n      try {\n        service.stop();\n      } catch (Throwable t) {\n        LOG.info(\"Error stopping \" + service.getName(), t);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.start": "  public void start() {\n    try {\n      doSecureLogin();\n    } catch (IOException e) {\n      throw new YarnException(\"Failed NodeManager login\", e);\n    }\n    super.start();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.doSecureLogin": "  protected void doSecureLogin() throws IOException {\n    SecurityUtil.login(getConfig(), YarnConfiguration.NM_KEYTAB,\n        YarnConfiguration.NM_PRINCIPAL);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.main": "  public static void main(String[] args) {\n    StringUtils.startupShutdownMessage(NodeManager.class, args, LOG);\n    try {\n      NodeManager nodeManager = new NodeManager();\n      Runtime.getRuntime().addShutdownHook(\n          new CompositeServiceShutdownHook(nodeManager));\n      YarnConfiguration conf = new YarnConfiguration();\n      nodeManager.init(conf);\n      nodeManager.start();\n    } catch (Throwable t) {\n      LOG.fatal(\"Error starting NodeManager\", t);\n      System.exit(-1);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.init": "  public void init(Configuration conf) {\n\n    Context context = new NMContext();\n\n    // Create the secretManager if need be.\n    if (UserGroupInformation.isSecurityEnabled()) {\n      LOG.info(\"Security is enabled on NodeManager. \"\n          + \"Creating ContainerTokenSecretManager\");\n      this.containerTokenSecretManager = new ContainerTokenSecretManager();\n    }\n\n    ContainerExecutor exec = ReflectionUtils.newInstance(\n        conf.getClass(YarnConfiguration.NM_CONTAINER_EXECUTOR,\n          DefaultContainerExecutor.class, ContainerExecutor.class), conf);\n    DeletionService del = new DeletionService(exec);\n    addService(del);\n\n    // NodeManager level dispatcher\n    AsyncDispatcher dispatcher = new AsyncDispatcher();\n\n    NodeHealthCheckerService healthChecker = null;\n    if (NodeHealthCheckerService.shouldRun(conf)) {\n      healthChecker = new NodeHealthCheckerService();\n      addService(healthChecker);\n    }\n\n    NodeStatusUpdater nodeStatusUpdater =\n        createNodeStatusUpdater(context, dispatcher, healthChecker, \n        this.containerTokenSecretManager);\n\n    NodeResourceMonitor nodeResourceMonitor = createNodeResourceMonitor();\n    addService(nodeResourceMonitor);\n\n    ContainerManagerImpl containerManager =\n        createContainerManager(context, exec, del, nodeStatusUpdater,\n        this.containerTokenSecretManager);\n    addService(containerManager);\n\n    Service webServer =\n        createWebServer(context, containerManager.getContainersMonitor());\n    addService(webServer);\n\n    dispatcher.register(ContainerManagerEventType.class, containerManager);\n    addService(dispatcher);\n\n    DefaultMetricsSystem.initialize(\"NodeManager\");\n\n    // StatusUpdater should be added last so that it get started last \n    // so that we make sure everything is up before registering with RM. \n    addService(nodeStatusUpdater);\n\n    super.init(conf);\n    // TODO add local dirs to del\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start": "  public void start() {\n    String cmBindAddressStr =\n        getConfig().get(YarnConfiguration.NM_ADDRESS,\n            YarnConfiguration.DEFAULT_NM_ADDRESS);\n    InetSocketAddress cmBindAddress =\n        NetUtils.createSocketAddr(cmBindAddressStr);\n    String httpBindAddressStr =\n      getConfig().get(YarnConfiguration.NM_WEBAPP_ADDRESS,\n          YarnConfiguration.DEFAULT_NM_WEBAPP_ADDRESS);\n    InetSocketAddress httpBindAddress =\n      NetUtils.createSocketAddr(httpBindAddressStr);\n    try {\n      this.hostName = InetAddress.getLocalHost().getHostAddress();\n      this.containerManagerPort = cmBindAddress.getPort();\n      this.httpPort = httpBindAddress.getPort();\n      this.containerManagerBindAddress =\n          this.hostName + \":\" + this.containerManagerPort;\n      LOG.info(\"Configured ContainerManager Address is \"\n          + this.containerManagerBindAddress);\n      // Registration has to be in start so that ContainerManager can get the\n      // perNM tokens needed to authenticate ContainerTokens.\n      registerWithRM();\n      super.start();\n      startStatusUpdater();\n    } catch (Exception e) {\n      throw new AvroRuntimeException(e);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM": "  private void registerWithRM() throws YarnRemoteException {\n    this.resourceTracker = getRMClient();\n    LOG.info(\"Connected to ResourceManager at \" + this.rmAddress);\n    \n    RegisterNodeManagerRequest request = recordFactory.newRecordInstance(RegisterNodeManagerRequest.class);\n    this.nodeId = Records.newRecord(NodeId.class);\n    this.nodeId.setHost(this.hostName);\n    this.nodeId.setPort(this.containerManagerPort);\n    request.setHttpPort(this.httpPort);\n    request.setResource(this.totalResource);\n    request.setNodeId(this.nodeId);\n    RegistrationResponse regResponse =\n        this.resourceTracker.registerNodeManager(request).getRegistrationResponse();\n    if (UserGroupInformation.isSecurityEnabled()) {\n      this.secretKeyBytes = regResponse.getSecretKey().array();\n    }\n\n    // do this now so that its set before we start heartbeating to RM\n    if (UserGroupInformation.isSecurityEnabled()) {\n      LOG.info(\"Security enabled - updating secret keys now\");\n      // It is expected that status updater is started by this point and\n      // RM gives the shared secret in registration during StatusUpdater#start().\n      this.containerTokenSecretManager.setSecretKey(\n          this.getContainerManagerBindAddress(),\n          this.getRMNMSharedSecret());\n    }\n    LOG.info(\"Registered with ResourceManager as \" + this.containerManagerBindAddress\n        + \" with total resource of \" + this.totalResource);\n\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.startStatusUpdater": "  protected void startStatusUpdater() {\n\n    new Thread() {\n      @Override\n      public void run() {\n        int lastHeartBeatID = 0;\n        while (!isStopped) {\n          // Send heartbeat\n          try {\n            synchronized (heartbeatMonitor) {\n              heartbeatMonitor.wait(heartBeatInterval);\n            }\n            NodeStatus nodeStatus = getNodeStatus();\n            nodeStatus.setResponseId(lastHeartBeatID);\n            \n            NodeHeartbeatRequest request = recordFactory.newRecordInstance(NodeHeartbeatRequest.class);\n            request.setNodeStatus(nodeStatus);            \n            HeartbeatResponse response =\n              resourceTracker.nodeHeartbeat(request).getHeartbeatResponse();\n            lastHeartBeatID = response.getResponseId();\n            List<ContainerId> containersToCleanup = response\n                .getContainersToCleanupList();\n            if (containersToCleanup.size() != 0) {\n              dispatcher.getEventHandler().handle(\n                  new CMgrCompletedContainersEvent(containersToCleanup));\n            }\n            List<ApplicationId> appsToCleanup =\n                response.getApplicationsToCleanupList();\n            if (appsToCleanup.size() != 0) {\n              dispatcher.getEventHandler().handle(\n                  new CMgrCompletedAppsEvent(appsToCleanup));\n            }\n          } catch (Throwable e) {\n            LOG.error(\"Caught exception in status-updater\", e);\n            break;\n          }\n        }\n      }\n    }.start();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.createSocketAddr": "  public static InetSocketAddress createSocketAddr(String target,\n                                                   int defaultPort) {\n    if (target == null) {\n      throw new IllegalArgumentException(\"Target address cannot be null.\");\n    }\n    int colonIndex = target.indexOf(':');\n    if (colonIndex < 0 && defaultPort == -1) {\n      throw new RuntimeException(\"Not a host:port pair: \" + target);\n    }\n    String hostname;\n    int port = -1;\n    if (!target.contains(\"/\")) {\n      if (colonIndex == -1) {\n        hostname = target;\n      } else {\n        // must be the old style <host>:<port>\n        hostname = target.substring(0, colonIndex);\n        port = Integer.parseInt(target.substring(colonIndex + 1));\n      }\n    } else {\n      // a new uri\n      URI addr = new Path(target).toUri();\n      hostname = addr.getHost();\n      port = addr.getPort();\n    }\n\n    if (port == -1) {\n      port = defaultPort;\n    }\n  \n    if (getStaticResolution(hostname) != null) {\n      hostname = getStaticResolution(hostname);\n    }\n    return new InetSocketAddress(hostname, port);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.getStaticResolution": "  public static String getStaticResolution(String host) {\n    synchronized (hostToResolved) {\n      return hostToResolved.get(host);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.getRMClient": "  protected ResourceTracker getRMClient() {\n    YarnRPC rpc = YarnRPC.create(getConfig());\n    InetSocketAddress rmAddress = NetUtils.createSocketAddr(this.rmAddress);\n    Configuration rmClientConf = new Configuration(getConfig());\n    rmClientConf.setClass(\n        YarnConfiguration.YARN_SECURITY_INFO,\n        RMNMSecurityInfoClass.class, SecurityInfo.class);\n    return (ResourceTracker) rpc.getProxy(ResourceTracker.class, rmAddress,\n        rmClientConf);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.service.AbstractService.ensureCurrentState": "  private void ensureCurrentState(STATE currentState) {\n    if (state != currentState) {\n      throw new IllegalStateException(\"For this operation, current State must \" +\n        \"be \" + currentState + \" instead of \" + state);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.service.AbstractService.stop": "  public synchronized void stop() {\n    if (state == STATE.STOPPED) {\n      return;//already stopped\n    }\n    ensureCurrentState(STATE.STARTED);\n    changeState(STATE.STOPPED);\n    LOG.info(\"Service:\" + getName() + \" is stopped.\");\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.service.AbstractService.changeState": "  private void changeState(STATE newState) {\n    state = newState;\n    //notify listeners\n    for (ServiceStateChangeListener l : listeners) {\n      l.stateChanged(this);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.service.AbstractService.getName": "  public String getName() {\n    return name;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.stop": "  public synchronized void stop() {\n    // Interrupt the updater.\n    this.isStopped = true;\n    super.stop();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop": "  public void stop() {\n    super.stop();\n    DefaultMetricsSystem.shutdown();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.service.CompositeService.run": "    public void run() {\n      try {\n        // Stop the Composite Service\n        compositeService.stop();\n      } catch (Throwable t) {\n        LOG.info(\"Error stopping \" + compositeService.getName(), t);\n      }\n    }"
        },
        "bug_report": {
            "Title": "YARN NM fails to start",
            "Description": "Please check conf.get() calls. Every time I svn up, I get one of these.\n\n\n2011-09-21 15:36:33,534 INFO  service.AbstractService (AbstractService.java:stop(71)) - Service:org.apache.hadoop.yarn.server.nodemanager.DeletionService is stopped.\n2011-09-21 15:36:33,534 FATAL nodemanager.NodeManager (NodeManager.java:main(204)) - Error starting NodeManager\norg.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager\n\tat org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)\nCaused by: org.apache.avro.AvroRuntimeException: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)\n\tat org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)\n\t... 2 more\nCaused by: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address\n\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)\n\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.getRMClient(NodeStatusUpdaterImpl.java:154)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:164)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)\n\t... 3 more\n2011-09-21 15:36:33,535 INFO  service.CompositeService (CompositeService.java:stop(97)) - Error stopping org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl\njava.lang.IllegalStateException: For this operation, current State must be STARTED instead of INITED\n\tat org.apache.hadoop.yarn.service.AbstractService.ensureCurrentState(AbstractService.java:101)\n\tat org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:69)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.stop(NodeStatusUpdaterImpl.java:149)\n\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:95)\n\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:85)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:158)\n\tat org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:118)\n2011-09-21 15:36:33,535 INFO  nodemanager.NodeManager (StringUtils.java:run(605)) - SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down NodeManager at criccomi-ld/127.0.0.1\n************************************************************/\n2011-09-21 15:36:33,536 INFO  ipc.Server (Server.java:stop(1708)) - Stopping server on 45454\n2011-09-21 15:36:33,536 INFO  logaggregation.LogAggregationService (LogAggregationService.java:stop(116)) - org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService waiting for pending aggregation during exit\n2011-09-21 15:36:33,536 INFO  ipc.Server (Server.java:stop(1708)) - Stopping server on 4344\n2011-09-21 15:36:33,536 INFO  service.CompositeService (CompositeService.java:run(120)) - Error stopping org.apache.hadoop.yarn.server.nodemanager.NodeManager\njava.lang.IllegalStateException: For this operation, current State must be STARTED instead of INITED\n\tat org.apache.hadoop.yarn.service.AbstractService.ensureCurrentState(AbstractService.java:101)\n\tat org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:69)\n\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:87)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:158)\n\tat org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:118)\n"
        }
    },
    {
        "filename": "MAPREDUCE-3123.json",
        "creation_time": "2011-09-29T19:03:45.000+0000",
        "stack_trace": "```\norg.apache.hadoop.util.Shell$ExitCodeException:\n/tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001/task.sh:\nline 26: syntax error near unexpected token `-_+='       \n/tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001/task.sh:\nline 26: `ln -sf /tmp/mapred-local/usercache/hadoopqa/filecache/-1888139433818483070/InputDir test\nink!@$&*()-_+='kson-jaxrs-1.7.1.jar:/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.7.3/jackson-mapper-asl-1.7.3.jar:.m2/repository/org/codehaus/jackson/jackson-xc/1.7.1/jackson-xc-1.7.1.jar:\n\n     at org.apache.hadoop.util.Shell.runCommand(Shell.java:261)                                                        \n                                                                                                                       \n   at org.apache.hadoop.util.Shell.run(Shell.java:188)                                                                 \n                                                                                                                       \n at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:381)                                          \n                                                                                                                      \nat org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:174)   \n                                                                                                                     \nat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:197)  \n                                                                                                                     \nat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:62)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.runCommand": "  private void runCommand() throws IOException { \n    ProcessBuilder builder = new ProcessBuilder(getExecString());\n    Timer timeOutTimer = null;\n    ShellTimeoutTimerTask timeoutTimerTask = null;\n    timedOut = new AtomicBoolean(false);\n    completed = new AtomicBoolean(false);\n    \n    if (environment != null) {\n      builder.environment().putAll(this.environment);\n    }\n    if (dir != null) {\n      builder.directory(this.dir);\n    }\n    \n    process = builder.start();\n    if (timeOutInterval > 0) {\n      timeOutTimer = new Timer(\"Shell command timeout\");\n      timeoutTimerTask = new ShellTimeoutTimerTask(\n          this);\n      //One time scheduling.\n      timeOutTimer.schedule(timeoutTimerTask, timeOutInterval);\n    }\n    final BufferedReader errReader = \n            new BufferedReader(new InputStreamReader(process\n                                                     .getErrorStream()));\n    BufferedReader inReader = \n            new BufferedReader(new InputStreamReader(process\n                                                     .getInputStream()));\n    final StringBuffer errMsg = new StringBuffer();\n    \n    // read error and input streams as this would free up the buffers\n    // free the error stream buffer\n    Thread errThread = new Thread() {\n      @Override\n      public void run() {\n        try {\n          String line = errReader.readLine();\n          while((line != null) && !isInterrupted()) {\n            errMsg.append(line);\n            errMsg.append(System.getProperty(\"line.separator\"));\n            line = errReader.readLine();\n          }\n        } catch(IOException ioe) {\n          LOG.warn(\"Error reading the error stream\", ioe);\n        }\n      }\n    };\n    try {\n      errThread.start();\n    } catch (IllegalStateException ise) { }\n    try {\n      parseExecResult(inReader); // parse the output\n      // clear the input stream buffer\n      String line = inReader.readLine();\n      while(line != null) { \n        line = inReader.readLine();\n      }\n      // wait for the process to finish and check the exit code\n      exitCode  = process.waitFor();\n      try {\n        // make sure that the error thread exits\n        errThread.join();\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Interrupted while reading the error stream\", ie);\n      }\n      completed.set(true);\n      //the timeout thread handling\n      //taken care in finally block\n      if (exitCode != 0) {\n        throw new ExitCodeException(exitCode, errMsg.toString());\n      }\n    } catch (InterruptedException ie) {\n      throw new IOException(ie.toString());\n    } finally {\n      if (timeOutTimer != null) {\n        timeOutTimer.cancel();\n      }\n      // close the input stream\n      try {\n        inReader.close();\n      } catch (IOException ioe) {\n        LOG.warn(\"Error while closing the input stream\", ioe);\n      }\n      if (!completed.get()) {\n        errThread.interrupt();\n      }\n      try {\n        errReader.close();\n      } catch (IOException ioe) {\n        LOG.warn(\"Error while closing the error stream\", ioe);\n      }\n      process.destroy();\n      lastTime = System.currentTimeMillis();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.toString": "    public String toString() {\n      StringBuilder builder = new StringBuilder();\n      String[] args = getExecString();\n      for (String s : args) {\n        if (s.indexOf(' ') >= 0) {\n          builder.append('\"').append(s).append('\"');\n        } else {\n          builder.append(s);\n        }\n        builder.append(' ');\n      }\n      return builder.toString();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.parseExecResult": "    protected void parseExecResult(BufferedReader lines) throws IOException {\n      output = new StringBuffer();\n      char[] buf = new char[512];\n      int nRead;\n      while ( (nRead = lines.read(buf, 0, buf.length)) > 0 ) {\n        output.append(buf, 0, nRead);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.getExecString": "    public String[] getExecString() {\n      return command;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.run": "    public void run() {\n      Process p = shell.getProcess();\n      try {\n        p.exitValue();\n      } catch (Exception e) {\n        //Process has not terminated.\n        //So check if it has completed \n        //if not just destroy it.\n        if (p != null && !shell.completed.get()) {\n          shell.setTimedOut();\n          p.destroy();\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.getProcess": "  public Process getProcess() {\n    return process;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.setTimedOut": "  private void setTimedOut() {\n    this.timedOut.set(true);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.execute": "    public void execute() throws IOException {\n      this.run();    \n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer": "  public int launchContainer(Container container,\n      Path nmPrivateCotainerScriptPath, Path nmPrivateTokensPath,\n      String user, String appId, Path containerWorkDir) throws IOException {\n\n    ContainerId containerId = container.getContainerID();\n    String containerIdStr = ConverterUtils.toString(containerId);\n    List<String> command = new ArrayList<String>(\n      Arrays.asList(containerExecutorExe, \n                    user, \n                    Integer.toString(Commands.LAUNCH_CONTAINER.getValue()),\n                    appId,\n                    containerIdStr,\n                    containerWorkDir.toString(),\n                    nmPrivateCotainerScriptPath.toUri().getPath().toString(),\n                    nmPrivateTokensPath.toUri().getPath().toString()));\n    String[] commandArray = command.toArray(new String[command.size()]);\n    ShellCommandExecutor shExec = \n        new ShellCommandExecutor(\n            commandArray,\n            null,                                              // NM's cwd\n            container.getLaunchContext().getEnvironment());    // sanitized env\n    launchCommandObjs.put(containerId, shExec);\n    // DEBUG\n    LOG.info(\"launchContainer: \" + Arrays.toString(commandArray));\n    String output = shExec.getOutput();\n    try {\n      shExec.execute();\n      if (LOG.isDebugEnabled()) {\n        logOutput(output);\n      }\n    } catch (ExitCodeException e) {\n      int exitCode = shExec.getExitCode();\n      LOG.warn(\"Exit code from container is : \" + exitCode);\n      // 143 (SIGTERM) and 137 (SIGKILL) exit codes means the container was\n      // terminated/killed forcefully. In all other cases, log the\n      // container-executor's output\n      if (exitCode != 143 && exitCode != 137) {\n        LOG.warn(\"Exception from container-launch : \", e);\n        logOutput(output);\n        String diagnostics = \"Exception from container-launch: \\n\"\n            + StringUtils.stringifyException(e) + \"\\n\" + output;\n        container.handle(new ContainerDiagnosticsUpdateEvent(containerId,\n            diagnostics));\n      } else {\n        container.handle(new ContainerDiagnosticsUpdateEvent(containerId,\n            \"Container killed on request. Exit code is \" + exitCode));\n      }\n      return exitCode;\n    } finally {\n      launchCommandObjs.remove(containerId);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Output from LinuxContainerExecutor's launchContainer follows:\");\n      logOutput(output);\n    }\n    return 0;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call": "  public Integer call() {\n    final ContainerLaunchContext launchContext = container.getLaunchContext();\n    final Map<Path,String> localResources = container.getLocalizedResources();\n    String containerIdStr = ConverterUtils.toString(container.getContainerID());\n    final String user = launchContext.getUser();\n    final List<String> command = launchContext.getCommands();\n    int ret = -1;\n\n    try {\n      // /////////////////////////// Variable expansion\n      // Before the container script gets written out.\n      List<String> newCmds = new ArrayList<String>(command.size());\n      String appIdStr = app.toString();\n      Path containerLogDir =\n          this.logDirsSelector.getLocalPathForWrite(appIdStr + Path.SEPARATOR\n              + containerIdStr, LocalDirAllocator.SIZE_UNKNOWN, this.conf, \n              false);\n      for (String str : command) {\n        // TODO: Should we instead work via symlinks without this grammar?\n        newCmds.add(str.replace(ApplicationConstants.LOG_DIR_EXPANSION_VAR,\n            containerLogDir.toUri().getPath()));\n      }\n      launchContext.setCommands(newCmds);\n\n      Map<String, String> environment = launchContext.getEnvironment();\n      // Make a copy of env to iterate & do variable expansion\n      for (Entry<String, String> entry : environment.entrySet()) {\n        String value = entry.getValue();\n        entry.setValue(\n            value.replace(\n                ApplicationConstants.LOG_DIR_EXPANSION_VAR,\n                containerLogDir.toUri().getPath())\n            );\n      }\n      // /////////////////////////// End of variable expansion\n\n      FileContext lfs = FileContext.getLocalFSFileContext();\n      LocalDirAllocator lDirAllocator =\n          new LocalDirAllocator(YarnConfiguration.NM_LOCAL_DIRS); // TODO\n      Path nmPrivateContainerScriptPath =\n          lDirAllocator.getLocalPathForWrite(\n              ResourceLocalizationService.NM_PRIVATE_DIR + Path.SEPARATOR\n                  + appIdStr + Path.SEPARATOR + containerIdStr\n                  + Path.SEPARATOR + CONTAINER_SCRIPT, this.conf);\n      Path nmPrivateTokensPath =\n          lDirAllocator.getLocalPathForWrite(\n              ResourceLocalizationService.NM_PRIVATE_DIR\n                  + Path.SEPARATOR\n                  + containerIdStr\n                  + Path.SEPARATOR\n                  + String.format(ContainerLocalizer.TOKEN_FILE_NAME_FMT,\n                      containerIdStr), this.conf);\n      DataOutputStream containerScriptOutStream = null;\n      DataOutputStream tokensOutStream = null;\n\n      // Select the working directory for the container\n      Path containerWorkDir =\n          lDirAllocator.getLocalPathForWrite(ContainerLocalizer.USERCACHE\n              + Path.SEPARATOR + user + Path.SEPARATOR\n              + ContainerLocalizer.APPCACHE + Path.SEPARATOR + appIdStr\n              + Path.SEPARATOR + containerIdStr,\n              LocalDirAllocator.SIZE_UNKNOWN, this.conf, false);\n      try {\n        // /////////// Write out the container-script in the nmPrivate space.\n        String[] localDirs =\n            this.conf.getStrings(YarnConfiguration.NM_LOCAL_DIRS,\n                YarnConfiguration.DEFAULT_NM_LOCAL_DIRS);\n        List<Path> appDirs = new ArrayList<Path>(localDirs.length);\n        for (String localDir : localDirs) {\n          Path usersdir = new Path(localDir, ContainerLocalizer.USERCACHE);\n          Path userdir = new Path(usersdir, user);\n          Path appsdir = new Path(userdir, ContainerLocalizer.APPCACHE);\n          appDirs.add(new Path(appsdir, appIdStr));\n        }\n        containerScriptOutStream =\n          lfs.create(nmPrivateContainerScriptPath,\n              EnumSet.of(CREATE, OVERWRITE));\n\n        // Set the token location too.\n        environment.put(\n            ApplicationConstants.CONTAINER_TOKEN_FILE_ENV_NAME, \n            new Path(containerWorkDir, \n                FINAL_CONTAINER_TOKENS_FILE).toUri().getPath());\n\n        // Sanitize the container's environment\n        sanitizeEnv(environment, containerWorkDir, appDirs);\n        \n        // Write out the environment\n        writeLaunchEnv(containerScriptOutStream, environment, localResources,\n            launchContext.getCommands());\n        \n        // /////////// End of writing out container-script\n\n        // /////////// Write out the container-tokens in the nmPrivate space.\n        tokensOutStream =\n            lfs.create(nmPrivateTokensPath, EnumSet.of(CREATE, OVERWRITE));\n        Credentials creds = container.getCredentials();\n        creds.writeTokenStorageToStream(tokensOutStream);\n        // /////////// End of writing out container-tokens\n      } finally {\n        IOUtils.cleanup(LOG, containerScriptOutStream, tokensOutStream);\n      }\n\n      // LaunchContainer is a blocking call. We are here almost means the\n      // container is launched, so send out the event.\n      dispatcher.getEventHandler().handle(new ContainerEvent(\n            container.getContainerID(),\n            ContainerEventType.CONTAINER_LAUNCHED));\n\n      ret =\n          exec.launchContainer(container, nmPrivateContainerScriptPath,\n              nmPrivateTokensPath, user, appIdStr, containerWorkDir);\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to launch container\", e);\n      dispatcher.getEventHandler().handle(new ContainerExitEvent(\n            launchContext.getContainerId(),\n            ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, ret));\n      return ret;\n    }\n\n    if (ret == ExitCode.KILLED.getExitCode()) {\n      // If the process was killed, Send container_cleanedup_after_kill and\n      // just break out of this method.\n      dispatcher.getEventHandler().handle(\n            new ContainerExitEvent(launchContext.getContainerId(),\n                ContainerEventType.CONTAINER_KILLED_ON_REQUEST, ret));\n      return ret;\n    }\n\n    if (ret != 0) {\n      LOG.warn(\"Container exited with a non-zero exit code \" + ret);\n      this.dispatcher.getEventHandler().handle(new ContainerExitEvent(\n              launchContext.getContainerId(),\n              ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, ret));\n      return ret;\n    }\n\n    LOG.info(\"Container \" + containerIdStr + \" succeeded \");\n    dispatcher.getEventHandler().handle(\n        new ContainerEvent(launchContext.getContainerId(),\n            ContainerEventType.CONTAINER_EXITED_WITH_SUCCESS));\n    return 0;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.toString": "    public String toString() {\n      return sb.toString();\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.writeLaunchEnv": "  private static void writeLaunchEnv(OutputStream out,\n      Map<String,String> environment, Map<Path,String> resources,\n      List<String> command)\n      throws IOException {\n    ShellScriptBuilder sb = new ShellScriptBuilder();\n    if (environment != null) {\n      for (Map.Entry<String,String> env : environment.entrySet()) {\n        sb.env(env.getKey().toString(), env.getValue().toString());\n      }\n    }\n    if (resources != null) {\n      for (Map.Entry<Path,String> link : resources.entrySet()) {\n        sb.symlink(link.getKey(), link.getValue());\n      }\n    }\n    ArrayList<String> cmd = new ArrayList<String>(2 * command.size() + 5);\n    cmd.add(ContainerExecutor.isSetsidAvailable ? \"exec setsid \" : \"exec \");\n    cmd.add(\"/bin/bash \");\n    cmd.add(\"-c \");\n    cmd.add(\"\\\"\");\n    for (String cs : command) {\n      cmd.add(cs.toString());\n      cmd.add(\" \");\n    }\n    cmd.add(\"\\\"\");\n    sb.line(cmd.toArray(new String[cmd.size()]));\n    PrintStream pout = null;\n    try {\n      pout = new PrintStream(out);\n      sb.write(pout);\n    } finally {\n      if (out != null) {\n        out.close();\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.sanitizeEnv": "  public void sanitizeEnv(Map<String, String> environment, \n      Path pwd, List<Path> appDirs) {\n    /**\n     * Non-modifiable environment variables\n     */\n    \n    putEnvIfNotNull(environment, Environment.USER.name(), container.getUser());\n    \n    putEnvIfNotNull(environment, \n        Environment.LOGNAME.name(),container.getUser());\n    \n    putEnvIfNotNull(environment, \n        Environment.HOME.name(),\n        conf.get(\n            YarnConfiguration.NM_USER_HOME_DIR, \n            YarnConfiguration.DEFAULT_NM_USER_HOME_DIR\n            )\n        );\n    \n    putEnvIfNotNull(environment, Environment.PWD.name(), pwd.toString());\n    \n    putEnvIfNotNull(environment, \n        Environment.HADOOP_CONF_DIR.name(), \n        System.getenv(Environment.HADOOP_CONF_DIR.name())\n        );\n    \n    putEnvIfNotNull(environment, \n        ApplicationConstants.LOCAL_DIR_ENV, \n        StringUtils.join(\",\", appDirs)\n        );\n\n    if (!Shell.WINDOWS) {\n      environment.put(\"JVM_PID\", \"$$\");\n    }\n\n    /**\n     * Modifiable environment variables\n     */\n    \n    putEnvIfAbsent(environment, Environment.JAVA_HOME.name());\n    putEnvIfAbsent(environment, Environment.HADOOP_COMMON_HOME.name());\n    putEnvIfAbsent(environment, Environment.HADOOP_HDFS_HOME.name());\n    putEnvIfAbsent(environment, Environment.YARN_HOME.name());\n\n  }"
        },
        "bug_report": {
            "Title": "Symbolic links with special chars causing container/task.sh to fail",
            "Description": "the following job throws an exception when you have the special characters in it.\n\nhadoop jar hadoop-streaming.jar -Dmapreduce.job.acl-view-job=* -Dmapreduce.job.queuename=queue1 -files file:///homes/user/hadoop/Streaming/data/streaming-980//InputDir#testlink!@$&*()-_+= -input Streaming/streaming-980/input.txt  -mapper 'xargs cat' -reducer cat -output Streaming/streaming-980/Output -jobconf mapred.job.name=streamingTest-980 -jobconf mapreduce.job.acl-view-job=*\n\nException:\n2011-09-27 20:58:48,903 INFO org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: launchContainer:\n[container-executor, hadoopuser, 1, application_1317077272567_0239,\ncontainer_1317077272567 0239_01_000001,\ntmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001,\ntmp/mapred-local/nmPrivate/application_1317077272567_0239/container_1317077272567_0239_01 000001/task.sh,\ntmp/mapred-local/nmPrivate/container_1317077272567_0239_01_000001/container_1317077272567_0239_01_000001.tokens]1109221111-tests.jar:hadoop-mapreduce-p2011-09-27\n20:58:48,944 WARN org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: Exit code from container is : 2    \n                                                                                                           2011-09-27\n20:58:48,946 WARN org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: Exception from container-launch :  \n                                                                                                          \norg.apache.hadoop.util.Shell$ExitCodeException:\n/tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001/task.sh:\nline 26: syntax error near unexpected token `-_+='       \n/tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001/task.sh:\nline 26: `ln -sf /tmp/mapred-local/usercache/hadoopqa/filecache/-1888139433818483070/InputDir test\nink!@$&*()-_+='kson-jaxrs-1.7.1.jar:/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.7.3/jackson-mapper-asl-1.7.3.jar:.m2/repository/org/codehaus/jackson/jackson-xc/1.7.1/jackson-xc-1.7.1.jar:\n\n     at org.apache.hadoop.util.Shell.runCommand(Shell.java:261)                                                        \n                                                                                                                       \n   at org.apache.hadoop.util.Shell.run(Shell.java:188)                                                                 \n                                                                                                                       \n at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:381)                                          \n                                                                                                                      \nat org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:174)   \n                                                                                                                     \nat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:197)  \n                                                                                                                     \nat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:62)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\n2011-09-27 20:58:48,951 INFO org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor:\n2011-09-27 20:58:48,951 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Processing\ncontainer_1317077272567_0239_01_000001 of type UPDATE_DIAGNOSTICS_MSG\n2011-09-27 20:58:48,951 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:\nContainer exited with a non-zero exit code 2\n\n\n"
        }
    },
    {
        "filename": "MAPREDUCE-6439.json",
        "creation_time": "2015-07-30T20:31:00.000+0000",
        "stack_trace": "```\njava.lang.InterruptedException\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:245)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:469)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)\n\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)\nCaused by: java.lang.InterruptedException\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)\n\tat java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)\n\tat java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:240)\n\t... 11 more\n\norg.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.InterruptedException\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:245)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:469)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)\n\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)\nCaused by: java.lang.InterruptedException\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)\n\tat java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)\n\tat java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:240)\n\t... 11 more\n\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.YarnRuntimeException): java.lang.InterruptedException\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:245)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:469)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)\n\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)\nCaused by: java.lang.InterruptedException\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)\n\tat java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)\n\tat java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:240)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcRequestWrapper request = (RpcRequestWrapper) writableRequest;\n        RequestHeaderProto rpcRequest = request.requestHeader;\n        String methodName = rpcRequest.getMethodName();\n        \n        \n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = prototype.newBuilderForType()\n            .mergeFrom(request.theRequestRead).build();\n        \n        Message result;\n        long startTime = Time.now();\n        int qTime = (int) (startTime - receiveTime);\n        Exception exception = null;\n        try {\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n        } catch (ServiceException e) {\n          exception = (Exception) e.getCause();\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          exception = e;\n          throw e;\n        } finally {\n          int processingTime = (int) (Time.now() - startTime);\n          if (LOG.isDebugEnabled()) {\n            String msg = \"Served: \" + methodName + \" queueTime= \" + qTime +\n                \" procesingTime= \" + processingTime;\n            if (exception != null) {\n              msg += \" exception= \" + exception.getClass().getSimpleName();\n            }\n            LOG.debug(msg);\n          }\n          String detailedMetricsName = (exception == null) ?\n              methodName :\n              exception.getClass().getSimpleName();\n          server.rpcMetrics.addRpcQueueTime(qTime);\n          server.rpcMetrics.addRpcProcessingTime(processingTime);\n          server.rpcDetailedMetrics.addProcessingTime(detailedMetricsName,\n              processingTime);\n        }\n        return new RpcResponseWrapper(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getProtocolImpl": "      private static ProtoClassProtoImpl getProtocolImpl(RPC.Server server,\n          String protoName, long clientVersion) throws RpcServerException {\n        ProtoNameVer pv = new ProtoNameVer(protoName, clientVersion);\n        ProtoClassProtoImpl impl = \n            server.getProtocolImplMap(RPC.RpcKind.RPC_PROTOCOL_BUFFER).get(pv);\n        if (impl == null) { // no match for Protocol AND Version\n          VerProtocolImpl highest = \n              server.getHighestSupportedProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER, \n                  protoName);\n          if (highest == null) {\n            throw new RpcNoSuchProtocolException(\n                \"Unknown protocol: \" + protoName);\n          }\n          // protocol supported but not the version that client wants\n          throw new RPC.VersionMismatch(protoName, clientVersion,\n              highest.version);\n        }\n        return impl;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.call": "    public Writable call(RPC.RpcKind rpcKind, String protocol,\n        Writable rpcRequest, long receiveTime) throws Exception {\n      return getRpcInvoker(rpcKind).call(this, protocol, rpcRequest,\n          receiveTime);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRespond": "    void doRespond(Call call) throws IOException {\n      synchronized (call.connection.responseQueue) {\n        call.connection.responseQueue.addLast(call);\n        if (call.connection.responseQueue.size() == 1) {\n          processResponse(call.connection.responseQueue, true);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.toString": "    public String toString() {\n      return getHostAddress() + \":\" + remotePort; \n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getRpcErrorCodeProto": "    public RpcErrorCodeProto getRpcErrorCodeProto() {\n      return errCode;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    boolean close(Connection connection) {\n      boolean exists = remove(connection);\n      if (exists) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(Thread.currentThread().getName() +\n              \": disconnecting client \" + connection +\n              \". Number of active connections: \"+ size());\n        }\n        // only close if actually removed to avoid double-closing due\n        // to possible races\n        connection.close();\n      }\n      return exists;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.isTerse": "    boolean isTerse(Class<?> t) {\n      return terseExceptions.contains(t.toString());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.size": "    int size() {\n      return count.get();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws InterruptedException, IOException,  OutOfMemoryError {\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        channel.socket().setKeepAlive(true);\n        \n        Reader reader = getReader();\n        Connection c = connectionManager.register(channel);\n        // If the connectionManager can't take it, close the connection.\n        if (c == null) {\n          if (channel.isOpen()) {\n            IOUtils.cleanup(null, channel);\n          }\n          continue;\n        }\n        key.attach(c);  // so closeCurrentConnection can get the object\n        reader.addConnection(c);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(ByteArrayOutputStream responseBuf,\n                             Call call, RpcStatusProto status, RpcErrorCodeProto erCode,\n                             Writable rv, String errorClass, String error) \n  throws IOException {\n    responseBuf.reset();\n    DataOutputStream out = new DataOutputStream(responseBuf);\n    RpcResponseHeaderProto.Builder headerBuilder =  \n        RpcResponseHeaderProto.newBuilder();\n    headerBuilder.setClientId(ByteString.copyFrom(call.clientId));\n    headerBuilder.setCallId(call.callId);\n    headerBuilder.setRetryCount(call.retryCount);\n    headerBuilder.setStatus(status);\n    headerBuilder.setServerIpcVersionNum(CURRENT_VERSION);\n\n    if (status == RpcStatusProto.SUCCESS) {\n      RpcResponseHeaderProto header = headerBuilder.build();\n      final int headerLen = header.getSerializedSize();\n      int fullLength  = CodedOutputStream.computeRawVarint32Size(headerLen) +\n          headerLen;\n      try {\n        if (rv instanceof ProtobufRpcEngine.RpcWrapper) {\n          ProtobufRpcEngine.RpcWrapper resWrapper = \n              (ProtobufRpcEngine.RpcWrapper) rv;\n          fullLength += resWrapper.getLength();\n          out.writeInt(fullLength);\n          header.writeDelimitedTo(out);\n          rv.write(out);\n        } else { // Have to serialize to buffer to get len\n          final DataOutputBuffer buf = new DataOutputBuffer();\n          rv.write(buf);\n          byte[] data = buf.getData();\n          fullLength += buf.getLength();\n          out.writeInt(fullLength);\n          header.writeDelimitedTo(out);\n          out.write(data, 0, buf.getLength());\n        }\n      } catch (Throwable t) {\n        LOG.warn(\"Error serializing call response for call \" + call, t);\n        // Call back to same function - this is OK since the\n        // buffer is reset at the top, and since status is changed\n        // to ERROR it won't infinite loop.\n        setupResponse(responseBuf, call, RpcStatusProto.ERROR,\n            RpcErrorCodeProto.ERROR_SERIALIZING_RESPONSE,\n            null, t.getClass().getName(),\n            StringUtils.stringifyException(t));\n        return;\n      }\n    } else { // Rpc Failure\n      headerBuilder.setExceptionClassName(errorClass);\n      headerBuilder.setErrorMsg(error);\n      headerBuilder.setErrorDetail(erCode);\n      RpcResponseHeaderProto header = headerBuilder.build();\n      int headerLen = header.getSerializedSize();\n      final int fullLength  = \n          CodedOutputStream.computeRawVarint32Size(headerLen) + headerLen;\n      out.writeInt(fullLength);\n      header.writeDelimitedTo(out);\n    }\n    if (call.connection.useWrap) {\n      wrapWithSasl(responseBuf, call);\n    }\n    call.setResponse(ByteBuffer.wrap(responseBuf.toByteArray()));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.stopIdleScan": "    void stopIdleScan() {\n      idleScanTimer.cancel();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeIdle": "    synchronized void closeIdle(boolean scanAll) {\n      long minLastContact = Time.now() - maxIdleTime;\n      // concurrent iterator might miss new connections added\n      // during the iteration, but that's ok because they won't\n      // be idle yet anyway and will be caught on next scan\n      int closed = 0;\n      for (Connection connection : connections) {\n        // stop if connections dropped below threshold unless scanning all\n        if (!scanAll && size() < idleScanThreshold) {\n          break;\n        }\n        // stop if not scanning all and max connections are closed\n        if (connection.isIdle() &&\n            connection.getLastContact() < minLastContact &&\n            close(connection) &&\n            !scanAll && (++closed == maxIdleToClose)) {\n          break;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(RPC.RpcKind rpcKind, String protocol,\n      Writable param, long receiveTime) throws Exception;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param protocolName - the protocol\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  private void authorize(UserGroupInformation user, String protocolName,\n      InetAddress addr) throws AuthorizationException {\n    if (authorize) {\n      if (protocolName == null) {\n        throw new AuthorizationException(\"Null protocol not authorized\");\n      }\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(protocolName, getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         protocolName);\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeAll": "    void closeAll() {\n      // use a copy of the connections to be absolutely sure the concurrent\n      // iterator doesn't miss a connection\n      for (Connection connection : toArray()) {\n        close(connection);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.remove": "    private boolean remove(Connection connection) {\n      boolean removed = connections.remove(connection);\n      if (removed) {\n        count.getAndDecrement();\n      }\n      return removed;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.startIdleScan": "    void startIdleScan() {\n      scheduleIdleScanTask();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isValid() && key.isWritable()) {\n                  doAsyncWrite(key);\n              }\n            } catch (IOException e) {\n              LOG.info(Thread.currentThread().getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = Time.now();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<Call> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<Call>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              Call call = (Call)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n          \n          for(Call call : calls) {\n            doPurge(call, now);\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.scheduleIdleScanTask": "    private void scheduleIdleScanTask() {\n      if (!running) {\n        return;\n      }\n      TimerTask idleScanTask = new TimerTask(){\n        @Override\n        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }\n      };\n      idleScanTimer.schedule(idleScanTask, idleScanInterval);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcServerException.getRpcStatusProto": "  public RpcStatusProto getRpcStatusProto() {\n    return RpcStatusProto.ERROR;\n  }"
        },
        "bug_report": {
            "Title": "AM may fail instead of retrying if RM shuts down during the allocate call",
            "Description": "We are seeing cases where MR AM gets a YarnRuntimeException thats thrown in RM and gets sent back to AM causing it to think that it has exhausted the number of retries. Copying the error which causes the heartbeat thread to quit.\n\n{noformat}\n2015-07-25 20:07:27,346 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Error communicating with RM: java.lang.InterruptedException\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:245)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:469)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)\n\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)\nCaused by: java.lang.InterruptedException\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)\n\tat java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)\n\tat java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:240)\n\t... 11 more\n\norg.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.InterruptedException\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:245)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:469)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)\n\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)\nCaused by: java.lang.InterruptedException\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)\n\tat java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)\n\tat java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:240)\n\t... 11 more\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)\n\tat org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:107)\n\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:79)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy36.allocate(Unknown Source)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.makeRemoteRequest(RMContainerRequestor.java:188)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getResources(RMContainerAllocator.java:667)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:244)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:282)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.YarnRuntimeException): java.lang.InterruptedException\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:245)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:469)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)\n\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)\nCaused by: java.lang.InterruptedException\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)\n\tat java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)\n\tat java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:240)\n\t... 11 more\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1411)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1364)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n\tat com.sun.proxy.$Proxy35.allocate(Unknown Source)\n\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)\n\t... 12 more\n{noformat} \n"
        }
    },
    {
        "filename": "MAPREDUCE-4490.json",
        "creation_time": "2012-07-27T01:22:21.000+0000",
        "stack_trace": "```\nENOENT: No such file or directory\n        at org.apache.hadoop.io.nativeio.NativeIO.open(Native Method)\n        at org.apache.hadoop.io.SecureIOUtils.createForWrite(SecureIOUtils.java:161)\n        at org.apache.hadoop.mapred.TaskLog.writeToIndexFile(TaskLog.java:296)\n        at org.apache.hadoop.mapred.TaskLog.syncLogs(TaskLog.java:369)\n        at org.apache.hadoop.mapred.Child.main(Child.java:229)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.SecureIOUtils.createForWrite": "  public static FileOutputStream createForWrite(File f, int permissions)\n  throws IOException {\n    if (skipSecurity) {\n      return insecureCreateForWrite(f, permissions);\n    } else {\n      // Use the native wrapper around open(2)\n      try {\n        FileDescriptor fd = NativeIO.open(f.getAbsolutePath(),\n          NativeIO.O_WRONLY | NativeIO.O_CREAT | NativeIO.O_EXCL,\n          permissions);\n        return new FileOutputStream(fd);\n      } catch (NativeIOException nioe) {\n        if (nioe.getErrno() == Errno.EEXIST) {\n          throw new AlreadyExistsException(nioe);\n        }\n        throw nioe;\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.SecureIOUtils.insecureCreateForWrite": "  private static FileOutputStream insecureCreateForWrite(File f,\n      int permissions) throws IOException {\n    // If we can't do real security, do a racy exists check followed by an\n    // open and chmod\n    if (f.exists()) {\n      throw new AlreadyExistsException(\"File \" + f + \" already exists\");\n    }\n    FileOutputStream fos = new FileOutputStream(f);\n    boolean success = false;\n    try {\n      rawFilesystem.setPermission(new Path(f.getAbsolutePath()),\n        new FsPermission((short)permissions));\n      success = true;\n      return fos;\n    } finally {\n      if (!success) {\n        fos.close();\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.writeToIndexFile": "  void writeToIndexFile(String logLocation,\n                        boolean isCleanup) throws IOException {\n    // To ensure atomicity of updates to index file, write to temporary index\n    // file first and then rename.\n    File tmpIndexFile = getTmpIndexFile(currentTaskid, isCleanup);\n\n    BufferedOutputStream bos = \n      new BufferedOutputStream(\n        SecureIOUtils.createForWrite(tmpIndexFile, 0644));\n    DataOutputStream dos = new DataOutputStream(bos);\n    //the format of the index file is\n    //LOG_DIR: <the dir where the task logs are really stored>\n    //STDOUT: <start-offset in the stdout file> <length>\n    //STDERR: <start-offset in the stderr file> <length>\n    //SYSLOG: <start-offset in the syslog file> <length>   \n    try{\n      dos.writeBytes(LogFileDetail.LOCATION + logLocation + \"\\n\"\n          + LogName.STDOUT.toString() + \":\");\n      dos.writeBytes(Long.toString(prevOutLength) + \" \");\n      dos.writeBytes(Long.toString(new File(logLocation, LogName.STDOUT\n          .toString()).length() - prevOutLength)\n          + \"\\n\" + LogName.STDERR + \":\");\n      dos.writeBytes(Long.toString(prevErrLength) + \" \");\n      dos.writeBytes(Long.toString(new File(logLocation, LogName.STDERR\n          .toString()).length() - prevErrLength)\n          + \"\\n\" + LogName.SYSLOG.toString() + \":\");\n      dos.writeBytes(Long.toString(prevLogLength) + \" \");\n      dos.writeBytes(Long.toString(new File(logLocation, LogName.SYSLOG\n          .toString()).length() - prevLogLength)\n          + \"\\n\");\n      dos.close();\n      dos = null;\n    } finally {\n      IOUtils.cleanup(LOG, dos);\n    }\n\n    File indexFile = getIndexFile(currentTaskid, isCleanup);\n    Path indexFilePath = new Path(indexFile.getAbsolutePath());\n    Path tmpIndexFilePath = new Path(tmpIndexFile.getAbsolutePath());\n\n    if (localFS == null) {// set localFS once\n      localFS = FileSystem.getLocal(new Configuration());\n    }\n    localFS.rename (tmpIndexFilePath, indexFilePath);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.getTmpIndexFile": "  private static File getTmpIndexFile(TaskAttemptID taskid, boolean isCleanup) {\n    return new File(getAttemptDir(taskid, isCleanup), \"log.tmp\");\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.toString": "    public String toString() {\n      return prefix;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.close": "    public void close() throws IOException {\n      file.close();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.getIndexFile": "  static File getIndexFile(TaskAttemptID taskid, boolean isCleanup) {\n    return new File(getAttemptDir(taskid, isCleanup), \"log.index\");\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.syncLogs": "  public synchronized static void syncLogs(String logLocation, \n                                           TaskAttemptID taskid,\n                                           boolean isCleanup) \n  throws IOException {\n    System.out.flush();\n    System.err.flush();\n    Enumeration<Logger> allLoggers = LogManager.getCurrentLoggers();\n    while (allLoggers.hasMoreElements()) {\n      Logger l = allLoggers.nextElement();\n      Enumeration<Appender> allAppenders = l.getAllAppenders();\n      while (allAppenders.hasMoreElements()) {\n        Appender a = allAppenders.nextElement();\n        if (a instanceof TaskLogAppender) {\n          ((TaskLogAppender)a).flush();\n        }\n      }\n    }\n    if (currentTaskid != taskid) {\n      currentTaskid = taskid;\n      resetPrevLengths(logLocation);\n    }\n    writeToIndexFile(logLocation, isCleanup);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.resetPrevLengths": "  private static void resetPrevLengths(String logLocation) {\n    prevOutLength = new File(logLocation, LogName.STDOUT.toString()).length();\n    prevErrLength = new File(logLocation, LogName.STDERR.toString()).length();\n    prevLogLength = new File(logLocation, LogName.SYSLOG.toString()).length();\n  }"
        },
        "bug_report": {
            "Title": "JVM reuse is incompatible with LinuxTaskController (and therefore incompatible with Security)",
            "Description": "When using LinuxTaskController, JVM reuse (mapred.job.reuse.jvm.num.tasks > 1) with more map tasks in a job than there are map slots in the cluster will result in immediate task failures for the second task in each JVM (and then the JVM exits). We have investigated this bug and the root cause is as follows. When using LinuxTaskController, the userlog directory for a task attempt (../userlogs/job/task-attempt) is created only on the first invocation (when the JVM is launched) because userlogs directories are created by the task-controller binary which only runs *once* per JVM. Therefore, attempting to create log.index is guaranteed to fail with ENOENT leading to immediate task failure and child JVM exit.\n\n{quote}\n2012-07-24 14:29:11,914 INFO org.apache.hadoop.mapred.TaskLog: Starting logging for a new task attempt_201207241401_0013_m_000027_0 in the same JVM as that of the first task /var/log/hadoop/mapred/userlogs/job_201207241401_0013/attempt_201207241401_0013_m_000006_0\n2012-07-24 14:29:11,915 WARN org.apache.hadoop.mapred.Child: Error running child\nENOENT: No such file or directory\n        at org.apache.hadoop.io.nativeio.NativeIO.open(Native Method)\n        at org.apache.hadoop.io.SecureIOUtils.createForWrite(SecureIOUtils.java:161)\n        at org.apache.hadoop.mapred.TaskLog.writeToIndexFile(TaskLog.java:296)\n        at org.apache.hadoop.mapred.TaskLog.syncLogs(TaskLog.java:369)\n        at org.apache.hadoop.mapred.Child.main(Child.java:229)\n{quote}\n\nThe above error occurs in a JVM which runs tasks 6 and 27.  Task6 goes smoothly. Then Task27 starts. The directory /var/log/hadoop/mapred/userlogs/job_201207241401_0013/attempt_201207241401_0013_m_0000027_0 is never created so when mapred.Child tries to write the log.index file for Task27, it fails with ENOENT because the attempt_201207241401_0013_m_0000027_0 directory does not exist. Therefore, the second task in each JVM is guaranteed to fail (and then the JVM exits) every time when using LinuxTaskController. Note that this problem does not occur when using the DefaultTaskController because the userlogs directories are created for each task (not just for each JVM as with LinuxTaskController).\n\nFor each task, the TaskRunner calls the TaskController's createLogDir method before attempting to write out an index file.\n\n* DefaultTaskController#createLogDir: creates log directory for each task\n* LinuxTaskController#createLogDir: does nothing\n** task-controller binary creates log directory [create_attempt_directories] (but only for the first task)\n\nPossible Solution: add a new command to task-controller *initialize task* to create attempt directories.  Call that command, with ShellCommandExecutor, in the LinuxTaskController#createLogDir method\n\n\n\n\n"
        }
    },
    {
        "filename": "MAPREDUCE-3744.json",
        "creation_time": "2012-01-27T20:31:19.000+0000",
        "stack_trace": "```\nException in thread \"main\" java.io.FileNotFoundException: File /tmp/logs/application_1327694122989_0001 does not exist.\n\tat org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:226)\n\tat org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:217)\n\tat org.apache.hadoop.fs.Hdfs$2.<init>(Hdfs.java:192)\n\tat org.apache.hadoop.fs.Hdfs.listStatusIterator(Hdfs.java:192)\n\tat org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1371)\n\tat org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1)\n\tat org.apache.hadoop.fs.FileContext$FSLinkResolver.resolve(FileContext.java:2319)\n\tat org.apache.hadoop.fs.FileContext.listStatus(FileContext.java:1373)\n\tat org.apache.hadoop.yarn.logaggregation.LogDumper.dumpAllContainersLogs(LogDumper.java:191)\n\tat org.apache.hadoop.yarn.logaggregation.LogDumper.run(LogDumper.java:107)\n\tat org.apache.hadoop.yarn.logaggregation.LogDumper.main(LogDumper.java:226)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.LogDumper.dumpAllContainersLogs": "  private void dumpAllContainersLogs(ApplicationId appId, String appOwner,\n      DataOutputStream out) throws IOException {\n    Path remoteRootLogDir =\n        new Path(getConf().get(YarnConfiguration.NM_REMOTE_APP_LOG_DIR,\n            YarnConfiguration.DEFAULT_NM_REMOTE_APP_LOG_DIR));\n    String user = appOwner;\n    String logDirSuffix =\n        getConf().get(YarnConfiguration.NM_REMOTE_APP_LOG_DIR,\n            YarnConfiguration.DEFAULT_NM_REMOTE_APP_LOG_DIR_SUFFIX);\n    //TODO Change this to get a list of files from the LAS.\n    Path remoteAppLogDir =\n        LogAggregationUtils.getRemoteAppLogDir(remoteRootLogDir, appId, user,\n            logDirSuffix);\n    RemoteIterator<FileStatus> nodeFiles =\n        FileContext.getFileContext().listStatus(remoteAppLogDir);\n    while (nodeFiles.hasNext()) {\n      FileStatus thisNodeFile = nodeFiles.next();\n      AggregatedLogFormat.LogReader reader =\n          new AggregatedLogFormat.LogReader(getConf(),\n              new Path(remoteAppLogDir, thisNodeFile.getPath().getName()));\n      try {\n\n        DataInputStream valueStream;\n        LogKey key = new LogKey();\n        valueStream = reader.next(key);\n\n        while (valueStream != null) {\n          while (true) {\n            try {\n              LogReader.readAContainerLogsForALogType(valueStream, out);\n            } catch (EOFException eof) {\n              break;\n            }\n          }\n\n          // Next container\n          key = new LogKey();\n          valueStream = reader.next(key);\n        }\n      } finally {\n        reader.close();\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.LogDumper.run": "  public int run(String[] args) throws Exception {\n\n    Options opts = new Options();\n    opts.addOption(APPLICATION_ID_OPTION, true, \"ApplicationId\");\n    opts.addOption(CONTAINER_ID_OPTION, true, \"ContainerId\");\n    opts.addOption(NODE_ADDRESS_OPTION, true, \"NodeAddress\");\n    opts.addOption(APP_OWNER_OPTION, true, \"AppOwner\");\n\n    if (args.length < 1) {\n      HelpFormatter formatter = new HelpFormatter();\n      formatter.printHelp(\"general options are: \", opts);\n      return -1;\n    }\n\n    CommandLineParser parser = new GnuParser();\n    String appIdStr = null;\n    String containerIdStr = null;\n    String nodeAddress = null;\n    String appOwner = null;\n    try {\n      CommandLine commandLine = parser.parse(opts, args, true);\n      appIdStr = commandLine.getOptionValue(APPLICATION_ID_OPTION);\n      containerIdStr = commandLine.getOptionValue(CONTAINER_ID_OPTION);\n      nodeAddress = commandLine.getOptionValue(NODE_ADDRESS_OPTION);\n      appOwner = commandLine.getOptionValue(APP_OWNER_OPTION);\n    } catch (ParseException e) {\n      System.out.println(\"options parsing failed: \" + e.getMessage());\n\n      HelpFormatter formatter = new HelpFormatter();\n      formatter.printHelp(\"general options are: \", opts);\n      return -1;\n    }\n\n    if (appIdStr == null) {\n      System.out.println(\"ApplicationId cannot be null!\");\n      HelpFormatter formatter = new HelpFormatter();\n      formatter.printHelp(\"general options are: \", opts);\n      return -1;\n    }\n\n    RecordFactory recordFactory =\n        RecordFactoryProvider.getRecordFactory(getConf());\n    ApplicationId appId =\n        ConverterUtils.toApplicationId(recordFactory, appIdStr);\n\n    DataOutputStream out = new DataOutputStream(System.out);\n\n    if (appOwner == null || appOwner.isEmpty()) {\n      appOwner = UserGroupInformation.getCurrentUser().getShortUserName();\n    }\n    if (containerIdStr == null && nodeAddress == null) {\n      dumpAllContainersLogs(appId, appOwner, out);\n    } else if ((containerIdStr == null && nodeAddress != null)\n        || (containerIdStr != null && nodeAddress == null)) {\n      System.out.println(\"ContainerId or NodeAddress cannot be null!\");\n      HelpFormatter formatter = new HelpFormatter();\n      formatter.printHelp(\"general options are: \", opts);\n      return -1;\n    } else {\n      Path remoteRootLogDir =\n        new Path(getConf().get(YarnConfiguration.NM_REMOTE_APP_LOG_DIR,\n            YarnConfiguration.DEFAULT_NM_REMOTE_APP_LOG_DIR));\n      AggregatedLogFormat.LogReader reader =\n          new AggregatedLogFormat.LogReader(getConf(),\n              LogAggregationUtils.getRemoteNodeLogFileForApp(\n                  remoteRootLogDir,\n                  appId,\n                  appOwner,\n                  ConverterUtils.toNodeId(nodeAddress),\n                  getConf().get(YarnConfiguration.NM_REMOTE_APP_LOG_DIR_SUFFIX,\n                      YarnConfiguration.DEFAULT_NM_REMOTE_APP_LOG_DIR_SUFFIX)));\n      return dumpAContainerLogs(containerIdStr, reader, out);\n    }\n\n    return 0;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.LogDumper.dumpAContainerLogs": "  private int dumpAContainerLogs(String containerIdStr,\n      AggregatedLogFormat.LogReader reader, DataOutputStream out)\n      throws IOException {\n    DataInputStream valueStream;\n    LogKey key = new LogKey();\n    valueStream = reader.next(key);\n\n    while (valueStream != null && !key.toString().equals(containerIdStr)) {\n      // Next container\n      key = new LogKey();\n      valueStream = reader.next(key);\n    }\n\n    if (valueStream == null) {\n      System.out.println(\"Logs for container \" + containerIdStr\n          + \" are not present in this log-file.\");\n      return -1;\n    }\n\n    while (true) {\n      try {\n        LogReader.readAContainerLogsForALogType(valueStream, out);\n      } catch (EOFException eof) {\n        break;\n      }\n    }\n    return 0;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.LogDumper.main": "  public static void main(String[] args) throws Exception {\n    Configuration conf = new YarnConfiguration();\n    LogDumper logDumper = new LogDumper();\n    logDumper.setConf(conf);\n    logDumper.run(args);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat.close": "    public void close() throws IOException {\n      this.scanner.close();\n      this.fsDataIStream.close();\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat.next": "    public DataInputStream next(LogKey key) throws IOException {\n      if (!this.atBeginning) {\n        this.scanner.advance();\n      } else {\n        this.atBeginning = false;\n      }\n      if (this.scanner.atEnd()) {\n        return null;\n      }\n      TFile.Reader.Scanner.Entry entry = this.scanner.entry();\n      key.readFields(entry.getKeyStream());\n      // Skip META keys\n      if (RESERVED_KEYS.containsKey(key.toString())) {\n        return next(key);\n      }\n      DataInputStream valueStream = entry.getValueStream();\n      return valueStream;\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat.readFields": "    public void readFields(DataInput in) throws IOException {\n      this.keyString = in.readUTF();\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat.toString": "    public String toString() {\n      return this.keyString;\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.LogAggregationUtils.getRemoteAppLogDir": "  public static Path getRemoteAppLogDir(Path remoteRootLogDir,\n      ApplicationId appId, String user, String suffix) {\n    return new Path(getRemoteLogSuffixedDir(remoteRootLogDir, user, suffix),\n        appId.toString());\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.LogAggregationUtils.getRemoteLogSuffixedDir": "  public static Path getRemoteLogSuffixedDir(Path remoteRootLogDir,\n      String user, String suffix) {\n    if (suffix == null || suffix.isEmpty()) {\n      return getRemoteLogUserDir(remoteRootLogDir, user);\n    }\n    // TODO Maybe support suffix to be more than a single file.\n    return new Path(getRemoteLogUserDir(remoteRootLogDir, user), suffix);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.LogAggregationUtils.getRemoteNodeLogFileForApp": "  public static Path getRemoteNodeLogFileForApp(Path remoteRootLogDir,\n      ApplicationId appId, String user, NodeId nodeId, String suffix) {\n    return new Path(getRemoteAppLogDir(remoteRootLogDir, appId, user, suffix),\n        getNodeString(nodeId));\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.LogAggregationUtils.getNodeString": "  private static String getNodeString(NodeId nodeId) {\n    return nodeId.toString().replace(\":\", \"_\");\n  }"
        },
        "bug_report": {
            "Title": "Unable to retrieve application logs via \"yarn logs\" or \"mapred job -logs\"",
            "Description": "Trying to retrieve application logs via the \"yarn logs\" shell command results in an error similar to this:\n\nException in thread \"main\" java.io.FileNotFoundException: File /tmp/logs/application_1327694122989_0001 does not exist.\n\tat org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:226)\n\tat org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:217)\n\tat org.apache.hadoop.fs.Hdfs$2.<init>(Hdfs.java:192)\n\tat org.apache.hadoop.fs.Hdfs.listStatusIterator(Hdfs.java:192)\n\tat org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1371)\n\tat org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1)\n\tat org.apache.hadoop.fs.FileContext$FSLinkResolver.resolve(FileContext.java:2319)\n\tat org.apache.hadoop.fs.FileContext.listStatus(FileContext.java:1373)\n\tat org.apache.hadoop.yarn.logaggregation.LogDumper.dumpAllContainersLogs(LogDumper.java:191)\n\tat org.apache.hadoop.yarn.logaggregation.LogDumper.run(LogDumper.java:107)\n\tat org.apache.hadoop.yarn.logaggregation.LogDumper.main(LogDumper.java:226)\n\nTrying to grab the logs via the \"mapred jobs -logs\" command results in this error:\n\n2012-01-27 14:05:52,040 INFO  mapred.ClientServiceDelegate (ClientServiceDelegate.java:getProxy(246)) - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n2012-01-27 14:05:52,041 WARN  mapred.ClientServiceDelegate (ClientServiceDelegate.java:checkAndGetHSProxy(257)) - Job History Server is not configured.\nUnable to get log information for job: job_1327694122989_0001\n\nEven though the historyserver process is running."
        }
    },
    {
        "filename": "MAPREDUCE-6815.json",
        "creation_time": "2016-12-02T00:30:37.000+0000",
        "stack_trace": "```\njava.lang.AssertionError: Job state is not correct (timedout) expected:<SUCCEEDED> but was:<ERROR>\nat org.junit.Assert.fail(Assert.java:88)\nat org.junit.Assert.failNotEquals(Assert.java:743)\nat org.junit.Assert.assertEquals(Assert.java:118)\nat org.apache.hadoop.mapreduce.v2.app.MRApp.waitForState(MRApp.java:416)\nat org.apache.hadoop.mapreduce.v2.app.TestKill.testKillTask(TestKill.java:124)\n```",
        "source_code": {},
        "bug_report": {
            "Title": "Fix flaky TestKill.testKillTask()",
            "Description": "Error Message\nJob state is not correct (timedout) expected:<SUCCEEDED> but was:<ERROR>\nStacktrace\njava.lang.AssertionError: Job state is not correct (timedout) expected:<SUCCEEDED> but was:<ERROR>\nat org.junit.Assert.fail(Assert.java:88)\nat org.junit.Assert.failNotEquals(Assert.java:743)\nat org.junit.Assert.assertEquals(Assert.java:118)\nat org.apache.hadoop.mapreduce.v2.app.MRApp.waitForState(MRApp.java:416)\nat org.apache.hadoop.mapreduce.v2.app.TestKill.testKillTask(TestKill.java:124)"
        }
    },
    {
        "filename": "MAPREDUCE-3053.json",
        "creation_time": "2011-09-20T22:32:42.000+0000",
        "stack_trace": "```\nException in thread \"main\" java.lang.reflect.UndeclaredThrowableException\n\tat org.apache.hadoop.yarn.api.impl.pb.client.AMRMProtocolPBClientImpl.registerApplicationMaster(AMRMProtocolPBClientImpl.java:108)\n\tat kafka.yarn.util.ApplicationMasterHelper.registerWithResourceManager(YarnHelper.scala:48)\n\tat kafka.yarn.ApplicationMaster$.main(ApplicationMaster.scala:32)\n\tat kafka.yarn.ApplicationMaster.main(ApplicationMaster.scala)\nCaused by: com.google.protobuf.ServiceException: java.lang.NullPointerException: java.lang.NullPointerException\n\tat org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)\n\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:130)\n\tat $Proxy6.registerApplicationMaster(Unknown Source)\n\tat org.apache.hadoop.yarn.api.impl.pb.client.AMRMProtocolPBClientImpl.registerApplicationMaster(AMRMProtocolPBClientImpl.java:101)\n\t... 3 more\nCaused by: java.lang.NullPointerException: java.lang.NullPointerException\n\tat org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1084)\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:127)\n\t... 5 more\n\njava.lang.NullPointerException\n\tat org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.api.impl.pb.client.AMRMProtocolPBClientImpl.registerApplicationMaster": "  public RegisterApplicationMasterResponse registerApplicationMaster(\n      RegisterApplicationMasterRequest request) throws YarnRemoteException {\n    RegisterApplicationMasterRequestProto requestProto = ((RegisterApplicationMasterRequestPBImpl)request).getProto();\n    try {\n      return new RegisterApplicationMasterResponsePBImpl(proxy.registerApplicationMaster(null, requestProto));\n    } catch (ServiceException e) {\n      if (e.getCause() instanceof YarnRemoteException) {\n        throw (YarnRemoteException)e.getCause();\n      } else if (e.getCause() instanceof UndeclaredThrowableException) {\n        throw (UndeclaredThrowableException)e.getCause();\n      } else {\n        throw new UndeclaredThrowableException(e);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.call": "    public Writable call(String protocol, Writable writableRequest,\n        long receiveTime) throws IOException {\n      ProtoSpecificRequestWritable request = (ProtoSpecificRequestWritable) writableRequest;\n      ProtoSpecificRpcRequest rpcRequest = request.message;\n      String methodName = rpcRequest.getMethodName();\n      System.out.println(\"Call: protocol=\" + protocol + \", method=\"\n          + methodName);\n      if (verbose)\n        log(\"Call: protocol=\" + protocol + \", method=\"\n            + methodName);\n      MethodDescriptor methodDescriptor = service.getDescriptorForType()\n          .findMethodByName(methodName);\n      Message prototype = service.getRequestPrototype(methodDescriptor);\n      Message param = prototype.newBuilderForType()\n          .mergeFrom(rpcRequest.getRequestProto()).build();\n      Message result;\n      try {\n        result = service.callBlockingMethod(methodDescriptor, null, param);\n      } catch (ServiceException e) {\n        e.printStackTrace();\n        return handleException(e);\n      } catch (Exception e) {\n        return handleException(e);\n      }\n\n      ProtoSpecificRpcResponse response = constructProtoSpecificRpcSuccessResponse(result);\n      return new ProtoSpecificResponseWritable(response);\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.handleException": "    private ProtoSpecificResponseWritable handleException(Throwable e) {\n      ProtoSpecificRpcResponse.Builder builder = ProtoSpecificRpcResponse\n          .newBuilder();\n      builder.setIsError(true);\n      if (e.getCause() instanceof YarnRemoteExceptionPBImpl) {\n        builder.setException(((YarnRemoteExceptionPBImpl) e.getCause())\n            .getProto());\n      } else {\n        builder.setException(new YarnRemoteExceptionPBImpl(e).getProto());\n      }\n      ProtoSpecificRpcResponse response = builder.build();\n      return new ProtoSpecificResponseWritable(response);\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.constructProtoSpecificRpcSuccessResponse": "    private ProtoSpecificRpcResponse constructProtoSpecificRpcSuccessResponse(\n        Message message) {\n      ProtoSpecificRpcResponse res = ProtoSpecificRpcResponse.newBuilder()\n          .setResponseProto(message.toByteString()).build();\n      return res;\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.log": "  private static void log(String value) {\n    if (value != null && value.length() > 55)\n      value = value.substring(0, 55) + \"...\";\n    LOG.info(value);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "                     public Writable run() throws Exception {\n                       // make the call\n                       return call(call.connection.protocolName, \n                                   call.param, call.timestamp);\n\n                     }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRespond": "    void doRespond(Call call) throws IOException {\n      synchronized (call.connection.responseQueue) {\n        call.connection.responseQueue.addLast(call);\n        if (call.connection.responseQueue.size() == 1) {\n          processResponse(call.connection.responseQueue, true);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeConnection": "  private void closeConnection(Connection connection) {\n    synchronized (connectionList) {\n      if (connectionList.remove(connection))\n        numConnections--;\n    }\n    try {\n      connection.close();\n    } catch (IOException e) {\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.toString": "    public String toString() {\n      return getHostAddress() + \":\" + remotePort; \n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    private synchronized void close() throws IOException {\n      disposeSasl();\n      data = null;\n      dataLengthBuffer = null;\n      if (!channel.isOpen())\n        return;\n      try {socket.shutdownOutput();} catch(Exception e) {\n        LOG.warn(\"Ignoring socket shutdown exception\");\n      }\n      if (channel.isOpen()) {\n        try {channel.close();} catch(Exception e) {}\n      }\n      try {socket.close();} catch(Exception e) {}\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          if (LOG.isDebugEnabled())\n            LOG.debug(getName() + \": disconnecting client \" + c.getHostAddress());\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws IOException,  OutOfMemoryError {\n      Connection c = null;\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        \n        Reader reader = getReader();\n        try {\n          reader.startAdd();\n          SelectionKey readKey = reader.registerChannel(channel);\n          c = new Connection(readKey, channel, System.currentTimeMillis());\n          readKey.attach(c);\n          synchronized (connectionList) {\n            connectionList.add(numConnections, c);\n            numConnections++;\n          }\n          if (LOG.isDebugEnabled())\n            LOG.debug(\"Server connection from \" + c.toString() +\n                \"; # active connections: \" + numConnections +\n                \"; # queued calls: \" + callQueue.size());          \n        } finally {\n          reader.finishAdd(); \n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.cleanupConnections": "    private void cleanupConnections(boolean force) {\n      if (force || numConnections > thresholdIdleConnections) {\n        long currentTime = System.currentTimeMillis();\n        if (!force && (currentTime - lastCleanupRunTime) < cleanupInterval) {\n          return;\n        }\n        int start = 0;\n        int end = numConnections - 1;\n        if (!force) {\n          start = rand.nextInt() % numConnections;\n          end = rand.nextInt() % numConnections;\n          int temp;\n          if (end < start) {\n            temp = start;\n            start = end;\n            end = temp;\n          }\n        }\n        int i = start;\n        int numNuked = 0;\n        while (i <= end) {\n          Connection c;\n          synchronized (connectionList) {\n            try {\n              c = connectionList.get(i);\n            } catch (Exception e) {return;}\n          }\n          if (c.timedOut(currentTime)) {\n            if (LOG.isDebugEnabled())\n              LOG.debug(getName() + \": disconnecting client \" + c.getHostAddress());\n            closeConnection(c);\n            numNuked++;\n            end--;\n            c = null;\n            if (!force && numNuked == maxConnectionsToNuke) break;\n          }\n          else i++;\n        }\n        lastCleanupRunTime = System.currentTimeMillis();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(ByteArrayOutputStream response, \n                             Call call, Status status, \n                             Writable rv, String errorClass, String error) \n  throws IOException {\n    response.reset();\n    DataOutputStream out = new DataOutputStream(response);\n    out.writeInt(call.id);                // write call id\n    out.writeInt(status.state);           // write status\n\n    if (status == Status.SUCCESS) {\n      try {\n        rv.write(out);\n      } catch (Throwable t) {\n        LOG.warn(\"Error serializing call response for call \" + call, t);\n        // Call back to same function - this is OK since the\n        // buffer is reset at the top, and since status is changed\n        // to ERROR it won't infinite loop.\n        setupResponse(response, call, Status.ERROR,\n            null, t.getClass().getName(),\n            StringUtils.stringifyException(t));\n        return;\n      }\n    } else {\n      WritableUtils.writeString(out, errorClass);\n      WritableUtils.writeString(out, error);\n    }\n    if (call.connection.useWrap) {\n      wrapWithSasl(response, call);\n    }\n    call.setResponse(ByteBuffer.wrap(response.toByteArray()));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(String protocol,\n                               Writable param, long receiveTime)\n  throws IOException;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param connection incoming connection\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  public void authorize(UserGroupInformation user, \n                        ConnectionHeader connection,\n                        InetAddress addr\n                        ) throws AuthorizationException {\n    if (authorize) {\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(connection.getProtocol(), getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         connection.getProtocol());\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isValid() && key.isWritable()) {\n                  doAsyncWrite(key);\n              }\n            } catch (IOException e) {\n              LOG.info(getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = System.currentTimeMillis();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<Call> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<Call>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              Call call = (Call)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n          \n          for(Call call : calls) {\n            try {\n              doPurge(call, now);\n            } catch (IOException e) {\n              LOG.warn(\"Error in purging old calls \" + e);\n            }\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(pae,\"Unknown exception in doAs\");\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.invoke": "    public Object invoke(Object proxy, Method method, Object[] args)\n        throws Throwable {\n      long startTime = 0;\n      if (LOG.isDebugEnabled()) {\n        startTime = System.currentTimeMillis();\n      }\n\n      ProtoSpecificRpcRequest rpcRequest = constructRpcRequest(method, args);\n      ProtoSpecificResponseWritable val = null;\n      try {\n        val = (ProtoSpecificResponseWritable) client.call(\n            new ProtoSpecificRequestWritable(rpcRequest), remoteId);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      \n      ProtoSpecificRpcResponse response = val.message;\n   \n      if (LOG.isDebugEnabled()) {\n        long callTime = System.currentTimeMillis() - startTime;\n        LOG.debug(\"Call: \" + method.getName() + \" \" + callTime);\n      }\n \n      if (response.hasIsError() && response.getIsError() == true) {\n        YarnRemoteExceptionPBImpl exception = new YarnRemoteExceptionPBImpl(response.getException());\n        exception.fillInStackTrace();\n        ServiceException se = new ServiceException(exception);\n        throw se;\n      }\n      \n      Message prototype = null;\n      try {\n        prototype = getReturnProtoType(method);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      Message actualReturnMessage = prototype.newBuilderForType()\n          .mergeFrom(response.getResponseProto()).build();\n      return actualReturnMessage;\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.constructRpcRequest": "    private ProtoSpecificRpcRequest constructRpcRequest(Method method,\n        Object[] params) throws ServiceException {\n      ProtoSpecificRpcRequest rpcRequest;\n      ProtoSpecificRpcRequest.Builder builder;\n\n      builder = ProtoSpecificRpcRequest.newBuilder();\n      builder.setMethodName(method.getName());\n\n      if (params.length != 2) { // RpcController + Message\n        throw new ServiceException(\"Too many parameters for request. Method: [\"\n            + method.getName() + \"]\" + \", Expected: 2, Actual: \"\n            + params.length);\n      }\n      if (params[1] == null) {\n        throw new ServiceException(\"null param while calling Method: [\"\n            + method.getName() + \"]\");\n      }\n\n      Message param = (Message) params[1];\n      builder.setRequestProto(param.toByteString());\n\n      rpcRequest = builder.build();\n      return rpcRequest;\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.getReturnProtoType": "    private Message getReturnProtoType(Method method) throws Exception {\n      if (returnTypes.containsKey(method.getName())) {\n        return returnTypes.get(method.getName());\n      } else {\n        Class<?> returnType = method.getReturnType();\n\n        Method newInstMethod = returnType.getMethod(\"getDefaultInstance\");\n        newInstMethod.setAccessible(true);\n        Message prototype = (Message) newInstMethod.invoke(null,\n            (Object[]) null);\n        returnTypes.put(method.getName(), prototype);\n        return prototype;\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.call": "  public Writable[] call(Writable[] params, InetSocketAddress[] addresses,\n      Class<?> protocol, UserGroupInformation ticket, Configuration conf)\n      throws IOException, InterruptedException {\n    if (addresses.length == 0) return new Writable[0];\n\n    ParallelResults results = new ParallelResults(params.length);\n    synchronized (results) {\n      for (int i = 0; i < params.length; i++) {\n        ParallelCall call = new ParallelCall(params[i], results, i);\n        try {\n          ConnectionId remoteId = ConnectionId.getConnectionId(addresses[i],\n              protocol, ticket, 0, conf);\n          Connection connection = getConnection(remoteId, call);\n          connection.sendParam(call);             // send each parameter\n        } catch (IOException e) {\n          // log errors\n          LOG.info(\"Calling \"+addresses[i]+\" caught: \" + \n                   e.getMessage(),e);\n          results.size--;                         //  wait for one fewer result\n        }\n      }\n      while (results.count != results.size) {\n        try {\n          results.wait();                    // wait for all results\n        } catch (InterruptedException e) {}\n      }\n\n      return results.values;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnectionId": "    public static ConnectionId getConnectionId(InetSocketAddress addr,\n        Class<?> protocol, UserGroupInformation ticket, int rpcTimeout,\n        Configuration conf) throws IOException {\n      String remotePrincipal = getRemotePrincipal(conf, addr, protocol);\n      boolean doPing = conf.getBoolean(\"ipc.client.ping\", true);\n      return new ConnectionId(addr, protocol, ticket,\n          rpcTimeout, remotePrincipal,\n          conf.getInt(\"ipc.client.connection.maxidletime\", 10000), // 10s\n          conf.getInt(\"ipc.client.connect.max.retries\", 10),\n          conf.getBoolean(\"ipc.client.tcpnodelay\", false),\n          doPing, \n          (doPing ? Client.getPingInterval(conf) : 0));\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.sendParam": "    public void sendParam(Call call) {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n\n      DataOutputBuffer d=null;\n      try {\n        synchronized (this.out) {\n          if (LOG.isDebugEnabled())\n            LOG.debug(getName() + \" sending #\" + call.id);\n          \n          //for serializing the\n          //data to be written\n          d = new DataOutputBuffer();\n          d.writeInt(call.id);\n          call.param.write(d);\n          byte[] data = d.getData();\n          int dataLength = d.getLength();\n          out.writeInt(dataLength);      //first put the data length\n          out.write(data, 0, dataLength);//write the data\n          out.flush();\n        }\n      } catch(IOException e) {\n        markClosed(e);\n      } finally {\n        //the buffer is just an in-memory buffer, but it is still polite to\n        // close early\n        IOUtils.closeStream(d);\n      }\n    }  ",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.wrapException": "  private IOException wrapException(InetSocketAddress addr,\n                                         IOException exception) {\n    if (exception instanceof ConnectException) {\n      //connection refused; include the host:port in the error\n      return (ConnectException)new ConnectException(\n           \"Call to \" + addr + \" failed on connection exception: \" + exception)\n                    .initCause(exception);\n    } else if (exception instanceof SocketTimeoutException) {\n      return (SocketTimeoutException)new SocketTimeoutException(\n           \"Call to \" + addr + \" failed on socket timeout exception: \"\n                      + exception).initCause(exception);\n    } else {\n      return (IOException)new IOException(\n           \"Call to \" + addr + \" failed on local exception: \" + exception)\n                                 .initCause(exception);\n\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getAddress": "    InetSocketAddress getAddress() {\n      return address;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnection": "  private Connection getConnection(ConnectionId remoteId,\n                                   Call call)\n                                   throws IOException, InterruptedException {\n    if (!running.get()) {\n      // the client is stopped\n      throw new IOException(\"The client is stopped\");\n    }\n    Connection connection;\n    /* we could avoid this allocation for each RPC by having a  \n     * connectionsId object and with set() method. We need to manage the\n     * refs for keys in HashMap properly. For now its ok.\n     */\n    do {\n      synchronized (connections) {\n        connection = connections.get(remoteId);\n        if (connection == null) {\n          connection = new Connection(remoteId);\n          connections.put(remoteId, connection);\n        }\n      }\n    } while (!connection.addCall(call));\n    \n    //we don't invoke the method below inside \"synchronized (connections)\"\n    //block above. The reason for that is if the server happens to be slow,\n    //it will take longer to establish a connection and that will slow the\n    //entire system down.\n    connection.setupIOstreams();\n    return connection;\n  }"
        },
        "bug_report": {
            "Title": "YARN Protobuf RPC Failures in RM",
            "Description": "When I try to register my ApplicationMaster with YARN's RM, it fails.\n\nIn my ApplicationMaster's logs:\n\nException in thread \"main\" java.lang.reflect.UndeclaredThrowableException\n\tat org.apache.hadoop.yarn.api.impl.pb.client.AMRMProtocolPBClientImpl.registerApplicationMaster(AMRMProtocolPBClientImpl.java:108)\n\tat kafka.yarn.util.ApplicationMasterHelper.registerWithResourceManager(YarnHelper.scala:48)\n\tat kafka.yarn.ApplicationMaster$.main(ApplicationMaster.scala:32)\n\tat kafka.yarn.ApplicationMaster.main(ApplicationMaster.scala)\nCaused by: com.google.protobuf.ServiceException: java.lang.NullPointerException: java.lang.NullPointerException\n\tat org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)\n\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:130)\n\tat $Proxy6.registerApplicationMaster(Unknown Source)\n\tat org.apache.hadoop.yarn.api.impl.pb.client.AMRMProtocolPBClientImpl.registerApplicationMaster(AMRMProtocolPBClientImpl.java:101)\n\t... 3 more\nCaused by: java.lang.NullPointerException: java.lang.NullPointerException\n\tat org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1084)\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:127)\n\t... 5 more\n\n\nIn the ResourceManager's logs:\n\n2011-09-20 15:11:20,973 INFO  ipc.Server (Server.java:run(1497)) - IPC Server handler 2 on 8040, call: org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$ProtoSpecificRequestWritable@455dd32a from 127.0.0.1:33793, error: \njava.lang.NullPointerException\n\tat org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)\n\nMy registration code:\n\n    val appId = args(0).toInt\n    val attemptId = args(1).toInt\n    val timestamp = args(2).toLong\n\n    // these are our application master's parameters\n    val streamerClass = args(3)\n    val tasks = args(4).toInt\n\n    // TODO log params here\n\n    // start the application master helper\n    val conf = new Configuration\n    val applicationMasterHelper = new ApplicationMasterHelper(appId, attemptId, timestamp, conf)\n      .registerWithResourceManager\n\n  .....\n\n  val rpc = YarnRPC.create(conf)\n  val appId = Records.newRecord(classOf[ApplicationId])\n  val appAttemptId = Records.newRecord(classOf[ApplicationAttemptId])\n  val rmAddress = NetUtils.createSocketAddr(conf.get(YarnConfiguration.RM_ADDRESS, YarnConfiguration.DEFAULT_RM_ADDRESS))\n  val resourceManager = rpc.getProxy(classOf[AMRMProtocol], rmAddress, conf).asInstanceOf[AMRMProtocol]\n  var requestId = 0\n\n  appId.setClusterTimestamp(lTimestamp)\n  appId.setId(iAppId)\n  appAttemptId.setApplicationId(appId)\n  appAttemptId.setAttemptId(iAppAttemptId)\n\n  def registerWithResourceManager(): ApplicationMasterHelper = {\n    val req = Records.newRecord(classOf[RegisterApplicationMasterRequest])\n    req.setApplicationAttemptId(appAttemptId)\n    // TODO not sure why these are blank- This is how spark does it\n    req.setHost(\"\")\n    req.setRpcPort(1)\n    req.setTrackingUrl(\"\")\n    resourceManager.registerApplicationMaster(req)\n    this\n  }\n\nMy params are receiving the proper app/attempt/cluster timestamps:\n\napp - 1\nattempt - 1\ntimestamp - 1316556657998\n"
        }
    },
    {
        "filename": "MAPREDUCE-5884.json",
        "creation_time": "2014-04-16T01:07:22.000+0000",
        "stack_trace": "```\norg.apache.hadoop.security.AccessControlException: <someuser> is not authorized to cancel the token\n        at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.cancelToken(AbstractDelegationTokenSecretManager.java:429)\n        at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler.cancelDelegationToken(HistoryClientService.java:400)\n        at org.apache.hadoop.mapreduce.v2.api.impl.pb.service.MRClientProtocolPBServiceImpl.cancelDelegationToken(MRClientProtocolPBServiceImpl.java:286)\n        at org.apache.hadoop.yarn.proto.MRClientProtocol$MRClientProtocolService$2.callBlockingMethod(MRClientProtocol.java:301)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.cancelToken": "  public synchronized TokenIdent cancelToken(Token<TokenIdent> token,\n      String canceller) throws IOException {\n    ByteArrayInputStream buf = new ByteArrayInputStream(token.getIdentifier());\n    DataInputStream in = new DataInputStream(buf);\n    TokenIdent id = createIdentifier();\n    id.readFields(in);\n    LOG.info(\"Token cancelation requested for identifier: \"+id);\n    \n    if (id.getUser() == null) {\n      throw new InvalidToken(\"Token with no owner\");\n    }\n    String owner = id.getUser().getUserName();\n    Text renewer = id.getRenewer();\n    HadoopKerberosName cancelerKrbName = new HadoopKerberosName(canceller);\n    String cancelerShortName = cancelerKrbName.getShortName();\n    if (!canceller.equals(owner)\n        && (renewer == null || renewer.toString().isEmpty() || !cancelerShortName\n            .equals(renewer.toString()))) {\n      throw new AccessControlException(canceller\n          + \" is not authorized to cancel the token\");\n    }\n    DelegationTokenInformation info = null;\n    info = currentTokens.remove(id);\n    if (info == null) {\n      throw new InvalidToken(\"Token not found\");\n    }\n    removeStoredToken(id);\n    return id;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeStoredToken": "  protected void removeStoredToken(TokenIdent ident) throws IOException {\n\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.HistoryClientService.cancelDelegationToken": "    public CancelDelegationTokenResponse cancelDelegationToken(\n        CancelDelegationTokenRequest request) throws IOException {\n        if (!isAllowedDelegationTokenOp()) {\n          throw new IOException(\n              \"Delegation Token can be cancelled only with kerberos authentication\");\n        }\n\n        org.apache.hadoop.yarn.api.records.Token protoToken = request.getDelegationToken();\n        Token<MRDelegationTokenIdentifier> token =\n            new Token<MRDelegationTokenIdentifier>(\n                protoToken.getIdentifier().array(), protoToken.getPassword()\n                    .array(), new Text(protoToken.getKind()), new Text(\n                    protoToken.getService()));\n\n        String user = UserGroupInformation.getCurrentUser().getShortUserName();\n        jhsDTSecretManager.cancelToken(token, user);\n        return Records.newRecord(CancelDelegationTokenResponse.class);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.HistoryClientService.isAllowedDelegationTokenOp": "    private boolean isAllowedDelegationTokenOp() throws IOException {\n      if (UserGroupInformation.isSecurityEnabled()) {\n        return EnumSet.of(AuthenticationMethod.KERBEROS,\n                          AuthenticationMethod.KERBEROS_SSL,\n                          AuthenticationMethod.CERTIFICATE)\n            .contains(UserGroupInformation.getCurrentUser()\n                    .getRealAuthenticationMethod());\n      } else {\n        return true;\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.HistoryClientService.getDelegationToken": "    public GetDelegationTokenResponse getDelegationToken(\n        GetDelegationTokenRequest request) throws IOException {\n\n      UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n\n      // Verify that the connection is kerberos authenticated\n        if (!isAllowedDelegationTokenOp()) {\n          throw new IOException(\n              \"Delegation Token can be issued only with kerberos authentication\");\n        }\n\n      GetDelegationTokenResponse response = recordFactory.newRecordInstance(\n          GetDelegationTokenResponse.class);\n\n      String user = ugi.getUserName();\n      Text owner = new Text(user);\n      Text realUser = null;\n      if (ugi.getRealUser() != null) {\n        realUser = new Text(ugi.getRealUser().getUserName());\n      }\n      MRDelegationTokenIdentifier tokenIdentifier =\n          new MRDelegationTokenIdentifier(owner, new Text(\n            request.getRenewer()), realUser);\n      Token<MRDelegationTokenIdentifier> realJHSToken =\n          new Token<MRDelegationTokenIdentifier>(tokenIdentifier,\n              jhsDTSecretManager);\n      org.apache.hadoop.yarn.api.records.Token mrDToken =\n          org.apache.hadoop.yarn.api.records.Token.newInstance(\n            realJHSToken.getIdentifier(), realJHSToken.getKind().toString(),\n            realJHSToken.getPassword(), realJHSToken.getService().toString());\n      response.setDelegationToken(mrDToken);\n      return response;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.api.impl.pb.service.MRClientProtocolPBServiceImpl.cancelDelegationToken": "  public CancelDelegationTokenResponseProto cancelDelegationToken(\n      RpcController controller, CancelDelegationTokenRequestProto proto)\n      throws ServiceException {\n    CancelDelegationTokenRequestPBImpl request =\n        new CancelDelegationTokenRequestPBImpl(proto);\n      try {\n        CancelDelegationTokenResponse response = real.cancelDelegationToken(request);\n        return ((CancelDelegationTokenResponsePBImpl)response).getProto();\n      } catch (IOException e) {\n        throw new ServiceException(e);\n      }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcRequestWrapper request = (RpcRequestWrapper) writableRequest;\n        RequestHeaderProto rpcRequest = request.requestHeader;\n        String methodName = rpcRequest.getMethodName();\n        \n        \n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = prototype.newBuilderForType()\n            .mergeFrom(request.theRequestRead).build();\n        \n        Message result;\n        try {\n          long startTime = Time.now();\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n          int processingTime = (int) (Time.now() - startTime);\n          int qTime = (int) (startTime - receiveTime);\n          if (LOG.isDebugEnabled()) {\n            LOG.info(\"Served: \" + methodName + \" queueTime= \" + qTime +\n                      \" procesingTime= \" + processingTime);\n          }\n          server.rpcMetrics.addRpcQueueTime(qTime);\n          server.rpcMetrics.addRpcProcessingTime(processingTime);\n          server.rpcDetailedMetrics.addProcessingTime(methodName,\n              processingTime);\n        } catch (ServiceException e) {\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          throw e;\n        }\n        return new RpcResponseWrapper(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getProtocolImpl": "      private static ProtoClassProtoImpl getProtocolImpl(RPC.Server server,\n          String protoName, long clientVersion) throws RpcServerException {\n        ProtoNameVer pv = new ProtoNameVer(protoName, clientVersion);\n        ProtoClassProtoImpl impl = \n            server.getProtocolImplMap(RPC.RpcKind.RPC_PROTOCOL_BUFFER).get(pv);\n        if (impl == null) { // no match for Protocol AND Version\n          VerProtocolImpl highest = \n              server.getHighestSupportedProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER, \n                  protoName);\n          if (highest == null) {\n            throw new RpcNoSuchProtocolException(\n                \"Unknown protocol: \" + protoName);\n          }\n          // protocol supported but not the version that client wants\n          throw new RPC.VersionMismatch(protoName, clientVersion,\n              highest.version);\n        }\n        return impl;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.call": "    public Writable call(RPC.RpcKind rpcKind, String protocol,\n        Writable rpcRequest, long receiveTime) throws Exception {\n      return getRpcInvoker(rpcKind).call(this, protocol, rpcRequest,\n          receiveTime);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRespond": "    void doRespond(Call call) throws IOException {\n      synchronized (call.connection.responseQueue) {\n        call.connection.responseQueue.addLast(call);\n        if (call.connection.responseQueue.size() == 1) {\n          processResponse(call.connection.responseQueue, true);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.toString": "    public String toString() {\n      return getHostAddress() + \":\" + remotePort; \n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getRpcErrorCodeProto": "    public RpcErrorCodeProto getRpcErrorCodeProto() {\n      return errCode;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    boolean close(Connection connection) {\n      boolean exists = remove(connection);\n      if (exists) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(Thread.currentThread().getName() +\n              \": disconnecting client \" + connection +\n              \". Number of active connections: \"+ size());\n        }\n        // only close if actually removed to avoid double-closing due\n        // to possible races\n        connection.close();\n      }\n      return exists;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.isTerse": "    boolean isTerse(Class<?> t) {\n      return terseExceptions.contains(t.toString());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.size": "    int size() {\n      return count.get();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws InterruptedException, IOException,  OutOfMemoryError {\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        channel.socket().setKeepAlive(true);\n        \n        Reader reader = getReader();\n        Connection c = connectionManager.register(channel);\n        key.attach(c);  // so closeCurrentConnection can get the object\n        reader.addConnection(c);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(ByteArrayOutputStream responseBuf,\n                             Call call, RpcStatusProto status, RpcErrorCodeProto erCode,\n                             Writable rv, String errorClass, String error) \n  throws IOException {\n    responseBuf.reset();\n    DataOutputStream out = new DataOutputStream(responseBuf);\n    RpcResponseHeaderProto.Builder headerBuilder =  \n        RpcResponseHeaderProto.newBuilder();\n    headerBuilder.setClientId(ByteString.copyFrom(call.clientId));\n    headerBuilder.setCallId(call.callId);\n    headerBuilder.setRetryCount(call.retryCount);\n    headerBuilder.setStatus(status);\n    headerBuilder.setServerIpcVersionNum(CURRENT_VERSION);\n\n    if (status == RpcStatusProto.SUCCESS) {\n      RpcResponseHeaderProto header = headerBuilder.build();\n      final int headerLen = header.getSerializedSize();\n      int fullLength  = CodedOutputStream.computeRawVarint32Size(headerLen) +\n          headerLen;\n      try {\n        if (rv instanceof ProtobufRpcEngine.RpcWrapper) {\n          ProtobufRpcEngine.RpcWrapper resWrapper = \n              (ProtobufRpcEngine.RpcWrapper) rv;\n          fullLength += resWrapper.getLength();\n          out.writeInt(fullLength);\n          header.writeDelimitedTo(out);\n          rv.write(out);\n        } else { // Have to serialize to buffer to get len\n          final DataOutputBuffer buf = new DataOutputBuffer();\n          rv.write(buf);\n          byte[] data = buf.getData();\n          fullLength += buf.getLength();\n          out.writeInt(fullLength);\n          header.writeDelimitedTo(out);\n          out.write(data, 0, buf.getLength());\n        }\n      } catch (Throwable t) {\n        LOG.warn(\"Error serializing call response for call \" + call, t);\n        // Call back to same function - this is OK since the\n        // buffer is reset at the top, and since status is changed\n        // to ERROR it won't infinite loop.\n        setupResponse(responseBuf, call, RpcStatusProto.ERROR,\n            RpcErrorCodeProto.ERROR_SERIALIZING_RESPONSE,\n            null, t.getClass().getName(),\n            StringUtils.stringifyException(t));\n        return;\n      }\n    } else { // Rpc Failure\n      headerBuilder.setExceptionClassName(errorClass);\n      headerBuilder.setErrorMsg(error);\n      headerBuilder.setErrorDetail(erCode);\n      RpcResponseHeaderProto header = headerBuilder.build();\n      int headerLen = header.getSerializedSize();\n      final int fullLength  = \n          CodedOutputStream.computeRawVarint32Size(headerLen) + headerLen;\n      out.writeInt(fullLength);\n      header.writeDelimitedTo(out);\n    }\n    if (call.connection.useWrap) {\n      wrapWithSasl(responseBuf, call);\n    }\n    call.setResponse(ByteBuffer.wrap(responseBuf.toByteArray()));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.stopIdleScan": "    void stopIdleScan() {\n      idleScanTimer.cancel();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeIdle": "    synchronized void closeIdle(boolean scanAll) {\n      long minLastContact = Time.now() - maxIdleTime;\n      // concurrent iterator might miss new connections added\n      // during the iteration, but that's ok because they won't\n      // be idle yet anyway and will be caught on next scan\n      int closed = 0;\n      for (Connection connection : connections) {\n        // stop if connections dropped below threshold unless scanning all\n        if (!scanAll && size() < idleScanThreshold) {\n          break;\n        }\n        // stop if not scanning all and max connections are closed\n        if (connection.isIdle() &&\n            connection.getLastContact() < minLastContact &&\n            close(connection) &&\n            !scanAll && (++closed == maxIdleToClose)) {\n          break;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(RPC.RpcKind rpcKind, String protocol,\n      Writable param, long receiveTime) throws Exception;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param protocolName - the protocol\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  private void authorize(UserGroupInformation user, String protocolName,\n      InetAddress addr) throws AuthorizationException {\n    if (authorize) {\n      if (protocolName == null) {\n        throw new AuthorizationException(\"Null protocol not authorized\");\n      }\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(protocolName, getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         protocolName);\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeAll": "    void closeAll() {\n      // use a copy of the connections to be absolutely sure the concurrent\n      // iterator doesn't miss a connection\n      for (Connection connection : toArray()) {\n        close(connection);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.remove": "    private boolean remove(Connection connection) {\n      boolean removed = connections.remove(connection);\n      if (removed) {\n        count.getAndDecrement();\n      }\n      return removed;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.startIdleScan": "    void startIdleScan() {\n      scheduleIdleScanTask();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isValid() && key.isWritable()) {\n                  doAsyncWrite(key);\n              }\n            } catch (IOException e) {\n              LOG.info(Thread.currentThread().getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = Time.now();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<Call> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<Call>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              Call call = (Call)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n          \n          for(Call call : calls) {\n            doPurge(call, now);\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.scheduleIdleScanTask": "    private void scheduleIdleScanTask() {\n      if (!running) {\n        return;\n      }\n      TimerTask idleScanTask = new TimerTask(){\n        @Override\n        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }\n      };\n      idleScanTimer.schedule(idleScanTask, idleScanInterval);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcServerException.getRpcStatusProto": "  public RpcStatusProto getRpcStatusProto() {\n    return RpcStatusProto.ERROR;\n  }"
        },
        "bug_report": {
            "Title": "History server uses short user name when canceling tokens",
            "Description": "When the owner of a token tries to explicitly cancel the token, it gets the following error/exception\n{noformat} \n2014-04-14 20:07:35,744 WARN org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:<someuser>/<machine_name>.linkedin.com@<realm>.LINKEDIN.COM (auth:KERBEROS) cause:org.apache.hadoop.security.AccessControlException: <someuser> is not authorized to cancel the token\n2014-04-14 20:07:35,744 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 10020, call org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB.cancelDelegationToken from 172.20.158.61:49042 Call#4 Retry#0: error: org.apache.hadoop.security.AccessControlException: <someuser> is not authorized to cancel the token\norg.apache.hadoop.security.AccessControlException: <someuser> is not authorized to cancel the token\n        at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.cancelToken(AbstractDelegationTokenSecretManager.java:429)\n        at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler.cancelDelegationToken(HistoryClientService.java:400)\n        at org.apache.hadoop.mapreduce.v2.api.impl.pb.service.MRClientProtocolPBServiceImpl.cancelDelegationToken(MRClientProtocolPBServiceImpl.java:286)\n        at org.apache.hadoop.yarn.proto.MRClientProtocol$MRClientProtocolService$2.callBlockingMethod(MRClientProtocol.java:301)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)\n\n{noformat}\n\n\nDetails:\nAbstractDelegationTokenSecretManager.cacelToken() gets the owner as full principal name where as the canceller is the short name.\nThe potential code snippets:\n{code}\nString owner = id.getUser().getUserName(); \n    Text renewer = id.getRenewer();\n    HadoopKerberosName cancelerKrbName = new HadoopKerberosName(canceller);\n    String cancelerShortName = cancelerKrbName.getShortName();\n    if (!canceller.equals(owner)\n        && (renewer == null || renewer.toString().isEmpty() || !cancelerShortName\n            .equals(renewer.toString()))) {\n      throw new AccessControlException(canceller\n          + \" is not authorized to cancel the token\");\n    }\n{code}\n\nThe code shows 'owner' gets the full principal name. Where as the value of 'canceller' depends on who is calling it. \nIn some cases, it is the short name. REF: HistoryClientService.java\n{code}\nString user = UserGroupInformation.getCurrentUser().getShortUserName();\n        jhsDTSecretManager.cancelToken(token, user);\n{code}\n \nIn other cases, the value could be full principal name. REF: FSNamesystem.java.\n{code}\nString canceller = getRemoteUser().getUserName();\n      DelegationTokenIdentifier id = dtSecretManager\n        .cancelToken(token, canceller);\n{code}\n\nPossible resolution:\n--------------------------\nOption 1: in cancelToken() method, compare with both : short name and full principal name.\nPros: Easy. Have to change in one place.\nCons: Someone can argue that it is hacky!\n \nOption 2:\nAll the caller sends the consistent value as 'canceller' : either short name or full principal name.\n\nPros: Cleaner.\nCons: A lot of code changes and potential bug injections.\n\nI'm open for both options.\nPlease give your opinion.\n\nBtw, how it is working now in most cases?  The short name and the full principal name are usually the same for end-users.\n\n\n"
        }
    },
    {
        "filename": "MAPREDUCE-3706.json",
        "creation_time": "2012-01-21T04:01:30.000+0000",
        "stack_trace": "```\nCaused by:\n\norg.apache.commons.httpclient.CircularRedirectException: Circular redirect to 'http://amhost.domain.com:44869/mapreduce/attempts/job_1326992308313_4_4/m/NEW'\n        at org.apache.commons.httpclient.HttpMethodDirector.processRedirectResponse(HttpMethodDirector.java:638)\n        at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:179)\n        at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)\n        at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)\n        at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:148)\n        at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:269)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:66)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-web-proxy.src.main.java.org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink": "  private static void proxyLink(HttpServletRequest req, \n      HttpServletResponse resp, URI link,Cookie c) throws IOException {\n    org.apache.commons.httpclient.URI uri = \n      new org.apache.commons.httpclient.URI(link.toString(), false);\n    HttpClient client = new HttpClient();\n    HttpMethod method = new GetMethod(uri.getEscapedURI());\n\n    @SuppressWarnings(\"unchecked\")\n    Enumeration<String> names = req.getHeaderNames();\n    while(names.hasMoreElements()) {\n      String name = names.nextElement();\n      if(passThroughHeaders.contains(name)) {\n        String value = req.getHeader(name);\n        LOG.debug(\"REQ HEADER: \"+name+\" : \"+value);\n        method.setRequestHeader(name, value);\n      }\n    }\n\n    String user = req.getRemoteUser();\n    if(user != null && !user.isEmpty()) {\n      method.setRequestHeader(\"Cookie\",PROXY_USER_COOKIE_NAME+\"=\"+\n          URLEncoder.encode(user, \"ASCII\"));\n    }\n    OutputStream out = resp.getOutputStream();\n    try {\n      resp.setStatus(client.executeMethod(method));\n      for(Header header : method.getResponseHeaders()) {\n        resp.setHeader(header.getName(), header.getValue());\n      }\n      if(c != null) {\n        resp.addCookie(c);\n      }\n      InputStream in = method.getResponseBodyAsStream();\n      if(in != null) {\n        IOUtils.copyBytes(in, out, 4096, true);\n      }\n    } finally {\n      method.releaseConnection();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-web-proxy.src.main.java.org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet": "  protected void doGet(HttpServletRequest req, HttpServletResponse resp) \n  throws IOException{\n    try {\n      String userApprovedParamS = \n        req.getParameter(ProxyUriUtils.PROXY_APPROVAL_PARAM);\n      boolean userWasWarned = false;\n      boolean userApproved = \n        (userApprovedParamS != null && Boolean.valueOf(userApprovedParamS));\n      boolean securityEnabled = isSecurityEnabled();\n      final String remoteUser = req.getRemoteUser();\n      final String pathInfo = req.getPathInfo();\n\n      String parts[] = pathInfo.split(\"/\", 3);\n      if(parts.length < 2) {\n        LOG.warn(remoteUser+\" Gave an invalid proxy path \"+pathInfo);\n        notFound(resp, \"Your path appears to be formatted incorrectly.\");\n        return;\n      }\n      //parts[0] is empty because path info always starts with a /\n      String appId = parts[1];\n      String rest = parts.length > 2 ? parts[2] : \"\";\n      ApplicationId id = Apps.toAppID(appId);\n      if(id == null) {\n        LOG.warn(req.getRemoteUser()+\" Attempting to access \"+appId+\n        \" that is invalid\");\n        notFound(resp, appId+\" appears to be formatted incorrectly.\");\n        return;\n      }\n      \n      if(securityEnabled) {\n        String cookieName = getCheckCookieName(id); \n        for(Cookie c: req.getCookies()) {\n          if(cookieName.equals(c.getName())) {\n            userWasWarned = true;\n            userApproved = userApproved || Boolean.valueOf(c.getValue());\n            break;\n          }\n        }\n      }\n      \n      boolean checkUser = securityEnabled && (!userWasWarned || !userApproved);\n\n      ApplicationReport applicationReport = getApplicationReport(id);\n      if(applicationReport == null) {\n        LOG.warn(req.getRemoteUser()+\" Attempting to access \"+id+\n            \" that was not found\");\n        notFound(resp, \"Application \"+appId+\" could not be found, \" +\n        \t\t\"please try the history server\");\n        return;\n      }\n      URI trackingUri = ProxyUriUtils.getUriFromAMUrl(\n          applicationReport.getOriginalTrackingUrl());\n      \n      String runningUser = applicationReport.getUser();\n      if(checkUser && !runningUser.equals(remoteUser)) {\n        LOG.info(\"Asking \"+remoteUser+\" if they want to connect to the \" +\n            \"app master GUI of \"+appId+\" owned by \"+runningUser);\n        warnUserPage(resp, ProxyUriUtils.getPathAndQuery(id, rest, \n            req.getQueryString(), true), runningUser, id);\n        return;\n      }\n      \n      URI toFetch = new URI(req.getScheme(), \n          trackingUri.getAuthority(),\n          StringHelper.ujoin(trackingUri.getPath(), rest), req.getQueryString(),\n          null);\n      \n      LOG.info(req.getRemoteUser()+\" is accessing unchecked \"+toFetch+\n          \" which is the app master GUI of \"+appId+\" owned by \"+runningUser);\n\n      switch(applicationReport.getYarnApplicationState()) {\n      case KILLED:\n      case FINISHED:\n      case FAILED:\n        resp.sendRedirect(resp.encodeRedirectURL(toFetch.toString()));\n        return;\n      }\n      Cookie c = null;\n      if(userWasWarned && userApproved) {\n        c = makeCheckCookie(id, true);\n      }\n      proxyLink(req, resp, toFetch, c);\n\n    } catch(URISyntaxException e) {\n      throw new IOException(e); \n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-web-proxy.src.main.java.org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.warnUserPage": "  private static void warnUserPage(HttpServletResponse resp, String link, \n      String user, ApplicationId id) throws IOException {\n    //Set the cookie when we warn which overrides the query parameter\n    //This is so that if a user passes in the approved query parameter without\n    //having first visited this page then this page will still be displayed \n    resp.addCookie(makeCheckCookie(id, false));\n    resp.setContentType(MimeType.HTML);\n    Page p = new Page(resp.getWriter());\n    p.html().\n      h1(\"WARNING: The following page may not be safe!\").h3().\n      _(\"click \").a(link, \"here\").\n      _(\" to continue to an Application Master web interface owned by \", user).\n      _().\n    _();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-web-proxy.src.main.java.org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.isSecurityEnabled": "  private boolean isSecurityEnabled() {\n    Boolean b = (Boolean) getServletContext()\n        .getAttribute(WebAppProxy.IS_SECURITY_ENABLED_ATTRIBUTE);\n    if(b != null) return b;\n    return false;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-web-proxy.src.main.java.org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.getApplicationReport": "  private ApplicationReport getApplicationReport(ApplicationId id) throws IOException {\n    return ((AppReportFetcher) getServletContext()\n        .getAttribute(WebAppProxy.FETCHER_ATTRIBUTE)).getApplicationReport(id);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-web-proxy.src.main.java.org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.getCheckCookieName": "  private static String getCheckCookieName(ApplicationId id){\n    return \"checked_\"+id;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-web-proxy.src.main.java.org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.makeCheckCookie": "  private static Cookie makeCheckCookie(ApplicationId id, boolean isSet) {\n    Cookie c = new Cookie(getCheckCookieName(id),String.valueOf(isSet));\n    c.setPath(ProxyUriUtils.getPath(id));\n    c.setMaxAge(60 * 60 * 2); //2 hours in seconds\n    return c;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-web-proxy.src.main.java.org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.notFound": "  private static void notFound(HttpServletResponse resp, String message) \n    throws IOException {\n    resp.setStatus(HttpServletResponse.SC_NOT_FOUND);\n    resp.setContentType(MimeType.HTML);\n    Page p = new Page(resp.getWriter());\n    p.html().\n      h1(message).\n    _();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-web-proxy.src.main.java.org.apache.hadoop.yarn.server.webproxy.ProxyUriUtils.getUriFromAMUrl": "  public static URI getUriFromAMUrl(String noSchemeUrl) \n    throws URISyntaxException {\n    return new URI(\"http://\"+noSchemeUrl);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-web-proxy.src.main.java.org.apache.hadoop.yarn.server.webproxy.ProxyUriUtils.getPathAndQuery": "  public static String getPathAndQuery(ApplicationId id, String path, \n      String query, boolean approved) {\n    StringBuilder newp = new StringBuilder();\n    newp.append(getPath(id, path));\n    boolean first = appendQuery(newp, query, true);\n    if(approved) {\n      appendQuery(newp, PROXY_APPROVAL_PARAM+\"=true\", first);\n    }\n    return newp.toString();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-web-proxy.src.main.java.org.apache.hadoop.yarn.server.webproxy.ProxyUriUtils.appendQuery": "  private static boolean appendQuery(StringBuilder builder, String query, \n      boolean first) {\n    if(query != null && !query.isEmpty()) {\n      if(first && !query.startsWith(\"?\")) {\n        builder.append('?');\n      }\n      if(!first && !query.startsWith(\"&\")) {\n        builder.append('&');\n      }\n      builder.append(query);\n      return false;\n    }\n    return first;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-web-proxy.src.main.java.org.apache.hadoop.yarn.server.webproxy.ProxyUriUtils.getPath": "  public static String getPath(ApplicationId id, String path) {\n    if(path == null) {\n      return getPath(id);\n    } else {\n      return ujoin(getPath(id), path);\n    }\n  }"
        },
        "bug_report": {
            "Title": "HTTP Circular redirect error on the job attempts page",
            "Description": "submitted job and tried to go to following url:\n\nhttp://rmhost.domain.com:8088/proxy/application_1326992308313_0004/mapreduce/attempts/job_1326992308313_4_4/m/NEW\n\nThis resulted in the following HTTP ERROR:\n\nHTTP ERROR 500\n\nProblem accessing /proxy/application_1326992308313_0004/mapreduce/attempts/job_1326992308313_4_4/m/NEW. Reason:\n\n    Circular redirect to 'http://amhost.domain.com:44869/mapreduce/attempts/job_1326992308313_4_4/m/NEW'\n\nCaused by:\n\norg.apache.commons.httpclient.CircularRedirectException: Circular redirect to 'http://amhost.domain.com:44869/mapreduce/attempts/job_1326992308313_4_4/m/NEW'\n        at org.apache.commons.httpclient.HttpMethodDirector.processRedirectResponse(HttpMethodDirector.java:638)\n        at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:179)\n        at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)\n        at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)\n        at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:148)        at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:269)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:66)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)\n\n\nNote that if you first go to the proxy at: http://rmhost.domain.com:8088/proxy/application_1326992308313_0004/ and then click the links to get here you don't get the error."
        }
    },
    {
        "filename": "MAPREDUCE-5451.json",
        "creation_time": "2013-05-30T00:08:18.000+0000",
        "stack_trace": "```\njava.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:393)\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:928)\n\tat org.apache.hadoop.util.DiskChecker.checkAccessByFileMethods(DiskChecker.java:177)\n\tat org.apache.hadoop.util.DiskChecker.checkDirAccess(DiskChecker.java:164)\n\tat org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:98)\n\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:288)\n\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:431)\n\tat org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:164)\n\tat org.apache.hadoop.mapred.YarnChild.configureLocalDirs(YarnChild.java:235)\n\tat org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:294)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:143)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.nativeio.NativeIO.access": "    public static boolean access(String path, AccessRight desiredAccess)\n        throws IOException {\n      return access0(path, desiredAccess.accessRight());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.nativeio.NativeIO.accessRight": "      public int accessRight() {\n        return accessRight;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.nativeio.NativeIO.access0": "    private static native boolean access0(String path, int requestedAccess);\n\n    /**\n     * Checks whether the current process has desired access rights on\n     * the given path.\n     * \n     * Longer term this native function can be substituted with JDK7\n     * function Files#isReadable, isWritable, isExecutable.\n     *\n     * @param path input path\n     * @param desiredAccess ACCESS_READ, ACCESS_WRITE or ACCESS_EXECUTE\n     * @return true if access is allowed\n     * @throws IOException I/O exception on error\n     */\n    public static boolean access(String path, AccessRight desiredAccess)\n        throws IOException {\n      return access0(path, desiredAccess.accessRight());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileUtil.canRead": "  public static boolean canRead(File f) {\n    if (Shell.WINDOWS) {\n      try {\n        return NativeIO.Windows.access(f.getCanonicalPath(),\n            NativeIO.Windows.AccessRight.ACCESS_READ);\n      } catch (IOException e) {\n        return false;\n      }\n    } else {\n      return f.canRead();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.DiskChecker.checkAccessByFileMethods": "  private static void checkAccessByFileMethods(File dir)\n      throws DiskErrorException {\n    if (!FileUtil.canRead(dir)) {\n      throw new DiskErrorException(\"Directory is not readable: \"\n                                   + dir.toString());\n    }\n\n    if (!FileUtil.canWrite(dir)) {\n      throw new DiskErrorException(\"Directory is not writable: \"\n                                   + dir.toString());\n    }\n\n    if (!FileUtil.canExecute(dir)) {\n      throw new DiskErrorException(\"Directory is not executable: \"\n                                   + dir.toString());\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.DiskChecker.checkDirAccess": "  private static void checkDirAccess(File dir) throws DiskErrorException {\n    if (!dir.isDirectory()) {\n      throw new DiskErrorException(\"Not a directory: \"\n                                   + dir.toString());\n    }\n\n    checkAccessByFileMethods(dir);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.DiskChecker.checkDir": "  public static void checkDir(LocalFileSystem localFS, Path dir,\n                              FsPermission expected)\n  throws DiskErrorException, IOException {\n    mkdirsWithExistsAndPermissionCheck(localFS, dir, expected);\n    checkDirAccess(localFS.pathToFile(dir));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck": "  public static void mkdirsWithExistsAndPermissionCheck(\n      LocalFileSystem localFS, Path dir, FsPermission expected)\n      throws IOException {\n    File directory = localFS.pathToFile(dir);\n    boolean created = false;\n\n    if (!directory.exists())\n      created = mkdirsWithExistsCheck(directory);\n\n    if (created || !localFS.getFileStatus(dir).getPermission().equals(expected))\n        localFS.setPermission(dir, expected);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck": "  public static boolean mkdirsWithExistsCheck(File dir) {\n    if (dir.mkdir() || dir.exists()) {\n      return true;\n    }\n    File canonDir = null;\n    try {\n      canonDir = dir.getCanonicalFile();\n    } catch (IOException e) {\n      return false;\n    }\n    String parent = canonDir.getParent();\n    return (parent != null) && \n           (mkdirsWithExistsCheck(new File(parent)) &&\n                                      (canonDir.mkdir() || canonDir.exists()));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.LocalDirAllocator.confChanged": "    private synchronized void confChanged(Configuration conf) \n        throws IOException {\n      String newLocalDirs = conf.get(contextCfgItemName);\n      if (null == newLocalDirs) {\n        throw new IOException(contextCfgItemName + \" not configured\");\n      }\n      if (!newLocalDirs.equals(savedLocalDirs)) {\n        localDirs = StringUtils.getTrimmedStrings(newLocalDirs);\n        localFS = FileSystem.getLocal(conf);\n        int numDirs = localDirs.length;\n        ArrayList<String> dirs = new ArrayList<String>(numDirs);\n        ArrayList<DF> dfList = new ArrayList<DF>(numDirs);\n        for (int i = 0; i < numDirs; i++) {\n          try {\n            // filter problematic directories\n            Path tmpDir = new Path(localDirs[i]);\n            if(localFS.mkdirs(tmpDir)|| localFS.exists(tmpDir)) {\n              try {\n\n                File tmpFile = tmpDir.isAbsolute()\n                  ? new File(localFS.makeQualified(tmpDir).toUri())\n                  : new File(localDirs[i]);\n\n                DiskChecker.checkDir(tmpFile);\n                dirs.add(tmpFile.getPath());\n                dfList.add(new DF(tmpFile, 30000));\n\n              } catch (DiskErrorException de) {\n                LOG.warn( localDirs[i] + \" is not writable\\n\", de);\n              }\n            } else {\n              LOG.warn( \"Failed to create \" + localDirs[i]);\n            }\n          } catch (IOException ie) { \n            LOG.warn( \"Failed to create \" + localDirs[i] + \": \" +\n                ie.getMessage() + \"\\n\", ie);\n          } //ignore\n        }\n        localDirs = dirs.toArray(new String[dirs.size()]);\n        dirDF = dfList.toArray(new DF[dirs.size()]);\n        savedLocalDirs = newLocalDirs;\n        \n        // randomize the first disk picked in the round-robin selection \n        dirNumLastAccessed = dirIndexRandomizer.nextInt(dirs.size());\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead": "    public synchronized Path getLocalPathToRead(String pathStr, \n        Configuration conf) throws IOException {\n      confChanged(conf);\n      int numDirs = localDirs.length;\n      int numDirsSearched = 0;\n      //remove the leading slash from the path (to make sure that the uri\n      //resolution results in a valid path on the dir being checked)\n      if (pathStr.startsWith(\"/\")) {\n        pathStr = pathStr.substring(1);\n      }\n      while (numDirsSearched < numDirs) {\n        Path file = new Path(localDirs[numDirsSearched], pathStr);\n        if (localFS.exists(file)) {\n          return file;\n        }\n        numDirsSearched++;\n      }\n\n      //no path found\n      throw new DiskErrorException (\"Could not find \" + pathStr +\" in any of\" +\n      \" the configured local directories\");\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.LocalDirAllocator.obtainContext": "  private AllocatorPerContext obtainContext(String contextCfgItemName) {\n    synchronized (contexts) {\n      AllocatorPerContext l = contexts.get(contextCfgItemName);\n      if (l == null) {\n        contexts.put(contextCfgItemName, \n                    (l = new AllocatorPerContext(contextCfgItemName)));\n      }\n      return l;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.configureLocalDirs": "  private static void configureLocalDirs(Task task, JobConf job) throws IOException {\n    String[] localSysDirs = StringUtils.getTrimmedStrings(\n        System.getenv(Environment.LOCAL_DIRS.name()));\n    job.setStrings(MRConfig.LOCAL_DIR, localSysDirs);\n    LOG.info(MRConfig.LOCAL_DIR + \" for child: \" + job.get(MRConfig.LOCAL_DIR));\n    LocalDirAllocator lDirAlloc = new LocalDirAllocator(MRConfig.LOCAL_DIR);\n    Path workDir = null;\n    // First, try to find the JOB_LOCAL_DIR on this host.\n    try {\n      workDir = lDirAlloc.getLocalPathToRead(\"work\", job);\n    } catch (DiskErrorException e) {\n      // DiskErrorException means dir not found. If not found, it will\n      // be created below.\n    }\n    if (workDir == null) {\n      // JOB_LOCAL_DIR doesn't exist on this host -- Create it.\n      workDir = lDirAlloc.getLocalPathForWrite(\"work\", job);\n      FileSystem lfs = FileSystem.getLocal(job).getRaw();\n      boolean madeDir = false;\n      try {\n        madeDir = lfs.mkdirs(workDir);\n      } catch (FileAlreadyExistsException e) {\n        // Since all tasks will be running in their own JVM, the race condition\n        // exists where multiple tasks could be trying to create this directory\n        // at the same time. If this task loses the race, it's okay because\n        // the directory already exists.\n        madeDir = true;\n        workDir = lDirAlloc.getLocalPathToRead(\"work\", job);\n      }\n      if (!madeDir) {\n          throw new IOException(\"Mkdirs failed to create \"\n              + workDir.toString());\n      }\n    }\n    job.set(MRJobConfig.JOB_LOCAL_DIR,workDir.toString());\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.configureTask": "  private static JobConf configureTask(Task task, Credentials credentials,\n      Token<JobTokenIdentifier> jt) throws IOException {\n    final JobConf job = new JobConf(MRJobConfig.JOB_CONF_FILE);\n    job.setCredentials(credentials);\n\n    ApplicationAttemptId appAttemptId =\n        ConverterUtils.toContainerId(\n            System.getenv(Environment.CONTAINER_ID.name()))\n            .getApplicationAttemptId();\n    LOG.debug(\"APPLICATION_ATTEMPT_ID: \" + appAttemptId);\n    // Set it in conf, so as to be able to be used the the OutputCommitter.\n    job.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID,\n        appAttemptId.getAttemptId());\n\n    // set tcp nodelay\n    job.setBoolean(\"ipc.client.tcpnodelay\", true);\n    job.setClass(MRConfig.TASK_LOCAL_OUTPUT_CLASS,\n        YarnOutputFiles.class, MapOutputFile.class);\n    // set the jobToken and shuffle secrets into task\n    task.setJobTokenSecret(\n        JobTokenSecretManager.createSecretKey(jt.getPassword()));\n    byte[] shuffleSecret = TokenCache.getShuffleSecretKey(credentials);\n    if (shuffleSecret == null) {\n      LOG.warn(\"Shuffle secret missing from task credentials.\"\n          + \" Using job token secret as shuffle secret.\");\n      shuffleSecret = jt.getPassword();\n    }\n    task.setShuffleSecret(\n        JobTokenSecretManager.createSecretKey(shuffleSecret));\n\n    // setup the child's MRConfig.LOCAL_DIR.\n    configureLocalDirs(task, job);\n\n    // setup the child's attempt directories\n    // Do the task-type specific localization\n    task.localizeConfiguration(job);\n\n    // Set up the DistributedCache related configs\n    setupDistributedCacheConfig(job);\n\n    // Overwrite the localized task jobconf which is linked to in the current\n    // work-dir.\n    Path localTaskFile = new Path(MRJobConfig.JOB_CONF_FILE);\n    writeLocalJobFile(localTaskFile, job);\n    task.setJobFile(localTaskFile.toString());\n    task.setConf(job);\n    return job;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.writeLocalJobFile": "  private static void writeLocalJobFile(Path jobFile, JobConf conf)\n      throws IOException {\n    FileSystem localFs = FileSystem.getLocal(conf);\n    localFs.delete(jobFile);\n    OutputStream out = null;\n    try {\n      out = FileSystem.create(localFs, jobFile, urw_gr);\n      conf.writeXml(out);\n    } finally {\n      IOUtils.cleanup(LOG, out);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.setupDistributedCacheConfig": "  private static void setupDistributedCacheConfig(final JobConf job)\n      throws IOException {\n\n    String localWorkDir = System.getenv(\"PWD\");\n    //        ^ ^ all symlinks are created in the current work-dir\n\n    // Update the configuration object with localized archives.\n    URI[] cacheArchives = DistributedCache.getCacheArchives(job);\n    if (cacheArchives != null) {\n      List<String> localArchives = new ArrayList<String>();\n      for (int i = 0; i < cacheArchives.length; ++i) {\n        URI u = cacheArchives[i];\n        Path p = new Path(u);\n        Path name =\n            new Path((null == u.getFragment()) ? p.getName()\n                : u.getFragment());\n        String linkName = name.toUri().getPath();\n        localArchives.add(new Path(localWorkDir, linkName).toUri().getPath());\n      }\n      if (!localArchives.isEmpty()) {\n        job.set(MRJobConfig.CACHE_LOCALARCHIVES, StringUtils\n            .arrayToString(localArchives.toArray(new String[localArchives\n                .size()])));\n      }\n    }\n\n    // Update the configuration object with localized files.\n    URI[] cacheFiles = DistributedCache.getCacheFiles(job);\n    if (cacheFiles != null) {\n      List<String> localFiles = new ArrayList<String>();\n      for (int i = 0; i < cacheFiles.length; ++i) {\n        URI u = cacheFiles[i];\n        Path p = new Path(u);\n        Path name =\n            new Path((null == u.getFragment()) ? p.getName()\n                : u.getFragment());\n        String linkName = name.toUri().getPath();\n        localFiles.add(new Path(localWorkDir, linkName).toUri().getPath());\n      }\n      if (!localFiles.isEmpty()) {\n        job.set(MRJobConfig.CACHE_LOCALFILES,\n            StringUtils.arrayToString(localFiles\n                .toArray(new String[localFiles.size()])));\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.main": "  public static void main(String[] args) throws Throwable {\n    Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\n    LOG.debug(\"Child starting\");\n\n    final JobConf defaultConf = new JobConf();\n    defaultConf.addResource(MRJobConfig.JOB_CONF_FILE);\n    UserGroupInformation.setConfiguration(defaultConf);\n\n    String host = args[0];\n    int port = Integer.parseInt(args[1]);\n    final InetSocketAddress address =\n        NetUtils.createSocketAddrForHost(host, port);\n    final TaskAttemptID firstTaskid = TaskAttemptID.forName(args[2]);\n    int jvmIdInt = Integer.parseInt(args[3]);\n    JVMId jvmId = new JVMId(firstTaskid.getJobID(),\n        firstTaskid.getTaskType() == TaskType.MAP, jvmIdInt);\n\n    // initialize metrics\n    DefaultMetricsSystem.initialize(\n        StringUtils.camelize(firstTaskid.getTaskType().name()) +\"Task\");\n\n    // Security framework already loaded the tokens into current ugi\n    Credentials credentials =\n        UserGroupInformation.getCurrentUser().getCredentials();\n    LOG.info(\"Executing with tokens:\");\n    for (Token<?> token: credentials.getAllTokens()) {\n      LOG.info(token);\n    }\n\n    // Create TaskUmbilicalProtocol as actual task owner.\n    UserGroupInformation taskOwner =\n      UserGroupInformation.createRemoteUser(firstTaskid.getJobID().toString());\n    Token<JobTokenIdentifier> jt = TokenCache.getJobToken(credentials);\n    SecurityUtil.setTokenService(jt, address);\n    taskOwner.addToken(jt);\n    final TaskUmbilicalProtocol umbilical =\n      taskOwner.doAs(new PrivilegedExceptionAction<TaskUmbilicalProtocol>() {\n      @Override\n      public TaskUmbilicalProtocol run() throws Exception {\n        return (TaskUmbilicalProtocol)RPC.getProxy(TaskUmbilicalProtocol.class,\n            TaskUmbilicalProtocol.versionID, address, defaultConf);\n      }\n    });\n\n    // report non-pid to application master\n    JvmContext context = new JvmContext(jvmId, \"-1000\");\n    LOG.debug(\"PID: \" + System.getenv().get(\"JVM_PID\"));\n    Task task = null;\n    UserGroupInformation childUGI = null;\n\n    try {\n      int idleLoopCount = 0;\n      JvmTask myTask = null;;\n      // poll for new task\n      for (int idle = 0; null == myTask; ++idle) {\n        long sleepTimeMilliSecs = Math.min(idle * 500, 1500);\n        LOG.info(\"Sleeping for \" + sleepTimeMilliSecs\n            + \"ms before retrying again. Got null now.\");\n        MILLISECONDS.sleep(sleepTimeMilliSecs);\n        myTask = umbilical.getTask(context);\n      }\n      if (myTask.shouldDie()) {\n        return;\n      }\n\n      task = myTask.getTask();\n      YarnChild.taskid = task.getTaskID();\n\n      // Create the job-conf and set credentials\n      final JobConf job = configureTask(task, credentials, jt);\n\n      // log the system properties\n      String systemPropsToLog = MRApps.getSystemPropertiesToLog(job);\n      if (systemPropsToLog != null) {\n        LOG.info(systemPropsToLog);\n      }\n\n      // Initiate Java VM metrics\n      JvmMetrics.initSingleton(jvmId.toString(), job.getSessionId());\n      childUGI = UserGroupInformation.createRemoteUser(System\n          .getenv(ApplicationConstants.Environment.USER.toString()));\n      // Add tokens to new user so that it may execute its task correctly.\n      childUGI.addCredentials(credentials);\n\n      // set job classloader if configured before invoking the task\n      MRApps.setJobClassLoader(job);\n\n      // Create a final reference to the task for the doAs block\n      final Task taskFinal = task;\n      childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n        @Override\n        public Object run() throws Exception {\n          // use job-specified working directory\n          FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory());\n          taskFinal.run(job, umbilical); // run the task\n          return null;\n        }\n      });\n    } catch (FSError e) {\n      LOG.fatal(\"FSError from child\", e);\n      umbilical.fsError(taskid, e.getMessage());\n    } catch (Exception exception) {\n      LOG.warn(\"Exception running child : \"\n          + StringUtils.stringifyException(exception));\n      try {\n        if (task != null) {\n          // do cleanup for the task\n          if (childUGI == null) { // no need to job into doAs block\n            task.taskCleanup(umbilical);\n          } else {\n            final Task taskFinal = task;\n            childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n              @Override\n              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }\n            });\n          }\n        }\n      } catch (Exception e) {\n        LOG.info(\"Exception cleaning up: \" + StringUtils.stringifyException(e));\n      }\n      // Report back any failures, for diagnostic purposes\n      if (taskid != null) {\n        umbilical.fatalError(taskid, StringUtils.stringifyException(exception));\n      }\n    } catch (Throwable throwable) {\n      LOG.fatal(\"Error running child : \"\n    \t        + StringUtils.stringifyException(throwable));\n      if (taskid != null) {\n        Throwable tCause = throwable.getCause();\n        String cause = tCause == null\n                                 ? throwable.getMessage()\n                                 : StringUtils.stringifyException(tCause);\n        umbilical.fatalError(taskid, cause);\n      }\n    } finally {\n      RPC.stopProxy(umbilical);\n      DefaultMetricsSystem.shutdown();\n      // Shutting down log4j of the child-vm...\n      // This assumes that on return from Task.run()\n      // there is no more logging done.\n      LogManager.shutdown();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.run": "              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Path.isAbsolute": "  public boolean isAbsolute() {\n     return isUriPathAbsolute();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Path.isUriPathAbsolute": "  public boolean isUriPathAbsolute() {\n    int start = hasWindowsDrive(uri.getPath()) ? 3 : 0;\n    return uri.getPath().startsWith(SEPARATOR, start);\n   }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getLocal": "  public static LocalFileSystem getLocal(Configuration conf)\n    throws IOException {\n    return (LocalFileSystem)get(LocalFileSystem.NAME, conf);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.get": "    FileSystem get(URI uri, Configuration conf) throws IOException{\n      Key key = new Key(uri, conf);\n      return getInternal(uri, conf, key);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.localizeConfiguration": "  public void localizeConfiguration(JobConf conf) throws IOException {\n    conf.set(JobContext.TASK_ID, taskId.getTaskID().toString()); \n    conf.set(JobContext.TASK_ATTEMPT_ID, taskId.toString());\n    conf.setBoolean(JobContext.TASK_ISMAP, isMapTask());\n    conf.setInt(JobContext.TASK_PARTITION, partition);\n    conf.set(JobContext.ID, taskId.getJobID().toString());\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.toString": "  public String toString() { return taskId.toString(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getTaskID": "  public TaskAttemptID getTaskID() { return taskId; }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.isMapTask": "  public abstract boolean isMapTask();\n\n  public Progress getProgress() { return taskProgress; }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getJobID": "  public JobID getJobID() {\n    return taskId.getJobID();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setJobTokenSecret": "  public void setJobTokenSecret(SecretKey tokenSecret) {\n    this.tokenSecret = tokenSecret;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setConf": "  public void setConf(Configuration conf) {\n    if (conf instanceof JobConf) {\n      this.conf = (JobConf) conf;\n    } else {\n      this.conf = new JobConf(conf);\n    }\n    this.mapOutputFile = ReflectionUtils.newInstance(\n        conf.getClass(MRConfig.TASK_LOCAL_OUTPUT_CLASS,\n          MROutputFiles.class, MapOutputFile.class), conf);\n    this.lDirAlloc = new LocalDirAllocator(MRConfig.LOCAL_DIR);\n    // add the static resolutions (this is required for the junit to\n    // work on testcases that simulate multiple nodes on a single physical\n    // node.\n    String hostToResolved[] = conf.getStrings(MRConfig.STATIC_RESOLUTIONS);\n    if (hostToResolved != null) {\n      for (String str : hostToResolved) {\n        String name = str.substring(0, str.indexOf('='));\n        String resolvedName = str.substring(str.indexOf('=') + 1);\n        NetUtils.addStaticResolution(name, resolvedName);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setJobFile": "  public void setJobFile(String jobFile) { this.jobFile = jobFile; }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.setCredentials": "  public void setCredentials(Credentials credentials) {\n    this.credentials = credentials;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setShuffleSecret": "  public void setShuffleSecret(SecretKey shuffleSecret) {\n    this.shuffleSecret = shuffleSecret;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.toString": "  public String toString() { \n    return appendTo(new StringBuilder(JVM)).toString();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.appendTo": "  protected StringBuilder appendTo(StringBuilder builder) {\n    return jobId.appendTo(builder).\n                 append(SEPARATOR).\n                 append(isMap ? 'm' : 'r').\n                 append(SEPARATOR).\n                 append(idFormat.format(id));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getSessionId": "  public String getSessionId() {\n      return get(\"session.id\", \"\");\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fsError": "  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.taskCleanup": "  void taskCleanup(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    // set phase for this task\n    setPhase(TaskStatus.Phase.CLEANUP);\n    getProgress().setStatus(\"cleanup\");\n    statusUpdate(umbilical);\n    LOG.info(\"Runnning cleanup for the task\");\n    // do the cleanup\n    committer.abortTask(taskContext);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.statusUpdate": "  public void statusUpdate(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    int retries = MAX_RETRIES;\n    while (true) {\n      try {\n        if (!umbilical.statusUpdate(getTaskID(), taskStatus)) {\n          LOG.warn(\"Parent died.  Exiting \"+taskId);\n          System.exit(66);\n        }\n        taskStatus.clearStatus();\n        return;\n      } catch (InterruptedException ie) {\n        Thread.currentThread().interrupt(); // interrupt ourself\n      } catch (IOException ie) {\n        LOG.warn(\"Failure sending status update: \" + \n                  StringUtils.stringifyException(ie));\n        if (--retries == 0) {\n          throw ie;\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setPhase": "  protected synchronized void setPhase(TaskStatus.Phase phase){\n    this.taskStatus.setPhase(phase); \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setStatus": "    public void setStatus(String status) {\n      taskProgress.setStatus(normalizeStatus(status, conf));\n      // indicate that progress update needs to be sent\n      setProgressFlag();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getProgress": "    public float getProgress() {\n      return taskProgress.getProgress();\n    };",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.shouldDie": "  public boolean shouldDie() {\n    return shouldDie;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.getTask": "  public Task getTask() {\n    return t;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.getTask": "  JvmTask getTask(JvmContext context) throws IOException;\n  \n  /**\n   * Report child's progress to parent.\n   * \n   * @param taskId task-id of the child\n   * @param taskStatus status of the child\n   * @throws IOException\n   * @throws InterruptedException\n   * @return True if the task is known\n   */\n  boolean statusUpdate(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;\n  \n  /** Report error messages back to parent.  Calls should be sparing, since all\n   *  such messages are held in the job tracker.\n   *  @param taskid the id of the task involved\n   *  @param trace the text to report\n   */\n  void reportDiagnosticInfo(TaskAttemptID taskid, String trace) throws IOException;\n  \n  /**\n   * Report the record range which is going to process next by the Task.\n   * @param taskid the id of the task involved\n   * @param range the range of record sequence nos\n   * @throws IOException\n   */\n  void reportNextRecordRange(TaskAttemptID taskid, SortedRanges.Range range) \n    throws IOException;\n\n  /** Periodically called by child to check if parent is still alive. \n   * @return True if the task is known\n   */\n  boolean ping(TaskAttemptID taskid) throws IOException;\n\n  /** Report that the task is successfully completed.  Failure is assumed if\n   * the task process exits without calling this.\n   * @param taskid task's id\n   */\n  void done(TaskAttemptID taskid) throws IOException;\n  \n  /** \n   * Report that the task is complete, but its commit is pending.\n   * \n   * @param taskId task's id\n   * @param taskStatus status of the child\n   * @throws IOException\n   */\n  void commitPending(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;  \n\n  /**\n   * Polling to know whether the task can go-ahead with commit \n   * @param taskid\n   * @return true/false \n   * @throws IOException\n   */\n  boolean canCommit(TaskAttemptID taskid) throws IOException;\n\n  /** Report that a reduce-task couldn't shuffle map-outputs.*/\n  void shuffleError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Report that the task encounted a local filesystem error.*/\n  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.getJobID": "  public JobID getJobID() {\n    return (JobID) super.getJobID();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fatalError": "  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getWorkingDirectory": "  public Path getWorkingDirectory() {\n    String name = get(JobContext.WORKING_DIR);\n    if (name != null) {\n      return new Path(name);\n    } else {\n      try {\n        Path dir = FileSystem.get(this).getWorkingDirectory();\n        set(JobContext.WORKING_DIR, dir.toString());\n        return dir;\n      } catch (IOException e) {\n        throw new RuntimeException(e);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.forName": "  public static TaskAttemptID forName(String str\n                                      ) throws IllegalArgumentException {\n    return (TaskAttemptID) \n             org.apache.hadoop.mapreduce.TaskAttemptID.forName(str);\n  }"
        },
        "bug_report": {
            "Title": "MR uses LD_LIBRARY_PATH which doesn't mean anything in Windows",
            "Description": "In order to set the path for loading native libraries, MR relies on the default value of the mapreduce.admin.user.env configuration setting the LD_LIBRARY_PATH environment entry. There are two problems with this setting in Windows:\na) LD_LIBRARY_PATH doesn't mean anything in Windows.\nb) It sets it using $HADOOP_COMMON_HOME, instead of %HADOOP_COMMON_HOME%.\n\nThe default value here should be platform-dependent (use the PATH variable in Windows instead of LD_LIBRARY_PATH), or we should rely on another mechanism. The net effect is that in Windows unless this configuration is over-ridden MR jobs fail with this error:\n\n{code}\n2013-05-29 13:51:41,049 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:393)\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:928)\n\tat org.apache.hadoop.util.DiskChecker.checkAccessByFileMethods(DiskChecker.java:177)\n\tat org.apache.hadoop.util.DiskChecker.checkDirAccess(DiskChecker.java:164)\n\tat org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:98)\n\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:288)\n\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:431)\n\tat org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:164)\n\tat org.apache.hadoop.mapred.YarnChild.configureLocalDirs(YarnChild.java:235)\n\tat org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:294)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:143)\n{code}"
        }
    },
    {
        "filename": "MAPREDUCE-2463.json",
        "creation_time": "2011-05-02T06:35:07.000+0000",
        "stack_trace": "```\njava.lang.IllegalArgumentException: Wrong FS: hdfs://10.18.52.146:9000/history/job_201104291518_0001_root, expected: file:///\n\tat org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:402)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:58)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:419)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:294)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:215)\n\tat org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1516)\n\tat org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1492)\n\tat org.apache.hadoop.fs.FileSystem.moveFromLocalFile(FileSystem.java:1482)\n\tat org.apache.hadoop.mapreduce.jobhistory.JobHistory.moveToDoneNow(JobHistory.java:348)\n\tat org.apache.hadoop.mapreduce.jobhistory.JobHistory.access$200(JobHistory.java:61)\n\tat org.apache.hadoop.mapreduce.jobhistory.JobHistory$1.run(JobHistory.java:439)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:619)\n```",
        "source_code": {},
        "bug_report": {
            "Title": "Job History files are not moving to done folder when job history location is hdfs location",
            "Description": "If \"mapreduce.jobtracker.jobhistory.location\" is configured as HDFS location then either during initialization of Job Tracker (while moving old job history files) or after completion of the job, history files are not moving to done and giving following exception.\n\n{code:xml} \n2011-04-29 15:27:27,813 ERROR org.apache.hadoop.mapreduce.jobhistory.JobHistory: Unable to move history file to DONE folder.\njava.lang.IllegalArgumentException: Wrong FS: hdfs://10.18.52.146:9000/history/job_201104291518_0001_root, expected: file:///\n\tat org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:402)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:58)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:419)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:294)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:215)\n\tat org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1516)\n\tat org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1492)\n\tat org.apache.hadoop.fs.FileSystem.moveFromLocalFile(FileSystem.java:1482)\n\tat org.apache.hadoop.mapreduce.jobhistory.JobHistory.moveToDoneNow(JobHistory.java:348)\n\tat org.apache.hadoop.mapreduce.jobhistory.JobHistory.access$200(JobHistory.java:61)\n\tat org.apache.hadoop.mapreduce.jobhistory.JobHistory$1.run(JobHistory.java:439)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:619)\n\n{code} \n"
        }
    },
    {
        "filename": "MAPREDUCE-3531.json",
        "creation_time": "2011-12-12T05:42:24.000+0000",
        "stack_trace": "```\njava.lang.IllegalArgumentException: Invalid key to HMAC computation\n        at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:141)\n        at org.apache.hadoop.yarn.server.security.ContainerTokenSecretManager.createPassword(ContainerTokenSecretManager.java:61)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.createContainer(LeafQueue.java:1108)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getContainer(LeafQueue.java:1091)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1137)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1001)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:973)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:760)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:583)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:513)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:569)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:611)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:77)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:294)\n        at java.lang.Thread.run(Thread.java:619)\nCaused by: java.security.InvalidKeyException: Secret key expected\n        at com.sun.crypto.provider.HmacCore.a(DashoA13*..)\n        at com.sun.crypto.provider.HmacSHA1.engineInit(DashoA13*..)\n        at javax.crypto.Mac.init(DashoA13*..)\n        at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:139)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.SecretManager.createPassword": "  protected static byte[] createPassword(byte[] identifier, \n                                         SecretKey key) {\n    Mac mac = threadLocalMac.get();\n    try {\n      mac.init(key);\n    } catch (InvalidKeyException ike) {\n      throw new IllegalArgumentException(\"Invalid key to HMAC computation\", \n                                         ike);\n    }\n    return mac.doFinal(identifier);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.createContainer": "  public Container createContainer(SchedulerApp application, SchedulerNode node, \n      Resource capability, Priority priority) {\n\n    NodeId nodeId = node.getRMNode().getNodeID();\n    ContainerId containerId = BuilderUtils.newContainerId(application\n        .getApplicationAttemptId(), application.getNewContainerId());\n    ContainerToken containerToken = null;\n\n    // If security is enabled, send the container-tokens too.\n    if (UserGroupInformation.isSecurityEnabled()) {\n      ContainerTokenIdentifier tokenIdentifier = new ContainerTokenIdentifier(\n          containerId, nodeId.toString(), capability);\n      containerToken = BuilderUtils.newContainerToken(nodeId, ByteBuffer\n          .wrap(containerTokenSecretManager\n              .createPassword(tokenIdentifier)), tokenIdentifier);\n    }\n\n    // Create the container\n    Container container = BuilderUtils.newContainer(containerId, nodeId,\n        node.getRMNode().getHttpAddress(), capability, priority,\n        containerToken);\n\n    return container;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.toString": "  public String toString() {\n    return queueName + \":\" + capacity + \":\" + absoluteCapacity + \":\" + \n    getUsedCapacity() + \":\" + getUtilization() + \":\" + \n    getNumApplications() + \":\" + getNumContainers();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getContainer": "  private Container getContainer(RMContainer rmContainer, \n      SchedulerApp application, SchedulerNode node, \n      Resource capability, Priority priority) {\n    return (rmContainer != null) ? rmContainer.getContainer() :\n      createContainer(application, node, capability, priority);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer": "    public synchronized void assignContainer(Resource resource) {\n      Resources.addTo(consumed, resource);\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.reserve": "  private void reserve(SchedulerApp application, Priority priority, \n      SchedulerNode node, RMContainer rmContainer, Container container) {\n    // Update reserved metrics if this is the first reservation\n    if (rmContainer == null) {\n      getMetrics().reserveResource(\n          application.getUser(), container.getResource());\n    }\n\n    // Inform the application \n    rmContainer = application.reserve(node, priority, rmContainer, container);\n    \n    // Update the node\n    node.reserveResource(application, priority, rmContainer);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getUtilization": "  public synchronized float getUtilization() {\n    return utilization;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.unreserve": "  private void unreserve(SchedulerApp application, Priority priority, \n      SchedulerNode node, RMContainer rmContainer) {\n    // Done with the reservation?\n    application.unreserve(node, priority);\n    node.unreserveResource(application);\n      \n      // Update reserved metrics\n    getMetrics().unreserveResource(\n        application.getUser(), rmContainer.getContainer().getResource());\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers": "  private Resource assignNodeLocalContainers(Resource clusterResource, \n      SchedulerNode node, SchedulerApp application, \n      Priority priority, RMContainer reservedContainer) {\n    ResourceRequest request = \n        application.getResourceRequest(priority, node.getHostName());\n    if (request != null) {\n      if (canAssign(application, priority, node, NodeType.NODE_LOCAL, \n          reservedContainer)) {\n        return assignContainer(clusterResource, node, application, priority, \n            request, NodeType.NODE_LOCAL, reservedContainer);\n      }\n    }\n    \n    return Resources.none();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.canAssign": "  boolean canAssign(SchedulerApp application, Priority priority, \n      SchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Reserved... \n    if (reservedContainer != null) {\n      return true;\n    }\n    \n    // Clearly we need containers for this application...\n    if (type == NodeType.OFF_SWITCH) {\n      // 'Delay' off-switch\n      ResourceRequest offSwitchRequest = \n          application.getResourceRequest(priority, RMNode.ANY);\n      long missedOpportunities = application.getSchedulingOpportunities(priority);\n      long requiredContainers = offSwitchRequest.getNumContainers(); \n      \n      float localityWaitFactor = \n        application.getLocalityWaitFactor(priority, \n            scheduler.getNumClusterNodes());\n      \n      return ((requiredContainers * localityWaitFactor) < missedOpportunities);\n    }\n\n    // Check if we need containers on this rack \n    ResourceRequest rackLocalRequest = \n      application.getResourceRequest(priority, node.getRackName());\n    if (rackLocalRequest == null || rackLocalRequest.getNumContainers() <= 0) {\n      return false;\n    }\n      \n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type == NodeType.RACK_LOCAL) {\n      return true;\n    }\n\n    // Check if we need containers on this host\n    if (type == NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest = \n        application.getResourceRequest(priority, node.getHostName());\n      if (nodeLocalRequest != null) {\n        return nodeLocalRequest.getNumContainers() > 0;\n      }\n    }\n\n    return false;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode": "  private Resource assignContainersOnNode(Resource clusterResource, \n      SchedulerNode node, SchedulerApp application, \n      Priority priority, RMContainer reservedContainer) {\n\n    Resource assigned = Resources.none();\n\n    // Data-local\n    assigned = \n        assignNodeLocalContainers(clusterResource, node, application, priority,\n            reservedContainer); \n    if (Resources.greaterThan(assigned, Resources.none())) {\n      return assigned;\n    }\n\n    // Rack-local\n    assigned = \n        assignRackLocalContainers(clusterResource, node, application, priority, \n            reservedContainer);\n    if (Resources.greaterThan(assigned, Resources.none())) {\n      return assigned;\n    }\n    \n    // Off-switch\n    return assignOffSwitchContainers(clusterResource, node, application, \n        priority, reservedContainer);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignOffSwitchContainers": "  private Resource assignOffSwitchContainers(Resource clusterResource, SchedulerNode node, \n      SchedulerApp application, Priority priority, \n      RMContainer reservedContainer) {\n    ResourceRequest request = \n      application.getResourceRequest(priority, RMNode.ANY);\n    if (request != null) {\n      if (canAssign(application, priority, node, NodeType.OFF_SWITCH, \n          reservedContainer)) {\n        return assignContainer(clusterResource, node, application, priority, request, \n            NodeType.OFF_SWITCH, reservedContainer);\n      }\n    }\n    \n    return Resources.none();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignRackLocalContainers": "  private Resource assignRackLocalContainers(Resource clusterResource,  \n      SchedulerNode node, SchedulerApp application, Priority priority,\n      RMContainer reservedContainer) {\n    ResourceRequest request = \n      application.getResourceRequest(priority, node.getRackName());\n    if (request != null) {\n      if (canAssign(application, priority, node, NodeType.RACK_LOCAL, \n          reservedContainer)) {\n        return assignContainer(clusterResource, node, application, priority, request, \n            NodeType.RACK_LOCAL, reservedContainer);\n      }\n    }\n    return Resources.none();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers": "  public synchronized Resource \n  assignContainers(Resource clusterResource, SchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node=\" + node.getHostName()\n        + \" #applications=\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer = node.getReservedContainer();\n    if (reservedContainer != null) {\n      SchedulerApp application = \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return assignReservedContainer(application, node, reservedContainer, \n          clusterResource);\n    }\n    \n    // Try to assign containers to applications in order\n    for (SchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n      }\n      application.showRequests();\n\n      synchronized (application) {\n        computeAndSetUserResourceLimit(application, clusterResource);\n        \n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required = \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n          // Do we need containers at this 'priority'?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Are we going over limits by allocating to this application?\n          // Maximum Capacity of the queue\n          if (!assignToQueue(clusterResource, required)) {\n            return Resources.none();\n          }\n\n          // User limits\n          Resource userLimit = \n            computeUserLimit(application, clusterResource, required); \n          if (!assignToUser(application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          Resource assigned = \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n  \n          // Did we schedule or reserve a container?\n          if (Resources.greaterThan(assigned, Resources.none())) {\n            Resource assignedResource = \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n            // Book-keeping\n            allocateResource(clusterResource, \n                application, assignedResource);\n            \n            // Reset scheduling opportunities\n            application.resetSchedulingOpportunities(priority);\n            \n            // Done\n            return assignedResource; \n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return Resources.none();\n\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignToQueue": "  private synchronized boolean assignToQueue(Resource clusterResource, \n      Resource required) {\n    // Check how of the cluster's absolute capacity we are currently using...\n    float potentialNewCapacity = \n      (float)(usedResources.getMemory() + required.getMemory()) / \n        clusterResource.getMemory();\n    LOG.info(getQueueName() + \n        \" usedResources: \" + usedResources.getMemory() + \n        \" currentCapacity \" + ((float)usedResources.getMemory())/clusterResource.getMemory() + \n        \" required \" + required.getMemory() +\n        \" potentialNewCapacity: \" + potentialNewCapacity + \" ( \" +\n        \" max-capacity: \" + absoluteMaxCapacity + \")\");\n    if (potentialNewCapacity > absoluteMaxCapacity) {\n      return false;\n    }\n    return true;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getUser": "  private synchronized User getUser(String userName) {\n    User user = users.get(userName);\n    if (user == null) {\n      user = new User();\n      users.put(userName, user);\n    }\n    return user;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.needContainers": "  boolean needContainers(SchedulerApp application, Priority priority, Resource required) {\n    int requiredContainers = application.getTotalRequiredResources(priority);\n    int reservedContainers = application.getNumReservedContainers(priority);\n    int starvation = 0;\n    if (reservedContainers > 0) {\n      float nodeFactor = \n          ((float)required.getMemory() / getMaximumAllocation().getMemory());\n      \n      // Use percentage of node required to bias against large containers...\n      // Protect against corner case where you need the whole node with\n      // Math.min(nodeFactor, minimumAllocationFactor)\n      starvation = \n          (int)((application.getReReservations(priority) / (float)reservedContainers) * \n                (1.0f - (Math.min(nodeFactor, getMinimumAllocationFactor())))\n               );\n      \n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"needsContainers:\" +\n            \" app.#re-reserve=\" + application.getReReservations(priority) + \n            \" reserved=\" + reservedContainers + \n            \" nodeFactor=\" + nodeFactor + \n            \" minAllocFactor=\" + minimumAllocationFactor +\n            \" starvation=\" + starvation);\n      }\n    }\n    return (((starvation + requiredContainers) - reservedContainers) > 0);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.computeAndSetUserResourceLimit": "  private void computeAndSetUserResourceLimit(SchedulerApp application, \n      Resource clusterResource) {\n    Resource userLimit = \n        computeUserLimit(application, clusterResource, Resources.none());\n    application.setAvailableResourceLimit(userLimit);\n    metrics.setAvailableResourcesToUser(application.getUser(), \n        application.getHeadroom());\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignToUser": "  private synchronized boolean assignToUser(String userName, Resource limit) {\n\n    User user = getUser(userName);\n    \n    // Note: We aren't considering the current request since there is a fixed\n    // overhead of the AM, but it's a >= check, so... \n    if ((user.getConsumedResources().getMemory()) > limit.getMemory()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"User \" + userName + \" in queue \" + getQueueName() + \n            \" will exceed limit - \" +  \n            \" consumed: \" + user.getConsumedResources() + \n            \" limit: \" + limit\n        );\n      }\n      return false;\n    }\n\n    return true;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getApplication": "  private synchronized SchedulerApp getApplication(\n      ApplicationAttemptId applicationAttemptId) {\n    return applicationsMap.get(applicationAttemptId);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.computeUserLimit": "  private Resource computeUserLimit(SchedulerApp application, \n      Resource clusterResource, Resource required) {\n    // What is our current capacity? \n    // * It is equal to the max(required, queue-capacity) if\n    //   we're running below capacity. The 'max' ensures that jobs in queues\n    //   with miniscule capacity (< 1 slot) make progress\n    // * If we're running over capacity, then its\n    //   (usedResources + required) (which extra resources we are allocating)\n\n    // Allow progress for queues with miniscule capacity\n    final int queueCapacity = \n      Math.max(\n          roundUp((int)(absoluteCapacity * clusterResource.getMemory())), \n          required.getMemory());\n\n    final int consumed = usedResources.getMemory();\n    final int currentCapacity = \n      (consumed < queueCapacity) ? \n          queueCapacity : (consumed + required.getMemory());\n\n    // Never allow a single user to take more than the \n    // queue's configured capacity * user-limit-factor.\n    // Also, the queue's configured capacity should be higher than \n    // queue-hard-limit * ulMin\n\n    String userName = application.getUser();\n    \n    final int activeUsers = users.size();  \n    User user = getUser(userName);\n\n    int limit = \n      roundUp(\n          Math.min(\n              Math.max(divideAndCeil(currentCapacity, activeUsers), \n                       divideAndCeil((int)userLimit*currentCapacity, 100)),\n              (int)(queueCapacity * userLimitFactor)\n              )\n          );\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"User limit computation for \" + userName + \n          \" in queue \" + getQueueName() +\n          \" userLimit=\" + userLimit +\n          \" userLimitFactor=\" + userLimitFactor +\n          \" required: \" + required + \n          \" consumed: \" + user.getConsumedResources() + \n          \" limit: \" + limit +\n          \" queueCapacity: \" + queueCapacity + \n          \" qconsumed: \" + consumed +\n          \" currentCapacity: \" + currentCapacity +\n          \" activeUsers: \" + activeUsers +\n          \" clusterCapacity: \" + clusterResource.getMemory()\n      );\n    }\n\n    return Resources.createResource(limit);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.allocateResource": "  synchronized void allocateResource(Resource clusterResource, \n      SchedulerApp application, Resource resource) {\n    // Update queue metrics\n    Resources.addTo(usedResources, resource);\n    updateResource(clusterResource);\n    ++numContainers;\n\n    // Update user metrics\n    String userName = application.getUser();\n    User user = getUser(userName);\n    user.assignContainer(resource);\n    metrics.setAvailableResourcesToUser(userName, application.getHeadroom());\n    LOG.info(getQueueName() + \n        \" used=\" + usedResources + \" numContainers=\" + numContainers + \n        \" user=\" + userName + \" resources=\" + user.getConsumedResources());\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignReservedContainer": "  private synchronized Resource assignReservedContainer(SchedulerApp application, \n      SchedulerNode node, RMContainer rmContainer, Resource clusterResource) {\n    // Do we still need this reservation?\n    Priority priority = rmContainer.getReservedPriority();\n    if (application.getTotalRequiredResources(priority) == 0) {\n      // Release\n      Container container = rmContainer.getContainer();\n      completedContainer(clusterResource, application, node, \n          rmContainer, \n          SchedulerUtils.createAbnormalContainerStatus(\n              container.getId(), \n              SchedulerUtils.UNRESERVED_CONTAINER), \n          RMContainerEventType.RELEASED);\n      return container.getResource();\n    }\n\n    // Try to assign if we have sufficient resources\n    assignContainersOnNode(clusterResource, node, application, priority, rmContainer);\n    \n    // Doesn't matter... since it's already charged for at time of reservation\n    // \"re-reservation\" is *free*\n    return org.apache.hadoop.yarn.server.resourcemanager.resource.Resource.NONE;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues": "  synchronized Resource assignContainersToChildQueues(Resource cluster, \n      SchedulerNode node) {\n    Resource assigned = Resources.createResource(0);\n    \n    printChildQueues();\n\n    // Try to assign to most 'under-served' sub-queue\n    for (Iterator<CSQueue> iter=childQueues.iterator(); iter.hasNext();) {\n      CSQueue childQueue = iter.next();\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"Trying to assign to queue: \" + childQueue.getQueuePath()\n          + \" stats: \" + childQueue);\n      }\n      assigned = childQueue.assignContainers(cluster, node);\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"Assignedto queue: \" + childQueue.getQueuePath()\n          + \" stats: \" + childQueue + \" --> \" + assigned.getMemory());\n      }\n\n      // If we do assign, remove the queue and re-insert in-order to re-sort\n      if (Resources.greaterThan(assigned, Resources.none())) {\n        // Remove and re-insert to sort\n        iter.remove();\n        LOG.info(\"Re-sorting queues since queue: \" + childQueue.getQueuePath() + \n            \" stats: \" + childQueue);\n        childQueues.add(childQueue);\n        printChildQueues();\n        break;\n      }\n    }\n    \n    return assigned;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers": "  public synchronized Resource assignContainers(\n      Resource clusterResource, SchedulerNode node) {\n    Resource assigned = Resources.createResource(0);\n\n    while (canAssign(node)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Trying to assign containers to child-queue of \"\n          + getQueueName());\n      }\n      \n      // Are we over maximum-capacity for this queue?\n      if (!assignToQueue(clusterResource)) {\n        break;\n      }\n      \n      // Schedule\n      Resource assignedToChild = \n          assignContainersToChildQueues(clusterResource, node);\n      \n      // Done if no child-queue assigned anything\n      if (Resources.greaterThan(assignedToChild, Resources.none())) {\n        // Track resource utilization for the parent-queue\n        allocateResource(clusterResource, assignedToChild);\n        \n        // Track resource utilization in this pass of the scheduler\n        Resources.addTo(assigned, assignedToChild);\n        \n        LOG.info(\"assignedContainer\" +\n            \" queue=\" + getQueueName() + \n            \" util=\" + getUtilization() + \n            \" used=\" + usedResources + \n            \" cluster=\" + clusterResource);\n\n      } else {\n        break;\n      }\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"ParentQ=\" + getQueueName()\n          + \" assignedSoFarInThisIteration=\" + assigned\n          + \" utilization=\" + getUtilization());\n      }\n\n      // Do not assign more than one container if this isn't the root queue\n      if (!rootQueue) {\n        break;\n      }\n    } \n    \n    return assigned;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.printChildQueues": "  void printChildQueues() {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"printChildQueues - queue: \" + getQueuePath()\n        + \" child-queues: \" + getChildQueuesToPrint());\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.getQueuePath": "  public String getQueuePath() {\n    String parentPath = ((parent == null) ? \"\" : (parent.getQueuePath() + \".\"));\n    return parentPath + getQueueName();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate": "  private synchronized void nodeUpdate(RMNode nm, \n      List<ContainerStatus> newlyLaunchedContainers,\n      List<ContainerStatus> completedContainers) {\n    LOG.info(\"nodeUpdate: \" + nm + \" clusterResources: \" + clusterResource);\n    \n    SchedulerNode node = getNode(nm.getNodeID());\n\n    // Processing the newly launched containers\n    for (ContainerStatus launchedContainer : newlyLaunchedContainers) {\n      containerLaunchedOnNode(launchedContainer.getContainerId(), node);\n    }\n\n    // Process completed containers\n    for (ContainerStatus completedContainer : completedContainers) {\n      ContainerId containerId = completedContainer.getContainerId();\n      LOG.debug(\"Container FINISHED: \" + containerId);\n      completedContainer(getRMContainer(containerId), \n          completedContainer, RMContainerEventType.FINISHED);\n    }\n\n    // Now node data structures are upto date and ready for scheduling.\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Node being looked for scheduling \" + nm\n        + \" availableResource: \" + node.getAvailableResource());\n    }\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer = node.getReservedContainer();\n    if (reservedContainer != null) {\n      SchedulerApp reservedApplication = \n          getApplication(reservedContainer.getApplicationAttemptId());\n      \n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \" + \n          reservedApplication.getApplicationId() + \" on node: \" + nm);\n      \n      LeafQueue queue = ((LeafQueue)reservedApplication.getQueue());\n      queue.assignContainers(clusterResource, node);\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() == null) {\n      root.assignContainers(clusterResource, node);\n    } else {\n      LOG.info(\"Skipping scheduling since node \" + nm + \n          \" is reserved by application \" + \n          node.getReservedContainer().getContainerId().getApplicationAttemptId()\n          );\n    }\n\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.containerLaunchedOnNode": "  private void containerLaunchedOnNode(ContainerId containerId, SchedulerNode node) {\n    // Get the application for the finished container\n    ApplicationAttemptId applicationAttemptId = containerId.getApplicationAttemptId();\n    SchedulerApp application = getApplication(applicationAttemptId);\n    if (application == null) {\n      LOG.info(\"Unknown application: \" + applicationAttemptId + \n          \" launched container \" + containerId +\n          \" on node: \" + node);\n      return;\n    }\n    \n    application.containerLaunchedOnNode(containerId);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getRMContainer": "  private RMContainer getRMContainer(ContainerId containerId) {\n    SchedulerApp application = \n        getApplication(containerId.getApplicationAttemptId());\n    return (application == null) ? null : application.getRMContainer(containerId);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getQueue": "  synchronized CSQueue getQueue(String queueName) {\n    return queues.get(queueName);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getNode": "  SchedulerNode getNode(NodeId nodeId) {\n    return nodes.get(nodeId);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getApplication": "  SchedulerApp getApplication(ApplicationAttemptId applicationAttemptId) {\n    return applications.get(applicationAttemptId);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainer": "  private synchronized void completedContainer(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) {\n    if (rmContainer == null) {\n      LOG.info(\"Null container completed...\");\n      return;\n    }\n    \n    Container container = rmContainer.getContainer();\n    \n    // Get the application for the finished container\n    ApplicationAttemptId applicationAttemptId = container.getId().getApplicationAttemptId();\n    SchedulerApp application = getApplication(applicationAttemptId);\n    if (application == null) {\n      LOG.info(\"Container \" + container + \" of\" +\n      \t\t\" unknown application \" + applicationAttemptId + \n          \" completed with event \" + event);\n      return;\n    }\n    \n    // Get the node on which the container was allocated\n    SchedulerNode node = getNode(container.getNodeId());\n    \n    // Inform the queue\n    LeafQueue queue = (LeafQueue)application.getQueue();\n    queue.completedContainer(clusterResource, application, node, \n        rmContainer, containerStatus, event);\n\n    LOG.info(\"Application \" + applicationAttemptId + \n        \" released container \" + container.getId() +\n        \" on node: \" + node + \n        \" with event: \" + event);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle": "  public void handle(SchedulerEvent event) {\n    switch(event.getType()) {\n    case NODE_ADDED:\n    {\n      NodeAddedSchedulerEvent nodeAddedEvent = (NodeAddedSchedulerEvent)event;\n      addNode(nodeAddedEvent.getAddedRMNode());\n    }\n    break;\n    case NODE_REMOVED:\n    {\n      NodeRemovedSchedulerEvent nodeRemovedEvent = (NodeRemovedSchedulerEvent)event;\n      removeNode(nodeRemovedEvent.getRemovedRMNode());\n    }\n    break;\n    case NODE_UPDATE:\n    {\n      NodeUpdateSchedulerEvent nodeUpdatedEvent = (NodeUpdateSchedulerEvent)event;\n      nodeUpdate(nodeUpdatedEvent.getRMNode(), \n          nodeUpdatedEvent.getNewlyLaunchedContainers(),\n          nodeUpdatedEvent.getCompletedContainers());\n    }\n    break;\n    case APP_ADDED:\n    {\n      AppAddedSchedulerEvent appAddedEvent = (AppAddedSchedulerEvent)event;\n      addApplication(appAddedEvent.getApplicationAttemptId(), appAddedEvent\n          .getQueue(), appAddedEvent.getUser());\n    }\n    break;\n    case APP_REMOVED:\n    {\n      AppRemovedSchedulerEvent appRemovedEvent = (AppRemovedSchedulerEvent)event;\n      doneApplication(appRemovedEvent.getApplicationAttemptID(),\n          appRemovedEvent.getFinalAttemptState());\n    }\n    break;\n    case CONTAINER_EXPIRED:\n    {\n      ContainerExpiredSchedulerEvent containerExpiredEvent = \n          (ContainerExpiredSchedulerEvent) event;\n      ContainerId containerId = containerExpiredEvent.getContainerId();\n      completedContainer(getRMContainer(containerId), \n          SchedulerUtils.createAbnormalContainerStatus(\n              containerId, \n              SchedulerUtils.EXPIRED_CONTAINER), \n          RMContainerEventType.EXPIRE);\n    }\n    break;\n    default:\n      LOG.error(\"Invalid eventtype \" + event.getType() + \". Ignoring!\");\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplication": "  private synchronized void\n      addApplication(ApplicationAttemptId applicationAttemptId,\n          String queueName, String user) {\n\n    // Sanity checks\n    CSQueue queue = getQueue(queueName);\n    if (queue == null) {\n      String message = \"Application \" + applicationAttemptId + \n      \" submitted by user \" + user + \" to unknown queue: \" + queueName;\n      this.rmContext.getDispatcher().getEventHandler().handle(\n          new RMAppAttemptRejectedEvent(applicationAttemptId, message));\n      return;\n    }\n    if (!(queue instanceof LeafQueue)) {\n      String message = \"Application \" + applicationAttemptId + \n          \" submitted by user \" + user + \" to non-leaf queue: \" + queueName;\n      this.rmContext.getDispatcher().getEventHandler().handle(\n          new RMAppAttemptRejectedEvent(applicationAttemptId, message));\n      return;\n    }\n\n    // TODO: Fix store\n    SchedulerApp SchedulerApp = \n        new SchedulerApp(applicationAttemptId, user, queue, rmContext, null);\n\n    // Submit to the queue\n    try {\n      queue.submitApplication(SchedulerApp, user, queueName);\n    } catch (AccessControlException ace) {\n      LOG.info(\"Failed to submit application \" + applicationAttemptId + \n          \" to queue \" + queueName + \" from user \" + user, ace);\n      this.rmContext.getDispatcher().getEventHandler().handle(\n          new RMAppAttemptRejectedEvent(applicationAttemptId, \n              ace.toString()));\n      return;\n    }\n\n    applications.put(applicationAttemptId, SchedulerApp);\n\n    LOG.info(\"Application Submission: \" + applicationAttemptId + \n        \", user: \" + user +\n        \" queue: \" + queue +\n        \", currently active: \" + applications.size());\n\n    rmContext.getDispatcher().getEventHandler().handle(\n        new RMAppAttemptEvent(applicationAttemptId,\n            RMAppAttemptEventType.APP_ACCEPTED));\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addNode": "  private synchronized void addNode(RMNode nodeManager) {\n    this.nodes.put(nodeManager.getNodeID(), new SchedulerNode(nodeManager));\n    Resources.addTo(clusterResource, nodeManager.getTotalCapability());\n    root.updateClusterResource(clusterResource);\n    ++numNodeManagers;\n    LOG.info(\"Added node \" + nodeManager.getNodeAddress() + \n        \" clusterResource: \" + clusterResource);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.doneApplication": "  private synchronized void doneApplication(\n      ApplicationAttemptId applicationAttemptId,\n      RMAppAttemptState rmAppAttemptFinalState) {\n    LOG.info(\"Application \" + applicationAttemptId + \" is done.\" +\n    \t\t\" finalState=\" + rmAppAttemptFinalState);\n    \n    SchedulerApp application = getApplication(applicationAttemptId);\n\n    if (application == null) {\n      //      throw new IOException(\"Unknown application \" + applicationId + \n      //          \" has completed!\");\n      LOG.info(\"Unknown application \" + applicationAttemptId + \" has completed!\");\n      return;\n    }\n    \n    // Release all the running containers \n    for (RMContainer rmContainer : application.getLiveContainers()) {\n      completedContainer(rmContainer, \n          SchedulerUtils.createAbnormalContainerStatus(\n              rmContainer.getContainerId(), \n              SchedulerUtils.COMPLETED_APPLICATION), \n          RMContainerEventType.KILL);\n    }\n    \n     // Release all reserved containers\n    for (RMContainer rmContainer : application.getReservedContainers()) {\n      completedContainer(rmContainer, \n          SchedulerUtils.createAbnormalContainerStatus(\n              rmContainer.getContainerId(), \n              \"Application Complete\"), \n          RMContainerEventType.KILL);\n    }\n    \n    // Clean up pending requests, metrics etc.\n    application.stop(rmAppAttemptFinalState);\n    \n    // Inform the queue\n    String queueName = application.getQueue().getQueueName();\n    CSQueue queue = queues.get(queueName);\n    if (!(queue instanceof LeafQueue)) {\n      LOG.error(\"Cannot finish application \" + \"from non-leaf queue: \"\n          + queueName);\n    } else {\n      queue.finishApplication(application, queue.getQueueName());\n    }\n    \n    // Remove from our data-structure\n    applications.remove(applicationAttemptId);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.removeNode": "  private synchronized void removeNode(RMNode nodeInfo) {\n    SchedulerNode node = this.nodes.get(nodeInfo.getNodeID());\n    Resources.subtractFrom(clusterResource, nodeInfo.getTotalCapability());\n    root.updateClusterResource(clusterResource);\n    --numNodeManagers;\n\n    // Remove running containers\n    List<RMContainer> runningContainers = node.getRunningContainers();\n    for (RMContainer container : runningContainers) {\n      completedContainer(container, \n          SchedulerUtils.createAbnormalContainerStatus(\n              container.getContainerId(), \n              SchedulerUtils.LOST_CONTAINER), \n          RMContainerEventType.KILL);\n    }\n    \n    // Remove reservations, if any\n    RMContainer reservedContainer = node.getReservedContainer();\n    if (reservedContainer != null) {\n      completedContainer(reservedContainer, \n          SchedulerUtils.createAbnormalContainerStatus(\n              reservedContainer.getContainerId(), \n              SchedulerUtils.LOST_CONTAINER), \n          RMContainerEventType.KILL);\n    }\n\n    this.nodes.remove(nodeInfo.getNodeID());\n    LOG.info(\"Removed node \" + nodeInfo.getNodeAddress() + \n        \" clusterResource: \" + clusterResource);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.run": "      public void run() {\n\n        SchedulerEvent event;\n\n        while (!Thread.currentThread().isInterrupted()) {\n          try {\n            event = eventQueue.take();\n          } catch (InterruptedException e) {\n            LOG.error(\"Returning, interrupted : \" + e);\n            return; // TODO: Kill RM.\n          }\n\n          try {\n            scheduler.handle(event);\n          } catch (Throwable t) {\n            LOG.error(\"Error in handling event type \" + event.getType()\n                + \" to the scheduler\", t);\n            return; // TODO: Kill RM.\n          }\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.handle": "    public void handle(RMNodeEvent event) {\n      NodeId nodeId = event.getNodeId();\n      RMNode node = this.rmContext.getRMNodes().get(nodeId);\n      if (node != null) {\n        try {\n          ((EventHandler<RMNodeEvent>) node).handle(event);\n        } catch (Throwable t) {\n          LOG.error(\"Error in handling event type \" + event.getType()\n              + \" for node \" + nodeId, t);\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue.assignContainers": "  public Resource assignContainers(Resource clusterResource, SchedulerNode node);\n  \n  /**\n   * A container assigned to the queue has completed.\n   * @param clusterResource the resource of the cluster\n   * @param application application to which the container was assigned\n   * @param node node on which the container completed\n   * @param container completed container, \n   *                  <code>null</code> if it was just a reservation\n   * @param containerStatus <code>ContainerStatus</code> for the completed \n   *                        container\n   * @param event event to be sent to the container\n   */\n  public void completedContainer(Resource clusterResource,\n      SchedulerApp application, SchedulerNode node, \n      RMContainer container, ContainerStatus containerStatus, \n      RMContainerEventType event);\n\n  /**\n   * Get the number of applications in the queue.\n   * @return number of applications\n   */\n  public int getNumApplications();\n\n  \n  /**\n   * Reinitialize the queue.\n   * @param queue new queue to re-initalize from\n   * @param clusterResource resources in the cluster\n   */\n  public void reinitialize(CSQueue queue, Resource clusterResource) \n  throws IOException;\n\n   /**\n   * Update the cluster resource for queues as we add/remove nodes\n   * @param clusterResource the current cluster resource\n   */\n  public void updateClusterResource(Resource clusterResource);\n  \n  /**\n   * Recover the state of the queue\n   * @param clusterResource the resource of the cluster\n   * @param application the application for which the container was allocated\n   * @param container the container that was recovered.\n   */\n  public void recoverContainer(Resource clusterResource, SchedulerApp application, \n      Container container);\n}"
        },
        "bug_report": {
            "Title": "Sometimes java.lang.IllegalArgumentException: Invalid key to HMAC computation in NODE_UPDATE also causing RM to stop scheduling ",
            "Description": "Filling this Jira a bit late\nStarted 350 cluster\nsbummited large sleep job.\nFoud that job was not running as RM has not allocated resouces to it.\n{code}\n2011-12-01 11:56:25,200 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: nodeUpdate: <NMHost>:48490 clusterResources: memory: 3225600\n2011-12-01 11:56:25,202 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event\ntype NODE_UPDATE to the scheduler\njava.lang.IllegalArgumentException: Invalid key to HMAC computation\n        at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:141)\n        at org.apache.hadoop.yarn.server.security.ContainerTokenSecretManager.createPassword(ContainerTokenSecretManager.java:61)\n        atorg.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.createContainer(LeafQueue.java:1108)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getContainer(LeafQueue.java:1091)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1137)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1001)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:973)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:760)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:583)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:513)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:569)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:611)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:77)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:294)\n        at java.lang.Thread.run(Thread.java:619)\nCaused by: java.security.InvalidKeyException: Secret key expected\n        at com.sun.crypto.provider.HmacCore.a(DashoA13*..)\n        at com.sun.crypto.provider.HmacSHA1.engineInit(DashoA13*..)\n        at javax.crypto.Mac.init(DashoA13*..)\n        at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:139)\n        ... 14 more\n{code}\nAs this stack is from 30 Nov checkou line number may be different"
        }
    },
    {
        "filename": "MAPREDUCE-3931.json",
        "creation_time": "2012-02-27T22:30:59.000+0000",
        "stack_trace": "```\njava.io.IOException: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875\n       at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:90)\n       at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:49)\n       at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:157)\n       at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:155)\n       at java.security.AccessController.doPrivileged(Native Method)\n       at javax.security.auth.Subject.doAs(Subject.java:396)\n       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)\n       at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:153)\n       at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:49)\n       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n       at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n       at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n       at java.lang.Thread.run(Thread.java:619)\norg.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875\n       at org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl.convertFromProtoFormat(LocalResourceStatusPBImpl.java:217)\n       at org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl.getException(LocalResourceStatusPBImpl.java:147)\n       at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.update(ResourceLocalizationService.java:827)\n       at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker.processHeartbeat(ResourceLocalizationService.java:497)\n       at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.heartbeat(ResourceLocalizationService.java:222)\n       at org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.service.LocalizationProtocolPBServiceImpl.heartbeat(LocalizationProtocolPBServiceImpl.java:46)\n       at org.apache.hadoop.yarn.proto.LocalizationProtocol$LocalizationProtocolService$2.callBlockingMethod(LocalizationProtocol.java:57)\n       at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:342)\n       at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1493)\n       at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n       at java.security.AccessController.doPrivileged(Native Method)\n       at javax.security.auth.Subject.doAs(Subject.java:396)\n       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)\n       at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1487)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.util.FSDownload.copy": "  private Path copy(Path sCopy, Path dstdir) throws IOException {\n    FileSystem sourceFs = sCopy.getFileSystem(conf);\n    Path dCopy = new Path(dstdir, sCopy.getName() + \".tmp\");\n    FileStatus sStat = sourceFs.getFileStatus(sCopy);\n    if (sStat.getModificationTime() != resource.getTimestamp()) {\n      throw new IOException(\"Resource \" + sCopy +\n          \" changed on src filesystem (expected \" + resource.getTimestamp() +\n          \", was \" + sStat.getModificationTime());\n    }\n\n    sourceFs.copyToLocalFile(sCopy, dCopy);\n    return dCopy;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.util.FSDownload.run": "        public Void run() throws Exception {\n          files.setPermission(path, fPerm);\n          return null;\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      LOG.error(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(pae,\"Unknown exception in doAs\");\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.util.FSDownload.call": "  public Path call() throws Exception {\n    final Path sCopy;\n    try {\n      sCopy = ConverterUtils.getPathFromYarnURL(resource.getResource());\n    } catch (URISyntaxException e) {\n      throw new IOException(\"Invalid resource\", e);\n    }\n\n    Path tmp;\n    do {\n      tmp = new Path(destDirPath, String.valueOf(rand.nextLong()));\n    } while (files.util().exists(tmp));\n    destDirPath = tmp;\n\n    files.mkdir(destDirPath, cachePerms, false);\n    final Path dst_work = new Path(destDirPath + \"_tmp\");\n    files.mkdir(dst_work, cachePerms, false);\n\n    Path dFinal = files.makeQualified(new Path(dst_work, sCopy.getName()));\n    try {\n      Path dTmp = null == userUgi\n        ? files.makeQualified(copy(sCopy, dst_work))\n        : userUgi.doAs(new PrivilegedExceptionAction<Path>() {\n            public Path run() throws Exception {\n              return files.makeQualified(copy(sCopy, dst_work));\n            };\n      });\n      unpack(new File(dTmp.toUri()), new File(dFinal.toUri()));\n      changePermissions(dFinal.getFileSystem(conf), dFinal);\n      files.rename(dst_work, destDirPath, Rename.OVERWRITE);\n    } catch (Exception e) {\n      try { files.delete(destDirPath, true); } catch (IOException ignore) { }\n      throw e;\n    } finally {\n      try {\n        files.delete(dst_work, true);\n      } catch (FileNotFoundException ignore) { }\n      // clear ref to internal var\n      rand = null;\n      conf = null;\n      resource = null;\n    }\n    return files.makeQualified(new Path(destDirPath, sCopy.getName()));\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.util.FSDownload.changePermissions": "  private void changePermissions(FileSystem fs, final Path path)\n      throws IOException, InterruptedException {\n    FileStatus fStatus = fs.getFileStatus(path);\n    FsPermission perm = cachePerms;\n    // set public perms as 755 or 555 based on dir or file\n    if (resource.getVisibility() == LocalResourceVisibility.PUBLIC) {\n      perm = fStatus.isDirectory() ? PUBLIC_DIR_PERMS : PUBLIC_FILE_PERMS;\n    }\n    // set private perms as 700 or 500\n    else {\n      // PRIVATE:\n      // APPLICATION:\n      perm = fStatus.isDirectory() ? PRIVATE_DIR_PERMS : PRIVATE_FILE_PERMS;\n    }\n    LOG.debug(\"Changing permissions for path \" + path\n        + \" to perm \" + perm);\n    final FsPermission fPerm = perm;\n    if (null == userUgi) {\n      files.setPermission(path, perm);\n    }\n    else {\n      userUgi.doAs(new PrivilegedExceptionAction<Void>() {\n        public Void run() throws Exception {\n          files.setPermission(path, fPerm);\n          return null;\n        }\n      });\n    }\n    if (fStatus.isDirectory()\n        && !fStatus.isSymlink()) {\n      FileStatus[] statuses = fs.listStatus(path);\n      for (FileStatus status : statuses) {\n        changePermissions(fs, status.getPath());\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.util.FSDownload.getResource": "  LocalResource getResource() {\n    return resource;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.util.FSDownload.unpack": "  private long unpack(File localrsrc, File dst) throws IOException {\n    switch (resource.getType()) {\n    case ARCHIVE:\n      String lowerDst = dst.getName().toLowerCase();\n      if (lowerDst.endsWith(\".jar\")) {\n        RunJar.unJar(localrsrc, dst);\n      } else if (lowerDst.endsWith(\".zip\")) {\n        FileUtil.unZip(localrsrc, dst);\n      } else if (lowerDst.endsWith(\".tar.gz\") ||\n                 lowerDst.endsWith(\".tgz\") ||\n                 lowerDst.endsWith(\".tar\")) {\n        FileUtil.unTar(localrsrc, dst);\n      } else {\n        LOG.warn(\"Cannot unpack \" + localrsrc);\n        if (!localrsrc.renameTo(dst)) {\n            throw new IOException(\"Unable to rename file: [\" + localrsrc\n              + \"] to [\" + dst + \"]\");\n        }\n      }\n      break;\n    case FILE:\n    default:\n      if (!localrsrc.renameTo(dst)) {\n        throw new IOException(\"Unable to rename file: [\" + localrsrc\n          + \"] to [\" + dst + \"]\");\n      }\n      break;\n    }\n    return 0;\n    // TODO Should calculate here before returning\n    //return FileUtil.getDU(destDir);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl.convertFromProtoFormat": "  private YarnRemoteExceptionPBImpl convertFromProtoFormat(YarnRemoteExceptionProto p) {\n    return new YarnRemoteExceptionPBImpl(p);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl.getException": "  public YarnRemoteException getException() {\n    LocalResourceStatusProtoOrBuilder p = viaProto ? proto : builder;\n    if (this.exception != null) {\n      return this.exception;\n    }\n    if (!p.hasException()) {\n      return null;\n    }\n    this.exception = convertFromProtoFormat(p.getException());\n    return this.exception;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.update": "    LocalizerHeartbeatResponse update(\n        List<LocalResourceStatus> remoteResourceStatuses) {\n      LocalizerHeartbeatResponse response =\n        recordFactory.newRecordInstance(LocalizerHeartbeatResponse.class);\n\n      // The localizer has just spawned. Start giving it resources for\n      // remote-fetching.\n      if (remoteResourceStatuses.isEmpty()) {\n        LocalResource next = findNextResource();\n        if (next != null) {\n          response.setLocalizerAction(LocalizerAction.LIVE);\n          response.addResource(next);\n        } else if (pending.isEmpty()) {\n          // TODO: Synchronization\n          response.setLocalizerAction(LocalizerAction.DIE);\n        } else {\n          response.setLocalizerAction(LocalizerAction.LIVE);\n        }\n        return response;\n      }\n\n      for (LocalResourceStatus stat : remoteResourceStatuses) {\n        LocalResource rsrc = stat.getResource();\n        LocalResourceRequest req = null;\n        try {\n          req = new LocalResourceRequest(rsrc);\n        } catch (URISyntaxException e) {\n          // TODO fail? Already translated several times...\n        }\n        LocalizerResourceRequestEvent assoc = scheduled.get(req);\n        if (assoc == null) {\n          // internal error\n          LOG.error(\"Unknown resource reported: \" + req);\n          continue;\n        }\n        switch (stat.getStatus()) {\n          case FETCH_SUCCESS:\n            // notify resource\n            try {\n              assoc.getResource().handle(\n                  new ResourceLocalizedEvent(req,\n                    ConverterUtils.getPathFromYarnURL(stat.getLocalPath()),\n                    stat.getLocalSize()));\n            } catch (URISyntaxException e) { }\n            if (pending.isEmpty()) {\n              // TODO: Synchronization\n              response.setLocalizerAction(LocalizerAction.DIE);\n              break;\n            }\n            response.setLocalizerAction(LocalizerAction.LIVE);\n            LocalResource next = findNextResource();\n            if (next != null) {\n              response.addResource(next);\n            }\n            break;\n          case FETCH_PENDING:\n            response.setLocalizerAction(LocalizerAction.LIVE);\n            break;\n          case FETCH_FAILURE:\n            LOG.info(\"DEBUG: FAILED \" + req, stat.getException());\n            assoc.getResource().unlock();\n            response.setLocalizerAction(LocalizerAction.DIE);\n            // TODO: Why is this event going directly to the container. Why not\n            // the resource itself? What happens to the resource? Is it removed?\n            dispatcher.getEventHandler().handle(\n                new ContainerResourceFailedEvent(context.getContainerId(),\n                  req, stat.getException()));\n            break;\n          default:\n            LOG.info(\"Unknown status: \" + stat.getStatus());\n            response.setLocalizerAction(LocalizerAction.DIE);\n            dispatcher.getEventHandler().handle(\n                new ContainerResourceFailedEvent(context.getContainerId(),\n                  req, stat.getException()));\n            break;\n        }\n      }\n      return response;\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.findNextResource": "    private LocalResource findNextResource() {\n      // TODO: Synchronization\n      for (Iterator<LocalizerResourceRequestEvent> i = pending.iterator();\n           i.hasNext();) {\n        LocalizerResourceRequestEvent evt = i.next();\n        LocalizedResource nRsrc = evt.getResource();\n        if (ResourceState.LOCALIZED.equals(nRsrc.getState())) {\n          i.remove();\n          continue;\n        }\n        if (nRsrc.tryAcquire()) {\n          LocalResourceRequest nextRsrc = nRsrc.getRequest();\n          LocalResource next =\n            recordFactory.newRecordInstance(LocalResource.class);\n          next.setResource(\n              ConverterUtils.getYarnUrlFromPath(nextRsrc.getPath()));\n          next.setTimestamp(nextRsrc.getTimestamp());\n          next.setType(nextRsrc.getType());\n          next.setVisibility(evt.getVisibility());\n          scheduled.put(nextRsrc, evt);\n          return next;\n        }\n      }\n      return null;\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.addResource": "    public void addResource(LocalizerResourceRequestEvent request) {\n      // TDOO: Synchronization\n      pending.add(request);\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.handle": "    public void handle(LocalizerEvent event) {\n      String locId = event.getLocalizerId();\n      switch (event.getType()) {\n      case REQUEST_RESOURCE_LOCALIZATION:\n        // 0) find running localizer or start new thread\n        LocalizerResourceRequestEvent req =\n          (LocalizerResourceRequestEvent)event;\n        switch (req.getVisibility()) {\n        case PUBLIC:\n          publicLocalizer.addResource(req);\n          break;\n        case PRIVATE:\n        case APPLICATION:\n          synchronized (privLocalizers) {\n            LocalizerRunner localizer = privLocalizers.get(locId);\n            if (null == localizer) {\n              LOG.info(\"Created localizer for \" + req.getLocalizerId());\n              localizer = new LocalizerRunner(req.getContext(),\n                  req.getLocalizerId());\n              privLocalizers.put(locId, localizer);\n              localizer.start();\n            }\n            // 1) propagate event\n            localizer.addResource(req);\n          }\n          break;\n        }\n        break;\n      case ABORT_LOCALIZATION:\n        // 0) find running localizer, interrupt and remove\n        synchronized (privLocalizers) {\n          LocalizerRunner localizer = privLocalizers.get(locId);\n          if (null == localizer) {\n            return; // ignore; already gone\n          }\n          privLocalizers.remove(locId);\n          localizer.interrupt();\n        }\n        break;\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.processHeartbeat": "    public LocalizerHeartbeatResponse processHeartbeat(LocalizerStatus status) {\n      String locId = status.getLocalizerId();\n      synchronized (privLocalizers) {\n        LocalizerRunner localizer = privLocalizers.get(locId);\n        if (null == localizer) {\n          // TODO process resources anyway\n          LOG.info(\"Unknown localizer with localizerId \" + locId\n              + \" is sending heartbeat. Ordering it to DIE\");\n          LocalizerHeartbeatResponse response =\n            recordFactory.newRecordInstance(LocalizerHeartbeatResponse.class);\n          response.setLocalizerAction(LocalizerAction.DIE);\n          return response;\n        }\n        return localizer.update(status.getResources());\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.heartbeat": "  public LocalizerHeartbeatResponse heartbeat(LocalizerStatus status) {\n    return localizerTracker.processHeartbeat(status);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.service.LocalizationProtocolPBServiceImpl.heartbeat": "  public LocalizerHeartbeatResponseProto heartbeat(RpcController controller,\n      LocalizerStatusProto proto) throws ServiceException {\n    LocalizerStatusPBImpl request = new LocalizerStatusPBImpl(proto);\n    try {\n      LocalizerHeartbeatResponse response = real.heartbeat(request);\n      return ((LocalizerHeartbeatResponsePBImpl)response).getProto();\n    } catch (YarnRemoteException e) {\n      throw new ServiceException(e);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.call": "    public Writable call(RpcKind rpcKind, String protocol, \n        Writable writableRequest, long receiveTime) throws IOException {\n      ProtoSpecificRequestWritable request = (ProtoSpecificRequestWritable) writableRequest;\n      ProtoSpecificRpcRequest rpcRequest = request.message;\n      String methodName = rpcRequest.getMethodName();\n      if (verbose) {\n        log(\"Call: protocol=\" + protocol + \", method=\"\n            + methodName);\n      }\n      MethodDescriptor methodDescriptor = service.getDescriptorForType()\n          .findMethodByName(methodName);\n      if (methodDescriptor == null) {\n        String msg = \"Unknown method \" + methodName + \" called on \"\n            + protocol + \" protocol.\";\n        LOG.warn(msg);\n        return handleException(new IOException(msg));\n      }\n      Message prototype = service.getRequestPrototype(methodDescriptor);\n      Message param = prototype.newBuilderForType()\n          .mergeFrom(rpcRequest.getRequestProto()).build();\n      Message result;\n      try {\n        result = service.callBlockingMethod(methodDescriptor, null, param);\n      } catch (ServiceException e) {\n        e.printStackTrace();\n        return handleException(e);\n      } catch (Exception e) {\n        return handleException(e);\n      }\n\n      ProtoSpecificRpcResponse response = constructProtoSpecificRpcSuccessResponse(result);\n      return new ProtoSpecificResponseWritable(response);\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.handleException": "    private ProtoSpecificResponseWritable handleException(Throwable e) {\n      ProtoSpecificRpcResponse.Builder builder = ProtoSpecificRpcResponse\n          .newBuilder();\n      builder.setIsError(true);\n      if (e.getCause() instanceof YarnRemoteExceptionPBImpl) {\n        builder.setException(((YarnRemoteExceptionPBImpl) e.getCause())\n            .getProto());\n      } else {\n        builder.setException(new YarnRemoteExceptionPBImpl(e).getProto());\n      }\n      ProtoSpecificRpcResponse response = builder.build();\n      return new ProtoSpecificResponseWritable(response);\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.constructProtoSpecificRpcSuccessResponse": "    private ProtoSpecificRpcResponse constructProtoSpecificRpcSuccessResponse(\n        Message message) {\n      ProtoSpecificRpcResponse res = ProtoSpecificRpcResponse.newBuilder()\n          .setResponseProto(message.toByteString()).build();\n      return res;\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.log": "  private static void log(String value) {\n    if (value != null && value.length() > 55)\n      value = value.substring(0, 55) + \"...\";\n    LOG.info(value);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "                     public Writable run() throws Exception {\n                       // make the call\n                       return call(call.rpcKind, call.connection.protocolName, \n                                   call.rpcRequest, call.timestamp);\n\n                     }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRespond": "    void doRespond(Call call) throws IOException {\n      synchronized (call.connection.responseQueue) {\n        call.connection.responseQueue.addLast(call);\n        if (call.connection.responseQueue.size() == 1) {\n          processResponse(call.connection.responseQueue, true);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeConnection": "  private void closeConnection(Connection connection) {\n    synchronized (connectionList) {\n      if (connectionList.remove(connection))\n        numConnections--;\n    }\n    try {\n      connection.close();\n    } catch (IOException e) {\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.toString": "    public String toString() {\n      return getHostAddress() + \":\" + remotePort; \n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    private synchronized void close() throws IOException {\n      disposeSasl();\n      data = null;\n      dataLengthBuffer = null;\n      if (!channel.isOpen())\n        return;\n      try {socket.shutdownOutput();} catch(Exception e) {\n        LOG.warn(\"Ignoring socket shutdown exception\");\n      }\n      if (channel.isOpen()) {\n        try {channel.close();} catch(Exception e) {}\n      }\n      try {socket.close();} catch(Exception e) {}\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          if (LOG.isDebugEnabled())\n            LOG.debug(getName() + \": disconnecting client \" + c.getHostAddress());\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws IOException,  OutOfMemoryError {\n      Connection c = null;\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        \n        Reader reader = getReader();\n        try {\n          reader.startAdd();\n          SelectionKey readKey = reader.registerChannel(channel);\n          c = new Connection(readKey, channel, System.currentTimeMillis());\n          readKey.attach(c);\n          synchronized (connectionList) {\n            connectionList.add(numConnections, c);\n            numConnections++;\n          }\n          if (LOG.isDebugEnabled())\n            LOG.debug(\"Server connection from \" + c.toString() +\n                \"; # active connections: \" + numConnections +\n                \"; # queued calls: \" + callQueue.size());          \n        } finally {\n          reader.finishAdd(); \n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.cleanupConnections": "    private void cleanupConnections(boolean force) {\n      if (force || numConnections > thresholdIdleConnections) {\n        long currentTime = System.currentTimeMillis();\n        if (!force && (currentTime - lastCleanupRunTime) < cleanupInterval) {\n          return;\n        }\n        int start = 0;\n        int end = numConnections - 1;\n        if (!force) {\n          start = rand.nextInt() % numConnections;\n          end = rand.nextInt() % numConnections;\n          int temp;\n          if (end < start) {\n            temp = start;\n            start = end;\n            end = temp;\n          }\n        }\n        int i = start;\n        int numNuked = 0;\n        while (i <= end) {\n          Connection c;\n          synchronized (connectionList) {\n            try {\n              c = connectionList.get(i);\n            } catch (Exception e) {return;}\n          }\n          if (c.timedOut(currentTime)) {\n            if (LOG.isDebugEnabled())\n              LOG.debug(getName() + \": disconnecting client \" + c.getHostAddress());\n            closeConnection(c);\n            numNuked++;\n            end--;\n            c = null;\n            if (!force && numNuked == maxConnectionsToNuke) break;\n          }\n          else i++;\n        }\n        lastCleanupRunTime = System.currentTimeMillis();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(ByteArrayOutputStream response, \n                             Call call, Status status, \n                             Writable rv, String errorClass, String error) \n  throws IOException {\n    response.reset();\n    DataOutputStream out = new DataOutputStream(response);\n    out.writeInt(call.callId);                // write call id\n    out.writeInt(status.state);           // write status\n\n    if (status == Status.SUCCESS) {\n      try {\n        rv.write(out);\n      } catch (Throwable t) {\n        LOG.warn(\"Error serializing call response for call \" + call, t);\n        // Call back to same function - this is OK since the\n        // buffer is reset at the top, and since status is changed\n        // to ERROR it won't infinite loop.\n        setupResponse(response, call, Status.ERROR,\n            null, t.getClass().getName(),\n            StringUtils.stringifyException(t));\n        return;\n      }\n    } else {\n      WritableUtils.writeString(out, errorClass);\n      WritableUtils.writeString(out, error);\n    }\n    if (call.connection.useWrap) {\n      wrapWithSasl(response, call);\n    }\n    call.setResponse(ByteBuffer.wrap(response.toByteArray()));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(RpcKind rpcKind, String protocol,\n      Writable param, long receiveTime) throws IOException;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param connection incoming connection\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  public void authorize(UserGroupInformation user, \n                        ConnectionHeader connection,\n                        InetAddress addr\n                        ) throws AuthorizationException {\n    if (authorize) {\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(connection.getProtocol(), getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         connection.getProtocol());\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isValid() && key.isWritable()) {\n                  doAsyncWrite(key);\n              }\n            } catch (IOException e) {\n              LOG.info(getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = System.currentTimeMillis();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<Call> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<Call>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              Call call = (Call)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n          \n          for(Call call : calls) {\n            try {\n              doPurge(call, now);\n            } catch (IOException e) {\n              LOG.warn(\"Error in purging old calls \" + e);\n            }\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.util.ConverterUtils.getPathFromYarnURL": "  public static Path getPathFromYarnURL(URL url) throws URISyntaxException {\n    String scheme = url.getScheme() == null ? \"\" : url.getScheme();\n    \n    String authority = \"\";\n    if (url.getHost() != null) {\n      authority = url.getHost();\n      if (url.getPort() > 0) {\n        authority += \":\" + url.getPort();\n      }\n    }\n    \n    return new Path(\n        (new URI(scheme, authority, url.getFile(), null, null)).normalize());\n  }"
        },
        "bug_report": {
            "Title": "MR tasks failing due to changing timestamps on Resources to download",
            "Description": "[~karams] reported this offline. Seems that tasks are randomly failing during gridmix runs:\n{code}\n2012-02-24 21:03:34,912 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1330116323296_0140_m_003868_0: RemoteTrace:\njava.io.IOException: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875\n       at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:90)\n       at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:49)\n       at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:157)\n       at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:155)\n       at java.security.AccessController.doPrivileged(Native Method)\n       at javax.security.auth.Subject.doAs(Subject.java:396)\n       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)\n       at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:153)\n       at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:49)\n       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n       at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n       at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n       at java.lang.Thread.run(Thread.java:619)\n at LocalTrace:\n       org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875\n       at org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl.convertFromProtoFormat(LocalResourceStatusPBImpl.java:217)\n       at org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl.getException(LocalResourceStatusPBImpl.java:147)\n       at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.update(ResourceLocalizationService.java:827)\n       at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker.processHeartbeat(ResourceLocalizationService.java:497)\n       at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.heartbeat(ResourceLocalizationService.java:222)\n       at org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.service.LocalizationProtocolPBServiceImpl.heartbeat(LocalizationProtocolPBServiceImpl.java:46)\n       at org.apache.hadoop.yarn.proto.LocalizationProtocol$LocalizationProtocolService$2.callBlockingMethod(LocalizationProtocol.java:57)\n       at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:342)\n       at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1493)\n       at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n       at java.security.AccessController.doPrivileged(Native Method)\n       at javax.security.auth.Subject.doAs(Subject.java:396)\n       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)\n       at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1487)\n{code}"
        }
    },
    {
        "filename": "MAPREDUCE-4467.json",
        "creation_time": "2012-07-20T18:20:20.000+0000",
        "stack_trace": "```\njava.lang.IllegalMonitorStateException\n\tat java.lang.Object.wait(Native Method)\n\tat org.apache.hadoop.mapred.IndexCache.getIndexInformation(IndexCache.java:74)\n\tat org.apache.hadoop.mapred.ShuffleHandler$Shuffle.sendMapOutput(ShuffleHandler.java:471)\n\tat org.apache.hadoop.mapred.ShuffleHandler$Shuffle.messageReceived(ShuffleHandler.java:397)\n\tat org.jboss.netty.handler.stream.ChunkedWriteHandler.handleUpstream(ChunkedWriteHandler.java:148)\n\tat org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:116)\n\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)\n\tat org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:522)\n\tat org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506)\n\tat org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443)\n\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)\n\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)\n\tat org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)\n\tat org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)\n\tat org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:662)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IndexCache.getIndexInformation": "  public IndexRecord getIndexInformation(String mapId, int reduce,\n                                         Path fileName, String expectedIndexOwner)\n    throws IOException {\n\n    IndexInformation info = cache.get(mapId);\n\n    if (info == null) {\n      info = readIndexFileToCache(fileName, mapId, expectedIndexOwner);\n    } else {\n      while (isUnderConstruction(info)) {\n        try {\n          // In case the entry is ready after the above check but\n          // before the following wait, we do timed wait.\n          info.wait(200);\n        } catch (InterruptedException e) {\n          throw new IOException(\"Interrupted waiting for construction\", e);\n        }\n      }\n      LOG.debug(\"IndexCache HIT: MapId \" + mapId + \" found\");\n    }\n\n    if (info.mapSpillRecord.size() == 0 ||\n        info.mapSpillRecord.size() <= reduce) {\n      throw new IOException(\"Invalid request \" +\n        \" Map Id = \" + mapId + \" Reducer = \" + reduce +\n        \" Index Info Length = \" + info.mapSpillRecord.size());\n    }\n    return info.mapSpillRecord.getIndex(reduce);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IndexCache.isUnderConstruction": "  private boolean isUnderConstruction(IndexInformation info) {\n    synchronized(info) {\n      return (null == info.mapSpillRecord);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.IndexCache.readIndexFileToCache": "  private IndexInformation readIndexFileToCache(Path indexFileName,\n                                                String mapId,\n                                                String expectedIndexOwner)\n    throws IOException {\n    IndexInformation info;\n    IndexInformation newInd = new IndexInformation();\n    if ((info = cache.putIfAbsent(mapId, newInd)) != null) {\n      while (isUnderConstruction(info)) {\n        try {\n          // In case the entry is ready after the above check but\n          // before the following wait, we do timed wait.\n          info.wait(200);\n        } catch (InterruptedException e) {\n          throw new IOException(\"Interrupted waiting for construction\", e);\n        }\n      }\n      LOG.debug(\"IndexCache HIT: MapId \" + mapId + \" found\");\n      return info;\n    }\n    LOG.debug(\"IndexCache MISS: MapId \" + mapId + \" not found\") ;\n    SpillRecord tmp = null;\n    try { \n      tmp = new SpillRecord(indexFileName, conf, expectedIndexOwner);\n    } catch (Throwable e) { \n      tmp = new SpillRecord(0);\n      cache.remove(mapId);\n      throw new IOException(\"Error Reading IndexFile\", e);\n    } finally { \n      synchronized (newInd) { \n        newInd.mapSpillRecord = tmp;\n        newInd.notifyAll();\n      } \n    } \n    queue.add(mapId);\n    \n    if (totalMemoryUsed.addAndGet(newInd.getSize()) > totalMemoryAllowed) {\n      freeIndexInformation();\n    }\n    return newInd;\n  }"
        },
        "bug_report": {
            "Title": "IndexCache failures due to missing synchronization",
            "Description": "TestMRJobs.testSleepJob fails randomly due to synchronization error in IndexCache:\n\n{code}\n2012-07-20 19:32:34,627 ERROR [New I/O server worker #2-1] mapred.ShuffleHandler (ShuffleHandler.java:exceptionCaught(528)) - Shuffle error: \njava.lang.IllegalMonitorStateException\n\tat java.lang.Object.wait(Native Method)\n\tat org.apache.hadoop.mapred.IndexCache.getIndexInformation(IndexCache.java:74)\n\tat org.apache.hadoop.mapred.ShuffleHandler$Shuffle.sendMapOutput(ShuffleHandler.java:471)\n\tat org.apache.hadoop.mapred.ShuffleHandler$Shuffle.messageReceived(ShuffleHandler.java:397)\n\tat org.jboss.netty.handler.stream.ChunkedWriteHandler.handleUpstream(ChunkedWriteHandler.java:148)\n\tat org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:116)\n\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)\n\tat org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:522)\n\tat org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506)\n\tat org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443)\n\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)\n\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)\n\tat org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)\n\tat org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)\n\tat org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:662)\n{code}\n\nA related issue is MAPREDUCE-4384. The change introduced there removed \"synchronized\" keyword and hence \"info.wait()\" call fails. Tbis needs to be wrapped into a \"synchronized\" block."
        }
    },
    {
        "filename": "MAPREDUCE-3463.json",
        "creation_time": "2011-11-23T13:45:46.000+0000",
        "stack_trace": "```\nRemoteTrace: \n at Local Trace: \n\torg.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Unknown job job_1322040898409_0005\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:151)\n\tat $Proxy10.getTaskAttemptCompletionEvents(Unknown Source)\n\tat org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getTaskAttemptCompletionEvents(MRClientProtocolPBClientImpl.java:172)\n\tat sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:273)\n\tat org.apache.hadoop.mapred.ClientServiceDelegate.getTaskCompletionEvents(ClientServiceDelegate.java:320)\n\tat org.apache.hadoop.mapred.YARNRunner.getTaskCompletionEvents(YARNRunner.java:438)\n\tat org.apache.hadoop.mapreduce.Job.getTaskCompletionEvents(Job.java:621)\n\tat org.apache.hadoop.mapreduce.Job.monitorAndPrintJob(Job.java:1231)\n\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1179)\n\tat org.apache.hadoop.examples.Sort.run(Sort.java:181)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)\n\tat org.apache.hadoop.examples.Sort.main(Sort.java:192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)\n\tat org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:68)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:189)\n\njava.lang.IllegalArgumentException: Invalid NodeId [<NMHostName>]. Expected host:port\n        at org.apache.hadoop.yarn.util.ConverterUtils.toNodeId(ConverterUtils.java:144)\n        at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.sendAssignedEvent(RecoveryService.java:410)\n        at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:314)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$RequestContainerTransition.transition(TaskAttemptImpl.java:1010)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$RequestContainerTransition.transition(TaskAttemptImpl.java:985)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:851)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:128)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:853)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:845)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)\n        at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:270)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n        at java.lang.Thread.run(Thread.java:619)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.invoke": "    public Object invoke(Object proxy, Method method, Object[] args)\n        throws Throwable {\n      long startTime = 0;\n      if (LOG.isDebugEnabled()) {\n        startTime = System.currentTimeMillis();\n      }\n\n      ProtoSpecificRpcRequest rpcRequest = constructRpcRequest(method, args);\n      ProtoSpecificResponseWritable val = null;\n      try {\n        val = (ProtoSpecificResponseWritable) client.call(\n            new ProtoSpecificRequestWritable(rpcRequest), remoteId);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      \n      ProtoSpecificRpcResponse response = val.message;\n   \n      if (LOG.isDebugEnabled()) {\n        long callTime = System.currentTimeMillis() - startTime;\n        LOG.debug(\"Call: \" + method.getName() + \" \" + callTime);\n      }\n \n      if (response.hasIsError() && response.getIsError() == true) {\n        YarnRemoteExceptionPBImpl exception = new YarnRemoteExceptionPBImpl(response.getException());\n        exception.fillInStackTrace();\n        ServiceException se = new ServiceException(exception);\n        throw se;\n      }\n      \n      Message prototype = null;\n      try {\n        prototype = getReturnProtoType(method);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      Message actualReturnMessage = prototype.newBuilderForType()\n          .mergeFrom(response.getResponseProto()).build();\n      return actualReturnMessage;\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.constructRpcRequest": "    private ProtoSpecificRpcRequest constructRpcRequest(Method method,\n        Object[] params) throws ServiceException {\n      ProtoSpecificRpcRequest rpcRequest;\n      ProtoSpecificRpcRequest.Builder builder;\n\n      builder = ProtoSpecificRpcRequest.newBuilder();\n      builder.setMethodName(method.getName());\n\n      if (params.length != 2) { // RpcController + Message\n        throw new ServiceException(\"Too many parameters for request. Method: [\"\n            + method.getName() + \"]\" + \", Expected: 2, Actual: \"\n            + params.length);\n      }\n      if (params[1] == null) {\n        throw new ServiceException(\"null param while calling Method: [\"\n            + method.getName() + \"]\");\n      }\n\n      Message param = (Message) params[1];\n      builder.setRequestProto(param.toByteString());\n\n      rpcRequest = builder.build();\n      return rpcRequest;\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.call": "    public Writable call(String protocol, Writable writableRequest,\n        long receiveTime) throws IOException {\n      ProtoSpecificRequestWritable request = (ProtoSpecificRequestWritable) writableRequest;\n      ProtoSpecificRpcRequest rpcRequest = request.message;\n      String methodName = rpcRequest.getMethodName();\n      System.out.println(\"Call: protocol=\" + protocol + \", method=\"\n          + methodName);\n      if (verbose)\n        log(\"Call: protocol=\" + protocol + \", method=\"\n            + methodName);\n      MethodDescriptor methodDescriptor = service.getDescriptorForType()\n          .findMethodByName(methodName);\n      if (methodDescriptor == null) {\n        String msg = \"Unknown method \" + methodName + \" called on \"\n            + protocol + \" protocol.\";\n        LOG.warn(msg);\n        return handleException(new IOException(msg));\n      }\n      Message prototype = service.getRequestPrototype(methodDescriptor);\n      Message param = prototype.newBuilderForType()\n          .mergeFrom(rpcRequest.getRequestProto()).build();\n      Message result;\n      try {\n        result = service.callBlockingMethod(methodDescriptor, null, param);\n      } catch (ServiceException e) {\n        e.printStackTrace();\n        return handleException(e);\n      } catch (Exception e) {\n        return handleException(e);\n      }\n\n      ProtoSpecificRpcResponse response = constructProtoSpecificRpcSuccessResponse(result);\n      return new ProtoSpecificResponseWritable(response);\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine.getReturnProtoType": "    private Message getReturnProtoType(Method method) throws Exception {\n      if (returnTypes.containsKey(method.getName())) {\n        return returnTypes.get(method.getName());\n      } else {\n        Class<?> returnType = method.getReturnType();\n\n        Method newInstMethod = returnType.getMethod(\"getDefaultInstance\");\n        newInstMethod.setAccessible(true);\n        Message prototype = (Message) newInstMethod.invoke(null,\n            (Object[]) null);\n        returnTypes.put(method.getName(), prototype);\n        return prototype;\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getTaskAttemptCompletionEvents": "  public GetTaskAttemptCompletionEventsResponse getTaskAttemptCompletionEvents(\n      GetTaskAttemptCompletionEventsRequest request) throws YarnRemoteException {\n    GetTaskAttemptCompletionEventsRequestProto requestProto = ((GetTaskAttemptCompletionEventsRequestPBImpl)request).getProto();\n    try {\n      return new GetTaskAttemptCompletionEventsResponsePBImpl(proxy.getTaskAttemptCompletionEvents(null, requestProto));\n    } catch (ServiceException e) {\n      if (e.getCause() instanceof YarnRemoteException) {\n        throw (YarnRemoteException)e.getCause();\n      } else if (e.getCause() instanceof UndeclaredThrowableException) {\n        throw (UndeclaredThrowableException)e.getCause();\n      } else {\n        throw new UndeclaredThrowableException(e);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.ClientServiceDelegate.invoke": "  private synchronized Object invoke(String method, Class argClass,\n      Object args) throws YarnRemoteException {\n    Method methodOb = null;\n    try {\n      methodOb = MRClientProtocol.class.getMethod(method, argClass);\n    } catch (SecurityException e) {\n      throw new YarnException(e);\n    } catch (NoSuchMethodException e) {\n      throw new YarnException(\"Method name mismatch\", e);\n    }\n    while (true) {\n      try {\n        return methodOb.invoke(getProxy(), args);\n      } catch (YarnRemoteException yre) {\n        LOG.warn(\"Exception thrown by remote end.\", yre);\n        throw yre;\n      } catch (InvocationTargetException e) {\n        if (e.getTargetException() instanceof YarnRemoteException) {\n          LOG.warn(\"Error from remote end: \" + e\n              .getTargetException().getLocalizedMessage());\n          LOG.debug(\"Tracing remote error \", e.getTargetException());\n          throw (YarnRemoteException) e.getTargetException();\n        }\n        LOG.info(\"Failed to contact AM/History for job \" + jobId + \n            \" retrying..\");\n        LOG.debug(\"Failed exception on AM/History contact\", \n            e.getTargetException());\n        // Force reconnection by setting the proxy to null.\n        realProxy = null;\n      } catch (Exception e) {\n        LOG.info(\"Failed to contact AM/History for job \" + jobId\n            + \"  Will retry..\");\n        LOG.debug(\"Failing to contact application master\", e);\n        // Force reconnection by setting the proxy to null.\n        realProxy = null;\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.ClientServiceDelegate.getProxy": "  private MRClientProtocol getProxy() throws YarnRemoteException {\n    if (realProxy != null) {\n      return realProxy;\n    }\n    \n    // Possibly allow nulls through the PB tunnel, otherwise deal with an exception\n    // and redirect to the history server.\n    ApplicationReport application = rm.getApplicationReport(appId);\n    if (application != null) {\n      trackingUrl = application.getTrackingUrl();\n    }\n    String serviceAddr = null;\n    while (application == null\n        || YarnApplicationState.RUNNING == application\n            .getYarnApplicationState()) {\n      if (application == null) {\n        LOG.info(\"Could not get Job info from RM for job \" + jobId\n            + \". Redirecting to job history server.\");\n        return checkAndGetHSProxy(null, JobState.NEW);\n      }\n      try {\n        if (application.getHost() == null || \"\".equals(application.getHost())) {\n          LOG.debug(\"AM not assigned to Job. Waiting to get the AM ...\");\n          Thread.sleep(2000);\n\n          LOG.debug(\"Application state is \" + application.getYarnApplicationState());\n          application = rm.getApplicationReport(appId);\n          continue;\n        }\n        serviceAddr = application.getHost() + \":\" + application.getRpcPort();\n        if (UserGroupInformation.isSecurityEnabled()) {\n          String clientTokenEncoded = application.getClientToken();\n          Token<ApplicationTokenIdentifier> clientToken =\n            new Token<ApplicationTokenIdentifier>();\n          clientToken.decodeFromUrlString(clientTokenEncoded);\n          // RPC layer client expects ip:port as service for tokens\n          InetSocketAddress addr = NetUtils.createSocketAddr(application\n              .getHost(), application.getRpcPort());\n          clientToken.setService(new Text(addr.getAddress().getHostAddress()\n              + \":\" + addr.getPort()));\n          UserGroupInformation.getCurrentUser().addToken(clientToken);\n        }\n        LOG.info(\"Tracking Url of JOB is \" + application.getTrackingUrl());\n        LOG.info(\"Connecting to \" + serviceAddr);\n        realProxy = instantiateAMProxy(serviceAddr);\n        return realProxy;\n      } catch (IOException e) {\n        //possibly the AM has crashed\n        //there may be some time before AM is restarted\n        //keep retrying by getting the address from RM\n        LOG.info(\"Could not connect to \" + serviceAddr +\n        \". Waiting for getting the latest AM address...\");\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e1) {\n          LOG.warn(\"getProxy() call interruped\", e1);\n          throw new YarnException(e1);\n        }\n        application = rm.getApplicationReport(appId);\n        if (application == null) {\n          LOG.info(\"Could not get Job info from RM for job \" + jobId\n              + \". Redirecting to job history server.\");\n          return checkAndGetHSProxy(null, JobState.RUNNING);\n        }\n      } catch (InterruptedException e) {\n        LOG.warn(\"getProxy() call interruped\", e);\n        throw new YarnException(e);\n      }\n    }\n\n    /** we just want to return if its allocating, so that we don't\n     * block on it. This is to be able to return job status\n     * on an allocating Application.\n     */\n    String user = application.getUser();\n    if (user == null) {\n      throw RPCUtil.getRemoteException(\"User is not set in the application report\");\n    }\n    if (application.getYarnApplicationState() == YarnApplicationState.NEW ||\n        application.getYarnApplicationState() == YarnApplicationState.SUBMITTED) {\n      realProxy = null;\n      return getNotRunningJob(application, JobState.NEW);\n    }\n\n    if (application.getYarnApplicationState() == YarnApplicationState.FAILED) {\n      realProxy = null;\n      return getNotRunningJob(application, JobState.FAILED);\n    }\n\n    if (application.getYarnApplicationState() == YarnApplicationState.KILLED) {\n      realProxy = null;\n      return getNotRunningJob(application, JobState.KILLED);\n    }\n\n    //History server can serve a job only if application\n    //succeeded.\n    if (application.getYarnApplicationState() == YarnApplicationState.FINISHED) {\n      LOG.info(\"Application state is completed. FinalApplicationStatus=\"\n          + application.getFinalApplicationStatus().toString()\n          + \". Redirecting to job history server\");\n      realProxy = checkAndGetHSProxy(application, JobState.SUCCEEDED);\n    }\n    return realProxy;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.ClientServiceDelegate.getTaskCompletionEvents": "  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1, int arg2)\n      throws IOException, InterruptedException {\n    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID = TypeConverter\n        .toYarn(arg0);\n    GetTaskAttemptCompletionEventsRequest request = recordFactory\n        .newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n    request.setJobId(jobID);\n    request.setFromEventId(arg1);\n    request.setMaxEvents(arg2);\n    List<org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent> list =\n      ((GetTaskAttemptCompletionEventsResponse) invoke(\n        \"getTaskAttemptCompletionEvents\", GetTaskAttemptCompletionEventsRequest.class, request)).\n        getCompletionEventList();\n    return TypeConverter\n        .fromYarn(list\n            .toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.YARNRunner.getTaskCompletionEvents": "  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1,\n      int arg2) throws IOException, InterruptedException {\n    return clientCache.getClient(arg0).getTaskCompletionEvents(arg0, arg1, arg2);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ToolRunner.run": "  public static int run(Tool tool, String[] args) \n    throws Exception{\n    return run(tool.getConf(), tool, args);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ProgramDriver.invoke": "    public void invoke(String[] args)\n      throws Throwable {\n      try {\n        main.invoke(null, new Object[]{args});\n      } catch (InvocationTargetException except) {\n        throw except.getCause();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ProgramDriver.driver": "  public int driver(String[] args) \n    throws Throwable \n  {\n    // Make sure they gave us a program name.\n    if (args.length == 0) {\n      System.out.println(\"An example program must be given as the\" + \n                         \" first argument.\");\n      printUsage(programs);\n      return -1;\n    }\n\t\n    // And that it is good.\n    ProgramDescription pgm = programs.get(args[0]);\n    if (pgm == null) {\n      System.out.println(\"Unknown program '\" + args[0] + \"' chosen.\");\n      printUsage(programs);\n      return -1;\n    }\n\t\n    // Remove the leading argument and call main\n    String[] new_args = new String[args.length - 1];\n    for(int i=1; i < args.length; ++i) {\n      new_args[i-1] = args[i];\n    }\n    pgm.invoke(new_args);\n    return 0;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ProgramDriver.printUsage": "  private static void printUsage(Map<String, ProgramDescription> programs) {\n    System.out.println(\"Valid program names are:\");\n    for(Map.Entry<String, ProgramDescription> item : programs.entrySet()) {\n      System.out.println(\"  \" + item.getKey() + \": \" +\n                         item.getValue().getDescription());         \n    } \n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.RunJar.main": "  public static void main(String[] args) throws Throwable {\n    String usage = \"RunJar jarFile [mainClass] args...\";\n\n    if (args.length < 1) {\n      System.err.println(usage);\n      System.exit(-1);\n    }\n\n    int firstArg = 0;\n    String fileName = args[firstArg++];\n    File file = new File(fileName);\n    String mainClassName = null;\n\n    JarFile jarFile;\n    try {\n      jarFile = new JarFile(fileName);\n    } catch(IOException io) {\n      throw new IOException(\"Error opening job jar: \" + fileName)\n        .initCause(io);\n    }\n\n    Manifest manifest = jarFile.getManifest();\n    if (manifest != null) {\n      mainClassName = manifest.getMainAttributes().getValue(\"Main-Class\");\n    }\n    jarFile.close();\n\n    if (mainClassName == null) {\n      if (args.length < 2) {\n        System.err.println(usage);\n        System.exit(-1);\n      }\n      mainClassName = args[firstArg++];\n    }\n    mainClassName = mainClassName.replaceAll(\"/\", \".\");\n\n    File tmpDir = new File(new Configuration().get(\"hadoop.tmp.dir\"));\n    ensureDirectory(tmpDir);\n\n    final File workDir;\n    try { \n      workDir = File.createTempFile(\"hadoop-unjar\", \"\", tmpDir);\n    } catch (IOException ioe) {\n      // If user has insufficient perms to write to tmpDir, default  \n      // \"Permission denied\" message doesn't specify a filename. \n      System.err.println(\"Error creating temp dir in hadoop.tmp.dir \"\n                         + tmpDir + \" due to \" + ioe.getMessage());\n      System.exit(-1);\n      return;\n    }\n\n    if (!workDir.delete()) {\n      System.err.println(\"Delete failed for \" + workDir);\n      System.exit(-1);\n    }\n    ensureDirectory(workDir);\n\n    Runtime.getRuntime().addShutdownHook(new Thread() {\n        public void run() {\n          FileUtil.fullyDelete(workDir);\n        }\n      });\n\n    unJar(file, workDir);\n\n    ArrayList<URL> classPath = new ArrayList<URL>();\n    classPath.add(new File(workDir+\"/\").toURI().toURL());\n    classPath.add(file.toURI().toURL());\n    classPath.add(new File(workDir, \"classes/\").toURI().toURL());\n    File[] libs = new File(workDir, \"lib\").listFiles();\n    if (libs != null) {\n      for (int i = 0; i < libs.length; i++) {\n        classPath.add(libs[i].toURI().toURL());\n      }\n    }\n    \n    ClassLoader loader =\n      new URLClassLoader(classPath.toArray(new URL[0]));\n\n    Thread.currentThread().setContextClassLoader(loader);\n    Class<?> mainClass = Class.forName(mainClassName, true, loader);\n    Method main = mainClass.getMethod(\"main\", new Class[] {\n      Array.newInstance(String.class, 0).getClass()\n    });\n    String[] newArgs = Arrays.asList(args)\n      .subList(firstArg, args.length).toArray(new String[0]);\n    try {\n      main.invoke(null, new Object[] { newArgs });\n    } catch (InvocationTargetException e) {\n      throw e.getTargetException();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.RunJar.ensureDirectory": "  private static void ensureDirectory(File dir) throws IOException {\n    if (!dir.mkdirs() && !dir.isDirectory()) {\n      throw new IOException(\"Mkdirs failed to create \" +\n                            dir.toString());\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.RunJar.unJar": "  public static void unJar(File jarFile, File toDir, Pattern unpackRegex)\n    throws IOException {\n    JarFile jar = new JarFile(jarFile);\n    try {\n      Enumeration<JarEntry> entries = jar.entries();\n      while (entries.hasMoreElements()) {\n        JarEntry entry = (JarEntry)entries.nextElement();\n        if (!entry.isDirectory() &&\n            unpackRegex.matcher(entry.getName()).matches()) {\n          InputStream in = jar.getInputStream(entry);\n          try {\n            File file = new File(toDir, entry.getName());\n            ensureDirectory(file.getParentFile());\n            OutputStream out = new FileOutputStream(file);\n            try {\n              IOUtils.copyBytes(in, out, 8192);\n            } finally {\n              out.close();\n            }\n          } finally {\n            in.close();\n          }\n        }\n      }\n    } finally {\n      jar.close();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.util.ConverterUtils.toNodeId": "  public static NodeId toNodeId(String nodeIdStr) {\n    String[] parts = nodeIdStr.split(\":\");\n    if (parts.length != 2) {\n      throw new IllegalArgumentException(\"Invalid NodeId [\" + nodeIdStr\n          + \"]. Expected host:port\");\n    }\n    try {\n      NodeId nodeId =\n          BuilderUtils.newNodeId(parts[0], Integer.parseInt(parts[1]));\n      return nodeId;\n    } catch (NumberFormatException e) {\n      throw new IllegalArgumentException(\"Invalid port: \" + parts[1], e);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService.sendAssignedEvent": "    private void sendAssignedEvent(TaskAttemptId yarnAttemptID,\n        TaskAttemptInfo attemptInfo) {\n      LOG.info(\"Sending assigned event to \" + yarnAttemptID);\n      ContainerId cId = attemptInfo.getContainerId();\n\n      NodeId nodeId = ConverterUtils.toNodeId(attemptInfo.getHostname());\n      // Resource/Priority/ApplicationACLs are only needed while launching the\n      // container on an NM, these are already completed tasks, so setting them\n      // to null\n      Container container = BuilderUtils.newContainer(cId, nodeId,\n          attemptInfo.getTrackerName() + \":\" + attemptInfo.getHttpPort(),\n          null, null, null);\n      actualHandler.handle(new TaskAttemptContainerAssignedEvent(yarnAttemptID,\n          container, null));\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService.handle": "    public void handle(Event event) {\n      if (!recoveryMode) {\n        // delegate to the dispatcher one\n        actualHandler.handle(event);\n        return;\n      }\n\n      else if (event.getType() == TaskEventType.T_SCHEDULE) {\n        TaskEvent taskEvent = (TaskEvent) event;\n        // delay the scheduling of new tasks till previous ones are recovered\n        if (completedTasks.get(taskEvent.getTaskID()) == null) {\n          LOG.debug(\"Adding to pending task events \"\n              + taskEvent.getTaskID());\n          pendingTaskScheduleEvents.add(taskEvent);\n          return;\n        }\n      }\n\n      else if (event.getType() == ContainerAllocator.EventType.CONTAINER_REQ) {\n        TaskAttemptId aId = ((ContainerAllocatorEvent) event).getAttemptID();\n        TaskAttemptInfo attInfo = getTaskAttemptInfo(aId);\n        LOG.debug(\"CONTAINER_REQ \" + aId);\n        sendAssignedEvent(aId, attInfo);\n        return;\n      }\n\n      else if (event.getType() == TaskCleaner.EventType.TASK_CLEAN) {\n        TaskAttemptId aId = ((TaskCleanupEvent) event).getAttemptID();\n        LOG.debug(\"TASK_CLEAN\");\n        actualHandler.handle(new TaskAttemptEvent(aId,\n            TaskAttemptEventType.TA_CLEANUP_DONE));\n        return;\n      }\n\n      else if (event.getType() == ContainerLauncher.EventType.CONTAINER_REMOTE_LAUNCH) {\n        TaskAttemptId aId = ((ContainerRemoteLaunchEvent) event)\n            .getTaskAttemptID();\n        TaskAttemptInfo attInfo = getTaskAttemptInfo(aId);\n        actualHandler.handle(new TaskAttemptContainerLaunchedEvent(aId,\n            attInfo.getShufflePort()));\n        // send the status update event\n        sendStatusUpdateEvent(aId, attInfo);\n\n        TaskAttemptState state = TaskAttemptState.valueOf(attInfo.getTaskStatus());\n        switch (state) {\n        case SUCCEEDED:\n          //recover the task output\n          TaskAttemptContext taskContext = new TaskAttemptContextImpl(getConfig(),\n              attInfo.getAttemptId());\n          try {\n            committer.recoverTask(taskContext);\n          } catch (IOException e) {\n            actualHandler.handle(new JobDiagnosticsUpdateEvent(\n                aId.getTaskId().getJobId(), \"Error in recovering task output \" + \n                e.getMessage()));\n            actualHandler.handle(new JobEvent(aId.getTaskId().getJobId(),\n                JobEventType.INTERNAL_ERROR));\n          }\n          LOG.info(\"Recovered output from task attempt \" + attInfo.getAttemptId());\n          \n          // send the done event\n          LOG.info(\"Sending done event to \" + aId);\n          actualHandler.handle(new TaskAttemptEvent(aId,\n              TaskAttemptEventType.TA_DONE));\n          break;\n        case KILLED:\n          LOG.info(\"Sending kill event to \" + aId);\n          actualHandler.handle(new TaskAttemptEvent(aId,\n              TaskAttemptEventType.TA_KILL));\n          break;\n        default:\n          LOG.info(\"Sending fail event to \" + aId);\n          actualHandler.handle(new TaskAttemptEvent(aId,\n              TaskAttemptEventType.TA_FAILMSG));\n          break;\n        }\n        return;\n      }\n\n      else if (event.getType() == \n        ContainerLauncher.EventType.CONTAINER_REMOTE_CLEANUP) {\n        TaskAttemptId aId = ((ContainerLauncherEvent) event)\n          .getTaskAttemptID();\n        actualHandler.handle(\n           new TaskAttemptEvent(aId,\n                TaskAttemptEventType.TA_CONTAINER_CLEANED));\n        return;\n      }\n\n      // delegate to the actual handler\n      actualHandler.handle(event);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.transition": "    public void transition(TaskAttemptImpl taskAttempt, \n        TaskAttemptEvent event) {\n      TaskAttemptDiagnosticsUpdateEvent diagEvent =\n          (TaskAttemptDiagnosticsUpdateEvent) event;\n      LOG.info(\"Diagnostics report from \" + taskAttempt.attemptId + \": \"\n          + diagEvent.getDiagnosticInfo());\n      taskAttempt.addDiagnosticInfo(diagEvent.getDiagnosticInfo());\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.addDiagnosticInfo": "  private void addDiagnosticInfo(String diag) {\n    if (diag != null && !diag.equals(\"\")) {\n      diagnostics.add(diag);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getNodeHttpAddress": "  public String getNodeHttpAddress() {\n    readLock.lock();\n    try {\n      return nodeHttpAddress;\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createRemoteTask": "  protected abstract org.apache.hadoop.mapred.Task createRemoteTask();\n\n  @Override\n  public TaskAttemptId getID() {\n    return attemptId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.setFinishTime": "  private void setFinishTime() {\n    //set the finish time only if launch time is set\n    if (launchTime != 0) {\n      finishTime = clock.getTime();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createTaskAttemptUnsuccessfulCompletionEvent": "      TaskAttemptUnsuccessfulCompletionEvent\n      createTaskAttemptUnsuccessfulCompletionEvent(TaskAttemptImpl taskAttempt,\n          TaskAttemptState attemptState) {\n    TaskAttemptUnsuccessfulCompletionEvent tauce =\n        new TaskAttemptUnsuccessfulCompletionEvent(\n            TypeConverter.fromYarn(taskAttempt.attemptId),\n            TypeConverter.fromYarn(taskAttempt.attemptId.getTaskId()\n                .getTaskType()), attemptState.toString(),\n            taskAttempt.finishTime,\n            taskAttempt.containerMgrAddress == null ? \"UNKNOWN\"\n                : taskAttempt.containerMgrAddress, StringUtils.join(\n                LINE_SEPARATOR, taskAttempt.getDiagnostics()), taskAttempt\n                .getProgressSplitBlock().burst());\n    return tauce;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.computeSlotMillis": "  private static long computeSlotMillis(TaskAttemptImpl taskAttempt) {\n    TaskType taskType = taskAttempt.getID().getTaskId().getTaskType();\n    int slotMemoryReq =\n       taskAttempt.getMemoryRequired(taskAttempt.conf, taskType);\n    int simSlotsRequired =\n        slotMemoryReq\n            / (taskType == TaskType.MAP ? MAP_MEMORY_MB_DEFAULT\n                : REDUCE_MEMORY_MB_DEFAULT);\n    // Simulating MRv1 slots for counters by assuming *_MEMORY_MB_DEFAULT\n    // corresponds to a MrV1 slot.\n    // Fallow slot millis is not applicable in MRv2 - since a container is\n    // either assigned with the required memory or is not. No partial\n    // reserveations\n    long slotMillisIncrement =\n        simSlotsRequired\n            * (taskAttempt.getFinishTime() - taskAttempt.getLaunchTime());\n    return slotMillisIncrement;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getShufflePort": "  public int getShufflePort() {\n    readLock.lock();\n    try {\n      return shufflePort;\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getLaunchTime": "  public long getLaunchTime() {\n    readLock.lock();\n    try {\n      return launchTime;\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getID": "  public TaskAttemptId getID() {\n    return attemptId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.logAttemptFinishedEvent": "  private void logAttemptFinishedEvent(TaskAttemptState state) {\n    //Log finished events only if an attempt started.\n    if (getLaunchTime() == 0) return; \n    if (attemptId.getTaskId().getTaskType() == TaskType.MAP) {\n      MapAttemptFinishedEvent mfe =\n         new MapAttemptFinishedEvent(TypeConverter.fromYarn(attemptId),\n         TypeConverter.fromYarn(attemptId.getTaskId().getTaskType()),\n         state.toString(),\n         this.reportedStatus.mapFinishTime,\n         finishTime,\n         this.containerNodeId == null ? \"UNKNOWN\"\n             : this.containerNodeId.getHost(),\n         this.nodeRackName == null ? \"UNKNOWN\" : this.nodeRackName,\n         this.reportedStatus.stateString,\n         TypeConverter.fromYarn(getCounters()),\n         getProgressSplitBlock().burst());\n         eventHandler.handle(\n           new JobHistoryEvent(attemptId.getTaskId().getJobId(), mfe));\n    } else {\n       ReduceAttemptFinishedEvent rfe =\n         new ReduceAttemptFinishedEvent(TypeConverter.fromYarn(attemptId),\n         TypeConverter.fromYarn(attemptId.getTaskId().getTaskType()),\n         state.toString(),\n         this.reportedStatus.shuffleFinishTime,\n         this.reportedStatus.sortFinishTime,\n         finishTime,\n         this.containerNodeId == null ? \"UNKNOWN\"\n                                         : this.containerNodeId.getHost(),\n         this.nodeRackName == null ? \"UNKNOWN\" : this.nodeRackName,\n         this.reportedStatus.stateString,\n         TypeConverter.fromYarn(getCounters()),\n         getProgressSplitBlock().burst());\n         eventHandler.handle(\n           new JobHistoryEvent(attemptId.getTaskId().getJobId(), rfe));\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createJobCounterUpdateEventTAFailed": "  private static JobCounterUpdateEvent createJobCounterUpdateEventTAFailed(\n      TaskAttemptImpl taskAttempt) {\n    TaskType taskType = taskAttempt.getID().getTaskId().getTaskType();\n    JobCounterUpdateEvent jce = new JobCounterUpdateEvent(taskAttempt.getID().getTaskId().getJobId());\n    \n    long slotMillisIncrement = computeSlotMillis(taskAttempt);\n    \n    if (taskType == TaskType.MAP) {\n      jce.addCounterUpdate(JobCounter.NUM_FAILED_MAPS, 1);\n      jce.addCounterUpdate(JobCounter.SLOTS_MILLIS_MAPS, slotMillisIncrement);\n    } else {\n      jce.addCounterUpdate(JobCounter.NUM_FAILED_REDUCES, 1);\n      jce.addCounterUpdate(JobCounter.SLOTS_MILLIS_REDUCES, slotMillisIncrement);\n    }\n    return jce;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.updateProgressSplits": "  private void updateProgressSplits() {\n    double newProgress = reportedStatus.progress;\n    Counters counters = reportedStatus.counters;\n    if (counters == null)\n      return;\n\n    WrappedProgressSplitsBlock splitsBlock = getProgressSplitBlock();\n    if (splitsBlock != null) {\n      long now = clock.getTime();\n      long start = getLaunchTime(); // TODO Ensure not 0\n\n      if (start != 0 && now - start <= Integer.MAX_VALUE) {\n        splitsBlock.getProgressWallclockTime().extend(newProgress,\n            (int) (now - start));\n      }\n\n      Counter cpuCounter = counters.getCounter(\n          TaskCounter.CPU_MILLISECONDS);\n      if (cpuCounter != null && cpuCounter.getValue() <= Integer.MAX_VALUE) {\n        splitsBlock.getProgressCPUTime().extend(newProgress,\n            (int) cpuCounter.getValue());\n      }\n\n      Counter virtualBytes = counters.getCounter(\n          TaskCounter.VIRTUAL_MEMORY_BYTES);\n      if (virtualBytes != null) {\n        splitsBlock.getProgressVirtualMemoryKbytes().extend(newProgress,\n            (int) (virtualBytes.getValue() / (MEMORY_SPLITS_RESOLUTION)));\n      }\n\n      Counter physicalBytes = counters.getCounter(\n          TaskCounter.PHYSICAL_MEMORY_BYTES);\n      if (physicalBytes != null) {\n        splitsBlock.getProgressPhysicalMemoryKbytes().extend(newProgress,\n            (int) (physicalBytes.getValue() / (MEMORY_SPLITS_RESOLUTION)));\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createContainerLaunchContext": "  private ContainerLaunchContext createContainerLaunchContext(\n      Map<ApplicationAccessType, String> applicationACLs) {\n\n    // Application resources\n    Map<String, LocalResource> localResources = \n        new HashMap<String, LocalResource>();\n    \n    // Application environment\n    Map<String, String> environment = new HashMap<String, String>();\n\n    // Service data\n    Map<String, ByteBuffer> serviceData = new HashMap<String, ByteBuffer>();\n\n    // Tokens\n    ByteBuffer tokens = ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS = FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) != null) {\n        Path remoteJobJar = (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRJobConfig.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path =\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir =\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath = \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up tokens\n      Credentials taskCredentials = new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token<? extends TokenIdentifier> token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob = new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      tokens = \n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Setup environment\n    MapReduceChildJVM.setVMEnv(environment, remoteTask);\n\n    // Set up the launch command\n    List<String> commands = MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask,\n        jvmID);\n    \n    // Construct the actual Container\n    ContainerLaunchContext container = BuilderUtils\n        .newContainerLaunchContext(containerID, conf\n            .get(MRJobConfig.USER_NAME), assignedCapability, localResources,\n            environment, commands, serviceData, tokens, applicationACLs);\n\n    return container;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getContainer": "        public ContainerLaunchContext getContainer() {\n          return taskAttempt.createContainerLaunchContext(cEvent\n              .getApplicationACLs());\n        }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getState": "  public TaskAttemptState getState() {\n    readLock.lock();\n    try {\n      return stateMachine.getCurrentState();\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.state.StateMachineFactory.doTransition": "    public synchronized STATE doTransition(EVENTTYPE eventType, EVENT event)\n         throws InvalidStateTransitonException  {\n      currentState = StateMachineFactory.this.doTransition\n          (operand, currentState, eventType, event);\n      return currentState;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle": "  public void handle(TaskAttemptEvent event) {\n    LOG.info(\"Processing \" + event.getTaskAttemptID() +\n        \" of type \" + event.getType());\n    writeLock.lock();\n    try {\n      final TaskAttemptState oldState = getState();\n      try {\n        stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state for \"\n            + this.attemptId, e);\n        eventHandler.handle(new JobDiagnosticsUpdateEvent(\n            this.attemptId.getTaskId().getJobId(), \"Invalid event \" + event.getType() + \n            \" on TaskAttempt \" + this.attemptId));\n        eventHandler.handle(new JobEvent(this.attemptId.getTaskId().getJobId(),\n            JobEventType.INTERNAL_ERROR));\n      }\n      if (oldState != getState()) {\n          LOG.info(attemptId + \" TaskAttempt Transitioned from \" \n           + oldState + \" to \"\n           + getState());\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.handle": "    public void handle(SpeculatorEvent event) {\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        // Speculator IS enabled, direct the event to there.\n        speculator.handle(event);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.cleanupStagingDir": "  public void cleanupStagingDir() throws IOException {\n    /* make sure we clean the staging files */\n    String jobTempDir = null;\n    FileSystem fs = getFileSystem(getConfig());\n    try {\n      if (!keepJobFiles(new JobConf(getConfig()))) {\n        jobTempDir = getConfig().get(MRJobConfig.MAPREDUCE_JOB_DIR);\n        if (jobTempDir == null) {\n          LOG.warn(\"Job Staging directory is null\");\n          return;\n        }\n        Path jobTempDirPath = new Path(jobTempDir);\n        LOG.info(\"Deleting staging directory \" + FileSystem.getDefaultUri(getConfig()) +\n            \" \" + jobTempDir);\n        fs.delete(jobTempDirPath, true);\n      }\n    } catch(IOException io) {\n      LOG.error(\"Failed to cleanup staging dir \" + jobTempDir, io);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop": "    public synchronized void stop() {\n      ((Service)this.containerLauncher).stop();\n      super.stop();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJobId": "  public JobId getJobId() {\n    return jobId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJob": "    public Job getJob(JobId jobID) {\n      return jobs.get(jobID);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.sysexit": "  protected void sysexit() {\n    System.exit(0);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch": "  protected void dispatch(Event event) {\n    //all events go thru this loop\n    LOG.debug(\"Dispatching the event \" + event.getClass().getName() + \".\"\n        + event.toString());\n\n    Class<? extends Enum> type = event.getType().getDeclaringClass();\n\n    try{\n      eventDispatchers.get(type).handle(event);\n    }\n    catch (Throwable t) {\n      //TODO Maybe log the state of the queue\n      LOG.fatal(\"Error in dispatcher thread. Exiting..\", t);\n      System.exit(-1);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.handle": "    public void handle(Event event) {\n      for (EventHandler<Event> handler: listofHandlers) {\n        handler.handle(event);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService.dispatch": "    public void dispatch(Event event) {\n      if (recoveryMode) {\n        if (event.getType() == TaskAttemptEventType.TA_CONTAINER_LAUNCHED) {\n          TaskAttemptInfo attInfo = getTaskAttemptInfo(((TaskAttemptEvent) event)\n              .getTaskAttemptID());\n          LOG.info(\"Attempt start time \" + attInfo.getStartTime());\n          clock.setTime(attInfo.getStartTime());\n\n        } else if (event.getType() == TaskAttemptEventType.TA_DONE\n            || event.getType() == TaskAttemptEventType.TA_FAILMSG\n            || event.getType() == TaskAttemptEventType.TA_KILL) {\n          TaskAttemptInfo attInfo = getTaskAttemptInfo(((TaskAttemptEvent) event)\n              .getTaskAttemptID());\n          LOG.info(\"Attempt finish time \" + attInfo.getFinishTime());\n          clock.setTime(attInfo.getFinishTime());\n        }\n\n        else if (event.getType() == TaskEventType.T_ATTEMPT_FAILED\n            || event.getType() == TaskEventType.T_ATTEMPT_KILLED\n            || event.getType() == TaskEventType.T_ATTEMPT_SUCCEEDED) {\n          TaskTAttemptEvent tEvent = (TaskTAttemptEvent) event;\n          LOG.info(\"Recovered Task attempt \" + tEvent.getTaskAttemptID());\n          TaskInfo taskInfo = completedTasks.get(tEvent.getTaskAttemptID()\n              .getTaskId());\n          taskInfo.getAllTaskAttempts().remove(\n              TypeConverter.fromYarn(tEvent.getTaskAttemptID()));\n          // remove the task info from completed tasks if all attempts are\n          // recovered\n          if (taskInfo.getAllTaskAttempts().size() == 0) {\n            completedTasks.remove(tEvent.getTaskAttemptID().getTaskId());\n            // checkForRecoveryComplete\n            LOG.info(\"CompletedTasks() \" + completedTasks.size());\n            if (completedTasks.size() == 0) {\n              recoveryMode = false;\n              clock.reset();\n              LOG.info(\"Setting the recovery mode to false. \" +\n                 \"Recovery is complete!\");\n\n              // send all pending tasks schedule events\n              for (TaskEvent tEv : pendingTaskScheduleEvents) {\n                actualHandler.handle(tEv);\n              }\n\n            }\n          }\n        }\n      }\n      super.dispatch(event);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService.getTaskAttemptInfo": "  private TaskAttemptInfo getTaskAttemptInfo(TaskAttemptId id) {\n    TaskInfo taskInfo = completedTasks.get(id.getTaskId());\n    return taskInfo.getAllTaskAttempts().get(TypeConverter.fromYarn(id));\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.run": "      public void run() {\n        while (!stopped && !Thread.currentThread().isInterrupted()) {\n          Event event;\n          try {\n            event = eventQueue.take();\n          } catch(InterruptedException ie) {\n            LOG.info(\"AsyncDispatcher thread interrupted\", ie);\n            return;\n          }\n          if (event != null) {\n            dispatch(event);\n          }\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.clientCache.getClient": "  public synchronized ClientServiceDelegate getClient(JobID jobId) {\n    if (hsProxy == null) {\n      try {\n        hsProxy = instantiateHistoryProxy();\n      } catch (IOException e) {\n        LOG.warn(\"Could not connect to History server.\", e);\n        throw new YarnException(\"Could not connect to History server.\", e);\n      }\n    }\n    ClientServiceDelegate client = cache.get(jobId);\n    if (client == null) {\n      client = new ClientServiceDelegate(conf, rm, jobId, hsProxy);\n      cache.put(jobId, client);\n    }\n    return client;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.clientCache.instantiateHistoryProxy": "  private MRClientProtocol instantiateHistoryProxy()\n      throws IOException {\n    final String serviceAddr = conf.get(JHAdminConfig.MR_HISTORY_ADDRESS);\n    if (StringUtils.isEmpty(serviceAddr)) {\n      return null;\n    }\n    LOG.info(\"Connecting to HistoryServer at: \" + serviceAddr);\n    final YarnRPC rpc = YarnRPC.create(conf);\n    LOG.info(\"Connected to HistoryServer at: \" + serviceAddr);\n    UserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\n    return currentUser.doAs(new PrivilegedAction<MRClientProtocol>() {\n      @Override\n      public MRClientProtocol run() {\n        return (MRClientProtocol) rpc.getProxy(MRClientProtocol.class,\n            NetUtils.createSocketAddr(serviceAddr), conf);\n      }\n    });\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs": "  public String[] getRemainingArgs() {\n    return (commandLine == null) ? new String[]{} : commandLine.getArgs();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.util.BuilderUtils.newNodeId": "  public static NodeId newNodeId(String host, int port) {\n    NodeId nodeId = recordFactory.newRecordInstance(NodeId.class);\n    nodeId.setHost(host);\n    nodeId.setPort(port);\n    return nodeId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.setConf": "  public void setConf(Configuration conf) {\n    this.conf = conf;\n    \n    numTries = Math.min(\n      conf.getInt(MRJobConfig.MR_JOB_END_RETRY_ATTEMPTS, 0) + 1\n      , conf.getInt(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_ATTEMPTS, 1)\n    );\n    waitInterval = Math.min(\n    conf.getInt(MRJobConfig.MR_JOB_END_RETRY_INTERVAL, 5)\n    , conf.getInt(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_RETRY_INTERVAL, 5)\n    );\n    waitInterval = (waitInterval < 0) ? 5 : waitInterval;\n\n    userUrl = conf.get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notify": "  public void notify(JobReport jobReport)\n    throws InterruptedException {\n    // Do we need job-end notification?\n    if (userUrl == null) {\n      Log.info(\"Job end notification URL not set, skipping.\");\n      return;\n    }\n\n    //Do string replacements for jobId and jobStatus\n    if (userUrl.contains(JOB_ID)) {\n      userUrl = userUrl.replace(JOB_ID, jobReport.getJobId().toString());\n    }\n    if (userUrl.contains(JOB_STATUS)) {\n      userUrl = userUrl.replace(JOB_STATUS, jobReport.getJobState().toString());\n    }\n\n    // Create the URL, ensure sanity\n    try {\n      urlToNotify = new URL(userUrl);\n    } catch (MalformedURLException mue) {\n      Log.warn(\"Job end notification couldn't parse \" + userUrl, mue);\n      return;\n    }\n\n    // Send notification\n    boolean success = false;\n    while (numTries-- > 0 && !success) {\n      Log.info(\"Job end notification attempts left \" + numTries);\n      success = notifyURLOnce();\n      if (!success) {\n        Thread.sleep(waitInterval);\n      }\n    }\n    if (!success) {\n      Log.warn(\"Job end notification failed to notify : \" + urlToNotify);\n    } else {\n      Log.info(\"Job end notification succeeded for \" + jobReport.getJobId());\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notifyURLOnce": "  protected boolean notifyURLOnce() {\n    boolean success = false;\n    try {\n      Log.info(\"Job end notification trying \" + urlToNotify);\n      URLConnection conn = urlToNotify.openConnection();\n      conn.setConnectTimeout(5*1000);\n      conn.setReadTimeout(5*1000);\n      conn.setAllowUserInteraction(false);\n      InputStream is = conn.getInputStream();\n      conn.getContent();\n      is.close();\n      success = true;\n      Log.info(\"Job end notification to \" + urlToNotify + \" succeeded\");\n    } catch(IOException ioe) {\n      Log.warn(\"Job end notification to \" + urlToNotify + \" failed\", ioe);\n    }\n    return success;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.Event.toString": "  String toString();\n}",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.Event.getType": "  TYPE getType();\n  long getTimestamp();\n  String toString();\n}"
        },
        "bug_report": {
            "Title": "Second AM fails to recover properly when first AM is killed with java.lang.IllegalArgumentException causing lost job",
            "Description": "Set yarn.resourcemanager.am.max-retries=5 in yarn-site.xml. Started yarn 4 Node cluster.\nFirst Ran Randowriter/Sort/Sort-validate successfully\nThen again sort, when job was 50% complete\nLogin node running AppMaster, and killed AppMaster with kill -9\nOn Client side failed with following:\n{code}\n11/11/23 10:57:27 INFO mapreduce.Job:  map 58% reduce 8%\n11/11/23 10:57:27 INFO mapred.ClientServiceDelegate: Failed to contact AM/History for job job_1322040898409_0005 retrying..\n11/11/23 10:57:28 INFO mapreduce.Job:  map 0% reduce 0%\n11/11/23 10:57:37 INFO mapred.ClientServiceDelegate: Application state is completed. FinalApplicationStatus=UNDEFINED. Redirecting to job history server\n11/11/23 10:57:37 INFO client.ClientTokenSelector: Looking for a token with service <RM Host>:Port\n11/11/23 10:57:37 INFO client.ClientTokenSelector: Token kind is YARN_CLIENT_TOKEN and the token's service name is <New AM Host>:Port\n11/11/23 10:57:38 WARN mapred.ClientServiceDelegate: Error from remote end: Unknown job job_1322040898409_0005\nRemoteTrace: \n at Local Trace: \n\torg.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Unknown job job_1322040898409_0005\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:151)\n\tat $Proxy10.getTaskAttemptCompletionEvents(Unknown Source)\n\tat org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getTaskAttemptCompletionEvents(MRClientProtocolPBClientImpl.java:172)\n\tat sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:273)\n\tat org.apache.hadoop.mapred.ClientServiceDelegate.getTaskCompletionEvents(ClientServiceDelegate.java:320)\n\tat org.apache.hadoop.mapred.YARNRunner.getTaskCompletionEvents(YARNRunner.java:438)\n\tat org.apache.hadoop.mapreduce.Job.getTaskCompletionEvents(Job.java:621)\n\tat org.apache.hadoop.mapreduce.Job.monitorAndPrintJob(Job.java:1231)\n\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1179)\n\tat org.apache.hadoop.examples.Sort.run(Sort.java:181)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)\n\tat org.apache.hadoop.examples.Sort.main(Sort.java:192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)\n\tat org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:68)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:189)\n{code}\n\nOn lookig RM logs found second AM was also lauched, it was saying -:\n{code}\n011-11-23 10:57:37,737 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1322040898409_0005_000002 State change from RUNNING to FINISHED\n2011-11-23 10:57:37,737 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Processing event for application_1322040898409_0005 of type ATTEMPT_FINISHED\n2011-11-23 10:57:37,737 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1322040898409_0005 State change from RUNNING to FINISHED\n2011-11-23 10:57:37,737 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application appattempt_1322040898409_0005_000002 is done. finalState=FINISHED\n{code}\n\nNow looking at AM logs and found Second AM was shutdown gracefully due to :-\n{code}\n2011-11-23 10:57:37,640 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService: Sending assigned event to attempt_1322040898409_0005_m_000000_0\n2011-11-23 10:57:37,641 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread. Exiting..\njava.lang.IllegalArgumentException: Invalid NodeId [<NMHostName>]. Expected host:port\n        at org.apache.hadoop.yarn.util.ConverterUtils.toNodeId(ConverterUtils.java:144)\n        at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.sendAssignedEvent(RecoveryService.java:410)\n        at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:314)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$RequestContainerTransition.transition(TaskAttemptImpl.java:1010)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$RequestContainerTransition.transition(TaskAttemptImpl.java:985)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:851)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:128)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:853)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:845)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)\n        at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:270)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n        at java.lang.Thread.run(Thread.java:619)\n2011-11-23 10:57:37,642 INFO [CompositeServiceShutdownHook for org.apache.hadoop.mapreduce.v2.app.MRAppMaster] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Stopping JobHistoryEventHandler\n{code}"
        }
    },
    {
        "filename": "MAPREDUCE-4164.json",
        "creation_time": "2012-04-17T21:30:13.000+0000",
        "stack_trace": "```\njava.io.IOException: Call to /127.0.0.1:35400 failed on local exception: java.nio.channels.ClosedByInterruptException\nat org.apache.hadoop.ipc.Client.wrapException(Client.java:1094)\nat org.apache.hadoop.ipc.Client.call(Client.java:1062)\nat org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:198)\nat $Proxy0.statusUpdate(Unknown Source)\nat org.apache.hadoop.mapred.Task$TaskReporter.run(Task.java:650)\nat java.lang.Thread.run(Thread.java:662)\nCaused by: java.nio.channels.ClosedByInterruptException\nat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)\nat sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:341)\nat org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:60)\nat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)\nat org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:151)\nat org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:112)\nat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)\nat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)\nat java.io.DataOutputStream.flush(DataOutputStream.java:106)\nat org.apache.hadoop.ipc.Client$Connection.sendParam(Client.java:769)\nat org.apache.hadoop.ipc.Client.call(Client.java:1040)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.call": "  public Writable[] call(Writable[] params, InetSocketAddress[] addresses,\n      Class<?> protocol, UserGroupInformation ticket, Configuration conf)\n      throws IOException, InterruptedException {\n    if (addresses.length == 0) return new Writable[0];\n\n    ParallelResults results = new ParallelResults(params.length);\n    synchronized (results) {\n      for (int i = 0; i < params.length; i++) {\n        ParallelCall call = new ParallelCall(params[i], results, i);\n        try {\n          ConnectionId remoteId = ConnectionId.getConnectionId(addresses[i],\n              protocol, ticket, 0, conf);\n          Connection connection = getConnection(remoteId, call);\n          connection.sendParam(call);             // send each parameter\n        } catch (IOException e) {\n          // log errors\n          LOG.info(\"Calling \"+addresses[i]+\" caught: \" + \n                   e.getMessage(),e);\n          results.size--;                         //  wait for one fewer result\n        }\n      }\n      while (results.count != results.size) {\n        try {\n          results.wait();                    // wait for all results\n        } catch (InterruptedException e) {}\n      }\n\n      return results.values;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnectionId": "    public static ConnectionId getConnectionId(InetSocketAddress addr,\n        Class<?> protocol, UserGroupInformation ticket, int rpcTimeout,\n        Configuration conf) throws IOException {\n      String remotePrincipal = getRemotePrincipal(conf, addr, protocol);\n      boolean doPing =\n        conf.getBoolean(CommonConfigurationKeys.IPC_CLIENT_PING_KEY, true);\n      return new ConnectionId(addr, protocol, ticket,\n          rpcTimeout, remotePrincipal,\n          conf.getInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECTION_MAXIDLETIME_KEY,\n              CommonConfigurationKeysPublic.IPC_CLIENT_CONNECTION_MAXIDLETIME_DEFAULT),\n          conf.getInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY,\n              CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_DEFAULT),\n          conf.getInt(\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_KEY,\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_DEFAULT),\n          conf.getBoolean(CommonConfigurationKeysPublic.IPC_CLIENT_TCPNODELAY_KEY,\n              CommonConfigurationKeysPublic.IPC_CLIENT_TCPNODELAY_DEFAULT),\n          doPing, \n          (doPing ? Client.getPingInterval(conf) : 0));\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.sendParam": "    public void sendParam(Call call) {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n\n      DataOutputBuffer d=null;\n      try {\n        synchronized (this.out) {\n          if (LOG.isDebugEnabled())\n            LOG.debug(getName() + \" sending #\" + call.id);\n          \n          //for serializing the\n          //data to be written\n          d = new DataOutputBuffer();\n          d.writeInt(0); // placeholder for data length\n          RpcPayloadHeader header = new RpcPayloadHeader(\n              call.rpcKind, RpcPayloadOperation.RPC_FINAL_PAYLOAD, call.id);\n          header.write(d);\n          call.rpcRequest.write(d);\n          byte[] data = d.getData();\n          int dataLength = d.getLength() - 4;\n          data[0] = (byte)((dataLength >>> 24) & 0xff);\n          data[1] = (byte)((dataLength >>> 16) & 0xff);\n          data[2] = (byte)((dataLength >>> 8) & 0xff);\n          data[3] = (byte)(dataLength & 0xff);\n          out.write(data, 0, dataLength + 4);//write the data\n          out.flush();\n        }\n      } catch(IOException e) {\n        markClosed(e);\n      } finally {\n        //the buffer is just an in-memory buffer, but it is still polite to\n        // close early\n        IOUtils.closeStream(d);\n      }\n    }  ",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRemoteAddress": "    public InetSocketAddress getRemoteAddress() {\n      return server;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnection": "  private Connection getConnection(ConnectionId remoteId,\n                                   Call call)\n                                   throws IOException, InterruptedException {\n    if (!running.get()) {\n      // the client is stopped\n      throw new IOException(\"The client is stopped\");\n    }\n    Connection connection;\n    /* we could avoid this allocation for each RPC by having a  \n     * connectionsId object and with set() method. We need to manage the\n     * refs for keys in HashMap properly. For now its ok.\n     */\n    do {\n      synchronized (connections) {\n        connection = connections.get(remoteId);\n        if (connection == null) {\n          connection = new Connection(remoteId);\n          connections.put(remoteId, connection);\n        }\n      }\n    } while (!connection.addCall(call));\n    \n    //we don't invoke the method below inside \"synchronized (connections)\"\n    //block above. The reason for that is if the server happens to be slow,\n    //it will take longer to establish a connection and that will slow the\n    //entire system down.\n    connection.setupIOstreams();\n    return connection;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.WritableRpcEngine.invoke": "    public Object invoke(Object proxy, Method method, Object[] args)\n      throws Throwable {\n      long startTime = 0;\n      if (LOG.isDebugEnabled()) {\n        startTime = System.currentTimeMillis();\n      }\n\n      ObjectWritable value = (ObjectWritable)\n        client.call(RpcKind.RPC_WRITABLE, new Invocation(method, args), remoteId);\n      if (LOG.isDebugEnabled()) {\n        long callTime = System.currentTimeMillis() - startTime;\n        LOG.debug(\"Call: \" + method.getName() + \" \" + callTime);\n      }\n      return value.get();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.WritableRpcEngine.call": "      public Writable call(org.apache.hadoop.ipc.RPC.Server server,\n          String protocolName, Writable rpcRequest, long receivedTime)\n          throws IOException {\n        try {\n          Invocation call = (Invocation)rpcRequest;\n          if (server.verbose) log(\"Call: \" + call);\n\n          // Verify rpc version\n          if (call.getRpcVersion() != writableRpcVersion) {\n            // Client is using a different version of WritableRpc\n            throw new IOException(\n                \"WritableRpc version mismatch, client side version=\"\n                    + call.getRpcVersion() + \", server side version=\"\n                    + writableRpcVersion);\n          }\n\n          long clientVersion = call.getProtocolVersion();\n          final String protoName;\n          ProtoClassProtoImpl protocolImpl;\n          if (call.declaringClassProtocolName.equals(VersionedProtocol.class.getName())) {\n            // VersionProtocol methods are often used by client to figure out\n            // which version of protocol to use.\n            //\n            // Versioned protocol methods should go the protocolName protocol\n            // rather than the declaring class of the method since the\n            // the declaring class is VersionedProtocol which is not \n            // registered directly.\n            // Send the call to the highest  protocol version\n            VerProtocolImpl highest = server.getHighestSupportedProtocol(\n                RpcKind.RPC_WRITABLE, protocolName);\n            if (highest == null) {\n              throw new IOException(\"Unknown protocol: \" + protocolName);\n            }\n            protocolImpl = highest.protocolTarget;\n          } else {\n            protoName = call.declaringClassProtocolName;\n\n            // Find the right impl for the protocol based on client version.\n            ProtoNameVer pv = \n                new ProtoNameVer(call.declaringClassProtocolName, clientVersion);\n            protocolImpl = \n                server.getProtocolImplMap(RpcKind.RPC_WRITABLE).get(pv);\n            if (protocolImpl == null) { // no match for Protocol AND Version\n               VerProtocolImpl highest = \n                   server.getHighestSupportedProtocol(RpcKind.RPC_WRITABLE, \n                       protoName);\n              if (highest == null) {\n                throw new IOException(\"Unknown protocol: \" + protoName);\n              } else { // protocol supported but not the version that client wants\n                throw new RPC.VersionMismatch(protoName, clientVersion,\n                  highest.version);\n              }\n            }\n          }\n          \n\n          // Invoke the protocol method\n\n          long startTime = System.currentTimeMillis();\n          Method method = \n              protocolImpl.protocolClass.getMethod(call.getMethodName(),\n              call.getParameterClasses());\n          method.setAccessible(true);\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          Object value = \n              method.invoke(protocolImpl.protocolImpl, call.getParameters());\n          int processingTime = (int) (System.currentTimeMillis() - startTime);\n          int qTime = (int) (startTime-receivedTime);\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Served: \" + call.getMethodName() +\n                      \" queueTime= \" + qTime +\n                      \" procesingTime= \" + processingTime);\n          }\n          server.rpcMetrics.addRpcQueueTime(qTime);\n          server.rpcMetrics.addRpcProcessingTime(processingTime);\n          server.rpcDetailedMetrics.addProcessingTime(call.getMethodName(),\n                                               processingTime);\n          if (server.verbose) log(\"Return: \"+value);\n\n          return new ObjectWritable(method.getReturnType(), value);\n\n        } catch (InvocationTargetException e) {\n          Throwable target = e.getTargetException();\n          if (target instanceof IOException) {\n            throw (IOException)target;\n          } else {\n            IOException ioe = new IOException(target.toString());\n            ioe.setStackTrace(target.getStackTrace());\n            throw ioe;\n          }\n        } catch (Throwable e) {\n          if (!(e instanceof IOException)) {\n            LOG.error(\"Unexpected throwable object \", e);\n          }\n          IOException ioe = new IOException(e.toString());\n          ioe.setStackTrace(e.getStackTrace());\n          throw ioe;\n        }\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketOutputStream.performIO": "    int performIO(ByteBuffer buf) throws IOException {\n      return channel.write(buf);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketOutputStream.write": "  public int write(ByteBuffer src) throws IOException {\n    return writer.doIO(src, SelectionKey.OP_WRITE);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.doIO": "  int doIO(ByteBuffer buf, int ops) throws IOException {\n    \n    /* For now only one thread is allowed. If user want to read or write\n     * from multiple threads, multiple streams could be created. In that\n     * case multiple threads work as well as underlying channel supports it.\n     */\n    if (!buf.hasRemaining()) {\n      throw new IllegalArgumentException(\"Buffer has no data left.\");\n      //or should we just return 0?\n    }\n\n    while (buf.hasRemaining()) {\n      if (closed) {\n        return -1;\n      }\n\n      try {\n        int n = performIO(buf);\n        if (n != 0) {\n          // successful io or an error.\n          return n;\n        }\n      } catch (IOException e) {\n        if (!channel.isOpen()) {\n          closed = true;\n        }\n        throw e;\n      }\n\n      //now wait for socket to be ready.\n      int count = 0;\n      try {\n        count = selector.select(channel, ops, timeout);  \n      } catch (IOException e) { //unexpected IOException.\n        closed = true;\n        throw e;\n      } \n\n      if (count == 0) {\n        throw new SocketTimeoutException(timeoutExceptionString(channel,\n                                                                timeout, ops));\n      }\n      // otherwise the socket should be ready for io.\n    }\n    \n    return 0; // does not reach here.\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.performIO": "  abstract int performIO(ByteBuffer buf) throws IOException;  \n  \n  /**\n   * Performs one IO and returns number of bytes read or written.\n   * It waits up to the specified timeout. If the channel is \n   * not read before the timeout, SocketTimeoutException is thrown.\n   * \n   * @param buf buffer for IO\n   * @param ops Selection Ops used for waiting. Suggested values: \n   *        SelectionKey.OP_READ while reading and SelectionKey.OP_WRITE while\n   *        writing. \n   *        \n   * @return number of bytes read or written. negative implies end of stream.\n   * @throws IOException\n   */\n  int doIO(ByteBuffer buf, int ops) throws IOException {\n    \n    /* For now only one thread is allowed. If user want to read or write\n     * from multiple threads, multiple streams could be created. In that\n     * case multiple threads work as well as underlying channel supports it.\n     */\n    if (!buf.hasRemaining()) {\n      throw new IllegalArgumentException(\"Buffer has no data left.\");\n      //or should we just return 0?\n    }\n\n    while (buf.hasRemaining()) {\n      if (closed) {\n        return -1;\n      }\n\n      try {\n        int n = performIO(buf);\n        if (n != 0) {\n          // successful io or an error.\n          return n;\n        }\n      } catch (IOException e) {\n        if (!channel.isOpen()) {\n          closed = true;\n        }\n        throw e;\n      }\n\n      //now wait for socket to be ready.\n      int count = 0;\n      try {\n        count = selector.select(channel, ops, timeout);  \n      } catch (IOException e) { //unexpected IOException.\n        closed = true;\n        throw e;\n      } \n\n      if (count == 0) {\n        throw new SocketTimeoutException(timeoutExceptionString(channel,\n                                                                timeout, ops));\n      }\n      // otherwise the socket should be ready for io.\n    }\n    \n    return 0; // does not reach here.\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.select": "    int select(SelectableChannel channel, int ops, long timeout) \n                                                   throws IOException {\n     \n      SelectorInfo info = get(channel);\n      \n      SelectionKey key = null;\n      int ret = 0;\n      \n      try {\n        while (true) {\n          long start = (timeout == 0) ? 0 : System.currentTimeMillis();\n\n          key = channel.register(info.selector, ops);\n          ret = info.selector.select(timeout);\n          \n          if (ret != 0) {\n            return ret;\n          }\n          \n          /* Sometimes select() returns 0 much before timeout for \n           * unknown reasons. So select again if required.\n           */\n          if (timeout > 0) {\n            timeout -= System.currentTimeMillis() - start;\n            if (timeout <= 0) {\n              return 0;\n            }\n          }\n          \n          if (Thread.currentThread().isInterrupted()) {\n            throw new InterruptedIOException(\"Interruped while waiting for \" +\n                                             \"IO on channel \" + channel +\n                                             \". \" + timeout + \n                                             \" millis timeout left.\");\n          }\n        }\n      } finally {\n        if (key != null) {\n          key.cancel();\n        }\n        \n        //clear the canceled key.\n        try {\n          info.selector.selectNow();\n        } catch (IOException e) {\n          LOG.info(\"Unexpected Exception while clearing selector : \", e);\n          // don't put the selector back.\n          info.close();\n          return ret; \n        }\n        \n        release(info);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.timeoutExceptionString": "  private static String timeoutExceptionString(SelectableChannel channel,\n                                               long timeout, int ops) {\n    \n    String waitingFor;\n    switch(ops) {\n    \n    case SelectionKey.OP_READ :\n      waitingFor = \"read\"; break;\n      \n    case SelectionKey.OP_WRITE :\n      waitingFor = \"write\"; break;      \n      \n    case SelectionKey.OP_CONNECT :\n      waitingFor = \"connect\"; break;\n      \n    default :\n      waitingFor = \"\" + ops;  \n    }\n    \n    return timeout + \" millis timeout while \" +\n           \"waiting for channel to be ready for \" + \n           waitingFor + \". ch : \" + channel;    \n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.isOpen": "  boolean isOpen() {\n    return !closed && channel.isOpen();\n  }"
        },
        "bug_report": {
            "Title": "Hadoop 22 Exception thrown after task completion causes its reexecution",
            "Description": "2012-02-28 19:17:08,504 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 1969310 bytes\n2012-02-28 19:17:08,694 INFO org.apache.hadoop.mapred.Task: Task:attempt_201202272306_0794_m_000094_0 is done. And is in the process of commiting\n2012-02-28 19:18:08,774 INFO org.apache.hadoop.mapred.Task: Communication exception: java.io.IOException: Call to /127.0.0.1:35400 failed on local exception: java.nio.channels.ClosedByInterruptException\nat org.apache.hadoop.ipc.Client.wrapException(Client.java:1094)\nat org.apache.hadoop.ipc.Client.call(Client.java:1062)\nat org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:198)\nat $Proxy0.statusUpdate(Unknown Source)\nat org.apache.hadoop.mapred.Task$TaskReporter.run(Task.java:650)\nat java.lang.Thread.run(Thread.java:662)\nCaused by: java.nio.channels.ClosedByInterruptException\nat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)\nat sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:341)\nat org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:60)\nat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)\nat org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:151)\nat org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:112)\nat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)\nat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)\nat java.io.DataOutputStream.flush(DataOutputStream.java:106)\nat org.apache.hadoop.ipc.Client$Connection.sendParam(Client.java:769)\nat org.apache.hadoop.ipc.Client.call(Client.java:1040)\n... 4 more\n\n2012-02-28 19:18:08,825 INFO org.apache.hadoop.mapred.Task: Task 'attempt_201202272306_0794_m_000094_0' done.\n\n\n================>>>>>> SHOULD be <++++++++++++++\n2012-02-28 19:17:02,214 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 1974104 bytes\n2012-02-28 19:17:02,408 INFO org.apache.hadoop.mapred.Task: Task:attempt_201202272306_0794_m_000000_0 is done. And is in the process of commiting\n2012-02-28 19:17:02,519 INFO org.apache.hadoop.mapred.Task: Task 'attempt_201202272306_0794_m_000000_0' done. "
        }
    },
    {
        "filename": "MAPREDUCE-5260.json",
        "creation_time": "2013-05-20T05:43:32.000+0000",
        "stack_trace": "```\njava.lang.Throwable: Child Error\n\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.getDetails(JvmManager.java:402)\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:387)\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:192)\n\tat org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:125)\n\tat org.apache.hadoop.mapred.TaskRunner.launchJvmAndWait(TaskRunner.java:292)\n\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:251)\n\n-------\njava.lang.Throwable: Child Error\n\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.getDetails(JvmManager.java:402)\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:387)\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:192)\n\tat org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:125)\n\tat org.apache.hadoop.mapred.TaskRunner.launchJvmAndWait(TaskRunner.java:292)\n\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:251)\n```",
        "source_code": {},
        "bug_report": {
            "Title": "Job failed because of JvmManager running into inconsistent state",
            "Description": "In our cluster, jobs failed due to randomly task initialization failed because of JvmManager running into inconsistent state and TaskTracker failed to exit:\n\njava.lang.Throwable: Child Error\n\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.getDetails(JvmManager.java:402)\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:387)\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:192)\n\tat org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:125)\n\tat org.apache.hadoop.mapred.TaskRunner.launchJvmAndWait(TaskRunner.java:292)\n\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:251)\n\n-------\njava.lang.Throwable: Child Error\n\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.getDetails(JvmManager.java:402)\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:387)\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:192)\n\tat org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:125)\n\tat org.apache.hadoop.mapred.TaskRunner.launchJvmAndWait(TaskRunner.java:292)\n\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:251)"
        }
    },
    {
        "filename": "MAPREDUCE-4774.json",
        "creation_time": "2012-11-06T11:28:43.000+0000",
        "stack_trace": "```\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at FAILED\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:309)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:454)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:716)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:917)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:79)\n        at java.lang.Thread.run(Thread.java:662)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle": "  public void handle(JobEvent event) {\n    LOG.debug(\"Processing \" + event.getJobId() + \" of type \" + event.getType());\n    try {\n      writeLock.lock();\n      JobStateInternal oldState = getInternalState();\n      try {\n         getStateMachine().doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        addDiagnostic(\"Invalid event \" + event.getType() + \n            \" on Job \" + this.jobId);\n        eventHandler.handle(new JobEvent(this.jobId,\n            JobEventType.INTERNAL_ERROR));\n      }\n      //notify the eventhandler of state change\n      if (oldState != getInternalState()) {\n        LOG.info(jobId + \"Job Transitioned from \" + oldState + \" to \"\n                 + getInternalState());\n      }\n    }\n    \n    finally {\n      writeLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getStateMachine": "  protected StateMachine<JobStateInternal, JobEventType, JobEvent> getStateMachine() {\n    return stateMachine;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getInternalState": "  public JobStateInternal getInternalState() {\n    readLock.lock();\n    try {\n     return getStateMachine().getCurrentState();\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.addDiagnostic": "  protected void addDiagnostic(String diag) {\n    diagnostics.add(diag);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.handle": "    public void handle(SpeculatorEvent event) {\n      if (disabled) {\n        return;\n      }\n\n      TaskId tId = event.getTaskID();\n      TaskType tType = null;\n      /* event's TaskId will be null if the event type is JOB_CREATE or\n       * ATTEMPT_STATUS_UPDATE\n       */\n      if (tId != null) {\n        tType = tId.getTaskType(); \n      }\n      boolean shouldMapSpec =\n              conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false);\n      boolean shouldReduceSpec =\n              conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false);\n\n      /* The point of the following is to allow the MAP and REDUCE speculative\n       * config values to be independent:\n       * IF spec-exec is turned on for maps AND the task is a map task\n       * OR IF spec-exec is turned on for reduces AND the task is a reduce task\n       * THEN call the speculator to handle the event.\n       */\n      if ( (shouldMapSpec && (tType == null || tType == TaskType.MAP))\n        || (shouldReduceSpec && (tType == null || tType == TaskType.REDUCE))) {\n        // Speculator IS enabled, direct the event to there.\n        speculator.handle(event);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJob": "    public Job getJob(JobId jobID) {\n      return jobs.get(jobID);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob": "  public void shutDownJob() {\n    // job has finished\n    // this is the only job, so shut down the Appmaster\n    // note in a workflow scenario, this may lead to creation of a new\n    // job (FIXME?)\n    // Send job-end notification\n    if (getConfig().get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL) != null) {\n      try {\n        LOG.info(\"Job end notification started for jobID : \"\n            + job.getReport().getJobId());\n        JobEndNotifier notifier = new JobEndNotifier();\n        notifier.setConf(getConfig());\n        notifier.notify(job.getReport());\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Job end notification interrupted for jobID : \"\n            + job.getReport().getJobId(), ie);\n      }\n    }\n\n    // TODO:currently just wait for some time so clients can know the\n    // final states. Will be removed once RM come on.\n    try {\n      Thread.sleep(5000);\n    } catch (InterruptedException e) {\n      e.printStackTrace();\n    }\n\n    try {\n      //We are finishing cleanly so this is the last retry\n      isLastAMRetry = true;\n      // Stop all services\n      // This will also send the final report to the ResourceManager\n      LOG.info(\"Calling stop for all the services\");\n      MRAppMaster.this.stop();\n\n    } catch (Throwable t) {\n      LOG.warn(\"Graceful stop failed \", t);\n    }\n\n    //Bring the process down by force.\n    //Not needed after HADOOP-7140\n    LOG.info(\"Exiting MR AppMaster..GoodBye!\");\n    sysexit();   \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.start": "  public void start() {\n\n    amInfos = new LinkedList<AMInfo>();\n\n    // Pull completedTasks etc from recovery\n    if (inRecovery) {\n      completedTasksFromPreviousRun = recoveryServ.getCompletedTasks();\n      amInfos = recoveryServ.getAMInfos();\n    } else {\n      // Get the amInfos anyways irrespective of whether recovery is enabled or\n      // not IF this is not the first AM generation\n      if (appAttemptID.getAttemptId() != 1) {\n        amInfos.addAll(readJustAMInfos());\n      }\n    }\n\n    // Current an AMInfo for the current AM generation.\n    AMInfo amInfo =\n        MRBuilderUtils.newAMInfo(appAttemptID, startTime, containerID, nmHost,\n            nmPort, nmHttpPort);\n    amInfos.add(amInfo);\n\n    // /////////////////// Create the job itself.\n    job = createJob(getConfig());\n\n    // End of creating the job.\n\n    // Send out an MR AM inited event for this AM and all previous AMs.\n    for (AMInfo info : amInfos) {\n      dispatcher.getEventHandler().handle(\n          new JobHistoryEvent(job.getID(), new AMStartedEvent(info\n              .getAppAttemptId(), info.getStartTime(), info.getContainerId(),\n              info.getNodeManagerHost(), info.getNodeManagerPort(), info\n                  .getNodeManagerHttpPort())));\n    }\n\n    // metrics system init is really init & start.\n    // It's more test friendly to put it here.\n    DefaultMetricsSystem.initialize(\"MRAppMaster\");\n\n    // create a job event for job intialization\n    JobEvent initJobEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT);\n    // Send init to the job (this does NOT trigger job execution)\n    // This is a synchronous call, not an event through dispatcher. We want\n    // job-init to be done completely here.\n    jobEventDispatcher.handle(initJobEvent);\n\n\n    // JobImpl's InitTransition is done (call above is synchronous), so the\n    // \"uber-decision\" (MR-1220) has been made.  Query job and switch to\n    // ubermode if appropriate (by registering different container-allocator\n    // and container-launcher services/event-handlers).\n\n    if (job.isUber()) {\n      speculatorEventDispatcher.disableSpeculation();\n      LOG.info(\"MRAppMaster uberizing job \" + job.getID()\n               + \" in local container (\\\"uber-AM\\\") on node \"\n               + nmHost + \":\" + nmPort + \".\");\n    } else {\n      // send init to speculator only for non-uber jobs. \n      // This won't yet start as dispatcher isn't started yet.\n      dispatcher.getEventHandler().handle(\n          new SpeculatorEvent(job.getID(), clock.getTime()));\n      LOG.info(\"MRAppMaster launching normal, non-uberized, multi-container \"\n               + \"job \" + job.getID() + \".\");\n    }\n\n    //start all the components\n    super.start();\n\n    // All components have started, start the job.\n    startJobs();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJobId": "  public JobId getJobId() {\n    return jobId;\n  }"
        },
        "bug_report": {
            "Title": "JobImpl does not handle asynchronous task events in FAILED state",
            "Description": "The test org.apache.hadoop.mapred.TestClusterMRNotification.testMR frequently  fails in mapred build (e.g. see https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2988/testReport/junit/org.apache.hadoop.mapred/TestClusterMRNotification/testMR/ , or \nhttps://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2982//testReport/org.apache.hadoop.mapred/TestClusterMRNotification/testMR/).\n\nThe test aims to check Job status notifications received through HTTP Servlet. It runs 3 jobs: successfull, killed, and failed. \nThe test expects the servlet to receive some expected notifications in some expected order. It also tries to test the retry-on-failure notification functionality, so on each 1st notification the servlet answers \"400 forcing error\", and on each 2nd notification attempt it answers \"ok\". \nIn general, the test fails because the actual number and/or type of the notifications differs from the expected.\n\nInvestigation shows that actual root cause of the problem is an incorrect job state transition: the 3rd job mapred task fails (by intentionally thrown  RuntimeException, see UtilsForTests#runJobFail()), and the state of the task changes from RUNNING to FAILED.\nAt this point JobEventType.JOB_TASK_ATTEMPT_COMPLETED event is submitted (in  method org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handleTaskAttemptCompletion(TaskAttemptId, TaskAttemptCompletionEventStatus)), and this event gets processed in AsyncDispatcher, but this transition is impossible according to the event transition map (see org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl#stateMachineFactory). This causes the following exception to be thrown upon the event processing:\n2012-11-06 12:22:02,335 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at FAILED\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:309)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:454)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:716)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:917)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:79)\n        at java.lang.Thread.run(Thread.java:662) \n\nSo, the job gets into state \"INTERNAL_ERROR\", the job end notification like this is sent:\nhttp://localhost:48656/notification/mapred?jobId=job_1352199715842_0002&amp;jobStatus=ERROR \n(here we can see \"ERROR\" status instead of \"FAILED\")\nAfter that the notification servlet receives either only \"ERROR\" notification, or one more notification \"ERROR\" after \"FAILED\", which finally causes the test to fail. (Some variation in the test behavior caused by racing conditions because there are many asynchronous processings there, and the test is flaky, in fact).\n\nIn any way, it looks like the root cause of the problem is the possibility of the forbidden transition \"Invalid event: JOB_TASK_ATTEMPT_COMPLETED at FAILED\". \nNeed an expert advice on how that should be fixed."
        }
    },
    {
        "filename": "MAPREDUCE-3005.json",
        "creation_time": "2011-09-14T04:29:08.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:244)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:206)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:230)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1120)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:961)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:933)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:725)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:577)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:509)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:579)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:620)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:75)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:266)\n        at java.lang.Thread.run(Thread.java:619)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal": "  synchronized private void allocateNodeLocal(SchedulerNode node, Priority priority,\n      ResourceRequest nodeLocalRequest, Container container) {\n    // Update consumption and track allocations\n    allocate(container);\n\n    // Update future requirements\n    nodeLocalRequest.setNumContainers(nodeLocalRequest.getNumContainers() - 1);\n    if (nodeLocalRequest.getNumContainers() == 0) {\n      this.requests.get(priority).remove(node.getHostName());\n    }\n\n    ResourceRequest rackLocalRequest = requests.get(priority).get(\n        node.getRackName());\n    rackLocalRequest.setNumContainers(rackLocalRequest.getNumContainers() - 1);\n    if (rackLocalRequest.getNumContainers() == 0) {\n      this.requests.get(priority).remove(node.getRackName());\n    }\n\n    // Do not remove ANY\n    ResourceRequest offSwitchRequest = requests.get(priority).get(\n        RMNode.ANY);\n    offSwitchRequest.setNumContainers(offSwitchRequest.getNumContainers() - 1);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate": "  synchronized private void allocate(Container container) {\n    // Update consumption and track allocations\n    //TODO: fixme sharad\n    /* try {\n        store.storeContainer(container);\n      } catch (IOException ie) {\n        // TODO fix this. we shouldnt ignore\n      }*/\n    \n    LOG.debug(\"allocate: applicationId=\" + applicationId + \" container=\"\n        + container.getId() + \" host=\"\n        + container.getNodeId().toString());\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate": "  synchronized public RMContainer allocate(NodeType type, SchedulerNode node,\n      Priority priority, ResourceRequest request, \n      Container container) {\n    \n    // Required sanity check - AM can call 'allocate' to update resource \n    // request without locking the scheduler, hence we need to check\n    if (getTotalRequiredResources(priority) <= 0) {\n      return null;\n    }\n    \n    // Create RMContainer\n    RMContainer rmContainer = new RMContainerImpl(container, this\n        .getApplicationAttemptId(), node.getNodeID(), this.rmContext\n        .getDispatcher().getEventHandler(), this.rmContext\n        .getContainerAllocationExpirer());\n\n    // Update consumption and track allocations\n    \n    // Inform the container\n    rmContainer.handle(\n        new RMContainerEvent(container.getId(), RMContainerEventType.START));\n\n    Resources.addTo(currentConsumption, container.getResource());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"allocate: applicationAttemptId=\" \n          + container.getId().getApplicationAttemptId() \n          + \" container=\" + container.getId() + \" host=\"\n          + container.getNodeId().getHost() + \" type=\" + type);\n    }\n    RMAuditLogger.logSuccess(getUser(), \n        AuditConstants.ALLOC_CONTAINER, \"SchedulerApp\", \n        getApplicationId(), container.getId());\n\n    // Add it to allContainers list.\n    newlyAllocatedContainers.add(rmContainer);\n    liveContainers.put(container.getId(), rmContainer);\n    \n    appSchedulingInfo.allocate(type, node, priority, request, container);\n    \n    return rmContainer;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.getApplicationId": "  public ApplicationId getApplicationId() {\n    return this.appSchedulingInfo.getApplicationId();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.getResource": "  public Resource getResource(Priority priority) {\n    return this.appSchedulingInfo.getResource(priority);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.getApplicationAttemptId": "  public ApplicationAttemptId getApplicationAttemptId() {\n    return this.appSchedulingInfo.getApplicationAttemptId();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.getTotalRequiredResources": "  public synchronized int getTotalRequiredResources(Priority priority) {\n    return getResourceRequest(priority, RMNode.ANY).getNumContainers();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.getUser": "  public String getUser() {\n    return this.appSchedulingInfo.getUser();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer": "    public synchronized void assignContainer(Resource resource) {\n      Resources.addTo(consumed, resource);\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getContainer": "  private Container getContainer(RMContainer rmContainer, \n      SchedulerApp application, SchedulerNode node, Resource capability) {\n    return (rmContainer != null) ? rmContainer.getContainer() :\n      createContainer(application, node, capability);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.reserve": "  private void reserve(SchedulerApp application, Priority priority, \n      SchedulerNode node, RMContainer rmContainer, Container container) {\n    rmContainer = application.reserve(node, priority, rmContainer, container);\n    node.reserveResource(application, priority, rmContainer);\n    \n    // Update reserved metrics if this is the first reservation\n    if (rmContainer == null) {\n      getMetrics().reserveResource(\n          application.getUser(), container.getResource());\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getUtilization": "  public synchronized float getUtilization() {\n    return utilization;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.unreserve": "  private void unreserve(SchedulerApp application, Priority priority, \n      SchedulerNode node, RMContainer rmContainer) {\n    // Done with the reservation?\n    application.unreserve(node, priority);\n    node.unreserveResource(application);\n      \n      // Update reserved metrics\n    getMetrics().unreserveResource(\n        application.getUser(), rmContainer.getContainer().getResource());\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.toString": "  public String toString() {\n    return queueName + \":\" + capacity + \":\" + absoluteCapacity + \":\" + \n    getUsedCapacity() + \":\" + getUtilization() + \":\" + \n    getNumApplications() + \":\" + getNumContainers();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers": "  private Resource assignNodeLocalContainers(Resource clusterResource, \n      SchedulerNode node, SchedulerApp application, \n      Priority priority, RMContainer reservedContainer) {\n    ResourceRequest request = \n        application.getResourceRequest(priority, node.getHostName());\n    if (request != null) {\n      if (canAssign(application, priority, node, NodeType.NODE_LOCAL, \n          reservedContainer)) {\n        return assignContainer(clusterResource, node, application, priority, \n            request, NodeType.NODE_LOCAL, reservedContainer);\n      }\n    }\n    \n    return Resources.none();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.canAssign": "  boolean canAssign(SchedulerApp application, Priority priority, \n      SchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Reserved... \n    if (reservedContainer != null) {\n      return true;\n    }\n    \n    // Clearly we need containers for this application...\n    if (type == NodeType.OFF_SWITCH) {\n      // 'Delay' off-switch\n      ResourceRequest offSwitchRequest = \n          application.getResourceRequest(priority, RMNode.ANY);\n      long missedOpportunities = application.getSchedulingOpportunities(priority);\n      long requiredContainers = offSwitchRequest.getNumContainers(); \n      \n      float localityWaitFactor = \n        application.getLocalityWaitFactor(priority, \n            scheduler.getNumClusterNodes());\n      \n      return ((requiredContainers * localityWaitFactor) < missedOpportunities);\n    }\n\n    // Check if we need containers on this rack \n    ResourceRequest rackLocalRequest = \n      application.getResourceRequest(priority, node.getRackName());\n    if (type == NodeType.RACK_LOCAL) {\n      if (rackLocalRequest == null) {\n        return false;\n      } else {\n        return rackLocalRequest.getNumContainers() > 0;      \n      }\n    }\n\n    // Check if we need containers on this host\n    if (type == NodeType.NODE_LOCAL) {\n      // First: Do we need containers on this rack?\n      if (rackLocalRequest != null && rackLocalRequest.getNumContainers() == 0) {\n        return false;\n      }\n      \n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest = \n        application.getResourceRequest(priority, node.getHostName());\n      if (nodeLocalRequest != null) {\n        return nodeLocalRequest.getNumContainers() > 0;\n      }\n    }\n\n    return false;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode": "  private Resource assignContainersOnNode(Resource clusterResource, \n      SchedulerNode node, SchedulerApp application, \n      Priority priority, RMContainer reservedContainer) {\n\n    Resource assigned = Resources.none();\n\n    // Data-local\n    assigned = \n        assignNodeLocalContainers(clusterResource, node, application, priority,\n            reservedContainer); \n    if (Resources.greaterThan(assigned, Resources.none())) {\n      return assigned;\n    }\n\n    // Rack-local\n    assigned = \n        assignRackLocalContainers(clusterResource, node, application, priority, \n            reservedContainer);\n    if (Resources.greaterThan(assigned, Resources.none())) {\n      return assigned;\n    }\n    \n    // Off-switch\n    return assignOffSwitchContainers(clusterResource, node, application, \n        priority, reservedContainer);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignOffSwitchContainers": "  private Resource assignOffSwitchContainers(Resource clusterResource, SchedulerNode node, \n      SchedulerApp application, Priority priority, \n      RMContainer reservedContainer) {\n    ResourceRequest request = \n      application.getResourceRequest(priority, RMNode.ANY);\n    if (request != null) {\n      if (canAssign(application, priority, node, NodeType.OFF_SWITCH, \n          reservedContainer)) {\n        return assignContainer(clusterResource, node, application, priority, request, \n            NodeType.OFF_SWITCH, reservedContainer);\n      }\n    }\n    \n    return Resources.none();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignRackLocalContainers": "  private Resource assignRackLocalContainers(Resource clusterResource,  \n      SchedulerNode node, SchedulerApp application, Priority priority,\n      RMContainer reservedContainer) {\n    ResourceRequest request = \n      application.getResourceRequest(priority, node.getRackName());\n    if (request != null) {\n      if (canAssign(application, priority, node, NodeType.RACK_LOCAL, \n          reservedContainer)) {\n        return assignContainer(clusterResource, node, application, priority, request, \n            NodeType.RACK_LOCAL, reservedContainer);\n      }\n    }\n    return Resources.none();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers": "  public synchronized Resource \n  assignContainers(Resource clusterResource, SchedulerNode node) {\n\n    LOG.info(\"DEBUG --- assignContainers:\" +\n        \" node=\" + node.getHostName() + \n        \" #applications=\" + activeApplications.size());\n    \n    // Check for reserved resources\n    RMContainer reservedContainer = node.getReservedContainer();\n    if (reservedContainer != null) {\n      SchedulerApp application = \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return assignReservedContainer(application, node, reservedContainer, \n          clusterResource);\n    }\n    \n    // Try to assign containers to applications in order\n    for (SchedulerApp application : activeApplications) {\n      \n      LOG.info(\"DEBUG --- pre-assignContainers for application \"\n          + application.getApplicationId());\n      application.showRequests();\n\n      synchronized (application) {\n        Resource userLimit = \n          computeUserLimit(application, clusterResource, Resources.none());\n        setUserResourceLimit(application, userLimit);\n        \n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required = \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n          // Do we need containers at this 'priority'?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Are we going over limits by allocating to this application?\n          // Maximum Capacity of the queue\n          if (!assignToQueue(clusterResource, required)) {\n            return Resources.none();\n          }\n\n          // User limits\n          userLimit = \n            computeUserLimit(application, clusterResource, required); \n          if (!assignToUser(application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          Resource assigned = \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n  \n          // Did we schedule or reserve a container?\n          if (Resources.greaterThan(assigned, Resources.none())) {\n            Resource assignedResource = \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n            // Book-keeping\n            allocateResource(clusterResource, \n                application.getUser(), assignedResource);\n            \n            // Reset scheduling opportunities\n            application.resetSchedulingOpportunities(priority);\n            \n            // Done\n            return assignedResource; \n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      LOG.info(\"DEBUG --- post-assignContainers for application \"\n          + application.getApplicationId());\n      application.showRequests();\n    }\n  \n    return Resources.none();\n\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignToQueue": "  private synchronized boolean assignToQueue(Resource clusterResource, \n      Resource required) {\n    // Check how of the cluster's absolute capacity we are currently using...\n    float potentialNewCapacity = \n      (float)(usedResources.getMemory() + required.getMemory()) / \n        clusterResource.getMemory();\n    if (potentialNewCapacity > absoluteMaxCapacity) {\n      LOG.info(getQueueName() + \n          \" usedResources: \" + usedResources.getMemory() + \n          \" currentCapacity \" + ((float)usedResources.getMemory())/clusterResource.getMemory() + \n          \" required \" + required.getMemory() +\n          \" potentialNewCapacity: \" + potentialNewCapacity + \" ( \" +\n          \" > max-capacity (\" + absoluteMaxCapacity + \")\");\n      return false;\n    }\n    return true;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getUser": "  private synchronized User getUser(String userName) {\n    User user = users.get(userName);\n    if (user == null) {\n      user = new User();\n      users.put(userName, user);\n    }\n    return user;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.needContainers": "  boolean needContainers(SchedulerApp application, Priority priority, Resource required) {\n    int requiredContainers = application.getTotalRequiredResources(priority);\n    int reservedContainers = application.getNumReservedContainers(priority);\n    int starvation = 0;\n    if (reservedContainers > 0) {\n      float nodeFactor = \n          ((float)required.getMemory() / getMaximumAllocation().getMemory());\n      \n      // Use percentage of node required to bias against large containers...\n      // Protect against corner case where you need the whole node with\n      // Math.min(nodeFactor, minimumAllocationFactor)\n      starvation = \n          (int)((application.getReReservations(priority) / (float)reservedContainers) * \n                (1.0f - (Math.min(nodeFactor, getMinimumAllocationFactor())))\n               );\n      \n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"needsContainers:\" +\n            \" app.#re-reserve=\" + application.getReReservations(priority) + \n            \" reserved=\" + reservedContainers + \n            \" nodeFactor=\" + nodeFactor + \n            \" minAllocFactor=\" + minimumAllocationFactor +\n            \" starvation=\" + starvation);\n      }\n    }\n    return (((starvation + requiredContainers) - reservedContainers) > 0);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.setUserResourceLimit": "  private void setUserResourceLimit(SchedulerApp application, \n      Resource resourceLimit) {\n    application.setAvailableResourceLimit(resourceLimit);\n    metrics.setAvailableResourcesToUser(application.getUser(), resourceLimit);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignToUser": "  private synchronized boolean assignToUser(String userName, Resource limit) {\n\n    User user = getUser(userName);\n    \n    // Note: We aren't considering the current request since there is a fixed\n    // overhead of the AM, but it's a >= check, so... \n    if ((user.getConsumedResources().getMemory()) > limit.getMemory()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"User \" + userName + \" in queue \" + getQueueName() + \n            \" will exceed limit - \" +  \n            \" consumed: \" + user.getConsumedResources() + \n            \" limit: \" + limit\n        );\n      }\n      return false;\n    }\n\n    return true;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getApplication": "  private synchronized SchedulerApp getApplication(\n      ApplicationAttemptId applicationAttemptId) {\n    return applicationsMap.get(applicationAttemptId);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.computeUserLimit": "  private Resource computeUserLimit(SchedulerApp application, \n      Resource clusterResource, Resource required) {\n    // What is our current capacity? \n    // * It is equal to the max(required, queue-capacity) if\n    //   we're running below capacity. The 'max' ensures that jobs in queues\n    //   with miniscule capacity (< 1 slot) make progress\n    // * If we're running over capacity, then its\n    //   (usedResources + required) (which extra resources we are allocating)\n\n    // Allow progress for queues with miniscule capacity\n    final int queueCapacity = \n      Math.max(\n          roundUp((int)(absoluteCapacity * clusterResource.getMemory())), \n          required.getMemory());\n\n    final int consumed = usedResources.getMemory();\n    final int currentCapacity = \n      (consumed < queueCapacity) ? \n          queueCapacity : (consumed + required.getMemory());\n\n    // Never allow a single user to take more than the \n    // queue's configured capacity * user-limit-factor.\n    // Also, the queue's configured capacity should be higher than \n    // queue-hard-limit * ulMin\n\n    String userName = application.getUser();\n    \n    final int activeUsers = users.size();  \n    User user = getUser(userName);\n\n    int limit = \n      roundUp(\n          Math.min(\n              Math.max(divideAndCeil(currentCapacity, activeUsers), \n                       divideAndCeil((int)userLimit*currentCapacity, 100)),\n              (int)(queueCapacity * userLimitFactor)\n              )\n          );\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"User limit computation for \" + userName + \n          \" in queue \" + getQueueName() +\n          \" userLimit=\" + userLimit +\n          \" userLimitFactor=\" + userLimitFactor +\n          \" required: \" + required + \n          \" consumed: \" + user.getConsumedResources() + \n          \" limit: \" + limit +\n          \" queueCapacity: \" + queueCapacity + \n          \" qconsumed: \" + consumed +\n          \" currentCapacity: \" + currentCapacity +\n          \" activeUsers: \" + activeUsers +\n          \" clusterCapacity: \" + clusterResource.getMemory()\n      );\n    }\n\n    return Resources.createResource(limit);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.allocateResource": "  synchronized void allocateResource(Resource clusterResource, \n      String userName, Resource resource) {\n    // Update queue metrics\n    Resources.addTo(usedResources, resource);\n    updateResource(clusterResource);\n    ++numContainers;\n\n    // Update user metrics\n    User user = getUser(userName);\n    user.assignContainer(resource);\n    \n    LOG.info(getQueueName() + \n        \" used=\" + usedResources + \" numContainers=\" + numContainers + \n        \" user=\" + userName + \" resources=\" + user.getConsumedResources());\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignReservedContainer": "  private synchronized Resource assignReservedContainer(SchedulerApp application, \n      SchedulerNode node, RMContainer rmContainer, Resource clusterResource) {\n    // Do we still need this reservation?\n    Priority priority = rmContainer.getReservedPriority();\n    if (application.getTotalRequiredResources(priority) == 0) {\n      // Release\n      Container container = rmContainer.getContainer();\n      completedContainer(clusterResource, application, node, \n          rmContainer, \n          SchedulerUtils.createAbnormalContainerStatus(\n              container.getId(), \n              SchedulerUtils.UNRESERVED_CONTAINER), \n          RMContainerEventType.RELEASED);\n      return container.getResource();\n    }\n\n    // Try to assign if we have sufficient resources\n    assignContainersOnNode(clusterResource, node, application, priority, rmContainer);\n    \n    // Doesn't matter... since it's already charged for at time of reservation\n    // \"re-reservation\" is *free*\n    return org.apache.hadoop.yarn.server.resourcemanager.resource.Resource.NONE;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues": "  synchronized Resource assignContainersToChildQueues(Resource cluster, \n      SchedulerNode node) {\n    Resource assigned = Resources.createResource(0);\n    \n    printChildQueues();\n\n    // Try to assign to most 'under-served' sub-queue\n    for (Iterator<CSQueue> iter=childQueues.iterator(); iter.hasNext();) {\n      CSQueue childQueue = iter.next();\n      LOG.info(\"DEBUG --- Trying to assign to\" +\n      \t\t\" queue: \" + childQueue.getQueuePath() + \n      \t\t\" stats: \" + childQueue);\n      assigned = childQueue.assignContainers(cluster, node);\n      LOG.info(\"DEBUG --- Assignedto\" +\n          \" queue: \" + childQueue.getQueuePath() + \n          \" stats: \" + childQueue + \" --> \" + assigned.getMemory());\n\n      // If we do assign, remove the queue and re-insert in-order to re-sort\n      if (Resources.greaterThan(assigned, Resources.none())) {\n        // Remove and re-insert to sort\n        iter.remove();\n        LOG.info(\"Re-sorting queues since queue: \" + childQueue.getQueuePath() + \n            \" stats: \" + childQueue);\n        childQueues.add(childQueue);\n        printChildQueues();\n        break;\n      }\n    }\n    \n    return assigned;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers": "  public synchronized Resource assignContainers(\n      Resource clusterResource, SchedulerNode node) {\n    Resource assigned = Resources.createResource(0);\n\n    while (canAssign(node)) {\n      LOG.info(\"DEBUG --- Trying to assign containers to child-queue of \" + \n          getQueueName());\n      \n      // Are we over maximum-capacity for this queue?\n      if (!assignToQueue(clusterResource)) {\n        break;\n      }\n      \n      // Schedule\n      Resource assignedToChild = \n          assignContainersToChildQueues(clusterResource, node);\n      \n      // Done if no child-queue assigned anything\n      if (Resources.greaterThan(assignedToChild, Resources.none())) {\n        // Track resource utilization for the parent-queue\n        allocateResource(clusterResource, assignedToChild);\n        \n        // Track resource utilization in this pass of the scheduler\n        Resources.addTo(assigned, assignedToChild);\n        \n        LOG.info(\"assignedContainer\" +\n            \" queue=\" + getQueueName() + \n            \" util=\" + getUtilization() + \n            \" used=\" + usedResources + \n            \" cluster=\" + clusterResource);\n\n      } else {\n        break;\n      }\n\n      LOG.info(\"DEBUG ---\" +\n      \t\t\" parentQ=\" + getQueueName() + \n      \t\t\" assignedSoFarInThisIteration=\" + assigned + \n      \t\t\" utilization=\" + getUtilization());\n      \n      // Do not assign more than one container if this isn't the root queue\n      if (!rootQueue) {\n        break;\n      }\n    } \n    \n    return assigned;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.printChildQueues": "  void printChildQueues() {\n    LOG.info(\"DEBUG --- printChildQueues - queue: \" + getQueuePath() + \n        \" child-queues: \" + getChildQueuesToPrint());\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.getQueuePath": "  public String getQueuePath() {\n    String parentPath = ((parent == null) ? \"\" : (parent.getQueuePath() + \".\"));\n    return parentPath + getQueueName();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate": "  private synchronized void nodeUpdate(RMNode nm, \n      List<ContainerStatus> newlyLaunchedContainers,\n      List<ContainerStatus> completedContainers) {\n    LOG.info(\"nodeUpdate: \" + nm + \" clusterResources: \" + clusterResource);\n    \n    SchedulerNode node = getNode(nm.getNodeID());\n\n    // Processing the newly launched containers\n    for (ContainerStatus launchedContainer : newlyLaunchedContainers) {\n      containerLaunchedOnNode(launchedContainer.getContainerId(), node);\n    }\n\n    // Process completed containers\n    for (ContainerStatus completedContainer : completedContainers) {\n      ContainerId containerId = completedContainer.getContainerId();\n      LOG.info(\"DEBUG --- Container FINISHED: \" + containerId);\n      completedContainer(getRMContainer(containerId), \n          completedContainer, RMContainerEventType.FINISHED);\n    }\n\n    // Now node data structures are upto date and ready for scheduling.\n    LOG.info(\"DEBUG -- Node being looked for scheduling \" + nm\n        + \" availableResource: \" + node.getAvailableResource());\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer = node.getReservedContainer();\n    if (reservedContainer != null) {\n      SchedulerApp reservedApplication = \n          getApplication(reservedContainer.getApplicationAttemptId());\n      \n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \" + \n          reservedApplication.getApplicationId() + \" on node: \" + nm);\n      \n      LeafQueue queue = ((LeafQueue)reservedApplication.getQueue());\n      queue.assignContainers(clusterResource, node);\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() == null) {\n      root.assignContainers(clusterResource, node);\n    } else {\n      LOG.info(\"Skipping scheduling since node \" + nm + \n          \" is reserved by application \" + \n          node.getReservedContainer().getContainerId().getApplicationAttemptId()\n          );\n    }\n\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getApplication": "  SchedulerApp getApplication(ApplicationAttemptId applicationAttemptId) {\n    return applications.get(applicationAttemptId);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getAvailableResource": "  public synchronized Resource getAvailableResource(NodeId nodeId) {\n    return nodes.get(nodeId).getAvailableResource();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getRMContainer": "  private RMContainer getRMContainer(ContainerId containerId) {\n    SchedulerApp application = \n        getApplication(containerId.getApplicationAttemptId());\n    return (application == null) ? null : application.getRMContainer(containerId);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getQueue": "  synchronized CSQueue getQueue(String queueName) {\n    return queues.get(queueName);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getNode": "  SchedulerNode getNode(NodeId nodeId) {\n    return nodes.get(nodeId);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainer": "  private synchronized void completedContainer(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) {\n    if (rmContainer == null) {\n      LOG.info(\"Null container completed...\");\n      return;\n    }\n    \n    Container container = rmContainer.getContainer();\n    \n    // Get the application for the finished container\n    ApplicationAttemptId applicationAttemptId = container.getId().getApplicationAttemptId();\n    SchedulerApp application = getApplication(applicationAttemptId);\n    if (application == null) {\n      LOG.info(\"Container \" + container + \" of\" +\n      \t\t\" unknown application \" + applicationAttemptId + \n          \" completed with event \" + event);\n      return;\n    }\n    \n    // Get the node on which the container was allocated\n    SchedulerNode node = getNode(container.getNodeId());\n    \n    // Inform the queue\n    LeafQueue queue = (LeafQueue)application.getQueue();\n    queue.completedContainer(clusterResource, application, node, \n        rmContainer, containerStatus, event);\n\n    LOG.info(\"Application \" + applicationAttemptId + \n        \" released container \" + container.getId() +\n        \" on node: \" + node + \n        \" with event: \" + event);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.containerLaunchedOnNode": "  private void containerLaunchedOnNode(ContainerId containerId, SchedulerNode node) {\n    // Get the application for the finished container\n    ApplicationAttemptId applicationAttemptId = containerId.getApplicationAttemptId();\n    SchedulerApp application = getApplication(applicationAttemptId);\n    if (application == null) {\n      LOG.info(\"Unknown application: \" + applicationAttemptId + \n          \" launched container \" + containerId +\n          \" on node: \" + node);\n      return;\n    }\n    \n    application.containerLaunchedOnNode(containerId);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle": "  public void handle(SchedulerEvent event) {\n    switch(event.getType()) {\n    case NODE_ADDED:\n    {\n      NodeAddedSchedulerEvent nodeAddedEvent = (NodeAddedSchedulerEvent)event;\n      addNode(nodeAddedEvent.getAddedRMNode());\n    }\n    break;\n    case NODE_REMOVED:\n    {\n      NodeRemovedSchedulerEvent nodeRemovedEvent = (NodeRemovedSchedulerEvent)event;\n      removeNode(nodeRemovedEvent.getRemovedRMNode());\n    }\n    break;\n    case NODE_UPDATE:\n    {\n      NodeUpdateSchedulerEvent nodeUpdatedEvent = (NodeUpdateSchedulerEvent)event;\n      nodeUpdate(nodeUpdatedEvent.getRMNode(), \n          nodeUpdatedEvent.getNewlyLaunchedContainers(),\n          nodeUpdatedEvent.getCompletedContainers());\n    }\n    break;\n    case APP_ADDED:\n    {\n      AppAddedSchedulerEvent appAddedEvent = (AppAddedSchedulerEvent)event;\n      addApplication(appAddedEvent.getApplicationAttemptId(), appAddedEvent\n          .getQueue(), appAddedEvent.getUser());\n    }\n    break;\n    case APP_REMOVED:\n    {\n      AppRemovedSchedulerEvent appRemovedEvent = (AppRemovedSchedulerEvent)event;\n      doneApplication(appRemovedEvent.getApplicationAttemptID(),\n          appRemovedEvent.getFinalAttemptState());\n    }\n    break;\n    case CONTAINER_EXPIRED:\n    {\n      ContainerExpiredSchedulerEvent containerExpiredEvent = \n          (ContainerExpiredSchedulerEvent) event;\n      ContainerId containerId = containerExpiredEvent.getContainerId();\n      completedContainer(getRMContainer(containerId), \n          SchedulerUtils.createAbnormalContainerStatus(\n              containerId, \n              SchedulerUtils.EXPIRED_CONTAINER), \n          RMContainerEventType.EXPIRE);\n    }\n    break;\n    default:\n      LOG.error(\"Invalid eventtype \" + event.getType() + \". Ignoring!\");\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplication": "  private synchronized void\n      addApplication(ApplicationAttemptId applicationAttemptId,\n          String queueName, String user) {\n\n    // Sanity checks\n    CSQueue queue = getQueue(queueName);\n    if (queue == null) {\n      String message = \"Application \" + applicationAttemptId + \n      \" submitted by user \" + user + \" to unknown queue: \" + queueName;\n      this.rmContext.getDispatcher().getEventHandler().handle(\n          new RMAppAttemptRejectedEvent(applicationAttemptId, message));\n      return;\n    }\n    if (!(queue instanceof LeafQueue)) {\n      String message = \"Application \" + applicationAttemptId + \n          \" submitted by user \" + user + \" to non-leaf queue: \" + queueName;\n      this.rmContext.getDispatcher().getEventHandler().handle(\n          new RMAppAttemptRejectedEvent(applicationAttemptId, message));\n      return;\n    }\n\n    // TODO: Fix store\n    SchedulerApp SchedulerApp = \n        new SchedulerApp(applicationAttemptId, user, queue, rmContext, null);\n\n    // Submit to the queue\n    try {\n      queue.submitApplication(SchedulerApp, user, queueName);\n    } catch (AccessControlException ace) {\n      LOG.info(\"Failed to submit application \" + applicationAttemptId + \n          \" to queue \" + queueName + \" from user \" + user, ace);\n      this.rmContext.getDispatcher().getEventHandler().handle(\n          new RMAppAttemptRejectedEvent(applicationAttemptId, \n              ace.toString()));\n      return;\n    }\n\n    applications.put(applicationAttemptId, SchedulerApp);\n\n    LOG.info(\"Application Submission: \" + applicationAttemptId + \n        \", user: \" + user +\n        \" queue: \" + queue +\n        \", currently active: \" + applications.size());\n\n    rmContext.getDispatcher().getEventHandler().handle(\n        new RMAppAttemptEvent(applicationAttemptId,\n            RMAppAttemptEventType.APP_ACCEPTED));\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addNode": "  private synchronized void addNode(RMNode nodeManager) {\n    this.nodes.put(nodeManager.getNodeID(), new SchedulerNode(nodeManager));\n    Resources.addTo(clusterResource, nodeManager.getTotalCapability());\n    root.updateClusterResource(clusterResource);\n    ++numNodeManagers;\n    LOG.info(\"Added node \" + nodeManager.getNodeAddress() + \n        \" clusterResource: \" + clusterResource);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.doneApplication": "  private synchronized void doneApplication(\n      ApplicationAttemptId applicationAttemptId,\n      RMAppAttemptState rmAppAttemptFinalState) {\n    LOG.info(\"Application \" + applicationAttemptId + \" is done.\" +\n    \t\t\" finalState=\" + rmAppAttemptFinalState);\n    \n    SchedulerApp application = getApplication(applicationAttemptId);\n\n    if (application == null) {\n      //      throw new IOException(\"Unknown application \" + applicationId + \n      //          \" has completed!\");\n      LOG.info(\"Unknown application \" + applicationAttemptId + \" has completed!\");\n      return;\n    }\n    \n    // Release all the running containers \n    for (RMContainer rmContainer : application.getLiveContainers()) {\n      completedContainer(rmContainer, \n          SchedulerUtils.createAbnormalContainerStatus(\n              rmContainer.getContainerId(), \n              SchedulerUtils.COMPLETED_APPLICATION), \n          RMContainerEventType.KILL);\n    }\n    \n     // Release all reserved containers\n    for (RMContainer rmContainer : application.getAllReservedContainers()) {\n      completedContainer(rmContainer, \n          SchedulerUtils.createAbnormalContainerStatus(\n              rmContainer.getContainerId(), \n              \"Application Complete\"), \n          RMContainerEventType.KILL);\n    }\n    \n    // Clean up pending requests, metrics etc.\n    application.stop(rmAppAttemptFinalState);\n    \n    // Inform the queue\n    String queueName = application.getQueue().getQueueName();\n    CSQueue queue = queues.get(queueName);\n    if (!(queue instanceof LeafQueue)) {\n      LOG.error(\"Cannot finish application \" + \"from non-leaf queue: \"\n          + queueName);\n    } else {\n      queue.finishApplication(application, queue.getQueueName());\n    }\n    \n    // Remove from our data-structure\n    applications.remove(applicationAttemptId);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.removeNode": "  private synchronized void removeNode(RMNode nodeInfo) {\n    SchedulerNode node = this.nodes.get(nodeInfo.getNodeID());\n    Resources.subtractFrom(clusterResource, nodeInfo.getTotalCapability());\n    root.updateClusterResource(clusterResource);\n    --numNodeManagers;\n\n    // Remove running containers\n    List<RMContainer> runningContainers = node.getRunningContainers();\n    for (RMContainer container : runningContainers) {\n      completedContainer(container, \n          SchedulerUtils.createAbnormalContainerStatus(\n              container.getContainerId(), \n              SchedulerUtils.LOST_CONTAINER), \n          RMContainerEventType.KILL);\n    }\n    \n    // Remove reservations, if any\n    RMContainer reservedContainer = node.getReservedContainer();\n    if (reservedContainer != null) {\n      completedContainer(reservedContainer, \n          SchedulerUtils.createAbnormalContainerStatus(\n              reservedContainer.getContainerId(), \n              SchedulerUtils.LOST_CONTAINER), \n          RMContainerEventType.KILL);\n    }\n\n    this.nodes.remove(nodeInfo.getNodeID());\n    LOG.info(\"Removed node \" + nodeInfo.getNodeAddress() + \n        \" clusterResource: \" + clusterResource);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.run": "      public void run() {\n\n        SchedulerEvent event;\n\n        while (!Thread.currentThread().isInterrupted()) {\n          try {\n            event = eventQueue.take();\n          } catch (InterruptedException e) {\n            LOG.error(\"Returning, interrupted : \" + e);\n            return; // TODO: Kill RM.\n          }\n\n          try {\n            scheduler.handle(event);\n          } catch (Throwable t) {\n            LOG.error(\"Error in handling event type \" + event.getType()\n                + \" to the scheduler\", t);\n            return; // TODO: Kill RM.\n          }\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.handle": "    public void handle(RMNodeEvent event) {\n      NodeId nodeId = event.getNodeId();\n      RMNode node = this.rmContext.getRMNodes().get(nodeId);\n      if (node != null) {\n        try {\n          ((EventHandler<RMNodeEvent>) node).handle(event);\n        } catch (Throwable t) {\n          LOG.error(\"Error in handling event type \" + event.getType()\n              + \" for node \" + nodeId, t);\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue.assignContainers": "  public Resource assignContainers(Resource clusterResource, SchedulerNode node);\n  \n  /**\n   * A container assigned to the queue has completed.\n   * @param clusterResource the resource of the cluster\n   * @param application application to which the container was assigned\n   * @param node node on which the container completed\n   * @param container completed container, \n   *                  <code>null</code> if it was just a reservation\n   * @param containerStatus <code>ContainerStatus</code> for the completed \n   *                        container\n   * @param event event to be sent to the container\n   */\n  public void completedContainer(Resource clusterResource,\n      SchedulerApp application, SchedulerNode node, \n      RMContainer container, ContainerStatus containerStatus, \n      RMContainerEventType event);\n\n  /**\n   * Get the number of applications in the queue.\n   * @return number of applications\n   */\n  public int getNumApplications();\n\n  \n  /**\n   * Reinitialize the queue.\n   * @param queue new queue to re-initalize from\n   * @param clusterResource resources in the cluster\n   */\n  public void reinitialize(CSQueue queue, Resource clusterResource) \n  throws IOException;\n\n   /**\n   * Update the cluster resource for queues as we add/remove nodes\n   * @param clusterResource the current cluster resource\n   */\n  public void updateClusterResource(Resource clusterResource);\n  \n  /**\n   * Recover the state of the queue\n   * @param clusterResource the resource of the cluster\n   * @param application the application for which the container was allocated\n   * @param container the container that was recovered.\n   */\n  public void recoverContainer(Resource clusterResource, SchedulerApp application, \n      Container container);\n}"
        },
        "bug_report": {
            "Title": "MR app hangs because of a NPE in ResourceManager",
            "Description": "The app hangs and it turns out to be a NPE in ResourceManager. This happened two of five times on [~karams]'s sort runs on a big cluster.\n{code}\n2011-09-12 15:02:33,715 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler\njava.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:244)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:206)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:230)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1120)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:961)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:933)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:725)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:577)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:509)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:579)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:620)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:75)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:266)\n        at java.lang.Thread.run(Thread.java:619)\n{code}"
        }
    },
    {
        "filename": "MAPREDUCE-6895.json",
        "creation_time": "2017-05-25T21:07:48.000+0000",
        "stack_trace": "```\norg.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:531)\n        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceStop(JobHistoryEventHandler.java:360)\n        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\n        at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)\n        at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)\n        at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)\n        at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStop(MRAppMaster.java:1476)\n        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop(MRAppMaster.java:1090)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob(MRAppMaster.java:554)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler$1.run(MRAppMaster.java:605)\nCaused by: java.nio.channels.ClosedChannelException\n        at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:1528)\n        at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:98)\n        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n        at java.io.DataOutputStream.write(DataOutputStream.java:107)\n        at org.codehaus.jackson.impl.Utf8Generator._flushBuffer(Utf8Generator.java:1754)\n        at org.codehaus.jackson.impl.Utf8Generator.flush(Utf8Generator.java:1088)\n        at org.apache.avro.io.JsonEncoder.flush(JsonEncoder.java:67)\n        at org.apache.hadoop.mapreduce.jobhistory.EventWriter.write(EventWriter.java:67)\n        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo.writeEvent(JobHistoryEventHandler.java:886)\n        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:520)```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent": "  public void handleEvent(JobHistoryEvent event) {\n    synchronized (lock) {\n\n      // If this is JobSubmitted Event, setup the writer\n      if (event.getHistoryEvent().getEventType() == EventType.AM_STARTED) {\n        try {\n          AMStartedEvent amStartedEvent =\n              (AMStartedEvent) event.getHistoryEvent();\n          setupEventWriter(event.getJobID(), amStartedEvent);\n        } catch (IOException ioe) {\n          LOG.error(\"Error JobHistoryEventHandler in handleEvent: \" + event,\n              ioe);\n          throw new YarnRuntimeException(ioe);\n        }\n      }\n\n      // For all events\n      // (1) Write it out\n      // (2) Process it for JobSummary\n      // (3) Process it for ATS (if enabled)\n      MetaInfo mi = fileMap.get(event.getJobID());\n      try {\n        HistoryEvent historyEvent = event.getHistoryEvent();\n        if (! (historyEvent instanceof NormalizedResourceEvent)) {\n          mi.writeEvent(historyEvent);\n        }\n        processEventForJobSummary(event.getHistoryEvent(), mi.getJobSummary(),\n            event.getJobID());\n        if (timelineV2Client != null) {\n          processEventForNewTimelineService(historyEvent, event.getJobID(),\n              event.getTimestamp());\n        } else if (timelineClient != null) {\n          processEventForTimelineServer(historyEvent, event.getJobID(),\n              event.getTimestamp());\n        }\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"In HistoryEventHandler \"\n              + event.getHistoryEvent().getEventType());\n        }\n      } catch (IOException e) {\n        LOG.error(\"Error writing History Event: \" + event.getHistoryEvent(),\n            e);\n        throw new YarnRuntimeException(e);\n      }\n\n      if (event.getHistoryEvent().getEventType() == EventType.JOB_SUBMITTED) {\n        JobSubmittedEvent jobSubmittedEvent =\n            (JobSubmittedEvent) event.getHistoryEvent();\n        mi.getJobIndexInfo().setSubmitTime(jobSubmittedEvent.getSubmitTime());\n        mi.getJobIndexInfo().setQueueName(jobSubmittedEvent.getJobQueueName());\n      }\n      //initialize the launchTime in the JobIndexInfo of MetaInfo\n      if(event.getHistoryEvent().getEventType() == EventType.JOB_INITED ){\n        JobInitedEvent jie = (JobInitedEvent) event.getHistoryEvent();\n        mi.getJobIndexInfo().setJobStartTime(jie.getLaunchTime());\n      }\n      \n      if (event.getHistoryEvent().getEventType() == EventType.JOB_QUEUE_CHANGED) {\n        JobQueueChangeEvent jQueueEvent =\n            (JobQueueChangeEvent) event.getHistoryEvent();\n        mi.getJobIndexInfo().setQueueName(jQueueEvent.getJobQueueName());\n      }\n\n      // If this is JobFinishedEvent, close the writer and setup the job-index\n      if (event.getHistoryEvent().getEventType() == EventType.JOB_FINISHED) {\n        try {\n          JobFinishedEvent jFinishedEvent =\n              (JobFinishedEvent) event.getHistoryEvent();\n          mi.getJobIndexInfo().setFinishTime(jFinishedEvent.getFinishTime());\n          mi.getJobIndexInfo().setNumMaps(jFinishedEvent.getFinishedMaps());\n          mi.getJobIndexInfo().setNumReduces(\n              jFinishedEvent.getFinishedReduces());\n          mi.getJobIndexInfo().setJobStatus(JobState.SUCCEEDED.toString());\n          closeEventWriter(event.getJobID());\n          processDoneFiles(event.getJobID());\n        } catch (IOException e) {\n          throw new YarnRuntimeException(e);\n        }\n      }\n      // In case of JOB_ERROR, only process all the Done files(e.g. job\n      // summary, job history file etc.) if it is last AM retry.\n      if (event.getHistoryEvent().getEventType() == EventType.JOB_ERROR) {\n        try {\n          JobUnsuccessfulCompletionEvent jucEvent =\n              (JobUnsuccessfulCompletionEvent) event.getHistoryEvent();\n          mi.getJobIndexInfo().setFinishTime(jucEvent.getFinishTime());\n          mi.getJobIndexInfo().setNumMaps(jucEvent.getFinishedMaps());\n          mi.getJobIndexInfo().setNumReduces(jucEvent.getFinishedReduces());\n          mi.getJobIndexInfo().setJobStatus(jucEvent.getStatus());\n          closeEventWriter(event.getJobID());\n          if(context.isLastAMRetry())\n            processDoneFiles(event.getJobID());\n        } catch (IOException e) {\n          throw new YarnRuntimeException(e);\n        }\n      }\n\n      if (event.getHistoryEvent().getEventType() == EventType.JOB_FAILED\n          || event.getHistoryEvent().getEventType() == EventType.JOB_KILLED) {\n        try {\n          JobUnsuccessfulCompletionEvent jucEvent = \n              (JobUnsuccessfulCompletionEvent) event\n              .getHistoryEvent();\n          mi.getJobIndexInfo().setFinishTime(jucEvent.getFinishTime());\n          mi.getJobIndexInfo().setNumMaps(jucEvent.getFinishedMaps());\n          mi.getJobIndexInfo().setNumReduces(jucEvent.getFinishedReduces());\n          mi.getJobIndexInfo().setJobStatus(jucEvent.getStatus());\n          closeEventWriter(event.getJobID());\n          processDoneFiles(event.getJobID());\n        } catch (IOException e) {\n          throw new YarnRuntimeException(e);\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.setupEventWriter": "  protected void setupEventWriter(JobId jobId, AMStartedEvent amStartedEvent)\n      throws IOException {\n    if (stagingDirPath == null) {\n      LOG.error(\"Log Directory is null, returning\");\n      throw new IOException(\"Missing Log Directory for History\");\n    }\n\n    MetaInfo oldFi = fileMap.get(jobId);\n    Configuration conf = getConfig();\n\n    // TODO Ideally this should be written out to the job dir\n    // (.staging/jobid/files - RecoveryService will need to be patched)\n    Path historyFile = JobHistoryUtils.getStagingJobHistoryFile(\n        stagingDirPath, jobId, startCount);\n    String user = UserGroupInformation.getCurrentUser().getShortUserName();\n    if (user == null) {\n      throw new IOException(\n          \"User is null while setting up jobhistory eventwriter\");\n    }\n\n    String jobName = context.getJob(jobId).getName();\n    EventWriter writer = (oldFi == null) ? null : oldFi.writer;\n \n    Path logDirConfPath =\n        JobHistoryUtils.getStagingConfFile(stagingDirPath, jobId, startCount);\n    if (writer == null) {\n      try {\n        writer = createEventWriter(historyFile);\n        LOG.info(\"Event Writer setup for JobId: \" + jobId + \", File: \"\n            + historyFile);\n      } catch (IOException ioe) {\n        LOG.info(\"Could not create log file: [\" + historyFile + \"] + for job \"\n            + \"[\" + jobName + \"]\");\n        throw ioe;\n      }\n      \n      //Write out conf only if the writer isn't already setup.\n      if (conf != null) {\n        // TODO Ideally this should be written out to the job dir\n        // (.staging/jobid/files - RecoveryService will need to be patched)\n        if (logDirConfPath != null) {\n          Configuration redactedConf = new Configuration(conf);\n          MRJobConfUtil.redact(redactedConf);\n          try (FSDataOutputStream jobFileOut = stagingDirFS\n              .create(logDirConfPath, true)) {\n            redactedConf.writeXml(jobFileOut);\n          } catch (IOException e) {\n            LOG.info(\"Failed to write the job configuration file\", e);\n            throw e;\n          }\n        }\n      }\n    }\n\n    String queueName = JobConf.DEFAULT_QUEUE_NAME;\n    if (conf != null) {\n      queueName = conf.get(MRJobConfig.QUEUE_NAME, JobConf.DEFAULT_QUEUE_NAME);\n    }\n\n    MetaInfo fi = new MetaInfo(historyFile, logDirConfPath, writer,\n        user, jobName, jobId, amStartedEvent.getForcedJobStateOnShutDown(),\n        queueName);\n    fi.getJobSummary().setJobId(jobId);\n    fi.getJobSummary().setJobLaunchTime(amStartedEvent.getStartTime());\n    fi.getJobSummary().setJobSubmitTime(amStartedEvent.getSubmitTime());\n    fi.getJobIndexInfo().setJobStartTime(amStartedEvent.getStartTime());\n    fi.getJobIndexInfo().setSubmitTime(amStartedEvent.getSubmitTime());\n    fileMap.put(jobId, fi);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.closeEventWriter": "  protected void closeEventWriter(JobId jobId) throws IOException {\n    final MetaInfo mi = fileMap.get(jobId);\n    if (mi == null) {\n      throw new IOException(\"No MetaInfo found for JobId: [\" + jobId + \"]\");\n    }\n\n    if (!mi.isWriterActive()) {\n      throw new IOException(\n          \"Inactive Writer: Likely received multiple JobFinished / \" +\n          \"JobUnsuccessful events for JobId: [\"\n              + jobId + \"]\");\n    }\n\n    // Close the Writer\n    try {\n      mi.closeWriter();\n    } catch (IOException e) {\n      LOG.error(\"Error closing writer for JobID: \" + jobId);\n      throw e;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.writeEvent": "    void writeEvent(HistoryEvent event) throws IOException {\n      LOG.debug(\"Writing event\");\n      synchronized (lock) {\n        if (writer != null) {\n          writer.write(event);\n          processEventForFlush(event);\n          maybeFlush(event);\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.processDoneFiles": "  protected void processDoneFiles(JobId jobId) throws IOException {\n\n    final MetaInfo mi = fileMap.get(jobId);\n    if (mi == null) {\n      throw new IOException(\"No MetaInfo found for JobId: [\" + jobId + \"]\");\n    }\n\n    if (mi.getHistoryFile() == null) {\n      LOG.warn(\"No file for job-history with \" + jobId + \" found in cache!\");\n    }\n    if (mi.getConfFile() == null) {\n      LOG.warn(\"No file for jobconf with \" + jobId + \" found in cache!\");\n    }\n      \n    // Writing out the summary file.\n    // TODO JH enhancement - reuse this file to store additional indexing info\n    // like ACLs, etc. JHServer can use HDFS append to build an index file\n    // with more info than is available via the filename.\n    Path qualifiedSummaryDoneFile = null;\n    FSDataOutputStream summaryFileOut = null;\n    try {\n      String doneSummaryFileName = getTempFileName(JobHistoryUtils\n          .getIntermediateSummaryFileName(jobId));\n      qualifiedSummaryDoneFile = doneDirFS.makeQualified(new Path(\n          doneDirPrefixPath, doneSummaryFileName));\n      summaryFileOut = doneDirFS.create(qualifiedSummaryDoneFile, true);\n      summaryFileOut.writeUTF(mi.getJobSummary().getJobSummaryString());\n      summaryFileOut.close();\n      doneDirFS.setPermission(qualifiedSummaryDoneFile, new FsPermission(\n          JobHistoryUtils.HISTORY_INTERMEDIATE_FILE_PERMISSIONS));\n    } catch (IOException e) {\n      LOG.info(\"Unable to write out JobSummaryInfo to [\"\n          + qualifiedSummaryDoneFile + \"]\", e);\n      throw e;\n    }\n\n    try {\n\n      // Move historyFile to Done Folder.\n      Path qualifiedDoneFile = null;\n      if (mi.getHistoryFile() != null) {\n        Path historyFile = mi.getHistoryFile();\n        Path qualifiedLogFile = stagingDirFS.makeQualified(historyFile);\n        int jobNameLimit =\n            getConfig().getInt(JHAdminConfig.MR_HS_JOBNAME_LIMIT,\n            JHAdminConfig.DEFAULT_MR_HS_JOBNAME_LIMIT);\n        String doneJobHistoryFileName =\n            getTempFileName(FileNameIndexUtils.getDoneFileName(mi\n                .getJobIndexInfo(), jobNameLimit));\n        qualifiedDoneFile =\n            doneDirFS.makeQualified(new Path(doneDirPrefixPath,\n                doneJobHistoryFileName));\n        moveToDoneNow(qualifiedLogFile, qualifiedDoneFile);\n      }\n\n      // Move confFile to Done Folder\n      Path qualifiedConfDoneFile = null;\n      if (mi.getConfFile() != null) {\n        Path confFile = mi.getConfFile();\n        Path qualifiedConfFile = stagingDirFS.makeQualified(confFile);\n        String doneConfFileName =\n            getTempFileName(JobHistoryUtils\n                .getIntermediateConfFileName(jobId));\n        qualifiedConfDoneFile =\n            doneDirFS.makeQualified(new Path(doneDirPrefixPath,\n                doneConfFileName));\n        moveToDoneNow(qualifiedConfFile, qualifiedConfDoneFile);\n      }\n      \n      moveTmpToDone(qualifiedSummaryDoneFile);\n      moveTmpToDone(qualifiedConfDoneFile);\n      moveTmpToDone(qualifiedDoneFile);\n\n    } catch (IOException e) {\n      LOG.error(\"Error closing writer for JobID: \" + jobId);\n      throw e;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.getJobIndexInfo": "    JobIndexInfo getJobIndexInfo() {\n      return jobIndexInfo;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.processEventForNewTimelineService": "  private void processEventForNewTimelineService(HistoryEvent event,\n      JobId jobId, long timestamp) {\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity tEntity =\n        null;\n    String taskId = null;\n    String taskAttemptId = null;\n    boolean setCreatedTime = false;\n\n    switch (event.getEventType()) {\n    // Handle job events\n    case JOB_SUBMITTED:\n      setCreatedTime = true;\n      break;\n    case JOB_STATUS_CHANGED:\n    case JOB_INFO_CHANGED:\n    case JOB_INITED:\n    case JOB_PRIORITY_CHANGED:\n    case JOB_QUEUE_CHANGED:\n    case JOB_FAILED:\n    case JOB_KILLED:\n    case JOB_ERROR:\n    case JOB_FINISHED:\n    case AM_STARTED:\n    case NORMALIZED_RESOURCE:\n      break;\n    // Handle task events\n    case TASK_STARTED:\n      setCreatedTime = true;\n      taskId = ((TaskStartedEvent)event).getTaskId().toString();\n      break;\n    case TASK_FAILED:\n      taskId = ((TaskFailedEvent)event).getTaskId().toString();\n      break;\n    case TASK_UPDATED:\n      taskId = ((TaskUpdatedEvent)event).getTaskId().toString();\n      break;\n    case TASK_FINISHED:\n      taskId = ((TaskFinishedEvent)event).getTaskId().toString();\n      break;\n    case MAP_ATTEMPT_STARTED:\n    case REDUCE_ATTEMPT_STARTED:\n      setCreatedTime = true;\n      taskId = ((TaskAttemptStartedEvent)event).getTaskId().toString();\n      taskAttemptId = ((TaskAttemptStartedEvent)event).\n          getTaskAttemptId().toString();\n      break;\n    case CLEANUP_ATTEMPT_STARTED:\n    case SETUP_ATTEMPT_STARTED:\n      taskId = ((TaskAttemptStartedEvent)event).getTaskId().toString();\n      taskAttemptId = ((TaskAttemptStartedEvent)event).\n          getTaskAttemptId().toString();\n      break;\n    case MAP_ATTEMPT_FAILED:\n    case CLEANUP_ATTEMPT_FAILED:\n    case REDUCE_ATTEMPT_FAILED:\n    case SETUP_ATTEMPT_FAILED:\n    case MAP_ATTEMPT_KILLED:\n    case CLEANUP_ATTEMPT_KILLED:\n    case REDUCE_ATTEMPT_KILLED:\n    case SETUP_ATTEMPT_KILLED:\n      taskId = ((TaskAttemptUnsuccessfulCompletionEvent)event).\n          getTaskId().toString();\n      taskAttemptId = ((TaskAttemptUnsuccessfulCompletionEvent)event).\n          getTaskAttemptId().toString();\n      break;\n    case MAP_ATTEMPT_FINISHED:\n      taskId = ((MapAttemptFinishedEvent)event).getTaskId().toString();\n      taskAttemptId = ((MapAttemptFinishedEvent)event).\n          getAttemptId().toString();\n      break;\n    case REDUCE_ATTEMPT_FINISHED:\n      taskId = ((ReduceAttemptFinishedEvent)event).getTaskId().toString();\n      taskAttemptId = ((ReduceAttemptFinishedEvent)event).\n          getAttemptId().toString();\n      break;\n    case SETUP_ATTEMPT_FINISHED:\n    case CLEANUP_ATTEMPT_FINISHED:\n      taskId = ((TaskAttemptFinishedEvent)event).getTaskId().toString();\n      taskAttemptId = ((TaskAttemptFinishedEvent)event).\n          getAttemptId().toString();\n      break;\n    default:\n      LOG.warn(\"EventType: \" + event.getEventType() + \" cannot be recognized\" +\n          \" and handled by timeline service.\");\n      return;\n    }\n\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity\n        appEntityWithJobMetrics = null;\n    if (taskId == null) {\n      // JobEntity\n      tEntity = createJobEntity(event, timestamp, jobId,\n          MAPREDUCE_JOB_ENTITY_TYPE, setCreatedTime);\n      if (event.getEventType() == EventType.JOB_FINISHED\n          && event.getTimelineMetrics() != null) {\n        appEntityWithJobMetrics = createAppEntityWithJobMetrics(event, jobId);\n      }\n    } else {\n      if (taskAttemptId == null) {\n        // TaskEntity\n        tEntity = createTaskEntity(event, timestamp, taskId,\n            MAPREDUCE_TASK_ENTITY_TYPE, MAPREDUCE_JOB_ENTITY_TYPE,\n            jobId, setCreatedTime);\n      } else {\n        // TaskAttemptEntity\n        tEntity = createTaskAttemptEntity(event, timestamp, taskAttemptId,\n            MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE, MAPREDUCE_TASK_ENTITY_TYPE,\n            taskId, setCreatedTime);\n      }\n    }\n    try {\n      if (appEntityWithJobMetrics == null) {\n        timelineV2Client.putEntitiesAsync(tEntity);\n      } else {\n        timelineV2Client.putEntities(tEntity, appEntityWithJobMetrics);\n      }\n    } catch (IOException | YarnException e) {\n      LOG.error(\"Failed to process Event \" + event.getEventType()\n          + \" for the job : \" + jobId, e);\n      return;\n    }\n    if (event.getEventType() == EventType.JOB_SUBMITTED) {\n      // Publish configs after main job submitted event has been posted.\n      publishConfigsOnJobSubmittedEvent((JobSubmittedEvent)event, jobId);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.getJobSummary": "    JobSummary getJobSummary() {\n      return jobSummary;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.processEventForTimelineServer": "  private void processEventForTimelineServer(HistoryEvent event, JobId jobId,\n          long timestamp) {\n    TimelineEvent tEvent = new TimelineEvent();\n    tEvent.setEventType(StringUtils.toUpperCase(event.getEventType().name()));\n    tEvent.setTimestamp(timestamp);\n    TimelineEntity tEntity = new TimelineEntity();\n\n    switch (event.getEventType()) {\n      case JOB_SUBMITTED:\n        JobSubmittedEvent jse =\n            (JobSubmittedEvent) event;\n        tEvent.addEventInfo(\"SUBMIT_TIME\", jse.getSubmitTime());\n        tEvent.addEventInfo(\"QUEUE_NAME\", jse.getJobQueueName());\n        tEvent.addEventInfo(\"JOB_NAME\", jse.getJobName());\n        tEvent.addEventInfo(\"USER_NAME\", jse.getUserName());\n        tEvent.addEventInfo(\"JOB_CONF_PATH\", jse.getJobConfPath());\n        tEvent.addEventInfo(\"ACLS\", jse.getJobAcls());\n        tEvent.addEventInfo(\"JOB_QUEUE_NAME\", jse.getJobQueueName());\n        tEvent.addEventInfo(\"WORKFLOW_ID\", jse.getWorkflowId());\n        tEvent.addEventInfo(\"WORKFLOW_NAME\", jse.getWorkflowName());\n        tEvent.addEventInfo(\"WORKFLOW_NAME_NAME\", jse.getWorkflowNodeName());\n        tEvent.addEventInfo(\"WORKFLOW_ADJACENCIES\",\n                jse.getWorkflowAdjacencies());\n        tEvent.addEventInfo(\"WORKFLOW_TAGS\", jse.getWorkflowTags());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(jobId.toString());\n        tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\n        break;\n      case JOB_STATUS_CHANGED:\n        JobStatusChangedEvent jsce = (JobStatusChangedEvent) event;\n        tEvent.addEventInfo(\"STATUS\", jsce.getStatus());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(jobId.toString());\n        tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\n        break;\n      case JOB_INFO_CHANGED:\n        JobInfoChangeEvent jice = (JobInfoChangeEvent) event;\n        tEvent.addEventInfo(\"SUBMIT_TIME\", jice.getSubmitTime());\n        tEvent.addEventInfo(\"LAUNCH_TIME\", jice.getLaunchTime());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(jobId.toString());\n        tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\n        break;\n      case JOB_INITED:\n        JobInitedEvent jie = (JobInitedEvent) event;\n        tEvent.addEventInfo(\"START_TIME\", jie.getLaunchTime());\n        tEvent.addEventInfo(\"STATUS\", jie.getStatus());\n        tEvent.addEventInfo(\"TOTAL_MAPS\", jie.getTotalMaps());\n        tEvent.addEventInfo(\"TOTAL_REDUCES\", jie.getTotalReduces());\n        tEvent.addEventInfo(\"UBERIZED\", jie.getUberized());\n        tEntity.setStartTime(jie.getLaunchTime());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(jobId.toString());\n        tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\n        break;\n      case JOB_PRIORITY_CHANGED:\n        JobPriorityChangeEvent jpce = (JobPriorityChangeEvent) event;\n        tEvent.addEventInfo(\"PRIORITY\", jpce.getPriority().toString());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(jobId.toString());\n        tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\n        break;\n      case JOB_QUEUE_CHANGED:\n        JobQueueChangeEvent jqe = (JobQueueChangeEvent) event;\n        tEvent.addEventInfo(\"QUEUE_NAMES\", jqe.getJobQueueName());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(jobId.toString());\n        tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\n        break;\n      case JOB_FAILED:\n      case JOB_KILLED:\n      case JOB_ERROR:\n        JobUnsuccessfulCompletionEvent juce =\n              (JobUnsuccessfulCompletionEvent) event;\n        tEvent.addEventInfo(\"FINISH_TIME\", juce.getFinishTime());\n        tEvent.addEventInfo(\"NUM_MAPS\", juce.getFinishedMaps());\n        tEvent.addEventInfo(\"NUM_REDUCES\", juce.getFinishedReduces());\n        tEvent.addEventInfo(\"JOB_STATUS\", juce.getStatus());\n        tEvent.addEventInfo(\"DIAGNOSTICS\", juce.getDiagnostics());\n        tEvent.addEventInfo(\"FINISHED_MAPS\", juce.getFinishedMaps());\n        tEvent.addEventInfo(\"FINISHED_REDUCES\", juce.getFinishedReduces());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(jobId.toString());\n        tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\n        break;\n      case JOB_FINISHED:\n        JobFinishedEvent jfe = (JobFinishedEvent) event;\n        tEvent.addEventInfo(\"FINISH_TIME\", jfe.getFinishTime());\n        tEvent.addEventInfo(\"NUM_MAPS\", jfe.getFinishedMaps());\n        tEvent.addEventInfo(\"NUM_REDUCES\", jfe.getFinishedReduces());\n        tEvent.addEventInfo(\"FAILED_MAPS\", jfe.getFailedMaps());\n        tEvent.addEventInfo(\"FAILED_REDUCES\", jfe.getFailedReduces());\n        tEvent.addEventInfo(\"FINISHED_MAPS\", jfe.getFinishedMaps());\n        tEvent.addEventInfo(\"FINISHED_REDUCES\", jfe.getFinishedReduces());\n        tEvent.addEventInfo(\"MAP_COUNTERS_GROUPS\",\n            JobHistoryEventUtils.countersToJSON(jfe.getMapCounters()));\n        tEvent.addEventInfo(\"REDUCE_COUNTERS_GROUPS\",\n            JobHistoryEventUtils.countersToJSON(jfe.getReduceCounters()));\n        tEvent.addEventInfo(\"TOTAL_COUNTERS_GROUPS\",\n            JobHistoryEventUtils.countersToJSON(jfe.getTotalCounters()));\n        tEvent.addEventInfo(\"JOB_STATUS\", JobState.SUCCEEDED.toString());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(jobId.toString());\n        tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\n        break;\n      case TASK_STARTED:\n        TaskStartedEvent tse = (TaskStartedEvent) event;\n        tEvent.addEventInfo(\"TASK_TYPE\", tse.getTaskType().toString());\n        tEvent.addEventInfo(\"START_TIME\", tse.getStartTime());\n        tEvent.addEventInfo(\"SPLIT_LOCATIONS\", tse.getSplitLocations());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(tse.getTaskId().toString());\n        tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\n        tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\n        break;\n      case TASK_FAILED:\n        TaskFailedEvent tfe = (TaskFailedEvent) event;\n        tEvent.addEventInfo(\"TASK_TYPE\", tfe.getTaskType().toString());\n        tEvent.addEventInfo(\"STATUS\", TaskStatus.State.FAILED.toString());\n        tEvent.addEventInfo(\"FINISH_TIME\", tfe.getFinishTime());\n        tEvent.addEventInfo(\"ERROR\", tfe.getError());\n        tEvent.addEventInfo(\"FAILED_ATTEMPT_ID\",\n                tfe.getFailedAttemptID() == null ?\n                \"\" : tfe.getFailedAttemptID().toString());\n        tEvent.addEventInfo(\"COUNTERS_GROUPS\",\n            JobHistoryEventUtils.countersToJSON(tfe.getCounters()));\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(tfe.getTaskId().toString());\n        tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\n        tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\n        break;\n      case TASK_UPDATED:\n        TaskUpdatedEvent tue = (TaskUpdatedEvent) event;\n        tEvent.addEventInfo(\"FINISH_TIME\", tue.getFinishTime());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(tue.getTaskId().toString());\n        tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\n        tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\n        break;\n      case TASK_FINISHED:\n        TaskFinishedEvent tfe2 = (TaskFinishedEvent) event;\n        tEvent.addEventInfo(\"TASK_TYPE\", tfe2.getTaskType().toString());\n        tEvent.addEventInfo(\"COUNTERS_GROUPS\",\n            JobHistoryEventUtils.countersToJSON(tfe2.getCounters()));\n        tEvent.addEventInfo(\"FINISH_TIME\", tfe2.getFinishTime());\n        tEvent.addEventInfo(\"STATUS\", TaskStatus.State.SUCCEEDED.toString());\n        tEvent.addEventInfo(\"SUCCESSFUL_TASK_ATTEMPT_ID\",\n            tfe2.getSuccessfulTaskAttemptId() == null ?\n            \"\" : tfe2.getSuccessfulTaskAttemptId().toString());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(tfe2.getTaskId().toString());\n        tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\n        tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\n        break;\n      case MAP_ATTEMPT_STARTED:\n      case CLEANUP_ATTEMPT_STARTED:\n      case REDUCE_ATTEMPT_STARTED:\n      case SETUP_ATTEMPT_STARTED:\n        TaskAttemptStartedEvent tase = (TaskAttemptStartedEvent) event;\n        tEvent.addEventInfo(\"TASK_TYPE\", tase.getTaskType().toString());\n        tEvent.addEventInfo(\"TASK_ATTEMPT_ID\",\n            tase.getTaskAttemptId().toString());\n        tEvent.addEventInfo(\"START_TIME\", tase.getStartTime());\n        tEvent.addEventInfo(\"HTTP_PORT\", tase.getHttpPort());\n        tEvent.addEventInfo(\"TRACKER_NAME\", tase.getTrackerName());\n        tEvent.addEventInfo(\"SHUFFLE_PORT\", tase.getShufflePort());\n        tEvent.addEventInfo(\"CONTAINER_ID\", tase.getContainerId() == null ?\n            \"\" : tase.getContainerId().toString());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(tase.getTaskId().toString());\n        tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\n        tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\n        break;\n      case MAP_ATTEMPT_FAILED:\n      case CLEANUP_ATTEMPT_FAILED:\n      case REDUCE_ATTEMPT_FAILED:\n      case SETUP_ATTEMPT_FAILED:\n      case MAP_ATTEMPT_KILLED:\n      case CLEANUP_ATTEMPT_KILLED:\n      case REDUCE_ATTEMPT_KILLED:\n      case SETUP_ATTEMPT_KILLED:\n        TaskAttemptUnsuccessfulCompletionEvent tauce =\n                (TaskAttemptUnsuccessfulCompletionEvent) event;\n        tEvent.addEventInfo(\"TASK_TYPE\", tauce.getTaskType().toString());\n        tEvent.addEventInfo(\"TASK_ATTEMPT_ID\",\n            tauce.getTaskAttemptId() == null ?\n            \"\" : tauce.getTaskAttemptId().toString());\n        tEvent.addEventInfo(\"FINISH_TIME\", tauce.getFinishTime());\n        tEvent.addEventInfo(\"ERROR\", tauce.getError());\n        tEvent.addEventInfo(\"STATUS\", tauce.getTaskStatus());\n        tEvent.addEventInfo(\"HOSTNAME\", tauce.getHostname());\n        tEvent.addEventInfo(\"PORT\", tauce.getPort());\n        tEvent.addEventInfo(\"RACK_NAME\", tauce.getRackName());\n        tEvent.addEventInfo(\"SHUFFLE_FINISH_TIME\", tauce.getFinishTime());\n        tEvent.addEventInfo(\"SORT_FINISH_TIME\", tauce.getFinishTime());\n        tEvent.addEventInfo(\"MAP_FINISH_TIME\", tauce.getFinishTime());\n        tEvent.addEventInfo(\"COUNTERS_GROUPS\",\n            JobHistoryEventUtils.countersToJSON(tauce.getCounters()));\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(tauce.getTaskId().toString());\n        tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\n        tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\n        break;\n      case MAP_ATTEMPT_FINISHED:\n        MapAttemptFinishedEvent mafe = (MapAttemptFinishedEvent) event;\n        tEvent.addEventInfo(\"TASK_TYPE\", mafe.getTaskType().toString());\n        tEvent.addEventInfo(\"FINISH_TIME\", mafe.getFinishTime());\n        tEvent.addEventInfo(\"STATUS\", mafe.getTaskStatus());\n        tEvent.addEventInfo(\"STATE\", mafe.getState());\n        tEvent.addEventInfo(\"MAP_FINISH_TIME\", mafe.getMapFinishTime());\n        tEvent.addEventInfo(\"COUNTERS_GROUPS\",\n            JobHistoryEventUtils.countersToJSON(mafe.getCounters()));\n        tEvent.addEventInfo(\"HOSTNAME\", mafe.getHostname());\n        tEvent.addEventInfo(\"PORT\", mafe.getPort());\n        tEvent.addEventInfo(\"RACK_NAME\", mafe.getRackName());\n        tEvent.addEventInfo(\"ATTEMPT_ID\", mafe.getAttemptId() == null ?\n            \"\" : mafe.getAttemptId().toString());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(mafe.getTaskId().toString());\n        tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\n        tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\n        break;\n      case REDUCE_ATTEMPT_FINISHED:\n        ReduceAttemptFinishedEvent rafe = (ReduceAttemptFinishedEvent) event;\n        tEvent.addEventInfo(\"TASK_TYPE\", rafe.getTaskType().toString());\n        tEvent.addEventInfo(\"ATTEMPT_ID\", rafe.getAttemptId() == null ?\n            \"\" : rafe.getAttemptId().toString());\n        tEvent.addEventInfo(\"FINISH_TIME\", rafe.getFinishTime());\n        tEvent.addEventInfo(\"STATUS\", rafe.getTaskStatus());\n        tEvent.addEventInfo(\"STATE\", rafe.getState());\n        tEvent.addEventInfo(\"SHUFFLE_FINISH_TIME\", rafe.getShuffleFinishTime());\n        tEvent.addEventInfo(\"SORT_FINISH_TIME\", rafe.getSortFinishTime());\n        tEvent.addEventInfo(\"COUNTERS_GROUPS\",\n            JobHistoryEventUtils.countersToJSON(rafe.getCounters()));\n        tEvent.addEventInfo(\"HOSTNAME\", rafe.getHostname());\n        tEvent.addEventInfo(\"PORT\", rafe.getPort());\n        tEvent.addEventInfo(\"RACK_NAME\", rafe.getRackName());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(rafe.getTaskId().toString());\n        tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\n        tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\n        break;\n      case SETUP_ATTEMPT_FINISHED:\n      case CLEANUP_ATTEMPT_FINISHED:\n        TaskAttemptFinishedEvent tafe = (TaskAttemptFinishedEvent) event;\n        tEvent.addEventInfo(\"TASK_TYPE\", tafe.getTaskType().toString());\n        tEvent.addEventInfo(\"ATTEMPT_ID\", tafe.getAttemptId() == null ?\n            \"\" : tafe.getAttemptId().toString());\n        tEvent.addEventInfo(\"FINISH_TIME\", tafe.getFinishTime());\n        tEvent.addEventInfo(\"STATUS\", tafe.getTaskStatus());\n        tEvent.addEventInfo(\"STATE\", tafe.getState());\n        tEvent.addEventInfo(\"COUNTERS_GROUPS\",\n            JobHistoryEventUtils.countersToJSON(tafe.getCounters()));\n        tEvent.addEventInfo(\"HOSTNAME\", tafe.getHostname());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(tafe.getTaskId().toString());\n        tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\n        tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\n        break;\n      case AM_STARTED:\n        AMStartedEvent ase = (AMStartedEvent) event;\n        tEvent.addEventInfo(\"APPLICATION_ATTEMPT_ID\",\n                ase.getAppAttemptId() == null ?\n                \"\" : ase.getAppAttemptId().toString());\n        tEvent.addEventInfo(\"CONTAINER_ID\", ase.getContainerId() == null ?\n                \"\" : ase.getContainerId().toString());\n        tEvent.addEventInfo(\"NODE_MANAGER_HOST\", ase.getNodeManagerHost());\n        tEvent.addEventInfo(\"NODE_MANAGER_PORT\", ase.getNodeManagerPort());\n        tEvent.addEventInfo(\"NODE_MANAGER_HTTP_PORT\",\n                ase.getNodeManagerHttpPort());\n        tEvent.addEventInfo(\"START_TIME\", ase.getStartTime());\n        tEvent.addEventInfo(\"SUBMIT_TIME\", ase.getSubmitTime());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(jobId.toString());\n        tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\n        break;\n      default:\n        break;\n    }\n\n    try {\n      TimelinePutResponse response = timelineClient.putEntities(tEntity);\n      List<TimelinePutResponse.TimelinePutError> errors = response.getErrors();\n      if (errors.size() == 0) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Timeline entities are successfully put in event \" + event\n              .getEventType());\n        }\n      } else {\n        for (TimelinePutResponse.TimelinePutError error : errors) {\n          LOG.error(\n              \"Error when publishing entity [\" + error.getEntityType() + \",\"\n                  + error.getEntityId() + \"], server side error code: \"\n                  + error.getErrorCode());\n        }\n      }\n    } catch (YarnException | IOException | ClientHandlerException ex) {\n      LOG.error(\"Error putting entity \" + tEntity.getEntityId() + \" to Timeline\"\n          + \"Server\", ex);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.processEventForJobSummary": "  public void processEventForJobSummary(HistoryEvent event, JobSummary summary, \n      JobId jobId) {\n    // context.getJob could be used for some of this info as well.\n    switch (event.getEventType()) {\n    case JOB_SUBMITTED:\n      JobSubmittedEvent jse = (JobSubmittedEvent) event;\n      summary.setUser(jse.getUserName());\n      summary.setQueue(jse.getJobQueueName());\n      summary.setJobSubmitTime(jse.getSubmitTime());\n      summary.setJobName(jse.getJobName());\n      break;\n    case NORMALIZED_RESOURCE:\n      NormalizedResourceEvent normalizedResourceEvent = \n            (NormalizedResourceEvent) event;\n      if (normalizedResourceEvent.getTaskType() == TaskType.MAP) {\n        summary.setResourcesPerMap((int) normalizedResourceEvent.getMemory());\n      } else if (normalizedResourceEvent.getTaskType() == TaskType.REDUCE) {\n        summary.setResourcesPerReduce((int) normalizedResourceEvent.getMemory());\n      }\n      break;  \n    case JOB_INITED:\n      JobInitedEvent jie = (JobInitedEvent) event;\n      summary.setJobLaunchTime(jie.getLaunchTime());\n      break;\n    case MAP_ATTEMPT_STARTED:\n      TaskAttemptStartedEvent mtase = (TaskAttemptStartedEvent) event;\n      if (summary.getFirstMapTaskLaunchTime() == 0)\n        summary.setFirstMapTaskLaunchTime(mtase.getStartTime());\n      break;\n    case REDUCE_ATTEMPT_STARTED:\n      TaskAttemptStartedEvent rtase = (TaskAttemptStartedEvent) event;\n      if (summary.getFirstReduceTaskLaunchTime() == 0)\n        summary.setFirstReduceTaskLaunchTime(rtase.getStartTime());\n      break;\n    case JOB_FINISHED:\n      JobFinishedEvent jfe = (JobFinishedEvent) event;\n      summary.setJobFinishTime(jfe.getFinishTime());\n      summary.setNumFinishedMaps(jfe.getFinishedMaps());\n      summary.setNumFailedMaps(jfe.getFailedMaps());\n      summary.setNumFinishedReduces(jfe.getFinishedReduces());\n      summary.setNumFailedReduces(jfe.getFailedReduces());\n      if (summary.getJobStatus() == null)\n        summary\n            .setJobStatus(org.apache.hadoop.mapreduce.JobStatus.State.SUCCEEDED\n                .toString());\n      // TODO JOB_FINISHED does not have state. Effectively job history does not\n      // have state about the finished job.\n      setSummarySlotSeconds(summary, jfe.getTotalCounters());\n      break;\n    case JOB_FAILED:\n    case JOB_KILLED:\n      JobUnsuccessfulCompletionEvent juce = (JobUnsuccessfulCompletionEvent) event;\n      summary.setJobStatus(juce.getStatus());\n      summary.setNumFinishedMaps(context.getJob(jobId).getTotalMaps());\n      summary.setNumFinishedReduces(context.getJob(jobId).getTotalReduces());\n      summary.setJobFinishTime(juce.getFinishTime());\n      setSummarySlotSeconds(summary, context.getJob(jobId).getAllCounters());\n      break;\n    default:\n      break;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceStop": "  protected void serviceStop() throws Exception {\n    LOG.info(\"Stopping JobHistoryEventHandler. \"\n        + \"Size of the outstanding queue size is \" + eventQueue.size());\n    stopped = true;\n    //do not interrupt while event handling is in progress\n    synchronized(lock) {\n      if (eventHandlingThread != null) {\n        LOG.debug(\"Interrupting Event Handling thread\");\n        eventHandlingThread.interrupt();\n      } else {\n        LOG.debug(\"Null event handling thread\");\n      }\n    }\n\n    try {\n      if (eventHandlingThread != null) {\n        LOG.debug(\"Waiting for Event Handling thread to complete\");\n        eventHandlingThread.join();\n      }\n    } catch (InterruptedException ie) {\n      LOG.info(\"Interrupted Exception while stopping\", ie);\n    }\n\n    // Cancel all timers - so that they aren't invoked during or after\n    // the metaInfo object is wrapped up.\n    for (MetaInfo mi : fileMap.values()) {\n      try {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Shutting down timer for \" + mi);\n        }\n        mi.shutDownTimer();\n      } catch (IOException e) {\n        LOG.info(\"Exception while canceling delayed flush timer. \"\n            + \"Likely caused by a failed flush \" + e.getMessage());\n      }\n    }\n\n    //write all the events remaining in queue\n    Iterator<JobHistoryEvent> it = eventQueue.iterator();\n    while(it.hasNext()) {\n      JobHistoryEvent ev = it.next();\n      LOG.info(\"In stop, writing event \" + ev.getType());\n      handleEvent(ev);\n    }\n\n    // Process JobUnsuccessfulCompletionEvent for jobIds which still haven't\n    // closed their event writers\n    if(forceJobCompletion) {\n      for (Map.Entry<JobId,MetaInfo> jobIt : fileMap.entrySet()) {\n        JobId toClose = jobIt.getKey();\n        MetaInfo mi = jobIt.getValue();\n        if(mi != null && mi.isWriterActive()) {\n          LOG.warn(\"Found jobId \" + toClose\n            + \" to have not been closed. Will close\");\n          //Create a JobFinishEvent so that it is written to the job history\n          final Job job = context.getJob(toClose);\n          JobUnsuccessfulCompletionEvent jucEvent =\n            new JobUnsuccessfulCompletionEvent(TypeConverter.fromYarn(toClose),\n                System.currentTimeMillis(), job.getCompletedMaps(),\n                job.getCompletedReduces(),\n                createJobStateForJobUnsuccessfulCompletionEvent(\n                    mi.getForcedJobStateOnShutDown()),\n                job.getDiagnostics());\n          JobHistoryEvent jfEvent = new JobHistoryEvent(toClose, jucEvent);\n          //Bypass the queue mechanism which might wait. Call the method directly\n          handleEvent(jfEvent);\n        }\n      }\n    }\n\n    //close all file handles\n    for (MetaInfo mi : fileMap.values()) {\n      try {\n        mi.closeWriter();\n      } catch (IOException e) {\n        LOG.info(\"Exception while closing file \" + e.getMessage());\n      }\n    }\n    if (timelineClient != null) {\n      timelineClient.stop();\n    } else if (timelineV2Client != null) {\n      timelineV2Client.stop();\n    }\n    LOG.info(\"Stopped JobHistoryEventHandler. super.stop()\");\n    super.serviceStop();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.closeWriter": "    void closeWriter() throws IOException {\n      LOG.debug(\"Closing Writer\");\n      synchronized (lock) {\n        if (writer != null) {\n          writer.close();\n        }\n        writer = null;\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.isWriterActive": "    boolean isWriterActive() {\n      return writer != null;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.getForcedJobStateOnShutDown": "    String getForcedJobStateOnShutDown() {\n      return forcedJobStateOnShutDown;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.stop": "    public void stop() {\n      shouldRun = false;\n      this.cancel();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.createJobStateForJobUnsuccessfulCompletionEvent": "  private String createJobStateForJobUnsuccessfulCompletionEvent(\n      String forcedJobStateOnShutDown) {\n    if (forcedJobStateOnShutDown == null || forcedJobStateOnShutDown\n        .isEmpty()) {\n      return JobState.KILLED.toString();\n    } else if (forcedJobStateOnShutDown.equals(\n        JobStateInternal.ERROR.toString()) ||\n        forcedJobStateOnShutDown.equals(JobStateInternal.FAILED.toString())) {\n      return JobState.FAILED.toString();\n    } else if (forcedJobStateOnShutDown.equals(JobStateInternal.SUCCEEDED\n        .toString())) {\n      return JobState.SUCCEEDED.toString();\n    }\n    return JobState.KILLED.toString();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.shutDownTimer": "    void shutDownTimer() throws IOException {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Shutting down timer \"+ toString());\n      }\n      synchronized (lock) {\n        isTimerShutDown = true;\n        flushTimer.cancel();\n        if (flushTimerTask != null && flushTimerTask.getException() != null) {\n          throw flushTimerTask.getException();\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.stop": "  public void stop() {\n    if (isInState(STATE.STOPPED)) {\n      return;\n    }\n    synchronized (stateChangeLock) {\n      if (enterState(STATE.STOPPED) != STATE.STOPPED) {\n        try {\n          serviceStop();\n        } catch (Exception e) {\n          //stop-time exceptions are logged if they are the first one,\n          noteFailure(e);\n          throw ServiceStateException.convert(e);\n        } finally {\n          //report that the service has terminated\n          terminationNotification.set(true);\n          synchronized (terminationNotification) {\n            terminationNotification.notifyAll();\n          }\n          //notify anything listening for events\n          notifyListeners();\n        }\n      } else {\n        //already stopped: note it\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Ignoring re-entrant call to stop()\");\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.isInState": "  public final boolean isInState(Service.STATE expected) {\n    return stateModel.isInState(expected);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.notifyListeners": "  private void notifyListeners() {\n    try {\n      listeners.notifyListeners(this);\n      globalListeners.notifyListeners(this);\n    } catch (Throwable e) {\n      LOG.warn(\"Exception while notifying listeners of \" + this + \": \" + e,\n               e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.enterState": "  private STATE enterState(STATE newState) {\n    assert stateModel != null : \"null state in \" + name + \" \" + this.getClass();\n    STATE oldState = stateModel.enterState(newState);\n    if (oldState != newState) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\n          \"Service: \" + getName() + \" entered state \" + getServiceState());\n      }\n      recordLifecycleEvent();\n    }\n    return oldState;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.serviceStop": "  protected void serviceStop() throws Exception {\n\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.noteFailure": "  protected final void noteFailure(Exception exception) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"noteFailure \" + exception, null);\n    }\n    if (exception == null) {\n      //make sure failure logic doesn't itself cause problems\n      return;\n    }\n    //record the failure details, and log it\n    synchronized (this) {\n      if (failureCause == null) {\n        failureCause = exception;\n        failureState = getServiceState();\n        LOG.info(\"Service \" + getName()\n                 + \" failed in state \" + failureState\n                 + \"; cause: \" + exception,\n                 exception);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceOperations.stop": "  public static void stop(Service service) {\n    if (service != null) {\n      service.stop();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceOperations.stopQuietly": "  public static Exception stopQuietly(Log log, Service service) {\n    try {\n      stop(service);\n    } catch (Exception e) {\n      log.warn(\"When stopping the service \" + service.getName()\n               + \" : \" + e,\n               e);\n      return e;\n    }\n    return null;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.CompositeService.stop": "  private void stop(int numOfServicesStarted, boolean stopOnlyStartedServices) {\n    // stop in reverse order of start\n    Exception firstException = null;\n    List<Service> services = getServices();\n    for (int i = numOfServicesStarted - 1; i >= 0; i--) {\n      Service service = services.get(i);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Stopping service #\" + i + \": \" + service);\n      }\n      STATE state = service.getServiceState();\n      //depending on the stop police\n      if (state == STATE.STARTED \n         || (!stopOnlyStartedServices && state == STATE.INITED)) {\n        Exception ex = ServiceOperations.stopQuietly(LOG, service);\n        if (ex != null && firstException == null) {\n          firstException = ex;\n        }\n      }\n    }\n    //after stopping all services, rethrow the first exception raised\n    if (firstException != null) {\n      throw ServiceStateException.convert(firstException);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.CompositeService.getServices": "  public List<Service> getServices() {\n    synchronized (serviceList) {\n      return new ArrayList<Service>(serviceList);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.CompositeService.serviceStop": "  protected void serviceStop() throws Exception {\n    //stop all services that were started\n    int numOfServicesToStop = serviceList.size();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(getName() + \": stopping services, size=\" + numOfServicesToStop);\n    }\n    stop(numOfServicesToStop, STOP_ONLY_STARTED_SERVICES);\n    super.serviceStop();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStop": "  protected void serviceStop() throws Exception {\n    super.serviceStop();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.cleanupStagingDir": "  public void cleanupStagingDir() throws IOException {\n    /* make sure we clean the staging files */\n    String jobTempDir = getConfig().get(MRJobConfig.MAPREDUCE_JOB_DIR);\n    FileSystem fs = getFileSystem(getConfig());\n    try {\n      if (!keepJobFiles(new JobConf(getConfig()), jobTempDir)) {\n        if (jobTempDir == null) {\n          LOG.warn(\"Job Staging directory is null\");\n          return;\n        }\n        Path jobTempDirPath = new Path(jobTempDir);\n        LOG.info(\"Deleting staging directory \" + FileSystem.getDefaultUri(getConfig()) +\n            \" \" + jobTempDir);\n        fs.delete(jobTempDirPath, true);\n      }\n    } catch(IOException io) {\n      LOG.error(\"Failed to cleanup staging dir \" + jobTempDir, io);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop": "  public void stop() {\n    super.stop();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob": "  public void shutDownJob() {\n    // job has finished\n    // this is the only job, so shut down the Appmaster\n    // note in a workflow scenario, this may lead to creation of a new\n    // job (FIXME?)\n\n    try {\n      //if isLastAMRetry comes as true, should never set it to false\n      if ( !isLastAMRetry){\n        if (((JobImpl)job).getInternalState() != JobStateInternal.REBOOT) {\n          LOG.info(\"Job finished cleanly, recording last MRAppMaster retry\");\n          isLastAMRetry = true;\n        }\n      }\n      notifyIsLastAMRetry(isLastAMRetry);\n      // Stop all services\n      // This will also send the final report to the ResourceManager\n      LOG.info(\"Calling stop for all the services\");\n      MRAppMaster.this.stop();\n\n      if (isLastAMRetry) {\n        // Send job-end notification when it is safe to report termination to\n        // users and it is the last AM retry\n        if (getConfig().get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL) != null) {\n          try {\n            LOG.info(\"Job end notification started for jobID : \"\n                + job.getReport().getJobId());\n            JobEndNotifier notifier = new JobEndNotifier();\n            notifier.setConf(getConfig());\n            JobReport report = job.getReport();\n            // If unregistration fails, the final state is unavailable. However,\n            // at the last AM Retry, the client will finally be notified FAILED\n            // from RM, so we should let users know FAILED via notifier as well\n            if (!context.hasSuccessfullyUnregistered()) {\n              report.setJobState(JobState.FAILED);\n            }\n            notifier.notify(report);\n          } catch (InterruptedException ie) {\n            LOG.warn(\"Job end notification interrupted for jobID : \"\n                + job.getReport().getJobId(), ie);\n          }\n        }\n      }\n\n      try {\n        Thread.sleep(5000);\n      } catch (InterruptedException e) {\n        e.printStackTrace();\n      }\n      clientService.stop();\n    } catch (Throwable t) {\n      LOG.warn(\"Graceful stop failed. Exiting.. \", t);\n      exitMRAppMaster(1, t);\n    }\n    exitMRAppMaster(0, null);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.notifyIsLastAMRetry": "  public void notifyIsLastAMRetry(boolean isLastAMRetry){\n    if(containerAllocator instanceof ContainerAllocatorRouter) {\n      LOG.info(\"Notify RMCommunicator isAMLastRetry: \" + isLastAMRetry);\n      ((ContainerAllocatorRouter) containerAllocator)\n        .setShouldUnregister(isLastAMRetry);\n    }\n    if(jobHistoryEventHandler != null) {\n      LOG.info(\"Notify JHEH isAMLastRetry: \" + isLastAMRetry);\n      jobHistoryEventHandler.setForcejobCompletion(isLastAMRetry);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.exitMRAppMaster": "  private void exitMRAppMaster(int status, Throwable t) {\n    if (!mainStarted) {\n      ExitUtil.disableSystemExit();\n    }\n    try {\n      if (t != null) {\n        ExitUtil.terminate(status, t);\n      } else {\n        ExitUtil.terminate(status);\n      }\n    } catch (ExitUtil.ExitException ee) {\n      // ExitUtil.ExitException is only thrown from the ExitUtil test code when\n      // SystemExit has been disabled. It is always thrown in in the test code,\n      // even when no error occurs. Ignore the exception so that tests don't\n      // need to handle it.\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.hasSuccessfullyUnregistered": "    public boolean hasSuccessfullyUnregistered() {\n      return successfullyUnregistered.get();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJobId": "  public JobId getJobId() {\n    return jobId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.run": "      public Object run() throws Exception {\n        appMaster.init(conf);\n        appMaster.start();\n        if(appMaster.errorHappenedShutDown) {\n          throw new IOException(\"Was asked to shut down.\");\n        }\n        return null;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FSOutputSummer.write": "  public synchronized void write(byte b[], int off, int len)\n      throws IOException {\n    \n    checkClosed();\n    \n    if (off < 0 || len < 0 || off > b.length - len) {\n      throw new ArrayIndexOutOfBoundsException();\n    }\n\n    for (int n=0;n<len;n+=write1(b, off+n, len-n)) {\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FSOutputSummer.flushBuffer": "  protected synchronized int flushBuffer(boolean keep,\n      boolean flushPartial) throws IOException {\n    int bufLen = count;\n    int partialLen = bufLen % sum.getBytesPerChecksum();\n    int lenToFlush = flushPartial ? bufLen : bufLen - partialLen;\n    if (lenToFlush != 0) {\n      writeChecksumChunks(buf, 0, lenToFlush);\n      if (!flushPartial || keep) {\n        count = partialLen;\n        System.arraycopy(buf, bufLen - count, buf, 0, count);\n      } else {\n        count = 0;\n      }\n    }\n\n    // total bytes left minus unflushed bytes left\n    return count - (bufLen - lenToFlush);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FSOutputSummer.checkClosed": "  protected abstract void checkClosed() throws IOException;\n\n  /** Write one byte */\n  @Override\n  public synchronized void write(int b) throws IOException {\n    buf[count++] = (byte)b;\n    if(count == buf.length) {\n      flushBuffer();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FSOutputSummer.write1": "  private int write1(byte b[], int off, int len) throws IOException {\n    if(count==0 && len>=buf.length) {\n      // local buffer is empty and user buffer size >= local buffer size, so\n      // simply checksum the user buffer and send it directly to the underlying\n      // stream\n      final int length = buf.length;\n      writeChecksumChunks(b, off, length);\n      return length;\n    }\n    \n    // copy user data to local buffer\n    int bytesToCopy = buf.length-count;\n    bytesToCopy = (len<bytesToCopy) ? len : bytesToCopy;\n    System.arraycopy(b, off, buf, count, bytesToCopy);\n    count += bytesToCopy;\n    if (count == buf.length) {\n      // local buffer is full\n      flushBuffer();\n    } \n    return bytesToCopy;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FSDataOutputStream.write": "    public void write(byte b[], int off, int len) throws IOException {\n      out.write(b, off, len);\n      position += len;                            // update position\n      if (statistics != null) {\n        statistics.incrementBytesWritten(len);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.EventWriter.write": "  synchronized void write(HistoryEvent event) throws IOException { \n    Event wrapper = new Event();\n    wrapper.setType(event.getEventType());\n    wrapper.setEvent(event.getDatum());\n    writer.write(wrapper, encoder);\n    encoder.flush();\n    if (this.jsonOutput) {\n      out.writeBytes(\"\\n\");\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.EventWriter.flush": "  void flush() throws IOException {\n    encoder.flush();\n    out.flush();\n    out.hflush();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobSubmittedEvent.getJobQueueName": "  public String getJobQueueName() {\n    if (datum.getJobQueueName() != null) {\n      return datum.getJobQueueName().toString();\n    }\n    return null;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobFinishedEvent.getFinishedReduces": "  public int getFinishedReduces() { return finishedReduces; }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent.getStatus": "  public String getStatus() { return datum.getJobStatus().toString(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent.getFinishedMaps": "  public int getFinishedMaps() { return datum.getFinishedMaps(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobQueueChangeEvent.getJobQueueName": "  public String getJobQueueName() {\n    if (datum.jobQueueName != null) {\n      return datum.jobQueueName.toString();\n    }\n    return null;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobFinishedEvent.getFinishedMaps": "  public int getFinishedMaps() { return finishedMaps; }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobSubmittedEvent.getSubmitTime": "  public long getSubmitTime() { return datum.getSubmitTime(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent.getFinishTime": "  public long getFinishTime() { return datum.getFinishTime(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEvent.getJobID": "  public JobId getJobID() {\n    return jobID;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent.getFinishedReduces": "  public int getFinishedReduces() { return datum.getFinishedReduces(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEvent.getHistoryEvent": "  public HistoryEvent getHistoryEvent() {\n    return historyEvent;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobFinishedEvent.getFinishTime": "  public long getFinishTime() { return finishTime; }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobInitedEvent.getLaunchTime": "  public long getLaunchTime() { return datum.getLaunchTime(); }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceStateException.convert": "  public static RuntimeException convert(String text, Throwable fault) {\n    if (fault instanceof RuntimeException) {\n      return (RuntimeException) fault;\n    } else {\n      return new ServiceStateException(text, fault);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.service.getName": "  String getName();\n\n  /**\n   * Get the configuration of this service.\n   * This is normally not a clone and may be manipulated, though there are no\n   * guarantees as to what the consequences of such actions may be\n   * @return the current configuration, unless a specific implentation chooses\n   * otherwise.\n   */\n  Configuration getConfig();\n\n  /**\n   * Get the current service state\n   * @return the state of the service\n   */\n  STATE getServiceState();\n\n  /**\n   * Get the service start time\n   * @return the start time of the service. This will be zero if the service\n   * has not yet been started.\n   */\n  long getStartTime();\n\n  /**\n   * Query to see if the service is in a specific state.\n   * In a multi-threaded system, the state may not hold for very long.\n   * @param state the expected state\n   * @return true if, at the time of invocation, the service was in that state.\n   */\n  boolean isInState(STATE state);\n\n  /**\n   * Get the first exception raised during the service failure. If null,\n   * no exception was logged\n   * @return the failure logged during a transition to the stopped state\n   */\n  Throwable getFailureCause();\n\n  /**\n   * Get the state in which the failure in {@link #getFailureCause()} occurred.",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.Service.getServiceState": "  STATE getServiceState();\n\n  /**\n   * Get the service start time\n   * @return the start time of the service. This will be zero if the service\n   * has not yet been started.\n   */\n  long getStartTime();\n\n  /**\n   * Query to see if the service is in a specific state.\n   * In a multi-threaded system, the state may not hold for very long.\n   * @param state the expected state\n   * @return true if, at the time of invocation, the service was in that state.\n   */\n  boolean isInState(STATE state);\n\n  /**\n   * Get the first exception raised during the service failure. If null,\n   * no exception was logged\n   * @return the failure logged during a transition to the stopped state\n   */\n  Throwable getFailureCause();\n\n  /**\n   * Get the state in which the failure in {@link #getFailureCause()} occurred.",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notify": "  public void notify(JobReport jobReport)\n    throws InterruptedException {\n    // Do we need job-end notification?\n    if (userUrl == null) {\n      Log.getLog().info(\"Job end notification URL not set, skipping.\");\n      return;\n    }\n\n    //Do string replacements for jobId and jobStatus\n    if (userUrl.contains(JOB_ID)) {\n      userUrl = userUrl.replace(JOB_ID, jobReport.getJobId().toString());\n    }\n    if (userUrl.contains(JOB_STATUS)) {\n      userUrl = userUrl.replace(JOB_STATUS, jobReport.getJobState().toString());\n    }\n\n    // Create the URL, ensure sanity\n    try {\n      urlToNotify = new URL(userUrl);\n    } catch (MalformedURLException mue) {\n      Log.getLog().warn(\"Job end notification couldn't parse \" + userUrl, mue);\n      return;\n    }\n\n    // Send notification\n    boolean success = false;\n    while (numTries-- > 0 && !success) {\n      Log.getLog().info(\"Job end notification attempts left \" + numTries);\n      success = notifyURLOnce();\n      if (!success) {\n        Thread.sleep(waitInterval);\n      }\n    }\n    if (!success) {\n      Log.getLog().warn(\"Job end notification failed to notify : \"\n          + urlToNotify);\n    } else {\n      Log.getLog().info(\"Job end notification succeeded for \"\n          + jobReport.getJobId());\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notifyURLOnce": "  protected boolean notifyURLOnce() {\n    boolean success = false;\n    try {\n      Log.getLog().info(\"Job end notification trying \" + urlToNotify);\n      HttpURLConnection conn =\n        (HttpURLConnection) urlToNotify.openConnection(proxyToUse);\n      conn.setConnectTimeout(timeout);\n      conn.setReadTimeout(timeout);\n      conn.setAllowUserInteraction(false);\n      if(conn.getResponseCode() != HttpURLConnection.HTTP_OK) {\n        Log.getLog().warn(\"Job end notification to \" + urlToNotify\n            + \" failed with code: \" + conn.getResponseCode() + \" and message \\\"\"\n            + conn.getResponseMessage() + \"\\\"\");\n      }\n      else {\n        success = true;\n        Log.getLog().info(\"Job end notification to \" + urlToNotify\n            + \" succeeded\");\n      }\n    } catch(IOException ioe) {\n      Log.getLog().warn(\"Job end notification to \" + urlToNotify + \" failed\",\n          ioe);\n    }\n    return success;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.setConf": "  public void setConf(Configuration conf) {\n    this.conf = conf;\n    \n    numTries = Math.min(\n      conf.getInt(MRJobConfig.MR_JOB_END_RETRY_ATTEMPTS, 0) + 1\n      , conf.getInt(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_ATTEMPTS, 1)\n    );\n    waitInterval = Math.min(\n    conf.getInt(MRJobConfig.MR_JOB_END_RETRY_INTERVAL, 5000)\n    , conf.getInt(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_RETRY_INTERVAL, 5000)\n    );\n    waitInterval = (waitInterval < 0) ? 5000 : waitInterval;\n\n    timeout = conf.getInt(JobContext.MR_JOB_END_NOTIFICATION_TIMEOUT,\n        JobContext.DEFAULT_MR_JOB_END_NOTIFICATION_TIMEOUT);\n\n    userUrl = conf.get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL);\n\n    proxyConf = conf.get(MRJobConfig.MR_JOB_END_NOTIFICATION_PROXY);\n\n    //Configure the proxy to use if its set. It should be set like\n    //proxyType@proxyHostname:port\n    if(proxyConf != null && !proxyConf.equals(\"\") &&\n         proxyConf.lastIndexOf(\":\") != -1) {\n      int typeIndex = proxyConf.indexOf(\"@\");\n      Proxy.Type proxyType = Proxy.Type.HTTP;\n      if(typeIndex != -1 &&\n        proxyConf.substring(0, typeIndex).compareToIgnoreCase(\"socks\") == 0) {\n        proxyType = Proxy.Type.SOCKS;\n      }\n      String hostname = proxyConf.substring(typeIndex + 1,\n        proxyConf.lastIndexOf(\":\"));\n      String portConf = proxyConf.substring(proxyConf.lastIndexOf(\":\") + 1);\n      try {\n        int port = Integer.parseInt(portConf);\n        proxyToUse = new Proxy(proxyType,\n          new InetSocketAddress(hostname, port));\n        Log.getLog().info(\"Job end notification using proxy type \\\"\"\n            + proxyType + \"\\\" hostname \\\"\" + hostname + \"\\\" and port \\\"\" + port\n            + \"\\\"\");\n      } catch(NumberFormatException nfe) {\n        Log.getLog().warn(\"Job end notification couldn't parse configured\"\n            + \"proxy's port \" + portConf + \". Not going to use a proxy\");\n      }\n    }\n\n  }"
        },
        "bug_report": {
            "Title": "Job end notification not send due to YarnRuntimeException",
            "Description": "MRAppMaster.this.stop() throw out YarnRuntimeException as below log shows, it caused job end notification not send.\n{quote}\n2017-05-24 12:14:02,165 WARN [Thread-693] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Graceful stop failed\norg.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:531)\n        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceStop(JobHistoryEventHandler.java:360)\n        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\n        at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)\n        at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)\n        at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)\n        at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStop(MRAppMaster.java:1476)\n        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop(MRAppMaster.java:1090)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob(MRAppMaster.java:554)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler$1.run(MRAppMaster.java:605)\nCaused by: java.nio.channels.ClosedChannelException\n        at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:1528)\n        at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:98)\n        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n        at java.io.DataOutputStream.write(DataOutputStream.java:107)\n        at org.codehaus.jackson.impl.Utf8Generator._flushBuffer(Utf8Generator.java:1754)\n        at org.codehaus.jackson.impl.Utf8Generator.flush(Utf8Generator.java:1088)\n        at org.apache.avro.io.JsonEncoder.flush(JsonEncoder.java:67)\n        at org.apache.hadoop.mapreduce.jobhistory.EventWriter.write(EventWriter.java:67)\n        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo.writeEvent(JobHistoryEventHandler.java:886)\n        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:520)\n        ... 11 more\n2017-05-24 12:14:02,165 INFO [Thread-693] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Exiting MR AppMaster..GoodBye!\n{quote}"
        }
    },
    {
        "filename": "MAPREDUCE-4451.json",
        "creation_time": "2012-07-17T10:34:13.000+0000",
        "stack_trace": "```\njava.io.IOException: Call to /192.168.7.80:8020 failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at org.apache.hadoop.ipc.Client.wrapException(Client.java:1129)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1097)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)\n        at $Proxy7.getProtocolVersion(Unknown Source)\n        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:411)\n        at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:125)\n        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:329)\n        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:294)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:100)\n        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1411)\n        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1429)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)\n        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)\n        at org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:169)\n        at org.apache.hadoop.mapred.JobInProgress.generateAndStoreTokens(JobInProgress.java:3558)\n        at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:696)\n        at org.apache.hadoop.mapred.JobTracker.initJob(JobTracker.java:3911)\n        at org.apache.hadoop.mapred.FairScheduler$JobInitializer$InitJob.run(FairScheduler.java:301)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:662)\nCaused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:543)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)\n        at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:488)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:590)\n        at org.apache.hadoop.ipc.Client$Connection.access$2100(Client.java:187)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1228)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1072)\n        ... 20 more\nCaused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:194)\n        at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:134)\n        at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:385)\n        at org.apache.hadoop.ipc.Client$Connection.access$1200(Client.java:187)\n        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:583)\n        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:580)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:579)\n        ... 23 more\nCaused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)\n        at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:130)\n        at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:106)\n        at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:172)\n        at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:209)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:195)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:162)\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:175)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.call": "  public Writable call(RPC.RpcKind rpcKind, Writable rpcRequest,\n      ConnectionId remoteId) throws InterruptedException, IOException {\n    Call call = new Call(rpcKind, rpcRequest);\n    Connection connection = getConnection(remoteId, call);\n    connection.sendParam(call);                 // send the parameter\n    boolean interrupted = false;\n    synchronized (call) {\n      while (!call.done) {\n        try {\n          call.wait();                           // wait for the result\n        } catch (InterruptedException ie) {\n          // save the fact that we were interrupted\n          interrupted = true;\n        }\n      }\n\n      if (interrupted) {\n        // set the interrupt flag now that we are done waiting\n        Thread.currentThread().interrupt();\n      }\n\n      if (call.error != null) {\n        if (call.error instanceof RemoteException) {\n          call.error.fillInStackTrace();\n          throw call.error;\n        } else { // local exception\n          InetSocketAddress address = connection.getRemoteAddress();\n          throw NetUtils.wrapException(address.getHostName(),\n                  address.getPort(),\n                  NetUtils.getHostname(),\n                  0,\n                  call.error);\n        }\n      } else {\n        return call.rpcResponse;\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnectionId": "    static ConnectionId getConnectionId(InetSocketAddress addr,\n        Class<?> protocol, UserGroupInformation ticket, int rpcTimeout,\n        RetryPolicy connectionRetryPolicy, Configuration conf) throws IOException {\n\n      if (connectionRetryPolicy == null) {\n        final int max = conf.getInt(\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY,\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_DEFAULT);\n        connectionRetryPolicy = RetryPolicies.retryUpToMaximumCountWithFixedSleep(\n            max, 1, TimeUnit.SECONDS);\n      }\n\n      String remotePrincipal = getRemotePrincipal(conf, addr, protocol);\n      boolean doPing =\n        conf.getBoolean(CommonConfigurationKeys.IPC_CLIENT_PING_KEY, true);\n      return new ConnectionId(addr, protocol, ticket,\n          rpcTimeout, remotePrincipal,\n          conf.getInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECTION_MAXIDLETIME_KEY,\n              CommonConfigurationKeysPublic.IPC_CLIENT_CONNECTION_MAXIDLETIME_DEFAULT),\n          connectionRetryPolicy,\n          conf.getInt(\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_KEY,\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_DEFAULT),\n          conf.getBoolean(CommonConfigurationKeysPublic.IPC_CLIENT_TCPNODELAY_KEY,\n              CommonConfigurationKeysPublic.IPC_CLIENT_TCPNODELAY_DEFAULT),\n          doPing, \n          (doPing ? Client.getPingInterval(conf) : 0));\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.sendParam": "    public void sendParam(Call call) {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n\n      DataOutputBuffer d=null;\n      try {\n        synchronized (this.out) {\n          if (LOG.isDebugEnabled())\n            LOG.debug(getName() + \" sending #\" + call.id);\n          \n          // Serializing the data to be written.\n          // Format:\n          // 0) Length of rest below (1 + 2)\n          // 1) PayloadHeader  - is serialized Delimited hence contains length\n          // 2) the Payload - the RpcRequest\n          //\n          d = new DataOutputBuffer();\n          RpcPayloadHeaderProto header = ProtoUtil.makeRpcPayloadHeader(\n             call.rpcKind, RpcPayloadOperationProto.RPC_FINAL_PAYLOAD, call.id);\n          header.writeDelimitedTo(d);\n          call.rpcRequest.write(d);\n          byte[] data = d.getData();\n   \n          int totalLength = d.getLength();\n          out.writeInt(totalLength); // Total Length\n          out.write(data, 0, totalLength);//PayloadHeader + RpcRequest\n          out.flush();\n        }\n      } catch(IOException e) {\n        markClosed(e);\n      } finally {\n        //the buffer is just an in-memory buffer, but it is still polite to\n        // close early\n        IOUtils.closeStream(d);\n      }\n    }  ",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRemoteAddress": "    public InetSocketAddress getRemoteAddress() {\n      return server;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnection": "  private Connection getConnection(ConnectionId remoteId,\n                                   Call call)\n                                   throws IOException, InterruptedException {\n    if (!running.get()) {\n      // the client is stopped\n      throw new IOException(\"The client is stopped\");\n    }\n    Connection connection;\n    /* we could avoid this allocation for each RPC by having a  \n     * connectionsId object and with set() method. We need to manage the\n     * refs for keys in HashMap properly. For now its ok.\n     */\n    do {\n      synchronized (connections) {\n        connection = connections.get(remoteId);\n        if (connection == null) {\n          connection = new Connection(remoteId);\n          connections.put(remoteId, connection);\n        }\n      }\n    } while (!connection.addCall(call));\n    \n    //we don't invoke the method below inside \"synchronized (connections)\"\n    //block above. The reason for that is if the server happens to be slow,\n    //it will take longer to establish a connection and that will slow the\n    //entire system down.\n    connection.setupIOstreams();\n    return connection;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.getProxy": "   public static <T> T getProxy(Class<T> protocol,\n                                 long clientVersion,\n                                 InetSocketAddress addr, Configuration conf)\n     throws IOException {\n\n     return getProtocolProxy(protocol, clientVersion, addr, conf).getProxy();\n   }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.getProtocolProxy": "  public static <T> ProtocolProxy<T> getProtocolProxy(Class<T> protocol,\n                                long clientVersion,\n                                InetSocketAddress addr, Configuration conf)\n    throws IOException {\n\n    return getProtocolProxy(protocol, clientVersion, addr, conf, NetUtils\n        .getDefaultSocketFactory(conf));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.createFileSystem": "  private static FileSystem createFileSystem(URI uri, Configuration conf\n      ) throws IOException {\n    Class<?> clazz = getFileSystemClass(uri.getScheme(), conf);\n    if (clazz == null) {\n      throw new IOException(\"No FileSystem for scheme: \" + uri.getScheme());\n    }\n    FileSystem fs = (FileSystem)ReflectionUtils.newInstance(clazz, conf);\n    fs.initialize(uri, conf);\n    return fs;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.newInstance": "  public static FileSystem newInstance(Configuration conf) throws IOException {\n    return newInstance(getDefaultUri(conf), conf);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getFileSystemClass": "  public static Class<? extends FileSystem> getFileSystemClass(String scheme,\n      Configuration conf) throws IOException {\n    if (!FILE_SYSTEMS_LOADED) {\n      loadFileSystems();\n    }\n    Class<? extends FileSystem> clazz = null;\n    if (conf != null) {\n      clazz = (Class<? extends FileSystem>) conf.getClass(\"fs.\" + scheme + \".impl\", null);\n    }\n    if (clazz == null) {\n      clazz = SERVICE_FILE_SYSTEMS.get(scheme);\n    }\n    if (clazz == null) {\n      throw new IOException(\"No FileSystem for scheme: \" + scheme);\n    }\n    return clazz;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getScheme": "    public String getScheme() {\n      return scheme;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.initialize": "  public void initialize(URI name, Configuration conf) throws IOException {\n    statistics = getStatistics(name.getScheme(), getClass());    \n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.get": "    FileSystem get(URI uri, Configuration conf) throws IOException{\n      Key key = new Key(uri, conf);\n      return getInternal(uri, conf, key);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getInternal": "    private FileSystem getInternal(URI uri, Configuration conf, Key key) throws IOException{\n      FileSystem fs;\n      synchronized (this) {\n        fs = map.get(key);\n      }\n      if (fs != null) {\n        return fs;\n      }\n\n      fs = createFileSystem(uri, conf);\n      synchronized (this) { // refetch the lock again\n        FileSystem oldfs = map.get(key);\n        if (oldfs != null) { // a file system is created while lock is releasing\n          fs.close(); // close the new file system\n          return oldfs;  // return the old file system\n        }\n        \n        // now insert the new file system into the map\n        if (map.isEmpty() ) {\n          ShutdownHookManager.get().addShutdownHook(clientFinalizer, SHUTDOWN_HOOK_PRIORITY);\n        }\n        fs.key = key;\n        map.put(key, fs);\n        if (conf.getBoolean(\"fs.automatic.close\", true)) {\n          toAutoClose.add(key);\n        }\n        return fs;\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getDefaultUri": "  public static URI getDefaultUri(Configuration conf) {\n    return URI.create(fixName(conf.get(FS_DEFAULT_NAME_KEY, DEFAULT_FS)));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.equals": "      public boolean equals(Object obj) {\n        if (obj == this) {\n          return true;\n        }\n        if (obj != null && obj instanceof Key) {\n          Key that = (Key)obj;\n          return isEqual(this.scheme, that.scheme)\n                 && isEqual(this.authority, that.authority)\n                 && isEqual(this.ugi, that.ugi)\n                 && (this.unique == that.unique);\n        }\n        return false;        \n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Path.getFileSystem": "  public FileSystem getFileSystem(Configuration conf) throws IOException {\n    return FileSystem.get(this.toUri(), conf);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Path.toUri": "  public URI toUri() { return uri; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.Credentials.writeTokenStorageFile": "  public void writeTokenStorageFile(Path filename, \n                                    Configuration conf) throws IOException {\n    FSDataOutputStream os = filename.getFileSystem(conf).create(filename);\n    writeTokenStorageToStream(os);\n    os.close();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.Credentials.writeTokenStorageToStream": "  public void writeTokenStorageToStream(DataOutputStream os)\n    throws IOException {\n    os.write(TOKEN_STORAGE_MAGIC);\n    os.write(TOKEN_STORAGE_VERSION);\n    write(os);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.run": "    public void run() {\n      while (initialized) {\n        try {\n          Thread.sleep(UPDATE_INTERVAL);\n          update();\n          preemptTasksIfNecessary();\n        } catch (Exception e) {\n          LOG.error(\"Exception in fair scheduler UpdateThread\", e);\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.preemptTasksIfNecessary": "  protected void preemptTasksIfNecessary() {\n    if (!preemptionEnabled)\n      return;\n\n    long curTime = clock.getTime();\n    if (curTime - lastPreemptCheckTime < preemptionInterval)\n      return;\n    lastPreemptCheckTime = curTime;\n\n    Resource resToPreempt = Resources.none();\n\n    for (FSQueueSchedulable sched: getQueueSchedulables()) {\n      resToPreempt = Resources.add(resToPreempt, resToPreempt(sched, curTime));\n    }\n    if (Resources.greaterThan(resToPreempt, Resources.none())) {\n      preemptResources(getQueueSchedulables(), resToPreempt);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.update": "  protected void update() {\n    synchronized (this) {\n      queueMgr.reloadAllocsIfNecessary(); // Relaod alloc file\n      updateRunnability(); // Set job runnability based on user/queue limits\n      updatePreemptionVariables(); // Determine if any queues merit preemption\n\n      // Update demands of apps and queues\n      for (FSQueue queue: queueMgr.getQueues()) {\n        queue.getQueueSchedulable().updateDemand();\n      }\n\n      // Compute fair shares based on updated demands\n      List<FSQueueSchedulable> queueScheds = this.getQueueSchedulables();\n      SchedulingAlgorithms.computeFairShares(\n          queueScheds, clusterCapacity);\n\n      // Update queue metrics for this queue\n      for (FSQueueSchedulable sched : queueScheds) {\n        sched.getMetrics().setAvailableResourcesToQueue(sched.getFairShare());\n      }\n\n      // Use the computed shares to assign shares within each queue\n      for (FSQueue queue: queueMgr.getQueues()) {\n        queue.getQueueSchedulable().redistributeShare();\n      }\n\n      // Update recorded capacity of root queue (child queues are updated\n      // when fair share is calculated).\n      rootMetrics.setAvailableResourcesToQueue(clusterCapacity);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.run": "    public void run() {\n      if (LOG.isDebugEnabled())\n        LOG.debug(getName() + \": starting, having connections \" \n            + connections.size());\n\n      try {\n        while (waitForWork()) {//wait here for work - read or close connection\n          receiveResponse();\n        }\n      } catch (Throwable t) {\n        // This truly is unexpected, since we catch IOException in receiveResponse\n        // -- this is only to be really sure that we don't leave a client hanging\n        // forever.\n        LOG.warn(\"Unexpected error reading responses on connection \" + this, t);\n        markClosed(new IOException(\"Error reading responses\", t));\n      }\n      \n      close();\n      \n      if (LOG.isDebugEnabled())\n        LOG.debug(getName() + \": stopped, remaining connections \"\n            + connections.size());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.receiveResponse": "    private void receiveResponse() {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n      touch();\n      \n      try {\n        RpcResponseHeaderProto response = \n            RpcResponseHeaderProto.parseDelimitedFrom(in);\n        if (response == null) {\n          throw new IOException(\"Response is null.\");\n        }\n\n        int callId = response.getCallId();\n        if (LOG.isDebugEnabled())\n          LOG.debug(getName() + \" got value #\" + callId);\n\n        Call call = calls.get(callId);\n        RpcStatusProto status = response.getStatus();\n        if (status == RpcStatusProto.SUCCESS) {\n          Writable value = ReflectionUtils.newInstance(valueClass, conf);\n          value.readFields(in);                 // read value\n          call.setRpcResponse(value);\n          calls.remove(callId);\n        } else if (status == RpcStatusProto.ERROR) {\n          call.setException(new RemoteException(WritableUtils.readString(in),\n                                                WritableUtils.readString(in)));\n          calls.remove(callId);\n        } else if (status == RpcStatusProto.FATAL) {\n          // Close the connection\n          markClosed(new RemoteException(WritableUtils.readString(in), \n                                         WritableUtils.readString(in)));\n        }\n      } catch (IOException e) {\n        markClosed(e);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.closeConnection": "    private void closeConnection() {\n      if (socket == null) {\n        return;\n      }\n      // close the current connection\n      try {\n        socket.close();\n      } catch (IOException e) {\n        LOG.warn(\"Not able to close a socket\", e);\n      }\n      // set socket to null so that the next call to setupIOstreams\n      // can start the process of connect all over again.\n      socket = null;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.close": "    private synchronized void close() {\n      if (!shouldCloseConnection.get()) {\n        LOG.error(\"The connection is not in the closed state\");\n        return;\n      }\n\n      // release the resources\n      // first thing to do;take the connection out of the connection list\n      synchronized (connections) {\n        if (connections.get(remoteId) == this) {\n          connections.remove(remoteId);\n        }\n      }\n\n      // close the streams and therefore the socket\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(in);\n      disposeSasl();\n\n      // clean up all calls\n      if (closeException == null) {\n        if (!calls.isEmpty()) {\n          LOG.warn(\n              \"A connection is closed for no cause and calls are not empty\");\n\n          // clean up calls anyway\n          closeException = new IOException(\"Unexpected closed connection\");\n          cleanupCalls();\n        }\n      } else {\n        // log the info\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"closing ipc connection to \" + server + \": \" +\n              closeException.getMessage(),closeException);\n        }\n\n        // cleanup calls\n        cleanupCalls();\n      }\n      if (LOG.isDebugEnabled())\n        LOG.debug(getName() + \": closed\");\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupSaslConnection": "    private synchronized boolean setupSaslConnection(final InputStream in2, \n        final OutputStream out2) \n        throws IOException {\n      saslRpcClient = new SaslRpcClient(authMethod, token, serverPrincipal);\n      return saslRpcClient.saslConnect(in2, out2);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.disposeSasl": "    private synchronized void disposeSasl() {\n      if (saslRpcClient != null) {\n        try {\n          saslRpcClient.dispose();\n          saslRpcClient = null;\n        } catch (IOException ignored) {\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.waitForWork": "    private synchronized boolean waitForWork() {\n      if (calls.isEmpty() && !shouldCloseConnection.get()  && running.get())  {\n        long timeout = maxIdleTime-\n              (Time.now()-lastActivity.get());\n        if (timeout>0) {\n          try {\n            wait(timeout);\n          } catch (InterruptedException e) {}\n        }\n      }\n      \n      if (!calls.isEmpty() && !shouldCloseConnection.get() && running.get()) {\n        return true;\n      } else if (shouldCloseConnection.get()) {\n        return false;\n      } else if (calls.isEmpty()) { // idle connection closed or stopped\n        markClosed(null);\n        return false;\n      } else { // get stopped but there are still pending requests \n        markClosed((IOException)new IOException().initCause(\n            new InterruptedException()));\n        return false;\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.markClosed": "    private synchronized void markClosed(IOException e) {\n      if (shouldCloseConnection.compareAndSet(false, true)) {\n        closeException = e;\n        notifyAll();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.shouldAuthenticateOverKrb": "    private synchronized boolean shouldAuthenticateOverKrb() throws IOException {\n      UserGroupInformation loginUser = UserGroupInformation.getLoginUser();\n      UserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\n      UserGroupInformation realUser = currentUser.getRealUser();\n      if (authMethod == AuthMethod.KERBEROS && loginUser != null &&\n      // Make sure user logged in using Kerberos either keytab or TGT\n          loginUser.hasKerberosCredentials() &&\n          // relogin only in case it is the login user (e.g. JT)\n          // or superuser (like oozie).\n          (loginUser.equals(currentUser) || loginUser.equals(realUser))) {\n        return true;\n      }\n      return false;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      LOG.error(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(pae,\"Unknown exception in doAs\");\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.handleSaslConnectionFailure": "    private synchronized void handleSaslConnectionFailure(\n        final int currRetries, final int maxRetries, final Exception ex,\n        final Random rand, final UserGroupInformation ugi) throws IOException,\n        InterruptedException {\n      ugi.doAs(new PrivilegedExceptionAction<Object>() {\n        public Object run() throws IOException, InterruptedException {\n          final short MAX_BACKOFF = 5000;\n          closeConnection();\n          disposeSasl();\n          if (shouldAuthenticateOverKrb()) {\n            if (currRetries < maxRetries) {\n              if(LOG.isDebugEnabled()) {\n                LOG.debug(\"Exception encountered while connecting to \"\n                    + \"the server : \" + ex);\n              }\n              // try re-login\n              if (UserGroupInformation.isLoginKeytabBased()) {\n                UserGroupInformation.getLoginUser().reloginFromKeytab();\n              } else {\n                UserGroupInformation.getLoginUser().reloginFromTicketCache();\n              }\n              // have granularity of milliseconds\n              //we are sleeping with the Connection lock held but since this\n              //connection instance is being used for connecting to the server\n              //in question, it is okay\n              Thread.sleep((rand.nextInt(MAX_BACKOFF) + 1));\n              return null;\n            } else {\n              String msg = \"Couldn't setup connection for \"\n                  + UserGroupInformation.getLoginUser().getUserName() + \" to \"\n                  + serverPrincipal;\n              LOG.warn(msg);\n              throw (IOException) new IOException(msg).initCause(ex);\n            }\n          } else {\n            LOG.warn(\"Exception encountered while connecting to \"\n                + \"the server : \" + ex);\n          }\n          if (ex instanceof RemoteException)\n            throw (RemoteException) ex;\n          throw new IOException(ex);\n        }\n      });\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupIOstreams": "    private synchronized void setupIOstreams() throws InterruptedException {\n      if (socket != null || shouldCloseConnection.get()) {\n        return;\n      } \n      try {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Connecting to \"+server);\n        }\n        short numRetries = 0;\n        final short MAX_RETRIES = 5;\n        Random rand = null;\n        while (true) {\n          setupConnection();\n          InputStream inStream = NetUtils.getInputStream(socket);\n          OutputStream outStream = NetUtils.getOutputStream(socket);\n          writeConnectionHeader(outStream);\n          if (useSasl) {\n            final InputStream in2 = inStream;\n            final OutputStream out2 = outStream;\n            UserGroupInformation ticket = remoteId.getTicket();\n            if (authMethod == AuthMethod.KERBEROS) {\n              if (ticket.getRealUser() != null) {\n                ticket = ticket.getRealUser();\n              }\n            }\n            boolean continueSasl = false;\n            try {\n              continueSasl = ticket\n                  .doAs(new PrivilegedExceptionAction<Boolean>() {\n                    @Override\n                    public Boolean run() throws IOException {\n                      return setupSaslConnection(in2, out2);\n                    }\n                  });\n            } catch (Exception ex) {\n              if (rand == null) {\n                rand = new Random();\n              }\n              handleSaslConnectionFailure(numRetries++, MAX_RETRIES, ex, rand,\n                  ticket);\n              continue;\n            }\n            if (continueSasl) {\n              // Sasl connect is successful. Let's set up Sasl i/o streams.\n              inStream = saslRpcClient.getInputStream(inStream);\n              outStream = saslRpcClient.getOutputStream(outStream);\n            } else {\n              // fall back to simple auth because server told us so.\n              authMethod = AuthMethod.SIMPLE;\n              // remake the connectionContext             \n              connectionContext = ProtoUtil.makeIpcConnectionContext(\n                  connectionContext.getProtocol(), \n                  ProtoUtil.getUgi(connectionContext.getUserInfo()),\n                  authMethod);\n              useSasl = false;\n            }\n          }\n        \n          if (doPing) {\n            this.in = new DataInputStream(new BufferedInputStream(\n                new PingInputStream(inStream)));\n          } else {\n            this.in = new DataInputStream(new BufferedInputStream(inStream));\n          }\n          this.out = new DataOutputStream(new BufferedOutputStream(outStream));\n          writeConnectionContext();\n\n          // update last activity time\n          touch();\n\n          // start the receiver thread after the socket connection has been set\n          // up\n          start();\n          return;\n        }\n      } catch (Throwable t) {\n        if (t instanceof IOException) {\n          markClosed((IOException)t);\n        } else {\n          markClosed(new IOException(\"Couldn't set up IO streams\", t));\n        }\n        close();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupConnection": "    private synchronized void setupConnection() throws IOException {\n      short ioFailures = 0;\n      short timeoutFailures = 0;\n      while (true) {\n        try {\n          this.socket = socketFactory.createSocket();\n          this.socket.setTcpNoDelay(tcpNoDelay);\n          \n          /*\n           * Bind the socket to the host specified in the principal name of the\n           * client, to ensure Server matching address of the client connection\n           * to host name in principal passed.\n           */\n          if (UserGroupInformation.isSecurityEnabled()) {\n            KerberosInfo krbInfo = \n              remoteId.getProtocol().getAnnotation(KerberosInfo.class);\n            if (krbInfo != null && krbInfo.clientPrincipal() != null) {\n              String host = \n                SecurityUtil.getHostFromPrincipal(remoteId.getTicket().getUserName());\n              \n              // If host name is a valid local address then bind socket to it\n              InetAddress localAddr = NetUtils.getLocalInetAddress(host);\n              if (localAddr != null) {\n                this.socket.bind(new InetSocketAddress(localAddr, 0));\n              }\n            }\n          }\n          \n          // connection time out is 20s\n          NetUtils.connect(this.socket, server, 20000);\n          if (rpcTimeout > 0) {\n            pingInterval = rpcTimeout;  // rpcTimeout overwrites pingInterval\n          }\n          this.socket.setSoTimeout(pingInterval);\n          return;\n        } catch (SocketTimeoutException toe) {\n          /* Check for an address change and update the local reference.\n           * Reset the failure counter if the address was changed\n           */\n          if (updateAddress()) {\n            timeoutFailures = ioFailures = 0;\n          }\n          handleConnectionFailure(timeoutFailures++,\n              maxRetriesOnSocketTimeouts, toe);\n        } catch (IOException ie) {\n          if (updateAddress()) {\n            timeoutFailures = ioFailures = 0;\n          }\n          handleConnectionFailure(ioFailures++, ie);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.touch": "    private void touch() {\n      lastActivity.set(Time.now());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.writeConnectionHeader": "    private void writeConnectionHeader(OutputStream outStream)\n        throws IOException {\n      DataOutputStream out = new DataOutputStream(new BufferedOutputStream(outStream));\n      // Write out the header, version and authentication method\n      out.write(Server.HEADER.array());\n      out.write(Server.CURRENT_VERSION);\n      authMethod.write(out);\n      Server.IpcSerializationType.PROTOBUF.write(out);\n      out.flush();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.writeConnectionContext": "    private void writeConnectionContext() throws IOException {\n      // Write out the ConnectionHeader\n      DataOutputBuffer buf = new DataOutputBuffer();\n      connectionContext.writeTo(buf);\n      \n      // Write out the payload length\n      int bufLen = buf.getLength();\n\n      out.writeInt(bufLen);\n      out.write(buf.getData(), 0, bufLen);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getProtocol": "    Class<?> getProtocol() {\n      return protocol;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getTicket": "    UserGroupInformation getTicket() {\n      return ticket;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SaslRpcClient.saslConnect": "  public boolean saslConnect(InputStream inS, OutputStream outS)\n      throws IOException {\n    DataInputStream inStream = new DataInputStream(new BufferedInputStream(inS));\n    DataOutputStream outStream = new DataOutputStream(new BufferedOutputStream(\n        outS));\n\n    try {\n      byte[] saslToken = new byte[0];\n      if (saslClient.hasInitialResponse())\n        saslToken = saslClient.evaluateChallenge(saslToken);\n      if (saslToken != null) {\n        outStream.writeInt(saslToken.length);\n        outStream.write(saslToken, 0, saslToken.length);\n        outStream.flush();\n        if (LOG.isDebugEnabled())\n          LOG.debug(\"Have sent token of size \" + saslToken.length\n              + \" from initSASLContext.\");\n      }\n      if (!saslClient.isComplete()) {\n        readStatus(inStream);\n        int len = inStream.readInt();\n        if (len == SaslRpcServer.SWITCH_TO_SIMPLE_AUTH) {\n          if (LOG.isDebugEnabled())\n            LOG.debug(\"Server asks us to fall back to simple auth.\");\n          saslClient.dispose();\n          return false;\n        }\n        saslToken = new byte[len];\n        if (LOG.isDebugEnabled())\n          LOG.debug(\"Will read input token of size \" + saslToken.length\n              + \" for processing by initSASLContext\");\n        inStream.readFully(saslToken);\n      }\n\n      while (!saslClient.isComplete()) {\n        saslToken = saslClient.evaluateChallenge(saslToken);\n        if (saslToken != null) {\n          if (LOG.isDebugEnabled())\n            LOG.debug(\"Will send token of size \" + saslToken.length\n                + \" from initSASLContext.\");\n          outStream.writeInt(saslToken.length);\n          outStream.write(saslToken, 0, saslToken.length);\n          outStream.flush();\n        }\n        if (!saslClient.isComplete()) {\n          readStatus(inStream);\n          saslToken = new byte[inStream.readInt()];\n          if (LOG.isDebugEnabled())\n            LOG.debug(\"Will read input token of size \" + saslToken.length\n                + \" for processing by initSASLContext\");\n          inStream.readFully(saslToken);\n        }\n      }\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"SASL client context established. Negotiated QoP: \"\n            + saslClient.getNegotiatedProperty(Sasl.QOP));\n      }\n      return true;\n    } catch (IOException e) {\n      try {\n        saslClient.dispose();\n      } catch (SaslException ignored) {\n        // ignore further exceptions during cleanup\n      }\n      throw e;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SaslRpcClient.readStatus": "  private static void readStatus(DataInputStream inStream) throws IOException {\n    int status = inStream.readInt(); // read status\n    if (status != SaslStatus.SUCCESS.state) {\n      throw new RemoteException(WritableUtils.readString(inStream),\n          WritableUtils.readString(inStream));\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SaslRpcClient.dispose": "  public void dispose() throws SaslException {\n    saslClient.dispose();\n  }"
        },
        "bug_report": {
            "Title": "fairscheduler fail to init job with kerberos authentication configured",
            "Description": "Using FairScheduler in Hadoop 1.0.3 with kerberos authentication configured. Job initialization fails:\n\n{code}\n2012-07-17 15:15:09,220 ERROR org.apache.hadoop.mapred.JobTracker: Job initialization failed:\njava.io.IOException: Call to /192.168.7.80:8020 failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at org.apache.hadoop.ipc.Client.wrapException(Client.java:1129)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1097)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)\n        at $Proxy7.getProtocolVersion(Unknown Source)\n        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:411)\n        at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:125)\n        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:329)\n        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:294)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:100)\n        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1411)\n        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1429)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)\n        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)\n        at org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:169)\n        at org.apache.hadoop.mapred.JobInProgress.generateAndStoreTokens(JobInProgress.java:3558)\n        at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:696)\n        at org.apache.hadoop.mapred.JobTracker.initJob(JobTracker.java:3911)\n        at org.apache.hadoop.mapred.FairScheduler$JobInitializer$InitJob.run(FairScheduler.java:301)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:662)\nCaused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:543)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)\n        at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:488)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:590)\n        at org.apache.hadoop.ipc.Client$Connection.access$2100(Client.java:187)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1228)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1072)\n        ... 20 more\nCaused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:194)\n        at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:134)\n        at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:385)\n        at org.apache.hadoop.ipc.Client$Connection.access$1200(Client.java:187)\n        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:583)\n        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:580)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:579)\n        ... 23 more\nCaused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)\n        at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:130)\n        at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:106)\n        at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:172)\n        at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:209)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:195)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:162)\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:175)\n        ... 32 more\n\n{code}\n\nWhen a job is submitted, fairscheduler calls JobTracker.initJob, which calls JobInProgress.generateAndStoreTokens to write security keys to hdfs. However, the operation is involved in the server side rpc call path, using UGI created by UserGroupInformation.createRemoteUser in rpc server, which have no tgt. This should be done with UGI used by JobTracker."
        }
    },
    {
        "filename": "MAPREDUCE-4144.json",
        "creation_time": "2012-04-12T16:28:30.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:261)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:223)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:246)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1229)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1078)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1048)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignReservedContainer(LeafQueue.java:859)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:756)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:573)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:622)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:78)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:302)\n        at java.lang.Thread.run(Thread.java:619)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal": "  synchronized private void allocateNodeLocal(SchedulerNode node, Priority priority,\n      ResourceRequest nodeLocalRequest, Container container) {\n    // Update consumption and track allocations\n    allocate(container);\n\n    // Update future requirements\n    nodeLocalRequest.setNumContainers(nodeLocalRequest.getNumContainers() - 1);\n    if (nodeLocalRequest.getNumContainers() == 0) {\n      this.requests.get(priority).remove(node.getHostName());\n    }\n\n    ResourceRequest rackLocalRequest = requests.get(priority).get(\n        node.getRackName());\n    rackLocalRequest.setNumContainers(rackLocalRequest.getNumContainers() - 1);\n    if (rackLocalRequest.getNumContainers() == 0) {\n      this.requests.get(priority).remove(node.getRackName());\n    }\n\n    decrementOutstanding(requests.get(priority).get(RMNode.ANY));\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.decrementOutstanding": "  synchronized private void decrementOutstanding(\n      ResourceRequest offSwitchRequest) {\n    int numOffSwitchContainers = offSwitchRequest.getNumContainers() - 1;\n\n    // Do not remove ANY\n    offSwitchRequest.setNumContainers(numOffSwitchContainers);\n    \n    // Do we have any outstanding requests?\n    // If there is nothing, we need to deactivate this application\n    if (numOffSwitchContainers == 0) {\n      checkForDeactivation();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate": "  synchronized private void allocate(Container container) {\n    // Update consumption and track allocations\n    //TODO: fixme sharad\n    /* try {\n        store.storeContainer(container);\n      } catch (IOException ie) {\n        // TODO fix this. we shouldnt ignore\n      }*/\n    \n    LOG.debug(\"allocate: applicationId=\" + applicationId + \" container=\"\n        + container.getId() + \" host=\"\n        + container.getNodeId().toString());\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate": "  synchronized public RMContainer allocate(NodeType type, SchedulerNode node,\n      Priority priority, ResourceRequest request, \n      Container container) {\n    \n    // Required sanity check - AM can call 'allocate' to update resource \n    // request without locking the scheduler, hence we need to check\n    if (getTotalRequiredResources(priority) <= 0) {\n      return null;\n    }\n    \n    // Create RMContainer\n    RMContainer rmContainer = new RMContainerImpl(container, this\n        .getApplicationAttemptId(), node.getNodeID(), this.rmContext\n        .getDispatcher().getEventHandler(), this.rmContext\n        .getContainerAllocationExpirer());\n\n    // Add it to allContainers list.\n    newlyAllocatedContainers.add(rmContainer);\n    liveContainers.put(container.getId(), rmContainer);    \n\n    // Update consumption and track allocations\n    appSchedulingInfo.allocate(type, node, priority, request, container);\n    Resources.addTo(currentConsumption, container.getResource());\n\n    // Inform the container\n    rmContainer.handle(\n        new RMContainerEvent(container.getId(), RMContainerEventType.START));\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"allocate: applicationAttemptId=\" \n          + container.getId().getApplicationAttemptId() \n          + \" container=\" + container.getId() + \" host=\"\n          + container.getNodeId().getHost() + \" type=\" + type);\n    }\n    RMAuditLogger.logSuccess(getUser(), \n        AuditConstants.ALLOC_CONTAINER, \"SchedulerApp\", \n        getApplicationId(), container.getId());\n    \n    return rmContainer;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.getApplicationId": "  public ApplicationId getApplicationId() {\n    return this.appSchedulingInfo.getApplicationId();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.getResource": "  public Resource getResource(Priority priority) {\n    return this.appSchedulingInfo.getResource(priority);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.getApplicationAttemptId": "  public ApplicationAttemptId getApplicationAttemptId() {\n    return this.appSchedulingInfo.getApplicationAttemptId();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.getTotalRequiredResources": "  public synchronized int getTotalRequiredResources(Priority priority) {\n    return getResourceRequest(priority, RMNode.ANY).getNumContainers();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.getUser": "  public String getUser() {\n    return this.appSchedulingInfo.getUser();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer": "    public synchronized void assignContainer(Resource resource) {\n      Resources.addTo(consumed, resource);\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getContainer": "  private Container getContainer(RMContainer rmContainer, \n      SchedulerApp application, SchedulerNode node, \n      Resource capability, Priority priority) {\n    return (rmContainer != null) ? rmContainer.getContainer() :\n      createContainer(application, node, capability, priority);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.reserve": "  private void reserve(SchedulerApp application, Priority priority, \n      SchedulerNode node, RMContainer rmContainer, Container container) {\n    // Update reserved metrics if this is the first reservation\n    if (rmContainer == null) {\n      getMetrics().reserveResource(\n          application.getUser(), container.getResource());\n    }\n\n    // Inform the application \n    rmContainer = application.reserve(node, priority, rmContainer, container);\n    \n    // Update the node\n    node.reserveResource(application, priority, rmContainer);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getUsedCapacity": "  public synchronized float getUsedCapacity() {\n    return usedCapacity;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getAbsoluteUsedCapacity": "  public synchronized float getAbsoluteUsedCapacity() {\n    return absoluteUsedCapacity;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.unreserve": "  private void unreserve(SchedulerApp application, Priority priority, \n      SchedulerNode node, RMContainer rmContainer) {\n    // Done with the reservation?\n    application.unreserve(node, priority);\n    node.unreserveResource(application);\n      \n      // Update reserved metrics\n    getMetrics().unreserveResource(\n        application.getUser(), rmContainer.getContainer().getResource());\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.toString": "  public String toString() {\n    return queueName + \": \" + \n        \"capacity=\" + capacity + \", \" + \n        \"absoluteCapacity=\" + absoluteCapacity + \", \" + \n        \"usedResources=\" + usedResources.getMemory() + \"MB, \" + \n        \"usedCapacity=\" + getUsedCapacity() + \", \" + \n        \"absoluteUsedCapacity=\" + getAbsoluteUsedCapacity() + \", \" +\n        \"numApps=\" + getNumApplications() + \", \" + \n        \"numContainers=\" + getNumContainers();  \n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers": "  private Resource assignNodeLocalContainers(Resource clusterResource, \n      SchedulerNode node, SchedulerApp application, \n      Priority priority, RMContainer reservedContainer) {\n    ResourceRequest request = \n        application.getResourceRequest(priority, node.getHostName());\n    if (request != null) {\n      if (canAssign(application, priority, node, NodeType.NODE_LOCAL, \n          reservedContainer)) {\n        return assignContainer(clusterResource, node, application, priority, \n            request, NodeType.NODE_LOCAL, reservedContainer);\n      }\n    }\n    \n    return Resources.none();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.canAssign": "  boolean canAssign(SchedulerApp application, Priority priority, \n      SchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Reserved... \n    if (reservedContainer != null) {\n      return true;\n    }\n    \n    // Clearly we need containers for this application...\n    if (type == NodeType.OFF_SWITCH) {\n      // 'Delay' off-switch\n      ResourceRequest offSwitchRequest = \n          application.getResourceRequest(priority, RMNode.ANY);\n      long missedOpportunities = application.getSchedulingOpportunities(priority);\n      long requiredContainers = offSwitchRequest.getNumContainers(); \n      \n      float localityWaitFactor = \n        application.getLocalityWaitFactor(priority, \n            scheduler.getNumClusterNodes());\n      \n      return ((requiredContainers * localityWaitFactor) < missedOpportunities);\n    }\n\n    // Check if we need containers on this rack \n    ResourceRequest rackLocalRequest = \n      application.getResourceRequest(priority, node.getRackName());\n    if (rackLocalRequest == null || rackLocalRequest.getNumContainers() <= 0) {\n      return false;\n    }\n      \n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type == NodeType.RACK_LOCAL) {\n      return true;\n    }\n\n    // Check if we need containers on this host\n    if (type == NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest = \n        application.getResourceRequest(priority, node.getHostName());\n      if (nodeLocalRequest != null) {\n        return nodeLocalRequest.getNumContainers() > 0;\n      }\n    }\n\n    return false;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode": "  private CSAssignment assignContainersOnNode(Resource clusterResource, \n      SchedulerNode node, SchedulerApp application, \n      Priority priority, RMContainer reservedContainer) {\n\n    Resource assigned = Resources.none();\n\n    // Data-local\n    assigned = \n        assignNodeLocalContainers(clusterResource, node, application, priority,\n            reservedContainer); \n    if (Resources.greaterThan(assigned, Resources.none())) {\n      return new CSAssignment(assigned, NodeType.NODE_LOCAL);\n    }\n\n    // Rack-local\n    assigned = \n        assignRackLocalContainers(clusterResource, node, application, priority, \n            reservedContainer);\n    if (Resources.greaterThan(assigned, Resources.none())) {\n      return new CSAssignment(assigned, NodeType.RACK_LOCAL);\n    }\n    \n    // Off-switch\n    return new CSAssignment(\n        assignOffSwitchContainers(clusterResource, node, application, \n            priority, reservedContainer), \n        NodeType.OFF_SWITCH);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignOffSwitchContainers": "  private Resource assignOffSwitchContainers(Resource clusterResource, SchedulerNode node, \n      SchedulerApp application, Priority priority, \n      RMContainer reservedContainer) {\n    ResourceRequest request = \n      application.getResourceRequest(priority, RMNode.ANY);\n    if (request != null) {\n      if (canAssign(application, priority, node, NodeType.OFF_SWITCH, \n          reservedContainer)) {\n        return assignContainer(clusterResource, node, application, priority, request, \n            NodeType.OFF_SWITCH, reservedContainer);\n      }\n    }\n    \n    return Resources.none();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignRackLocalContainers": "  private Resource assignRackLocalContainers(Resource clusterResource,  \n      SchedulerNode node, SchedulerApp application, Priority priority,\n      RMContainer reservedContainer) {\n    ResourceRequest request = \n      application.getResourceRequest(priority, node.getRackName());\n    if (request != null) {\n      if (canAssign(application, priority, node, NodeType.RACK_LOCAL, \n          reservedContainer)) {\n        return assignContainer(clusterResource, node, application, priority, request, \n            NodeType.RACK_LOCAL, reservedContainer);\n      }\n    }\n    return Resources.none();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignReservedContainer": "  private synchronized Resource assignReservedContainer(SchedulerApp application, \n      SchedulerNode node, RMContainer rmContainer, Resource clusterResource) {\n    // Do we still need this reservation?\n    Priority priority = rmContainer.getReservedPriority();\n    if (application.getTotalRequiredResources(priority) == 0) {\n      // Release\n      Container container = rmContainer.getContainer();\n      completedContainer(clusterResource, application, node, \n          rmContainer, \n          SchedulerUtils.createAbnormalContainerStatus(\n              container.getId(), \n              SchedulerUtils.UNRESERVED_CONTAINER), \n          RMContainerEventType.RELEASED);\n      return container.getResource(); // Ugh, return resource to force re-sort\n    }\n\n    // Try to assign if we have sufficient resources\n    assignContainersOnNode(clusterResource, node, application, priority, \n        rmContainer);\n    \n    // Doesn't matter... since it's already charged for at time of reservation\n    // \"re-reservation\" is *free*\n    return org.apache.hadoop.yarn.server.resourcemanager.resource.Resource.NONE;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.completedContainer": "  public void completedContainer(Resource clusterResource, \n      SchedulerApp application, SchedulerNode node, RMContainer rmContainer, \n      ContainerStatus containerStatus, RMContainerEventType event) {\n    if (application != null) {\n      // Careful! Locking order is important!\n      synchronized (this) {\n\n        Container container = rmContainer.getContainer();\n        \n        // Inform the application & the node\n        // Note: It's safe to assume that all state changes to RMContainer\n        // happen under scheduler's lock... \n        // So, this is, in effect, a transaction across application & node\n        if (rmContainer.getState() == RMContainerState.RESERVED) {\n          unreserve(application, rmContainer.getReservedPriority(), \n              node, rmContainer);\n        } else {\n          application.containerCompleted(rmContainer, containerStatus, event);\n          node.releaseContainer(container);\n        }\n\n\n        // Book-keeping\n        releaseResource(clusterResource, \n            application, container.getResource());\n\n        LOG.info(\"completedContainer\" +\n            \" container=\" + container +\n            \" resource=\" + container.getResource() +\n        \t\t\" queue=\" + this + \n            \" usedCapacity=\" + getUsedCapacity() +\n            \" absoluteUsedCapacity=\" + getAbsoluteUsedCapacity() +\n            \" used=\" + usedResources + \n            \" cluster=\" + clusterResource);\n      }\n\n      // Inform the parent queue\n      parent.completedContainer(clusterResource, application, \n          node, rmContainer, null, event);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers": "  public synchronized CSAssignment \n  assignContainers(Resource clusterResource, SchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node=\" + node.getHostName()\n        + \" #applications=\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer = node.getReservedContainer();\n    if (reservedContainer != null) {\n      SchedulerApp application = \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return new CSAssignment(\n          assignReservedContainer(application, node, reservedContainer, \n              clusterResource),\n          NodeType.NODE_LOCAL); // Don't care about locality constraints \n                                // for reserved containers\n    }\n    \n    // Try to assign containers to applications in order\n    for (SchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required = \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n          // Do we need containers at this 'priority'?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Compute user-limit & set headroom\n          // Note: We compute both user-limit & headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit = \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required);          \n          \n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment =  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did we schedule or reserve a container?\n          Resource assigned = assignment.getResource();\n          if (Resources.greaterThan(assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned);\n            \n            // Reset scheduling opportunities\n            application.resetSchedulingOpportunities(priority);\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignToQueue": "  private synchronized boolean assignToQueue(Resource clusterResource, \n      Resource required) {\n    // Check how of the cluster's absolute capacity we are currently using...\n    float potentialNewCapacity = \n      (float)(usedResources.getMemory() + required.getMemory()) / \n        clusterResource.getMemory();\n    if (potentialNewCapacity > absoluteMaxCapacity) {\n      LOG.info(getQueueName() + \n          \" usedResources: \" + usedResources.getMemory() +\n          \" clusterResources: \" + clusterResource.getMemory() +\n          \" currentCapacity \" + ((float)usedResources.getMemory())/clusterResource.getMemory() + \n          \" required \" + required.getMemory() +\n          \" potentialNewCapacity: \" + potentialNewCapacity + \" ( \" +\n          \" max-capacity: \" + absoluteMaxCapacity + \")\");\n      return false;\n    }\n    return true;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getUser": "  private synchronized User getUser(String userName) {\n    User user = users.get(userName);\n    if (user == null) {\n      user = new User();\n      users.put(userName, user);\n    }\n    return user;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.computeUserLimitAndSetHeadroom": "  private Resource computeUserLimitAndSetHeadroom(\n      SchedulerApp application, Resource clusterResource, Resource required) {\n    \n    String user = application.getUser();\n    \n    /** \n     * Headroom is min((userLimit, queue-max-cap) - consumed)\n     */\n\n    Resource userLimit =                          // User limit\n        computeUserLimit(application, clusterResource, required);\n    \n\n    Resource queueMaxCap =                        // Queue Max-Capacity\n        Resources.createResource(\n            CSQueueUtils.roundDown(minimumAllocation, \n                (int)(absoluteMaxCapacity * clusterResource.getMemory()))\n            );\n    \n    Resource userConsumed = getUser(user).getConsumedResources(); \n    Resource headroom = \n        Resources.subtract(Resources.min(userLimit, queueMaxCap), userConsumed);\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Headroom calculation for user \" + user + \": \" + \n          \" userLimit=\" + userLimit + \n          \" queueMaxCap=\" + queueMaxCap + \n          \" consumed=\" + userConsumed + \n          \" headroom=\" + headroom);\n    }\n    \n    application.setHeadroom(headroom);\n    metrics.setAvailableResourcesToUser(user, headroom);\n    \n    return userLimit;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.needContainers": "  boolean needContainers(SchedulerApp application, Priority priority, Resource required) {\n    int requiredContainers = application.getTotalRequiredResources(priority);\n    int reservedContainers = application.getNumReservedContainers(priority);\n    int starvation = 0;\n    if (reservedContainers > 0) {\n      float nodeFactor = \n          ((float)required.getMemory() / getMaximumAllocation().getMemory());\n      \n      // Use percentage of node required to bias against large containers...\n      // Protect against corner case where you need the whole node with\n      // Math.min(nodeFactor, minimumAllocationFactor)\n      starvation = \n          (int)((application.getReReservations(priority) / (float)reservedContainers) * \n                (1.0f - (Math.min(nodeFactor, getMinimumAllocationFactor())))\n               );\n      \n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"needsContainers:\" +\n            \" app.#re-reserve=\" + application.getReReservations(priority) + \n            \" reserved=\" + reservedContainers + \n            \" nodeFactor=\" + nodeFactor + \n            \" minAllocFactor=\" + minimumAllocationFactor +\n            \" starvation=\" + starvation);\n      }\n    }\n    return (((starvation + requiredContainers) - reservedContainers) > 0);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignToUser": "  private synchronized boolean assignToUser(String userName, Resource limit) {\n\n    User user = getUser(userName);\n    \n    // Note: We aren't considering the current request since there is a fixed\n    // overhead of the AM, but it's a > check, not a >= check, so... \n    if ((user.getConsumedResources().getMemory()) > limit.getMemory()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"User \" + userName + \" in queue \" + getQueueName() + \n            \" will exceed limit - \" +  \n            \" consumed: \" + user.getConsumedResources() + \n            \" limit: \" + limit\n        );\n      }\n      return false;\n    }\n\n    return true;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getApplication": "  private synchronized SchedulerApp getApplication(\n      ApplicationAttemptId applicationAttemptId) {\n    return applicationsMap.get(applicationAttemptId);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.allocateResource": "  synchronized void allocateResource(Resource clusterResource, \n      SchedulerApp application, Resource resource) {\n    // Update queue metrics\n    Resources.addTo(usedResources, resource);\n    CSQueueUtils.updateQueueStatistics(\n        this, parent, clusterResource, minimumAllocation);\n    ++numContainers;\n\n    // Update user metrics\n    String userName = application.getUser();\n    User user = getUser(userName);\n    user.assignContainer(resource);\n    Resources.subtractFrom(application.getHeadroom(), resource); // headroom\n    metrics.setAvailableResourcesToUser(userName, application.getHeadroom());\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.info(getQueueName() + \n          \" user=\" + userName + \n          \" used=\" + usedResources + \" numContainers=\" + numContainers +\n          \" headroom = \" + application.getHeadroom() +\n          \" user-resources=\" + user.getConsumedResources()\n          );\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate": "  private synchronized void nodeUpdate(RMNode nm, \n      List<ContainerStatus> newlyLaunchedContainers,\n      List<ContainerStatus> completedContainers) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"nodeUpdate: \" + nm + \" clusterResources: \" + clusterResource);\n    }\n                  \n    SchedulerNode node = getNode(nm.getNodeID());\n\n    // Processing the newly launched containers\n    for (ContainerStatus launchedContainer : newlyLaunchedContainers) {\n      containerLaunchedOnNode(launchedContainer.getContainerId(), node);\n    }\n\n    // Process completed containers\n    for (ContainerStatus completedContainer : completedContainers) {\n      ContainerId containerId = completedContainer.getContainerId();\n      LOG.debug(\"Container FINISHED: \" + containerId);\n      completedContainer(getRMContainer(containerId), \n          completedContainer, RMContainerEventType.FINISHED);\n    }\n\n    // Now node data structures are upto date and ready for scheduling.\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Node being looked for scheduling \" + nm\n        + \" availableResource: \" + node.getAvailableResource());\n    }\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer = node.getReservedContainer();\n    if (reservedContainer != null) {\n      SchedulerApp reservedApplication = \n          getApplication(reservedContainer.getApplicationAttemptId());\n      \n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \" + \n          reservedApplication.getApplicationId() + \" on node: \" + nm);\n      \n      LeafQueue queue = ((LeafQueue)reservedApplication.getQueue());\n      queue.assignContainers(clusterResource, node);\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() == null) {\n      root.assignContainers(clusterResource, node);\n    } else {\n      LOG.info(\"Skipping scheduling since node \" + nm + \n          \" is reserved by application \" + \n          node.getReservedContainer().getContainerId().getApplicationAttemptId()\n          );\n    }\n\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.containerLaunchedOnNode": "  private void containerLaunchedOnNode(ContainerId containerId, SchedulerNode node) {\n    // Get the application for the finished container\n    ApplicationAttemptId applicationAttemptId = containerId.getApplicationAttemptId();\n    SchedulerApp application = getApplication(applicationAttemptId);\n    if (application == null) {\n      LOG.info(\"Unknown application: \" + applicationAttemptId + \n          \" launched container \" + containerId +\n          \" on node: \" + node);\n      this.rmContext.getDispatcher().getEventHandler()\n        .handle(new RMNodeCleanContainerEvent(node.getNodeID(), containerId));\n      return;\n    }\n    \n    application.containerLaunchedOnNode(containerId, node.getNodeID());\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getRMContainer": "  private RMContainer getRMContainer(ContainerId containerId) {\n    SchedulerApp application = \n        getApplication(containerId.getApplicationAttemptId());\n    return (application == null) ? null : application.getRMContainer(containerId);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getQueue": "  synchronized CSQueue getQueue(String queueName) {\n    return queues.get(queueName);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getNode": "  SchedulerNode getNode(NodeId nodeId) {\n    return nodes.get(nodeId);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getApplication": "  SchedulerApp getApplication(ApplicationAttemptId applicationAttemptId) {\n    return applications.get(applicationAttemptId);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainer": "  private synchronized void completedContainer(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) {\n    if (rmContainer == null) {\n      LOG.info(\"Null container completed...\");\n      return;\n    }\n    \n    Container container = rmContainer.getContainer();\n    \n    // Get the application for the finished container\n    ApplicationAttemptId applicationAttemptId = container.getId().getApplicationAttemptId();\n    SchedulerApp application = getApplication(applicationAttemptId);\n    if (application == null) {\n      LOG.info(\"Container \" + container + \" of\" +\n      \t\t\" unknown application \" + applicationAttemptId + \n          \" completed with event \" + event);\n      return;\n    }\n    \n    // Get the node on which the container was allocated\n    SchedulerNode node = getNode(container.getNodeId());\n    \n    // Inform the queue\n    LeafQueue queue = (LeafQueue)application.getQueue();\n    queue.completedContainer(clusterResource, application, node, \n        rmContainer, containerStatus, event);\n\n    LOG.info(\"Application \" + applicationAttemptId + \n        \" released container \" + container.getId() +\n        \" on node: \" + node + \n        \" with event: \" + event);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle": "  public void handle(SchedulerEvent event) {\n    switch(event.getType()) {\n    case NODE_ADDED:\n    {\n      NodeAddedSchedulerEvent nodeAddedEvent = (NodeAddedSchedulerEvent)event;\n      addNode(nodeAddedEvent.getAddedRMNode());\n    }\n    break;\n    case NODE_REMOVED:\n    {\n      NodeRemovedSchedulerEvent nodeRemovedEvent = (NodeRemovedSchedulerEvent)event;\n      removeNode(nodeRemovedEvent.getRemovedRMNode());\n    }\n    break;\n    case NODE_UPDATE:\n    {\n      NodeUpdateSchedulerEvent nodeUpdatedEvent = (NodeUpdateSchedulerEvent)event;\n      nodeUpdate(nodeUpdatedEvent.getRMNode(), \n          nodeUpdatedEvent.getNewlyLaunchedContainers(),\n          nodeUpdatedEvent.getCompletedContainers());\n    }\n    break;\n    case APP_ADDED:\n    {\n      AppAddedSchedulerEvent appAddedEvent = (AppAddedSchedulerEvent)event;\n      addApplication(appAddedEvent.getApplicationAttemptId(), appAddedEvent\n          .getQueue(), appAddedEvent.getUser());\n    }\n    break;\n    case APP_REMOVED:\n    {\n      AppRemovedSchedulerEvent appRemovedEvent = (AppRemovedSchedulerEvent)event;\n      doneApplication(appRemovedEvent.getApplicationAttemptID(),\n          appRemovedEvent.getFinalAttemptState());\n    }\n    break;\n    case CONTAINER_EXPIRED:\n    {\n      ContainerExpiredSchedulerEvent containerExpiredEvent = \n          (ContainerExpiredSchedulerEvent) event;\n      ContainerId containerId = containerExpiredEvent.getContainerId();\n      completedContainer(getRMContainer(containerId), \n          SchedulerUtils.createAbnormalContainerStatus(\n              containerId, \n              SchedulerUtils.EXPIRED_CONTAINER), \n          RMContainerEventType.EXPIRE);\n    }\n    break;\n    default:\n      LOG.error(\"Invalid eventtype \" + event.getType() + \". Ignoring!\");\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplication": "  private synchronized void\n      addApplication(ApplicationAttemptId applicationAttemptId,\n          String queueName, String user) {\n\n    // Sanity checks\n    CSQueue queue = getQueue(queueName);\n    if (queue == null) {\n      String message = \"Application \" + applicationAttemptId + \n      \" submitted by user \" + user + \" to unknown queue: \" + queueName;\n      this.rmContext.getDispatcher().getEventHandler().handle(\n          new RMAppAttemptRejectedEvent(applicationAttemptId, message));\n      return;\n    }\n    if (!(queue instanceof LeafQueue)) {\n      String message = \"Application \" + applicationAttemptId + \n          \" submitted by user \" + user + \" to non-leaf queue: \" + queueName;\n      this.rmContext.getDispatcher().getEventHandler().handle(\n          new RMAppAttemptRejectedEvent(applicationAttemptId, message));\n      return;\n    }\n\n    // TODO: Fix store\n    SchedulerApp SchedulerApp = \n        new SchedulerApp(applicationAttemptId, user, queue, \n            queue.getActiveUsersManager(), rmContext, null);\n\n    // Submit to the queue\n    try {\n      queue.submitApplication(SchedulerApp, user, queueName);\n    } catch (AccessControlException ace) {\n      LOG.info(\"Failed to submit application \" + applicationAttemptId + \n          \" to queue \" + queueName + \" from user \" + user, ace);\n      this.rmContext.getDispatcher().getEventHandler().handle(\n          new RMAppAttemptRejectedEvent(applicationAttemptId, \n              ace.toString()));\n      return;\n    }\n\n    applications.put(applicationAttemptId, SchedulerApp);\n\n    LOG.info(\"Application Submission: \" + applicationAttemptId + \n        \", user: \" + user +\n        \" queue: \" + queue +\n        \", currently active: \" + applications.size());\n\n    rmContext.getDispatcher().getEventHandler().handle(\n        new RMAppAttemptEvent(applicationAttemptId,\n            RMAppAttemptEventType.APP_ACCEPTED));\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addNode": "  private synchronized void addNode(RMNode nodeManager) {\n    this.nodes.put(nodeManager.getNodeID(), new SchedulerNode(nodeManager));\n    Resources.addTo(clusterResource, nodeManager.getTotalCapability());\n    root.updateClusterResource(clusterResource);\n    ++numNodeManagers;\n    LOG.info(\"Added node \" + nodeManager.getNodeAddress() + \n        \" clusterResource: \" + clusterResource);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.doneApplication": "  private synchronized void doneApplication(\n      ApplicationAttemptId applicationAttemptId,\n      RMAppAttemptState rmAppAttemptFinalState) {\n    LOG.info(\"Application \" + applicationAttemptId + \" is done.\" +\n    \t\t\" finalState=\" + rmAppAttemptFinalState);\n    \n    SchedulerApp application = getApplication(applicationAttemptId);\n\n    if (application == null) {\n      //      throw new IOException(\"Unknown application \" + applicationId + \n      //          \" has completed!\");\n      LOG.info(\"Unknown application \" + applicationAttemptId + \" has completed!\");\n      return;\n    }\n    \n    // Release all the running containers \n    for (RMContainer rmContainer : application.getLiveContainers()) {\n      completedContainer(rmContainer, \n          SchedulerUtils.createAbnormalContainerStatus(\n              rmContainer.getContainerId(), \n              SchedulerUtils.COMPLETED_APPLICATION), \n          RMContainerEventType.KILL);\n    }\n    \n     // Release all reserved containers\n    for (RMContainer rmContainer : application.getReservedContainers()) {\n      completedContainer(rmContainer, \n          SchedulerUtils.createAbnormalContainerStatus(\n              rmContainer.getContainerId(), \n              \"Application Complete\"), \n          RMContainerEventType.KILL);\n    }\n    \n    // Clean up pending requests, metrics etc.\n    application.stop(rmAppAttemptFinalState);\n    \n    // Inform the queue\n    String queueName = application.getQueue().getQueueName();\n    CSQueue queue = queues.get(queueName);\n    if (!(queue instanceof LeafQueue)) {\n      LOG.error(\"Cannot finish application \" + \"from non-leaf queue: \"\n          + queueName);\n    } else {\n      queue.finishApplication(application, queue.getQueueName());\n    }\n    \n    // Remove from our data-structure\n    applications.remove(applicationAttemptId);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.removeNode": "  private synchronized void removeNode(RMNode nodeInfo) {\n    SchedulerNode node = this.nodes.get(nodeInfo.getNodeID());\n    if (node == null) {\n      return;\n    }\n    Resources.subtractFrom(clusterResource, node.getRMNode().getTotalCapability());\n    root.updateClusterResource(clusterResource);\n    --numNodeManagers;\n\n    // Remove running containers\n    List<RMContainer> runningContainers = node.getRunningContainers();\n    for (RMContainer container : runningContainers) {\n      completedContainer(container, \n          SchedulerUtils.createAbnormalContainerStatus(\n              container.getContainerId(), \n              SchedulerUtils.LOST_CONTAINER), \n          RMContainerEventType.KILL);\n    }\n    \n    // Remove reservations, if any\n    RMContainer reservedContainer = node.getReservedContainer();\n    if (reservedContainer != null) {\n      completedContainer(reservedContainer, \n          SchedulerUtils.createAbnormalContainerStatus(\n              reservedContainer.getContainerId(), \n              SchedulerUtils.LOST_CONTAINER), \n          RMContainerEventType.KILL);\n    }\n\n    this.nodes.remove(nodeInfo.getNodeID());\n    LOG.info(\"Removed node \" + nodeInfo.getNodeAddress() + \n        \" clusterResource: \" + clusterResource);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.run": "      public void run() {\n\n        SchedulerEvent event;\n\n        while (!stopped && !Thread.currentThread().isInterrupted()) {\n          try {\n            event = eventQueue.take();\n          } catch (InterruptedException e) {\n            LOG.error(\"Returning, interrupted : \" + e);\n            return; // TODO: Kill RM.\n          }\n\n          try {\n            scheduler.handle(event);\n          } catch (Throwable t) {\n            LOG.fatal(\"Error in handling event type \" + event.getType()\n                + \" to the scheduler\", t);\n            if (getConfig().getBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY,\n              Dispatcher.DEFAULT_DISPATCHER_EXIT_ON_ERROR)) {\n              LOG.info(\"Exiting, bbye..\");\n              System.exit(-1);\n            }\n          }\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.handle": "    public void handle(RMNodeEvent event) {\n      NodeId nodeId = event.getNodeId();\n      RMNode node = this.rmContext.getRMNodes().get(nodeId);\n      if (node != null) {\n        try {\n          ((EventHandler<RMNodeEvent>) node).handle(event);\n        } catch (Throwable t) {\n          LOG.error(\"Error in handling event type \" + event.getType()\n              + \" for node \" + nodeId, t);\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment.getResource": "  public Resource getResource() {\n    return resource;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue.assignContainers": "  public CSAssignment assignContainers(\n      Resource clusterResource, SchedulerNode node);\n  \n  /**\n   * A container assigned to the queue has completed.\n   * @param clusterResource the resource of the cluster\n   * @param application application to which the container was assigned\n   * @param node node on which the container completed\n   * @param container completed container, \n   *                  <code>null</code> if it was just a reservation\n   * @param containerStatus <code>ContainerStatus</code> for the completed \n   *                        container\n   * @param event event to be sent to the container\n   */\n  public void completedContainer(Resource clusterResource,\n      SchedulerApp application, SchedulerNode node, \n      RMContainer container, ContainerStatus containerStatus, \n      RMContainerEventType event);\n\n  /**\n   * Get the number of applications in the queue.\n   * @return number of applications\n   */\n  public int getNumApplications();\n\n  \n  /**\n   * Reinitialize the queue.\n   * @param queue new queue to re-initalize from\n   * @param clusterResource resources in the cluster\n   */\n  public void reinitialize(CSQueue queue, Resource clusterResource) \n  throws IOException;\n\n   /**\n   * Update the cluster resource for queues as we add/remove nodes\n   * @param clusterResource the current cluster resource\n   */\n  public void updateClusterResource(Resource clusterResource);\n  \n  /**\n   * Get the {@link ActiveUsersManager} for the queue."
        },
        "bug_report": {
            "Title": "ResourceManager NPE while handling NODE_UPDATE",
            "Description": "The RM on one of our clusters has exited twice in the past few days because of an NPE while trying to handle a NODE_UPDATE:\n\n{noformat}\n2012-04-12 02:09:01,672 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler\n [ResourceManager Event Processor]java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:261)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:223)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:246)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1229)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1078)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1048)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignReservedContainer(LeafQueue.java:859)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:756)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:573)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:622)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:78)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:302)\n        at java.lang.Thread.run(Thread.java:619)\n{noformat}\n\nThis is very similar to the failure reported in MAPREDUCE-3005."
        }
    },
    {
        "filename": "MAPREDUCE-5924.json",
        "creation_time": "2014-06-13T05:02:09.000+0000",
        "stack_trace": "```\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_COMMIT_PENDING at COMMIT_PENDING\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1058)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:145)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1271)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1263)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)\n        at java.lang.Thread.run(Thread.java:722)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle": "  public void handle(TaskAttemptEvent event) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Processing \" + event.getTaskAttemptID() + \" of type \"\n          + event.getType());\n    }\n    writeLock.lock();\n    try {\n      final TaskAttemptStateInternal oldState = getInternalState()  ;\n      try {\n        stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state for \"\n            + this.attemptId, e);\n        eventHandler.handle(new JobDiagnosticsUpdateEvent(\n            this.attemptId.getTaskId().getJobId(), \"Invalid event \" + event.getType() + \n            \" on TaskAttempt \" + this.attemptId));\n        eventHandler.handle(new JobEvent(this.attemptId.getTaskId().getJobId(),\n            JobEventType.INTERNAL_ERROR));\n      }\n      if (oldState != getInternalState()) {\n          LOG.info(attemptId + \" TaskAttempt Transitioned from \" \n           + oldState + \" to \"\n           + getInternalState());\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getInternalState": "  public TaskAttemptStateInternal getInternalState() {\n    readLock.lock();\n    try {\n      return stateMachine.getCurrentState();\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.handle": "    public void handle(Event event) {\n      //Empty\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJob": "    public Job getJob(JobId jobID) {\n      return jobs.get(jobID);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob": "  public void shutDownJob() {\n    // job has finished\n    // this is the only job, so shut down the Appmaster\n    // note in a workflow scenario, this may lead to creation of a new\n    // job (FIXME?)\n\n    try {\n      //if isLastAMRetry comes as true, should never set it to false\n      if ( !isLastAMRetry){\n        if (((JobImpl)job).getInternalState() != JobStateInternal.REBOOT) {\n          LOG.info(\"We are finishing cleanly so this is the last retry\");\n          isLastAMRetry = true;\n        }\n      }\n      notifyIsLastAMRetry(isLastAMRetry);\n      // Stop all services\n      // This will also send the final report to the ResourceManager\n      LOG.info(\"Calling stop for all the services\");\n      MRAppMaster.this.stop();\n\n      if (isLastAMRetry) {\n        // Send job-end notification when it is safe to report termination to\n        // users and it is the last AM retry\n        if (getConfig().get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL) != null) {\n          try {\n            LOG.info(\"Job end notification started for jobID : \"\n                + job.getReport().getJobId());\n            JobEndNotifier notifier = new JobEndNotifier();\n            notifier.setConf(getConfig());\n            JobReport report = job.getReport();\n            // If unregistration fails, the final state is unavailable. However,\n            // at the last AM Retry, the client will finally be notified FAILED\n            // from RM, so we should let users know FAILED via notifier as well\n            if (!context.hasSuccessfullyUnregistered()) {\n              report.setJobState(JobState.FAILED);\n            }\n            notifier.notify(report);\n          } catch (InterruptedException ie) {\n            LOG.warn(\"Job end notification interrupted for jobID : \"\n                + job.getReport().getJobId(), ie);\n          }\n        }\n      }\n\n      try {\n        Thread.sleep(5000);\n      } catch (InterruptedException e) {\n        e.printStackTrace();\n      }\n      clientService.stop();\n    } catch (Throwable t) {\n      LOG.warn(\"Graceful stop failed \", t);\n    }\n\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJobId": "  public JobId getJobId() {\n    return jobId;\n  }"
        },
        "bug_report": {
            "Title": "Windows: Sort Job failed due to 'Invalid event: TA_COMMIT_PENDING at COMMIT_PENDING'",
            "Description": "The Sort job over 1GB data failed with below error\n{code}\n2014-06-09 09:15:38,746 INFO [Socket Reader #1 for port 63415] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1402304714683_0002 (auth:SIMPLE)\n2014-06-09 09:15:38,750 INFO [IPC Server handler 13 on 63415] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Commit-pending state update from attempt_1402304714683_0002_r_000015_1000\n2014-06-09 09:15:38,751 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Can't handle this event at current state for attempt_1402304714683_0002_r_000015_1000\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_COMMIT_PENDING at COMMIT_PENDING\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1058)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:145)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1271)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1263)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)\n        at java.lang.Thread.run(Thread.java:722)\n2014-06-09 09:15:38,753 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1402304714683_0002Job Transitioned from RUNNING to ERROR\n\n{code}\n\nThe JobHistory Url prints job state = ERROR"
        }
    },
    {
        "filename": "MAPREDUCE-3649.json",
        "creation_time": "2012-01-09T23:53:06.000+0000",
        "stack_trace": "```\njava.net.UnknownServiceException: no content-type\n\tat java.net.URLConnection.getContentHandler(URLConnection.java:1192)\n\tat java.net.URLConnection.getContent(URLConnection.java:689)\n\tat org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notifyURLOnce(JobEndNotifier.java:95)\n\tat org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notify(JobEndNotifier.java:139)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:388)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:375)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notifyURLOnce": "  protected boolean notifyURLOnce() {\n    boolean success = false;\n    try {\n      Log.info(\"Job end notification trying \" + urlToNotify);\n      URLConnection conn = urlToNotify.openConnection();\n      conn.setConnectTimeout(5*1000);\n      conn.setReadTimeout(5*1000);\n      conn.setAllowUserInteraction(false);\n      InputStream is = conn.getInputStream();\n      conn.getContent();\n      is.close();\n      success = true;\n      Log.info(\"Job end notification to \" + urlToNotify + \" succeeded\");\n    } catch(IOException ioe) {\n      Log.warn(\"Job end notification to \" + urlToNotify + \" failed\", ioe);\n    }\n    return success;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notify": "  public void notify(JobReport jobReport)\n    throws InterruptedException {\n    // Do we need job-end notification?\n    if (userUrl == null) {\n      Log.info(\"Job end notification URL not set, skipping.\");\n      return;\n    }\n\n    //Do string replacements for jobId and jobStatus\n    if (userUrl.contains(JOB_ID)) {\n      userUrl = userUrl.replace(JOB_ID, jobReport.getJobId().toString());\n    }\n    if (userUrl.contains(JOB_STATUS)) {\n      userUrl = userUrl.replace(JOB_STATUS, jobReport.getJobState().toString());\n    }\n\n    // Create the URL, ensure sanity\n    try {\n      urlToNotify = new URL(userUrl);\n    } catch (MalformedURLException mue) {\n      Log.warn(\"Job end notification couldn't parse \" + userUrl, mue);\n      return;\n    }\n\n    // Send notification\n    boolean success = false;\n    while (numTries-- > 0 && !success) {\n      Log.info(\"Job end notification attempts left \" + numTries);\n      success = notifyURLOnce();\n      if (!success) {\n        Thread.sleep(waitInterval);\n      }\n    }\n    if (!success) {\n      Log.warn(\"Job end notification failed to notify : \" + urlToNotify);\n    } else {\n      Log.info(\"Job end notification succeeded for \" + jobReport.getJobId());\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.handle": "    public void handle(SpeculatorEvent event) {\n      if (!disabled && \n          (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false))) {\n        // Speculator IS enabled, direct the event to there.\n        speculator.handle(event);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.cleanupStagingDir": "  public void cleanupStagingDir() throws IOException {\n    /* make sure we clean the staging files */\n    String jobTempDir = null;\n    FileSystem fs = getFileSystem(getConfig());\n    try {\n      if (!keepJobFiles(new JobConf(getConfig()))) {\n        jobTempDir = getConfig().get(MRJobConfig.MAPREDUCE_JOB_DIR);\n        if (jobTempDir == null) {\n          LOG.warn(\"Job Staging directory is null\");\n          return;\n        }\n        Path jobTempDirPath = new Path(jobTempDir);\n        LOG.info(\"Deleting staging directory \" + FileSystem.getDefaultUri(getConfig()) +\n            \" \" + jobTempDir);\n        fs.delete(jobTempDirPath, true);\n      }\n    } catch(IOException io) {\n      LOG.error(\"Failed to cleanup staging dir \" + jobTempDir, io);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop": "    public synchronized void stop() {\n      ((Service)this.containerLauncher).stop();\n      super.stop();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJobId": "  public JobId getJobId() {\n    return jobId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJob": "    public Job getJob(JobId jobID) {\n      return jobs.get(jobID);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.sysexit": "  protected void sysexit() {\n    System.exit(0);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch": "  protected void dispatch(Event event) {\n    //all events go thru this loop\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Dispatching the event \" + event.getClass().getName() + \".\"\n          + event.toString());\n    }\n\n    Class<? extends Enum> type = event.getType().getDeclaringClass();\n\n    try{\n      eventDispatchers.get(type).handle(event);\n    }\n    catch (Throwable t) {\n      //TODO Maybe log the state of the queue\n      LOG.fatal(\"Error in dispatcher thread. Exiting..\", t);\n      if (exitOnDispatchException) {\n        System.exit(-1);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.handle": "    public void handle(Event event) {\n      for (EventHandler<Event> handler: listofHandlers) {\n        handler.handle(event);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.run": "      public void run() {\n        while (!stopped && !Thread.currentThread().isInterrupted()) {\n          Event event;\n          try {\n            event = eventQueue.take();\n          } catch(InterruptedException ie) {\n            LOG.warn(\"AsyncDispatcher thread interrupted\", ie);\n            return;\n          }\n          if (event != null) {\n            dispatch(event);\n          }\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.setConf": "  public void setConf(Configuration conf) {\n    this.conf = conf;\n    \n    numTries = Math.min(\n      conf.getInt(MRJobConfig.MR_JOB_END_RETRY_ATTEMPTS, 0) + 1\n      , conf.getInt(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_ATTEMPTS, 1)\n    );\n    waitInterval = Math.min(\n    conf.getInt(MRJobConfig.MR_JOB_END_RETRY_INTERVAL, 5)\n    , conf.getInt(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_RETRY_INTERVAL, 5)\n    );\n    waitInterval = (waitInterval < 0) ? 5 : waitInterval;\n\n    userUrl = conf.get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.Event.toString": "  String toString();\n}",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.Event.getType": "  TYPE getType();\n  long getTimestamp();\n  String toString();\n}"
        },
        "bug_report": {
            "Title": "Job End notification gives an error on calling back.",
            "Description": "When calling job end notification for oozie the AM fails with the following trace:\n\n{noformat}\n2012-01-09 23:45:41,732 WARN [AsyncDispatcher event handler] org.mortbay.log: Job end notification to http://HOST:11000/oozie/v0/callback?id=0000000-120109234442311-oozie-oozi-W@mr-node&status=SUCCEEDED& failed\njava.net.UnknownServiceException: no content-type\n\tat java.net.URLConnection.getContentHandler(URLConnection.java:1192)\n\tat java.net.URLConnection.getContent(URLConnection.java:689)\n\tat org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notifyURLOnce(JobEndNotifier.java:95)\n\tat org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notify(JobEndNotifier.java:139)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:388)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:375)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)\n{noformat}"
        }
    },
    {
        "filename": "MAPREDUCE-7077.json",
        "creation_time": "2018-04-11T18:35:34.000+0000",
        "stack_trace": "```\njava.io.FileNotFoundException: /grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/jobTokenPassword (Permission denied)\n\n at java.io.FileOutputStream.open0(Native Method)\n\n at java.io.FileOutputStream.open(FileOutputStream.java:270)\n\n at java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\n at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:236)\n\n at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:219)\n\n at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:318)\n\n at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:307)\n\n at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:338)\n\n at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:401)\n\n at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:464)\n\n at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1169)\n\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1149)\n\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1038)\n\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1026)\n\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:703)\n\n at org.apache.hadoop.mapred.pipes.Application.writePasswordToLocalFile(Application.java:173)\n\n at org.apache.hadoop.mapred.pipes.Application.<init>(Application.java:109)\n\n at org.apache.hadoop.mapred.pipes.PipesReducer.startApplication(PipesReducer.java:87)\n\n at org.apache.hadoop.mapred.pipes.PipesReducer.reduce(PipesReducer.java:65)\n\n at org.apache.hadoop.mapred.pipes.PipesReducer.reduce(PipesReducer.java:38)\n\n at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:445)\n\n at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:393)\n\n at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n\n at java.security.AccessController.doPrivileged(Native Method)\n\n at javax.security.auth.Subject.doAs(Subject.java:422)\n\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)\n\n at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode": "  protected OutputStream createOutputStreamWithMode(Path f, boolean append,\n      FsPermission permission) throws IOException {\n    return new LocalFSFileOutputStream(f, append, permission);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.RawLocalFileSystem.create": "  public FSDataOutputStream create(Path f, FsPermission permission,\n    boolean overwrite, int bufferSize, short replication, long blockSize,\n    Progressable progress) throws IOException {\n\n    FSDataOutputStream out = create(f, overwrite, true, bufferSize, replication,\n        blockSize, progress, permission);\n    return out;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.RawLocalFileSystem.toString": "  public String toString() {\n    return \"LocalFS\";\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.RawLocalFileSystem.mkdirs": "  public boolean mkdirs(Path f, FsPermission permission) throws IOException {\n    return mkdirsWithOptionalPermission(f, permission);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.ChecksumFileSystem.create": "  private FSDataOutputStream create(Path f, FsPermission permission,\n      boolean overwrite, boolean createParent, int bufferSize,\n      short replication, long blockSize,\n      Progressable progress) throws IOException {\n    Path parent = f.getParent();\n    if (parent != null) {\n      if (!createParent && !exists(parent)) {\n        throw new FileNotFoundException(\"Parent directory doesn't exist: \"\n            + parent);\n      } else if (!mkdirs(parent)) {\n        throw new IOException(\"Mkdirs failed to create \" + parent\n            + \" (exists=\" + exists(parent) + \", cwd=\" + getWorkingDirectory()\n            + \")\");\n      }\n    }\n    final FSDataOutputStream out;\n    if (writeChecksum) {\n      out = new FSDataOutputStream(\n          new ChecksumFSOutputSummer(this, f, overwrite, bufferSize, replication,\n              blockSize, progress, permission), null);\n    } else {\n      out = fs.create(f, permission, overwrite, bufferSize, replication,\n          blockSize, progress);\n      // remove the checksum file since we aren't writing one\n      Path checkFile = getChecksumFile(f);\n      if (fs.exists(checkFile)) {\n        fs.delete(checkFile, true);\n      }\n    }\n    return out;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.ChecksumFileSystem.delete": "  public boolean delete(Path f, boolean recursive) throws IOException{\n    FileStatus fstatus = null;\n    try {\n      fstatus = fs.getFileStatus(f);\n    } catch(FileNotFoundException e) {\n      return false;\n    }\n    if (fstatus.isDirectory()) {\n      //this works since the crcs are in the same\n      //directories and the files. so we just delete\n      //everything in the underlying filesystem\n      return fs.delete(f, recursive);\n    } else {\n      Path checkFile = getChecksumFile(f);\n      if (fs.exists(checkFile)) {\n        fs.delete(checkFile, true);\n      }\n      return fs.delete(f, true);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.ChecksumFileSystem.mkdirs": "  public boolean mkdirs(Path f) throws IOException {\n    return fs.mkdirs(f);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.ChecksumFileSystem.getChecksumFile": "  public Path getChecksumFile(Path file) {\n    return new Path(file.getParent(), \".\" + file.getName() + \".crc\");\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.create": "  public FSDataOutputStream create(Path f,\n      FsPermission permission,\n      EnumSet<CreateFlag> flags,\n      int bufferSize,\n      short replication,\n      long blockSize,\n      Progressable progress,\n      ChecksumOpt checksumOpt) throws IOException {\n    // Checksum options are ignored by default. The file systems that\n    // implement checksum need to override this method. The full\n    // support is currently only available in DFS.\n    return create(f, permission, flags.contains(CreateFlag.OVERWRITE),\n        bufferSize, replication, blockSize, progress);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.setPermission": "  public void setPermission(Path p, FsPermission permission\n      ) throws IOException {\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getDefaultReplication": "  public short getDefaultReplication(Path path) {\n    return getDefaultReplication();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getDefaultBlockSize": "  public long getDefaultBlockSize(Path f) {\n    return getDefaultBlockSize();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.pipes.Application.writePasswordToLocalFile": "  private void writePasswordToLocalFile(String localPasswordFile,\n      byte[] password, JobConf conf) throws IOException {\n    FileSystem localFs = FileSystem.getLocal(conf);\n    Path localPath = new Path(localPasswordFile);\n    FSDataOutputStream out = FileSystem.create(localFs, localPath,\n        new FsPermission(\"400\"));\n    out.write(password);\n    out.close();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.pipes.PipesReducer.startApplication": "  private void startApplication(OutputCollector<K3, V3> output, Reporter reporter) throws IOException {\n    if (application == null) {\n      try {\n        LOG.info(\"starting application\");\n        application = \n          new Application<K2, V2, K3, V3>(\n              job, null, output, reporter, \n              (Class<? extends K3>) job.getOutputKeyClass(), \n              (Class<? extends V3>) job.getOutputValueClass());\n        downlink = application.getDownlink();\n      } catch (InterruptedException ie) {\n        throw new RuntimeException(\"interrupted\", ie);\n      }\n      int reduce=0;\n      downlink.runReduce(reduce, Submitter.getIsJavaRecordWriter(job));\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.pipes.PipesReducer.reduce": "  public void reduce(K2 key, Iterator<V2> values, \n                     OutputCollector<K3, V3> output, Reporter reporter\n                     ) throws IOException {\n    isOk = false;\n    startApplication(output, reporter);\n    downlink.reduceKey(key);\n    while (values.hasNext()) {\n      downlink.reduceValue(values.next());\n    }\n    if(skipping) {\n      //flush the streams on every record input if running in skip mode\n      //so that we don't buffer other records surrounding a bad record.\n      downlink.flush();\n    }\n    isOk = true;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.runOldReducer": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runOldReducer(JobConf job,\n                     TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator<INKEY> comparator,\n                     Class<INKEY> keyClass,\n                     Class<INVALUE> valueClass) throws IOException {\n    Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer = \n      ReflectionUtils.newInstance(job.getReducerClass(), job);\n    // make output collector\n    String finalName = getOutputName(getPartition());\n\n    RecordWriter<OUTKEY, OUTVALUE> out = new OldTrackingRecordWriter<OUTKEY, OUTVALUE>(\n        this, job, reporter, finalName);\n    final RecordWriter<OUTKEY, OUTVALUE> finalOut = out;\n    \n    OutputCollector<OUTKEY,OUTVALUE> collector = \n      new OutputCollector<OUTKEY,OUTVALUE>() {\n        public void collect(OUTKEY key, OUTVALUE value)\n          throws IOException {\n          finalOut.write(key, value);\n          // indicate that progress update needs to be sent\n          reporter.progress();\n        }\n      };\n    \n    // apply reduce function\n    try {\n      //increment processed counter only if skipping feature is enabled\n      boolean incrProcCount = SkipBadRecords.getReducerMaxSkipGroups(job)>0 &&\n        SkipBadRecords.getAutoIncrReducerProcCount(job);\n      \n      ReduceValuesIterator<INKEY,INVALUE> values = isSkipping() ? \n          new SkippingReduceValuesIterator<INKEY,INVALUE>(rIter, \n              comparator, keyClass, valueClass, \n              job, reporter, umbilical) :\n          new ReduceValuesIterator<INKEY,INVALUE>(rIter, \n          comparator, keyClass, valueClass,\n          job, reporter);\n      values.informReduceProgress();\n      while (values.more()) {\n        reduceInputKeyCounter.increment(1);\n        reducer.reduce(values.getKey(), values, collector, reporter);\n        if(incrProcCount) {\n          reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n              SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);\n        }\n        values.nextKey();\n        values.informReduceProgress();\n      }\n\n      reducer.close();\n      reducer = null;\n      \n      out.close(reporter);\n      out = null;\n    } finally {\n      IOUtils.cleanupWithLogger(LOG, reducer);\n      closeQuietly(out, reporter);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.close": "      public void close() throws IOException {\n        rawIter.close();\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.nextKey": "     public void nextKey() throws IOException {\n       super.nextKey();\n       mayBeSkip();\n     }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.newInstance": "         public Writable newInstance() { return new ReduceTask(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.getKey": "      public DataInputBuffer getKey() throws IOException {\n        return rawIter.getKey();\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.write": "    public void write(K key, V value) throws IOException, InterruptedException {\n      long bytesOutPrev = getOutputBytes(fsStats);\n      real.write(key,value);\n      long bytesOutCurr = getOutputBytes(fsStats);\n      fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);\n      outputRecordCounter.increment(1);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.informReduceProgress": "    public void informReduceProgress() {\n      reducePhase.set(super.in.getProgress().getProgress()); // update progress\n      reporter.progress();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.more": "     public boolean more() { \n       return super.more() && hasNext; \n     }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.closeQuietly": "  private <OUTKEY, OUTVALUE>\n  void closeQuietly(RecordWriter<OUTKEY, OUTVALUE> c, Reporter r) {\n    if (c != null) {\n      try {\n        c.close(r);\n      } catch (Exception e) {\n        LOG.info(\"Exception in closing \" + c, e);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.run": "  public void run(JobConf job, final TaskUmbilicalProtocol umbilical)\n    throws IOException, InterruptedException, ClassNotFoundException {\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n\n    if (isMapOrReduce()) {\n      copyPhase = getProgress().addPhase(\"copy\");\n      sortPhase  = getProgress().addPhase(\"sort\");\n      reducePhase = getProgress().addPhase(\"reduce\");\n    }\n    // start thread that will handle communication with parent\n    TaskReporter reporter = startReporter(umbilical);\n    \n    boolean useNewApi = job.getUseNewReducer();\n    initialize(job, getJobID(), reporter, useNewApi);\n\n    // check if it is a cleanupJobTask\n    if (jobCleanup) {\n      runJobCleanupTask(umbilical, reporter);\n      return;\n    }\n    if (jobSetup) {\n      runJobSetupTask(umbilical, reporter);\n      return;\n    }\n    if (taskCleanup) {\n      runTaskCleanupTask(umbilical, reporter);\n      return;\n    }\n    \n    // Initialize the codec\n    codec = initCodec();\n    RawKeyValueIterator rIter = null;\n    ShuffleConsumerPlugin shuffleConsumerPlugin = null;\n    \n    Class combinerClass = conf.getCombinerClass();\n    CombineOutputCollector combineCollector = \n      (null != combinerClass) ? \n     new CombineOutputCollector(reduceCombineOutputCounter, reporter, conf) : null;\n\n    Class<? extends ShuffleConsumerPlugin> clazz =\n          job.getClass(MRConfig.SHUFFLE_CONSUMER_PLUGIN, Shuffle.class, ShuffleConsumerPlugin.class);\n\t\t\t\t\t\n    shuffleConsumerPlugin = ReflectionUtils.newInstance(clazz, job);\n    LOG.info(\"Using ShuffleConsumerPlugin: \" + shuffleConsumerPlugin);\n\n    ShuffleConsumerPlugin.Context shuffleContext = \n      new ShuffleConsumerPlugin.Context(getTaskID(), job, FileSystem.getLocal(job), umbilical, \n                  super.lDirAlloc, reporter, codec, \n                  combinerClass, combineCollector, \n                  spilledRecordsCounter, reduceCombineInputCounter,\n                  shuffledMapsCounter,\n                  reduceShuffleBytes, failedShuffleCounter,\n                  mergedMapOutputsCounter,\n                  taskStatus, copyPhase, sortPhase, this,\n                  mapOutputFile, localMapFiles);\n    shuffleConsumerPlugin.init(shuffleContext);\n\n    rIter = shuffleConsumerPlugin.run();\n\n    // free up the data structures\n    mapOutputFilesOnDisk.clear();\n    \n    sortPhase.complete();                         // sort is complete\n    setPhase(TaskStatus.Phase.REDUCE); \n    statusUpdate(umbilical);\n    Class keyClass = job.getMapOutputKeyClass();\n    Class valueClass = job.getMapOutputValueClass();\n    RawComparator comparator = job.getOutputValueGroupingComparator();\n\n    if (useNewApi) {\n      runNewReducer(job, umbilical, reporter, rIter, comparator, \n                    keyClass, valueClass);\n    } else {\n      runOldReducer(job, umbilical, reporter, rIter, comparator, \n                    keyClass, valueClass);\n    }\n\n    shuffleConsumerPlugin.close();\n    done(umbilical, reporter);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.getProgress": "      public Progress getProgress() {\n        return rawIter.getProgress();\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.runNewReducer": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runNewReducer(JobConf job,\n                     final TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator<INKEY> comparator,\n                     Class<INKEY> keyClass,\n                     Class<INVALUE> valueClass\n                     ) throws IOException,InterruptedException, \n                              ClassNotFoundException {\n    // wrap value iterator to report progress.\n    final RawKeyValueIterator rawIter = rIter;\n    rIter = new RawKeyValueIterator() {\n      public void close() throws IOException {\n        rawIter.close();\n      }\n      public DataInputBuffer getKey() throws IOException {\n        return rawIter.getKey();\n      }\n      public Progress getProgress() {\n        return rawIter.getProgress();\n      }\n      public DataInputBuffer getValue() throws IOException {\n        return rawIter.getValue();\n      }\n      public boolean next() throws IOException {\n        boolean ret = rawIter.next();\n        reporter.setProgress(rawIter.getProgress().getProgress());\n        return ret;\n      }\n    };\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,\n          getTaskID(), reporter);\n    // make a reducer\n    org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer =\n      (org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE>)\n        ReflectionUtils.newInstance(taskContext.getReducerClass(), job);\n    org.apache.hadoop.mapreduce.RecordWriter<OUTKEY,OUTVALUE> trackedRW = \n      new NewTrackingRecordWriter<OUTKEY, OUTVALUE>(this, taskContext);\n    job.setBoolean(\"mapred.skip.on\", isSkipping());\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.Reducer.Context \n         reducerContext = createReduceContext(reducer, job, getTaskID(),\n                                               rIter, reduceInputKeyCounter, \n                                               reduceInputValueCounter, \n                                               trackedRW,\n                                               committer,\n                                               reporter, comparator, keyClass,\n                                               valueClass);\n    try {\n      reducer.run(reducerContext);\n    } finally {\n      trackedRW.close(reducerContext);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.initCodec": "  private CompressionCodec initCodec() {\n    // check if map-outputs are to be compressed\n    if (conf.getCompressMapOutput()) {\n      Class<? extends CompressionCodec> codecClass =\n        conf.getMapOutputCompressorClass(DefaultCodec.class);\n      return ReflectionUtils.newInstance(codecClass, conf);\n    } \n\n    return null;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.run": "              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.setEncryptedSpillKeyIfRequired": "  public static void setEncryptedSpillKeyIfRequired(Task task) throws\n          Exception {\n    if ((task != null) && (task.getEncryptedSpillKey() != null) && (task\n            .getEncryptedSpillKey().length > 1)) {\n      Credentials creds =\n              UserGroupInformation.getCurrentUser().getCredentials();\n      TokenCache.setEncryptedSpillKey(task.getEncryptedSpillKey(), creds);\n      UserGroupInformation.getCurrentUser().addCredentials(creds);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause == null) {\n        throw new RuntimeException(\"PrivilegedActionException with no \" +\n                \"underlying cause. UGI [\" + this + \"]\" +\": \" + pae, pae);\n      } else if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.main": "  public static void main(String[] args) throws Throwable {\n    Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\n    LOG.debug(\"Child starting\");\n\n    final JobConf job = new JobConf(MRJobConfig.JOB_CONF_FILE);\n    // Initing with our JobConf allows us to avoid loading confs twice\n    Limits.init(job);\n    UserGroupInformation.setConfiguration(job);\n    // MAPREDUCE-6565: need to set configuration for SecurityUtil.\n    SecurityUtil.setConfiguration(job);\n\n    String host = args[0];\n    int port = Integer.parseInt(args[1]);\n    final InetSocketAddress address =\n        NetUtils.createSocketAddrForHost(host, port);\n    final TaskAttemptID firstTaskid = TaskAttemptID.forName(args[2]);\n    long jvmIdLong = Long.parseLong(args[3]);\n    JVMId jvmId = new JVMId(firstTaskid.getJobID(),\n        firstTaskid.getTaskType() == TaskType.MAP, jvmIdLong);\n    \n    CallerContext.setCurrent(\n        new CallerContext.Builder(\"mr_\" + firstTaskid.toString()).build());\n\n    // initialize metrics\n    DefaultMetricsSystem.initialize(\n        StringUtils.camelize(firstTaskid.getTaskType().name()) +\"Task\");\n\n    // Security framework already loaded the tokens into current ugi\n    Credentials credentials =\n        UserGroupInformation.getCurrentUser().getCredentials();\n    LOG.info(\"Executing with tokens: {}\", credentials.getAllTokens());\n\n    // Create TaskUmbilicalProtocol as actual task owner.\n    UserGroupInformation taskOwner =\n      UserGroupInformation.createRemoteUser(firstTaskid.getJobID().toString());\n    Token<JobTokenIdentifier> jt = TokenCache.getJobToken(credentials);\n    SecurityUtil.setTokenService(jt, address);\n    taskOwner.addToken(jt);\n    final TaskUmbilicalProtocol umbilical =\n      taskOwner.doAs(new PrivilegedExceptionAction<TaskUmbilicalProtocol>() {\n      @Override\n      public TaskUmbilicalProtocol run() throws Exception {\n        return (TaskUmbilicalProtocol)RPC.getProxy(TaskUmbilicalProtocol.class,\n            TaskUmbilicalProtocol.versionID, address, job);\n      }\n    });\n\n    // report non-pid to application master\n    JvmContext context = new JvmContext(jvmId, \"-1000\");\n    LOG.debug(\"PID: \" + System.getenv().get(\"JVM_PID\"));\n    Task task = null;\n    UserGroupInformation childUGI = null;\n    ScheduledExecutorService logSyncer = null;\n\n    try {\n      int idleLoopCount = 0;\n      JvmTask myTask = null;\n      // poll for new task\n      for (int idle = 0; null == myTask; ++idle) {\n        long sleepTimeMilliSecs = Math.min(idle * 500, 1500);\n        LOG.info(\"Sleeping for \" + sleepTimeMilliSecs\n            + \"ms before retrying again. Got null now.\");\n        MILLISECONDS.sleep(sleepTimeMilliSecs);\n        myTask = umbilical.getTask(context);\n      }\n      if (myTask.shouldDie()) {\n        return;\n      }\n\n      task = myTask.getTask();\n      YarnChild.taskid = task.getTaskID();\n\n      // Create the job-conf and set credentials\n      configureTask(job, task, credentials, jt);\n\n      // log the system properties\n      String systemPropsToLog = MRApps.getSystemPropertiesToLog(job);\n      if (systemPropsToLog != null) {\n        LOG.info(systemPropsToLog);\n      }\n\n      // Initiate Java VM metrics\n      JvmMetrics.initSingleton(jvmId.toString(), job.getSessionId());\n      childUGI = UserGroupInformation.createRemoteUser(System\n          .getenv(ApplicationConstants.Environment.USER.toString()));\n      // Add tokens to new user so that it may execute its task correctly.\n      childUGI.addCredentials(credentials);\n\n      // set job classloader if configured before invoking the task\n      MRApps.setJobClassLoader(job);\n\n      logSyncer = TaskLog.createLogSyncer();\n\n      // Create a final reference to the task for the doAs block\n      final Task taskFinal = task;\n      childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n        @Override\n        public Object run() throws Exception {\n          // use job-specified working directory\n          setEncryptedSpillKeyIfRequired(taskFinal);\n          FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory());\n          taskFinal.run(job, umbilical); // run the task\n          return null;\n        }\n      });\n    } catch (FSError e) {\n      LOG.error(\"FSError from child\", e);\n      if (!ShutdownHookManager.get().isShutdownInProgress()) {\n        umbilical.fsError(taskid, e.getMessage());\n      }\n    } catch (Exception exception) {\n      LOG.warn(\"Exception running child : \"\n          + StringUtils.stringifyException(exception));\n      try {\n        if (task != null) {\n          // do cleanup for the task\n          if (childUGI == null) { // no need to job into doAs block\n            task.taskCleanup(umbilical);\n          } else {\n            final Task taskFinal = task;\n            childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n              @Override\n              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }\n            });\n          }\n        }\n      } catch (Exception e) {\n        LOG.info(\"Exception cleaning up: \" + StringUtils.stringifyException(e));\n      }\n      // Report back any failures, for diagnostic purposes\n      if (taskid != null) {\n        if (!ShutdownHookManager.get().isShutdownInProgress()) {\n          umbilical.fatalError(taskid,\n              StringUtils.stringifyException(exception), false);\n        }\n      }\n    } catch (Throwable throwable) {\n      LOG.error(\"Error running child : \"\n    \t        + StringUtils.stringifyException(throwable));\n      if (taskid != null) {\n        if (!ShutdownHookManager.get().isShutdownInProgress()) {\n          Throwable tCause = throwable.getCause();\n          String cause =\n              tCause == null ? throwable.getMessage() : StringUtils\n                  .stringifyException(tCause);\n          umbilical.fatalError(taskid, cause, false);\n        }\n      }\n    } finally {\n      RPC.stopProxy(umbilical);\n      DefaultMetricsSystem.shutdown();\n      TaskLog.syncLogsShutdown(logSyncer);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.configureTask": "  private static void configureTask(JobConf job, Task task,\n      Credentials credentials, Token<JobTokenIdentifier> jt) throws IOException {\n    job.setCredentials(credentials);\n\n    ApplicationAttemptId appAttemptId = ContainerId.fromString(\n        System.getenv(Environment.CONTAINER_ID.name()))\n        .getApplicationAttemptId();\n    LOG.debug(\"APPLICATION_ATTEMPT_ID: \" + appAttemptId);\n    // Set it in conf, so as to be able to be used the the OutputCommitter.\n    job.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID,\n        appAttemptId.getAttemptId());\n\n    // set tcp nodelay\n    job.setBoolean(\"ipc.client.tcpnodelay\", true);\n    job.setClass(MRConfig.TASK_LOCAL_OUTPUT_CLASS,\n        YarnOutputFiles.class, MapOutputFile.class);\n    // set the jobToken and shuffle secrets into task\n    task.setJobTokenSecret(\n        JobTokenSecretManager.createSecretKey(jt.getPassword()));\n    byte[] shuffleSecret = TokenCache.getShuffleSecretKey(credentials);\n    if (shuffleSecret == null) {\n      LOG.warn(\"Shuffle secret missing from task credentials.\"\n          + \" Using job token secret as shuffle secret.\");\n      shuffleSecret = jt.getPassword();\n    }\n    task.setShuffleSecret(\n        JobTokenSecretManager.createSecretKey(shuffleSecret));\n\n    // setup the child's MRConfig.LOCAL_DIR.\n    configureLocalDirs(task, job);\n\n    // setup the child's attempt directories\n    // Do the task-type specific localization\n    task.localizeConfiguration(job);\n\n    // Set up the DistributedCache related configs\n    MRApps.setupDistributedCacheLocal(job);\n\n    // Overwrite the localized task jobconf which is linked to in the current\n    // work-dir.\n    Path localTaskFile = new Path(MRJobConfig.JOB_CONF_FILE);\n    writeLocalJobFile(localTaskFile, job);\n    task.setJobFile(localTaskFile.toString());\n    task.setConf(job);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.exists": "  public boolean exists(Path f) throws IOException {\n    try {\n      return getFileStatus(f) != null;\n    } catch (FileNotFoundException e) {\n      return false;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getFileStatus": "  public abstract FileStatus getFileStatus(Path f) throws IOException;\n\n  /**\n   * Checks if the user can access a path.  The mode specifies which access\n   * checks to perform.  If the requested permissions are granted, then the\n   * method returns normally.  If access is denied, then the method throws an\n   * {@link AccessControlException}.",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.pipes.application.getDownlink": "  DownwardProtocol<K1, V1> getDownlink() {\n    return downlink;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.pipes.Submitter.getIsJavaRecordWriter": "  public static boolean getIsJavaRecordWriter(JobConf conf) {\n    return conf.getBoolean(Submitter.IS_JAVA_RW, false);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.SkipBadRecords.getReducerMaxSkipGroups": "  public static long getReducerMaxSkipGroups(Configuration conf) {\n    return conf.getLong(REDUCER_MAX_SKIP_GROUPS, 0);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.SkipBadRecords.getAutoIncrReducerProcCount": "  public static boolean getAutoIncrReducerProcCount(Configuration conf) {\n    return conf.getBoolean(AUTO_INCR_REDUCE_PROC_COUNT, true);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Reducer.reduce": "  void reduce(K2 key, Iterator<V2> values,\n              OutputCollector<K3, V3> output, Reporter reporter)\n    throws IOException;\n\n}",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ShuffleConsumerPlugin.init": "  public void init(Context<K, V> context);\n\n  public RawKeyValueIterator run() throws IOException, InterruptedException;\n\n  public void close();\n\n  @InterfaceAudience.LimitedPrivate(\"mapreduce\")\n  @InterfaceStability.Unstable\n  public static class Context<K,V> {\n    private final org.apache.hadoop.mapreduce.TaskAttemptID reduceId;\n    private final JobConf jobConf;\n    private final FileSystem localFS;\n    private final TaskUmbilicalProtocol umbilical;\n    private final LocalDirAllocator localDirAllocator;\n    private final Reporter reporter;\n    private final CompressionCodec codec;\n    private final Class<? extends Reducer> combinerClass;\n    private final CombineOutputCollector<K, V> combineCollector;\n    private final Counters.Counter spilledRecordsCounter;\n    private final Counters.Counter reduceCombineInputCounter;\n    private final Counters.Counter shuffledMapsCounter;\n    private final Counters.Counter reduceShuffleBytes;\n    private final Counters.Counter failedShuffleCounter;\n    private final Counters.Counter mergedMapOutputsCounter;\n    private final TaskStatus status;\n    private final Progress copyPhase;\n    private final Progress mergePhase;\n    private final Task reduceTask;\n    private final MapOutputFile mapOutputFile;\n    private final Map<TaskAttemptID, MapOutputFile> localMapFiles;\n\n    public Context(org.apache.hadoop.mapreduce.TaskAttemptID reduceId,\n                   JobConf jobConf, FileSystem localFS,\n                   TaskUmbilicalProtocol umbilical,\n                   LocalDirAllocator localDirAllocator,\n                   Reporter reporter, CompressionCodec codec,\n                   Class<? extends Reducer> combinerClass,\n                   CombineOutputCollector<K,V> combineCollector,\n                   Counters.Counter spilledRecordsCounter,\n                   Counters.Counter reduceCombineInputCounter,\n                   Counters.Counter shuffledMapsCounter,\n                   Counters.Counter reduceShuffleBytes,\n                   Counters.Counter failedShuffleCounter,\n                   Counters.Counter mergedMapOutputsCounter,\n                   TaskStatus status, Progress copyPhase, Progress mergePhase,\n                   Task reduceTask, MapOutputFile mapOutputFile,\n                   Map<TaskAttemptID, MapOutputFile> localMapFiles) {\n      this.reduceId = reduceId;\n      this.jobConf = jobConf;\n      this.localFS = localFS;\n      this. umbilical = umbilical;\n      this.localDirAllocator = localDirAllocator;\n      this.reporter = reporter;\n      this.codec = codec;\n      this.combinerClass = combinerClass;\n      this.combineCollector = combineCollector;\n      this.spilledRecordsCounter = spilledRecordsCounter;\n      this.reduceCombineInputCounter = reduceCombineInputCounter;\n      this.shuffledMapsCounter = shuffledMapsCounter;\n      this.reduceShuffleBytes = reduceShuffleBytes;\n      this.failedShuffleCounter = failedShuffleCounter;\n      this.mergedMapOutputsCounter = mergedMapOutputsCounter;\n      this.status = status;\n      this.copyPhase = copyPhase;\n      this.mergePhase = mergePhase;\n      this.reduceTask = reduceTask;\n      this.mapOutputFile = mapOutputFile;\n      this.localMapFiles = localMapFiles;\n    }\n\n    public org.apache.hadoop.mapreduce.TaskAttemptID getReduceId() {\n      return reduceId;\n    }\n    public JobConf getJobConf() {\n      return jobConf;\n    }\n    public FileSystem getLocalFS() {\n      return localFS;\n    }\n    public TaskUmbilicalProtocol getUmbilical() {\n      return umbilical;\n    }\n    public LocalDirAllocator getLocalDirAllocator() {\n      return localDirAllocator;\n    }\n    public Reporter getReporter() {\n      return reporter;\n    }\n    public CompressionCodec getCodec() {\n      return codec;\n    }\n    public Class<? extends Reducer> getCombinerClass() {\n      return combinerClass;\n    }\n    public CombineOutputCollector<K, V> getCombineCollector() {\n      return combineCollector;\n    }\n    public Counters.Counter getSpilledRecordsCounter() {\n      return spilledRecordsCounter;\n    }\n    public Counters.Counter getReduceCombineInputCounter() {\n      return reduceCombineInputCounter;\n    }\n    public Counters.Counter getShuffledMapsCounter() {\n      return shuffledMapsCounter;\n    }\n    public Counters.Counter getReduceShuffleBytes() {\n      return reduceShuffleBytes;\n    }\n    public Counters.Counter getFailedShuffleCounter() {\n      return failedShuffleCounter;\n    }\n    public Counters.Counter getMergedMapOutputsCounter() {\n      return mergedMapOutputsCounter;\n    }\n    public TaskStatus getStatus() {\n      return status;\n    }\n    public Progress getCopyPhase() {\n      return copyPhase;\n    }\n    public Progress getMergePhase() {\n      return mergePhase;\n    }\n    public Task getReduceTask() {\n      return reduceTask;\n    }\n    public MapOutputFile getMapOutputFile() {\n      return mapOutputFile;\n    }\n    public Map<TaskAttemptID, MapOutputFile> getLocalMapFiles() {\n      return localMapFiles;\n    }\n  } // end of public static class Context<K,V>",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.taskCleanup": "  void taskCleanup(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    // set phase for this task\n    setPhase(TaskStatus.Phase.CLEANUP);\n    getProgress().setStatus(\"cleanup\");\n    statusUpdate(umbilical);\n    LOG.info(\"Running cleanup for the task\");\n    // do the cleanup\n    committer.abortTask(taskContext);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.statusUpdate": "  public void statusUpdate(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    int retries = MAX_RETRIES;\n\n    while (true) {\n      try {\n        if (!umbilical.statusUpdate(getTaskID(), taskStatus).getTaskFound()) {\n          if (uberized) {\n            LOG.warn(\"Task no longer available: \" + taskId);\n            break;\n          } else {\n            LOG.warn(\"Parent died.  Exiting \" + taskId);\n            ExitUtil.terminate(66);\n          }\n        }\n        taskStatus.clearStatus();\n        return;\n      } catch (InterruptedException ie) {\n        Thread.currentThread().interrupt(); // interrupt ourself\n      } catch (IOException ie) {\n        LOG.warn(\"Failure sending status update: \" + \n                  StringUtils.stringifyException(ie));\n        if (--retries == 0) {\n          throw ie;\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setPhase": "  protected synchronized void setPhase(TaskStatus.Phase phase){\n    this.taskStatus.setPhase(phase); \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setStatus": "    public void setStatus(String status) {\n      taskProgress.setStatus(normalizeStatus(status, conf));\n      // indicate that progress update needs to be sent\n      setProgressFlag();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getProgress": "    public float getProgress() {\n      return taskProgress.getProgress();\n    };",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getWorkingDirectory": "  public Path getWorkingDirectory() {\n    String name = get(JobContext.WORKING_DIR);\n    if (name != null) {\n      return new Path(name);\n    } else {\n      try {\n        Path dir = FileSystem.get(this).getWorkingDirectory();\n        set(JobContext.WORKING_DIR, dir.toString());\n        return dir;\n      } catch (IOException e) {\n        throw new RuntimeException(e);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.toString": "  public String toString() { \n    return appendTo(new StringBuilder(JVM)).toString();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.appendTo": "  protected StringBuilder appendTo(StringBuilder builder) {\n    return jobId.appendTo(builder).\n                 append(SEPARATOR).\n                 append(isMap ? 'm' : 'r').\n                 append(SEPARATOR).\n                 append(idFormat.format(jvmId));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getSessionId": "  public String getSessionId() {\n      return get(\"session.id\", \"\");\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.createLogSyncer": "  public static ScheduledExecutorService createLogSyncer() {\n    final ScheduledExecutorService scheduler =\n        HadoopExecutors.newSingleThreadScheduledExecutor(\n            new ThreadFactory() {\n              @Override\n              public Thread newThread(Runnable r) {\n                final Thread t = Executors.defaultThreadFactory().newThread(r);\n                t.setDaemon(true);\n                t.setName(\"Thread for syncLogs\");\n                return t;\n              }\n            });\n    ShutdownHookManager.get().addShutdownHook(new Runnable() {\n      @Override\n      public void run() {\n        TaskLog.syncLogsShutdown(scheduler);\n      }\n    }, 50);\n    scheduler.scheduleWithFixedDelay(\n        new Runnable() {\n          @Override\n          public void run() {\n            TaskLog.syncLogs();\n          }\n        }, 0L, 5L, TimeUnit.SECONDS);\n    return scheduler;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.newThread": "              public Thread newThread(Runnable r) {\n                final Thread t = Executors.defaultThreadFactory().newThread(r);\n                t.setDaemon(true);\n                t.setName(\"Thread for syncLogs\");\n                return t;\n              }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.syncLogsShutdown": "  public static synchronized void syncLogsShutdown(\n    ScheduledExecutorService scheduler) \n  {\n    // flush standard streams\n    //\n    System.out.flush();\n    System.err.flush();\n\n    if (scheduler != null) {\n      scheduler.shutdownNow();\n    }\n\n    // flush & close all appenders\n    LogManager.shutdown(); \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.syncLogs": "  public static synchronized void syncLogs() {\n    // flush standard streams\n    //\n    System.out.flush();\n    System.err.flush();\n\n    // flush flushable appenders\n    //\n    final Logger rootLogger = Logger.getRootLogger();\n    flushAppenders(rootLogger);\n    final Enumeration<Logger> allLoggers = rootLogger.getLoggerRepository().\n      getCurrentLoggers();\n    while (allLoggers.hasMoreElements()) {\n      final Logger l = allLoggers.nextElement();\n      flushAppenders(l);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fsError": "  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /**\n   * Report that the task encounted a fatal error.\n   * @param taskId task's id\n   * @param message fail message\n   * @param fastFail flag to enable fast fail for task\n   */\n  void fatalError(TaskAttemptID taskId, String message, boolean fastFail) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.shouldDie": "  public boolean shouldDie() {\n    return shouldDie;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fatalError": "  void fatalError(TaskAttemptID taskId, String message, boolean fastFail) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.getTask": "  public Task getTask() {\n    return t;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.getTask": "  JvmTask getTask(JvmContext context) throws IOException;\n  \n  /**\n   * Report child's progress to parent. Also invoked to report still alive (used\n   * to be in ping). It reports an AMFeedback used to propagate preemption requests.\n   * \n   * @param taskId task-id of the child\n   * @param taskStatus status of the child\n   * @throws IOException\n   * @throws InterruptedException\n   * @return True if the task is known\n   */\n  AMFeedback statusUpdate(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;\n  \n  /** Report error messages back to parent.  Calls should be sparing, since all\n   *  such messages are held in the job tracker.\n   *  @param taskid the id of the task involved\n   *  @param trace the text to report\n   */\n  void reportDiagnosticInfo(TaskAttemptID taskid, String trace) throws IOException;\n  \n  /**\n   * Report the record range which is going to process next by the Task.\n   * @param taskid the id of the task involved\n   * @param range the range of record sequence nos\n   * @throws IOException\n   */\n  void reportNextRecordRange(TaskAttemptID taskid, SortedRanges.Range range) \n    throws IOException;\n\n  /** Report that the task is successfully completed.  Failure is assumed if\n   * the task process exits without calling this.\n   * @param taskid task's id\n   */\n  void done(TaskAttemptID taskid) throws IOException;\n  \n  /** \n   * Report that the task is complete, but its commit is pending.\n   * \n   * @param taskId task's id\n   * @param taskStatus status of the child\n   * @throws IOException\n   */\n  void commitPending(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;  \n\n  /**\n   * Polling to know whether the task can go-ahead with commit \n   * @param taskid\n   * @return true/false \n   * @throws IOException\n   */\n  boolean canCommit(TaskAttemptID taskid) throws IOException;\n\n  /** Report that a reduce-task couldn't shuffle map-outputs.*/\n  void shuffleError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Report that the task encounted a local filesystem error.*/\n  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /**\n   * Report that the task encounted a fatal error.\n   * @param taskId task's id\n   * @param message fail message\n   * @param fastFail flag to enable fast fail for task\n   */\n  void fatalError(TaskAttemptID taskId, String message, boolean fastFail) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.getJobID": "  public JobID getJobID() {\n    return (JobID) super.getJobID();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.forName": "  public static TaskAttemptID forName(String str\n                                      ) throws IllegalArgumentException {\n    return (TaskAttemptID) \n             org.apache.hadoop.mapreduce.TaskAttemptID.forName(str);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getTaskID": "  public TaskAttemptID getTaskID() { return taskId; }"
        },
        "bug_report": {
            "Title": "Pipe mapreduce job fails with Permission denied for jobTokenPassword",
            "Description": "Steps:\r\n\r\nLaunch wordcount example with pipe\r\n{code}\r\n/usr/hdp/current/hadoop-client/bin/hadoop pipes \"-Dhadoop.pipes.java.recordreader=true\" \"-Dhadoop.pipes.java.recordwriter=true\" -input pipeInput -output pipeOutput -program bin/wordcount{code}\r\n\r\nThe application fails with below stacktrace\r\n{code:title=AM}\r\nattempt_1517534613368_0041_r_000000_2 is : 0.0\r\n\r\n2018-02-02 02:40:51,071 ERROR [IPC Server handler 16 on 43391] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1517534613368_0041_r_000000_2 - exited : java.io.FileNotFoundException: /grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/jobTokenPassword (Permission denied)\r\n\r\n at java.io.FileOutputStream.open0(Native Method)\r\n\r\n at java.io.FileOutputStream.open(FileOutputStream.java:270)\r\n\r\n at java.io.FileOutputStream.<init>(FileOutputStream.java:213)\r\n\r\n at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:236)\r\n\r\n at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:219)\r\n\r\n at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:318)\r\n\r\n at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:307)\r\n\r\n at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:338)\r\n\r\n at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:401)\r\n\r\n at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:464)\r\n\r\n at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\r\n\r\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1169)\r\n\r\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1149)\r\n\r\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1038)\r\n\r\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1026)\r\n\r\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:703)\r\n\r\n at org.apache.hadoop.mapred.pipes.Application.writePasswordToLocalFile(Application.java:173)\r\n\r\n at org.apache.hadoop.mapred.pipes.Application.<init>(Application.java:109)\r\n\r\n at org.apache.hadoop.mapred.pipes.PipesReducer.startApplication(PipesReducer.java:87)\r\n\r\n at org.apache.hadoop.mapred.pipes.PipesReducer.reduce(PipesReducer.java:65)\r\n\r\n at org.apache.hadoop.mapred.pipes.PipesReducer.reduce(PipesReducer.java:38)\r\n\r\n at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:445)\r\n\r\n at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:393)\r\n\r\n at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\r\n\r\n at java.security.AccessController.doPrivileged(Native Method)\r\n\r\n at javax.security.auth.Subject.doAs(Subject.java:422)\r\n\r\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)\r\n\r\n at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\r\n{code}\r\n"
        }
    },
    {
        "filename": "MAPREDUCE-4102.json",
        "creation_time": "2012-04-03T19:51:52.000+0000",
        "stack_trace": "```\nCaused by: com.google.inject.ProvisionException: Guice provision errors:\n\njava.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n...\n..\n...\n\n1) Error injecting constructor, java.lang.NullPointerException\n  at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.<init>(CountersBlock.java:56)\n  while locating org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock\n...\n..\n...\nCaused by: java.lang.NullPointerException    \n    at org.apache.hadoop.mapreduce.counters.AbstractCounters.incrAllCounters(AbstractCounters.java:328)\n    at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.getCounters(CountersBlock.java:188)\n    at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.<init>(CountersBlock.java:57)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.counters.AbstractCounters.incrAllCounters": "  public synchronized void incrAllCounters(AbstractCounters<C, G> other) {\n    for(G right : other) {\n      String groupName = right.getName();\n      G left = (isFrameworkGroup(groupName) ? fgroups : groups).get(groupName);\n      if (left == null) {\n        left = addGroup(groupName, right.getDisplayName());\n      }\n      left.incrAllCounters(right);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.counters.AbstractCounters.addGroup": "  public G addGroup(String name, String displayName) {\n    return addGroup(groupFactory.newGroup(name, displayName, limits));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.getCounters": "  private void getCounters(AppContext ctx) {\n    JobId jobID = null;\n    TaskId taskID = null;\n    String tid = $(TASK_ID);\n    if (!tid.isEmpty()) {\n      taskID = MRApps.toTaskID(tid);\n      jobID = taskID.getJobId();\n    } else {\n      String jid = $(JOB_ID);\n      if (jid != null && !jid.isEmpty()) {\n        jobID = MRApps.toJobID(jid);\n      }\n    }\n    if (jobID == null) {\n      return;\n    }\n    job = ctx.getJob(jobID);\n    if (job == null) {\n      return;\n    }\n    if (taskID != null) {\n      task = job.getTask(taskID);\n      if (task == null) {\n        return;\n      }\n      total = task.getCounters();\n      return;\n    }\n    // Get all types of counters\n    Map<TaskId, Task> tasks = job.getTasks();\n    total = job.getAllCounters();\n    map = new Counters();\n    reduce = new Counters();\n    for (Task t : tasks.values()) {\n      Counters counters = t.getCounters();\n      switch (t.getType()) {\n        case MAP:     map.incrAllCounters(counters);     break;\n        case REDUCE:  reduce.incrAllCounters(counters);  break;\n      }\n    }\n  }"
        },
        "bug_report": {
            "Title": "job counters not available in Jobhistory webui for killed jobs",
            "Description": "Run a simple wordcount or sleep, and kill the job before it finishes.  Go to the job history web ui and click the \"Counters\" link for that job. It displays \"500 error\".\n\nThe job history log has:\n\nCaused by: com.google.inject.ProvisionException: Guice provision errors:\n\n2012-04-03 19:42:53,148 ERROR org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /jobhistory/jobcounters/job_1333482028750_0001\njava.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n...\n..\n...\n\n1) Error injecting constructor, java.lang.NullPointerException\n  at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.<init>(CountersBlock.java:56)\n  while locating org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock\n...\n..\n...\nCaused by: java.lang.NullPointerException    at org.apache.hadoop.mapreduce.counters.AbstractCounters.incrAllCounters(AbstractCounters.java:328)\n    at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.getCounters(CountersBlock.java:188)\n    at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.<init>(CountersBlock.java:57)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\n\nThere are task counters available if you drill down into successful tasks though."
        }
    },
    {
        "filename": "MAPREDUCE-6353.json",
        "creation_time": "2015-05-03T14:10:44.000+0000",
        "stack_trace": "```\njava.lang.ArithmeticException: / by zero\nat org.apache.hadoop.mapreduce.v2.app.rm.ResourceCalculatorUtils.computeAvailableContainers(ResourceCalculatorUtils.java:38)\nat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:947)\nat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.access$200(RMContainerAllocator.java:840)\nat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:247)\nat org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:282)\nat java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.ResourceCalculatorUtils.computeAvailableContainers": "  public static int computeAvailableContainers(Resource available,\n      Resource required, EnumSet<SchedulerResourceTypes> resourceTypes) {\n    if (resourceTypes.contains(SchedulerResourceTypes.CPU)) {\n      return Math.min(available.getMemory() / required.getMemory(),\n        available.getVirtualCores() / required.getVirtualCores());\n    }\n    return available.getMemory() / required.getMemory();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.assign": "    private void assign(List<Container> allocatedContainers) {\n      Iterator<Container> it = allocatedContainers.iterator();\n      LOG.info(\"Got allocated containers \" + allocatedContainers.size());\n      containersAllocated += allocatedContainers.size();\n      while (it.hasNext()) {\n        Container allocated = it.next();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Assigning container \" + allocated.getId()\n              + \" with priority \" + allocated.getPriority() + \" to NM \"\n              + allocated.getNodeId());\n        }\n        \n        // check if allocated container meets memory requirements \n        // and whether we have any scheduled tasks that need \n        // a container to be assigned\n        boolean isAssignable = true;\n        Priority priority = allocated.getPriority();\n        Resource allocatedResource = allocated.getResource();\n        if (PRIORITY_FAST_FAIL_MAP.equals(priority) \n            || PRIORITY_MAP.equals(priority)) {\n          if (ResourceCalculatorUtils.computeAvailableContainers(allocatedResource,\n              mapResourceRequest, getSchedulerResourceTypes()) <= 0\n              || maps.isEmpty()) {\n            LOG.info(\"Cannot assign container \" + allocated \n                + \" for a map as either \"\n                + \" container memory less than required \" + mapResourceRequest\n                + \" or no pending map tasks - maps.isEmpty=\" \n                + maps.isEmpty()); \n            isAssignable = false; \n          }\n        } \n        else if (PRIORITY_REDUCE.equals(priority)) {\n          if (ResourceCalculatorUtils.computeAvailableContainers(allocatedResource,\n              reduceResourceRequest, getSchedulerResourceTypes()) <= 0\n              || reduces.isEmpty()) {\n            LOG.info(\"Cannot assign container \" + allocated \n                + \" for a reduce as either \"\n                + \" container memory less than required \" + reduceResourceRequest\n                + \" or no pending reduce tasks - reduces.isEmpty=\" \n                + reduces.isEmpty()); \n            isAssignable = false;\n          }\n        } else {\n          LOG.warn(\"Container allocated at unwanted priority: \" + priority + \n              \". Returning to RM...\");\n          isAssignable = false;\n        }\n        \n        if(!isAssignable) {\n          // release container if we could not assign it \n          containerNotAssigned(allocated);\n          it.remove();\n          continue;\n        }\n        \n        // do not assign if allocated container is on a  \n        // blacklisted host\n        String allocatedHost = allocated.getNodeId().getHost();\n        if (isNodeBlacklisted(allocatedHost)) {\n          // we need to request for a new container \n          // and release the current one\n          LOG.info(\"Got allocated container on a blacklisted \"\n              + \" host \"+allocatedHost\n              +\". Releasing container \" + allocated);\n\n          // find the request matching this allocated container \n          // and replace it with a new one \n          ContainerRequest toBeReplacedReq = \n              getContainerReqToReplace(allocated);\n          if (toBeReplacedReq != null) {\n            LOG.info(\"Placing a new container request for task attempt \" \n                + toBeReplacedReq.attemptID);\n            ContainerRequest newReq = \n                getFilteredContainerRequest(toBeReplacedReq);\n            decContainerReq(toBeReplacedReq);\n            if (toBeReplacedReq.attemptID.getTaskId().getTaskType() ==\n                TaskType.MAP) {\n              maps.put(newReq.attemptID, newReq);\n            }\n            else {\n              reduces.put(newReq.attemptID, newReq);\n            }\n            addContainerReq(newReq);\n          }\n          else {\n            LOG.info(\"Could not map allocated container to a valid request.\"\n                + \" Releasing allocated container \" + allocated);\n          }\n          \n          // release container if we could not assign it \n          containerNotAssigned(allocated);\n          it.remove();\n          continue;\n        }\n      }\n\n      assignContainers(allocatedContainers);\n       \n      // release container if we could not assign it \n      it = allocatedContainers.iterator();\n      while (it.hasNext()) {\n        Container allocated = it.next();\n        LOG.info(\"Releasing unassigned container \" + allocated);\n        containerNotAssigned(allocated);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getContainerReqToReplace": "    private ContainerRequest getContainerReqToReplace(Container allocated) {\n      LOG.info(\"Finding containerReq for allocated container: \" + allocated);\n      Priority priority = allocated.getPriority();\n      ContainerRequest toBeReplaced = null;\n      if (PRIORITY_FAST_FAIL_MAP.equals(priority)) {\n        LOG.info(\"Replacing FAST_FAIL_MAP container \" + allocated.getId());\n        Iterator<TaskAttemptId> iter = earlierFailedMaps.iterator();\n        while (toBeReplaced == null && iter.hasNext()) {\n          toBeReplaced = maps.get(iter.next());\n        }\n        LOG.info(\"Found replacement: \" + toBeReplaced);\n        return toBeReplaced;\n      }\n      else if (PRIORITY_MAP.equals(priority)) {\n        LOG.info(\"Replacing MAP container \" + allocated.getId());\n        // allocated container was for a map\n        String host = allocated.getNodeId().getHost();\n        LinkedList<TaskAttemptId> list = mapsHostMapping.get(host);\n        if (list != null && list.size() > 0) {\n          TaskAttemptId tId = list.removeLast();\n          if (maps.containsKey(tId)) {\n            toBeReplaced = maps.remove(tId);\n          }\n        }\n        else {\n          TaskAttemptId tId = maps.keySet().iterator().next();\n          toBeReplaced = maps.remove(tId);          \n        }        \n      }\n      else if (PRIORITY_REDUCE.equals(priority)) {\n        TaskAttemptId tId = reduces.keySet().iterator().next();\n        toBeReplaced = reduces.remove(tId);    \n      }\n      LOG.info(\"Found replacement: \" + toBeReplaced);\n      return toBeReplaced;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.assignContainers": "    private void assignContainers(List<Container> allocatedContainers) {\n      Iterator<Container> it = allocatedContainers.iterator();\n      while (it.hasNext()) {\n        Container allocated = it.next();\n        ContainerRequest assigned = assignWithoutLocality(allocated);\n        if (assigned != null) {\n          containerAssigned(allocated, assigned);\n          it.remove();\n        }\n      }\n\n      assignMapsWithLocality(allocatedContainers);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getHost": "  private static String getHost(String contMgrAddress) {\n    String host = contMgrAddress;\n    String[] hostport = host.split(\":\");\n    if (hostport.length == 2) {\n      host = hostport[0];\n    }\n    return host;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.containerNotAssigned": "    private void containerNotAssigned(Container allocated) {\n      containersReleased++;\n      pendingRelease.add(allocated.getId());\n      release(allocated.getId());      \n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.remove": "    boolean remove(TaskAttemptId tId) {\n      ContainerId containerId = null;\n      if (tId.getTaskId().getTaskType().equals(TaskType.MAP)) {\n        containerId = maps.remove(tId).getId();\n      } else {\n        containerId = reduces.remove(tId).getId();\n        if (containerId != null) {\n          boolean preempted = preemptionWaitingReduces.remove(tId);\n          if (preempted) {\n            LOG.info(\"Reduce preemption successful \" + tId);\n          }\n        }\n      }\n      \n      if (containerId != null) {\n        containerToAttemptMap.remove(containerId);\n        return true;\n      }\n      return false;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat": "  protected synchronized void heartbeat() throws Exception {\n    scheduleStats.updateAndLogIfChanged(\"Before Scheduling: \");\n    List<Container> allocatedContainers = getResources();\n    if (allocatedContainers != null && allocatedContainers.size() > 0) {\n      scheduledRequests.assign(allocatedContainers);\n    }\n\n    int completedMaps = getJob().getCompletedMaps();\n    int completedTasks = completedMaps + getJob().getCompletedReduces();\n    if ((lastCompletedTasks != completedTasks) ||\n          (scheduledRequests.maps.size() > 0)) {\n      lastCompletedTasks = completedTasks;\n      recalculateReduceSchedule = true;\n    }\n\n    if (recalculateReduceSchedule) {\n      preemptReducesIfNeeded();\n      scheduleReduces(\n          getJob().getTotalMaps(), completedMaps,\n          scheduledRequests.maps.size(), scheduledRequests.reduces.size(), \n          assignedRequests.maps.size(), assignedRequests.reduces.size(),\n          mapResourceRequest, reduceResourceRequest,\n          pendingReduces.size(), \n          maxReduceRampupLimit, reduceSlowStart);\n      recalculateReduceSchedule = false;\n    }\n\n    scheduleStats.updateAndLogIfChanged(\"After Scheduling: \");\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.preemptReducesIfNeeded": "  void preemptReducesIfNeeded() {\n    if (reduceResourceRequest.equals(Resources.none())) {\n      return; // no reduces\n    }\n    //check if reduces have taken over the whole cluster and there are \n    //unassigned maps\n    if (scheduledRequests.maps.size() > 0) {\n      Resource resourceLimit = getResourceLimit();\n      Resource availableResourceForMap =\n          Resources.subtract(\n            resourceLimit,\n            Resources.multiply(reduceResourceRequest,\n              assignedRequests.reduces.size()\n                  - assignedRequests.preemptionWaitingReduces.size()));\n      // availableMemForMap must be sufficient to run at least 1 map\n      if (ResourceCalculatorUtils.computeAvailableContainers(availableResourceForMap,\n        mapResourceRequest, getSchedulerResourceTypes()) <= 0) {\n        // to make sure new containers are given to maps and not reduces\n        // ramp down all scheduled reduces if any\n        // (since reduces are scheduled at higher priority than maps)\n        LOG.info(\"Ramping down all scheduled reduces:\"\n            + scheduledRequests.reduces.size());\n        for (ContainerRequest req : scheduledRequests.reduces.values()) {\n          pendingReduces.add(req);\n        }\n        scheduledRequests.reduces.clear();\n\n        //do further checking to find the number of map requests that were\n        //hanging around for a while\n        int hangingMapRequests = getNumOfHangingRequests(scheduledRequests.maps);\n        if (hangingMapRequests > 0) {\n          // preempt for making space for at least one map\n          int preemptionReduceNumForOneMap =\n              ResourceCalculatorUtils.divideAndCeilContainers(mapResourceRequest,\n                reduceResourceRequest, getSchedulerResourceTypes());\n          int preemptionReduceNumForPreemptionLimit =\n              ResourceCalculatorUtils.divideAndCeilContainers(\n                Resources.multiply(resourceLimit, maxReducePreemptionLimit),\n                reduceResourceRequest, getSchedulerResourceTypes());\n          int preemptionReduceNumForAllMaps =\n              ResourceCalculatorUtils.divideAndCeilContainers(\n                Resources.multiply(mapResourceRequest, hangingMapRequests),\n                reduceResourceRequest, getSchedulerResourceTypes());\n          int toPreempt =\n              Math.min(Math.max(preemptionReduceNumForOneMap,\n                preemptionReduceNumForPreemptionLimit),\n                preemptionReduceNumForAllMaps);\n\n          LOG.info(\"Going to preempt \" + toPreempt\n              + \" due to lack of space for maps\");\n          assignedRequests.preemptReduce(toPreempt);\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getResources": "  private List<Container> getResources() throws Exception {\n    applyConcurrentTaskLimits();\n\n    // will be null the first time\n    Resource headRoom =\n        getAvailableResources() == null ? Resources.none() :\n            Resources.clone(getAvailableResources());\n    AllocateResponse response;\n    /*\n     * If contact with RM is lost, the AM will wait MR_AM_TO_RM_WAIT_INTERVAL_MS\n     * milliseconds before aborting. During this interval, AM will still try\n     * to contact the RM.\n     */\n    try {\n      response = makeRemoteRequest();\n      // Reset retry count if no exception occurred.\n      retrystartTime = System.currentTimeMillis();\n    } catch (ApplicationAttemptNotFoundException e ) {\n      // This can happen if the RM has been restarted. If it is in that state,\n      // this application must clean itself up.\n      eventHandler.handle(new JobEvent(this.getJob().getID(),\n        JobEventType.JOB_AM_REBOOT));\n      throw new YarnRuntimeException(\n        \"Resource Manager doesn't recognize AttemptId: \"\n            + this.getContext().getApplicationAttemptId(), e);\n    } catch (ApplicationMasterNotRegisteredException e) {\n      LOG.info(\"ApplicationMaster is out of sync with ResourceManager,\"\n          + \" hence resync and send outstanding requests.\");\n      // RM may have restarted, re-register with RM.\n      lastResponseID = 0;\n      register();\n      addOutstandingRequestOnResync();\n      return null;\n    } catch (Exception e) {\n      // This can happen when the connection to the RM has gone down. Keep\n      // re-trying until the retryInterval has expired.\n      if (System.currentTimeMillis() - retrystartTime >= retryInterval) {\n        LOG.error(\"Could not contact RM after \" + retryInterval + \" milliseconds.\");\n        eventHandler.handle(new JobEvent(this.getJob().getID(),\n                                         JobEventType.JOB_AM_REBOOT));\n        throw new YarnRuntimeException(\"Could not contact RM after \" +\n                                retryInterval + \" milliseconds.\");\n      }\n      // Throw this up to the caller, which may decide to ignore it and\n      // continue to attempt to contact the RM.\n      throw e;\n    }\n    Resource newHeadRoom =\n        getAvailableResources() == null ? Resources.none()\n            : getAvailableResources();\n    List<Container> newContainers = response.getAllocatedContainers();\n    // Setting NMTokens\n    if (response.getNMTokens() != null) {\n      for (NMToken nmToken : response.getNMTokens()) {\n        NMTokenCache.setNMToken(nmToken.getNodeId().toString(),\n            nmToken.getToken());\n      }\n    }\n\n    // Setting AMRMToken\n    if (response.getAMRMToken() != null) {\n      updateAMRMToken(response.getAMRMToken());\n    }\n\n    List<ContainerStatus> finishedContainers = response.getCompletedContainersStatuses();\n\n    // propagate preemption requests\n    final PreemptionMessage preemptReq = response.getPreemptionMessage();\n    if (preemptReq != null) {\n      preemptionPolicy.preempt(\n          new PreemptionContext(assignedRequests), preemptReq);\n    }\n\n    if (newContainers.size() + finishedContainers.size() > 0\n        || !headRoom.equals(newHeadRoom)) {\n      //something changed\n      recalculateReduceSchedule = true;\n      if (LOG.isDebugEnabled() && !headRoom.equals(newHeadRoom)) {\n        LOG.debug(\"headroom=\" + newHeadRoom);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      for (Container cont : newContainers) {\n        LOG.debug(\"Received new Container :\" + cont);\n      }\n    }\n\n    //Called on each allocation. Will know about newly blacklisted/added hosts.\n    computeIgnoreBlacklisting();\n\n    handleUpdatedNodes(response);\n\n    for (ContainerStatus cont : finishedContainers) {\n      LOG.info(\"Received completed container \" + cont.getContainerId());\n      TaskAttemptId attemptID = assignedRequests.get(cont.getContainerId());\n      if (attemptID == null) {\n        LOG.error(\"Container complete event for unknown container id \"\n            + cont.getContainerId());\n      } else {\n        pendingRelease.remove(cont.getContainerId());\n        assignedRequests.remove(attemptID);\n        \n        // send the container completed event to Task attempt\n        eventHandler.handle(createContainerFinishedEvent(cont, attemptID));\n        \n        // Send the diagnostics\n        String diagnostics = StringInterner.weakIntern(cont.getDiagnostics());\n        eventHandler.handle(new TaskAttemptDiagnosticsUpdateEvent(attemptID,\n            diagnostics));\n\n        preemptionPolicy.handleCompletedContainer(attemptID);\n      }\n    }\n    return newContainers;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.updateAndLogIfChanged": "    public void updateAndLogIfChanged(String msgPrefix) {\n      boolean changed = false;\n\n      // synchronized to fix findbug warnings\n      synchronized (RMContainerAllocator.this) {\n        changed |= (numPendingReduces != pendingReduces.size());\n        numPendingReduces = pendingReduces.size();\n        changed |= (numScheduledMaps != scheduledRequests.maps.size());\n        numScheduledMaps = scheduledRequests.maps.size();\n        changed |= (numScheduledReduces != scheduledRequests.reduces.size());\n        numScheduledReduces = scheduledRequests.reduces.size();\n        changed |= (numAssignedMaps != assignedRequests.maps.size());\n        numAssignedMaps = assignedRequests.maps.size();\n        changed |= (numAssignedReduces != assignedRequests.reduces.size());\n        numAssignedReduces = assignedRequests.reduces.size();\n        changed |= (numCompletedMaps != getJob().getCompletedMaps());\n        numCompletedMaps = getJob().getCompletedMaps();\n        changed |= (numCompletedReduces != getJob().getCompletedReduces());\n        numCompletedReduces = getJob().getCompletedReduces();\n        changed |= (numContainersAllocated != containersAllocated);\n        numContainersAllocated = containersAllocated;\n        changed |= (numContainersReleased != containersReleased);\n        numContainersReleased = containersReleased;\n      }\n\n      if (changed) {\n        log(msgPrefix);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.scheduleReduces": "  public void scheduleReduces(\n      int totalMaps, int completedMaps,\n      int scheduledMaps, int scheduledReduces,\n      int assignedMaps, int assignedReduces,\n      Resource mapResourceReqt, Resource reduceResourceReqt,\n      int numPendingReduces,\n      float maxReduceRampupLimit, float reduceSlowStart) {\n    \n    if (numPendingReduces == 0) {\n      return;\n    }\n    \n    // get available resources for this job\n    Resource headRoom = getAvailableResources();\n    if (headRoom == null) {\n      headRoom = Resources.none();\n    }\n\n    LOG.info(\"Recalculating schedule, headroom=\" + headRoom);\n    \n    //check for slow start\n    if (!getIsReduceStarted()) {//not set yet\n      int completedMapsForReduceSlowstart = (int)Math.ceil(reduceSlowStart * \n                      totalMaps);\n      if(completedMaps < completedMapsForReduceSlowstart) {\n        LOG.info(\"Reduce slow start threshold not met. \" +\n              \"completedMapsForReduceSlowstart \" + \n            completedMapsForReduceSlowstart);\n        return;\n      } else {\n        LOG.info(\"Reduce slow start threshold reached. Scheduling reduces.\");\n        setIsReduceStarted(true);\n      }\n    }\n    \n    //if all maps are assigned, then ramp up all reduces irrespective of the\n    //headroom\n    if (scheduledMaps == 0 && numPendingReduces > 0) {\n      LOG.info(\"All maps assigned. \" +\n          \"Ramping up all remaining reduces:\" + numPendingReduces);\n      scheduleAllReduces();\n      return;\n    }\n\n    float completedMapPercent = 0f;\n    if (totalMaps != 0) {//support for 0 maps\n      completedMapPercent = (float)completedMaps/totalMaps;\n    } else {\n      completedMapPercent = 1;\n    }\n    \n    Resource netScheduledMapResource =\n        Resources.multiply(mapResourceReqt, (scheduledMaps + assignedMaps));\n\n    Resource netScheduledReduceResource =\n        Resources.multiply(reduceResourceReqt,\n          (scheduledReduces + assignedReduces));\n\n    Resource finalMapResourceLimit;\n    Resource finalReduceResourceLimit;\n\n    // ramp up the reduces based on completed map percentage\n    Resource totalResourceLimit = getResourceLimit();\n\n    Resource idealReduceResourceLimit =\n        Resources.multiply(totalResourceLimit,\n          Math.min(completedMapPercent, maxReduceRampupLimit));\n    Resource ideaMapResourceLimit =\n        Resources.subtract(totalResourceLimit, idealReduceResourceLimit);\n\n    // check if there aren't enough maps scheduled, give the free map capacity\n    // to reduce.\n    // Even when container number equals, there may be unused resources in one\n    // dimension\n    if (ResourceCalculatorUtils.computeAvailableContainers(ideaMapResourceLimit,\n      mapResourceReqt, getSchedulerResourceTypes()) >= (scheduledMaps + assignedMaps)) {\n      // enough resource given to maps, given the remaining to reduces\n      Resource unusedMapResourceLimit =\n          Resources.subtract(ideaMapResourceLimit, netScheduledMapResource);\n      finalReduceResourceLimit =\n          Resources.add(idealReduceResourceLimit, unusedMapResourceLimit);\n      finalMapResourceLimit =\n          Resources.subtract(totalResourceLimit, finalReduceResourceLimit);\n    } else {\n      finalMapResourceLimit = ideaMapResourceLimit;\n      finalReduceResourceLimit = idealReduceResourceLimit;\n    }\n\n    LOG.info(\"completedMapPercent \" + completedMapPercent\n        + \" totalResourceLimit:\" + totalResourceLimit\n        + \" finalMapResourceLimit:\" + finalMapResourceLimit\n        + \" finalReduceResourceLimit:\" + finalReduceResourceLimit\n        + \" netScheduledMapResource:\" + netScheduledMapResource\n        + \" netScheduledReduceResource:\" + netScheduledReduceResource);\n\n    int rampUp =\n        ResourceCalculatorUtils.computeAvailableContainers(Resources.subtract(\n                finalReduceResourceLimit, netScheduledReduceResource),\n            reduceResourceReqt, getSchedulerResourceTypes());\n\n    if (rampUp > 0) {\n      rampUp = Math.min(rampUp, numPendingReduces);\n      LOG.info(\"Ramping up \" + rampUp);\n      rampUpReduces(rampUp);\n    } else if (rampUp < 0) {\n      int rampDown = -1 * rampUp;\n      rampDown = Math.min(rampDown, scheduledReduces);\n      LOG.info(\"Ramping down \" + rampDown);\n      rampDownReduces(rampDown);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator.run": "      public void run() {\n        while (!stopped.get() && !Thread.currentThread().isInterrupted()) {\n          try {\n            Thread.sleep(rmPollInterval);\n            try {\n              heartbeat();\n            } catch (YarnRuntimeException e) {\n              LOG.error(\"Error communicating with RM: \" + e.getMessage() , e);\n              return;\n            } catch (Exception e) {\n              LOG.error(\"ERROR IN CONTACTING RM. \", e);\n              continue;\n              // TODO: for other exceptions\n            }\n\n            lastHeartbeatTime = context.getClock().getTime();\n            executeHeartbeatCallbacks();\n          } catch (InterruptedException e) {\n            if (!stopped.get()) {\n              LOG.warn(\"Allocated thread interrupted. Returning.\");\n            }\n            return;\n          }\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator.executeHeartbeatCallbacks": "  private void executeHeartbeatCallbacks() {\n    Runnable callback = null;\n    while ((callback = heartbeatCallbacks.poll()) != null) {\n      callback.run();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator.heartbeat": "  protected abstract void heartbeat() throws Exception;\n\n  private void executeHeartbeatCallbacks() {\n    Runnable callback = null;\n    while ((callback = heartbeatCallbacks.poll()) != null) {\n      callback.run();\n    }\n  }"
        },
        "bug_report": {
            "Title": "Divide by zero error in MR AM when calculating available containers",
            "Description": "When running a sleep job with zero CPU vcores i see the following exception\n\n2015-04-30 06:41:06,954 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. \njava.lang.ArithmeticException: / by zero\nat org.apache.hadoop.mapreduce.v2.app.rm.ResourceCalculatorUtils.computeAvailableContainers(ResourceCalculatorUtils.java:38)\nat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:947)\nat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.access$200(RMContainerAllocator.java:840)\nat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:247)\nat org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:282)\nat java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "MAPREDUCE-4913.json",
        "creation_time": "2013-01-05T02:41:54.000+0000",
        "stack_trace": "```\njava.lang.Exception: No handler for registered for class org.apache.hadoop.mapreduce.jobhistory.EventType, cannot deliver EventType: AM_STARTED\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:132)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)\n        at java.lang.Thread.run(Thread.java:662)\n```",
        "source_code": {},
        "bug_report": {
            "Title": "TestMRAppMaster#testMRAppMasterMissingStaging occasionally exits",
            "Description": "testMRAppMasterMissingStaging will sometimes cause the JVM to exit due to this error from AsyncDispatcher:\n\n{noformat}\n2013-01-05 02:14:54,682 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(137)) - Error in dispatcher thread\njava.lang.Exception: No handler for registered for class org.apache.hadoop.mapreduce.jobhistory.EventType, cannot deliver EventType: AM_STARTED\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:132)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)\n        at java.lang.Thread.run(Thread.java:662)\n2013-01-05 02:14:54,682 INFO  [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(140)) - Exiting, bbye..\n{noformat}\n\nThis can cause a build to fail since the test process exits without unregistering from surefire which treats it as a build error rather than a test failure."
        }
    },
    {
        "filename": "MAPREDUCE-6492.json",
        "creation_time": "2015-09-28T10:16:44.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendJHStartEventForAssignedFailTask(TaskAttemptImpl.java:1494)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.access$2900(TaskAttemptImpl.java:147)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1700)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1686)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1190)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:146)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1415)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1407)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)\n\tat java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendJHStartEventForAssignedFailTask": "  private static void\n      sendJHStartEventForAssignedFailTask(TaskAttemptImpl taskAttempt) {\n    TaskAttemptContainerLaunchedEvent event;\n    taskAttempt.launchTime = taskAttempt.clock.getTime();\n\n    InetSocketAddress nodeHttpInetAddr =\n        NetUtils.createSocketAddr(taskAttempt.container.getNodeHttpAddress());\n    taskAttempt.trackerName = nodeHttpInetAddr.getHostName();\n    taskAttempt.httpPort = nodeHttpInetAddr.getPort();\n    taskAttempt.sendLaunchedEvents();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendLaunchedEvents": "  private void sendLaunchedEvents() {\n    JobCounterUpdateEvent jce = new JobCounterUpdateEvent(attemptId.getTaskId()\n        .getJobId());\n    jce.addCounterUpdate(attemptId.getTaskId().getTaskType() == TaskType.MAP ?\n        JobCounter.TOTAL_LAUNCHED_MAPS : JobCounter.TOTAL_LAUNCHED_REDUCES, 1);\n    eventHandler.handle(jce);\n\n    LOG.info(\"TaskAttempt: [\" + attemptId\n        + \"] using containerId: [\" + container.getId() + \" on NM: [\"\n        + StringInterner.weakIntern(container.getNodeId().toString()) + \"]\");\n    TaskAttemptStartedEvent tase =\n      new TaskAttemptStartedEvent(TypeConverter.fromYarn(attemptId),\n          TypeConverter.fromYarn(attemptId.getTaskId().getTaskType()),\n          launchTime, trackerName, httpPort, shufflePort, container.getId(),\n          locality.toString(), avataar.toString());\n    eventHandler.handle(\n        new JobHistoryEvent(attemptId.getTaskId().getJobId(), tase));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.transition": "    public void transition(TaskAttemptImpl taskAttempt, \n        TaskAttemptEvent event) {\n      TaskAttemptDiagnosticsUpdateEvent diagEvent =\n          (TaskAttemptDiagnosticsUpdateEvent) event;\n      LOG.info(\"Diagnostics report from \" + taskAttempt.attemptId + \": \"\n          + diagEvent.getDiagnosticInfo());\n      taskAttempt.addDiagnosticInfo(diagEvent.getDiagnosticInfo());\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getAssignedContainerMgrAddress": "  public String getAssignedContainerMgrAddress() {\n    readLock.lock();\n    try {\n      return container == null ? null : StringInterner.weakIntern(container\n        .getNodeId().toString());\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createJobCounterUpdateEventTASucceeded": "  private static JobCounterUpdateEvent createJobCounterUpdateEventTASucceeded(\n      TaskAttemptImpl taskAttempt) {\n    TaskId taskId = taskAttempt.attemptId.getTaskId();\n    JobCounterUpdateEvent jce = new JobCounterUpdateEvent(taskId.getJobId());\n    updateMillisCounters(jce, taskAttempt);\n    return jce;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.addDiagnosticInfo": "  private void addDiagnosticInfo(String diag) {\n    if (diag != null && !diag.equals(\"\")) {\n      diagnostics.add(diag);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getAssignedContainerID": "  public ContainerId getAssignedContainerID() {\n    readLock.lock();\n    try {\n      return container == null ? null : container.getId();\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.notifyTaskAttemptFailed": "  private static void notifyTaskAttemptFailed(TaskAttemptImpl taskAttempt) {\n    if (taskAttempt.getLaunchTime() == 0) {\n      sendJHStartEventForAssignedFailTask(taskAttempt);\n    }\n    // set the finish time\n    taskAttempt.setFinishTime();\n    taskAttempt.eventHandler\n        .handle(createJobCounterUpdateEventTAFailed(taskAttempt, false));\n    TaskAttemptUnsuccessfulCompletionEvent tauce =\n        createTaskAttemptUnsuccessfulCompletionEvent(taskAttempt,\n            TaskAttemptStateInternal.FAILED);\n    taskAttempt.eventHandler.handle(new JobHistoryEvent(\n        taskAttempt.attemptId.getTaskId().getJobId(), tauce));\n\n    taskAttempt.eventHandler.handle(new TaskTAttemptEvent(\n        taskAttempt.attemptId, TaskEventType.T_ATTEMPT_FAILED));\n\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createRemoteTask": "  protected abstract org.apache.hadoop.mapred.Task createRemoteTask();\n\n  @Override\n  public TaskAttemptId getID() {\n    return attemptId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.setFinishTime": "  private void setFinishTime() {\n    //set the finish time only if launch time is set\n    if (launchTime != 0) {\n      finishTime = clock.getTime();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getFinishTime": "  public long getFinishTime() {\n    readLock.lock();\n    try {\n      return finishTime;\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.finalizeProgress": "  private static void finalizeProgress(TaskAttemptImpl taskAttempt) {\n    // unregister it to TaskAttemptListener so that it stops listening\n    taskAttempt.taskAttemptListener.unregister(\n        taskAttempt.attemptId, taskAttempt.jvmID);\n    taskAttempt.reportedStatus.progress = 1.0f;\n    taskAttempt.updateProgressSplits();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createTaskAttemptUnsuccessfulCompletionEvent": "      TaskAttemptUnsuccessfulCompletionEvent\n      createTaskAttemptUnsuccessfulCompletionEvent(TaskAttemptImpl taskAttempt,\n          TaskAttemptStateInternal attemptState) {\n    TaskAttemptUnsuccessfulCompletionEvent tauce =\n        new TaskAttemptUnsuccessfulCompletionEvent(\n            TypeConverter.fromYarn(taskAttempt.attemptId),\n            TypeConverter.fromYarn(taskAttempt.attemptId.getTaskId()\n                .getTaskType()), attemptState.toString(),\n            taskAttempt.finishTime,\n            taskAttempt.container == null ? \"UNKNOWN\"\n                : taskAttempt.container.getNodeId().getHost(),\n            taskAttempt.container == null ? -1 \n                : taskAttempt.container.getNodeId().getPort(),    \n            taskAttempt.nodeRackName == null ? \"UNKNOWN\" \n                : taskAttempt.nodeRackName,\n            StringUtils.join(\n                LINE_SEPARATOR, taskAttempt.getDiagnostics()),\n                taskAttempt.getCounters(), taskAttempt\n                .getProgressSplitBlock().burst());\n    return tauce;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getShufflePort": "  public int getShufflePort() {\n    readLock.lock();\n    try {\n      return shufflePort;\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getLaunchTime": "  public long getLaunchTime() {\n    readLock.lock();\n    try {\n      return launchTime;\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getID": "  public TaskAttemptId getID() {\n    return attemptId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.logAttemptFinishedEvent": "  private void logAttemptFinishedEvent(TaskAttemptStateInternal state) {\n    //Log finished events only if an attempt started.\n    if (getLaunchTime() == 0) return; \n    String containerHostName = this.container == null ? \"UNKNOWN\"\n         : this.container.getNodeId().getHost();\n    int containerNodePort =\n        this.container == null ? -1 : this.container.getNodeId().getPort();\n    if (attemptId.getTaskId().getTaskType() == TaskType.MAP) {\n      MapAttemptFinishedEvent mfe =\n         new MapAttemptFinishedEvent(TypeConverter.fromYarn(attemptId),\n         TypeConverter.fromYarn(attemptId.getTaskId().getTaskType()),\n         state.toString(),\n         this.reportedStatus.mapFinishTime,\n         finishTime,\n         containerHostName,\n         containerNodePort,\n         this.nodeRackName == null ? \"UNKNOWN\" : this.nodeRackName,\n         this.reportedStatus.stateString,\n         getCounters(),\n         getProgressSplitBlock().burst());\n         eventHandler.handle(\n           new JobHistoryEvent(attemptId.getTaskId().getJobId(), mfe));\n    } else {\n       ReduceAttemptFinishedEvent rfe =\n         new ReduceAttemptFinishedEvent(TypeConverter.fromYarn(attemptId),\n         TypeConverter.fromYarn(attemptId.getTaskId().getTaskType()),\n         state.toString(),\n         this.reportedStatus.shuffleFinishTime,\n         this.reportedStatus.sortFinishTime,\n         finishTime,\n         containerHostName,\n         containerNodePort,\n         this.nodeRackName == null ? \"UNKNOWN\" : this.nodeRackName,\n         this.reportedStatus.stateString,\n         getCounters(),\n         getProgressSplitBlock().burst());\n         eventHandler.handle(\n           new JobHistoryEvent(attemptId.getTaskId().getJobId(), rfe));\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createJobCounterUpdateEventTAFailed": "  private static JobCounterUpdateEvent createJobCounterUpdateEventTAFailed(\n      TaskAttemptImpl taskAttempt, boolean taskAlreadyCompleted) {\n    TaskType taskType = taskAttempt.getID().getTaskId().getTaskType();\n    JobCounterUpdateEvent jce = new JobCounterUpdateEvent(taskAttempt.getID().getTaskId().getJobId());\n    \n    if (taskType == TaskType.MAP) {\n      jce.addCounterUpdate(JobCounter.NUM_FAILED_MAPS, 1);\n    } else {\n      jce.addCounterUpdate(JobCounter.NUM_FAILED_REDUCES, 1);\n    }\n    if (!taskAlreadyCompleted) {\n      updateMillisCounters(jce, taskAttempt);\n    }\n    return jce;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.updateProgressSplits": "  private void updateProgressSplits() {\n    double newProgress = reportedStatus.progress;\n    newProgress = Math.max(Math.min(newProgress, 1.0D), 0.0D);\n    Counters counters = reportedStatus.counters;\n    if (counters == null)\n      return;\n\n    WrappedProgressSplitsBlock splitsBlock = getProgressSplitBlock();\n    if (splitsBlock != null) {\n      long now = clock.getTime();\n      long start = getLaunchTime(); // TODO Ensure not 0\n\n      if (start != 0 && now - start <= Integer.MAX_VALUE) {\n        splitsBlock.getProgressWallclockTime().extend(newProgress,\n            (int) (now - start));\n      }\n\n      Counter cpuCounter = counters.findCounter(TaskCounter.CPU_MILLISECONDS);\n      if (cpuCounter != null && cpuCounter.getValue() <= Integer.MAX_VALUE) {\n        splitsBlock.getProgressCPUTime().extend(newProgress,\n            (int) cpuCounter.getValue()); // long to int? TODO: FIX. Same below\n      }\n\n      Counter virtualBytes = counters\n        .findCounter(TaskCounter.VIRTUAL_MEMORY_BYTES);\n      if (virtualBytes != null) {\n        splitsBlock.getProgressVirtualMemoryKbytes().extend(newProgress,\n            (int) (virtualBytes.getValue() / (MEMORY_SPLITS_RESOLUTION)));\n      }\n\n      Counter physicalBytes = counters\n        .findCounter(TaskCounter.PHYSICAL_MEMORY_BYTES);\n      if (physicalBytes != null) {\n        splitsBlock.getProgressPhysicalMemoryKbytes().extend(newProgress,\n            (int) (physicalBytes.getValue() / (MEMORY_SPLITS_RESOLUTION)));\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendContainerCleanup": "  private static void sendContainerCleanup(TaskAttemptImpl taskAttempt,\n      TaskAttemptEvent event) {\n    if (event instanceof TaskAttemptKillEvent) {\n      taskAttempt.addDiagnosticInfo(\n          ((TaskAttemptKillEvent) event).getMessage());\n    }\n    //send the cleanup event to containerLauncher\n    taskAttempt.eventHandler.handle(new ContainerLauncherEvent(\n        taskAttempt.attemptId,\n        taskAttempt.container.getId(), StringInterner\n        .weakIntern(taskAttempt.container.getNodeId().toString()),\n        taskAttempt.container.getContainerToken(),\n        ContainerLauncher.EventType.CONTAINER_REMOTE_CLEANUP));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.recover": "  public TaskAttemptStateInternal recover(TaskAttemptInfo taInfo,\n      OutputCommitter committer, boolean recoverOutput) {\n    ContainerId containerId = taInfo.getContainerId();\n    NodeId containerNodeId = ConverterUtils.toNodeId(taInfo.getHostname() + \":\"\n        + taInfo.getPort());\n    String nodeHttpAddress = StringInterner.weakIntern(taInfo.getHostname() + \":\"\n        + taInfo.getHttpPort());\n    // Resource/Priority/Tokens are only needed while launching the container on\n    // an NM, these are already completed tasks, so setting them to null\n    container =\n        Container.newInstance(containerId, containerNodeId,\n          nodeHttpAddress, null, null, null);\n    computeRackAndLocality();\n    launchTime = taInfo.getStartTime();\n    finishTime = (taInfo.getFinishTime() != -1) ?\n        taInfo.getFinishTime() : clock.getTime();\n    shufflePort = taInfo.getShufflePort();\n    trackerName = taInfo.getHostname();\n    httpPort = taInfo.getHttpPort();\n    sendLaunchedEvents();\n\n    reportedStatus.id = attemptId;\n    reportedStatus.progress = 1.0f;\n    reportedStatus.counters = taInfo.getCounters();\n    reportedStatus.stateString = taInfo.getState();\n    reportedStatus.phase = Phase.CLEANUP;\n    reportedStatus.mapFinishTime = taInfo.getMapFinishTime();\n    reportedStatus.shuffleFinishTime = taInfo.getShuffleFinishTime();\n    reportedStatus.sortFinishTime = taInfo.getSortFinishTime();\n    addDiagnosticInfo(taInfo.getError());\n\n    boolean needToClean = false;\n    String recoveredState = taInfo.getTaskStatus();\n    if (recoverOutput\n        && TaskAttemptState.SUCCEEDED.toString().equals(recoveredState)) {\n      TaskAttemptContext tac = new TaskAttemptContextImpl(conf,\n          TypeConverter.fromYarn(attemptId));\n      try {\n        committer.recoverTask(tac);\n        LOG.info(\"Recovered output from task attempt \" + attemptId);\n      } catch (Exception e) {\n        LOG.error(\"Unable to recover task attempt \" + attemptId, e);\n        LOG.info(\"Task attempt \" + attemptId + \" will be recovered as KILLED\");\n        recoveredState = TaskAttemptState.KILLED.toString();\n        needToClean = true;\n      }\n    }\n\n    TaskAttemptStateInternal attemptState;\n    if (TaskAttemptState.SUCCEEDED.toString().equals(recoveredState)) {\n      attemptState = TaskAttemptStateInternal.SUCCEEDED;\n      reportedStatus.taskState = TaskAttemptState.SUCCEEDED;\n      eventHandler.handle(createJobCounterUpdateEventTASucceeded(this));\n      logAttemptFinishedEvent(attemptState);\n    } else if (TaskAttemptState.FAILED.toString().equals(recoveredState)) {\n      attemptState = TaskAttemptStateInternal.FAILED;\n      reportedStatus.taskState = TaskAttemptState.FAILED;\n      eventHandler.handle(createJobCounterUpdateEventTAFailed(this, false));\n      TaskAttemptUnsuccessfulCompletionEvent tauce =\n          createTaskAttemptUnsuccessfulCompletionEvent(this,\n              TaskAttemptStateInternal.FAILED);\n      eventHandler.handle(\n          new JobHistoryEvent(attemptId.getTaskId().getJobId(), tauce));\n    } else {\n      if (!TaskAttemptState.KILLED.toString().equals(recoveredState)) {\n        if (String.valueOf(recoveredState).isEmpty()) {\n          LOG.info(\"TaskAttempt\" + attemptId\n              + \" had not completed, recovering as KILLED\");\n        } else {\n          LOG.warn(\"TaskAttempt \" + attemptId + \" found in unexpected state \"\n              + recoveredState + \", recovering as KILLED\");\n        }\n        addDiagnosticInfo(\"Killed during application recovery\");\n        needToClean = true;\n      }\n      attemptState = TaskAttemptStateInternal.KILLED;\n      reportedStatus.taskState = TaskAttemptState.KILLED;\n      eventHandler.handle(createJobCounterUpdateEventTAKilled(this, false));\n      TaskAttemptUnsuccessfulCompletionEvent tauce =\n          createTaskAttemptUnsuccessfulCompletionEvent(this,\n              TaskAttemptStateInternal.KILLED);\n      eventHandler.handle(\n          new JobHistoryEvent(attemptId.getTaskId().getJobId(), tauce));\n    }\n\n    if (needToClean) {\n      TaskAttemptContext tac = new TaskAttemptContextImpl(conf,\n          TypeConverter.fromYarn(attemptId));\n      try {\n        committer.abortTask(tac);\n      } catch (Exception e) {\n        LOG.warn(\"Task cleanup failed for attempt \" + attemptId, e);\n      }\n    }\n\n    return attemptState;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createContainerLaunchContext": "  static ContainerLaunchContext createContainerLaunchContext(\n      Map<ApplicationAccessType, String> applicationACLs,\n      Configuration conf, Token<JobTokenIdentifier> jobToken, Task remoteTask,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      WrappedJvmID jvmID,\n      TaskAttemptListener taskAttemptListener,\n      Credentials credentials) {\n\n    synchronized (commonContainerSpecLock) {\n      if (commonContainerSpec == null) {\n        commonContainerSpec = createCommonContainerLaunchContext(\n            applicationACLs, conf, jobToken, oldJobId, credentials);\n      }\n    }\n\n    // Fill in the fields needed per-container that are missing in the common\n    // spec.\n\n    boolean userClassesTakesPrecedence =\n      conf.getBoolean(MRJobConfig.MAPREDUCE_JOB_USER_CLASSPATH_FIRST, false);\n\n    // Setup environment by cloning from common env.\n    Map<String, String> env = commonContainerSpec.getEnvironment();\n    Map<String, String> myEnv = new HashMap<String, String>(env.size());\n    myEnv.putAll(env);\n    if (userClassesTakesPrecedence) {\n      myEnv.put(Environment.CLASSPATH_PREPEND_DISTCACHE.name(), \"true\");\n    }\n    MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n\n    // Set up the launch command\n    List<String> commands = MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, jvmID);\n\n    // Duplicate the ByteBuffers for access by multiple containers.\n    Map<String, ByteBuffer> myServiceData = new HashMap<String, ByteBuffer>();\n    for (Entry<String, ByteBuffer> entry : commonContainerSpec\n                .getServiceData().entrySet()) {\n      myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n    }\n\n    // Construct the actual Container\n    ContainerLaunchContext container = ContainerLaunchContext.newInstance(\n        commonContainerSpec.getLocalResources(), myEnv, commands,\n        myServiceData, commonContainerSpec.getTokens().duplicate(),\n        applicationACLs);\n\n    return container;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendContainerCompleted": "  private static void sendContainerCompleted(TaskAttemptImpl taskAttempt) {\n    taskAttempt.eventHandler.handle(new ContainerLauncherEvent(\n        taskAttempt.attemptId,\n        taskAttempt.container.getId(), StringInterner\n        .weakIntern(taskAttempt.container.getNodeId().toString()),\n        taskAttempt.container.getContainerToken(),\n        ContainerLauncher.EventType.CONTAINER_COMPLETED));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createJobCounterUpdateEventTAKilled": "  private static JobCounterUpdateEvent createJobCounterUpdateEventTAKilled(\n      TaskAttemptImpl taskAttempt, boolean taskAlreadyCompleted) {\n    TaskType taskType = taskAttempt.getID().getTaskId().getTaskType();\n    JobCounterUpdateEvent jce = new JobCounterUpdateEvent(taskAttempt.getID().getTaskId().getJobId());\n    \n    if (taskType == TaskType.MAP) {\n      jce.addCounterUpdate(JobCounter.NUM_KILLED_MAPS, 1);\n    } else {\n      jce.addCounterUpdate(JobCounter.NUM_KILLED_REDUCES, 1);\n    }\n    if (!taskAlreadyCompleted) {\n      updateMillisCounters(jce, taskAttempt);\n    }\n    return jce;\n  }  ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getState": "  public TaskAttemptState getState() {\n    readLock.lock();\n    try {\n      return getExternalState(stateMachine.getCurrentState());\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.computeRackAndLocality": "  private void computeRackAndLocality() {\n    NodeId containerNodeId = container.getNodeId();\n    nodeRackName = RackResolver.resolve(\n        containerNodeId.getHost()).getNetworkLocation();\n\n    locality = Locality.OFF_SWITCH;\n    if (dataLocalHosts.size() > 0) {\n      String cHost = resolveHost(containerNodeId.getHost());\n      if (dataLocalHosts.contains(cHost)) {\n        locality = Locality.NODE_LOCAL;\n      }\n    }\n    if (locality == Locality.OFF_SWITCH) {\n      if (dataLocalRacks.contains(nodeRackName)) {\n        locality = Locality.RACK_LOCAL;\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle": "  public void handle(TaskAttemptEvent event) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Processing \" + event.getTaskAttemptID() + \" of type \"\n          + event.getType());\n    }\n    writeLock.lock();\n    try {\n      final TaskAttemptStateInternal oldState = getInternalState()  ;\n      try {\n        stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitionException e) {\n        LOG.error(\"Can't handle this event at current state for \"\n            + this.attemptId, e);\n        eventHandler.handle(new JobDiagnosticsUpdateEvent(\n            this.attemptId.getTaskId().getJobId(), \"Invalid event \" + event.getType() + \n            \" on TaskAttempt \" + this.attemptId));\n        eventHandler.handle(new JobEvent(this.attemptId.getTaskId().getJobId(),\n            JobEventType.INTERNAL_ERROR));\n      }\n      if (oldState != getInternalState()) {\n          LOG.info(attemptId + \" TaskAttempt Transitioned from \" \n           + oldState + \" to \"\n           + getInternalState());\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getInternalState": "  public TaskAttemptStateInternal getInternalState() {\n    readLock.lock();\n    try {\n      return stateMachine.getCurrentState();\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.handle": "    public void handle(Event event) {\n      //Empty\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJob": "    public Job getJob(JobId jobID) {\n      return jobs.get(jobID);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob": "  public void shutDownJob() {\n    // job has finished\n    // this is the only job, so shut down the Appmaster\n    // note in a workflow scenario, this may lead to creation of a new\n    // job (FIXME?)\n\n    try {\n      //if isLastAMRetry comes as true, should never set it to false\n      if ( !isLastAMRetry){\n        if (((JobImpl)job).getInternalState() != JobStateInternal.REBOOT) {\n          LOG.info(\"Job finished cleanly, recording last MRAppMaster retry\");\n          isLastAMRetry = true;\n        }\n      }\n      notifyIsLastAMRetry(isLastAMRetry);\n      // Stop all services\n      // This will also send the final report to the ResourceManager\n      LOG.info(\"Calling stop for all the services\");\n      MRAppMaster.this.stop();\n\n      if (isLastAMRetry) {\n        // Send job-end notification when it is safe to report termination to\n        // users and it is the last AM retry\n        if (getConfig().get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL) != null) {\n          try {\n            LOG.info(\"Job end notification started for jobID : \"\n                + job.getReport().getJobId());\n            JobEndNotifier notifier = new JobEndNotifier();\n            notifier.setConf(getConfig());\n            JobReport report = job.getReport();\n            // If unregistration fails, the final state is unavailable. However,\n            // at the last AM Retry, the client will finally be notified FAILED\n            // from RM, so we should let users know FAILED via notifier as well\n            if (!context.hasSuccessfullyUnregistered()) {\n              report.setJobState(JobState.FAILED);\n            }\n            notifier.notify(report);\n          } catch (InterruptedException ie) {\n            LOG.warn(\"Job end notification interrupted for jobID : \"\n                + job.getReport().getJobId(), ie);\n          }\n        }\n      }\n\n      try {\n        Thread.sleep(5000);\n      } catch (InterruptedException e) {\n        e.printStackTrace();\n      }\n      clientService.stop();\n    } catch (Throwable t) {\n      LOG.warn(\"Graceful stop failed. Exiting.. \", t);\n      exitMRAppMaster(1, t);\n    }\n    exitMRAppMaster(0, null);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.callWithJobClassLoader": "  <T> T callWithJobClassLoader(Configuration conf, ExceptionAction<T> action)\n      throws IOException {\n    // if the job classloader is enabled, we may need it to load the (custom)\n    // classes; we make the job classloader available and unset it once it is\n    // done\n    ClassLoader currentClassLoader = conf.getClassLoader();\n    boolean setJobClassLoader =\n        jobClassLoader != null && currentClassLoader != jobClassLoader;\n    if (setJobClassLoader) {\n      MRApps.setClassLoader(jobClassLoader, conf);\n    }\n    try {\n      return action.call(conf);\n    } catch (IOException e) {\n      throw e;\n    } catch (YarnRuntimeException e) {\n      throw e;\n    } catch (Exception e) {\n      // wrap it with a YarnRuntimeException\n      throw new YarnRuntimeException(e);\n    } finally {\n      if (setJobClassLoader) {\n        // restore the original classloader\n        MRApps.setClassLoader(currentClassLoader, conf);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJobId": "  public JobId getJobId() {\n    return jobId;\n  }"
        },
        "bug_report": {
            "Title": "AsyncDispatcher exit with NPE on TaskAttemptImpl#sendJHStartEventForAssignedFailTask",
            "Description": "For {{TaskAttemptImpl#DeallocateContainerTransition}} {{sendJHStartEventForAssignedFailTask}} is send for TaskAttemptStateInternal.UNASSIGNED also .\n\n\nCausing NPE on {{taskAttempt.container.getNodeHttpAddress()}} \n\n\n{noformat}\n2015-09-28 18:01:48,656 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread\njava.lang.NullPointerException\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendJHStartEventForAssignedFailTask(TaskAttemptImpl.java:1494)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.access$2900(TaskAttemptImpl.java:147)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1700)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1686)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1190)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:146)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1415)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1407)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)\n\tat java.lang.Thread.run(Thread.java:745)\n2015-09-28 18:01:48,660 INFO [AsyncDispatcher ShutDown handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye..\n2015-09-28 18:01:48,660 INFO [ContainerLauncher #6] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_e04_1443430524957_0006_01_000059 taskAttempt attempt_1443430524957_0006_m_000000_9\n{noformat}\n\nLog aggregation fail for mapreduce application.\n"
        }
    },
    {
        "filename": "MAPREDUCE-5744.json",
        "creation_time": "2014-02-06T19:05:01.000+0000",
        "stack_trace": "```\njava.lang.IllegalArgumentException: Comparison method violates its general contract!\n     at java.util.TimSort.mergeLo(TimSort.java:747)\n     at java.util.TimSort.mergeAt(TimSort.java:483)\n     at java.util.TimSort.mergeCollapse(TimSort.java:408)\n     at java.util.TimSort.sort(TimSort.java:214)\n     at java.util.TimSort.sort(TimSort.java:173)\n     at java.util.Arrays.sort(Arrays.java:659)\n     at java.util.Collections.sort(Collections.java:217)\n     at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests.preemptReduce(RMContainerAllocator.java:1106)\n     at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.preemptReducesIfNeeded(RMContainerAllocator.java:416)\n     at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:230)\n     at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:252)\n     at java.lang.Thread.run(Thread.java:744)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.preemptReduce": "    void preemptReduce(int toPreempt) {\n      List<TaskAttemptId> reduceList = new ArrayList<TaskAttemptId>\n        (reduces.keySet());\n      //sort reduces on progress\n      Collections.sort(reduceList,\n          new Comparator<TaskAttemptId>() {\n        @Override\n        public int compare(TaskAttemptId o1, TaskAttemptId o2) {\n          float p = getJob().getTask(o1.getTaskId()).getAttempt(o1).getProgress() -\n              getJob().getTask(o2.getTaskId()).getAttempt(o2).getProgress();\n          return p >= 0 ? 1 : -1;\n        }\n      });\n      \n      for (int i = 0; i < toPreempt && reduceList.size() > 0; i++) {\n        TaskAttemptId id = reduceList.remove(0);//remove the one on top\n        LOG.info(\"Preempting \" + id);\n        preemptionWaitingReduces.add(id);\n        eventHandler.handle(new TaskAttemptEvent(id, TaskAttemptEventType.TA_KILL));\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.add": "    void add(Container container, TaskAttemptId tId) {\n      LOG.info(\"Assigned container \" + container.getId().toString() + \" to \" + tId);\n      containerToAttemptMap.put(container.getId(), tId);\n      if (tId.getTaskId().getTaskType().equals(TaskType.MAP)) {\n        maps.put(tId, container);\n      } else {\n        reduces.put(tId, container);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.handle": "  public void handle(ContainerAllocatorEvent event) {\n    int qSize = eventQueue.size();\n    if (qSize != 0 && qSize % 1000 == 0) {\n      LOG.info(\"Size of event-queue in RMContainerAllocator is \" + qSize);\n    }\n    int remCapacity = eventQueue.remainingCapacity();\n    if (remCapacity < 1000) {\n      LOG.warn(\"Very low remaining capacity in the event-queue \"\n          + \"of RMContainerAllocator: \" + remCapacity);\n    }\n    try {\n      eventQueue.put(event);\n    } catch (InterruptedException e) {\n      throw new YarnRuntimeException(e);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.remove": "    boolean remove(TaskAttemptId tId) {\n      ContainerId containerId = null;\n      if (tId.getTaskId().getTaskType().equals(TaskType.MAP)) {\n        containerId = maps.remove(tId).getId();\n      } else {\n        containerId = reduces.remove(tId).getId();\n        if (containerId != null) {\n          boolean preempted = preemptionWaitingReduces.remove(tId);\n          if (preempted) {\n            LOG.info(\"Reduce preemption successful \" + tId);\n          }\n        }\n      }\n      \n      if (containerId != null) {\n        containerToAttemptMap.remove(containerId);\n        return true;\n      }\n      return false;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.preemptReducesIfNeeded": "  private void preemptReducesIfNeeded() {\n    if (reduceResourceReqt == 0) {\n      return; //no reduces\n    }\n    //check if reduces have taken over the whole cluster and there are \n    //unassigned maps\n    if (scheduledRequests.maps.size() > 0) {\n      int memLimit = getMemLimit();\n      int availableMemForMap = memLimit - ((assignedRequests.reduces.size() -\n          assignedRequests.preemptionWaitingReduces.size()) * reduceResourceReqt);\n      //availableMemForMap must be sufficient to run atleast 1 map\n      if (availableMemForMap < mapResourceReqt) {\n        //to make sure new containers are given to maps and not reduces\n        //ramp down all scheduled reduces if any\n        //(since reduces are scheduled at higher priority than maps)\n        LOG.info(\"Ramping down all scheduled reduces:\" + scheduledRequests.reduces.size());\n        for (ContainerRequest req : scheduledRequests.reduces.values()) {\n          pendingReduces.add(req);\n        }\n        scheduledRequests.reduces.clear();\n        \n        //preempt for making space for at least one map\n        int premeptionLimit = Math.max(mapResourceReqt, \n            (int) (maxReducePreemptionLimit * memLimit));\n        \n        int preemptMem = Math.min(scheduledRequests.maps.size() * mapResourceReqt, \n            premeptionLimit);\n        \n        int toPreempt = (int) Math.ceil((float) preemptMem/reduceResourceReqt);\n        toPreempt = Math.min(toPreempt, assignedRequests.reduces.size());\n        \n        LOG.info(\"Going to preempt \" + toPreempt + \" due to lack of space for maps\");\n        assignedRequests.preemptReduce(toPreempt);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getMemLimit": "  public int getMemLimit() {\n    int headRoom = getAvailableResources() != null ? getAvailableResources().getMemory() : 0;\n    return headRoom + assignedRequests.maps.size() * mapResourceReqt + \n       assignedRequests.reduces.size() * reduceResourceReqt;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat": "  protected synchronized void heartbeat() throws Exception {\n    scheduleStats.updateAndLogIfChanged(\"Before Scheduling: \");\n    List<Container> allocatedContainers = getResources();\n    if (allocatedContainers.size() > 0) {\n      scheduledRequests.assign(allocatedContainers);\n    }\n\n    int completedMaps = getJob().getCompletedMaps();\n    int completedTasks = completedMaps + getJob().getCompletedReduces();\n    if ((lastCompletedTasks != completedTasks) ||\n          (scheduledRequests.maps.size() > 0)) {\n      lastCompletedTasks = completedTasks;\n      recalculateReduceSchedule = true;\n    }\n\n    if (recalculateReduceSchedule) {\n      preemptReducesIfNeeded();\n      scheduleReduces(\n          getJob().getTotalMaps(), completedMaps,\n          scheduledRequests.maps.size(), scheduledRequests.reduces.size(), \n          assignedRequests.maps.size(), assignedRequests.reduces.size(),\n          mapResourceReqt, reduceResourceReqt,\n          pendingReduces.size(), \n          maxReduceRampupLimit, reduceSlowStart);\n      recalculateReduceSchedule = false;\n    }\n\n    scheduleStats.updateAndLogIfChanged(\"After Scheduling: \");\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getResources": "  private List<Container> getResources() throws Exception {\n    int headRoom = getAvailableResources() != null\n        ? getAvailableResources().getMemory() : 0;//first time it would be null\n    AllocateResponse response;\n    /*\n     * If contact with RM is lost, the AM will wait MR_AM_TO_RM_WAIT_INTERVAL_MS\n     * milliseconds before aborting. During this interval, AM will still try\n     * to contact the RM.\n     */\n    try {\n      response = makeRemoteRequest();\n      // Reset retry count if no exception occurred.\n      retrystartTime = System.currentTimeMillis();\n    } catch (Exception e) {\n      // This can happen when the connection to the RM has gone down. Keep\n      // re-trying until the retryInterval has expired.\n      if (System.currentTimeMillis() - retrystartTime >= retryInterval) {\n        LOG.error(\"Could not contact RM after \" + retryInterval + \" milliseconds.\");\n        eventHandler.handle(new JobEvent(this.getJob().getID(),\n                                         JobEventType.INTERNAL_ERROR));\n        throw new YarnRuntimeException(\"Could not contact RM after \" +\n                                retryInterval + \" milliseconds.\");\n      }\n      // Throw this up to the caller, which may decide to ignore it and\n      // continue to attempt to contact the RM.\n      throw e;\n    }\n    if (response.getAMCommand() != null) {\n      switch(response.getAMCommand()) {\n      case AM_RESYNC:\n      case AM_SHUTDOWN:\n        // This can happen if the RM has been restarted. If it is in that state,\n        // this application must clean itself up.\n        eventHandler.handle(new JobEvent(this.getJob().getID(),\n                                         JobEventType.JOB_AM_REBOOT));\n        throw new YarnRuntimeException(\"Resource Manager doesn't recognize AttemptId: \" +\n                                 this.getContext().getApplicationID());\n      default:\n        String msg =\n              \"Unhandled value of AMCommand: \" + response.getAMCommand();\n        LOG.error(msg);\n        throw new YarnRuntimeException(msg);\n      }\n    }\n    int newHeadRoom = getAvailableResources() != null ? getAvailableResources().getMemory() : 0;\n    List<Container> newContainers = response.getAllocatedContainers();\n    // Setting NMTokens\n    if (response.getNMTokens() != null) {\n      for (NMToken nmToken : response.getNMTokens()) {\n        NMTokenCache.setNMToken(nmToken.getNodeId().toString(),\n            nmToken.getToken());\n      }\n    }\n    \n    List<ContainerStatus> finishedContainers = response.getCompletedContainersStatuses();\n\n    // propagate preemption requests\n    final PreemptionMessage preemptReq = response.getPreemptionMessage();\n    if (preemptReq != null) {\n      preemptionPolicy.preempt(\n          new PreemptionContext(assignedRequests), preemptReq);\n    }\n\n    if (newContainers.size() + finishedContainers.size() > 0 || headRoom != newHeadRoom) {\n      //something changed\n      recalculateReduceSchedule = true;\n      if (LOG.isDebugEnabled() && headRoom != newHeadRoom) {\n        LOG.debug(\"headroom=\" + newHeadRoom);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      for (Container cont : newContainers) {\n        LOG.debug(\"Received new Container :\" + cont);\n      }\n    }\n\n    //Called on each allocation. Will know about newly blacklisted/added hosts.\n    computeIgnoreBlacklisting();\n\n    handleUpdatedNodes(response);\n\n    for (ContainerStatus cont : finishedContainers) {\n      LOG.info(\"Received completed container \" + cont.getContainerId());\n      TaskAttemptId attemptID = assignedRequests.get(cont.getContainerId());\n      if (attemptID == null) {\n        LOG.error(\"Container complete event for unknown container id \"\n            + cont.getContainerId());\n      } else {\n        assignedRequests.remove(attemptID);\n        \n        // send the container completed event to Task attempt\n        eventHandler.handle(createContainerFinishedEvent(cont, attemptID));\n        \n        // Send the diagnostics\n        String diagnostics = StringInterner.weakIntern(cont.getDiagnostics());\n        eventHandler.handle(new TaskAttemptDiagnosticsUpdateEvent(attemptID,\n            diagnostics));\n\n        preemptionPolicy.handleCompletedContainer(attemptID);\n      }\n    }\n    return newContainers;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.assign": "    private void assign(List<Container> allocatedContainers) {\n      Iterator<Container> it = allocatedContainers.iterator();\n      LOG.info(\"Got allocated containers \" + allocatedContainers.size());\n      containersAllocated += allocatedContainers.size();\n      while (it.hasNext()) {\n        Container allocated = it.next();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Assigning container \" + allocated.getId()\n              + \" with priority \" + allocated.getPriority() + \" to NM \"\n              + allocated.getNodeId());\n        }\n        \n        // check if allocated container meets memory requirements \n        // and whether we have any scheduled tasks that need \n        // a container to be assigned\n        boolean isAssignable = true;\n        Priority priority = allocated.getPriority();\n        int allocatedMemory = allocated.getResource().getMemory();\n        if (PRIORITY_FAST_FAIL_MAP.equals(priority) \n            || PRIORITY_MAP.equals(priority)) {\n          if (allocatedMemory < mapResourceReqt\n              || maps.isEmpty()) {\n            LOG.info(\"Cannot assign container \" + allocated \n                + \" for a map as either \"\n                + \" container memory less than required \" + mapResourceReqt\n                + \" or no pending map tasks - maps.isEmpty=\" \n                + maps.isEmpty()); \n            isAssignable = false; \n          }\n        } \n        else if (PRIORITY_REDUCE.equals(priority)) {\n          if (allocatedMemory < reduceResourceReqt\n              || reduces.isEmpty()) {\n            LOG.info(\"Cannot assign container \" + allocated \n                + \" for a reduce as either \"\n                + \" container memory less than required \" + reduceResourceReqt\n                + \" or no pending reduce tasks - reduces.isEmpty=\" \n                + reduces.isEmpty()); \n            isAssignable = false;\n          }\n        } else {\n          LOG.warn(\"Container allocated at unwanted priority: \" + priority + \n              \". Returning to RM...\");\n          isAssignable = false;\n        }\n        \n        if(!isAssignable) {\n          // release container if we could not assign it \n          containerNotAssigned(allocated);\n          it.remove();\n          continue;\n        }\n        \n        // do not assign if allocated container is on a  \n        // blacklisted host\n        String allocatedHost = allocated.getNodeId().getHost();\n        if (isNodeBlacklisted(allocatedHost)) {\n          // we need to request for a new container \n          // and release the current one\n          LOG.info(\"Got allocated container on a blacklisted \"\n              + \" host \"+allocatedHost\n              +\". Releasing container \" + allocated);\n\n          // find the request matching this allocated container \n          // and replace it with a new one \n          ContainerRequest toBeReplacedReq = \n              getContainerReqToReplace(allocated);\n          if (toBeReplacedReq != null) {\n            LOG.info(\"Placing a new container request for task attempt \" \n                + toBeReplacedReq.attemptID);\n            ContainerRequest newReq = \n                getFilteredContainerRequest(toBeReplacedReq);\n            decContainerReq(toBeReplacedReq);\n            if (toBeReplacedReq.attemptID.getTaskId().getTaskType() ==\n                TaskType.MAP) {\n              maps.put(newReq.attemptID, newReq);\n            }\n            else {\n              reduces.put(newReq.attemptID, newReq);\n            }\n            addContainerReq(newReq);\n          }\n          else {\n            LOG.info(\"Could not map allocated container to a valid request.\"\n                + \" Releasing allocated container \" + allocated);\n          }\n          \n          // release container if we could not assign it \n          containerNotAssigned(allocated);\n          it.remove();\n          continue;\n        }\n      }\n\n      assignContainers(allocatedContainers);\n       \n      // release container if we could not assign it \n      it = allocatedContainers.iterator();\n      while (it.hasNext()) {\n        Container allocated = it.next();\n        LOG.info(\"Releasing unassigned and invalid container \" \n            + allocated + \". RM may have assignment issues\");\n        containerNotAssigned(allocated);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.updateAndLogIfChanged": "    public void updateAndLogIfChanged(String msgPrefix) {\n      boolean changed = false;\n\n      // synchronized to fix findbug warnings\n      synchronized (RMContainerAllocator.this) {\n        changed |= (numPendingReduces != pendingReduces.size());\n        numPendingReduces = pendingReduces.size();\n        changed |= (numScheduledMaps != scheduledRequests.maps.size());\n        numScheduledMaps = scheduledRequests.maps.size();\n        changed |= (numScheduledReduces != scheduledRequests.reduces.size());\n        numScheduledReduces = scheduledRequests.reduces.size();\n        changed |= (numAssignedMaps != assignedRequests.maps.size());\n        numAssignedMaps = assignedRequests.maps.size();\n        changed |= (numAssignedReduces != assignedRequests.reduces.size());\n        numAssignedReduces = assignedRequests.reduces.size();\n        changed |= (numCompletedMaps != getJob().getCompletedMaps());\n        numCompletedMaps = getJob().getCompletedMaps();\n        changed |= (numCompletedReduces != getJob().getCompletedReduces());\n        numCompletedReduces = getJob().getCompletedReduces();\n        changed |= (numContainersAllocated != containersAllocated);\n        numContainersAllocated = containersAllocated;\n        changed |= (numContainersReleased != containersReleased);\n        numContainersReleased = containersReleased;\n      }\n\n      if (changed) {\n        log(msgPrefix);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.scheduleReduces": "  public void scheduleReduces(\n      int totalMaps, int completedMaps,\n      int scheduledMaps, int scheduledReduces,\n      int assignedMaps, int assignedReduces,\n      int mapResourceReqt, int reduceResourceReqt,\n      int numPendingReduces,\n      float maxReduceRampupLimit, float reduceSlowStart) {\n    \n    if (numPendingReduces == 0) {\n      return;\n    }\n    \n    int headRoom = getAvailableResources() != null ?\n        getAvailableResources().getMemory() : 0;\n    LOG.info(\"Recalculating schedule, headroom=\" + headRoom);\n    \n    //check for slow start\n    if (!getIsReduceStarted()) {//not set yet\n      int completedMapsForReduceSlowstart = (int)Math.ceil(reduceSlowStart * \n                      totalMaps);\n      if(completedMaps < completedMapsForReduceSlowstart) {\n        LOG.info(\"Reduce slow start threshold not met. \" +\n              \"completedMapsForReduceSlowstart \" + \n            completedMapsForReduceSlowstart);\n        return;\n      } else {\n        LOG.info(\"Reduce slow start threshold reached. Scheduling reduces.\");\n        setIsReduceStarted(true);\n      }\n    }\n    \n    //if all maps are assigned, then ramp up all reduces irrespective of the\n    //headroom\n    if (scheduledMaps == 0 && numPendingReduces > 0) {\n      LOG.info(\"All maps assigned. \" +\n          \"Ramping up all remaining reduces:\" + numPendingReduces);\n      scheduleAllReduces();\n      return;\n    }\n\n    float completedMapPercent = 0f;\n    if (totalMaps != 0) {//support for 0 maps\n      completedMapPercent = (float)completedMaps/totalMaps;\n    } else {\n      completedMapPercent = 1;\n    }\n    \n    int netScheduledMapMem = \n        (scheduledMaps + assignedMaps) * mapResourceReqt;\n\n    int netScheduledReduceMem = \n        (scheduledReduces + assignedReduces) * reduceResourceReqt;\n\n    int finalMapMemLimit = 0;\n    int finalReduceMemLimit = 0;\n    \n    // ramp up the reduces based on completed map percentage\n    int totalMemLimit = getMemLimit();\n    int idealReduceMemLimit = \n        Math.min(\n            (int)(completedMapPercent * totalMemLimit),\n            (int) (maxReduceRampupLimit * totalMemLimit));\n    int idealMapMemLimit = totalMemLimit - idealReduceMemLimit;\n\n    // check if there aren't enough maps scheduled, give the free map capacity\n    // to reduce\n    if (idealMapMemLimit > netScheduledMapMem) {\n      int unusedMapMemLimit = idealMapMemLimit - netScheduledMapMem;\n      finalReduceMemLimit = idealReduceMemLimit + unusedMapMemLimit;\n      finalMapMemLimit = totalMemLimit - finalReduceMemLimit;\n    } else {\n      finalMapMemLimit = idealMapMemLimit;\n      finalReduceMemLimit = idealReduceMemLimit;\n    }\n    \n    LOG.info(\"completedMapPercent \" + completedMapPercent +\n        \" totalMemLimit:\" + totalMemLimit +\n        \" finalMapMemLimit:\" + finalMapMemLimit +\n        \" finalReduceMemLimit:\" + finalReduceMemLimit + \n        \" netScheduledMapMem:\" + netScheduledMapMem +\n        \" netScheduledReduceMem:\" + netScheduledReduceMem);\n    \n    int rampUp = \n        (finalReduceMemLimit - netScheduledReduceMem) / reduceResourceReqt;\n    \n    if (rampUp > 0) {\n      rampUp = Math.min(rampUp, numPendingReduces);\n      LOG.info(\"Ramping up \" + rampUp);\n      rampUpReduces(rampUp);\n    } else if (rampUp < 0){\n      int rampDown = -1 * rampUp;\n      rampDown = Math.min(rampDown, scheduledReduces);\n      LOG.info(\"Ramping down \" + rampDown);\n      rampDownReduces(rampDown);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator.run": "      public void run() {\n        while (!stopped.get() && !Thread.currentThread().isInterrupted()) {\n          try {\n            Thread.sleep(rmPollInterval);\n            try {\n              heartbeat();\n            } catch (YarnRuntimeException e) {\n              LOG.error(\"Error communicating with RM: \" + e.getMessage() , e);\n              return;\n            } catch (Exception e) {\n              LOG.error(\"ERROR IN CONTACTING RM. \", e);\n              continue;\n              // TODO: for other exceptions\n            }\n\n            lastHeartbeatTime = context.getClock().getTime();\n            executeHeartbeatCallbacks();\n          } catch (InterruptedException e) {\n            if (!stopped.get()) {\n              LOG.warn(\"Allocated thread interrupted. Returning.\");\n            }\n            return;\n          }\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator.executeHeartbeatCallbacks": "  private void executeHeartbeatCallbacks() {\n    Runnable callback = null;\n    while ((callback = heartbeatCallbacks.poll()) != null) {\n      callback.run();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator.heartbeat": "  protected abstract void heartbeat() throws Exception;\n\n  private void executeHeartbeatCallbacks() {\n    Runnable callback = null;\n    while ((callback = heartbeatCallbacks.poll()) != null) {\n      callback.run();\n    }\n  }"
        },
        "bug_report": {
            "Title": "Job hangs because RMContainerAllocator$AssignedRequests.preemptReduce() violates the comparator contract",
            "Description": "We ran into a situation where tasks are not getting assigned because RMContainerAllocator$AssignedRequests.preemptReduce() fails repeatedly with the following exception:\n\n{code}\n2014-02-06 16:43:45,183 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.\njava.lang.IllegalArgumentException: Comparison method violates its general contract!\n     at java.util.TimSort.mergeLo(TimSort.java:747)\n     at java.util.TimSort.mergeAt(TimSort.java:483)\n     at java.util.TimSort.mergeCollapse(TimSort.java:408)\n     at java.util.TimSort.sort(TimSort.java:214)\n     at java.util.TimSort.sort(TimSort.java:173)\n     at java.util.Arrays.sort(Arrays.java:659)\n     at java.util.Collections.sort(Collections.java:217)\n     at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests.preemptReduce(RMContainerAllocator.java:1106)\n     at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.preemptReducesIfNeeded(RMContainerAllocator.java:416)\n     at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:230)\n     at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:252)\n     at java.lang.Thread.run(Thread.java:744)\n{code}\n\nIt is because the comparator that's defined in this method does not abide by the contract, specifically if p == 0.\n\nComparator.compare(): http://docs.oracle.com/javase/7/docs/api/java/util/Comparator.html#compare(T, T)"
        }
    },
    {
        "filename": "MAPREDUCE-3583.json",
        "creation_time": "2011-12-20T15:07:55.000+0000",
        "stack_trace": "```\njava.lang.NumberFormatException: For input string: \"18446743988060683582\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)\n\tat java.lang.Long.parseLong(Long.java:422)\n\tat java.lang.Long.parseLong(Long.java:468)\n\tat org.apache.hadoop.util.ProcfsBasedProcessTree.constructProcessInfo(ProcfsBasedProcessTree.java:413)\n\tat org.apache.hadoop.util.ProcfsBasedProcessTree.getProcessTree(ProcfsBasedProcessTree.java:148)\n\tat org.apache.hadoop.util.LinuxResourceCalculatorPlugin.getProcResourceValues(LinuxResourceCalculatorPlugin.java:401)\n\tat org.apache.hadoop.mapred.Task.initialize(Task.java:536)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:353)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree.constructProcessInfo": "  private static ProcessInfo constructProcessInfo(ProcessInfo pinfo, \n                                                    String procfsDir) {\n    ProcessInfo ret = null;\n    // Read \"procfsDir/<pid>/stat\" file - typically /proc/<pid>/stat\n    BufferedReader in = null;\n    FileReader fReader = null;\n    try {\n      File pidDir = new File(procfsDir, String.valueOf(pinfo.getPid()));\n      fReader = new FileReader(new File(pidDir, PROCFS_STAT_FILE));\n      in = new BufferedReader(fReader);\n    } catch (FileNotFoundException f) {\n      // The process vanished in the interim!\n      LOG.warn(\"The process \" + pinfo.getPid()\n          + \" may have finished in the interim.\");\n      return ret;\n    }\n\n    ret = pinfo;\n    try {\n      String str = in.readLine(); // only one line\n      Matcher m = PROCFS_STAT_FILE_FORMAT.matcher(str);\n      boolean mat = m.find();\n      if (mat) {\n        // Set (name) (ppid) (pgrpId) (session) (utime) (stime) (vsize) (rss)\n        pinfo.updateProcessInfo(m.group(2), Integer.parseInt(m.group(3)),\n                Integer.parseInt(m.group(4)), Integer.parseInt(m.group(5)),\n                Long.parseLong(m.group(7)), Long.parseLong(m.group(8)),\n                Long.parseLong(m.group(10)), Long.parseLong(m.group(11)));\n      } else {\n        LOG.warn(\"Unexpected: procfs stat file is not in the expected format\"\n            + \" for process with pid \" + pinfo.getPid());\n        ret = null;\n      }\n    } catch (IOException io) {\n      LOG.warn(\"Error reading the stream \" + io);\n      ret = null;\n    } finally {\n      // Close the streams\n      try {\n        fReader.close();\n        try {\n          in.close();\n        } catch (IOException i) {\n          LOG.warn(\"Error closing the stream \" + in);\n        }\n      } catch (IOException i) {\n        LOG.warn(\"Error closing the stream \" + fReader);\n      }\n    }\n\n    return ret;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree.updateProcessInfo": "    public void updateProcessInfo(String name, Integer ppid, Integer pgrpId,\n        Integer sessionId, Long utime, Long stime, Long vmem, Long rssmem) {\n      this.name = name;\n      this.ppid = ppid;\n      this.pgrpId = pgrpId;\n      this.sessionId = sessionId;\n      this.utime = utime;\n      this.stime = stime;\n      this.vmem = vmem;\n      this.rssmemPage = rssmem;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree.getPid": "    public Integer getPid() {\n      return pid;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree.getProcessTree": "  public ProcfsBasedProcessTree getProcessTree() {\n    if (pid != -1) {\n      // Get the list of processes\n      List<Integer> processList = getProcessList();\n\n      Map<Integer, ProcessInfo> allProcessInfo = new HashMap<Integer, ProcessInfo>();\n      \n      // cache the processTree to get the age for processes\n      Map<Integer, ProcessInfo> oldProcs = \n              new HashMap<Integer, ProcessInfo>(processTree);\n      processTree.clear();\n\n      ProcessInfo me = null;\n      for (Integer proc : processList) {\n        // Get information for each process\n        ProcessInfo pInfo = new ProcessInfo(proc);\n        if (constructProcessInfo(pInfo, procfsDir) != null) {\n          allProcessInfo.put(proc, pInfo);\n          if (proc.equals(this.pid)) {\n            me = pInfo; // cache 'me'\n            processTree.put(proc, pInfo);\n          }\n        }\n      }\n\n      if (me == null) {\n        return this; \n      }\n\n      // Add each process to its parent.\n      for (Map.Entry<Integer, ProcessInfo> entry : allProcessInfo.entrySet()) {\n        Integer pID = entry.getKey();\n        if (pID != 1) {\n          ProcessInfo pInfo = entry.getValue();\n          ProcessInfo parentPInfo = allProcessInfo.get(pInfo.getPpid());\n          if (parentPInfo != null) {\n            parentPInfo.addChild(pInfo);\n          }\n        }\n      }\n\n      // now start constructing the process-tree\n      LinkedList<ProcessInfo> pInfoQueue = new LinkedList<ProcessInfo>();\n      pInfoQueue.addAll(me.getChildren());\n      while (!pInfoQueue.isEmpty()) {\n        ProcessInfo pInfo = pInfoQueue.remove();\n        if (!processTree.containsKey(pInfo.getPid())) {\n          processTree.put(pInfo.getPid(), pInfo);\n        }\n        pInfoQueue.addAll(pInfo.getChildren());\n      }\n\n      // update age values and compute the number of jiffies since last update\n      for (Map.Entry<Integer, ProcessInfo> procs : processTree.entrySet()) {\n        ProcessInfo oldInfo = oldProcs.get(procs.getKey());\n        if (procs.getValue() != null) {\n          procs.getValue().updateJiffy(oldInfo);\n          if (oldInfo != null) {\n            procs.getValue().updateAge(oldInfo);  \n          }\n        }\n      }\n\n      if (LOG.isDebugEnabled()) {\n        // Log.debug the ProcfsBasedProcessTree\n        LOG.debug(this.toString());\n      }\n    }\n    return this;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree.updateJiffy": "    public void updateJiffy(ProcessInfo oldInfo) {\n      this.dtime = (oldInfo == null ? this.utime + this.stime\n              : (this.utime + this.stime) - (oldInfo.utime + oldInfo.stime));\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree.getProcessList": "  private List<Integer> getProcessList() {\n    String[] processDirs = (new File(procfsDir)).list();\n    List<Integer> processList = new ArrayList<Integer>();\n\n    for (String dir : processDirs) {\n      try {\n        int pd = Integer.parseInt(dir);\n        if ((new File(procfsDir, dir)).isDirectory()) {\n          processList.add(Integer.valueOf(pd));\n        }\n      } catch (NumberFormatException n) {\n        // skip this directory\n      } catch (SecurityException s) {\n        // skip this process\n      }\n    }\n    return processList;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree.addChild": "    public boolean addChild(ProcessInfo p) {\n      return children.add(p);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree.getPpid": "    public Integer getPpid() {\n      return ppid;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree.toString": "  public String toString() {\n    StringBuffer pTree = new StringBuffer(\"[ \");\n    for (Integer p : processTree.keySet()) {\n      pTree.append(p);\n      pTree.append(\" \");\n    }\n    return pTree.substring(0, pTree.length()) + \"]\";\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree.getChildren": "    public List<ProcessInfo> getChildren() {\n      return children;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree.updateAge": "    public void updateAge(ProcessInfo oldInfo) {\n      this.age = oldInfo.age + 1;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.util.LinuxResourceCalculatorPlugin.getProcResourceValues": "  public ProcResourceValues getProcResourceValues() {\n    pTree = pTree.getProcessTree();\n    long cpuTime = pTree.getCumulativeCpuTime();\n    long pMem = pTree.getCumulativeRssmem();\n    long vMem = pTree.getCumulativeVmem();\n    return new ProcResourceValues(cpuTime, pMem, vMem);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.util.LinuxResourceCalculatorPlugin.getCumulativeCpuTime": "  public long getCumulativeCpuTime() {\n    readProcStatFile();\n    return cumulativeCpuTime;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.run": "      public void run() {\n        spillLock.lock();\n        spillThreadRunning = true;\n        try {\n          while (true) {\n            spillDone.signal();\n            while (!spillInProgress) {\n              spillReady.await();\n            }\n            try {\n              spillLock.unlock();\n              sortAndSpill();\n            } catch (Throwable t) {\n              sortSpillException = t;\n            } finally {\n              spillLock.lock();\n              if (bufend < bufstart) {\n                bufvoid = kvbuffer.length;\n              }\n              kvstart = kvend;\n              bufstart = bufend;\n              spillInProgress = false;\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        } finally {\n          spillLock.unlock();\n          spillThreadRunning = false;\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.isMapTask": "  public boolean isMapTask() {\n    return true;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.initialize": "    public void initialize(org.apache.hadoop.mapreduce.InputSplit split,\n                           org.apache.hadoop.mapreduce.TaskAttemptContext context\n                           ) throws IOException, InterruptedException {\n      long bytesInPrev = getInputBytes(fsStats);\n      real.initialize(split, context);\n      long bytesInCurr = getInputBytes(fsStats);\n      fileInputByteCounter.increment(bytesInCurr - bytesInPrev);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.runNewMapper": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runNewMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, ClassNotFoundException,\n                             InterruptedException {\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, \n                                                                  getTaskID(),\n                                                                  reporter);\n    // make a mapper\n    org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE> mapper =\n      (org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>)\n        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);\n    // make the input format\n    org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE> inputFormat =\n      (org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE>)\n        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);\n    // rebuild the input split\n    org.apache.hadoop.mapreduce.InputSplit split = null;\n    split = getSplitDetails(new Path(splitIndex.getSplitLocation()),\n        splitIndex.getStartOffset());\n\n    org.apache.hadoop.mapreduce.RecordReader<INKEY,INVALUE> input =\n      new NewTrackingRecordReader<INKEY,INVALUE>\n        (split, inputFormat, reporter, taskContext);\n    \n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.RecordWriter output = null;\n    \n    // get an output object\n    if (job.getNumReduceTasks() == 0) {\n      output = \n        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);\n    } else {\n      output = new NewOutputCollector(taskContext, job, umbilical, reporter);\n    }\n\n    org.apache.hadoop.mapreduce.MapContext<INKEY, INVALUE, OUTKEY, OUTVALUE> \n    mapContext = \n      new MapContextImpl<INKEY, INVALUE, OUTKEY, OUTVALUE>(job, getTaskID(), \n          input, output, \n          committer, \n          reporter, split);\n\n    org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context \n        mapperContext = \n          new WrappedMapper<INKEY, INVALUE, OUTKEY, OUTVALUE>().getMapContext(\n              mapContext);\n\n    input.initialize(split, mapperContext);\n    mapper.run(mapperContext);\n    mapPhase.complete();\n    setPhase(TaskStatus.Phase.SORT);\n    statusUpdate(umbilical);\n    input.close();\n    output.close(mapperContext);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getProgress": "      public Progress getProgress() {\n        return null;\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.runOldMapper": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runOldMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, InterruptedException,\n                             ClassNotFoundException {\n    InputSplit inputSplit = getSplitDetails(new Path(splitIndex.getSplitLocation()),\n           splitIndex.getStartOffset());\n\n    updateJobWithSplit(job, inputSplit);\n    reporter.setInputSplit(inputSplit);\n\n    RecordReader<INKEY,INVALUE> in = isSkipping() ? \n        new SkippingRecordReader<INKEY,INVALUE>(umbilical, reporter, job) :\n          new TrackedRecordReader<INKEY,INVALUE>(reporter, job);\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n\n\n    int numReduceTasks = conf.getNumReduceTasks();\n    LOG.info(\"numReduceTasks: \" + numReduceTasks);\n    MapOutputCollector collector = null;\n    if (numReduceTasks > 0) {\n      collector = new MapOutputBuffer(umbilical, job, reporter);\n    } else { \n      collector = new DirectMapOutputCollector(umbilical, job, reporter);\n    }\n    MapRunnable<INKEY,INVALUE,OUTKEY,OUTVALUE> runner =\n      ReflectionUtils.newInstance(job.getMapRunnerClass(), job);\n\n    try {\n      runner.run(in, new OldOutputCollector(collector, conf), reporter);\n      mapPhase.complete();\n      // start the sort phase only if there are reducers\n      if (numReduceTasks > 0) {\n        setPhase(TaskStatus.Phase.SORT);\n      }\n      statusUpdate(umbilical);\n      collector.flush();\n    } finally {\n      //close\n      in.close();                               // close input\n      collector.close();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.sortAndSpill": "    private void sortAndSpill() throws IOException, ClassNotFoundException,\n                                       InterruptedException {\n      //approximate the length of the output file to be the length of the\n      //buffer + header lengths for the partitions\n      final long size = (bufend >= bufstart\n          ? bufend - bufstart\n          : (bufvoid - bufend) + bufstart) +\n                  partitions * APPROX_HEADER_LENGTH;\n      FSDataOutputStream out = null;\n      try {\n        // create spill file\n        final SpillRecord spillRec = new SpillRecord(partitions);\n        final Path filename =\n            mapOutputFile.getSpillFileForWrite(numSpills, size);\n        out = rfs.create(filename);\n\n        final int mstart = kvend / NMETA;\n        final int mend = 1 + // kvend is a valid record\n          (kvstart >= kvend\n          ? kvstart\n          : kvmeta.capacity() + kvstart) / NMETA;\n        sorter.sort(MapOutputBuffer.this, mstart, mend, reporter);\n        int spindex = mstart;\n        final IndexRecord rec = new IndexRecord();\n        final InMemValBytes value = new InMemValBytes();\n        for (int i = 0; i < partitions; ++i) {\n          IFile.Writer<K, V> writer = null;\n          try {\n            long segmentStart = out.getPos();\n            writer = new Writer<K, V>(job, out, keyClass, valClass, codec,\n                                      spilledRecordsCounter);\n            if (combinerRunner == null) {\n              // spill directly\n              DataInputBuffer key = new DataInputBuffer();\n              while (spindex < mend &&\n                  kvmeta.get(offsetFor(spindex % maxRec) + PARTITION) == i) {\n                final int kvoff = offsetFor(spindex % maxRec);\n                key.reset(kvbuffer, kvmeta.get(kvoff + KEYSTART),\n                          (kvmeta.get(kvoff + VALSTART) -\n                           kvmeta.get(kvoff + KEYSTART)));\n                getVBytesForOffset(kvoff, value);\n                writer.append(key, value);\n                ++spindex;\n              }\n            } else {\n              int spstart = spindex;\n              while (spindex < mend &&\n                  kvmeta.get(offsetFor(spindex % maxRec)\n                            + PARTITION) == i) {\n                ++spindex;\n              }\n              // Note: we would like to avoid the combiner if we've fewer\n              // than some threshold of records for a partition\n              if (spstart != spindex) {\n                combineCollector.setWriter(writer);\n                RawKeyValueIterator kvIter =\n                  new MRResultIterator(spstart, spindex);\n                combinerRunner.combine(kvIter, combineCollector);\n              }\n            }\n\n            // close the writer\n            writer.close();\n\n            // record offsets\n            rec.startOffset = segmentStart;\n            rec.rawLength = writer.getRawLength();\n            rec.partLength = writer.getCompressedLength();\n            spillRec.putIndex(rec, i);\n\n            writer = null;\n          } finally {\n            if (null != writer) writer.close();\n          }\n        }\n\n        if (totalIndexCacheMemory >= indexCacheMemoryLimit) {\n          // create spill index file\n          Path indexFilename =\n              mapOutputFile.getSpillIndexFileForWrite(numSpills, partitions\n                  * MAP_OUTPUT_INDEX_RECORD_LENGTH);\n          spillRec.writeToFile(indexFilename, job);\n        } else {\n          indexCacheList.add(spillRec);\n          totalIndexCacheMemory +=\n            spillRec.size() * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n        }\n        LOG.info(\"Finished spill \" + numSpills);\n        ++numSpills;\n      } finally {\n        if (out != null) out.close();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      LOG.error(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(pae,\"Unknown exception in doAs\");\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }"
        },
        "bug_report": {
            "Title": "ProcfsBasedProcessTree#constructProcessInfo() may throw NumberFormatException",
            "Description": "HBase PreCommit builds frequently gave us NumberFormatException.\n\nFrom https://builds.apache.org/job/PreCommit-HBASE-Build/553//testReport/org.apache.hadoop.hbase.mapreduce/TestHFileOutputFormat/testMRIncrementalLoad/:\n{code}\n2011-12-20 01:44:01,180 WARN  [main] mapred.JobClient(784): No job jar file set.  User classes may not be found. See JobConf(Class) or JobConf#setJar(String).\njava.lang.NumberFormatException: For input string: \"18446743988060683582\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)\n\tat java.lang.Long.parseLong(Long.java:422)\n\tat java.lang.Long.parseLong(Long.java:468)\n\tat org.apache.hadoop.util.ProcfsBasedProcessTree.constructProcessInfo(ProcfsBasedProcessTree.java:413)\n\tat org.apache.hadoop.util.ProcfsBasedProcessTree.getProcessTree(ProcfsBasedProcessTree.java:148)\n\tat org.apache.hadoop.util.LinuxResourceCalculatorPlugin.getProcResourceValues(LinuxResourceCalculatorPlugin.java:401)\n\tat org.apache.hadoop.mapred.Task.initialize(Task.java:536)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:353)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\n{code}\nFrom hadoop 0.20.205 source code, looks like ppid was 18446743988060683582, causing NFE:\n{code}\n        // Set (name) (ppid) (pgrpId) (session) (utime) (stime) (vsize) (rss)\n         pinfo.updateProcessInfo(m.group(2), Integer.parseInt(m.group(3)),\n{code}\nYou can find information on the OS at the beginning of https://builds.apache.org/job/PreCommit-HBASE-Build/553/console:\n{code}\nasf011.sp2.ygridcore.net\nLinux asf011.sp2.ygridcore.net 2.6.32-33-server #71-Ubuntu SMP Wed Jul 20 17:42:25 UTC 2011 x86_64 GNU/Linux\ncore file size          (blocks, -c) 0\ndata seg size           (kbytes, -d) unlimited\nscheduling priority             (-e) 20\nfile size               (blocks, -f) unlimited\npending signals                 (-i) 16382\nmax locked memory       (kbytes, -l) 64\nmax memory size         (kbytes, -m) unlimited\nopen files                      (-n) 60000\npipe size            (512 bytes, -p) 8\nPOSIX message queues     (bytes, -q) 819200\nreal-time priority              (-r) 0\nstack size              (kbytes, -s) 8192\ncpu time               (seconds, -t) unlimited\nmax user processes              (-u) 2048\nvirtual memory          (kbytes, -v) unlimited\nfile locks                      (-x) unlimited\n60000\nRunning in Jenkins mode\n{code}\n\nFrom Nicolas Sze:\n{noformat}\nIt looks like that the ppid is a 64-bit positive integer but Java long is signed and so only works with 63-bit positive integers.  In your case,\n\n  2^64 > 18446743988060683582 > 2^63.\n\nTherefore, there is a NFE. \n{noformat}\n\nI propose changing allProcessInfo to Map<String, ProcessInfo> so that we don't encounter this problem by avoiding parsing large integer."
        }
    },
    {
        "filename": "MAPREDUCE-2238.json",
        "creation_time": "2011-01-03T22:57:22.000+0000",
        "stack_trace": "```\nhudson.util.IOException2: remote file operation failed: /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk at hudson.remoting.Channel@2545938c:hadoop7\n\tat hudson.FilePath.act(FilePath.java:749)\n\tat hudson.FilePath.act(FilePath.java:735)\n\tat hudson.scm.SubversionSCM.checkout(SubversionSCM.java:589)\n\tat hudson.scm.SubversionSCM.checkout(SubversionSCM.java:537)\n\tat hudson.model.AbstractProject.checkout(AbstractProject.java:1116)\n\tat hudson.model.AbstractBuild$AbstractRunner.checkout(AbstractBuild$AbstractRunner.java:479)\n\tat hudson.model.AbstractBuild$AbstractRunner.run(AbstractBuild$AbstractRunner.java:411)\n\tat hudson.model.Run.run(Run.java:1324)\n\tat hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:46)\n\tat hudson.model.ResourceController.execute(ResourceController.java:88)\n\tat hudson.model.Executor.run(Executor.java:139)\nCaused by: java.io.IOException: Unable to delete /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk/trunk/build/test/logs/userlogs/job_20101230131139886_0001/attempt_20101230131139886_0001_m_000000_0\n```",
        "source_code": {},
        "bug_report": {
            "Title": "Undeletable build directories ",
            "Description": "The MR hudson job is failing, looks like it's due to a test chmod'ing a build directory so the checkout can't clean the build dir.\n\nhttps://hudson.apache.org/hudson/job/Hadoop-Mapreduce-trunk/549/console\n\nBuilding remotely on hadoop7\nhudson.util.IOException2: remote file operation failed: /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk at hudson.remoting.Channel@2545938c:hadoop7\n\tat hudson.FilePath.act(FilePath.java:749)\n\tat hudson.FilePath.act(FilePath.java:735)\n\tat hudson.scm.SubversionSCM.checkout(SubversionSCM.java:589)\n\tat hudson.scm.SubversionSCM.checkout(SubversionSCM.java:537)\n\tat hudson.model.AbstractProject.checkout(AbstractProject.java:1116)\n\tat hudson.model.AbstractBuild$AbstractRunner.checkout(AbstractBuild.java:479)\n\tat hudson.model.AbstractBuild$AbstractRunner.run(AbstractBuild.java:411)\n\tat hudson.model.Run.run(Run.java:1324)\n\tat hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:46)\n\tat hudson.model.ResourceController.execute(ResourceController.java:88)\n\tat hudson.model.Executor.run(Executor.java:139)\nCaused by: java.io.IOException: Unable to delete /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk/trunk/build/test/logs/userlogs/job_20101230131139886_0001/attempt_20101230131139886_0001_m_000000_0"
        }
    },
    {
        "filename": "MAPREDUCE-6410.json",
        "creation_time": "2015-06-04T06:34:33.000+0000",
        "stack_trace": "```\njava.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)];\n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1414)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1363)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n        at com.sun.proxy.$Proxy9.getListing(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getListing(ClientNamenodeProtocolTranslatorPB.java:519)\n        at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy10.getListing(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1767)\n        at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1750)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:691)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:102)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:753)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:749)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:749)\n        at org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService$LogDeletionTask.run(AggregatedLogDeletionService.java:68)\n        at java.util.TimerThread.mainLoop(Timer.java:555)\n        at java.util.TimerThread.run(Timer.java:505)\nCaused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:677)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1641)\n        at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:640)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:724)\n        at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1381)\n        ... 21 more\nCaused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)\n        at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:411)\n        at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:550)\n        at org.apache.hadoop.ipc.Client$Connection.access$1800(Client.java:367)\n        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:716)\n        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:712)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1641)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:711)\n        ... 24 more\nCaused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)\n        at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)\n        at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121)\n        at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)\n        at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.wrapException": "  public static IOException wrapException(final String destHost,\n                                          final int destPort,\n                                          final String localHost,\n                                          final int localPort,\n                                          final IOException exception) {\n    if (exception instanceof BindException) {\n      return wrapWithMessage(exception,\n          \"Problem binding to [\"\n              + localHost\n              + \":\"\n              + localPort\n              + \"] \"\n              + exception\n              + \";\"\n              + see(\"BindException\"));\n    } else if (exception instanceof ConnectException) {\n      // connection refused; include the host:port in the error\n      return wrapWithMessage(exception, \n          \"Call From \"\n              + localHost\n              + \" to \"\n              + destHost\n              + \":\"\n              + destPort\n              + \" failed on connection exception: \"\n              + exception\n              + \";\"\n              + see(\"ConnectionRefused\"));\n    } else if (exception instanceof UnknownHostException) {\n      return wrapWithMessage(exception,\n          \"Invalid host name: \"\n              + getHostDetailsAsString(destHost, destPort, localHost)\n              + exception\n              + \";\"\n              + see(\"UnknownHost\"));\n    } else if (exception instanceof SocketTimeoutException) {\n      return wrapWithMessage(exception,\n          \"Call From \"\n              + localHost + \" to \" + destHost + \":\" + destPort\n              + \" failed on socket timeout exception: \" + exception\n              + \";\"\n              + see(\"SocketTimeout\"));\n    } else if (exception instanceof NoRouteToHostException) {\n      return wrapWithMessage(exception,\n          \"No Route to Host from  \"\n              + localHost + \" to \" + destHost + \":\" + destPort\n              + \" failed on socket timeout exception: \" + exception\n              + \";\"\n              + see(\"NoRouteToHost\"));\n    } else if (exception instanceof EOFException) {\n      return wrapWithMessage(exception,\n          \"End of File Exception between \"\n              + getHostDetailsAsString(destHost,  destPort, localHost)\n              + \": \" + exception\n              + \";\"\n              + see(\"EOFException\"));\n    }\n    else {\n      return (IOException) new IOException(\"Failed on local exception: \"\n                                               + exception\n                                               + \"; Host Details : \"\n                                               + getHostDetailsAsString(destHost, destPort, localHost))\n          .initCause(exception);\n\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.getHostDetailsAsString": "  private static String getHostDetailsAsString(final String destHost,\n                                               final int destPort,\n                                               final String localHost) {\n    StringBuilder hostDetails = new StringBuilder(27);\n    hostDetails.append(\"local host is: \")\n        .append(quoteHost(localHost))\n        .append(\"; \");\n    hostDetails.append(\"destination host is: \").append(quoteHost(destHost))\n        .append(\":\")\n        .append(destPort).append(\"; \");\n    return hostDetails.toString();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.wrapWithMessage": "  private static <T extends IOException> T wrapWithMessage(\n      T exception, String msg) {\n    Class<? extends Throwable> clazz = exception.getClass();\n    try {\n      Constructor<? extends Throwable> ctor = clazz.getConstructor(String.class);\n      Throwable t = ctor.newInstance(msg);\n      return (T)(t.initCause(exception));\n    } catch (Throwable e) {\n      LOG.warn(\"Unable to wrap exception of type \" +\n          clazz + \": it has no (String) constructor\", e);\n      return exception;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.see": "  private static String see(final String entry) {\n    return FOR_MORE_DETAILS_SEE + HADOOP_WIKI + entry;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.call": "          public Connection call() throws Exception {\n            return new Connection(remoteId, serviceClass);\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnectionId": "    static ConnectionId getConnectionId(InetSocketAddress addr,\n        Class<?> protocol, UserGroupInformation ticket, int rpcTimeout,\n        RetryPolicy connectionRetryPolicy, Configuration conf) throws IOException {\n\n      if (connectionRetryPolicy == null) {\n        final int max = conf.getInt(\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY,\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_DEFAULT);\n        final int retryInterval = conf.getInt(\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_RETRY_INTERVAL_KEY,\n            CommonConfigurationKeysPublic\n                .IPC_CLIENT_CONNECT_RETRY_INTERVAL_DEFAULT);\n\n        connectionRetryPolicy = RetryPolicies.retryUpToMaximumCountWithFixedSleep(\n            max, retryInterval, TimeUnit.MILLISECONDS);\n      }\n\n      return new ConnectionId(addr, protocol, ticket, rpcTimeout,\n          connectionRetryPolicy, conf);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRpcResponse": "    public synchronized Writable getRpcResponse() {\n      return rpcResponse;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRemoteAddress": "    public InetSocketAddress getRemoteAddress() {\n      return server;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.sendRpcRequest": "    public void sendRpcRequest(final Call call)\n        throws InterruptedException, IOException {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n\n      // Serialize the call to be sent. This is done from the actual\n      // caller thread, rather than the sendParamsExecutor thread,\n      \n      // so that if the serialization throws an error, it is reported\n      // properly. This also parallelizes the serialization.\n      //\n      // Format of a call on the wire:\n      // 0) Length of rest below (1 + 2)\n      // 1) RpcRequestHeader  - is serialized Delimited hence contains length\n      // 2) RpcRequest\n      //\n      // Items '1' and '2' are prepared here. \n      final DataOutputBuffer d = new DataOutputBuffer();\n      RpcRequestHeaderProto header = ProtoUtil.makeRpcRequestHeader(\n          call.rpcKind, OperationProto.RPC_FINAL_PACKET, call.id, call.retry,\n          clientId);\n      header.writeDelimitedTo(d);\n      call.rpcRequest.write(d);\n\n      synchronized (sendRpcRequestLock) {\n        Future<?> senderFuture = sendParamsExecutor.submit(new Runnable() {\n          @Override\n          public void run() {\n            try {\n              synchronized (Connection.this.out) {\n                if (shouldCloseConnection.get()) {\n                  return;\n                }\n                \n                if (LOG.isDebugEnabled())\n                  LOG.debug(getName() + \" sending #\" + call.id);\n         \n                byte[] data = d.getData();\n                int totalLength = d.getLength();\n                out.writeInt(totalLength); // Total Length\n                out.write(data, 0, totalLength);// RpcRequestHeader + RpcRequest\n                out.flush();\n              }\n            } catch (IOException e) {\n              // exception at this point would leave the connection in an\n              // unrecoverable state (eg half a call left on the wire).\n              // So, close the connection, killing any outstanding calls\n              markClosed(e);\n            } finally {\n              //the buffer is just an in-memory buffer, but it is still polite to\n              // close early\n              IOUtils.closeStream(d);\n            }\n          }\n        });\n      \n        try {\n          senderFuture.get();\n        } catch (ExecutionException e) {\n          Throwable cause = e.getCause();\n          \n          // cause should only be a RuntimeException as the Runnable above\n          // catches IOException\n          if (cause instanceof RuntimeException) {\n            throw (RuntimeException) cause;\n          } else {\n            throw new RuntimeException(\"unexpected checked exception\", cause);\n          }\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.createCall": "  Call createCall(RPC.RpcKind rpcKind, Writable rpcRequest) {\n    return new Call(rpcKind, rpcRequest);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnection": "  private Connection getConnection(\n      final ConnectionId remoteId,\n      Call call, final int serviceClass, AtomicBoolean fallbackToSimpleAuth)\n      throws IOException {\n    if (!running.get()) {\n      // the client is stopped\n      throw new IOException(\"The client is stopped\");\n    }\n    Connection connection;\n    /* we could avoid this allocation for each RPC by having a  \n     * connectionsId object and with set() method. We need to manage the\n     * refs for keys in HashMap properly. For now its ok.\n     */\n    while(true) {\n      try {\n        connection = connections.get(remoteId, new Callable<Connection>() {\n          @Override\n          public Connection call() throws Exception {\n            return new Connection(remoteId, serviceClass);\n          }\n        });\n      } catch (ExecutionException e) {\n        throw new IOException(e);\n      }\n      if (connection.addCall(call)) {\n        break;\n      } else {\n        connections.invalidate(remoteId);\n      }\n    }\n    \n    //we don't invoke the method below inside \"synchronized (connections)\"\n    //block above. The reason for that is if the server happens to be slow,\n    //it will take longer to establish a connection and that will slow the\n    //entire system down.\n    connection.setupIOstreams(fallbackToSimpleAuth);\n    return connection;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.invoke": "    public Object invoke(Object proxy, Method method, Object[] args)\n        throws ServiceException {\n      long startTime = 0;\n      if (LOG.isDebugEnabled()) {\n        startTime = Time.now();\n      }\n      \n      if (args.length != 2) { // RpcController + Message\n        throw new ServiceException(\"Too many parameters for request. Method: [\"\n            + method.getName() + \"]\" + \", Expected: 2, Actual: \"\n            + args.length);\n      }\n      if (args[1] == null) {\n        throw new ServiceException(\"null param while calling Method: [\"\n            + method.getName() + \"]\");\n      }\n\n      TraceScope traceScope = null;\n      // if Tracing is on then start a new span for this rpc.\n      // guard it in the if statement to make sure there isn't\n      // any extra string manipulation.\n      if (Trace.isTracing()) {\n        traceScope = Trace.startSpan(RpcClientUtil.methodToTraceString(method));\n      }\n\n      RequestHeaderProto rpcRequestHeader = constructRpcRequestHeader(method);\n      \n      if (LOG.isTraceEnabled()) {\n        LOG.trace(Thread.currentThread().getId() + \": Call -> \" +\n            remoteId + \": \" + method.getName() +\n            \" {\" + TextFormat.shortDebugString((Message) args[1]) + \"}\");\n      }\n\n\n      Message theRequest = (Message) args[1];\n      final RpcResponseWrapper val;\n      try {\n        val = (RpcResponseWrapper) client.call(RPC.RpcKind.RPC_PROTOCOL_BUFFER,\n            new RpcRequestWrapper(rpcRequestHeader, theRequest), remoteId,\n            fallbackToSimpleAuth);\n\n      } catch (Throwable e) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Exception <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + e + \"}\");\n        }\n        if (Trace.isTracing()) {\n          traceScope.getSpan().addTimelineAnnotation(\n              \"Call got exception: \" + e.getMessage());\n        }\n        throw new ServiceException(e);\n      } finally {\n        if (traceScope != null) traceScope.close();\n      }\n\n      if (LOG.isDebugEnabled()) {\n        long callTime = Time.now() - startTime;\n        LOG.debug(\"Call: \" + method.getName() + \" took \" + callTime + \"ms\");\n      }\n      \n      Message prototype = null;\n      try {\n        prototype = getReturnProtoType(method);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      Message returnMessage;\n      try {\n        returnMessage = prototype.newBuilderForType()\n            .mergeFrom(val.theResponseRead).build();\n\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Response <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + TextFormat.shortDebugString(returnMessage) + \"}\");\n        }\n\n      } catch (Throwable e) {\n        throw new ServiceException(e);\n      }\n      return returnMessage;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.close": "    public void close() throws IOException {\n      if (!isClosed) {\n        isClosed = true;\n        CLIENTS.stopClient(client);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcRequestWrapper request = (RpcRequestWrapper) writableRequest;\n        RequestHeaderProto rpcRequest = request.requestHeader;\n        String methodName = rpcRequest.getMethodName();\n        \n        \n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = prototype.newBuilderForType()\n            .mergeFrom(request.theRequestRead).build();\n        \n        Message result;\n        long startTime = Time.now();\n        int qTime = (int) (startTime - receiveTime);\n        Exception exception = null;\n        try {\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n        } catch (ServiceException e) {\n          exception = (Exception) e.getCause();\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          exception = e;\n          throw e;\n        } finally {\n          int processingTime = (int) (Time.now() - startTime);\n          if (LOG.isDebugEnabled()) {\n            String msg = \"Served: \" + methodName + \" queueTime= \" + qTime +\n                \" procesingTime= \" + processingTime;\n            if (exception != null) {\n              msg += \" exception= \" + exception.getClass().getSimpleName();\n            }\n            LOG.debug(msg);\n          }\n          String detailedMetricsName = (exception == null) ?\n              methodName :\n              exception.getClass().getSimpleName();\n          server.rpcMetrics.addRpcQueueTime(qTime);\n          server.rpcMetrics.addRpcProcessingTime(processingTime);\n          server.rpcDetailedMetrics.addProcessingTime(detailedMetricsName,\n              processingTime);\n        }\n        return new RpcResponseWrapper(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getReturnProtoType": "    private Message getReturnProtoType(Method method) throws Exception {\n      if (returnTypes.containsKey(method.getName())) {\n        return returnTypes.get(method.getName());\n      }\n      \n      Class<?> returnType = method.getReturnType();\n      Method newInstMethod = returnType.getMethod(\"getDefaultInstance\");\n      newInstMethod.setAccessible(true);\n      Message prototype = (Message) newInstMethod.invoke(null, (Object[]) null);\n      returnTypes.put(method.getName(), prototype);\n      return prototype;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.constructRpcRequestHeader": "    private RequestHeaderProto constructRpcRequestHeader(Method method) {\n      RequestHeaderProto.Builder builder = RequestHeaderProto\n          .newBuilder();\n      builder.setMethodName(method.getName());\n     \n\n      // For protobuf, {@code protocol} used when creating client side proxy is\n      // the interface extending BlockingInterface, which has the annotations \n      // such as ProtocolName etc.\n      //\n      // Using Method.getDeclaringClass(), as in WritableEngine to get at\n      // the protocol interface will return BlockingInterface, from where \n      // the annotation ProtocolName and Version cannot be\n      // obtained.\n      //\n      // Hence we simply use the protocol class used to create the proxy.\n      // For PB this may limit the use of mixins on client side.\n      builder.setDeclaringClassProtocolName(protocolName);\n      builder.setClientProtocolVersion(clientProtocolVersion);\n      return builder.build();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod": "  protected Object invokeMethod(Method method, Object[] args) throws Throwable {\n    try {\n      if (!method.isAccessible()) {\n        method.setAccessible(true);\n      }\n      return method.invoke(currentProxy.proxy, args);\n    } catch (InvocationTargetException e) {\n      throw e.getCause();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invoke": "  public Object invoke(Object proxy, Method method, Object[] args)\n    throws Throwable {\n    RetryPolicy policy = methodNameToPolicyMap.get(method.getName());\n    if (policy == null) {\n      policy = defaultPolicy;\n    }\n    \n    // The number of times this method invocation has been failed over.\n    int invocationFailoverCount = 0;\n    final boolean isRpc = isRpcInvocation(currentProxy.proxy);\n    final int callId = isRpc? Client.nextCallId(): RpcConstants.INVALID_CALL_ID;\n    int retries = 0;\n    while (true) {\n      // The number of times this invocation handler has ever been failed over,\n      // before this method invocation attempt. Used to prevent concurrent\n      // failed method invocations from triggering multiple failover attempts.\n      long invocationAttemptFailoverCount;\n      synchronized (proxyProvider) {\n        invocationAttemptFailoverCount = proxyProviderFailoverCount;\n      }\n\n      if (isRpc) {\n        Client.setCallIdAndRetryCount(callId, retries);\n      }\n      try {\n        Object ret = invokeMethod(method, args);\n        hasMadeASuccessfulCall = true;\n        return ret;\n      } catch (Exception e) {\n        boolean isIdempotentOrAtMostOnce = proxyProvider.getInterface()\n            .getMethod(method.getName(), method.getParameterTypes())\n            .isAnnotationPresent(Idempotent.class);\n        if (!isIdempotentOrAtMostOnce) {\n          isIdempotentOrAtMostOnce = proxyProvider.getInterface()\n              .getMethod(method.getName(), method.getParameterTypes())\n              .isAnnotationPresent(AtMostOnce.class);\n        }\n        RetryAction action = policy.shouldRetry(e, retries++,\n            invocationFailoverCount, isIdempotentOrAtMostOnce);\n        if (action.action == RetryAction.RetryDecision.FAIL) {\n          if (action.reason != null) {\n            LOG.warn(\"Exception while invoking \" + currentProxy.proxy.getClass()\n                + \".\" + method.getName() + \" over \" + currentProxy.proxyInfo\n                + \". Not retrying because \" + action.reason, e);\n          }\n          throw e;\n        } else { // retry or failover\n          // avoid logging the failover if this is the first call on this\n          // proxy object, and we successfully achieve the failover without\n          // any flip-flopping\n          boolean worthLogging = \n            !(invocationFailoverCount == 0 && !hasMadeASuccessfulCall);\n          worthLogging |= LOG.isDebugEnabled();\n          if (action.action == RetryAction.RetryDecision.FAILOVER_AND_RETRY &&\n              worthLogging) {\n            String msg = \"Exception while invoking \" + method.getName()\n                + \" of class \" + currentProxy.proxy.getClass().getSimpleName()\n                + \" over \" + currentProxy.proxyInfo;\n\n            if (invocationFailoverCount > 0) {\n              msg += \" after \" + invocationFailoverCount + \" fail over attempts\"; \n            }\n            msg += \". Trying to fail over \" + formatSleepMessage(action.delayMillis);\n            LOG.info(msg, e);\n          } else {\n            if(LOG.isDebugEnabled()) {\n              LOG.debug(\"Exception while invoking \" + method.getName()\n                  + \" of class \" + currentProxy.proxy.getClass().getSimpleName()\n                  + \" over \" + currentProxy.proxyInfo + \". Retrying \"\n                  + formatSleepMessage(action.delayMillis), e);\n            }\n          }\n          \n          if (action.delayMillis > 0) {\n            Thread.sleep(action.delayMillis);\n          }\n          \n          if (action.action == RetryAction.RetryDecision.FAILOVER_AND_RETRY) {\n            // Make sure that concurrent failed method invocations only cause a\n            // single actual fail over.\n            synchronized (proxyProvider) {\n              if (invocationAttemptFailoverCount == proxyProviderFailoverCount) {\n                proxyProvider.performFailover(currentProxy.proxy);\n                proxyProviderFailoverCount++;\n              } else {\n                LOG.warn(\"A failover has occurred since the start of this method\"\n                    + \" invocation attempt.\");\n              }\n              currentProxy = proxyProvider.getProxy();\n            }\n            invocationFailoverCount++;\n          }\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystemLinkResolver.resolve": "  public T resolve(final FileSystem filesys, final Path path)\n      throws IOException {\n    int count = 0;\n    T in = null;\n    Path p = path;\n    // Assumes path belongs to this FileSystem.\n    // Callers validate this by passing paths through FileSystem#checkPath\n    FileSystem fs = filesys;\n    for (boolean isLink = true; isLink;) {\n      try {\n        in = doCall(p);\n        isLink = false;\n      } catch (UnresolvedLinkException e) {\n        if (!filesys.resolveSymlinks) {\n          throw new IOException(\"Path \" + path + \" contains a symlink\"\n              + \" and symlink resolution is disabled (\"\n              + CommonConfigurationKeys.FS_CLIENT_RESOLVE_REMOTE_SYMLINKS_KEY\n              + \").\", e);\n        }\n        if (!FileSystem.areSymlinksEnabled()) {\n          throw new IOException(\"Symlink resolution is disabled in\" +\n              \" this version of Hadoop.\");\n        }\n        if (count++ > FsConstants.MAX_PATH_LINKS) {\n          throw new IOException(\"Possible cyclic loop while \" +\n                                \"following symbolic link \" + path);\n        }\n        // Resolve the first unresolved path component\n        p = FSLinkResolver.qualifySymlinkTarget(fs.getUri(), p,\n            filesys.resolveLink(p));\n        fs = FileSystem.getFSofPath(p, filesys.getConf());\n        // Have to call next if it's a new FS\n        if (!fs.equals(filesys)) {\n          return next(fs, p);\n        }\n        // Else, we keep resolving with this filesystem\n      }\n    }\n    // Successful call, path was fully resolved\n    return in;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystemLinkResolver.next": "  abstract public T next(final FileSystem fs, final Path p) throws IOException;\n\n  /**\n   * Attempt calling overridden {@link #doCall(Path)} method with",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystemLinkResolver.doCall": "  abstract public T doCall(final Path p) throws IOException,\n      UnresolvedLinkException;\n\n  /**\n   * Calls the abstract FileSystem call equivalent to the specialized subclass\n   * implementation in {@link #doCall(Path)}. This is used when retrying the",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.run": "          public void run() {\n            try {\n              synchronized (Connection.this.out) {\n                if (shouldCloseConnection.get()) {\n                  return;\n                }\n                \n                if (LOG.isDebugEnabled())\n                  LOG.debug(getName() + \" sending #\" + call.id);\n         \n                byte[] data = d.getData();\n                int totalLength = d.getLength();\n                out.writeInt(totalLength); // Total Length\n                out.write(data, 0, totalLength);// RpcRequestHeader + RpcRequest\n                out.flush();\n              }\n            } catch (IOException e) {\n              // exception at this point would leave the connection in an\n              // unrecoverable state (eg half a call left on the wire).\n              // So, close the connection, killing any outstanding calls\n              markClosed(e);\n            } finally {\n              //the buffer is just an in-memory buffer, but it is still polite to\n              // close early\n              IOUtils.closeStream(d);\n            }\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.closeConnection": "    private void closeConnection() {\n      if (socket == null) {\n        return;\n      }\n      // close the current connection\n      try {\n        socket.close();\n      } catch (IOException e) {\n        LOG.warn(\"Not able to close a socket\", e);\n      }\n      // set socket to null so that the next call to setupIOstreams\n      // can start the process of connect all over again.\n      socket = null;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.close": "    private synchronized void close() {\n      if (!shouldCloseConnection.get()) {\n        LOG.error(\"The connection is not in the closed state\");\n        return;\n      }\n\n      connections.invalidate(remoteId);\n\n      // close the streams and therefore the socket\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(in);\n      disposeSasl();\n\n      // clean up all calls\n      if (closeException == null) {\n        if (!calls.isEmpty()) {\n          LOG.warn(\n              \"A connection is closed for no cause and calls are not empty\");\n\n          // clean up calls anyway\n          closeException = new IOException(\"Unexpected closed connection\");\n          cleanupCalls();\n        }\n      } else {\n        // log the info\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"closing ipc connection to \" + server + \": \" +\n              closeException.getMessage(),closeException);\n        }\n\n        // cleanup calls\n        cleanupCalls();\n      }\n      closeConnection();\n      if (LOG.isDebugEnabled())\n        LOG.debug(getName() + \": closed\");\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupSaslConnection": "    private synchronized AuthMethod setupSaslConnection(final InputStream in2, \n        final OutputStream out2) throws IOException {\n      // Do not use Client.conf here! We must use ConnectionId.conf, since the\n      // Client object is cached and shared between all RPC clients, even those\n      // for separate services.\n      saslRpcClient = new SaslRpcClient(remoteId.getTicket(),\n          remoteId.getProtocol(), remoteId.getAddress(), remoteId.conf);\n      return saslRpcClient.saslConnect(in2, out2);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.disposeSasl": "    private synchronized void disposeSasl() {\n      if (saslRpcClient != null) {\n        try {\n          saslRpcClient.dispose();\n          saslRpcClient = null;\n        } catch (IOException ignored) {\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.waitForWork": "    private synchronized boolean waitForWork() {\n      if (calls.isEmpty() && !shouldCloseConnection.get()  && running.get())  {\n        long timeout = maxIdleTime-\n              (Time.now()-lastActivity.get());\n        if (timeout>0) {\n          try {\n            wait(timeout);\n          } catch (InterruptedException e) {}\n        }\n      }\n      \n      if (!calls.isEmpty() && !shouldCloseConnection.get() && running.get()) {\n        return true;\n      } else if (shouldCloseConnection.get()) {\n        return false;\n      } else if (calls.isEmpty()) { // idle connection closed or stopped\n        markClosed(null);\n        return false;\n      } else { // get stopped but there are still pending requests \n        markClosed((IOException)new IOException().initCause(\n            new InterruptedException()));\n        return false;\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.markClosed": "    private synchronized void markClosed(IOException e) {\n      if (shouldCloseConnection.compareAndSet(false, true)) {\n        closeException = e;\n        notifyAll();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.receiveRpcResponse": "    private void receiveRpcResponse() {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n      touch();\n      \n      try {\n        int totalLen = in.readInt();\n        RpcResponseHeaderProto header = \n            RpcResponseHeaderProto.parseDelimitedFrom(in);\n        checkResponse(header);\n\n        int headerLen = header.getSerializedSize();\n        headerLen += CodedOutputStream.computeRawVarint32Size(headerLen);\n\n        int callId = header.getCallId();\n        if (LOG.isDebugEnabled())\n          LOG.debug(getName() + \" got value #\" + callId);\n\n        Call call = calls.get(callId);\n        RpcStatusProto status = header.getStatus();\n        if (status == RpcStatusProto.SUCCESS) {\n          Writable value = ReflectionUtils.newInstance(valueClass, conf);\n          value.readFields(in);                 // read value\n          calls.remove(callId);\n          call.setRpcResponse(value);\n          \n          // verify that length was correct\n          // only for ProtobufEngine where len can be verified easily\n          if (call.getRpcResponse() instanceof ProtobufRpcEngine.RpcWrapper) {\n            ProtobufRpcEngine.RpcWrapper resWrapper = \n                (ProtobufRpcEngine.RpcWrapper) call.getRpcResponse();\n            if (totalLen != headerLen + resWrapper.getLength()) { \n              throw new RpcClientException(\n                  \"RPC response length mismatch on rpc success\");\n            }\n          }\n        } else { // Rpc Request failed\n          // Verify that length was correct\n          if (totalLen != headerLen) {\n            throw new RpcClientException(\n                \"RPC response length mismatch on rpc error\");\n          }\n          \n          final String exceptionClassName = header.hasExceptionClassName() ?\n                header.getExceptionClassName() : \n                  \"ServerDidNotSetExceptionClassName\";\n          final String errorMsg = header.hasErrorMsg() ? \n                header.getErrorMsg() : \"ServerDidNotSetErrorMsg\" ;\n          final RpcErrorCodeProto erCode = \n                    (header.hasErrorDetail() ? header.getErrorDetail() : null);\n          if (erCode == null) {\n             LOG.warn(\"Detailed error code not set by server on rpc error\");\n          }\n          RemoteException re = new RemoteException(exceptionClassName, errorMsg, erCode);\n          if (status == RpcStatusProto.ERROR) {\n            calls.remove(callId);\n            call.setException(re);\n          } else if (status == RpcStatusProto.FATAL) {\n            // Close the connection\n            markClosed(re);\n          }\n        }\n      } catch (IOException e) {\n        markClosed(e);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.shouldAuthenticateOverKrb": "    private synchronized boolean shouldAuthenticateOverKrb() throws IOException {\n      UserGroupInformation loginUser = UserGroupInformation.getLoginUser();\n      UserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\n      UserGroupInformation realUser = currentUser.getRealUser();\n      if (authMethod == AuthMethod.KERBEROS && loginUser != null &&\n      // Make sure user logged in using Kerberos either keytab or TGT\n          loginUser.hasKerberosCredentials() &&\n          // relogin only in case it is the login user (e.g. JT)\n          // or superuser (like oozie).\n          (loginUser.equals(currentUser) || loginUser.equals(realUser))) {\n        return true;\n      }\n      return false;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.handleSaslConnectionFailure": "    private synchronized void handleSaslConnectionFailure(\n        final int currRetries, final int maxRetries, final Exception ex,\n        final Random rand, final UserGroupInformation ugi) throws IOException,\n        InterruptedException {\n      ugi.doAs(new PrivilegedExceptionAction<Object>() {\n        @Override\n        public Object run() throws IOException, InterruptedException {\n          final short MAX_BACKOFF = 5000;\n          closeConnection();\n          disposeSasl();\n          if (shouldAuthenticateOverKrb()) {\n            if (currRetries < maxRetries) {\n              if(LOG.isDebugEnabled()) {\n                LOG.debug(\"Exception encountered while connecting to \"\n                    + \"the server : \" + ex);\n              }\n              // try re-login\n              if (UserGroupInformation.isLoginKeytabBased()) {\n                UserGroupInformation.getLoginUser().reloginFromKeytab();\n              } else if (UserGroupInformation.isLoginTicketBased()) {\n                UserGroupInformation.getLoginUser().reloginFromTicketCache();\n              }\n              // have granularity of milliseconds\n              //we are sleeping with the Connection lock held but since this\n              //connection instance is being used for connecting to the server\n              //in question, it is okay\n              Thread.sleep((rand.nextInt(MAX_BACKOFF) + 1));\n              return null;\n            } else {\n              String msg = \"Couldn't setup connection for \"\n                  + UserGroupInformation.getLoginUser().getUserName() + \" to \"\n                  + remoteId;\n              LOG.warn(msg, ex);\n              throw (IOException) new IOException(msg).initCause(ex);\n            }\n          } else {\n            LOG.warn(\"Exception encountered while connecting to \"\n                + \"the server : \" + ex);\n          }\n          if (ex instanceof RemoteException)\n            throw (RemoteException) ex;\n          throw new IOException(ex);\n        }\n      });\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupIOstreams": "    private synchronized void setupIOstreams(\n        AtomicBoolean fallbackToSimpleAuth) {\n      if (socket != null || shouldCloseConnection.get()) {\n        return;\n      } \n      try {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Connecting to \"+server);\n        }\n        if (Trace.isTracing()) {\n          Trace.addTimelineAnnotation(\"IPC client connecting to \" + server);\n        }\n        short numRetries = 0;\n        Random rand = null;\n        while (true) {\n          setupConnection();\n          InputStream inStream = NetUtils.getInputStream(socket);\n          OutputStream outStream = NetUtils.getOutputStream(socket);\n          writeConnectionHeader(outStream);\n          if (authProtocol == AuthProtocol.SASL) {\n            final InputStream in2 = inStream;\n            final OutputStream out2 = outStream;\n            UserGroupInformation ticket = remoteId.getTicket();\n            if (ticket.getRealUser() != null) {\n              ticket = ticket.getRealUser();\n            }\n            try {\n              authMethod = ticket\n                  .doAs(new PrivilegedExceptionAction<AuthMethod>() {\n                    @Override\n                    public AuthMethod run()\n                        throws IOException, InterruptedException {\n                      return setupSaslConnection(in2, out2);\n                    }\n                  });\n            } catch (Exception ex) {\n              authMethod = saslRpcClient.getAuthMethod();\n              if (rand == null) {\n                rand = new Random();\n              }\n              handleSaslConnectionFailure(numRetries++, maxRetriesOnSasl, ex,\n                  rand, ticket);\n              continue;\n            }\n            if (authMethod != AuthMethod.SIMPLE) {\n              // Sasl connect is successful. Let's set up Sasl i/o streams.\n              inStream = saslRpcClient.getInputStream(inStream);\n              outStream = saslRpcClient.getOutputStream(outStream);\n              // for testing\n              remoteId.saslQop =\n                  (String)saslRpcClient.getNegotiatedProperty(Sasl.QOP);\n              LOG.debug(\"Negotiated QOP is :\" + remoteId.saslQop);\n              if (fallbackToSimpleAuth != null) {\n                fallbackToSimpleAuth.set(false);\n              }\n            } else if (UserGroupInformation.isSecurityEnabled()) {\n              if (!fallbackAllowed) {\n                throw new IOException(\"Server asks us to fall back to SIMPLE \" +\n                    \"auth, but this client is configured to only allow secure \" +\n                    \"connections.\");\n              }\n              if (fallbackToSimpleAuth != null) {\n                fallbackToSimpleAuth.set(true);\n              }\n            }\n          }\n        \n          if (doPing) {\n            inStream = new PingInputStream(inStream);\n          }\n          this.in = new DataInputStream(new BufferedInputStream(inStream));\n\n          // SASL may have already buffered the stream\n          if (!(outStream instanceof BufferedOutputStream)) {\n            outStream = new BufferedOutputStream(outStream);\n          }\n          this.out = new DataOutputStream(outStream);\n          \n          writeConnectionContext(remoteId, authMethod);\n\n          // update last activity time\n          touch();\n\n          if (Trace.isTracing()) {\n            Trace.addTimelineAnnotation(\"IPC client connected to \" + server);\n          }\n\n          // start the receiver thread after the socket connection has been set\n          // up\n          start();\n          return;\n        }\n      } catch (Throwable t) {\n        if (t instanceof IOException) {\n          markClosed((IOException)t);\n        } else {\n          markClosed(new IOException(\"Couldn't set up IO streams\", t));\n        }\n        close();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupConnection": "    private synchronized void setupConnection() throws IOException {\n      short ioFailures = 0;\n      short timeoutFailures = 0;\n      while (true) {\n        try {\n          this.socket = socketFactory.createSocket();\n          this.socket.setTcpNoDelay(tcpNoDelay);\n          this.socket.setKeepAlive(true);\n          \n          if (tcpLowLatency) {\n            /*\n             * This allows intermediate switches to shape IPC traffic\n             * differently from Shuffle/HDFS DataStreamer traffic.\n             *\n             * IPTOS_RELIABILITY (0x04) | IPTOS_LOWDELAY (0x10)\n             *\n             * Prefer to optimize connect() speed & response latency over net\n             * throughput.\n             */\n            this.socket.setTrafficClass(0x04 | 0x10);\n            this.socket.setPerformancePreferences(1, 2, 0);\n          }\n\n          /*\n           * Bind the socket to the host specified in the principal name of the\n           * client, to ensure Server matching address of the client connection\n           * to host name in principal passed.\n           */\n          UserGroupInformation ticket = remoteId.getTicket();\n          if (ticket != null && ticket.hasKerberosCredentials()) {\n            KerberosInfo krbInfo = \n              remoteId.getProtocol().getAnnotation(KerberosInfo.class);\n            if (krbInfo != null && krbInfo.clientPrincipal() != null) {\n              String host = \n                SecurityUtil.getHostFromPrincipal(remoteId.getTicket().getUserName());\n              \n              // If host name is a valid local address then bind socket to it\n              InetAddress localAddr = NetUtils.getLocalInetAddress(host);\n              if (localAddr != null) {\n                this.socket.bind(new InetSocketAddress(localAddr, 0));\n              }\n            }\n          }\n          \n          NetUtils.connect(this.socket, server, connectionTimeout);\n          if (rpcTimeout > 0) {\n            pingInterval = rpcTimeout;  // rpcTimeout overwrites pingInterval\n          }\n          this.socket.setSoTimeout(pingInterval);\n          return;\n        } catch (ConnectTimeoutException toe) {\n          /* Check for an address change and update the local reference.\n           * Reset the failure counter if the address was changed\n           */\n          if (updateAddress()) {\n            timeoutFailures = ioFailures = 0;\n          }\n          handleConnectionTimeout(timeoutFailures++,\n              maxRetriesOnSocketTimeouts, toe);\n        } catch (IOException ie) {\n          if (updateAddress()) {\n            timeoutFailures = ioFailures = 0;\n          }\n          handleConnectionFailure(ioFailures++, ie);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.touch": "    private void touch() {\n      lastActivity.set(Time.now());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.writeConnectionHeader": "    private void writeConnectionHeader(OutputStream outStream)\n        throws IOException {\n      DataOutputStream out = new DataOutputStream(new BufferedOutputStream(outStream));\n      // Write out the header, version and authentication method\n      out.write(RpcConstants.HEADER.array());\n      out.write(RpcConstants.CURRENT_VERSION);\n      out.write(serviceClass);\n      out.write(authProtocol.callId);\n      out.flush();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.writeConnectionContext": "    private void writeConnectionContext(ConnectionId remoteId,\n                                        AuthMethod authMethod)\n                                            throws IOException {\n      // Write out the ConnectionHeader\n      IpcConnectionContextProto message = ProtoUtil.makeIpcConnectionContext(\n          RPC.getProtocolName(remoteId.getProtocol()),\n          remoteId.getTicket(),\n          authMethod);\n      RpcRequestHeaderProto connectionContextHeader = ProtoUtil\n          .makeRpcRequestHeader(RpcKind.RPC_PROTOCOL_BUFFER,\n              OperationProto.RPC_FINAL_PACKET, CONNECTION_CONTEXT_CALL_ID,\n              RpcConstants.INVALID_RETRY_COUNT, clientId);\n      RpcRequestMessageWrapper request =\n          new RpcRequestMessageWrapper(connectionContextHeader, message);\n      \n      // Write out the packet length\n      out.writeInt(request.getLength());\n      request.write(out);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getTicket": "    UserGroupInformation getTicket() {\n      return ticket;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SaslRpcClient.saslConnect": "  public AuthMethod saslConnect(InputStream inS, OutputStream outS)\n      throws IOException {\n    DataInputStream inStream = new DataInputStream(new BufferedInputStream(inS));\n    DataOutputStream outStream = new DataOutputStream(new BufferedOutputStream(\n        outS));\n    \n    // redefined if/when a SASL negotiation starts, can be queried if the\n    // negotiation fails\n    authMethod = AuthMethod.SIMPLE;\n    \n    sendSaslMessage(outStream, negotiateRequest);\n    \n    // loop until sasl is complete or a rpc error occurs\n    boolean done = false;\n    do {\n      int totalLen = inStream.readInt();\n      RpcResponseMessageWrapper responseWrapper =\n          new RpcResponseMessageWrapper();\n      responseWrapper.readFields(inStream);\n      RpcResponseHeaderProto header = responseWrapper.getMessageHeader();\n      switch (header.getStatus()) {\n        case ERROR: // might get a RPC error during \n        case FATAL:\n          throw new RemoteException(header.getExceptionClassName(),\n                                    header.getErrorMsg());\n        default: break;\n      }\n      if (totalLen != responseWrapper.getLength()) {\n        throw new SaslException(\"Received malformed response length\");\n      }\n      \n      if (header.getCallId() != AuthProtocol.SASL.callId) {\n        throw new SaslException(\"Non-SASL response during negotiation\");\n      }\n      RpcSaslProto saslMessage =\n          RpcSaslProto.parseFrom(responseWrapper.getMessageBytes());\n      // handle sasl negotiation process\n      RpcSaslProto.Builder response = null;\n      switch (saslMessage.getState()) {\n        case NEGOTIATE: {\n          // create a compatible SASL client, throws if no supported auths\n          SaslAuth saslAuthType = selectSaslClient(saslMessage.getAuthsList());\n          // define auth being attempted, caller can query if connect fails\n          authMethod = AuthMethod.valueOf(saslAuthType.getMethod());\n          \n          byte[] responseToken = null;\n          if (authMethod == AuthMethod.SIMPLE) { // switching to SIMPLE\n            done = true; // not going to wait for success ack\n          } else {\n            byte[] challengeToken = null;\n            if (saslAuthType.hasChallenge()) {\n              // server provided the first challenge\n              challengeToken = saslAuthType.getChallenge().toByteArray();\n              saslAuthType =\n                  SaslAuth.newBuilder(saslAuthType).clearChallenge().build();\n            } else if (saslClient.hasInitialResponse()) {\n              challengeToken = new byte[0];\n            }\n            responseToken = (challengeToken != null)\n                ? saslClient.evaluateChallenge(challengeToken)\n                    : new byte[0];\n          }\n          response = createSaslReply(SaslState.INITIATE, responseToken);\n          response.addAuths(saslAuthType);\n          break;\n        }\n        case CHALLENGE: {\n          if (saslClient == null) {\n            // should probably instantiate a client to allow a server to\n            // demand a specific negotiation\n            throw new SaslException(\"Server sent unsolicited challenge\");\n          }\n          byte[] responseToken = saslEvaluateToken(saslMessage, false);\n          response = createSaslReply(SaslState.RESPONSE, responseToken);\n          break;\n        }\n        case SUCCESS: {\n          // simple server sends immediate success to a SASL client for\n          // switch to simple\n          if (saslClient == null) {\n            authMethod = AuthMethod.SIMPLE;\n          } else {\n            saslEvaluateToken(saslMessage, true);\n          }\n          done = true;\n          break;\n        }\n        default: {\n          throw new SaslException(\n              \"RPC client doesn't support SASL \" + saslMessage.getState());\n        }\n      }\n      if (response != null) {\n        sendSaslMessage(outStream, response.build());\n      }\n    } while (!done);\n    return authMethod;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SaslRpcClient.sendSaslMessage": "  private void sendSaslMessage(DataOutputStream out, RpcSaslProto message)\n      throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending sasl message \"+message);\n    }\n    RpcRequestMessageWrapper request =\n        new RpcRequestMessageWrapper(saslHeader, message);\n    out.writeInt(request.getLength());\n    request.write(out);\n    out.flush();    \n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SaslRpcClient.saslEvaluateToken": "  private byte[] saslEvaluateToken(RpcSaslProto saslResponse,\n      boolean serverIsDone) throws SaslException {\n    byte[] saslToken = null;\n    if (saslResponse.hasToken()) {\n      saslToken = saslResponse.getToken().toByteArray();\n      saslToken = saslClient.evaluateChallenge(saslToken);\n    } else if (!serverIsDone) {\n      // the server may only omit a token when it's done\n      throw new SaslException(\"Server challenge contains no token\");\n    }\n    if (serverIsDone) {\n      // server tried to report success before our client completed\n      if (!saslClient.isComplete()) {\n        throw new SaslException(\"Client is out of sync with server\");\n      }\n      // a client cannot generate a response to a success message\n      if (saslToken != null) {\n        throw new SaslException(\"Client generated spurious response\");        \n      }\n    }\n    return saslToken;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SaslRpcClient.selectSaslClient": "  private SaslAuth selectSaslClient(List<SaslAuth> authTypes)\n      throws SaslException, AccessControlException, IOException {\n    SaslAuth selectedAuthType = null;\n    boolean switchToSimple = false;\n    for (SaslAuth authType : authTypes) {\n      if (!isValidAuthType(authType)) {\n        continue; // don't know what it is, try next\n      }\n      AuthMethod authMethod = AuthMethod.valueOf(authType.getMethod());\n      if (authMethod == AuthMethod.SIMPLE) {\n        switchToSimple = true;\n      } else {\n        saslClient = createSaslClient(authType);\n        if (saslClient == null) { // client lacks credentials, try next\n          continue;\n        }\n      }\n      selectedAuthType = authType;\n      break;\n    }\n    if (saslClient == null && !switchToSimple) {\n      List<String> serverAuthMethods = new ArrayList<String>();\n      for (SaslAuth authType : authTypes) {\n        serverAuthMethods.add(authType.getMethod());\n      }\n      throw new AccessControlException(\n          \"Client cannot authenticate via:\" + serverAuthMethods);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Use \" + selectedAuthType.getMethod() +\n          \" authentication for protocol \" + protocol.getSimpleName());\n    }\n    return selectedAuthType;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SaslRpcClient.createSaslReply": "  private RpcSaslProto.Builder createSaslReply(SaslState state,\n                                               byte[] responseToken) {\n    RpcSaslProto.Builder response = RpcSaslProto.newBuilder();\n    response.setState(state);\n    if (responseToken != null) {\n      response.setToken(ByteString.copyFrom(responseToken));\n    }\n    return response;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcClientUtil.methodToTraceString": "  public static String methodToTraceString(Method method) {\n    Class<?> clazz = method.getDeclaringClass();\n    while (true) {\n      Class<?> next = clazz.getEnclosingClass();\n      if (next == null || next.getEnclosingClass() == null) break;\n      clazz = next;\n    }\n    return clazz.getSimpleName() + \"#\" + method.getName();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.equals": "      public boolean equals(Object obj) {\n        if (obj == this) {\n          return true;\n        }\n        if (obj != null && obj instanceof Key) {\n          Key that = (Key)obj;\n          return isEqual(this.scheme, that.scheme)\n                 && isEqual(this.authority, that.authority)\n                 && isEqual(this.ugi, that.ugi)\n                 && (this.unique == that.unique);\n        }\n        return false;        \n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.isEqual": "      static boolean isEqual(Object a, Object b) {\n        return a == b || (a != null && a.equals(b));        \n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getFSofPath": "  protected static FileSystem getFSofPath(final Path absOrFqPath,\n      final Configuration conf)\n      throws UnsupportedFileSystemException, IOException {\n    absOrFqPath.checkNotSchemeWithRelative();\n    absOrFqPath.checkNotRelative();\n\n    // Uses the default file system if not fully qualified\n    return get(absOrFqPath.toUri(), conf);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.get": "    FileSystem get(URI uri, Configuration conf) throws IOException{\n      Key key = new Key(uri, conf);\n      return getInternal(uri, conf, key);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FSLinkResolver.qualifySymlinkTarget": "  public static Path qualifySymlinkTarget(final URI pathURI,\n      Path pathWithLink, Path target) {\n    // NB: makeQualified uses the target's scheme and authority, if\n    // specified, and the scheme and authority of pathURI, if not.\n    final URI targetUri = target.toUri();\n    final String scheme = targetUri.getScheme();\n    final String auth = targetUri.getAuthority();\n    return (scheme == null && auth == null) ? target.makeQualified(pathURI,\n        pathWithLink.getParent()) : target;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.areSymlinksEnabled": "  public static boolean areSymlinksEnabled() {\n    return symlinksEnabled;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getUri": "  public abstract URI getUri();\n  \n  /**\n   * Return a canonicalized form of this FileSystem's URI.\n   * \n   * The default implementation simply calls {@link #canonicalizeUri(URI)}"
        },
        "bug_report": {
            "Title": "Aggregated Logs Deletion doesnt work after refreshing Log Retention Settings in secure cluster",
            "Description": "{{GSSException}} is thrown everytime log aggregation deletion is attempted after executing bin/mapred hsadmin -refreshLogRetentionSettings in a secure cluster.\n\nThe problem can be reproduced by following steps:\n1. startup historyserver in secure cluster.\n2. Log deletion happens as per expectation. \n3. execute {{mapred hsadmin -refreshLogRetentionSettings}} command to refresh the configuration value.\n4. All the subsequent attempts of log deletion fail with {{GSSException}}\n\nFollowing exception can be found in historyserver's log if log deletion is enabled. \n{noformat}\n2015-06-04 14:14:40,070 | ERROR | Timer-3 | Error reading root log dir this deletion attempt is being aborted | AggregatedLogDeletionService.java:127\njava.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: \"vm-31/9.91.12.31\"; destination host is: \"vm-33\":25000; \n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1414)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1363)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n        at com.sun.proxy.$Proxy9.getListing(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getListing(ClientNamenodeProtocolTranslatorPB.java:519)\n        at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy10.getListing(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1767)\n        at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1750)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:691)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:102)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:753)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:749)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:749)\n        at org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService$LogDeletionTask.run(AggregatedLogDeletionService.java:68)\n        at java.util.TimerThread.mainLoop(Timer.java:555)\n        at java.util.TimerThread.run(Timer.java:505)\nCaused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:677)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1641)\n        at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:640)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:724)\n        at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1381)\n        ... 21 more\nCaused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)\n        at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:411)\n        at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:550)\n        at org.apache.hadoop.ipc.Client$Connection.access$1800(Client.java:367)\n        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:716)\n        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:712)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1641)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:711)\n        ... 24 more\nCaused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)\n        at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)\n        at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121)\n        at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)\n        at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)\n        ... 33 more\n{noformat}"
        }
    },
    {
        "filename": "MAPREDUCE-6693.json",
        "creation_time": "2016-05-10T11:30:38.000+0000",
        "stack_trace": "```\njava.lang.ArrayIndexOutOfBoundsException: 50\n\tat org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.trimURLEncodedString(FileNameIndexUtils.java:326)\n\tat org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.getDoneFileName(FileNameIndexUtils.java:86)\n\tat org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.processDoneFiles(JobHistoryEventHandler.java:1147)\n\tat org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:635)\n\tat org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$1.run(JobHistoryEventHandler.java:341)\n\tat java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.trimURLEncodedString": "  private static String trimURLEncodedString(\n      String encodedString, int limitLength) {\n    assert(limitLength >= 0) : \"limitLength should be positive integer\";\n\n    if (encodedString.length() < limitLength) {\n      return encodedString;\n    }\n\n    int index = 0;\n    int increase = 0;\n    byte[] strBytes = encodedString.getBytes(UTF_8);\n\n    // calculate effective character length based on UTF-8 specification.\n    // The size of a character coded in UTF-8 should be 4-byte at most.\n    // See RFC3629\n    while (true) {\n      byte b = strBytes[index];\n      if (b == '%') {\n        byte minuend1 = strBytes[index + 1];\n        byte subtrahend1 = (byte)(Character.isDigit(\n            minuend1) ? '0' : 'A' - 10);\n        byte minuend2 = strBytes[index + 2];\n        byte subtrahend2 = (byte)(Character.isDigit(\n            minuend2) ? '0' : 'A' - 10);\n        int initialHex =\n            ((Character.toUpperCase(minuend1) - subtrahend1) << 4) +\n            (Character.toUpperCase(minuend2) - subtrahend2);\n\n        if (0x00 <= initialHex && initialHex <= 0x7F) {\n          // For 1-byte UTF-8 characters\n          increase = 3;\n        } else if (0xC2 <= initialHex && initialHex <= 0xDF) {\n          // For 2-byte UTF-8 characters\n          increase = 6;\n        } else if (0xE0 <= initialHex && initialHex <= 0xEF) {\n          // For 3-byte UTF-8 characters\n          increase = 9;\n        } else {\n          // For 4-byte UTF-8 characters\n          increase = 12;\n        }\n      } else {\n        increase = 1;\n      }\n      if (index + increase > limitLength) {\n        break;\n      } else {\n        index += increase;\n      }\n    }\n\n    return encodedString.substring(0, index);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.getDoneFileName": "  public static String getDoneFileName(JobIndexInfo indexInfo,\n      int jobNameLimit) throws IOException {\n    StringBuilder sb = new StringBuilder();\n    //JobId\n    sb.append(encodeJobHistoryFileName(escapeDelimiters(\n        TypeConverter.fromYarn(indexInfo.getJobId()).toString())));\n    sb.append(DELIMITER);\n\n    //SubmitTime\n    sb.append(encodeJobHistoryFileName(String.valueOf(\n        indexInfo.getSubmitTime())));\n    sb.append(DELIMITER);\n\n    //UserName\n    sb.append(encodeJobHistoryFileName(escapeDelimiters(\n        getUserName(indexInfo))));\n    sb.append(DELIMITER);\n\n    //JobName\n    sb.append(trimURLEncodedString(encodeJobHistoryFileName(escapeDelimiters(\n        getJobName(indexInfo))), jobNameLimit));\n    sb.append(DELIMITER);\n\n    //FinishTime\n    sb.append(encodeJobHistoryFileName(\n        String.valueOf(indexInfo.getFinishTime())));\n    sb.append(DELIMITER);\n\n    //NumMaps\n    sb.append(encodeJobHistoryFileName(\n        String.valueOf(indexInfo.getNumMaps())));\n    sb.append(DELIMITER);\n\n    //NumReduces\n    sb.append(encodeJobHistoryFileName(\n        String.valueOf(indexInfo.getNumReduces())));\n    sb.append(DELIMITER);\n\n    //JobStatus\n    sb.append(encodeJobHistoryFileName(indexInfo.getJobStatus()));\n    sb.append(DELIMITER);\n\n    //QueueName\n    sb.append(escapeDelimiters(encodeJobHistoryFileName(\n        getQueueName(indexInfo))));\n    sb.append(DELIMITER);\n\n    //JobStartTime\n    sb.append(encodeJobHistoryFileName(\n        String.valueOf(indexInfo.getJobStartTime())));\n\n    sb.append(encodeJobHistoryFileName(\n        JobHistoryUtils.JOB_HISTORY_FILE_EXTENSION));\n    return sb.toString();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.getQueueName": "  private static String getQueueName(JobIndexInfo indexInfo) {\n    return getNonEmptyString(indexInfo.getQueueName());\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.escapeDelimiters": "  private static String escapeDelimiters(String escapee) {\n    return escapee.replaceAll(DELIMITER, DELIMITER_ESCAPE);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.encodeJobHistoryFileName": "  public static String encodeJobHistoryFileName(String logFileName)\n  throws IOException {\n    String replacementDelimiterEscape = null;\n\n    // Temporarily protect the escape delimiters from encoding\n    if (logFileName.contains(DELIMITER_ESCAPE)) {\n      replacementDelimiterEscape = nonOccursString(logFileName);\n\n      logFileName = logFileName.replaceAll(\n          DELIMITER_ESCAPE, replacementDelimiterEscape);\n    }\n\n    String encodedFileName = null;\n    try {\n      encodedFileName = URLEncoder.encode(logFileName, \"UTF-8\");\n    } catch (UnsupportedEncodingException uee) {\n      IOException ioe = new IOException();\n      ioe.initCause(uee);\n      ioe.setStackTrace(uee.getStackTrace());\n      throw ioe;\n    }\n\n    // Restore protected escape delimiters after encoding\n    if (replacementDelimiterEscape != null) {\n      encodedFileName = encodedFileName.replaceAll(\n          replacementDelimiterEscape, DELIMITER_ESCAPE);\n    }\n\n    return encodedFileName;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.getUserName": "  private static String getUserName(JobIndexInfo indexInfo) {\n    return getNonEmptyString(indexInfo.getUser());\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.getJobName": "  private static String getJobName(JobIndexInfo indexInfo) {\n    return getNonEmptyString(indexInfo.getJobName());\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.processDoneFiles": "  protected void processDoneFiles(JobId jobId) throws IOException {\n\n    final MetaInfo mi = fileMap.get(jobId);\n    if (mi == null) {\n      throw new IOException(\"No MetaInfo found for JobId: [\" + jobId + \"]\");\n    }\n\n    if (mi.getHistoryFile() == null) {\n      LOG.warn(\"No file for job-history with \" + jobId + \" found in cache!\");\n    }\n    if (mi.getConfFile() == null) {\n      LOG.warn(\"No file for jobconf with \" + jobId + \" found in cache!\");\n    }\n      \n    // Writing out the summary file.\n    // TODO JH enhancement - reuse this file to store additional indexing info\n    // like ACLs, etc. JHServer can use HDFS append to build an index file\n    // with more info than is available via the filename.\n    Path qualifiedSummaryDoneFile = null;\n    FSDataOutputStream summaryFileOut = null;\n    try {\n      String doneSummaryFileName = getTempFileName(JobHistoryUtils\n          .getIntermediateSummaryFileName(jobId));\n      qualifiedSummaryDoneFile = doneDirFS.makeQualified(new Path(\n          doneDirPrefixPath, doneSummaryFileName));\n      summaryFileOut = doneDirFS.create(qualifiedSummaryDoneFile, true);\n      summaryFileOut.writeUTF(mi.getJobSummary().getJobSummaryString());\n      summaryFileOut.close();\n      doneDirFS.setPermission(qualifiedSummaryDoneFile, new FsPermission(\n          JobHistoryUtils.HISTORY_INTERMEDIATE_FILE_PERMISSIONS));\n    } catch (IOException e) {\n      LOG.info(\"Unable to write out JobSummaryInfo to [\"\n          + qualifiedSummaryDoneFile + \"]\", e);\n      throw e;\n    }\n\n    try {\n\n      // Move historyFile to Done Folder.\n      Path qualifiedDoneFile = null;\n      if (mi.getHistoryFile() != null) {\n        Path historyFile = mi.getHistoryFile();\n        Path qualifiedLogFile = stagingDirFS.makeQualified(historyFile);\n        int jobNameLimit =\n            getConfig().getInt(JHAdminConfig.MR_HS_JOBNAME_LIMIT,\n            JHAdminConfig.DEFAULT_MR_HS_JOBNAME_LIMIT);\n        String doneJobHistoryFileName =\n            getTempFileName(FileNameIndexUtils.getDoneFileName(mi\n                .getJobIndexInfo(), jobNameLimit));\n        qualifiedDoneFile =\n            doneDirFS.makeQualified(new Path(doneDirPrefixPath,\n                doneJobHistoryFileName));\n        moveToDoneNow(qualifiedLogFile, qualifiedDoneFile);\n      }\n\n      // Move confFile to Done Folder\n      Path qualifiedConfDoneFile = null;\n      if (mi.getConfFile() != null) {\n        Path confFile = mi.getConfFile();\n        Path qualifiedConfFile = stagingDirFS.makeQualified(confFile);\n        String doneConfFileName =\n            getTempFileName(JobHistoryUtils\n                .getIntermediateConfFileName(jobId));\n        qualifiedConfDoneFile =\n            doneDirFS.makeQualified(new Path(doneDirPrefixPath,\n                doneConfFileName));\n        moveToDoneNow(qualifiedConfFile, qualifiedConfDoneFile);\n      }\n      \n      moveTmpToDone(qualifiedSummaryDoneFile);\n      moveTmpToDone(qualifiedConfDoneFile);\n      moveTmpToDone(qualifiedDoneFile);\n\n    } catch (IOException e) {\n      LOG.error(\"Error closing writer for JobID: \" + jobId);\n      throw e;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.getHistoryFile": "    Path getHistoryFile() {\n      return historyFile;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.getJobIndexInfo": "    JobIndexInfo getJobIndexInfo() {\n      return jobIndexInfo;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.moveTmpToDone": "  private void moveTmpToDone(Path tmpPath) throws IOException {\n    if (tmpPath != null) {\n      String tmpFileName = tmpPath.getName();\n      String fileName = getFileNameFromTmpFN(tmpFileName);\n      Path path = new Path(tmpPath.getParent(), fileName);\n      doneDirFS.rename(tmpPath, path);\n      LOG.info(\"Moved tmp to done: \" + tmpPath + \" to \" + path);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.moveToDoneNow": "  private void moveToDoneNow(Path fromPath, Path toPath) throws IOException {\n    // check if path exists, in case of retries it may not exist\n    if (stagingDirFS.exists(fromPath)) {\n      LOG.info(\"Copying \" + fromPath.toString() + \" to \" + toPath.toString());\n      // TODO temporarily removing the existing dst\n      if (doneDirFS.exists(toPath)) {\n        doneDirFS.delete(toPath, true);\n      }\n      boolean copied = FileUtil.copy(stagingDirFS, fromPath, doneDirFS, toPath,\n          false, getConfig());\n\n      if (copied)\n        LOG.info(\"Copied to done location: \" + toPath);\n      else \n        LOG.info(\"copy failed\");\n      doneDirFS.setPermission(toPath, new FsPermission(\n          JobHistoryUtils.HISTORY_INTERMEDIATE_FILE_PERMISSIONS));\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.getJobSummary": "    JobSummary getJobSummary() {\n      return jobSummary;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.getConfFile": "    Path getConfFile() {\n      return confFile;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.getTempFileName": "  private String getTempFileName(String srcFile) {\n    return srcFile + \"_tmp\";\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent": "  public void handleEvent(JobHistoryEvent event) {\n    synchronized (lock) {\n\n      // If this is JobSubmitted Event, setup the writer\n      if (event.getHistoryEvent().getEventType() == EventType.AM_STARTED) {\n        try {\n          AMStartedEvent amStartedEvent =\n              (AMStartedEvent) event.getHistoryEvent();\n          setupEventWriter(event.getJobID(), amStartedEvent);\n        } catch (IOException ioe) {\n          LOG.error(\"Error JobHistoryEventHandler in handleEvent: \" + event,\n              ioe);\n          throw new YarnRuntimeException(ioe);\n        }\n      }\n\n      // For all events\n      // (1) Write it out\n      // (2) Process it for JobSummary\n      // (3) Process it for ATS (if enabled)\n      MetaInfo mi = fileMap.get(event.getJobID());\n      try {\n        HistoryEvent historyEvent = event.getHistoryEvent();\n        if (! (historyEvent instanceof NormalizedResourceEvent)) {\n          mi.writeEvent(historyEvent);\n        }\n        processEventForJobSummary(event.getHistoryEvent(), mi.getJobSummary(),\n            event.getJobID());\n        if (timelineClient != null) {\n          processEventForTimelineServer(historyEvent, event.getJobID(),\n              event.getTimestamp());\n        }\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"In HistoryEventHandler \"\n              + event.getHistoryEvent().getEventType());\n        }\n      } catch (IOException e) {\n        LOG.error(\"Error writing History Event: \" + event.getHistoryEvent(),\n            e);\n        throw new YarnRuntimeException(e);\n      }\n\n      if (event.getHistoryEvent().getEventType() == EventType.JOB_SUBMITTED) {\n        JobSubmittedEvent jobSubmittedEvent =\n            (JobSubmittedEvent) event.getHistoryEvent();\n        mi.getJobIndexInfo().setSubmitTime(jobSubmittedEvent.getSubmitTime());\n        mi.getJobIndexInfo().setQueueName(jobSubmittedEvent.getJobQueueName());\n      }\n      //initialize the launchTime in the JobIndexInfo of MetaInfo\n      if(event.getHistoryEvent().getEventType() == EventType.JOB_INITED ){\n        JobInitedEvent jie = (JobInitedEvent) event.getHistoryEvent();\n        mi.getJobIndexInfo().setJobStartTime(jie.getLaunchTime());\n      }\n      \n      if (event.getHistoryEvent().getEventType() == EventType.JOB_QUEUE_CHANGED) {\n        JobQueueChangeEvent jQueueEvent =\n            (JobQueueChangeEvent) event.getHistoryEvent();\n        mi.getJobIndexInfo().setQueueName(jQueueEvent.getJobQueueName());\n      }\n\n      // If this is JobFinishedEvent, close the writer and setup the job-index\n      if (event.getHistoryEvent().getEventType() == EventType.JOB_FINISHED) {\n        try {\n          JobFinishedEvent jFinishedEvent =\n              (JobFinishedEvent) event.getHistoryEvent();\n          mi.getJobIndexInfo().setFinishTime(jFinishedEvent.getFinishTime());\n          mi.getJobIndexInfo().setNumMaps(jFinishedEvent.getFinishedMaps());\n          mi.getJobIndexInfo().setNumReduces(\n              jFinishedEvent.getFinishedReduces());\n          mi.getJobIndexInfo().setJobStatus(JobState.SUCCEEDED.toString());\n          closeEventWriter(event.getJobID());\n          processDoneFiles(event.getJobID());\n        } catch (IOException e) {\n          throw new YarnRuntimeException(e);\n        }\n      }\n      // In case of JOB_ERROR, only process all the Done files(e.g. job\n      // summary, job history file etc.) if it is last AM retry.\n      if (event.getHistoryEvent().getEventType() == EventType.JOB_ERROR) {\n        try {\n          JobUnsuccessfulCompletionEvent jucEvent =\n              (JobUnsuccessfulCompletionEvent) event.getHistoryEvent();\n          mi.getJobIndexInfo().setFinishTime(jucEvent.getFinishTime());\n          mi.getJobIndexInfo().setNumMaps(jucEvent.getFinishedMaps());\n          mi.getJobIndexInfo().setNumReduces(jucEvent.getFinishedReduces());\n          mi.getJobIndexInfo().setJobStatus(jucEvent.getStatus());\n          closeEventWriter(event.getJobID());\n          if(context.isLastAMRetry())\n            processDoneFiles(event.getJobID());\n        } catch (IOException e) {\n          throw new YarnRuntimeException(e);\n        }\n      }\n\n      if (event.getHistoryEvent().getEventType() == EventType.JOB_FAILED\n          || event.getHistoryEvent().getEventType() == EventType.JOB_KILLED) {\n        try {\n          JobUnsuccessfulCompletionEvent jucEvent = \n              (JobUnsuccessfulCompletionEvent) event\n              .getHistoryEvent();\n          mi.getJobIndexInfo().setFinishTime(jucEvent.getFinishTime());\n          mi.getJobIndexInfo().setNumMaps(jucEvent.getFinishedMaps());\n          mi.getJobIndexInfo().setNumReduces(jucEvent.getFinishedReduces());\n          mi.getJobIndexInfo().setJobStatus(jucEvent.getStatus());\n          closeEventWriter(event.getJobID());\n          processDoneFiles(event.getJobID());\n        } catch (IOException e) {\n          throw new YarnRuntimeException(e);\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.setupEventWriter": "  protected void setupEventWriter(JobId jobId, AMStartedEvent amStartedEvent)\n      throws IOException {\n    if (stagingDirPath == null) {\n      LOG.error(\"Log Directory is null, returning\");\n      throw new IOException(\"Missing Log Directory for History\");\n    }\n\n    MetaInfo oldFi = fileMap.get(jobId);\n    Configuration conf = getConfig();\n\n    // TODO Ideally this should be written out to the job dir\n    // (.staging/jobid/files - RecoveryService will need to be patched)\n    Path historyFile = JobHistoryUtils.getStagingJobHistoryFile(\n        stagingDirPath, jobId, startCount);\n    String user = UserGroupInformation.getCurrentUser().getShortUserName();\n    if (user == null) {\n      throw new IOException(\n          \"User is null while setting up jobhistory eventwriter\");\n    }\n\n    String jobName = context.getJob(jobId).getName();\n    EventWriter writer = (oldFi == null) ? null : oldFi.writer;\n \n    Path logDirConfPath =\n        JobHistoryUtils.getStagingConfFile(stagingDirPath, jobId, startCount);\n    if (writer == null) {\n      try {\n        writer = createEventWriter(historyFile);\n        LOG.info(\"Event Writer setup for JobId: \" + jobId + \", File: \"\n            + historyFile);\n      } catch (IOException ioe) {\n        LOG.info(\"Could not create log file: [\" + historyFile + \"] + for job \"\n            + \"[\" + jobName + \"]\");\n        throw ioe;\n      }\n      \n      //Write out conf only if the writer isn't already setup.\n      if (conf != null) {\n        // TODO Ideally this should be written out to the job dir\n        // (.staging/jobid/files - RecoveryService will need to be patched)\n        FSDataOutputStream jobFileOut = null;\n        try {\n          if (logDirConfPath != null) {\n            jobFileOut = stagingDirFS.create(logDirConfPath, true);\n            conf.writeXml(jobFileOut);\n            jobFileOut.close();\n          }\n        } catch (IOException e) {\n          LOG.info(\"Failed to write the job configuration file\", e);\n          throw e;\n        }\n      }\n    }\n\n    String queueName = JobConf.DEFAULT_QUEUE_NAME;\n    if (conf != null) {\n      queueName = conf.get(MRJobConfig.QUEUE_NAME, JobConf.DEFAULT_QUEUE_NAME);\n    }\n\n    MetaInfo fi = new MetaInfo(historyFile, logDirConfPath, writer,\n        user, jobName, jobId, amStartedEvent.getForcedJobStateOnShutDown(),\n        queueName);\n    fi.getJobSummary().setJobId(jobId);\n    fi.getJobSummary().setJobLaunchTime(amStartedEvent.getStartTime());\n    fi.getJobSummary().setJobSubmitTime(amStartedEvent.getSubmitTime());\n    fi.getJobIndexInfo().setJobStartTime(amStartedEvent.getStartTime());\n    fi.getJobIndexInfo().setSubmitTime(amStartedEvent.getSubmitTime());\n    fileMap.put(jobId, fi);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.closeEventWriter": "  protected void closeEventWriter(JobId jobId) throws IOException {\n    final MetaInfo mi = fileMap.get(jobId);\n    if (mi == null) {\n      throw new IOException(\"No MetaInfo found for JobId: [\" + jobId + \"]\");\n    }\n\n    if (!mi.isWriterActive()) {\n      throw new IOException(\n          \"Inactive Writer: Likely received multiple JobFinished / \" +\n          \"JobUnsuccessful events for JobId: [\"\n              + jobId + \"]\");\n    }\n\n    // Close the Writer\n    try {\n      mi.closeWriter();\n    } catch (IOException e) {\n      LOG.error(\"Error closing writer for JobID: \" + jobId);\n      throw e;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.writeEvent": "    void writeEvent(HistoryEvent event) throws IOException {\n      LOG.debug(\"Writing event\");\n      synchronized (lock) {\n        if (writer != null) {\n          writer.write(event);\n          processEventForFlush(event);\n          maybeFlush(event);\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.processEventForTimelineServer": "  private void processEventForTimelineServer(HistoryEvent event, JobId jobId,\n          long timestamp) {\n    TimelineEvent tEvent = new TimelineEvent();\n    tEvent.setEventType(StringUtils.toUpperCase(event.getEventType().name()));\n    tEvent.setTimestamp(timestamp);\n    TimelineEntity tEntity = new TimelineEntity();\n\n    switch (event.getEventType()) {\n      case JOB_SUBMITTED:\n        JobSubmittedEvent jse =\n            (JobSubmittedEvent) event;\n        tEvent.addEventInfo(\"SUBMIT_TIME\", jse.getSubmitTime());\n        tEvent.addEventInfo(\"QUEUE_NAME\", jse.getJobQueueName());\n        tEvent.addEventInfo(\"JOB_NAME\", jse.getJobName());\n        tEvent.addEventInfo(\"USER_NAME\", jse.getUserName());\n        tEvent.addEventInfo(\"JOB_CONF_PATH\", jse.getJobConfPath());\n        tEvent.addEventInfo(\"ACLS\", jse.getJobAcls());\n        tEvent.addEventInfo(\"JOB_QUEUE_NAME\", jse.getJobQueueName());\n        tEvent.addEventInfo(\"WORKFLOW_ID\", jse.getWorkflowId());\n        tEvent.addEventInfo(\"WORKFLOW_NAME\", jse.getWorkflowName());\n        tEvent.addEventInfo(\"WORKFLOW_NAME_NAME\", jse.getWorkflowNodeName());\n        tEvent.addEventInfo(\"WORKFLOW_ADJACENCIES\",\n                jse.getWorkflowAdjacencies());\n        tEvent.addEventInfo(\"WORKFLOW_TAGS\", jse.getWorkflowTags());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(jobId.toString());\n        tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\n        break;\n      case JOB_STATUS_CHANGED:\n        JobStatusChangedEvent jsce = (JobStatusChangedEvent) event;\n        tEvent.addEventInfo(\"STATUS\", jsce.getStatus());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(jobId.toString());\n        tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\n        break;\n      case JOB_INFO_CHANGED:\n        JobInfoChangeEvent jice = (JobInfoChangeEvent) event;\n        tEvent.addEventInfo(\"SUBMIT_TIME\", jice.getSubmitTime());\n        tEvent.addEventInfo(\"LAUNCH_TIME\", jice.getLaunchTime());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(jobId.toString());\n        tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\n        break;\n      case JOB_INITED:\n        JobInitedEvent jie = (JobInitedEvent) event;\n        tEvent.addEventInfo(\"START_TIME\", jie.getLaunchTime());\n        tEvent.addEventInfo(\"STATUS\", jie.getStatus());\n        tEvent.addEventInfo(\"TOTAL_MAPS\", jie.getTotalMaps());\n        tEvent.addEventInfo(\"TOTAL_REDUCES\", jie.getTotalReduces());\n        tEvent.addEventInfo(\"UBERIZED\", jie.getUberized());\n        tEntity.setStartTime(jie.getLaunchTime());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(jobId.toString());\n        tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\n        break;\n      case JOB_PRIORITY_CHANGED:\n        JobPriorityChangeEvent jpce = (JobPriorityChangeEvent) event;\n        tEvent.addEventInfo(\"PRIORITY\", jpce.getPriority().toString());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(jobId.toString());\n        tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\n        break;\n      case JOB_QUEUE_CHANGED:\n        JobQueueChangeEvent jqe = (JobQueueChangeEvent) event;\n        tEvent.addEventInfo(\"QUEUE_NAMES\", jqe.getJobQueueName());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(jobId.toString());\n        tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\n        break;\n      case JOB_FAILED:\n      case JOB_KILLED:\n      case JOB_ERROR:\n        JobUnsuccessfulCompletionEvent juce =\n              (JobUnsuccessfulCompletionEvent) event;\n        tEvent.addEventInfo(\"FINISH_TIME\", juce.getFinishTime());\n        tEvent.addEventInfo(\"NUM_MAPS\", juce.getFinishedMaps());\n        tEvent.addEventInfo(\"NUM_REDUCES\", juce.getFinishedReduces());\n        tEvent.addEventInfo(\"JOB_STATUS\", juce.getStatus());\n        tEvent.addEventInfo(\"DIAGNOSTICS\", juce.getDiagnostics());\n        tEvent.addEventInfo(\"FINISHED_MAPS\", juce.getFinishedMaps());\n        tEvent.addEventInfo(\"FINISHED_REDUCES\", juce.getFinishedReduces());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(jobId.toString());\n        tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\n        break;\n      case JOB_FINISHED:\n        JobFinishedEvent jfe = (JobFinishedEvent) event;\n        tEvent.addEventInfo(\"FINISH_TIME\", jfe.getFinishTime());\n        tEvent.addEventInfo(\"NUM_MAPS\", jfe.getFinishedMaps());\n        tEvent.addEventInfo(\"NUM_REDUCES\", jfe.getFinishedReduces());\n        tEvent.addEventInfo(\"FAILED_MAPS\", jfe.getFailedMaps());\n        tEvent.addEventInfo(\"FAILED_REDUCES\", jfe.getFailedReduces());\n        tEvent.addEventInfo(\"FINISHED_MAPS\", jfe.getFinishedMaps());\n        tEvent.addEventInfo(\"FINISHED_REDUCES\", jfe.getFinishedReduces());\n        tEvent.addEventInfo(\"MAP_COUNTERS_GROUPS\",\n                countersToJSON(jfe.getMapCounters()));\n        tEvent.addEventInfo(\"REDUCE_COUNTERS_GROUPS\",\n                countersToJSON(jfe.getReduceCounters()));\n        tEvent.addEventInfo(\"TOTAL_COUNTERS_GROUPS\",\n                countersToJSON(jfe.getTotalCounters()));\n        tEvent.addEventInfo(\"JOB_STATUS\", JobState.SUCCEEDED.toString());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(jobId.toString());\n        tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\n        break;\n      case TASK_STARTED:\n        TaskStartedEvent tse = (TaskStartedEvent) event;\n        tEvent.addEventInfo(\"TASK_TYPE\", tse.getTaskType().toString());\n        tEvent.addEventInfo(\"START_TIME\", tse.getStartTime());\n        tEvent.addEventInfo(\"SPLIT_LOCATIONS\", tse.getSplitLocations());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(tse.getTaskId().toString());\n        tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\n        tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\n        break;\n      case TASK_FAILED:\n        TaskFailedEvent tfe = (TaskFailedEvent) event;\n        tEvent.addEventInfo(\"TASK_TYPE\", tfe.getTaskType().toString());\n        tEvent.addEventInfo(\"STATUS\", TaskStatus.State.FAILED.toString());\n        tEvent.addEventInfo(\"FINISH_TIME\", tfe.getFinishTime());\n        tEvent.addEventInfo(\"ERROR\", tfe.getError());\n        tEvent.addEventInfo(\"FAILED_ATTEMPT_ID\",\n                tfe.getFailedAttemptID() == null ?\n                \"\" : tfe.getFailedAttemptID().toString());\n        tEvent.addEventInfo(\"COUNTERS_GROUPS\",\n                countersToJSON(tfe.getCounters()));\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(tfe.getTaskId().toString());\n        tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\n        tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\n        break;\n      case TASK_UPDATED:\n        TaskUpdatedEvent tue = (TaskUpdatedEvent) event;\n        tEvent.addEventInfo(\"FINISH_TIME\", tue.getFinishTime());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(tue.getTaskId().toString());\n        tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\n        tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\n        break;\n      case TASK_FINISHED:\n        TaskFinishedEvent tfe2 = (TaskFinishedEvent) event;\n        tEvent.addEventInfo(\"TASK_TYPE\", tfe2.getTaskType().toString());\n        tEvent.addEventInfo(\"COUNTERS_GROUPS\",\n                countersToJSON(tfe2.getCounters()));\n        tEvent.addEventInfo(\"FINISH_TIME\", tfe2.getFinishTime());\n        tEvent.addEventInfo(\"STATUS\", TaskStatus.State.SUCCEEDED.toString());\n        tEvent.addEventInfo(\"SUCCESSFUL_TASK_ATTEMPT_ID\",\n            tfe2.getSuccessfulTaskAttemptId() == null ?\n            \"\" : tfe2.getSuccessfulTaskAttemptId().toString());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(tfe2.getTaskId().toString());\n        tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\n        tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\n        break;\n      case MAP_ATTEMPT_STARTED:\n      case CLEANUP_ATTEMPT_STARTED:\n      case REDUCE_ATTEMPT_STARTED:\n      case SETUP_ATTEMPT_STARTED:\n        TaskAttemptStartedEvent tase = (TaskAttemptStartedEvent) event;\n        tEvent.addEventInfo(\"TASK_TYPE\", tase.getTaskType().toString());\n        tEvent.addEventInfo(\"TASK_ATTEMPT_ID\",\n            tase.getTaskAttemptId().toString());\n        tEvent.addEventInfo(\"START_TIME\", tase.getStartTime());\n        tEvent.addEventInfo(\"HTTP_PORT\", tase.getHttpPort());\n        tEvent.addEventInfo(\"TRACKER_NAME\", tase.getTrackerName());\n        tEvent.addEventInfo(\"TASK_TYPE\", tase.getTaskType().toString());\n        tEvent.addEventInfo(\"SHUFFLE_PORT\", tase.getShufflePort());\n        tEvent.addEventInfo(\"CONTAINER_ID\", tase.getContainerId() == null ?\n            \"\" : tase.getContainerId().toString());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(tase.getTaskId().toString());\n        tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\n        tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\n        break;\n      case MAP_ATTEMPT_FAILED:\n      case CLEANUP_ATTEMPT_FAILED:\n      case REDUCE_ATTEMPT_FAILED:\n      case SETUP_ATTEMPT_FAILED:\n      case MAP_ATTEMPT_KILLED:\n      case CLEANUP_ATTEMPT_KILLED:\n      case REDUCE_ATTEMPT_KILLED:\n      case SETUP_ATTEMPT_KILLED:\n        TaskAttemptUnsuccessfulCompletionEvent tauce =\n                (TaskAttemptUnsuccessfulCompletionEvent) event;\n        tEvent.addEventInfo(\"TASK_TYPE\", tauce.getTaskType().toString());\n        tEvent.addEventInfo(\"TASK_ATTEMPT_ID\",\n            tauce.getTaskAttemptId() == null ?\n            \"\" : tauce.getTaskAttemptId().toString());\n        tEvent.addEventInfo(\"FINISH_TIME\", tauce.getFinishTime());\n        tEvent.addEventInfo(\"ERROR\", tauce.getError());\n        tEvent.addEventInfo(\"STATUS\", tauce.getTaskStatus());\n        tEvent.addEventInfo(\"HOSTNAME\", tauce.getHostname());\n        tEvent.addEventInfo(\"PORT\", tauce.getPort());\n        tEvent.addEventInfo(\"RACK_NAME\", tauce.getRackName());\n        tEvent.addEventInfo(\"SHUFFLE_FINISH_TIME\", tauce.getFinishTime());\n        tEvent.addEventInfo(\"SORT_FINISH_TIME\", tauce.getFinishTime());\n        tEvent.addEventInfo(\"MAP_FINISH_TIME\", tauce.getFinishTime());\n        tEvent.addEventInfo(\"COUNTERS_GROUPS\",\n                countersToJSON(tauce.getCounters()));\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(tauce.getTaskId().toString());\n        tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\n        tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\n        break;\n      case MAP_ATTEMPT_FINISHED:\n        MapAttemptFinishedEvent mafe = (MapAttemptFinishedEvent) event;\n        tEvent.addEventInfo(\"TASK_TYPE\", mafe.getTaskType().toString());\n        tEvent.addEventInfo(\"FINISH_TIME\", mafe.getFinishTime());\n        tEvent.addEventInfo(\"STATUS\", mafe.getTaskStatus());\n        tEvent.addEventInfo(\"STATE\", mafe.getState());\n        tEvent.addEventInfo(\"MAP_FINISH_TIME\", mafe.getMapFinishTime());\n        tEvent.addEventInfo(\"COUNTERS_GROUPS\",\n                countersToJSON(mafe.getCounters()));\n        tEvent.addEventInfo(\"HOSTNAME\", mafe.getHostname());\n        tEvent.addEventInfo(\"PORT\", mafe.getPort());\n        tEvent.addEventInfo(\"RACK_NAME\", mafe.getRackName());\n        tEvent.addEventInfo(\"ATTEMPT_ID\", mafe.getAttemptId() == null ?\n            \"\" : mafe.getAttemptId().toString());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(mafe.getTaskId().toString());\n        tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\n        tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\n        break;\n      case REDUCE_ATTEMPT_FINISHED:\n        ReduceAttemptFinishedEvent rafe = (ReduceAttemptFinishedEvent) event;\n        tEvent.addEventInfo(\"TASK_TYPE\", rafe.getTaskType().toString());\n        tEvent.addEventInfo(\"ATTEMPT_ID\", rafe.getAttemptId() == null ?\n            \"\" : rafe.getAttemptId().toString());\n        tEvent.addEventInfo(\"FINISH_TIME\", rafe.getFinishTime());\n        tEvent.addEventInfo(\"STATUS\", rafe.getTaskStatus());\n        tEvent.addEventInfo(\"STATE\", rafe.getState());\n        tEvent.addEventInfo(\"SHUFFLE_FINISH_TIME\", rafe.getShuffleFinishTime());\n        tEvent.addEventInfo(\"SORT_FINISH_TIME\", rafe.getSortFinishTime());\n        tEvent.addEventInfo(\"COUNTERS_GROUPS\",\n                countersToJSON(rafe.getCounters()));\n        tEvent.addEventInfo(\"HOSTNAME\", rafe.getHostname());\n        tEvent.addEventInfo(\"PORT\", rafe.getPort());\n        tEvent.addEventInfo(\"RACK_NAME\", rafe.getRackName());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(rafe.getTaskId().toString());\n        tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\n        tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\n        break;\n      case SETUP_ATTEMPT_FINISHED:\n      case CLEANUP_ATTEMPT_FINISHED:\n        TaskAttemptFinishedEvent tafe = (TaskAttemptFinishedEvent) event;\n        tEvent.addEventInfo(\"TASK_TYPE\", tafe.getTaskType().toString());\n        tEvent.addEventInfo(\"ATTEMPT_ID\", tafe.getAttemptId() == null ?\n            \"\" : tafe.getAttemptId().toString());\n        tEvent.addEventInfo(\"FINISH_TIME\", tafe.getFinishTime());\n        tEvent.addEventInfo(\"STATUS\", tafe.getTaskStatus());\n        tEvent.addEventInfo(\"STATE\", tafe.getState());\n        tEvent.addEventInfo(\"COUNTERS_GROUPS\",\n                countersToJSON(tafe.getCounters()));\n        tEvent.addEventInfo(\"HOSTNAME\", tafe.getHostname());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(tafe.getTaskId().toString());\n        tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\n        tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\n        break;\n      case AM_STARTED:\n        AMStartedEvent ase = (AMStartedEvent) event;\n        tEvent.addEventInfo(\"APPLICATION_ATTEMPT_ID\",\n                ase.getAppAttemptId() == null ?\n                \"\" : ase.getAppAttemptId().toString());\n        tEvent.addEventInfo(\"CONTAINER_ID\", ase.getContainerId() == null ?\n                \"\" : ase.getContainerId().toString());\n        tEvent.addEventInfo(\"NODE_MANAGER_HOST\", ase.getNodeManagerHost());\n        tEvent.addEventInfo(\"NODE_MANAGER_PORT\", ase.getNodeManagerPort());\n        tEvent.addEventInfo(\"NODE_MANAGER_HTTP_PORT\",\n                ase.getNodeManagerHttpPort());\n        tEvent.addEventInfo(\"START_TIME\", ase.getStartTime());\n        tEvent.addEventInfo(\"SUBMIT_TIME\", ase.getSubmitTime());\n        tEntity.addEvent(tEvent);\n        tEntity.setEntityId(jobId.toString());\n        tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\n        break;\n      default:\n        break;\n    }\n\n    try {\n      TimelinePutResponse response = timelineClient.putEntities(tEntity);\n      List<TimelinePutResponse.TimelinePutError> errors = response.getErrors();\n      if (errors.size() == 0) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Timeline entities are successfully put in event \" + event\n              .getEventType());\n        }\n      } else {\n        for (TimelinePutResponse.TimelinePutError error : errors) {\n          LOG.error(\n              \"Error when publishing entity [\" + error.getEntityType() + \",\"\n                  + error.getEntityId() + \"], server side error code: \"\n                  + error.getErrorCode());\n        }\n      }\n    } catch (YarnException | IOException | ClientHandlerException ex) {\n      LOG.error(\"Error putting entity \" + tEntity.getEntityId() + \" to Timeline\"\n          + \"Server\", ex);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.processEventForJobSummary": "  public void processEventForJobSummary(HistoryEvent event, JobSummary summary, \n      JobId jobId) {\n    // context.getJob could be used for some of this info as well.\n    switch (event.getEventType()) {\n    case JOB_SUBMITTED:\n      JobSubmittedEvent jse = (JobSubmittedEvent) event;\n      summary.setUser(jse.getUserName());\n      summary.setQueue(jse.getJobQueueName());\n      summary.setJobSubmitTime(jse.getSubmitTime());\n      summary.setJobName(jse.getJobName());\n      break;\n    case NORMALIZED_RESOURCE:\n      NormalizedResourceEvent normalizedResourceEvent = \n            (NormalizedResourceEvent) event;\n      if (normalizedResourceEvent.getTaskType() == TaskType.MAP) {\n        summary.setResourcesPerMap(normalizedResourceEvent.getMemory());\n      } else if (normalizedResourceEvent.getTaskType() == TaskType.REDUCE) {\n        summary.setResourcesPerReduce(normalizedResourceEvent.getMemory());\n      }\n      break;  \n    case JOB_INITED:\n      JobInitedEvent jie = (JobInitedEvent) event;\n      summary.setJobLaunchTime(jie.getLaunchTime());\n      break;\n    case MAP_ATTEMPT_STARTED:\n      TaskAttemptStartedEvent mtase = (TaskAttemptStartedEvent) event;\n      if (summary.getFirstMapTaskLaunchTime() == 0)\n        summary.setFirstMapTaskLaunchTime(mtase.getStartTime());\n      break;\n    case REDUCE_ATTEMPT_STARTED:\n      TaskAttemptStartedEvent rtase = (TaskAttemptStartedEvent) event;\n      if (summary.getFirstReduceTaskLaunchTime() == 0)\n        summary.setFirstReduceTaskLaunchTime(rtase.getStartTime());\n      break;\n    case JOB_FINISHED:\n      JobFinishedEvent jfe = (JobFinishedEvent) event;\n      summary.setJobFinishTime(jfe.getFinishTime());\n      summary.setNumFinishedMaps(jfe.getFinishedMaps());\n      summary.setNumFailedMaps(jfe.getFailedMaps());\n      summary.setNumFinishedReduces(jfe.getFinishedReduces());\n      summary.setNumFailedReduces(jfe.getFailedReduces());\n      if (summary.getJobStatus() == null)\n        summary\n            .setJobStatus(org.apache.hadoop.mapreduce.JobStatus.State.SUCCEEDED\n                .toString());\n      // TODO JOB_FINISHED does not have state. Effectively job history does not\n      // have state about the finished job.\n      setSummarySlotSeconds(summary, jfe.getTotalCounters());\n      break;\n    case JOB_FAILED:\n    case JOB_KILLED:\n      JobUnsuccessfulCompletionEvent juce = (JobUnsuccessfulCompletionEvent) event;\n      summary.setJobStatus(juce.getStatus());\n      summary.setNumFinishedMaps(context.getJob(jobId).getTotalMaps());\n      summary.setNumFinishedReduces(context.getJob(jobId).getTotalReduces());\n      summary.setJobFinishTime(juce.getFinishTime());\n      setSummarySlotSeconds(summary, context.getJob(jobId).getAllCounters());\n      break;\n    default:\n      break;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.run": "    public void run() {\n      LOG.debug(\"In flush timer task\");\n      synchronized (lock) {\n        try {\n          if (!metaInfo.isTimerShutDown() && shouldRun)\n            metaInfo.flush();\n        } catch (IOException e) {\n          ioe = e;\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.isTimerShutDown": "    boolean isTimerShutDown() {\n      return isTimerShutDown;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.flush": "    void flush() throws IOException {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Flushing \" + toString());\n      }\n      synchronized (lock) {\n        if (numUnflushedCompletionEvents != 0) { // skipped timer cancel.\n          writer.flush();\n          numUnflushedCompletionEvents = 0;\n          resetFlushTimer();\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.jobhistory.JobIndexInfo.getJobId": "  public JobId getJobId() {\n    return jobId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.jobhistory.JobIndexInfo.getNumReduces": "  public int getNumReduces() {\n    return numReduces;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.jobhistory.JobIndexInfo.getJobStartTime": "  public long getJobStartTime() {\n      return jobStartTime;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.jobhistory.JobIndexInfo.getFinishTime": "  public long getFinishTime() {\n    return finishTime;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.jobhistory.JobIndexInfo.getJobStatus": "  public String getJobStatus() {\n    return jobStatus;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.jobhistory.JobIndexInfo.getSubmitTime": "  public long getSubmitTime() {\n    return submitTime;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.jobhistory.JobIndexInfo.getNumMaps": "  public int getNumMaps() {\n    return numMaps;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobSubmittedEvent.getJobQueueName": "  public String getJobQueueName() {\n    if (datum.getJobQueueName() != null) {\n      return datum.getJobQueueName().toString();\n    }\n    return null;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobFinishedEvent.getFinishedReduces": "  public int getFinishedReduces() { return finishedReduces; }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent.getStatus": "  public String getStatus() { return datum.getJobStatus().toString(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent.getFinishedMaps": "  public int getFinishedMaps() { return datum.getFinishedMaps(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobQueueChangeEvent.getJobQueueName": "  public String getJobQueueName() {\n    if (datum.jobQueueName != null) {\n      return datum.jobQueueName.toString();\n    }\n    return null;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobFinishedEvent.getFinishedMaps": "  public int getFinishedMaps() { return finishedMaps; }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobSubmittedEvent.getSubmitTime": "  public long getSubmitTime() { return datum.getSubmitTime(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent.getFinishTime": "  public long getFinishTime() { return datum.getFinishTime(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEvent.getJobID": "  public JobId getJobID() {\n    return jobID;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent.getFinishedReduces": "  public int getFinishedReduces() { return datum.getFinishedReduces(); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEvent.getHistoryEvent": "  public HistoryEvent getHistoryEvent() {\n    return historyEvent;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobFinishedEvent.getFinishTime": "  public long getFinishTime() { return finishTime; }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobInitedEvent.getLaunchTime": "  public long getLaunchTime() { return datum.getLaunchTime(); }"
        },
        "bug_report": {
            "Title": "ArrayIndexOutOfBoundsException occurs when the length of the job name is equal to mapreduce.jobhistory.jobname.limit",
            "Description": "Job history entry missing when JOB name is of {{mapreduce.jobhistory.jobname.limit}} character\n\n{noformat}\n2016-05-10 06:51:00,674 DEBUG [Thread-73] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Interrupting Event Handling thread\n2016-05-10 06:51:00,674 DEBUG [Thread-73] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Waiting for Event Handling thread to complete\n2016-05-10 06:51:00,674 ERROR [eventHandlingThread] org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[eventHandlingThread,5,main] threw an Exception.\njava.lang.ArrayIndexOutOfBoundsException: 50\n\tat org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.trimURLEncodedString(FileNameIndexUtils.java:326)\n\tat org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.getDoneFileName(FileNameIndexUtils.java:86)\n\tat org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.processDoneFiles(JobHistoryEventHandler.java:1147)\n\tat org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:635)\n\tat org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$1.run(JobHistoryEventHandler.java:341)\n\tat java.lang.Thread.run(Thread.java:745)\n2016-05-10 06:51:00,675 DEBUG [Thread-73] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Shutting down timer for Job MetaInfo for job_1462840033869_0009 history file hdfs://hacluster:9820/staging-dir/dsperf/.staging/job_1462840033869_0009/job_1462840033869_0009_1.jhist\n2016-05-10 06:51:00,675 DEBUG [Thread-73] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Shutting down timer Job MetaInfo for job_1462840033869_0009 history file hdfs://hacluster:9820/staging-dir/dsperf/.staging/job_1462840033869_0009/job_1462840033869_0009_1.jhist\n2016-05-10 06:51:00,676 DEBUG [Thread-73] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Closing Writer\n{noformat}\n\nLooks like 50 character check is going wrong"
        }
    },
    {
        "filename": "MAPREDUCE-5912.json",
        "creation_time": "2014-06-04T15:37:03.000+0000",
        "stack_trace": "```\nException running child : java.lang.IllegalArgumentException: Pathname /c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out from c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out is not a valid DFS filename.\n       at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:187)\n       at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:101)\n       at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1024)\n       at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1020)\n       at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n       at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1020)\n       at org.apache.hadoop.mapred.Task.calculateOutputSize(Task.java:1124)\n       at org.apache.hadoop.mapred.Task.sendLastUpdate(Task.java:1102)\n       at org.apache.hadoop.mapred.Task.done(Task.java:1048)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystemLinkResolver.resolve": "  public T resolve(final FileSystem filesys, final Path path)\n      throws IOException {\n    int count = 0;\n    T in = null;\n    Path p = path;\n    // Assumes path belongs to this FileSystem.\n    // Callers validate this by passing paths through FileSystem#checkPath\n    FileSystem fs = filesys;\n    for (boolean isLink = true; isLink;) {\n      try {\n        in = doCall(p);\n        isLink = false;\n      } catch (UnresolvedLinkException e) {\n        if (!filesys.resolveSymlinks) {\n          throw new IOException(\"Path \" + path + \" contains a symlink\"\n              + \" and symlink resolution is disabled (\"\n              + CommonConfigurationKeys.FS_CLIENT_RESOLVE_REMOTE_SYMLINKS_KEY\n              + \").\", e);\n        }\n        if (count++ > FsConstants.MAX_PATH_LINKS) {\n          throw new IOException(\"Possible cyclic loop while \" +\n                                \"following symbolic link \" + path);\n        }\n        // Resolve the first unresolved path component\n        p = FSLinkResolver.qualifySymlinkTarget(fs.getUri(), p,\n            filesys.resolveLink(p));\n        fs = FileSystem.getFSofPath(p, filesys.getConf());\n        // Have to call next if it's a new FS\n        if (!fs.equals(filesys)) {\n          return next(fs, p);\n        }\n        // Else, we keep resolving with this filesystem\n      }\n    }\n    // Successful call, path was fully resolved\n    return in;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystemLinkResolver.next": "  abstract public T next(final FileSystem fs, final Path p) throws IOException;\n\n  /**\n   * Attempt calling overridden {@link #doCall(Path)} method with",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystemLinkResolver.doCall": "  abstract public T doCall(final Path p) throws IOException,\n      UnresolvedLinkException;\n\n  /**\n   * Calls the abstract FileSystem call equivalent to the specialized subclass\n   * implementation in {@link #doCall(Path)}. This is used when retrying the",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.equals": "      public boolean equals(Object obj) {\n        if (obj == this) {\n          return true;\n        }\n        if (obj != null && obj instanceof Key) {\n          Key that = (Key)obj;\n          return isEqual(this.scheme, that.scheme)\n                 && isEqual(this.authority, that.authority)\n                 && isEqual(this.ugi, that.ugi)\n                 && (this.unique == that.unique);\n        }\n        return false;        \n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.isEqual": "      static boolean isEqual(Object a, Object b) {\n        return a == b || (a != null && a.equals(b));        \n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getFSofPath": "  protected static FileSystem getFSofPath(final Path absOrFqPath,\n      final Configuration conf)\n      throws UnsupportedFileSystemException, IOException {\n    absOrFqPath.checkNotSchemeWithRelative();\n    absOrFqPath.checkNotRelative();\n\n    // Uses the default file system if not fully qualified\n    return get(absOrFqPath.toUri(), conf);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.get": "    FileSystem get(URI uri, Configuration conf) throws IOException{\n      Key key = new Key(uri, conf);\n      return getInternal(uri, conf, key);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FSLinkResolver.qualifySymlinkTarget": "  public static Path qualifySymlinkTarget(final URI pathURI,\n      Path pathWithLink, Path target) {\n    // NB: makeQualified uses the target's scheme and authority, if\n    // specified, and the scheme and authority of pathURI, if not.\n    final URI targetUri = target.toUri();\n    final String scheme = targetUri.getScheme();\n    final String auth = targetUri.getAuthority();\n    return (scheme == null && auth == null) ? target.makeQualified(pathURI,\n        pathWithLink.getParent()) : target;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getUri": "  public abstract URI getUri();\n  \n  /**\n   * Return a canonicalized form of this FileSystem's URI.\n   * \n   * The default implementation simply calls {@link #canonicalizeUri(URI)}"
        },
        "bug_report": {
            "Title": "Task.calculateOutputSize does not handle Windows files after MAPREDUCE-5196",
            "Description": "{code}\n@@ -1098,8 +1120,8 @@ private long calculateOutputSize() throws IOException {\n     if (isMapTask() && conf.getNumReduceTasks() > 0) {\n       try {\n         Path mapOutput =  mapOutputFile.getOutputFile();\n-        FileSystem localFS = FileSystem.getLocal(conf);\n-        return localFS.getFileStatus(mapOutput).getLen();\n+        FileSystem fs = mapOutput.getFileSystem(conf);\n+        return fs.getFileStatus(mapOutput).getLen();\n       } catch (IOException e) {\n         LOG.warn (\"Could not find output size \" , e);\n       }\n{code}\n\ncauses Windows local output files to be routed through HDFS:\n\n{code}\n2014-06-02 00:14:53,891 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.IllegalArgumentException: Pathname /c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out from c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out is not a valid DFS filename.\n       at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:187)\n       at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:101)\n       at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1024)\n       at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1020)\n       at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n       at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1020)\n       at org.apache.hadoop.mapred.Task.calculateOutputSize(Task.java:1124)\n       at org.apache.hadoop.mapred.Task.sendLastUpdate(Task.java:1102)\n       at org.apache.hadoop.mapred.Task.done(Task.java:1048)\n{code}\n"
        }
    },
    {
        "filename": "MAPREDUCE-5952.json",
        "creation_time": "2014-06-30T23:08:52.000+0000",
        "stack_trace": "```\njava.io.FileNotFoundException: File /Users/gshegalov/workspace/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapreduce.v2.TestMRJobs/org.apache.hadoop.mapreduce.v2.TestMRJobs-localDir-nm-2_3/usercache/gshegalov/appcache/application_1404164272885_0001/output/file.out.index does not exist\n  at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:517)\n  at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:726)\n  at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:507)\n  at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)                      \n  at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)                          \n  at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:334)    \n  at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:504)\n  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.renameMapOutputForReduce(LocalContainerLauncher.java:471)\n  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:370)\n  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:292)\n  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:178)\n  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:221)\n  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)    \n  at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)         \n  at java.util.concurrent.FutureTask.run(FutureTask.java:138)                       \n  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\n  at java.lang.Thread.run(Thread.java:695)         \n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus": "  private FileStatus deprecatedGetFileStatus(Path f) throws IOException {\n    File path = pathToFile(f);\n    if (path.exists()) {\n      return new DeprecatedRawLocalFileStatus(pathToFile(f),\n          getDefaultBlockSize(f), this);\n    } else {\n      throw new FileNotFoundException(\"File \" + f + \" does not exist\");\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.RawLocalFileSystem.pathToFile": "  public File pathToFile(Path path) {\n    checkPath(path);\n    if (!path.isAbsolute()) {\n      path = new Path(getWorkingDirectory(), path);\n    }\n    return new File(path.toUri().getPath());\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal": "  private FileStatus getFileLinkStatusInternal(final Path f,\n      boolean dereference) throws IOException {\n    if (!useDeprecatedFileStatus) {\n      return getNativeFileLinkStatus(f, dereference);\n    } else if (dereference) {\n      return deprecatedGetFileStatus(f);\n    } else {\n      return deprecatedGetFileLinkStatusInternal(f);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.RawLocalFileSystem.getNativeFileLinkStatus": "  private FileStatus getNativeFileLinkStatus(final Path f,\n      boolean dereference) throws IOException {\n    checkPath(f);\n    Stat stat = new Stat(f, getDefaultBlockSize(f), dereference, this);\n    FileStatus status = stat.getFileStatus();\n    return status;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal": "  private FileStatus deprecatedGetFileLinkStatusInternal(final Path f)\n      throws IOException {\n    String target = FileUtil.readLink(new File(f.toString()));\n\n    try {\n      FileStatus fs = getFileStatus(f);\n      // If f refers to a regular file or directory\n      if (target.isEmpty()) {\n        return fs;\n      }\n      // Otherwise f refers to a symlink\n      return new FileStatus(fs.getLen(),\n          false,\n          fs.getReplication(),\n          fs.getBlockSize(),\n          fs.getModificationTime(),\n          fs.getAccessTime(),\n          fs.getPermission(),\n          fs.getOwner(),\n          fs.getGroup(),\n          new Path(target),\n          f);\n    } catch (FileNotFoundException e) {\n      /* The exists method in the File class returns false for dangling\n       * links so we can get a FileNotFoundException for links that exist.\n       * It's also possible that we raced with a delete of the link. Use\n       * the readBasicFileAttributes method in java.nio.file.attributes\n       * when available.\n       */\n      if (!target.isEmpty()) {\n        return new FileStatus(0, false, 0, 0, 0, 0, FsPermission.getDefault(),\n            \"\", \"\", new Path(target), f);\n      }\n      // f refers to a file or directory that does not exist\n      throw e;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus": "  public FileStatus getFileStatus(Path f) throws IOException {\n    return getFileLinkStatusInternal(f, true);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileUtil.copy": "  private static boolean copy(FileSystem srcFS, FileStatus srcStatus,\n                              File dst, boolean deleteSource,\n                              Configuration conf) throws IOException {\n    Path src = srcStatus.getPath();\n    if (srcStatus.isDirectory()) {\n      if (!dst.mkdirs()) {\n        return false;\n      }\n      FileStatus contents[] = srcFS.listStatus(src);\n      for (int i = 0; i < contents.length; i++) {\n        copy(srcFS, contents[i],\n             new File(dst, contents[i].getPath().getName()),\n             deleteSource, conf);\n      }\n    } else {\n      InputStream in = srcFS.open(src);\n      IOUtils.copyBytes(in, new FileOutputStream(dst), conf);\n    }\n    if (deleteSource) {\n      return srcFS.delete(src, true);\n    } else {\n      return true;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileUtil.listFiles": "  public static File[] listFiles(File dir) throws IOException {\n    File[] files = dir.listFiles();\n    if(files == null) {\n      throw new IOException(\"Invalid directory or I/O error occurred for dir: \"\n                + dir.toString());\n    }\n    return files;\n  }  ",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileUtil.fullyDelete": "  public static void fullyDelete(FileSystem fs, Path dir) \n  throws IOException {\n    fs.delete(dir, true);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileUtil.checkDependencies": "  private static void checkDependencies(FileSystem srcFS, \n                                        Path src, \n                                        FileSystem dstFS, \n                                        Path dst)\n                                        throws IOException {\n    if (srcFS == dstFS) {\n      String srcq = src.makeQualified(srcFS).toString() + Path.SEPARATOR;\n      String dstq = dst.makeQualified(dstFS).toString() + Path.SEPARATOR;\n      if (dstq.startsWith(srcq)) {\n        if (srcq.length() == dstq.length()) {\n          throw new IOException(\"Cannot copy \" + src + \" to itself.\");\n        } else {\n          throw new IOException(\"Cannot copy \" + src + \" to its subdirectory \" +\n                                dst);\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileUtil.checkDest": "  private static Path checkDest(String srcName, FileSystem dstFS, Path dst,\n      boolean overwrite) throws IOException {\n    if (dstFS.exists(dst)) {\n      FileStatus sdst = dstFS.getFileStatus(dst);\n      if (sdst.isDirectory()) {\n        if (null == srcName) {\n          throw new IOException(\"Target \" + dst + \" is a directory\");\n        }\n        return checkDest(null, dstFS, new Path(dst, srcName), overwrite);\n      } else if (!overwrite) {\n        throw new IOException(\"Target \" + dst + \" already exists\");\n      }\n    }\n    return dst;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.RawLocalFileSystem.rename": "  public boolean rename(Path src, Path dst) throws IOException {\n    // Attempt rename using Java API.\n    File srcFile = pathToFile(src);\n    File dstFile = pathToFile(dst);\n    if (srcFile.renameTo(dstFile)) {\n      return true;\n    }\n\n    // Enforce POSIX rename behavior that a source directory replaces an existing\n    // destination if the destination is an empty directory.  On most platforms,\n    // this is already handled by the Java API call above.  Some platforms\n    // (notably Windows) do not provide this behavior, so the Java API call above\n    // fails.  Delete destination and attempt rename again.\n    if (this.exists(dst)) {\n      FileStatus sdst = this.getFileStatus(dst);\n      if (sdst.isDirectory() && dstFile.list().length == 0) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Deleting empty destination and renaming \" + src + \" to \" +\n            dst);\n        }\n        if (this.delete(dst, false) && srcFile.renameTo(dstFile)) {\n          return true;\n        }\n      }\n    }\n\n    // The fallback behavior accomplishes the rename by a full copy.\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Falling through to a copy of \" + src + \" to \" + dst);\n    }\n    return FileUtil.copy(this, src, this, dst, true, getConf());\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.RawLocalFileSystem.delete": "  public boolean delete(Path p, boolean recursive) throws IOException {\n    File f = pathToFile(p);\n    if (f.isFile()) {\n      return f.delete();\n    } else if (!recursive && f.isDirectory() && \n        (FileUtil.listFiles(f).length != 0)) {\n      throw new IOException(\"Directory \" + f.toString() + \" is not empty\");\n    }\n    return FileUtil.fullyDelete(f);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.ChecksumFileSystem.rename": "  public boolean rename(Path src, Path dst) throws IOException {\n    if (fs.isDirectory(src)) {\n      return fs.rename(src, dst);\n    } else {\n      if (fs.isDirectory(dst)) {\n        dst = new Path(dst, src.getName());\n      }\n\n      boolean value = fs.rename(src, dst);\n      if (!value)\n        return false;\n\n      Path srcCheckFile = getChecksumFile(src);\n      Path dstCheckFile = getChecksumFile(dst);\n      if (fs.exists(srcCheckFile)) { //try to rename checksum\n        value = fs.rename(srcCheckFile, dstCheckFile);\n      } else if (fs.exists(dstCheckFile)) {\n        // no src checksum, so remove dst checksum\n        value = fs.delete(dstCheckFile, true); \n      }\n\n      return value;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.ChecksumFileSystem.delete": "  public boolean delete(Path f, boolean recursive) throws IOException{\n    FileStatus fstatus = null;\n    try {\n      fstatus = fs.getFileStatus(f);\n    } catch(FileNotFoundException e) {\n      return false;\n    }\n    if (fstatus.isDirectory()) {\n      //this works since the crcs are in the same\n      //directories and the files. so we just delete\n      //everything in the underlying filesystem\n      return fs.delete(f, recursive);\n    } else {\n      Path checkFile = getChecksumFile(f);\n      if (fs.exists(checkFile)) {\n        fs.delete(checkFile, true);\n      }\n      return fs.delete(f, true);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.ChecksumFileSystem.getChecksumFile": "  public Path getChecksumFile(Path file) {\n    return new Path(file.getParent(), \".\" + file.getName() + \".crc\");\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.LocalContainerLauncher.renameMapOutputForReduce": "    private MapOutputFile renameMapOutputForReduce(JobConf conf,\n        TaskAttemptId mapId, MapOutputFile subMapOutputFile) throws IOException {\n      FileSystem localFs = FileSystem.getLocal(conf);\n      // move map output to reduce input\n      Path mapOut = subMapOutputFile.getOutputFile();\n      FileStatus mStatus = localFs.getFileStatus(mapOut);      \n      Path reduceIn = subMapOutputFile.getInputFileForWrite(\n          TypeConverter.fromYarn(mapId).getTaskID(), mStatus.getLen());\n      Path mapOutIndex = new Path(mapOut.toString() + \".index\");\n      Path reduceInIndex = new Path(reduceIn.toString() + \".index\");\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Renaming map output file for task attempt \"\n            + mapId.toString() + \" from original location \" + mapOut.toString()\n            + \" to destination \" + reduceIn.toString());\n      }\n      if (!localFs.mkdirs(reduceIn.getParent())) {\n        throw new IOException(\"Mkdirs failed to create \"\n            + reduceIn.getParent().toString());\n      }\n      if (!localFs.rename(mapOut, reduceIn))\n        throw new IOException(\"Couldn't rename \" + mapOut);\n      if (!localFs.rename(mapOutIndex, reduceInIndex))\n        throw new IOException(\"Couldn't rename \" + mapOutIndex);\n      \n      return new RenamedMapOutputFile(reduceIn);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.LocalContainerLauncher.getInputFileForWrite": "    public Path getInputFileForWrite(TaskID mapId, long size)\n        throws IOException {\n      throw new UnsupportedOperationException();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.LocalContainerLauncher.getOutputFile": "    public Path getOutputFile() throws IOException {\n      return path;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.LocalContainerLauncher.runSubtask": "    private void runSubtask(org.apache.hadoop.mapred.Task task,\n                            final TaskType taskType,\n                            TaskAttemptId attemptID,\n                            final int numMapTasks,\n                            boolean renameOutputs,\n                            Map<TaskAttemptID, MapOutputFile> localMapFiles)\n    throws RuntimeException, IOException {\n      org.apache.hadoop.mapred.TaskAttemptID classicAttemptID =\n          TypeConverter.fromYarn(attemptID);\n\n      try {\n        JobConf conf = new JobConf(getConfig());\n        conf.set(JobContext.TASK_ID, task.getTaskID().toString());\n        conf.set(JobContext.TASK_ATTEMPT_ID, classicAttemptID.toString());\n        conf.setBoolean(JobContext.TASK_ISMAP, (taskType == TaskType.MAP));\n        conf.setInt(JobContext.TASK_PARTITION, task.getPartition());\n        conf.set(JobContext.ID, task.getJobID().toString());\n\n        // Use the AM's local dir env to generate the intermediate step \n        // output files\n        String[] localSysDirs = StringUtils.getTrimmedStrings(\n            System.getenv(Environment.LOCAL_DIRS.name()));\n        conf.setStrings(MRConfig.LOCAL_DIR, localSysDirs);\n        LOG.info(MRConfig.LOCAL_DIR + \" for uber task: \"\n            + conf.get(MRConfig.LOCAL_DIR));\n\n        // mark this as an uberized subtask so it can set task counter\n        // (longer-term/FIXME:  could redefine as job counter and send\n        // \"JobCounterEvent\" to JobImpl on [successful] completion of subtask;\n        // will need new Job state-machine transition and JobImpl jobCounters\n        // map to handle)\n        conf.setBoolean(\"mapreduce.task.uberized\", true);\n\n        // META-FIXME: do we want the extra sanity-checking (doneWithMaps,\n        // etc.), or just assume/hope the state machine(s) and uber-AM work\n        // as expected?\n        if (taskType == TaskType.MAP) {\n          if (doneWithMaps) {\n            LOG.error(\"CONTAINER_REMOTE_LAUNCH contains a map task (\"\n                      + attemptID + \"), but should be finished with maps\");\n            throw new RuntimeException();\n          }\n\n          MapTask map = (MapTask)task;\n          map.setConf(conf);\n\n          map.run(conf, umbilical);\n\n          if (renameOutputs) {\n            MapOutputFile renamed = renameMapOutputForReduce(conf, attemptID,\n                map.getMapOutputFile());\n            localMapFiles.put(classicAttemptID, renamed);\n          }\n          relocalize();\n\n          if (++finishedSubMaps == numMapTasks) {\n            doneWithMaps = true;\n          }\n\n        } else /* TaskType.REDUCE */ {\n\n          if (!doneWithMaps) {\n            // check if event-queue empty?  whole idea of counting maps vs. \n            // checking event queue is a tad wacky...but could enforce ordering\n            // (assuming no \"lost events\") at LocalMRAppMaster [CURRENT BUG(?): \n            // doesn't send reduce event until maps all done]\n            LOG.error(\"CONTAINER_REMOTE_LAUNCH contains a reduce task (\"\n                      + attemptID + \"), but not yet finished with maps\");\n            throw new RuntimeException();\n          }\n\n          // a.k.a. \"mapreduce.jobtracker.address\" in LocalJobRunner:\n          // set framework name to local to make task local\n          conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\n          conf.set(MRConfig.MASTER_ADDRESS, \"local\");  // bypass shuffle\n\n          ReduceTask reduce = (ReduceTask)task;\n          reduce.setLocalMapFiles(localMapFiles);\n          reduce.setConf(conf);          \n\n          reduce.run(conf, umbilical);\n          relocalize();\n        }\n\n      } catch (FSError e) {\n        LOG.fatal(\"FSError from child\", e);\n        // umbilical:  MRAppMaster creates (taskAttemptListener), passes to us\n        umbilical.fsError(classicAttemptID, e.getMessage());\n        throw new RuntimeException();\n\n      } catch (Exception exception) {\n        LOG.warn(\"Exception running local (uberized) 'child' : \"\n            + StringUtils.stringifyException(exception));\n        try {\n          if (task != null) {\n            // do cleanup for the task\n            task.taskCleanup(umbilical);\n          }\n        } catch (Exception e) {\n          LOG.info(\"Exception cleaning up: \"\n              + StringUtils.stringifyException(e));\n        }\n        // Report back any failures, for diagnostic purposes\n        umbilical.reportDiagnosticInfo(classicAttemptID, \n            StringUtils.stringifyException(exception));\n        throw new RuntimeException();\n\n      } catch (Throwable throwable) {\n        LOG.fatal(\"Error running local (uberized) 'child' : \"\n            + StringUtils.stringifyException(throwable));\n        Throwable tCause = throwable.getCause();\n        String cause = (tCause == null)\n            ? throwable.getMessage()\n                : StringUtils.stringifyException(tCause);\n            umbilical.fatalError(classicAttemptID, cause);\n        throw new RuntimeException();\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.LocalContainerLauncher.run": "            public void run() {\n              runTask(launchEv, localMapFiles);\n            }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.LocalContainerLauncher.relocalize": "    private void relocalize() {\n      File[] curLocalFiles = curDir.listFiles();\n      for (int j = 0; j < curLocalFiles.length; ++j) {\n        if (!localizedFiles.contains(curLocalFiles[j])) {\n          // found one that wasn't there before:  delete it\n          boolean deleted = false;\n          try {\n            if (curFC != null) {\n              // this is recursive, unlike File delete():\n              deleted = curFC.delete(new Path(curLocalFiles[j].getName()),true);\n            }\n          } catch (IOException e) {\n            deleted = false;\n          }\n          if (!deleted) {\n            LOG.warn(\"Unable to delete unexpected local file/dir \"\n                + curLocalFiles[j].getName() + \": insufficient permissions?\");\n          }\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.LocalContainerLauncher.runTask": "    private void runTask(ContainerRemoteLaunchEvent launchEv,\n        Map<TaskAttemptID, MapOutputFile> localMapFiles) {\n      TaskAttemptId attemptID = launchEv.getTaskAttemptID(); \n\n      Job job = context.getAllJobs().get(attemptID.getTaskId().getJobId());\n      int numMapTasks = job.getTotalMaps();\n      int numReduceTasks = job.getTotalReduces();\n\n      // YARN (tracking) Task:\n      org.apache.hadoop.mapreduce.v2.app.job.Task ytask =\n          job.getTask(attemptID.getTaskId());\n      // classic mapred Task:\n      org.apache.hadoop.mapred.Task remoteTask = launchEv.getRemoteTask();\n\n      // after \"launching,\" send launched event to task attempt to move\n      // state from ASSIGNED to RUNNING (also nukes \"remoteTask\", so must\n      // do getRemoteTask() call first)\n      \n      //There is no port number because we are not really talking to a task\n      // tracker.  The shuffle is just done through local files.  So the\n      // port number is set to -1 in this case.\n      context.getEventHandler().handle(\n          new TaskAttemptContainerLaunchedEvent(attemptID, -1));\n\n      if (numMapTasks == 0) {\n        doneWithMaps = true;\n      }\n\n      try {\n        if (remoteTask.isMapOrReduce()) {\n          JobCounterUpdateEvent jce = new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n          jce.addCounterUpdate(JobCounter.TOTAL_LAUNCHED_UBERTASKS, 1);\n          if (remoteTask.isMapTask()) {\n            jce.addCounterUpdate(JobCounter.NUM_UBER_SUBMAPS, 1);\n          } else {\n            jce.addCounterUpdate(JobCounter.NUM_UBER_SUBREDUCES, 1);\n          }\n          context.getEventHandler().handle(jce);\n        }\n        runSubtask(remoteTask, ytask.getType(), attemptID, numMapTasks,\n                   (numReduceTasks > 0), localMapFiles);\n        \n      } catch (RuntimeException re) {\n        JobCounterUpdateEvent jce = new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n        jce.addCounterUpdate(JobCounter.NUM_FAILED_UBERTASKS, 1);\n        context.getEventHandler().handle(jce);\n        // this is our signal that the subtask failed in some way, so\n        // simulate a failed JVM/container and send a container-completed\n        // event to task attempt (i.e., move state machine from RUNNING\n        // to FAIL_CONTAINER_CLEANUP [and ultimately to FAILED])\n        context.getEventHandler().handle(new TaskAttemptEvent(attemptID,\n            TaskAttemptEventType.TA_CONTAINER_COMPLETED));\n      } catch (IOException ioe) {\n        // if umbilical itself barfs (in error-handler of runSubMap()),\n        // we're pretty much hosed, so do what YarnChild main() does\n        // (i.e., exit clumsily--but can never happen, so no worries!)\n        LOG.fatal(\"oopsie...  this can never happen: \"\n            + StringUtils.stringifyException(ioe));\n        ExitUtil.terminate(-1);\n      } finally {\n        // remove my future\n        if (futures.remove(attemptID) != null) {\n          LOG.info(\"removed attempt \" + attemptID +\n              \" from the futures to keep track of\");\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.LocalContainerLauncher.handle": "  public void handle(ContainerLauncherEvent event) {\n    try {\n      eventQueue.put(event);\n    } catch (InterruptedException e) {\n      throw new YarnRuntimeException(e);  // FIXME? YarnRuntimeException is \"for runtime exceptions only\"\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Path.toString": "  public String toString() {\n    // we can't use uri.toString(), which escapes everything, because we want\n    // illegal characters unescaped in the string, for glob processing, etc.\n    StringBuilder buffer = new StringBuilder();\n    if (uri.getScheme() != null) {\n      buffer.append(uri.getScheme());\n      buffer.append(\":\");\n    }\n    if (uri.getAuthority() != null) {\n      buffer.append(\"//\");\n      buffer.append(uri.getAuthority());\n    }\n    if (uri.getPath() != null) {\n      String path = uri.getPath();\n      if (path.indexOf('/')==0 &&\n          hasWindowsDrive(path) &&                // has windows drive\n          uri.getScheme() == null &&              // but no scheme\n          uri.getAuthority() == null)             // or authority\n        path = path.substring(1);                 // remove slash before drive\n      buffer.append(path);\n    }\n    if (uri.getFragment() != null) {\n      buffer.append(\"#\");\n      buffer.append(uri.getFragment());\n    }\n    return buffer.toString();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Path.hasWindowsDrive": "  private static boolean hasWindowsDrive(String path) {\n    return (WINDOWS && hasDriveLetterSpecifier.matcher(path).find());\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Path.getName": "  public String getName() {\n    String path = uri.getPath();\n    int slash = path.lastIndexOf(SEPARATOR);\n    return path.substring(slash+1);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileStatus.isDirectory": "  public boolean isDirectory() {\n    return isdir;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.exists": "  public boolean exists(Path f) throws IOException {\n    try {\n      return getFileStatus(f) != null;\n    } catch (FileNotFoundException e) {\n      return false;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getFileStatus": "  public abstract FileStatus getFileStatus(Path f) throws IOException;\n\n  /**\n   * See {@link FileContext#fixRelativePart}",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.isDirectory": "  public boolean isDirectory(Path f) throws IOException {\n    try {\n      return getFileStatus(f).isDirectory();\n    } catch (FileNotFoundException e) {\n      return false;               // f does not exist\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.task.getPartition": "  public int getPartition() {\n    return partition;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.ReduceTask.setLocalMapFiles": "  public void setLocalMapFiles(Map<TaskAttemptID, MapOutputFile> mapFiles) {\n    this.localMapFiles = mapFiles;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.task.getJobID": "  public JobID getJobID() {\n    return taskId.getJobID();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.task.taskCleanup": "  void taskCleanup(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    // set phase for this task\n    setPhase(TaskStatus.Phase.CLEANUP);\n    getProgress().setStatus(\"cleanup\");\n    statusUpdate(umbilical);\n    LOG.info(\"Runnning cleanup for the task\");\n    // do the cleanup\n    committer.abortTask(taskContext);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.task.statusUpdate": "  public void statusUpdate(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    int retries = MAX_RETRIES;\n    while (true) {\n      try {\n        if (!umbilical.statusUpdate(getTaskID(), taskStatus).getTaskFound()) {\n          LOG.warn(\"Parent died.  Exiting \"+taskId);\n          System.exit(66);\n        }\n        taskStatus.clearStatus();\n        return;\n      } catch (InterruptedException ie) {\n        Thread.currentThread().interrupt(); // interrupt ourself\n      } catch (IOException ie) {\n        LOG.warn(\"Failure sending status update: \" + \n                  StringUtils.stringifyException(ie));\n        if (--retries == 0) {\n          throw ie;\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.task.setPhase": "  protected synchronized void setPhase(TaskStatus.Phase phase){\n    this.taskStatus.setPhase(phase); \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.task.setStatus": "    public void setStatus(String status) {\n      taskProgress.setStatus(normalizeStatus(status, conf));\n      // indicate that progress update needs to be sent\n      setProgressFlag();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.task.getProgress": "    public float getProgress() {\n      return taskProgress.getProgress();\n    };",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.task.getTaskID": "  public TaskAttemptID getTaskID() { return taskId; }"
        },
        "bug_report": {
            "Title": "LocalContainerLauncher#renameMapOutputForReduce incorrectly assumes a single dir for mapOutIndex",
            "Description": "The javadoc comment for {{renameMapOutputForReduce}} incorrectly refers to a single map output directory, whereas this depends on LOCAL_DIRS.\nmapOutIndex should be set to subMapOutputFile.getOutputIndexFile()\n\n{code}\n2014-06-30 14:48:35,574 WARN [uber-SubtaskRunner] org.apache.hadoop.mapred.LocalContainerLauncher: Exception running local (uberized) 'child' : java.io.FileNotFoundException: File /Users/gshegalov/workspace/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapreduce.v2.TestMRJobs/org.apache.hadoop.mapreduce.v2.          TestMRJobs-localDir-nm-2_3/usercache/gshegalov/appcache/application_1404164272885_0001/output/file.out.index does not exist\n  at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:517)\n  at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:726)\n  at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:507)\n  at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)                      \n  at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)                          \n  at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:334)    \n  at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:504)\n  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.renameMapOutputForReduce(LocalContainerLauncher.java:471)\n  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:370)\n  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:292)\n  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:178)\n  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:221)\n  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)    \n  at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)         \n  at java.util.concurrent.FutureTask.run(FutureTask.java:138)                       \n  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\n  at java.lang.Thread.run(Thread.java:695)         \n{code}"
        }
    },
    {
        "filename": "MAPREDUCE-3306.json",
        "creation_time": "2011-10-28T16:59:34.000+0000",
        "stack_trace": "```\njava.util.NoSuchElementException\n        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:796)\n        at java.util.HashMap$ValueIterator.next(HashMap.java:822)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:251)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:245)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:385)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:407)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:399)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n        at java.lang.Thread.run(Thread.java:662)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.transition": "    public void transition(ApplicationImpl app, ApplicationEvent event) {\n      ApplicationId appId = event.getApplicationID();\n      app.context.getApplications().remove(appId);\n      app.aclsManager.removeApplication(appId);\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.getContainers": "  public Map<ContainerId, Container> getContainers() {\n    this.readLock.lock();\n    try {\n      return this.containers;\n    } finally {\n      this.readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.toString": "  public String toString() {\n    return appId.toString();\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handleAppFinishWithContainersCleanedup": "  void handleAppFinishWithContainersCleanedup() {\n    // Delete Application level resources\n    this.dispatcher.getEventHandler().handle(\n        new ApplicationLocalizationEvent(\n            LocalizationEventType.DESTROY_APPLICATION_RESOURCES, this));\n\n    // tell any auxiliary services that the app is done \n    this.dispatcher.getEventHandler().handle(\n        new AuxServicesEvent(AuxServicesEventType.APPLICATION_STOP, appId));\n\n    // TODO: Trigger the LogsManager\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.getAppId": "  public ApplicationId getAppId() {\n    return appId;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.getApplicationState": "  public ApplicationState getApplicationState() {\n    this.readLock.lock();\n    try {\n      return this.stateMachine.getCurrentState();\n    } finally {\n      this.readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle": "  public void handle(ApplicationEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId applicationID = event.getApplicationID();\n      LOG.info(\"Processing \" + applicationID + \" of type \" + event.getType());\n\n      ApplicationState oldState = stateMachine.getCurrentState();\n      ApplicationState newState = null;\n      try {\n        // queue event requesting init of the same app\n        newState = stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.warn(\"Can't handle this event at current state\", e);\n      }\n      if (oldState != newState) {\n        LOG.info(\"Application \" + applicationID + \" transitioned from \"\n            + oldState + \" to \" + newState);\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.state.StateMachineFactory.doTransition": "    public synchronized STATE doTransition(EVENTTYPE eventType, EVENT event)\n         throws InvalidStateTransitonException  {\n      currentState = StateMachineFactory.this.doTransition\n          (operand, currentState, eventType, event);\n      return currentState;\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.handle": "  public void handle(ContainerManagerEvent event) {\n    switch (event.getType()) {\n    case FINISH_APPS:\n      CMgrCompletedAppsEvent appsFinishedEvent =\n          (CMgrCompletedAppsEvent) event;\n      for (ApplicationId appID : appsFinishedEvent.getAppsToCleanup()) {\n        this.dispatcher.getEventHandler().handle(\n            new ApplicationEvent(appID,\n                ApplicationEventType.FINISH_APPLICATION));\n      }\n      break;\n    case FINISH_CONTAINERS:\n      CMgrCompletedContainersEvent containersFinishedEvent =\n          (CMgrCompletedContainersEvent) event;\n      for (ContainerId container : containersFinishedEvent\n          .getContainersToCleanup()) {\n        this.dispatcher.getEventHandler().handle(\n            new ContainerKillEvent(container,\n                \"Container Killed by ResourceManager\"));\n      }\n      break;\n    default:\n      LOG.warn(\"Invalid event \" + event.getType() + \". Ignoring.\");\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch": "  protected void dispatch(Event event) {\n    //all events go thru this loop\n    LOG.debug(\"Dispatching the event \" + event.getClass().getName() + \".\"\n        + event.toString());\n\n    Class<? extends Enum> type = event.getType().getDeclaringClass();\n\n    try{\n      eventDispatchers.get(type).handle(event);\n    }\n    catch (Throwable t) {\n      //TODO Maybe log the state of the queue\n      LOG.fatal(\"Error in dispatcher thread. Exiting..\", t);\n      System.exit(-1);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.handle": "    public void handle(Event event) {\n      for (EventHandler<Event> handler: listofHandlers) {\n        handler.handle(event);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.run": "      public void run() {\n        while (!stopped && !Thread.currentThread().isInterrupted()) {\n          Event event;\n          try {\n            event = eventQueue.take();\n          } catch(InterruptedException ie) {\n            LOG.info(\"AsyncDispatcher thread interrupted\", ie);\n            return;\n          }\n          if (event != null) {\n            dispatch(event);\n          }\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationInitEvent.getApplicationACLs": "  public Map<ApplicationAccessType, String> getApplicationACLs() {\n    return this.applicationACLs;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationContainerFinishedEvent.getContainerID": "  public ContainerId getContainerID() {\n    return this.containerID;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.Event.toString": "  String toString();\n}",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.Event.getType": "  TYPE getType();\n  long getTimestamp();\n  String toString();\n}"
        },
        "bug_report": {
            "Title": "Cannot run apps after MAPREDUCE-2989",
            "Description": "Seeing this in NM logs when trying to run jobs.\n{code}\n2011-10-28 21:40:21,263 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Processing application_1319818154209_0001 of type APPLICATION_INITED\n2011-10-28 21:40:21,264 FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread. Exiting..\njava.util.NoSuchElementException\n        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:796)\n        at java.util.HashMap$ValueIterator.next(HashMap.java:822)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:251)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:245)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:385)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:407)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:399)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n        at java.lang.Thread.run(Thread.java:662)\n{code}"
        }
    },
    {
        "filename": "MAPREDUCE-6554.json",
        "creation_time": "2015-11-21T07:34:31.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at java.io.StringReader.<init>(StringReader.java:50)\n        at org.apache.avro.Schema$Parser.parse(Schema.java:917)\n        at org.apache.avro.Schema.parse(Schema.java:966)\n        at org.apache.hadoop.mapreduce.jobhistory.EventReader.<init>(EventReader.java:75)\n        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.parse(JobHistoryParser.java:139)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.parsePreviousJobHistory(MRAppMaster.java:1256)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.processRecovery(MRAppMaster.java:1225)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1087)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$4.run(MRAppMaster.java:1570)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1673)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1566)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1499)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.parse": "  public synchronized JobInfo parse(EventReader reader) throws IOException {\n\n    if (info != null) {\n      return info;\n    }\n\n    info = new JobInfo();\n    parse(reader, this);\n    return info;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.handleEvent": "  public void handleEvent(HistoryEvent event)  { \n    EventType type = event.getEventType();\n\n    switch (type) {\n    case JOB_SUBMITTED:\n      handleJobSubmittedEvent((JobSubmittedEvent)event);\n      break;\n    case JOB_STATUS_CHANGED:\n      break;\n    case JOB_INFO_CHANGED:\n      handleJobInfoChangeEvent((JobInfoChangeEvent) event);\n      break;\n    case JOB_INITED:\n      handleJobInitedEvent((JobInitedEvent) event);\n      break;\n    case JOB_PRIORITY_CHANGED:\n      handleJobPriorityChangeEvent((JobPriorityChangeEvent) event);\n      break;\n    case JOB_QUEUE_CHANGED:\n      handleJobQueueChangeEvent((JobQueueChangeEvent) event);\n      break;\n    case JOB_FAILED:\n    case JOB_KILLED:\n    case JOB_ERROR:\n      handleJobFailedEvent((JobUnsuccessfulCompletionEvent) event);\n      break;\n    case JOB_FINISHED:\n      handleJobFinishedEvent((JobFinishedEvent)event);\n      break;\n    case TASK_STARTED:\n      handleTaskStartedEvent((TaskStartedEvent) event);\n      break;\n    case TASK_FAILED:\n      handleTaskFailedEvent((TaskFailedEvent) event);\n      break;\n    case TASK_UPDATED:\n      handleTaskUpdatedEvent((TaskUpdatedEvent) event);\n      break;\n    case TASK_FINISHED:\n      handleTaskFinishedEvent((TaskFinishedEvent) event);\n      break;\n    case MAP_ATTEMPT_STARTED:\n    case CLEANUP_ATTEMPT_STARTED:\n    case REDUCE_ATTEMPT_STARTED:\n    case SETUP_ATTEMPT_STARTED:\n      handleTaskAttemptStartedEvent((TaskAttemptStartedEvent) event);\n      break;\n    case MAP_ATTEMPT_FAILED:\n    case CLEANUP_ATTEMPT_FAILED:\n    case REDUCE_ATTEMPT_FAILED:\n    case SETUP_ATTEMPT_FAILED:\n    case MAP_ATTEMPT_KILLED:\n    case CLEANUP_ATTEMPT_KILLED:\n    case REDUCE_ATTEMPT_KILLED:\n    case SETUP_ATTEMPT_KILLED:\n      handleTaskAttemptFailedEvent(\n          (TaskAttemptUnsuccessfulCompletionEvent) event);\n      break;\n    case MAP_ATTEMPT_FINISHED:\n      handleMapAttemptFinishedEvent((MapAttemptFinishedEvent) event);\n      break;\n    case REDUCE_ATTEMPT_FINISHED:\n      handleReduceAttemptFinishedEvent((ReduceAttemptFinishedEvent) event);\n      break;\n    case SETUP_ATTEMPT_FINISHED:\n    case CLEANUP_ATTEMPT_FINISHED:\n      handleTaskAttemptFinishedEvent((TaskAttemptFinishedEvent) event);\n      break;\n    case AM_STARTED:\n      handleAMStartedEvent((AMStartedEvent) event);\n      break;\n    default:\n      break;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.parsePreviousJobHistory": "  private void parsePreviousJobHistory() throws IOException {\n    FSDataInputStream in = getPreviousJobHistoryStream(getConfig(),\n        appAttemptID);\n    JobHistoryParser parser = new JobHistoryParser(in);\n    JobInfo jobInfo = parser.parse();\n    Exception parseException = parser.getParseException();\n    if (parseException != null) {\n      LOG.info(\"Got an error parsing job-history file\" +\n          \", ignoring incomplete events.\", parseException);\n    }\n    Map<org.apache.hadoop.mapreduce.TaskID, TaskInfo> taskInfos = jobInfo\n        .getAllTasks();\n    for (TaskInfo taskInfo : taskInfos.values()) {\n      if (TaskState.SUCCEEDED.toString().equals(taskInfo.getTaskStatus())) {\n        Iterator<Entry<TaskAttemptID, TaskAttemptInfo>> taskAttemptIterator =\n            taskInfo.getAllTaskAttempts().entrySet().iterator();\n        while (taskAttemptIterator.hasNext()) {\n          Map.Entry<TaskAttemptID, TaskAttemptInfo> currentEntry = taskAttemptIterator.next();\n          if (!jobInfo.getAllCompletedTaskAttempts().containsKey(currentEntry.getKey())) {\n            taskAttemptIterator.remove();\n          }\n        }\n        completedTasksFromPreviousRun\n            .put(TypeConverter.toYarn(taskInfo.getTaskId()), taskInfo);\n        LOG.info(\"Read from history task \"\n            + TypeConverter.toYarn(taskInfo.getTaskId()));\n      }\n    }\n    LOG.info(\"Read completed tasks from history \"\n        + completedTasksFromPreviousRun.size());\n    recoveredJobStartTime = jobInfo.getLaunchTime();\n\n    // recover AMInfos\n    List<JobHistoryParser.AMInfo> jhAmInfoList = jobInfo.getAMInfos();\n    if (jhAmInfoList != null) {\n      for (JobHistoryParser.AMInfo jhAmInfo : jhAmInfoList) {\n        AMInfo amInfo = MRBuilderUtils.newAMInfo(jhAmInfo.getAppAttemptId(),\n            jhAmInfo.getStartTime(), jhAmInfo.getContainerId(),\n            jhAmInfo.getNodeManagerHost(), jhAmInfo.getNodeManagerPort(),\n            jhAmInfo.getNodeManagerHttpPort());\n        amInfos.add(amInfo);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getStartTime": "    public long getStartTime() {\n      return startTime;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getPreviousJobHistoryStream": "  private static FSDataInputStream getPreviousJobHistoryStream(\n      Configuration conf, ApplicationAttemptId appAttemptId)\n      throws IOException {\n    Path historyFile = JobHistoryUtils.getPreviousJobHistoryPath(conf,\n        appAttemptId);\n    LOG.info(\"Previous history file is at \" + historyFile);\n    return historyFile.getFileSystem(conf).open(historyFile);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.processRecovery": "  private void processRecovery() throws IOException{\n    if (appAttemptID.getAttemptId() == 1) {\n      return;  // no need to recover on the first attempt\n    }\n\n    boolean recoveryEnabled = getConfig().getBoolean(\n        MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE,\n        MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE_DEFAULT);\n\n    boolean recoverySupportedByCommitter = isRecoverySupported();\n\n    // If a shuffle secret was not provided by the job client then this app\n    // attempt will generate one.  However that disables recovery if there\n    // are reducers as the shuffle secret would be app attempt specific.\n    int numReduceTasks = getConfig().getInt(MRJobConfig.NUM_REDUCES, 0);\n    boolean shuffleKeyValidForRecovery =\n        TokenCache.getShuffleSecretKey(jobCredentials) != null;\n\n    if (recoveryEnabled && recoverySupportedByCommitter\n        && (numReduceTasks <= 0 || shuffleKeyValidForRecovery)) {\n      LOG.info(\"Recovery is enabled. \"\n          + \"Will try to recover from previous life on best effort basis.\");\n      try {\n        parsePreviousJobHistory();\n      } catch (IOException e) {\n        LOG.warn(\"Unable to parse prior job history, aborting recovery\", e);\n        // try to get just the AMInfos\n        amInfos.addAll(readJustAMInfos());\n      }\n    } else {\n      LOG.info(\"Will not try to recover. recoveryEnabled: \"\n            + recoveryEnabled + \" recoverySupportedByCommitter: \"\n            + recoverySupportedByCommitter + \" numReduceTasks: \"\n            + numReduceTasks + \" shuffleKeyValidForRecovery: \"\n            + shuffleKeyValidForRecovery + \" ApplicationAttemptID: \"\n            + appAttemptID.getAttemptId());\n      // Get the amInfos anyways whether recovery is enabled or not\n      amInfos.addAll(readJustAMInfos());\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.isRecoverySupported": "  private boolean isRecoverySupported() throws IOException {\n    boolean isSupported = false;\n    Configuration conf = getConfig();\n    if (committer != null) {\n      final JobContext _jobContext = getJobContextFromConf(conf);\n      isSupported = callWithJobClassLoader(conf,\n          new ExceptionAction<Boolean>() {\n            public Boolean call(Configuration conf) throws IOException {\n              return committer.isRecoverySupported(_jobContext);\n            }\n      });\n    }\n    return isSupported;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.readJustAMInfos": "  private List<AMInfo> readJustAMInfos() {\n    List<AMInfo> amInfos = new ArrayList<AMInfo>();\n    FSDataInputStream inputStream = null;\n    try {\n      inputStream = getPreviousJobHistoryStream(getConfig(), appAttemptID);\n      EventReader jobHistoryEventReader = new EventReader(inputStream);\n\n      // All AMInfos are contiguous. Track when the first AMStartedEvent\n      // appears.\n      boolean amStartedEventsBegan = false;\n\n      HistoryEvent event;\n      while ((event = jobHistoryEventReader.getNextEvent()) != null) {\n        if (event.getEventType() == EventType.AM_STARTED) {\n          if (!amStartedEventsBegan) {\n            // First AMStartedEvent.\n            amStartedEventsBegan = true;\n          }\n          AMStartedEvent amStartedEvent = (AMStartedEvent) event;\n          amInfos.add(MRBuilderUtils.newAMInfo(\n            amStartedEvent.getAppAttemptId(), amStartedEvent.getStartTime(),\n            amStartedEvent.getContainerId(),\n            StringInterner.weakIntern(amStartedEvent.getNodeManagerHost()),\n            amStartedEvent.getNodeManagerPort(),\n            amStartedEvent.getNodeManagerHttpPort()));\n        } else if (amStartedEventsBegan) {\n          // This means AMStartedEvents began and this event is a\n          // non-AMStarted event.\n          // No need to continue reading all the other events.\n          break;\n        }\n      }\n    } catch (IOException e) {\n      LOG.warn(\"Could not parse the old history file. \"\n          + \"Will not have old AMinfos \", e);\n    } finally {\n      if (inputStream != null) {\n        IOUtils.closeQuietly(inputStream);\n      }\n    }\n    return amInfos;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart": "  protected void serviceStart() throws Exception {\n\n    amInfos = new LinkedList<AMInfo>();\n    completedTasksFromPreviousRun = new HashMap<TaskId, TaskInfo>();\n    processRecovery();\n\n    // Current an AMInfo for the current AM generation.\n    AMInfo amInfo =\n        MRBuilderUtils.newAMInfo(appAttemptID, startTime, containerID, nmHost,\n            nmPort, nmHttpPort);\n\n    // /////////////////// Create the job itself.\n    job = createJob(getConfig(), forcedState, shutDownMessage);\n\n    // End of creating the job.\n\n    // Send out an MR AM inited event for all previous AMs.\n    for (AMInfo info : amInfos) {\n      dispatcher.getEventHandler().handle(\n          new JobHistoryEvent(job.getID(), new AMStartedEvent(info\n              .getAppAttemptId(), info.getStartTime(), info.getContainerId(),\n              info.getNodeManagerHost(), info.getNodeManagerPort(), info\n                  .getNodeManagerHttpPort(), appSubmitTime)));\n    }\n\n    // Send out an MR AM inited event for this AM.\n    dispatcher.getEventHandler().handle(\n        new JobHistoryEvent(job.getID(), new AMStartedEvent(amInfo\n            .getAppAttemptId(), amInfo.getStartTime(), amInfo.getContainerId(),\n            amInfo.getNodeManagerHost(), amInfo.getNodeManagerPort(), amInfo\n                .getNodeManagerHttpPort(), this.forcedState == null ? null\n                    : this.forcedState.toString(), appSubmitTime)));\n    amInfos.add(amInfo);\n\n    // metrics system init is really init & start.\n    // It's more test friendly to put it here.\n    DefaultMetricsSystem.initialize(\"MRAppMaster\");\n\n    boolean initFailed = false;\n    if (!errorHappenedShutDown) {\n      // create a job event for job intialization\n      JobEvent initJobEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT);\n      // Send init to the job (this does NOT trigger job execution)\n      // This is a synchronous call, not an event through dispatcher. We want\n      // job-init to be done completely here.\n      jobEventDispatcher.handle(initJobEvent);\n\n      // If job is still not initialized, an error happened during\n      // initialization. Must complete starting all of the services so failure\n      // events can be processed.\n      initFailed = (((JobImpl)job).getInternalState() != JobStateInternal.INITED);\n\n      // JobImpl's InitTransition is done (call above is synchronous), so the\n      // \"uber-decision\" (MR-1220) has been made.  Query job and switch to\n      // ubermode if appropriate (by registering different container-allocator\n      // and container-launcher services/event-handlers).\n\n      if (job.isUber()) {\n        speculatorEventDispatcher.disableSpeculation();\n        LOG.info(\"MRAppMaster uberizing job \" + job.getID()\n            + \" in local container (\\\"uber-AM\\\") on node \"\n            + nmHost + \":\" + nmPort + \".\");\n      } else {\n        // send init to speculator only for non-uber jobs. \n        // This won't yet start as dispatcher isn't started yet.\n        dispatcher.getEventHandler().handle(\n            new SpeculatorEvent(job.getID(), clock.getTime()));\n        LOG.info(\"MRAppMaster launching normal, non-uberized, multi-container \"\n            + \"job \" + job.getID() + \".\");\n      }\n      // Start ClientService here, since it's not initialized if\n      // errorHappenedShutDown is true\n      clientService.start();\n    }\n    //start all the components\n    super.serviceStart();\n\n    // finally set the job classloader\n    MRApps.setClassLoader(jobClassLoader, getConfig());\n    // set job classloader if configured\n    Limits.init(getConfig());\n\n    if (initFailed) {\n      JobEvent initFailedEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT_FAILED);\n      jobEventDispatcher.handle(initFailedEvent);\n    } else {\n      // All components have started, start the job.\n      startJobs();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.startJobs": "  protected void startJobs() {\n    /** create a job-start event to get this ball rolling */\n    JobEvent startJobEvent = new JobStartEvent(job.getID(),\n        recoveredJobStartTime);\n    /** send the job-start event. this triggers the job execution. */\n    dispatcher.getEventHandler().handle(startJobEvent);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.disableSpeculation": "    public void disableSpeculation() {\n      disabled = true;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getEventHandler": "    public EventHandler getEventHandler() {\n      return dispatcher.getEventHandler();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.handle": "    public void handle(Event event) {\n      //Empty\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createJob": "  protected Job createJob(Configuration conf, JobStateInternal forcedState, \n      String diagnostic) {\n\n    // create single job\n    Job newJob =\n        new JobImpl(jobId, appAttemptID, conf, dispatcher.getEventHandler(),\n            taskAttemptListener, jobTokenSecretManager, jobCredentials, clock,\n            completedTasksFromPreviousRun, metrics,\n            committer, newApiCommitter,\n            currentUser.getUserName(), appSubmitTime, amInfos, context, \n            forcedState, diagnostic);\n    ((RunningAppContext) context).jobs.put(newJob.getID(), newJob);\n\n    dispatcher.register(JobFinishEvent.Type.class,\n        createJobFinishEventHandler());     \n    return newJob;\n  } // end createJob()",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.start": "  public void start() {\n    if (isInState(STATE.STARTED)) {\n      return;\n    }\n    //enter the started state\n    synchronized (stateChangeLock) {\n      if (stateModel.enterState(STATE.STARTED) != STATE.STARTED) {\n        try {\n          startTime = System.currentTimeMillis();\n          serviceStart();\n          if (isInState(STATE.STARTED)) {\n            //if the service started (and isn't now in a later state), notify\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Service \" + getName() + \" is started\");\n            }\n            notifyListeners();\n          }\n        } catch (Exception e) {\n          noteFailure(e);\n          ServiceOperations.stopQuietly(LOG, this);\n          throw ServiceStateException.convert(e);\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.isInState": "  public final boolean isInState(Service.STATE expected) {\n    return stateModel.isInState(expected);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.getName": "  public String getName() {\n    return name;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.notifyListeners": "  private void notifyListeners() {\n    try {\n      listeners.notifyListeners(this);\n      globalListeners.notifyListeners(this);\n    } catch (Throwable e) {\n      LOG.warn(\"Exception while notifying listeners of \" + this + \": \" + e,\n               e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.enterState": "  private STATE enterState(STATE newState) {\n    assert stateModel != null : \"null state in \" + name + \" \" + this.getClass();\n    STATE oldState = stateModel.enterState(newState);\n    if (oldState != newState) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\n          \"Service: \" + getName() + \" entered state \" + getServiceState());\n      }\n      recordLifecycleEvent();\n    }\n    return oldState;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.serviceStart": "  protected void serviceStart() throws Exception {\n\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.noteFailure": "  protected final void noteFailure(Exception exception) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"noteFailure \" + exception, null);\n    }\n    if (exception == null) {\n      //make sure failure logic doesn't itself cause problems\n      return;\n    }\n    //record the failure details, and log it\n    synchronized (this) {\n      if (failureCause == null) {\n        failureCause = exception;\n        failureState = getServiceState();\n        LOG.info(\"Service \" + getName()\n                 + \" failed in state \" + failureState\n                 + \"; cause: \" + exception,\n                 exception);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.run": "      public Object run() throws Exception {\n        appMaster.init(conf);\n        appMaster.start();\n        if(appMaster.errorHappenedShutDown) {\n          throw new IOException(\"Was asked to shut down.\");\n        }\n        return null;\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.notifyIsLastAMRetry": "  public void notifyIsLastAMRetry(boolean isLastAMRetry){\n    if(containerAllocator instanceof ContainerAllocatorRouter) {\n      LOG.info(\"Notify RMCommunicator isAMLastRetry: \" + isLastAMRetry);\n      ((ContainerAllocatorRouter) containerAllocator)\n        .setShouldUnregister(isLastAMRetry);\n    }\n    if(jobHistoryEventHandler != null) {\n      LOG.info(\"Notify JHEH isAMLastRetry: \" + isLastAMRetry);\n      jobHistoryEventHandler.setForcejobCompletion(isLastAMRetry);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob": "  public void shutDownJob() {\n    // job has finished\n    // this is the only job, so shut down the Appmaster\n    // note in a workflow scenario, this may lead to creation of a new\n    // job (FIXME?)\n\n    try {\n      //if isLastAMRetry comes as true, should never set it to false\n      if ( !isLastAMRetry){\n        if (((JobImpl)job).getInternalState() != JobStateInternal.REBOOT) {\n          LOG.info(\"Job finished cleanly, recording last MRAppMaster retry\");\n          isLastAMRetry = true;\n        }\n      }\n      notifyIsLastAMRetry(isLastAMRetry);\n      // Stop all services\n      // This will also send the final report to the ResourceManager\n      LOG.info(\"Calling stop for all the services\");\n      MRAppMaster.this.stop();\n\n      if (isLastAMRetry) {\n        // Send job-end notification when it is safe to report termination to\n        // users and it is the last AM retry\n        if (getConfig().get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL) != null) {\n          try {\n            LOG.info(\"Job end notification started for jobID : \"\n                + job.getReport().getJobId());\n            JobEndNotifier notifier = new JobEndNotifier();\n            notifier.setConf(getConfig());\n            JobReport report = job.getReport();\n            // If unregistration fails, the final state is unavailable. However,\n            // at the last AM Retry, the client will finally be notified FAILED\n            // from RM, so we should let users know FAILED via notifier as well\n            if (!context.hasSuccessfullyUnregistered()) {\n              report.setJobState(JobState.FAILED);\n            }\n            notifier.notify(report);\n          } catch (InterruptedException ie) {\n            LOG.warn(\"Job end notification interrupted for jobID : \"\n                + job.getReport().getJobId(), ie);\n          }\n        }\n      }\n\n      try {\n        Thread.sleep(5000);\n      } catch (InterruptedException e) {\n        e.printStackTrace();\n      }\n      clientService.stop();\n    } catch (Throwable t) {\n      LOG.warn(\"Graceful stop failed. Exiting.. \", t);\n      exitMRAppMaster(1, t);\n    }\n    exitMRAppMaster(0, null);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop": "  public void stop() {\n    super.stop();\n    shutdownTaskLog();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause == null) {\n        throw new RuntimeException(\"PrivilegedActionException with no \" +\n                \"underlying cause. UGI [\" + this + \"]\", pae);\n      } else if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster": "  protected static void initAndStartAppMaster(final MRAppMaster appMaster,\n      final JobConf conf, String jobUserName) throws IOException,\n      InterruptedException {\n    UserGroupInformation.setConfiguration(conf);\n    // Security framework already loaded the tokens into current UGI, just use\n    // them\n    Credentials credentials =\n        UserGroupInformation.getCurrentUser().getCredentials();\n    LOG.info(\"Executing with tokens:\");\n    for (Token<?> token : credentials.getAllTokens()) {\n      LOG.info(token);\n    }\n    \n    UserGroupInformation appMasterUgi = UserGroupInformation\n        .createRemoteUser(jobUserName);\n    appMasterUgi.addCredentials(credentials);\n\n    // Now remove the AM->RM token so tasks don't have it\n    Iterator<Token<?>> iter = credentials.getAllTokens().iterator();\n    while (iter.hasNext()) {\n      Token<?> token = iter.next();\n      if (token.getKind().equals(AMRMTokenIdentifier.KIND_NAME)) {\n        iter.remove();\n      }\n    }\n    conf.getCredentials().addAll(credentials);\n    appMasterUgi.doAs(new PrivilegedExceptionAction<Object>() {\n      @Override\n      public Object run() throws Exception {\n        appMaster.init(conf);\n        appMaster.start();\n        if(appMaster.errorHappenedShutDown) {\n          throw new IOException(\"Was asked to shut down.\");\n        }\n        return null;\n      }\n    });\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getCredentials": "  protected Credentials getCredentials() {\n    return jobCredentials;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main": "  public static void main(String[] args) {\n    try {\n      mainStarted = true;\n      Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\n      String containerIdStr =\n          System.getenv(Environment.CONTAINER_ID.name());\n      String nodeHostString = System.getenv(Environment.NM_HOST.name());\n      String nodePortString = System.getenv(Environment.NM_PORT.name());\n      String nodeHttpPortString =\n          System.getenv(Environment.NM_HTTP_PORT.name());\n      String appSubmitTimeStr =\n          System.getenv(ApplicationConstants.APP_SUBMIT_TIME_ENV);\n      \n      validateInputParam(containerIdStr,\n          Environment.CONTAINER_ID.name());\n      validateInputParam(nodeHostString, Environment.NM_HOST.name());\n      validateInputParam(nodePortString, Environment.NM_PORT.name());\n      validateInputParam(nodeHttpPortString,\n          Environment.NM_HTTP_PORT.name());\n      validateInputParam(appSubmitTimeStr,\n          ApplicationConstants.APP_SUBMIT_TIME_ENV);\n\n      ContainerId containerId = ConverterUtils.toContainerId(containerIdStr);\n      ApplicationAttemptId applicationAttemptId =\n          containerId.getApplicationAttemptId();\n      long appSubmitTime = Long.parseLong(appSubmitTimeStr);\n      \n      \n      MRAppMaster appMaster =\n          new MRAppMaster(applicationAttemptId, containerId, nodeHostString,\n              Integer.parseInt(nodePortString),\n              Integer.parseInt(nodeHttpPortString), appSubmitTime);\n      ShutdownHookManager.get().addShutdownHook(\n        new MRAppMasterShutdownHook(appMaster), SHUTDOWN_HOOK_PRIORITY);\n      JobConf conf = new JobConf(new YarnConfiguration());\n      conf.addResource(new Path(MRJobConfig.JOB_CONF_FILE));\n      \n      MRWebAppUtil.initialize(conf);\n      // log the system properties\n      String systemPropsToLog = MRApps.getSystemPropertiesToLog(conf);\n      if (systemPropsToLog != null) {\n        LOG.info(systemPropsToLog);\n      }\n\n      String jobUserName = System\n          .getenv(ApplicationConstants.Environment.USER.name());\n      conf.set(MRJobConfig.USER_NAME, jobUserName);\n      initAndStartAppMaster(appMaster, conf, jobUserName);\n    } catch (Throwable t) {\n      LOG.fatal(\"Error starting MRAppMaster\", t);\n      ExitUtil.terminate(1, t);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getApplicationAttemptId": "    public ApplicationAttemptId getApplicationAttemptId() {\n      return appAttemptID;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.validateInputParam": "  private static void validateInputParam(String value, String param)\n      throws IOException {\n    if (value == null) {\n      String msg = param + \" is null\";\n      LOG.error(msg);\n      throw new IOException(msg);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceOperations.stopQuietly": "  public static Exception stopQuietly(Log log, Service service) {\n    try {\n      stop(service);\n    } catch (Exception e) {\n      log.warn(\"When stopping the service \" + service.getName()\n               + \" : \" + e,\n               e);\n      return e;\n    }\n    return null;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceOperations.stop": "  public static void stop(Service service) {\n    if (service != null) {\n      service.stop();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceStateException.convert": "  public static RuntimeException convert(String text, Throwable fault) {\n    if (fault instanceof RuntimeException) {\n      return (RuntimeException) fault;\n    } else {\n      return new ServiceStateException(text, fault);\n    }\n  }"
        },
        "bug_report": {
            "Title": "MRAppMaster servicestart failing  with NPE in MRAppMaster#parsePreviousJobHistory",
            "Description": "Create scenario so that MR app master gets preempted.\nOn next MRAppMaster launch tried to recover previous job history file {{MRAppMaster#parsePreviousJobHistory}}\n\n\n{noformat}\n2015-11-21 13:52:27,722 INFO [main] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state STARTED; cause: java.lang.NullPointerException\njava.lang.NullPointerException\n        at java.io.StringReader.<init>(StringReader.java:50)\n        at org.apache.avro.Schema$Parser.parse(Schema.java:917)\n        at org.apache.avro.Schema.parse(Schema.java:966)\n        at org.apache.hadoop.mapreduce.jobhistory.EventReader.<init>(EventReader.java:75)\n        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.parse(JobHistoryParser.java:139)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.parsePreviousJobHistory(MRAppMaster.java:1256)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.processRecovery(MRAppMaster.java:1225)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1087)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$4.run(MRAppMaster.java:1570)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1673)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1566)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1499)\n2015-11-21 13:52:27,725 INFO [main] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Stopping JobHistoryEventHandler. Size of the outstanding queue size is 0\n\n{noformat}\n\nEventReader(EventReader stream)\n{noformat}\n this.version = in.readLine();\n...\n    Schema myschema = new SpecificData(Event.class.getClassLoader()).getSchema(Event.class);\n    this.schema = Schema.parse(in.readLine());\n{noformat}\n\n"
        }
    },
    {
        "filename": "MAPREDUCE-4457.json",
        "creation_time": "2012-07-18T21:38:17.000+0000",
        "stack_trace": "```\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event:\nTA_TOO_MANY_FETCH_FAILURE at FAILED\n    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:954)\n    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:133)\n    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:913)\n    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:905)\n    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)\n    at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryService.java:285)\n    at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:281)\n    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n    at java.lang.Thread.run(Thread.java:619)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.state.StateMachineFactory.doTransition": "    public synchronized STATE doTransition(EVENTTYPE eventType, EVENT event)\n         throws InvalidStateTransitonException  {\n      currentState = StateMachineFactory.this.doTransition\n          (operand, currentState, eventType, event);\n      return currentState;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle": "  public void handle(TaskAttemptEvent event) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Processing \" + event.getTaskAttemptID() + \" of type \"\n          + event.getType());\n    }\n    writeLock.lock();\n    try {\n      final TaskAttemptState oldState = getState();\n      try {\n        stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state for \"\n            + this.attemptId, e);\n        eventHandler.handle(new JobDiagnosticsUpdateEvent(\n            this.attemptId.getTaskId().getJobId(), \"Invalid event \" + event.getType() + \n            \" on TaskAttempt \" + this.attemptId));\n        eventHandler.handle(new JobEvent(this.attemptId.getTaskId().getJobId(),\n            JobEventType.INTERNAL_ERROR));\n      }\n      if (oldState != getState()) {\n          LOG.info(attemptId + \" TaskAttempt Transitioned from \" \n           + oldState + \" to \"\n           + getState());\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.getState": "  public TaskAttemptState getState() {\n    readLock.lock();\n    try {\n      return stateMachine.getCurrentState();\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.handle": "    public void handle(SpeculatorEvent event) {\n      if (disabled) {\n        return;\n      }\n\n      TaskId tId = event.getTaskID();\n      TaskType tType = null;\n      /* event's TaskId will be null if the event type is JOB_CREATE or\n       * ATTEMPT_STATUS_UPDATE\n       */\n      if (tId != null) {\n        tType = tId.getTaskType(); \n      }\n      boolean shouldMapSpec =\n              conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false);\n      boolean shouldReduceSpec =\n              conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false);\n\n      /* The point of the following is to allow the MAP and REDUCE speculative\n       * config values to be independent:\n       * IF spec-exec is turned on for maps AND the task is a map task\n       * OR IF spec-exec is turned on for reduces AND the task is a reduce task\n       * THEN call the speculator to handle the event.\n       */\n      if ( (shouldMapSpec && (tType == null || tType == TaskType.MAP))\n        || (shouldReduceSpec && (tType == null || tType == TaskType.REDUCE))) {\n        // Speculator IS enabled, direct the event to there.\n        speculator.handle(event);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop": "    public synchronized void stop() {\n      try {\n        cleanupStagingDir();\n      } catch (IOException io) {\n        LOG.error(\"Failed to cleanup staging dir: \", io);\n      }\n      super.stop();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJobId": "  public JobId getJobId() {\n    return jobId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJob": "    public Job getJob(JobId jobID) {\n      return jobs.get(jobID);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.sysexit": "  protected void sysexit() {\n    System.exit(0);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch": "  protected void dispatch(Event event) {\n    //all events go thru this loop\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Dispatching the event \" + event.getClass().getName() + \".\"\n          + event.toString());\n    }\n\n    Class<? extends Enum> type = event.getType().getDeclaringClass();\n\n    try{\n      eventDispatchers.get(type).handle(event);\n    }\n    catch (Throwable t) {\n      //TODO Maybe log the state of the queue\n      LOG.fatal(\"Error in dispatcher thread\", t);\n      if (exitOnDispatchException\n          && (ShutdownHookManager.get().isShutdownInProgress()) == false) {\n        LOG.info(\"Exiting, bbye..\");\n        System.exit(-1);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.handle": "    public void handle(Event event) {\n      for (EventHandler<Event> handler: listofHandlers) {\n        handler.handle(event);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService.realDispatch": "    public void realDispatch(Event event) {\n      super.dispatch(event);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService.dispatch": "    public void dispatch(Event event) {\n      if (recoveryMode) {\n        if (event.getType() == TaskAttemptEventType.TA_CONTAINER_LAUNCHED) {\n          TaskAttemptInfo attInfo = getTaskAttemptInfo(((TaskAttemptEvent) event)\n              .getTaskAttemptID());\n          LOG.info(\"Recovered Attempt start time \" + attInfo.getStartTime());\n          clock.setTime(attInfo.getStartTime());\n\n        } else if (event.getType() == TaskAttemptEventType.TA_DONE\n            || event.getType() == TaskAttemptEventType.TA_FAILMSG\n            || event.getType() == TaskAttemptEventType.TA_KILL) {\n          TaskAttemptInfo attInfo = getTaskAttemptInfo(((TaskAttemptEvent) event)\n              .getTaskAttemptID());\n          LOG.info(\"Recovered Attempt finish time \" + attInfo.getFinishTime());\n          clock.setTime(attInfo.getFinishTime());\n        }\n\n        else if (event.getType() == TaskEventType.T_ATTEMPT_FAILED\n            || event.getType() == TaskEventType.T_ATTEMPT_KILLED\n            || event.getType() == TaskEventType.T_ATTEMPT_SUCCEEDED) {\n          TaskTAttemptEvent tEvent = (TaskTAttemptEvent) event;\n          LOG.info(\"Recovered Task attempt \" + tEvent.getTaskAttemptID());\n          TaskInfo taskInfo = completedTasks.get(tEvent.getTaskAttemptID()\n              .getTaskId());\n          taskInfo.getAllTaskAttempts().remove(\n              TypeConverter.fromYarn(tEvent.getTaskAttemptID()));\n          // remove the task info from completed tasks if all attempts are\n          // recovered\n          if (taskInfo.getAllTaskAttempts().size() == 0) {\n            completedTasks.remove(tEvent.getTaskAttemptID().getTaskId());\n            // checkForRecoveryComplete\n            LOG.info(\"CompletedTasks() \" + completedTasks.size());\n            if (completedTasks.size() == 0) {\n              recoveryMode = false;\n              clock.reset();\n              LOG.info(\"Setting the recovery mode to false. \" +\n                 \"Recovery is complete!\");\n\n              // send all pending tasks schedule events\n              for (TaskEvent tEv : pendingTaskScheduleEvents) {\n                actualHandler.handle(tEv);\n              }\n\n            }\n          }\n        }\n      }\n      realDispatch(event);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService.handle": "    public void handle(Event event) {\n      if (!recoveryMode) {\n        // delegate to the dispatcher one\n        actualHandler.handle(event);\n        return;\n      }\n\n      else if (event.getType() == TaskEventType.T_SCHEDULE) {\n        TaskEvent taskEvent = (TaskEvent) event;\n        // delay the scheduling of new tasks till previous ones are recovered\n        if (completedTasks.get(taskEvent.getTaskID()) == null) {\n          LOG.debug(\"Adding to pending task events \"\n              + taskEvent.getTaskID());\n          pendingTaskScheduleEvents.add(taskEvent);\n          return;\n        }\n      }\n\n      else if (event.getType() == ContainerAllocator.EventType.CONTAINER_REQ) {\n        TaskAttemptId aId = ((ContainerAllocatorEvent) event).getAttemptID();\n        TaskAttemptInfo attInfo = getTaskAttemptInfo(aId);\n        LOG.debug(\"CONTAINER_REQ \" + aId);\n        sendAssignedEvent(aId, attInfo);\n        return;\n      }\n\n      else if (event.getType() == TaskCleaner.EventType.TASK_CLEAN) {\n        TaskAttemptId aId = ((TaskCleanupEvent) event).getAttemptID();\n        LOG.debug(\"TASK_CLEAN\");\n        actualHandler.handle(new TaskAttemptEvent(aId,\n            TaskAttemptEventType.TA_CLEANUP_DONE));\n        return;\n      }\n\n      else if (event.getType() == ContainerLauncher.EventType.CONTAINER_REMOTE_LAUNCH) {\n        TaskAttemptId aId = ((ContainerRemoteLaunchEvent) event)\n            .getTaskAttemptID();\n        TaskAttemptInfo attInfo = getTaskAttemptInfo(aId);\n        actualHandler.handle(new TaskAttemptContainerLaunchedEvent(aId,\n            attInfo.getShufflePort()));\n        // send the status update event\n        sendStatusUpdateEvent(aId, attInfo);\n\n        TaskAttemptState state = TaskAttemptState.valueOf(attInfo.getTaskStatus());\n        switch (state) {\n        case SUCCEEDED:\n          //recover the task output\n          TaskAttemptContext taskContext = new TaskAttemptContextImpl(getConfig(),\n              attInfo.getAttemptId());\n          try { \n            TaskType type = taskContext.getTaskAttemptID().getTaskID().getTaskType();\n            int numReducers = taskContext.getConfiguration().getInt(MRJobConfig.NUM_REDUCES, 1); \n            if(type == TaskType.REDUCE || (type == TaskType.MAP && numReducers <= 0)) {\n              committer.recoverTask(taskContext);\n              LOG.info(\"Recovered output from task attempt \" + attInfo.getAttemptId());\n            } else {\n              LOG.info(\"Will not try to recover output for \"\n                  + taskContext.getTaskAttemptID());\n            }\n          } catch (IOException e) {\n            LOG.error(\"Caught an exception while trying to recover task \"+aId, e);\n            actualHandler.handle(new JobDiagnosticsUpdateEvent(\n                aId.getTaskId().getJobId(), \"Error in recovering task output \" + \n                e.getMessage()));\n            actualHandler.handle(new JobEvent(aId.getTaskId().getJobId(),\n                JobEventType.INTERNAL_ERROR));\n          }\n          \n          // send the done event\n          LOG.info(\"Sending done event to recovered attempt \" + aId);\n          actualHandler.handle(new TaskAttemptEvent(aId,\n              TaskAttemptEventType.TA_DONE));\n          break;\n        case KILLED:\n          LOG.info(\"Sending kill event to recovered attempt \" + aId);\n          actualHandler.handle(new TaskAttemptEvent(aId,\n              TaskAttemptEventType.TA_KILL));\n          break;\n        default:\n          LOG.info(\"Sending fail event to recovered attempt \" + aId);\n          actualHandler.handle(new TaskAttemptEvent(aId,\n              TaskAttemptEventType.TA_FAILMSG));\n          break;\n        }\n        return;\n      }\n\n      else if (event.getType() == \n        ContainerLauncher.EventType.CONTAINER_REMOTE_CLEANUP) {\n        TaskAttemptId aId = ((ContainerLauncherEvent) event)\n          .getTaskAttemptID();\n        actualHandler.handle(\n           new TaskAttemptEvent(aId,\n                TaskAttemptEventType.TA_CONTAINER_CLEANED));\n        return;\n      }\n\n      // delegate to the actual handler\n      actualHandler.handle(event);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService.getTaskAttemptInfo": "  private TaskAttemptInfo getTaskAttemptInfo(TaskAttemptId id) {\n    TaskInfo taskInfo = completedTasks.get(id.getTaskId());\n    return taskInfo.getAllTaskAttempts().get(TypeConverter.fromYarn(id));\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.run": "      public void run() {\n        while (!stopped && !Thread.currentThread().isInterrupted()) {\n          Event event;\n          try {\n            event = eventQueue.take();\n          } catch(InterruptedException ie) {\n            LOG.warn(\"AsyncDispatcher thread interrupted\", ie);\n            return;\n          }\n          if (event != null) {\n            dispatch(event);\n          }\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.setConf": "  public void setConf(Configuration conf) {\n    this.conf = conf;\n    \n    numTries = Math.min(\n      conf.getInt(MRJobConfig.MR_JOB_END_RETRY_ATTEMPTS, 0) + 1\n      , conf.getInt(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_ATTEMPTS, 1)\n    );\n    waitInterval = Math.min(\n    conf.getInt(MRJobConfig.MR_JOB_END_RETRY_INTERVAL, 5)\n    , conf.getInt(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_RETRY_INTERVAL, 5)\n    );\n    waitInterval = (waitInterval < 0) ? 5 : waitInterval;\n\n    userUrl = conf.get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL);\n\n    proxyConf = conf.get(MRJobConfig.MR_JOB_END_NOTIFICATION_PROXY);\n\n    //Configure the proxy to use if its set. It should be set like\n    //proxyType@proxyHostname:port\n    if(proxyConf != null && !proxyConf.equals(\"\") &&\n         proxyConf.lastIndexOf(\":\") != -1) {\n      int typeIndex = proxyConf.indexOf(\"@\");\n      Proxy.Type proxyType = Proxy.Type.HTTP;\n      if(typeIndex != -1 &&\n        proxyConf.substring(0, typeIndex).compareToIgnoreCase(\"socks\") == 0) {\n        proxyType = Proxy.Type.SOCKS;\n      }\n      String hostname = proxyConf.substring(typeIndex + 1,\n        proxyConf.lastIndexOf(\":\"));\n      String portConf = proxyConf.substring(proxyConf.lastIndexOf(\":\") + 1);\n      try {\n        int port = Integer.parseInt(portConf);\n        proxyToUse = new Proxy(proxyType,\n          new InetSocketAddress(hostname, port));\n        Log.info(\"Job end notification using proxy type \\\"\" + proxyType + \n        \"\\\" hostname \\\"\" + hostname + \"\\\" and port \\\"\" + port + \"\\\"\");\n      } catch(NumberFormatException nfe) {\n        Log.warn(\"Job end notification couldn't parse configured proxy's port \"\n          + portConf + \". Not going to use a proxy\");\n      }\n    }\n\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notify": "  public void notify(JobReport jobReport)\n    throws InterruptedException {\n    // Do we need job-end notification?\n    if (userUrl == null) {\n      Log.info(\"Job end notification URL not set, skipping.\");\n      return;\n    }\n\n    //Do string replacements for jobId and jobStatus\n    if (userUrl.contains(JOB_ID)) {\n      userUrl = userUrl.replace(JOB_ID, jobReport.getJobId().toString());\n    }\n    if (userUrl.contains(JOB_STATUS)) {\n      userUrl = userUrl.replace(JOB_STATUS, jobReport.getJobState().toString());\n    }\n\n    // Create the URL, ensure sanity\n    try {\n      urlToNotify = new URL(userUrl);\n    } catch (MalformedURLException mue) {\n      Log.warn(\"Job end notification couldn't parse \" + userUrl, mue);\n      return;\n    }\n\n    // Send notification\n    boolean success = false;\n    while (numTries-- > 0 && !success) {\n      Log.info(\"Job end notification attempts left \" + numTries);\n      success = notifyURLOnce();\n      if (!success) {\n        Thread.sleep(waitInterval);\n      }\n    }\n    if (!success) {\n      Log.warn(\"Job end notification failed to notify : \" + urlToNotify);\n    } else {\n      Log.info(\"Job end notification succeeded for \" + jobReport.getJobId());\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notifyURLOnce": "  protected boolean notifyURLOnce() {\n    boolean success = false;\n    try {\n      Log.info(\"Job end notification trying \" + urlToNotify);\n      HttpURLConnection conn =\n        (HttpURLConnection) urlToNotify.openConnection(proxyToUse);\n      conn.setConnectTimeout(5*1000);\n      conn.setReadTimeout(5*1000);\n      conn.setAllowUserInteraction(false);\n      if(conn.getResponseCode() != HttpURLConnection.HTTP_OK) {\n        Log.warn(\"Job end notification to \" + urlToNotify +\" failed with code: \"\n        + conn.getResponseCode() + \" and message \\\"\" + conn.getResponseMessage()\n        +\"\\\"\");\n      }\n      else {\n        success = true;\n        Log.info(\"Job end notification to \" + urlToNotify + \" succeeded\");\n      }\n    } catch(IOException ioe) {\n      Log.warn(\"Job end notification to \" + urlToNotify + \" failed\", ioe);\n    }\n    return success;\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.Event.toString": "  String toString();\n}",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.Event.getType": "  TYPE getType();\n  long getTimestamp();\n  String toString();\n}"
        },
        "bug_report": {
            "Title": "mr job invalid transition TA_TOO_MANY_FETCH_FAILURE at FAILED",
            "Description": "we saw a job go into the ERROR state from an invalid state transition.\n\n3,600 INFO [AsyncDispatcher event handler]\norg.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:\nattempt_1342238829791_2501_m_007743_0 TaskAttempt Transitioned from SUCCEEDED\nto FAILED\n2012-07-16 08:49:53,600 INFO [AsyncDispatcher event handler]\norg.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:\nattempt_1342238829791_2501_m_008850_0 TaskAttempt Transitioned from SUCCEEDED\nto FAILED\n2012-07-16 08:49:53,600 INFO [AsyncDispatcher event handler]\norg.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:\nattempt_1342238829791_2501_m_017344_1000 TaskAttempt Transitioned from RUNNING\nto SUCCESS_CONTAINER_CLEANUP\n2012-07-16 08:49:53,601 ERROR [AsyncDispatcher event handler]\norg.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Can't handle this\nevent at current state for attempt_1342238829791_2501_m_000027_0\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event:\nTA_TOO_MANY_FETCH_FAILURE at FAILED\n    at\norg.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n    at\norg.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n    at\norg.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n    at\norg.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:954)\n    at\norg.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:133)\n    at\norg.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:913)\n    at\norg.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:905)\n    at\norg.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)\n    at\norg.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryService.java:285)\n    at\norg.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:281)\n    at\norg.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n    at java.lang.Thread.run(Thread.java:619)\n2012-07-16 08:49:53,601 INFO [AsyncDispatcher event handler]\norg.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:\nattempt_1342238829791_2501_m_029091_1000 TaskAttempt Transitioned from RUNNING\nto SUCCESS_CONTAINER_CLEANUP\n2012-07-16 08:49:53,601 INFO [IPC Server handler 17 on 47153]\norg.apache.hadoop.mapred.TaskAttemptListenerImpl: Status update from\nattempt_1342238829791_2501_r_000461_1000\n\n\nIt looks like we possibly got 2 TA_TOO_MANY_FETCH_FAILURE events. The first one moved it to FAILED and then the second one failed because no valid transition."
        }
    },
    {
        "filename": "MAPREDUCE-2716.json",
        "creation_time": "2011-07-20T21:13:36.000+0000",
        "stack_trace": "```\njava.lang.IllegalArgumentException: Can not create a Path from an empty string\n        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:88)\n        at org.apache.hadoop.fs.Path.<init>(Path.java:96)\n        at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:445)\n        at org.apache.hadoop.mapreduce.Cluster.getJobs(Cluster.java:104)\n        at org.apache.hadoop.mapreduce.Cluster.getAllJobs(Cluster.java:218)\n        at org.apache.hadoop.mapred.JobClient.getAllJobs(JobClient.java:757)\n        at org.apache.hadoop.mapred.JobClient.jobsToComplete(JobClient.java:741)\n        at org.apache.hadoop.mapred.ReliabilityTest.runTest(ReliabilityTest.java:219)\n        at org.apache.hadoop.mapred.ReliabilityTest.runSleepJobTest(ReliabilityTest.java:133)\n        at org.apache.hadoop.mapred.ReliabilityTest.run(ReliabilityTest.java:116)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)\n        at org.apache.hadoop.mapred.ReliabilityTest.main(ReliabilityTest.java:504)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)\n        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)\n        at org.apache.hadoop.test.MapredTestDriver.run(MapredTestDriver.java:111)\n        at org.apache.hadoop.test.MapredTestDriver.main(MapredTestDriver.java:118)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:192)\n```",
        "source_code": {
            "mapreduce.src.java.org.apache.hadoop.mapreduce.Cluster.getJobs": "  private Job[] getJobs(JobStatus[] stats) throws IOException {\n    List<Job> jobs = new ArrayList<Job>();\n    for (JobStatus stat : stats) {\n      jobs.add(new Job(this, stat, new JobConf(stat.getJobFile())));\n    }\n    return jobs.toArray(new Job[0]);\n  }",
            "mapreduce.src.java.org.apache.hadoop.mapreduce.Cluster.getAllJobs": "  public Job[] getAllJobs() throws IOException, InterruptedException {\n    return getJobs(client.getAllJobs());\n  }",
            "mapreduce.src.java.org.apache.hadoop.mapred.JobClient.getAllJobs": "  public JobStatus[] getAllJobs() throws IOException {\n    try {\n      org.apache.hadoop.mapreduce.JobStatus[] jobs = cluster.getAllJobStatuses();\n      JobStatus[] stats = new JobStatus[jobs.length];\n      for (int i = 0; i < jobs.length; i++) {\n        stats[i] = JobStatus.downgrade(jobs[i]);\n      }\n      return stats;\n    } catch (InterruptedException ie) {\n      throw new IOException(ie);\n    }\n  }",
            "mapreduce.src.java.org.apache.hadoop.mapred.JobClient.jobsToComplete": "  public JobStatus[] jobsToComplete() throws IOException {\n    List<JobStatus> stats = new ArrayList<JobStatus>();\n    for (JobStatus stat : getAllJobs()) {\n      if (!stat.isJobComplete()) {\n        stats.add(stat);\n      }\n    }\n    return stats.toArray(new JobStatus[0]);\n  }",
            "mapreduce.src.java.org.apache.hadoop.mapred.JobStatus.downgrade": "  public static JobStatus downgrade(org.apache.hadoop.mapreduce.JobStatus stat){\n    JobStatus old = new JobStatus(JobID.downgrade(stat.getJobID()),\n      stat.getSetupProgress(), stat.getMapProgress(), stat.getReduceProgress(),\n      stat.getCleanupProgress(), stat.getState().getValue(), \n      JobPriority.valueOf(stat.getPriority().name()),\n      stat.getUsername(), stat.getJobName(), stat.getJobFile(),\n      stat.getTrackingUrl());\n    old.setStartTime(stat.getStartTime());\n    old.setFinishTime(stat.getFinishTime());\n    old.setSchedulingInfo(stat.getSchedulingInfo());\n    old.setHistoryFile(stat.getHistoryFile());\n    return old;\n  }",
            "mapreduce.src.java.org.apache.hadoop.mapred.JobStatus.setFinishTime": "   protected synchronized void setFinishTime(long finishTime) {\n     super.setFinishTime(finishTime);\n   }",
            "mapreduce.src.java.org.apache.hadoop.mapred.JobStatus.setSchedulingInfo": "   protected synchronized void setSchedulingInfo(String schedulingInfo) {\n     super.setSchedulingInfo(schedulingInfo);\n   }",
            "mapreduce.src.java.org.apache.hadoop.mapred.JobStatus.setStartTime": "   protected synchronized void setStartTime(long startTime) { \n     super.setStartTime(startTime);\n   }",
            "mapreduce.src.java.org.apache.hadoop.mapred.JobStatus.getJobID": "  public JobID getJobID() { return JobID.downgrade(super.getJobID()); }",
            "mapreduce.src.java.org.apache.hadoop.mapred.JobStatus.setHistoryFile": "   protected synchronized void setHistoryFile(String historyFile) {\n     super.setHistoryFile(historyFile);\n   }"
        },
        "bug_report": {
            "Title": "MR279: MRReliabilityTest job fails because of missing job-file.",
            "Description": "The ApplicationReport should have the jobFile (e.g. hdfs://localhost:9000/tmp/hadoop-<USER>/mapred/staging/<USER>/.staging/job_201107121640_0001/job.xml)\n\n\nWithout it, jobs such as MRReliabilityTest fail with the following error (caused by the fact that jobFile is hardcoded to \"\" in TypeConverter.java):\ne.g. java.lang.IllegalArgumentException: Can not create a Path from an empty string\n        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:88)\n        at org.apache.hadoop.fs.Path.<init>(Path.java:96)\n        at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:445)\n        at org.apache.hadoop.mapreduce.Cluster.getJobs(Cluster.java:104)\n        at org.apache.hadoop.mapreduce.Cluster.getAllJobs(Cluster.java:218)\n        at org.apache.hadoop.mapred.JobClient.getAllJobs(JobClient.java:757)\n        at org.apache.hadoop.mapred.JobClient.jobsToComplete(JobClient.java:741)\n        at org.apache.hadoop.mapred.ReliabilityTest.runTest(ReliabilityTest.java:219)\n        at org.apache.hadoop.mapred.ReliabilityTest.runSleepJobTest(ReliabilityTest.java:133)\n        at org.apache.hadoop.mapred.ReliabilityTest.run(ReliabilityTest.java:116)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)\n        at org.apache.hadoop.mapred.ReliabilityTest.main(ReliabilityTest.java:504)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)\n        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)\n        at org.apache.hadoop.test.MapredTestDriver.run(MapredTestDriver.java:111)\n        at org.apache.hadoop.test.MapredTestDriver.main(MapredTestDriver.java:118)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:192)"
        }
    },
    {
        "filename": "MAPREDUCE-6702.json",
        "creation_time": "2016-05-17T22:11:38.000+0000",
        "stack_trace": "```\njava.lang.AssertionError: The environment checker job failed.\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.apache.hadoop.mapred.TestMiniMRChildTask.runTestTaskEnv(TestMiniMRChildTask.java:550)\n\tat org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskEnv(TestMiniMRChildTask.java:472)\n\njava.lang.AssertionError: The environment checker job failed.\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.apache.hadoop.mapred.TestMiniMRChildTask.runTestTaskEnv(TestMiniMRChildTask.java:550)\n\tat org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskOldEnv(TestMiniMRChildTask.java:496)\n```",
        "source_code": {},
        "bug_report": {
            "Title": "TestMiniMRChildTask.testTaskEnv and TestMiniMRChildTask.testTaskOldEnv are failing",
            "Description": "-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\nRunning org.apache.hadoop.mapred.TestMiniMRChildTask\nTests run: 3, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 92.48 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestMiniMRChildTask\ntestTaskEnv(org.apache.hadoop.mapred.TestMiniMRChildTask)  Time elapsed: 21.906 sec  <<< FAILURE!\njava.lang.AssertionError: The environment checker job failed.\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.apache.hadoop.mapred.TestMiniMRChildTask.runTestTaskEnv(TestMiniMRChildTask.java:550)\n\tat org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskEnv(TestMiniMRChildTask.java:472)\n\ntestTaskOldEnv(org.apache.hadoop.mapred.TestMiniMRChildTask)  Time elapsed: 17.452 sec  <<< FAILURE!\njava.lang.AssertionError: The environment checker job failed.\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.apache.hadoop.mapred.TestMiniMRChildTask.runTestTaskEnv(TestMiniMRChildTask.java:550)\n\tat org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskOldEnv(TestMiniMRChildTask.java:496)"
        }
    },
    {
        "filename": "MAPREDUCE-4748.json",
        "creation_time": "2012-10-24T21:53:19.000+0000",
        "stack_trace": "```\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: T_ATTEMPT_SUCCEEDED at SUCCEEDED\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:604)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:89)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:914)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:908)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n        at java.lang.Thread.run(Thread.java:619)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle": "  public void handle(TaskEvent event) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Processing \" + event.getTaskID() + \" of type \"\n          + event.getType());\n    }\n    try {\n      writeLock.lock();\n      TaskStateInternal oldState = getInternalState();\n      try {\n        stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state for \"\n            + this.taskId, e);\n        internalError(event.getType());\n      }\n      if (oldState != getInternalState()) {\n        LOG.info(taskId + \" Task Transitioned from \" + oldState + \" to \"\n            + getInternalState());\n      }\n\n    } finally {\n      writeLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.getInternalState": "  public TaskStateInternal getInternalState() {\n    readLock.lock();\n    try {\n      return stateMachine.getCurrentState();\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.internalError": "  protected void internalError(TaskEventType type) {\n    LOG.error(\"Invalid event \" + type + \" on Task \" + this.taskId);\n    eventHandler.handle(new JobDiagnosticsUpdateEvent(\n        this.taskId.getJobId(), \"Invalid event \" + type + \n        \" on Task \" + this.taskId));\n    eventHandler.handle(new JobEvent(this.taskId.getJobId(),\n        JobEventType.INTERNAL_ERROR));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.handle": "    public void handle(SpeculatorEvent event) {\n      if (disabled) {\n        return;\n      }\n\n      TaskId tId = event.getTaskID();\n      TaskType tType = null;\n      /* event's TaskId will be null if the event type is JOB_CREATE or\n       * ATTEMPT_STATUS_UPDATE\n       */\n      if (tId != null) {\n        tType = tId.getTaskType(); \n      }\n      boolean shouldMapSpec =\n              conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false);\n      boolean shouldReduceSpec =\n              conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false);\n\n      /* The point of the following is to allow the MAP and REDUCE speculative\n       * config values to be independent:\n       * IF spec-exec is turned on for maps AND the task is a map task\n       * OR IF spec-exec is turned on for reduces AND the task is a reduce task\n       * THEN call the speculator to handle the event.\n       */\n      if ( (shouldMapSpec && (tType == null || tType == TaskType.MAP))\n        || (shouldReduceSpec && (tType == null || tType == TaskType.REDUCE))) {\n        // Speculator IS enabled, direct the event to there.\n        speculator.handle(event);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJob": "    public Job getJob(JobId jobID) {\n      return jobs.get(jobID);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob": "  public void shutDownJob() {\n    // job has finished\n    // this is the only job, so shut down the Appmaster\n    // note in a workflow scenario, this may lead to creation of a new\n    // job (FIXME?)\n    // Send job-end notification\n    if (getConfig().get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL) != null) {\n      try {\n        LOG.info(\"Job end notification started for jobID : \"\n            + job.getReport().getJobId());\n        JobEndNotifier notifier = new JobEndNotifier();\n        notifier.setConf(getConfig());\n        notifier.notify(job.getReport());\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Job end notification interrupted for jobID : \"\n            + job.getReport().getJobId(), ie);\n      }\n    }\n\n    // TODO:currently just wait for some time so clients can know the\n    // final states. Will be removed once RM come on.\n    try {\n      Thread.sleep(5000);\n    } catch (InterruptedException e) {\n      e.printStackTrace();\n    }\n\n    try {\n      //We are finishing cleanly so this is the last retry\n      isLastAMRetry = true;\n      // Stop all services\n      // This will also send the final report to the ResourceManager\n      LOG.info(\"Calling stop for all the services\");\n      MRAppMaster.this.stop();\n\n    } catch (Throwable t) {\n      LOG.warn(\"Graceful stop failed \", t);\n    }\n\n    //Bring the process down by force.\n    //Not needed after HADOOP-7140\n    LOG.info(\"Exiting MR AppMaster..GoodBye!\");\n    sysexit();   \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJobId": "  public JobId getJobId() {\n    return jobId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.start": "  public void start() {\n\n    // Pull completedTasks etc from recovery\n    if (inRecovery) {\n      completedTasksFromPreviousRun = recoveryServ.getCompletedTasks();\n      amInfos = recoveryServ.getAMInfos();\n    }\n\n    // / Create the AMInfo for the current AppMaster\n    if (amInfos == null) {\n      amInfos = new LinkedList<AMInfo>();\n    }\n    AMInfo amInfo =\n        MRBuilderUtils.newAMInfo(appAttemptID, startTime, containerID, nmHost,\n            nmPort, nmHttpPort);\n    amInfos.add(amInfo);\n\n    // /////////////////// Create the job itself.\n    job = createJob(getConfig());\n\n    // End of creating the job.\n\n    // Send out an MR AM inited event for this AM and all previous AMs.\n    for (AMInfo info : amInfos) {\n      dispatcher.getEventHandler().handle(\n          new JobHistoryEvent(job.getID(), new AMStartedEvent(info\n              .getAppAttemptId(), info.getStartTime(), info.getContainerId(),\n              info.getNodeManagerHost(), info.getNodeManagerPort(), info\n                  .getNodeManagerHttpPort())));\n    }\n\n    // metrics system init is really init & start.\n    // It's more test friendly to put it here.\n    DefaultMetricsSystem.initialize(\"MRAppMaster\");\n\n    // create a job event for job intialization\n    JobEvent initJobEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT);\n    // Send init to the job (this does NOT trigger job execution)\n    // This is a synchronous call, not an event through dispatcher. We want\n    // job-init to be done completely here.\n    jobEventDispatcher.handle(initJobEvent);\n\n\n    // JobImpl's InitTransition is done (call above is synchronous), so the\n    // \"uber-decision\" (MR-1220) has been made.  Query job and switch to\n    // ubermode if appropriate (by registering different container-allocator\n    // and container-launcher services/event-handlers).\n\n    if (job.isUber()) {\n      speculatorEventDispatcher.disableSpeculation();\n      LOG.info(\"MRAppMaster uberizing job \" + job.getID()\n               + \" in local container (\\\"uber-AM\\\") on node \"\n               + nmHost + \":\" + nmPort + \".\");\n    } else {\n      // send init to speculator only for non-uber jobs. \n      // This won't yet start as dispatcher isn't started yet.\n      dispatcher.getEventHandler().handle(\n          new SpeculatorEvent(job.getID(), clock.getTime()));\n      LOG.info(\"MRAppMaster launching normal, non-uberized, multi-container \"\n               + \"job \" + job.getID() + \".\");\n    }\n\n    //start all the components\n    super.start();\n\n    // All components have started, start the job.\n    startJobs();\n  }"
        },
        "bug_report": {
            "Title": "Invalid event: T_ATTEMPT_SUCCEEDED at SUCCEEDED",
            "Description": "We saw this happen when running a large pig script.\n\n{noformat}\n2012-10-23 22:45:24,986 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: Can't handle this event at current state for task_1350837501057_21978_m_040453\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: T_ATTEMPT_SUCCEEDED at SUCCEEDED\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:604)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:89)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:914)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:908)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n        at java.lang.Thread.run(Thread.java:619)\n{noformat}\n\nSpeculative execution was enabled, and that task did speculate so it looks like this is an error in the state machine either between the task attempts or just within that single task."
        }
    },
    {
        "filename": "MAPREDUCE-3062.json",
        "creation_time": "2011-09-21T17:31:02.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.admin.address\n\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)\n\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)\n\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.init(AdminService.java:88)\n\tat org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.init(ResourceManager.java:191)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:497)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.createSocketAddr": "  public static InetSocketAddress createSocketAddr(String target,\n                                                   int defaultPort) {\n    if (target == null) {\n      throw new IllegalArgumentException(\"Target address cannot be null.\");\n    }\n    int colonIndex = target.indexOf(':');\n    if (colonIndex < 0 && defaultPort == -1) {\n      throw new RuntimeException(\"Not a host:port pair: \" + target);\n    }\n    String hostname;\n    int port = -1;\n    if (!target.contains(\"/\")) {\n      if (colonIndex == -1) {\n        hostname = target;\n      } else {\n        // must be the old style <host>:<port>\n        hostname = target.substring(0, colonIndex);\n        port = Integer.parseInt(target.substring(colonIndex + 1));\n      }\n    } else {\n      // a new uri\n      URI addr = new Path(target).toUri();\n      hostname = addr.getHost();\n      port = addr.getPort();\n    }\n\n    if (port == -1) {\n      port = defaultPort;\n    }\n  \n    if (getStaticResolution(hostname) != null) {\n      hostname = getStaticResolution(hostname);\n    }\n    return new InetSocketAddress(hostname, port);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.getStaticResolution": "  public static String getStaticResolution(String host) {\n    synchronized (hostToResolved) {\n      return hostToResolved.get(host);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.init": "  public void init(Configuration conf) {\n    super.init(conf);\n    String bindAddress =\n      conf.get(YarnConfiguration.RM_ADMIN_ADDRESS,\n          YarnConfiguration.RM_ADMIN_ADDRESS);\n    masterServiceAddress =  NetUtils.createSocketAddr(bindAddress);\n    adminAcl = \n      new AccessControlList(\n          conf.get(YarnConfiguration.RM_ADMIN_ACL, YarnConfiguration.DEFAULT_RM_ADMIN_ACL));\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.service.CompositeService.init": "  public synchronized void init(Configuration conf) {\n    for (Service service : serviceList) {\n      service.init(conf);\n    }\n    super.init(conf);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.init": "  public synchronized void init(Configuration conf) {\n\n    this.rmDispatcher = new AsyncDispatcher();\n    addIfService(this.rmDispatcher);\n\n    this.containerAllocationExpirer = new ContainerAllocationExpirer(\n        this.rmDispatcher);\n    addService(this.containerAllocationExpirer);\n\n    AMLivelinessMonitor amLivelinessMonitor = createAMLivelinessMonitor();\n    addService(amLivelinessMonitor);\n\n    this.rmContext = new RMContextImpl(this.store, this.rmDispatcher,\n        this.containerAllocationExpirer, amLivelinessMonitor);\n\n    addService(nodesListManager);\n\n    // Initialize the config\n    this.conf = new YarnConfiguration(conf);\n    // Initialize the scheduler\n    this.scheduler = createScheduler();\n    this.schedulerDispatcher = new SchedulerEventDispatcher(this.scheduler);\n    addService(this.schedulerDispatcher);\n    this.rmDispatcher.register(SchedulerEventType.class,\n        this.schedulerDispatcher);\n\n    // Register event handler for RmAppEvents\n    this.rmDispatcher.register(RMAppEventType.class,\n        new ApplicationEventDispatcher(this.rmContext));\n\n    // Register event handler for RmAppAttemptEvents\n    this.rmDispatcher.register(RMAppAttemptEventType.class,\n        new ApplicationAttemptEventDispatcher(this.rmContext));\n\n    // Register event handler for RmNodes\n    this.rmDispatcher.register(RMNodeEventType.class,\n        new NodeEventDispatcher(this.rmContext));\n\n    //TODO change this to be random\n    this.appTokenSecretManager.setMasterKey(ApplicationTokenSecretManager\n        .createSecretKey(\"Dummy\".getBytes()));\n\n    this.nmLivelinessMonitor = createNMLivelinessMonitor();\n    addService(this.nmLivelinessMonitor);\n\n    this.resourceTracker = createResourceTrackerService();\n    addService(resourceTracker);\n  \n    try {\n      this.scheduler.reinitialize(this.conf,\n          this.containerTokenSecretManager, this.rmContext);\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Failed to initialize scheduler\", ioe);\n    }\n\n    masterService = createApplicationMasterService();\n    addService(masterService) ;\n\n    this.rmAppManager = createRMAppManager();\n    // Register event handler for RMAppManagerEvents\n    this.rmDispatcher.register(RMAppManagerEventType.class,\n        this.rmAppManager);\n\n    clientRM = createClientRMService();\n    addService(clientRM);\n    \n    adminService = createAdminService();\n    addService(adminService);\n\n    this.applicationMasterLauncher = createAMLauncher();\n    addService(applicationMasterLauncher);\n\n    super.init(conf);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createNMLivelinessMonitor": "  private NMLivelinessMonitor createNMLivelinessMonitor() {\n    return new NMLivelinessMonitor(this.rmContext\n        .getDispatcher());\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createClientRMService": "  protected ClientRMService createClientRMService() {\n    return new ClientRMService(this.rmContext, scheduler, this.rmAppManager);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createResourceTrackerService": "  protected ResourceTrackerService createResourceTrackerService() {\n    return new ResourceTrackerService(this.rmContext, this.nodesListManager,\n        this.nmLivelinessMonitor, this.containerTokenSecretManager);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAMLivelinessMonitor": "  protected AMLivelinessMonitor createAMLivelinessMonitor() {\n    return new AMLivelinessMonitor(this.rmDispatcher);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAMLauncher": "  protected ApplicationMasterLauncher createAMLauncher() {\n    return new ApplicationMasterLauncher(\n        this.appTokenSecretManager, this.clientToAMSecretManager,\n        this.rmContext);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createApplicationMasterService": "  protected ApplicationMasterService createApplicationMasterService() {\n    return new ApplicationMasterService(this.rmContext,\n        this.appTokenSecretManager, scheduler);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.addIfService": "  protected void addIfService(Object object) {\n    if (object instanceof Service) {\n      addService((Service) object);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAdminService": "  protected AdminService createAdminService() {\n    return new AdminService(conf, scheduler, rmContext, this.nodesListManager);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createRMAppManager": "  protected RMAppManager createRMAppManager() {\n    return new RMAppManager(this.rmContext, this.clientToAMSecretManager,\n        this.scheduler, this.masterService, this.conf);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createScheduler": "  protected ResourceScheduler createScheduler() {\n    return \n    ReflectionUtils.newInstance(\n        conf.getClass(YarnConfiguration.RM_SCHEDULER, \n            FifoScheduler.class, ResourceScheduler.class), \n        this.conf);\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main": "  public static void main(String argv[]) {\n    StringUtils.startupShutdownMessage(ResourceManager.class, argv, LOG);\n    try {\n      Configuration conf = new YarnConfiguration();\n      Store store =  StoreFactory.getStore(conf);\n      ResourceManager resourceManager = new ResourceManager(store);\n      Runtime.getRuntime().addShutdownHook(\n          new CompositeServiceShutdownHook(resourceManager));\n      resourceManager.init(conf);\n      //resourceManager.recover(store.restore());\n      //store.doneWithRecovery();\n      resourceManager.start();\n    } catch (Throwable t) {\n      LOG.fatal(\"Error starting ResourceManager\", t);\n      System.exit(-1);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start": "  public void start() {\n    try {\n      doSecureLogin();\n    } catch(IOException ie) {\n      throw new YarnException(\"Failed to login\", ie);\n    }\n\n    startWepApp();\n    DefaultMetricsSystem.initialize(\"ResourceManager\");\n\n    super.start();\n\n    /*synchronized(shutdown) {\n      try {\n        while(!shutdown.get()) {\n          shutdown.wait();\n        }\n      } catch(InterruptedException ie) {\n        LOG.info(\"Interrupted while waiting\", ie);\n      }\n    }*/\n  }"
        },
        "bug_report": {
            "Title": "YARN NM/RM fail to start",
            "Description": "2011-09-21 10:21:41,932 FATAL resourcemanager.ResourceManager (ResourceManager.java:main(502)) - Error starting ResourceManager\njava.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.admin.address\n\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)\n\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)\n\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.init(AdminService.java:88)\n\tat org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.init(ResourceManager.java:191)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:497)\n\nAnother copy and paste issue. Similar to https://issues.apache.org/jira/browse/MAPREDUCE-3042."
        }
    },
    {
        "filename": "MAPREDUCE-5724.json",
        "creation_time": "2014-01-15T00:58:12.000+0000",
        "stack_trace": "```\norg.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://localhost:8020/tmp/hadoop-yarn/staging/history/done]\n\tat org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:505)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistory.serviceInit(JobHistory.java:94)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n\tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:143)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.launchJobHistoryServer(JobHistoryServer.java:207)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:217)\nCaused by: java.net.ConnectException: Call From dontknow.local/172.20.10.4 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1410)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1359)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n\tat com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:185)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:101)\n\tat com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:671)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1722)\n\tat org.apache.hadoop.fs.Hdfs.getFileStatus(Hdfs.java:124)\n\tat org.apache.hadoop.fs.FileContext$14.next(FileContext.java:1106)\n\tat org.apache.hadoop.fs.FileContext$14.next(FileContext.java:1102)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.getFileStatus(FileContext.java:1102)\n\tat org.apache.hadoop.fs.FileContext$Util.exists(FileContext.java:1514)\n\tat org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.mkdir(HistoryFileManager.java:561)\n\tat org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:502)\n\t... 8 more\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:601)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:696)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2700(Client.java:367)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1458)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1377)```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit": "  protected void serviceInit(Configuration conf) throws Exception {\n    this.conf = conf;\n\n    int serialNumberLowDigits = 3;\n    serialNumberFormat = (\"%0\"\n        + (JobHistoryUtils.SERIAL_NUMBER_DIRECTORY_DIGITS + serialNumberLowDigits)\n        + \"d\");\n\n    String doneDirPrefix = null;\n    doneDirPrefix = JobHistoryUtils\n        .getConfiguredHistoryServerDoneDirPrefix(conf);\n    try {\n      doneDirPrefixPath = FileContext.getFileContext(conf).makeQualified(\n          new Path(doneDirPrefix));\n      doneDirFc = FileContext.getFileContext(doneDirPrefixPath.toUri(), conf);\n      doneDirFc.setUMask(JobHistoryUtils.HISTORY_DONE_DIR_UMASK);\n      mkdir(doneDirFc, doneDirPrefixPath, new FsPermission(\n          JobHistoryUtils.HISTORY_DONE_DIR_PERMISSION));\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Error creating done directory: [\"\n          + doneDirPrefixPath + \"]\", e);\n    }\n\n    String intermediateDoneDirPrefix = null;\n    intermediateDoneDirPrefix = JobHistoryUtils\n        .getConfiguredHistoryIntermediateDoneDirPrefix(conf);\n    try {\n      intermediateDoneDirPath = FileContext.getFileContext(conf).makeQualified(\n          new Path(intermediateDoneDirPrefix));\n      intermediateDoneDirFc = FileContext.getFileContext(\n          intermediateDoneDirPath.toUri(), conf);\n      mkdir(intermediateDoneDirFc, intermediateDoneDirPath, new FsPermission(\n          JobHistoryUtils.HISTORY_INTERMEDIATE_DONE_DIR_PERMISSIONS.toShort()));\n    } catch (IOException e) {\n      LOG.info(\"error creating done directory on dfs \" + e);\n      throw new YarnRuntimeException(\"Error creating intermediate done directory: [\"\n          + intermediateDoneDirPath + \"]\", e);\n    }\n\n    this.aclsMgr = new JobACLsManager(conf);\n\n    maxHistoryAge = conf.getLong(JHAdminConfig.MR_HISTORY_MAX_AGE_MS,\n        JHAdminConfig.DEFAULT_MR_HISTORY_MAX_AGE);\n    \n    jobListCache = createJobListCache();\n\n    serialNumberIndex = new SerialNumberIndex(conf.getInt(\n        JHAdminConfig.MR_HISTORY_DATESTRING_CACHE_SIZE,\n        JHAdminConfig.DEFAULT_MR_HISTORY_DATESTRING_CACHE_SIZE));\n\n    int numMoveThreads = conf.getInt(\n        JHAdminConfig.MR_HISTORY_MOVE_THREAD_COUNT,\n        JHAdminConfig.DEFAULT_MR_HISTORY_MOVE_THREAD_COUNT);\n    ThreadFactory tf = new ThreadFactoryBuilder().setNameFormat(\n        \"MoveIntermediateToDone Thread #%d\").build();\n    moveToDoneExecutor = new ThreadPoolExecutor(numMoveThreads, numMoveThreads,\n        1, TimeUnit.HOURS, new LinkedBlockingQueue<Runnable>(), tf);\n\n    super.serviceInit(conf);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.mkdir": "  private void mkdir(FileContext fc, Path path, FsPermission fsp)\n      throws IOException {\n    if (!fc.util().exists(path)) {\n      try {\n        fc.mkdir(path, fsp, true);\n\n        FileStatus fsStatus = fc.getFileStatus(path);\n        LOG.info(\"Perms after creating \" + fsStatus.getPermission().toShort()\n            + \", Expected: \" + fsp.toShort());\n        if (fsStatus.getPermission().toShort() != fsp.toShort()) {\n          LOG.info(\"Explicitly setting permissions to : \" + fsp.toShort()\n              + \", \" + fsp);\n          fc.setPermission(path, fsp);\n        }\n      } catch (FileAlreadyExistsException e) {\n        LOG.info(\"Directory: [\" + path + \"] already exists.\");\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.createJobListCache": "  protected JobListCache createJobListCache() {\n    return new JobListCache(conf.getInt(\n        JHAdminConfig.MR_HISTORY_JOBLIST_CACHE_SIZE,\n        JHAdminConfig.DEFAULT_MR_HISTORY_JOBLIST_CACHE_SIZE), maxHistoryAge);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.init": "  public void init(Configuration conf) {\n    if (conf == null) {\n      throw new ServiceStateException(\"Cannot initialize service \"\n                                      + getName() + \": null configuration\");\n    }\n    if (isInState(STATE.INITED)) {\n      return;\n    }\n    synchronized (stateChangeLock) {\n      if (enterState(STATE.INITED) != STATE.INITED) {\n        setConfig(conf);\n        try {\n          serviceInit(config);\n          if (isInState(STATE.INITED)) {\n            //if the service ended up here during init,\n            //notify the listeners\n            notifyListeners();\n          }\n        } catch (Exception e) {\n          noteFailure(e);\n          ServiceOperations.stopQuietly(LOG, this);\n          throw ServiceStateException.convert(e);\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.isInState": "  public final boolean isInState(Service.STATE expected) {\n    return stateModel.isInState(expected);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.setConfig": "  protected void setConfig(Configuration conf) {\n    this.config = conf;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.getName": "  public String getName() {\n    return name;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.notifyListeners": "  private void notifyListeners() {\n    try {\n      listeners.notifyListeners(this);\n      globalListeners.notifyListeners(this);\n    } catch (Throwable e) {\n      LOG.warn(\"Exception while notifying listeners of \" + this + \": \" + e,\n               e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.enterState": "  private STATE enterState(STATE newState) {\n    assert stateModel != null : \"null state in \" + name + \" \" + this.getClass();\n    STATE oldState = stateModel.enterState(newState);\n    if (oldState != newState) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\n          \"Service: \" + getName() + \" entered state \" + getServiceState());\n      }\n      recordLifecycleEvent();\n    }\n    return oldState;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.serviceInit": "  protected void serviceInit(Configuration conf) throws Exception {\n    if (conf != config) {\n      LOG.debug(\"Config has been overridden during init\");\n      setConfig(conf);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.noteFailure": "  protected final void noteFailure(Exception exception) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"noteFailure \" + exception, null);\n    }\n    if (exception == null) {\n      //make sure failure logic doesn't itself cause problems\n      return;\n    }\n    //record the failure details, and log it\n    synchronized (this) {\n      if (failureCause == null) {\n        failureCause = exception;\n        failureState = getServiceState();\n        LOG.info(\"Service \" + getName()\n                 + \" failed in state \" + failureState\n                 + \"; cause: \" + exception,\n                 exception);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.JobHistory.serviceInit": "  protected void serviceInit(Configuration conf) throws Exception {\n    LOG.info(\"JobHistory Init\");\n    this.conf = conf;\n    this.appID = ApplicationId.newInstance(0, 0);\n    this.appAttemptID = RecordFactoryProvider.getRecordFactory(conf)\n        .newRecordInstance(ApplicationAttemptId.class);\n\n    moveThreadInterval = conf.getLong(\n        JHAdminConfig.MR_HISTORY_MOVE_INTERVAL_MS,\n        JHAdminConfig.DEFAULT_MR_HISTORY_MOVE_INTERVAL_MS);\n\n    hsManager = createHistoryFileManager();\n    hsManager.init(conf);\n    try {\n      hsManager.initExisting();\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Failed to intialize existing directories\", e);\n    }\n\n    storage = createHistoryStorage();\n    \n    if (storage instanceof Service) {\n      ((Service) storage).init(conf);\n    }\n    storage.setHistoryFileManager(hsManager);\n\n    super.serviceInit(conf);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.JobHistory.createHistoryStorage": "  protected HistoryStorage createHistoryStorage() {\n    return ReflectionUtils.newInstance(conf.getClass(\n        JHAdminConfig.MR_HISTORY_STORAGE, CachedHistoryStorage.class,\n        HistoryStorage.class), conf);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.JobHistory.createHistoryFileManager": "  protected HistoryFileManager createHistoryFileManager() {\n    return new HistoryFileManager();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.CompositeService.serviceInit": "  protected void serviceInit(Configuration conf) throws Exception {\n    List<Service> services = getServices();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(getName() + \": initing services, size=\" + services.size());\n    }\n    for (Service service : services) {\n      service.init(conf);\n    }\n    super.serviceInit(conf);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.CompositeService.getServices": "  public List<Service> getServices() {\n    synchronized (serviceList) {\n      return Collections.unmodifiableList(serviceList);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit": "  protected void serviceInit(Configuration conf) throws Exception {\n    Configuration config = new YarnConfiguration(conf);\n\n    config.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    // This is required for WebApps to use https if enabled.\n    MRWebAppUtil.initialize(getConfig());\n    HttpConfig.setPolicy(MRWebAppUtil.getJHSHttpPolicy());\n    try {\n      doSecureLogin(conf);\n    } catch(IOException ie) {\n      throw new YarnRuntimeException(\"History Server Failed to login\", ie);\n    }\n    jobHistoryService = new JobHistory();\n    historyContext = (HistoryContext)jobHistoryService;\n    stateStore = createStateStore(conf);\n    this.jhsDTSecretManager = createJHSSecretManager(conf, stateStore);\n    clientService = createHistoryClientService();\n    aggLogDelService = new AggregatedLogDeletionService();\n    hsAdminServer = new HSAdminServer(aggLogDelService, jobHistoryService);\n    addService(stateStore);\n    addService(new HistoryServerSecretManagerService());\n    addService(jobHistoryService);\n    addService(clientService);\n    addService(aggLogDelService);\n    addService(hsAdminServer);\n    super.serviceInit(config);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.createStateStore": "  protected HistoryServerStateStoreService createStateStore(\n      Configuration conf) {\n    return HistoryServerStateStoreServiceFactory.getStore(conf);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.createHistoryClientService": "  protected HistoryClientService createHistoryClientService() {\n    return new HistoryClientService(historyContext, \n        this.jhsDTSecretManager);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.doSecureLogin": "  protected void doSecureLogin(Configuration conf) throws IOException {\n    SecurityUtil.login(conf, JHAdminConfig.MR_HISTORY_KEYTAB,\n        JHAdminConfig.MR_HISTORY_PRINCIPAL);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.createJHSSecretManager": "  protected JHSDelegationTokenSecretManager createJHSSecretManager(\n      Configuration conf, HistoryServerStateStoreService store) {\n    long secretKeyInterval = \n        conf.getLong(MRConfig.DELEGATION_KEY_UPDATE_INTERVAL_KEY, \n                     MRConfig.DELEGATION_KEY_UPDATE_INTERVAL_DEFAULT);\n      long tokenMaxLifetime =\n        conf.getLong(MRConfig.DELEGATION_TOKEN_MAX_LIFETIME_KEY,\n                     MRConfig.DELEGATION_TOKEN_MAX_LIFETIME_DEFAULT);\n      long tokenRenewInterval =\n        conf.getLong(MRConfig.DELEGATION_TOKEN_RENEW_INTERVAL_KEY, \n                     MRConfig.DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT);\n      \n    return new JHSDelegationTokenSecretManager(secretKeyInterval, \n        tokenMaxLifetime, tokenRenewInterval, 3600000, store);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.launchJobHistoryServer": "  static JobHistoryServer launchJobHistoryServer(String[] args) {\n    Thread.\n        setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\n    StringUtils.startupShutdownMessage(JobHistoryServer.class, args, LOG);\n    JobHistoryServer jobHistoryServer = null;\n    try {\n      jobHistoryServer = new JobHistoryServer();\n      ShutdownHookManager.get().addShutdownHook(\n          new CompositeServiceShutdownHook(jobHistoryServer),\n          SHUTDOWN_HOOK_PRIORITY);\n      YarnConfiguration conf = new YarnConfiguration(new JobConf());\n      jobHistoryServer.init(conf);\n      jobHistoryServer.start();\n    } catch (Throwable t) {\n      LOG.fatal(\"Error starting JobHistoryServer\", t);\n      ExitUtil.terminate(-1, \"Error starting JobHistoryServer\");\n    }\n    return jobHistoryServer;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-hs.src.main.java.org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main": "  public static void main(String[] args) {\n    launchJobHistoryServer(args);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.wrapWithMessage": "  private static <T extends IOException> T wrapWithMessage(\n      T exception, String msg) {\n    Class<? extends Throwable> clazz = exception.getClass();\n    try {\n      Constructor<? extends Throwable> ctor = clazz.getConstructor(String.class);\n      Throwable t = ctor.newInstance(msg);\n      return (T)(t.initCause(exception));\n    } catch (Throwable e) {\n      LOG.warn(\"Unable to wrap exception of type \" +\n          clazz + \": it has no (String) constructor\", e);\n      return exception;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.wrapException": "  public static IOException wrapException(final String destHost,\n                                          final int destPort,\n                                          final String localHost,\n                                          final int localPort,\n                                          final IOException exception) {\n    if (exception instanceof BindException) {\n      return new BindException(\n          \"Problem binding to [\"\n              + localHost\n              + \":\"\n              + localPort\n              + \"] \"\n              + exception\n              + \";\"\n              + see(\"BindException\"));\n    } else if (exception instanceof ConnectException) {\n      // connection refused; include the host:port in the error\n      return wrapWithMessage(exception, \n          \"Call From \"\n              + localHost\n              + \" to \"\n              + destHost\n              + \":\"\n              + destPort\n              + \" failed on connection exception: \"\n              + exception\n              + \";\"\n              + see(\"ConnectionRefused\"));\n    } else if (exception instanceof UnknownHostException) {\n      return wrapWithMessage(exception,\n          \"Invalid host name: \"\n              + getHostDetailsAsString(destHost, destPort, localHost)\n              + exception\n              + \";\"\n              + see(\"UnknownHost\"));\n    } else if (exception instanceof SocketTimeoutException) {\n      return wrapWithMessage(exception,\n          \"Call From \"\n              + localHost + \" to \" + destHost + \":\" + destPort\n              + \" failed on socket timeout exception: \" + exception\n              + \";\"\n              + see(\"SocketTimeout\"));\n    } else if (exception instanceof NoRouteToHostException) {\n      return wrapWithMessage(exception,\n          \"No Route to Host from  \"\n              + localHost + \" to \" + destHost + \":\" + destPort\n              + \" failed on socket timeout exception: \" + exception\n              + \";\"\n              + see(\"NoRouteToHost\"));\n    }\n    else {\n      return (IOException) new IOException(\"Failed on local exception: \"\n                                               + exception\n                                               + \"; Host Details : \"\n                                               + getHostDetailsAsString(destHost, destPort, localHost))\n          .initCause(exception);\n\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.getHostDetailsAsString": "  private static String getHostDetailsAsString(final String destHost,\n                                               final int destPort,\n                                               final String localHost) {\n    StringBuilder hostDetails = new StringBuilder(27);\n    hostDetails.append(\"local host is: \")\n        .append(quoteHost(localHost))\n        .append(\"; \");\n    hostDetails.append(\"destination host is: \").append(quoteHost(destHost))\n        .append(\":\")\n        .append(destPort).append(\"; \");\n    return hostDetails.toString();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.see": "  private static String see(final String entry) {\n    return FOR_MORE_DETAILS_SEE + HADOOP_WIKI + entry;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.call": "  public Writable call(RPC.RpcKind rpcKind, Writable rpcRequest,\n      ConnectionId remoteId, int serviceClass) throws IOException {\n    final Call call = createCall(rpcKind, rpcRequest);\n    Connection connection = getConnection(remoteId, call, serviceClass);\n    try {\n      connection.sendRpcRequest(call);                 // send the rpc request\n    } catch (RejectedExecutionException e) {\n      throw new IOException(\"connection has been closed\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      LOG.warn(\"interrupted waiting to send rpc request to server\", e);\n      throw new IOException(e);\n    }\n\n    boolean interrupted = false;\n    synchronized (call) {\n      while (!call.done) {\n        try {\n          call.wait();                           // wait for the result\n        } catch (InterruptedException ie) {\n          // save the fact that we were interrupted\n          interrupted = true;\n        }\n      }\n\n      if (interrupted) {\n        // set the interrupt flag now that we are done waiting\n        Thread.currentThread().interrupt();\n      }\n\n      if (call.error != null) {\n        if (call.error instanceof RemoteException) {\n          call.error.fillInStackTrace();\n          throw call.error;\n        } else { // local exception\n          InetSocketAddress address = connection.getRemoteAddress();\n          throw NetUtils.wrapException(address.getHostName(),\n                  address.getPort(),\n                  NetUtils.getHostname(),\n                  0,\n                  call.error);\n        }\n      } else {\n        return call.getRpcResponse();\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnectionId": "    static ConnectionId getConnectionId(InetSocketAddress addr,\n        Class<?> protocol, UserGroupInformation ticket, int rpcTimeout,\n        RetryPolicy connectionRetryPolicy, Configuration conf) throws IOException {\n\n      if (connectionRetryPolicy == null) {\n        final int max = conf.getInt(\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY,\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_DEFAULT);\n        final int retryInterval = conf.getInt(\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_RETRY_INTERVAL_KEY,\n            CommonConfigurationKeysPublic\n                .IPC_CLIENT_CONNECT_RETRY_INTERVAL_DEFAULT);\n\n        connectionRetryPolicy = RetryPolicies.retryUpToMaximumCountWithFixedSleep(\n            max, retryInterval, TimeUnit.MILLISECONDS);\n      }\n\n      boolean doPing =\n        conf.getBoolean(CommonConfigurationKeys.IPC_CLIENT_PING_KEY, true);\n      return new ConnectionId(addr, protocol, ticket, rpcTimeout,\n          conf.getInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECTION_MAXIDLETIME_KEY,\n              CommonConfigurationKeysPublic.IPC_CLIENT_CONNECTION_MAXIDLETIME_DEFAULT),\n          connectionRetryPolicy,\n          conf.getInt(\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_KEY,\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_DEFAULT),\n          conf.getBoolean(CommonConfigurationKeysPublic.IPC_CLIENT_TCPNODELAY_KEY,\n              CommonConfigurationKeysPublic.IPC_CLIENT_TCPNODELAY_DEFAULT),\n          doPing, \n          (doPing ? Client.getPingInterval(conf) : 0));\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRpcResponse": "    public synchronized Writable getRpcResponse() {\n      return rpcResponse;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRemoteAddress": "    public InetSocketAddress getRemoteAddress() {\n      return server;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.sendRpcRequest": "    public void sendRpcRequest(final Call call)\n        throws InterruptedException, IOException {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n\n      // Serialize the call to be sent. This is done from the actual\n      // caller thread, rather than the sendParamsExecutor thread,\n      \n      // so that if the serialization throws an error, it is reported\n      // properly. This also parallelizes the serialization.\n      //\n      // Format of a call on the wire:\n      // 0) Length of rest below (1 + 2)\n      // 1) RpcRequestHeader  - is serialized Delimited hence contains length\n      // 2) RpcRequest\n      //\n      // Items '1' and '2' are prepared here. \n      final DataOutputBuffer d = new DataOutputBuffer();\n      RpcRequestHeaderProto header = ProtoUtil.makeRpcRequestHeader(\n          call.rpcKind, OperationProto.RPC_FINAL_PACKET, call.id, call.retry,\n          clientId);\n      header.writeDelimitedTo(d);\n      call.rpcRequest.write(d);\n\n      synchronized (sendRpcRequestLock) {\n        Future<?> senderFuture = sendParamsExecutor.submit(new Runnable() {\n          @Override\n          public void run() {\n            try {\n              synchronized (Connection.this.out) {\n                if (shouldCloseConnection.get()) {\n                  return;\n                }\n                \n                if (LOG.isDebugEnabled())\n                  LOG.debug(getName() + \" sending #\" + call.id);\n         \n                byte[] data = d.getData();\n                int totalLength = d.getLength();\n                out.writeInt(totalLength); // Total Length\n                out.write(data, 0, totalLength);// RpcRequestHeader + RpcRequest\n                out.flush();\n              }\n            } catch (IOException e) {\n              // exception at this point would leave the connection in an\n              // unrecoverable state (eg half a call left on the wire).\n              // So, close the connection, killing any outstanding calls\n              markClosed(e);\n            } finally {\n              //the buffer is just an in-memory buffer, but it is still polite to\n              // close early\n              IOUtils.closeStream(d);\n            }\n          }\n        });\n      \n        try {\n          senderFuture.get();\n        } catch (ExecutionException e) {\n          Throwable cause = e.getCause();\n          \n          // cause should only be a RuntimeException as the Runnable above\n          // catches IOException\n          if (cause instanceof RuntimeException) {\n            throw (RuntimeException) cause;\n          } else {\n            throw new RuntimeException(\"unexpected checked exception\", cause);\n          }\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.createCall": "  Call createCall(RPC.RpcKind rpcKind, Writable rpcRequest) {\n    return new Call(rpcKind, rpcRequest);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnection": "  private Connection getConnection(ConnectionId remoteId,\n      Call call, int serviceClass) throws IOException {\n    if (!running.get()) {\n      // the client is stopped\n      throw new IOException(\"The client is stopped\");\n    }\n    Connection connection;\n    /* we could avoid this allocation for each RPC by having a  \n     * connectionsId object and with set() method. We need to manage the\n     * refs for keys in HashMap properly. For now its ok.\n     */\n    do {\n      synchronized (connections) {\n        connection = connections.get(remoteId);\n        if (connection == null) {\n          connection = new Connection(remoteId, serviceClass);\n          connections.put(remoteId, connection);\n        }\n      }\n    } while (!connection.addCall(call));\n    \n    //we don't invoke the method below inside \"synchronized (connections)\"\n    //block above. The reason for that is if the server happens to be slow,\n    //it will take longer to establish a connection and that will slow the\n    //entire system down.\n    connection.setupIOstreams();\n    return connection;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.invoke": "    public Object invoke(Object proxy, Method method, Object[] args)\n        throws ServiceException {\n      long startTime = 0;\n      if (LOG.isDebugEnabled()) {\n        startTime = Time.now();\n      }\n      \n      if (args.length != 2) { // RpcController + Message\n        throw new ServiceException(\"Too many parameters for request. Method: [\"\n            + method.getName() + \"]\" + \", Expected: 2, Actual: \"\n            + args.length);\n      }\n      if (args[1] == null) {\n        throw new ServiceException(\"null param while calling Method: [\"\n            + method.getName() + \"]\");\n      }\n\n      RequestHeaderProto rpcRequestHeader = constructRpcRequestHeader(method);\n      \n      if (LOG.isTraceEnabled()) {\n        LOG.trace(Thread.currentThread().getId() + \": Call -> \" +\n            remoteId + \": \" + method.getName() +\n            \" {\" + TextFormat.shortDebugString((Message) args[1]) + \"}\");\n      }\n\n\n      Message theRequest = (Message) args[1];\n      final RpcResponseWrapper val;\n      try {\n        val = (RpcResponseWrapper) client.call(RPC.RpcKind.RPC_PROTOCOL_BUFFER,\n            new RpcRequestWrapper(rpcRequestHeader, theRequest), remoteId);\n\n      } catch (Throwable e) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Exception <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + e + \"}\");\n        }\n\n        throw new ServiceException(e);\n      }\n\n      if (LOG.isDebugEnabled()) {\n        long callTime = Time.now() - startTime;\n        LOG.debug(\"Call: \" + method.getName() + \" took \" + callTime + \"ms\");\n      }\n      \n      Message prototype = null;\n      try {\n        prototype = getReturnProtoType(method);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      Message returnMessage;\n      try {\n        returnMessage = prototype.newBuilderForType()\n            .mergeFrom(val.theResponseRead).build();\n\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Response <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + TextFormat.shortDebugString(returnMessage) + \"}\");\n        }\n\n      } catch (Throwable e) {\n        throw new ServiceException(e);\n      }\n      return returnMessage;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.constructRpcRequestHeader": "    private RequestHeaderProto constructRpcRequestHeader(Method method) {\n      RequestHeaderProto.Builder builder = RequestHeaderProto\n          .newBuilder();\n      builder.setMethodName(method.getName());\n     \n\n      // For protobuf, {@code protocol} used when creating client side proxy is\n      // the interface extending BlockingInterface, which has the annotations \n      // such as ProtocolName etc.\n      //\n      // Using Method.getDeclaringClass(), as in WritableEngine to get at\n      // the protocol interface will return BlockingInterface, from where \n      // the annotation ProtocolName and Version cannot be\n      // obtained.\n      //\n      // Hence we simply use the protocol class used to create the proxy.\n      // For PB this may limit the use of mixins on client side.\n      builder.setDeclaringClassProtocolName(protocolName);\n      builder.setClientProtocolVersion(clientProtocolVersion);\n      return builder.build();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcRequestWrapper request = (RpcRequestWrapper) writableRequest;\n        RequestHeaderProto rpcRequest = request.requestHeader;\n        String methodName = rpcRequest.getMethodName();\n        \n        \n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = prototype.newBuilderForType()\n            .mergeFrom(request.theRequestRead).build();\n        \n        Message result;\n        try {\n          long startTime = Time.now();\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n          int processingTime = (int) (Time.now() - startTime);\n          int qTime = (int) (startTime - receiveTime);\n          if (LOG.isDebugEnabled()) {\n            LOG.info(\"Served: \" + methodName + \" queueTime= \" + qTime +\n                      \" procesingTime= \" + processingTime);\n          }\n          server.rpcMetrics.addRpcQueueTime(qTime);\n          server.rpcMetrics.addRpcProcessingTime(processingTime);\n          server.rpcDetailedMetrics.addProcessingTime(methodName,\n              processingTime);\n        } catch (ServiceException e) {\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          throw e;\n        }\n        return new RpcResponseWrapper(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getReturnProtoType": "    private Message getReturnProtoType(Method method) throws Exception {\n      if (returnTypes.containsKey(method.getName())) {\n        return returnTypes.get(method.getName());\n      }\n      \n      Class<?> returnType = method.getReturnType();\n      Method newInstMethod = returnType.getMethod(\"getDefaultInstance\");\n      newInstMethod.setAccessible(true);\n      Message prototype = (Message) newInstMethod.invoke(null, (Object[]) null);\n      returnTypes.put(method.getName(), prototype);\n      return prototype;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod": "  protected Object invokeMethod(Method method, Object[] args) throws Throwable {\n    try {\n      if (!method.isAccessible()) {\n        method.setAccessible(true);\n      }\n      return method.invoke(currentProxy, args);\n    } catch (InvocationTargetException e) {\n      throw e.getCause();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invoke": "  public Object invoke(Object proxy, Method method, Object[] args)\n    throws Throwable {\n    RetryPolicy policy = methodNameToPolicyMap.get(method.getName());\n    if (policy == null) {\n      policy = defaultPolicy;\n    }\n    \n    // The number of times this method invocation has been failed over.\n    int invocationFailoverCount = 0;\n    final boolean isRpc = isRpcInvocation(currentProxy);\n    final int callId = isRpc? Client.nextCallId(): RpcConstants.INVALID_CALL_ID;\n    int retries = 0;\n    while (true) {\n      // The number of times this invocation handler has ever been failed over,\n      // before this method invocation attempt. Used to prevent concurrent\n      // failed method invocations from triggering multiple failover attempts.\n      long invocationAttemptFailoverCount;\n      synchronized (proxyProvider) {\n        invocationAttemptFailoverCount = proxyProviderFailoverCount;\n      }\n\n      if (isRpc) {\n        Client.setCallIdAndRetryCount(callId, retries);\n      }\n      try {\n        Object ret = invokeMethod(method, args);\n        hasMadeASuccessfulCall = true;\n        return ret;\n      } catch (Exception e) {\n        boolean isIdempotentOrAtMostOnce = proxyProvider.getInterface()\n            .getMethod(method.getName(), method.getParameterTypes())\n            .isAnnotationPresent(Idempotent.class);\n        if (!isIdempotentOrAtMostOnce) {\n          isIdempotentOrAtMostOnce = proxyProvider.getInterface()\n              .getMethod(method.getName(), method.getParameterTypes())\n              .isAnnotationPresent(AtMostOnce.class);\n        }\n        RetryAction action = policy.shouldRetry(e, retries++,\n            invocationFailoverCount, isIdempotentOrAtMostOnce);\n        if (action.action == RetryAction.RetryDecision.FAIL) {\n          if (action.reason != null) {\n            LOG.warn(\"Exception while invoking \" + \n                currentProxy.getClass() + \".\" + method.getName() +\n                \". Not retrying because \" + action.reason, e);\n          }\n          throw e;\n        } else { // retry or failover\n          // avoid logging the failover if this is the first call on this\n          // proxy object, and we successfully achieve the failover without\n          // any flip-flopping\n          boolean worthLogging = \n            !(invocationFailoverCount == 0 && !hasMadeASuccessfulCall);\n          worthLogging |= LOG.isDebugEnabled();\n          if (action.action == RetryAction.RetryDecision.FAILOVER_AND_RETRY &&\n              worthLogging) {\n            String msg = \"Exception while invoking \" + method.getName()\n              + \" of class \" + currentProxy.getClass().getSimpleName();\n            if (invocationFailoverCount > 0) {\n              msg += \" after \" + invocationFailoverCount + \" fail over attempts\"; \n            }\n            msg += \". Trying to fail over \" + formatSleepMessage(action.delayMillis);\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(msg, e);\n            }\n          } else {\n            if(LOG.isDebugEnabled()) {\n              LOG.debug(\"Exception while invoking \" + method.getName()\n                  + \" of class \" + currentProxy.getClass().getSimpleName() +\n                  \". Retrying \" + formatSleepMessage(action.delayMillis), e);\n            }\n          }\n          \n          if (action.delayMillis > 0) {\n            ThreadUtil.sleepAtLeastIgnoreInterrupts(action.delayMillis);\n          }\n          \n          if (action.action == RetryAction.RetryDecision.FAILOVER_AND_RETRY) {\n            // Make sure that concurrent failed method invocations only cause a\n            // single actual fail over.\n            synchronized (proxyProvider) {\n              if (invocationAttemptFailoverCount == proxyProviderFailoverCount) {\n                proxyProvider.performFailover(currentProxy);\n                proxyProviderFailoverCount++;\n                currentProxy = proxyProvider.getProxy();\n              } else {\n                LOG.warn(\"A failover has occurred since the start of this method\"\n                    + \" invocation attempt.\");\n              }\n            }\n            invocationFailoverCount++;\n          }\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FSLinkResolver.resolve": "  public T resolve(final FileContext fc, final Path path) throws IOException {\n    int count = 0;\n    T in = null;\n    Path p = path;\n    // NB: More than one AbstractFileSystem can match a scheme, eg \n    // \"file\" resolves to LocalFs but could have come by RawLocalFs.\n    AbstractFileSystem fs = fc.getFSofPath(p);\n\n    // Loop until all symlinks are resolved or the limit is reached\n    for (boolean isLink = true; isLink;) {\n      try {\n        in = next(fs, p);\n        isLink = false;\n      } catch (UnresolvedLinkException e) {\n        if (!fc.resolveSymlinks) {\n          throw new IOException(\"Path \" + path + \" contains a symlink\"\n              + \" and symlink resolution is disabled (\"\n              + CommonConfigurationKeys.FS_CLIENT_RESOLVE_REMOTE_SYMLINKS_KEY + \").\", e);\n        }\n        if (count++ > FsConstants.MAX_PATH_LINKS) {\n          throw new IOException(\"Possible cyclic loop while \" +\n                                \"following symbolic link \" + path);\n        }\n        // Resolve the first unresolved path component\n        p = qualifySymlinkTarget(fs.getUri(), p, fs.getLinkTarget(p));\n        fs = fc.getFSofPath(p);\n      }\n    }\n    return in;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FSLinkResolver.qualifySymlinkTarget": "  public static Path qualifySymlinkTarget(final URI pathURI,\n      Path pathWithLink, Path target) {\n    // NB: makeQualified uses the target's scheme and authority, if\n    // specified, and the scheme and authority of pathURI, if not.\n    final URI targetUri = target.toUri();\n    final String scheme = targetUri.getScheme();\n    final String auth = targetUri.getAuthority();\n    return (scheme == null && auth == null) ? target.makeQualified(pathURI,\n        pathWithLink.getParent()) : target;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FSLinkResolver.next": "  abstract public T next(final AbstractFileSystem fs, final Path p)\n      throws IOException, UnresolvedLinkException;\n\n  /**\n   * Performs the operation specified by the next function, calling it\n   * repeatedly until all symlinks in the given path are resolved.\n   * @param fc FileContext used to access file systems.\n   * @param path The path to resolve symlinks on.\n   * @return Generic type determined by the implementation of next.\n   * @throws IOException\n   */\n  public T resolve(final FileContext fc, final Path path) throws IOException {\n    int count = 0;\n    T in = null;\n    Path p = path;\n    // NB: More than one AbstractFileSystem can match a scheme, eg \n    // \"file\" resolves to LocalFs but could have come by RawLocalFs.\n    AbstractFileSystem fs = fc.getFSofPath(p);\n\n    // Loop until all symlinks are resolved or the limit is reached\n    for (boolean isLink = true; isLink;) {\n      try {\n        in = next(fs, p);\n        isLink = false;\n      } catch (UnresolvedLinkException e) {\n        if (!fc.resolveSymlinks) {\n          throw new IOException(\"Path \" + path + \" contains a symlink\"\n              + \" and symlink resolution is disabled (\"\n              + CommonConfigurationKeys.FS_CLIENT_RESOLVE_REMOTE_SYMLINKS_KEY + \").\", e);\n        }\n        if (count++ > FsConstants.MAX_PATH_LINKS) {\n          throw new IOException(\"Possible cyclic loop while \" +\n                                \"following symbolic link \" + path);\n        }\n        // Resolve the first unresolved path component\n        p = qualifySymlinkTarget(fs.getUri(), p, fs.getLinkTarget(p));\n        fs = fc.getFSofPath(p);\n      }\n    }\n    return in;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.connect": "  static void connect(SocketChannel channel, \n                      SocketAddress endpoint, int timeout) throws IOException {\n    \n    boolean blockingOn = channel.isBlocking();\n    if (blockingOn) {\n      channel.configureBlocking(false);\n    }\n    \n    try { \n      if (channel.connect(endpoint)) {\n        return;\n      }\n\n      long timeoutLeft = timeout;\n      long endTime = (timeout > 0) ? (Time.now() + timeout): 0;\n      \n      while (true) {\n        // we might have to call finishConnect() more than once\n        // for some channels (with user level protocols)\n        \n        int ret = selector.select((SelectableChannel)channel, \n                                  SelectionKey.OP_CONNECT, timeoutLeft);\n        \n        if (ret > 0 && channel.finishConnect()) {\n          return;\n        }\n        \n        if (ret == 0 ||\n            (timeout > 0 &&  \n              (timeoutLeft = (endTime - Time.now())) <= 0)) {\n          throw new SocketTimeoutException(\n                    timeoutExceptionString(channel, timeout, \n                                           SelectionKey.OP_CONNECT));\n        }\n      }\n    } catch (IOException e) {\n      // javadoc for SocketChannel.connect() says channel should be closed.\n      try {\n        channel.close();\n      } catch (IOException ignored) {}\n      throw e;\n    } finally {\n      if (blockingOn && channel.isOpen()) {\n        channel.configureBlocking(true);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.close": "      void close() {\n        if (selector != null) {\n          try {\n            selector.close();\n          } catch (IOException e) {\n            LOG.warn(\"Unexpected exception while closing selector : \", e);\n          }\n        }\n      }    ",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.select": "    int select(SelectableChannel channel, int ops, long timeout) \n                                                   throws IOException {\n     \n      SelectorInfo info = get(channel);\n      \n      SelectionKey key = null;\n      int ret = 0;\n      \n      try {\n        while (true) {\n          long start = (timeout == 0) ? 0 : Time.now();\n\n          key = channel.register(info.selector, ops);\n          ret = info.selector.select(timeout);\n          \n          if (ret != 0) {\n            return ret;\n          }\n          \n          /* Sometimes select() returns 0 much before timeout for \n           * unknown reasons. So select again if required.\n           */\n          if (timeout > 0) {\n            timeout -= Time.now() - start;\n            if (timeout <= 0) {\n              return 0;\n            }\n          }\n          \n          if (Thread.currentThread().isInterrupted()) {\n            throw new InterruptedIOException(\"Interruped while waiting for \" +\n                                             \"IO on channel \" + channel +\n                                             \". \" + timeout + \n                                             \" millis timeout left.\");\n          }\n        }\n      } finally {\n        if (key != null) {\n          key.cancel();\n        }\n        \n        //clear the canceled key.\n        try {\n          info.selector.selectNow();\n        } catch (IOException e) {\n          LOG.info(\"Unexpected Exception while clearing selector : \", e);\n          // don't put the selector back.\n          info.close();\n          return ret; \n        }\n        \n        release(info);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.timeoutExceptionString": "  private static String timeoutExceptionString(SelectableChannel channel,\n                                               long timeout, int ops) {\n    \n    String waitingFor;\n    switch(ops) {\n    \n    case SelectionKey.OP_READ :\n      waitingFor = \"read\"; break;\n      \n    case SelectionKey.OP_WRITE :\n      waitingFor = \"write\"; break;      \n      \n    case SelectionKey.OP_CONNECT :\n      waitingFor = \"connect\"; break;\n      \n    default :\n      waitingFor = \"\" + ops;  \n    }\n    \n    return timeout + \" millis timeout while \" +\n           \"waiting for channel to be ready for \" + \n           waitingFor + \". ch : \" + channel;    \n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.isOpen": "  boolean isOpen() {\n    return !closed && channel.isOpen();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.connect": "  public static void connect(Socket socket, \n                             SocketAddress endpoint,\n                             SocketAddress localAddr,\n                             int timeout) throws IOException {\n    if (socket == null || endpoint == null || timeout < 0) {\n      throw new IllegalArgumentException(\"Illegal argument for connect()\");\n    }\n    \n    SocketChannel ch = socket.getChannel();\n    \n    if (localAddr != null) {\n      Class localClass = localAddr.getClass();\n      Class remoteClass = endpoint.getClass();\n      Preconditions.checkArgument(localClass.equals(remoteClass),\n          \"Local address %s must be of same family as remote address %s.\",\n          localAddr, endpoint);\n      socket.bind(localAddr);\n    }\n\n    try {\n      if (ch == null) {\n        // let the default implementation handle it.\n        socket.connect(endpoint, timeout);\n      } else {\n        SocketIOWithTimeout.connect(ch, endpoint, timeout);\n      }\n    } catch (SocketTimeoutException ste) {\n      throw new ConnectTimeoutException(ste.getMessage());\n    }\n\n    // There is a very rare case allowed by the TCP specification, such that\n    // if we are trying to connect to an endpoint on the local machine,\n    // and we end up choosing an ephemeral port equal to the destination port,\n    // we will actually end up getting connected to ourself (ie any data we\n    // send just comes right back). This is only possible if the target\n    // daemon is down, so we'll treat it like connection refused.\n    if (socket.getLocalPort() == socket.getPort() &&\n        socket.getLocalAddress().equals(socket.getInetAddress())) {\n      LOG.info(\"Detected a loopback TCP socket, disconnecting it\");\n      socket.close();\n      throw new ConnectException(\n        \"Localhost targeted connection resulted in a loopback. \" +\n        \"No daemon is listening on the target port.\");\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupConnection": "    private synchronized void setupConnection() throws IOException {\n      short ioFailures = 0;\n      short timeoutFailures = 0;\n      while (true) {\n        try {\n          this.socket = socketFactory.createSocket();\n          this.socket.setTcpNoDelay(tcpNoDelay);\n          this.socket.setKeepAlive(true);\n          \n          /*\n           * Bind the socket to the host specified in the principal name of the\n           * client, to ensure Server matching address of the client connection\n           * to host name in principal passed.\n           */\n          UserGroupInformation ticket = remoteId.getTicket();\n          if (ticket != null && ticket.hasKerberosCredentials()) {\n            KerberosInfo krbInfo = \n              remoteId.getProtocol().getAnnotation(KerberosInfo.class);\n            if (krbInfo != null && krbInfo.clientPrincipal() != null) {\n              String host = \n                SecurityUtil.getHostFromPrincipal(remoteId.getTicket().getUserName());\n              \n              // If host name is a valid local address then bind socket to it\n              InetAddress localAddr = NetUtils.getLocalInetAddress(host);\n              if (localAddr != null) {\n                this.socket.bind(new InetSocketAddress(localAddr, 0));\n              }\n            }\n          }\n          \n          NetUtils.connect(this.socket, server, connectionTimeout);\n          if (rpcTimeout > 0) {\n            pingInterval = rpcTimeout;  // rpcTimeout overwrites pingInterval\n          }\n          this.socket.setSoTimeout(pingInterval);\n          return;\n        } catch (ConnectTimeoutException toe) {\n          /* Check for an address change and update the local reference.\n           * Reset the failure counter if the address was changed\n           */\n          if (updateAddress()) {\n            timeoutFailures = ioFailures = 0;\n          }\n          handleConnectionTimeout(timeoutFailures++,\n              maxRetriesOnSocketTimeouts, toe);\n        } catch (IOException ie) {\n          if (updateAddress()) {\n            timeoutFailures = ioFailures = 0;\n          }\n          handleConnectionFailure(ioFailures++, ie);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.updateAddress": "    private synchronized boolean updateAddress() throws IOException {\n      // Do a fresh lookup with the old host name.\n      InetSocketAddress currentAddr = NetUtils.createSocketAddrForHost(\n                               server.getHostName(), server.getPort());\n\n      if (!server.equals(currentAddr)) {\n        LOG.warn(\"Address change detected. Old: \" + server.toString() +\n                                 \" New: \" + currentAddr.toString());\n        server = currentAddr;\n        return true;\n      }\n      return false;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.handleConnectionFailure": "    private void handleConnectionFailure(int curRetries, IOException ioe\n        ) throws IOException {\n      closeConnection();\n\n      final RetryAction action;\n      try {\n        action = connectionRetryPolicy.shouldRetry(ioe, curRetries, 0, true);\n      } catch(Exception e) {\n        throw e instanceof IOException? (IOException)e: new IOException(e);\n      }\n      if (action.action == RetryAction.RetryDecision.FAIL) {\n        if (action.reason != null) {\n          LOG.warn(\"Failed to connect to server: \" + server + \": \"\n              + action.reason, ioe);\n        }\n        throw ioe;\n      }\n\n      try {\n        Thread.sleep(action.delayMillis);\n      } catch (InterruptedException e) {\n        throw (IOException)new InterruptedIOException(\"Interrupted: action=\"\n            + action + \", retry policy=\" + connectionRetryPolicy).initCause(e);\n      }\n      LOG.info(\"Retrying connect to server: \" + server + \". Already tried \"\n          + curRetries + \" time(s); retry policy is \" + connectionRetryPolicy);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.handleConnectionTimeout": "    private void handleConnectionTimeout(\n        int curRetries, int maxRetries, IOException ioe) throws IOException {\n\n      closeConnection();\n\n      // throw the exception if the maximum number of retries is reached\n      if (curRetries >= maxRetries) {\n        throw ioe;\n      }\n      LOG.info(\"Retrying connect to server: \" + server + \". Already tried \"\n          + curRetries + \" time(s); maxRetries=\" + maxRetries);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getTicket": "    UserGroupInformation getTicket() {\n      return ticket;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getProtocol": "    Class<?> getProtocol() {\n      return protocol;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupIOstreams": "    private synchronized void setupIOstreams() {\n      if (socket != null || shouldCloseConnection.get()) {\n        return;\n      } \n      try {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Connecting to \"+server);\n        }\n        short numRetries = 0;\n        final short MAX_RETRIES = 5;\n        Random rand = null;\n        while (true) {\n          setupConnection();\n          InputStream inStream = NetUtils.getInputStream(socket);\n          OutputStream outStream = NetUtils.getOutputStream(socket);\n          writeConnectionHeader(outStream);\n          if (authProtocol == AuthProtocol.SASL) {\n            final InputStream in2 = inStream;\n            final OutputStream out2 = outStream;\n            UserGroupInformation ticket = remoteId.getTicket();\n            if (ticket.getRealUser() != null) {\n              ticket = ticket.getRealUser();\n            }\n            try {\n              authMethod = ticket\n                  .doAs(new PrivilegedExceptionAction<AuthMethod>() {\n                    @Override\n                    public AuthMethod run()\n                        throws IOException, InterruptedException {\n                      return setupSaslConnection(in2, out2);\n                    }\n                  });\n            } catch (Exception ex) {\n              authMethod = saslRpcClient.getAuthMethod();\n              if (rand == null) {\n                rand = new Random();\n              }\n              handleSaslConnectionFailure(numRetries++, MAX_RETRIES, ex, rand,\n                  ticket);\n              continue;\n            }\n            if (authMethod != AuthMethod.SIMPLE) {\n              // Sasl connect is successful. Let's set up Sasl i/o streams.\n              inStream = saslRpcClient.getInputStream(inStream);\n              outStream = saslRpcClient.getOutputStream(outStream);\n              // for testing\n              remoteId.saslQop =\n                  (String)saslRpcClient.getNegotiatedProperty(Sasl.QOP);\n            } else if (UserGroupInformation.isSecurityEnabled() &&\n                       !fallbackAllowed) {\n              throw new IOException(\"Server asks us to fall back to SIMPLE \" +\n                  \"auth, but this client is configured to only allow secure \" +\n                  \"connections.\");\n            }\n          }\n        \n          if (doPing) {\n            inStream = new PingInputStream(inStream);\n          }\n          this.in = new DataInputStream(new BufferedInputStream(inStream));\n\n          // SASL may have already buffered the stream\n          if (!(outStream instanceof BufferedOutputStream)) {\n            outStream = new BufferedOutputStream(outStream);\n          }\n          this.out = new DataOutputStream(outStream);\n          \n          writeConnectionContext(remoteId, authMethod);\n\n          // update last activity time\n          touch();\n\n          // start the receiver thread after the socket connection has been set\n          // up\n          start();\n          return;\n        }\n      } catch (Throwable t) {\n        if (t instanceof IOException) {\n          markClosed((IOException)t);\n        } else {\n          markClosed(new IOException(\"Couldn't set up IO streams\", t));\n        }\n        close();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.close": "    private synchronized void close() {\n      if (!shouldCloseConnection.get()) {\n        LOG.error(\"The connection is not in the closed state\");\n        return;\n      }\n\n      // release the resources\n      // first thing to do;take the connection out of the connection list\n      synchronized (connections) {\n        if (connections.get(remoteId) == this) {\n          connections.remove(remoteId);\n        }\n      }\n\n      // close the streams and therefore the socket\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(in);\n      disposeSasl();\n\n      // clean up all calls\n      if (closeException == null) {\n        if (!calls.isEmpty()) {\n          LOG.warn(\n              \"A connection is closed for no cause and calls are not empty\");\n\n          // clean up calls anyway\n          closeException = new IOException(\"Unexpected closed connection\");\n          cleanupCalls();\n        }\n      } else {\n        // log the info\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"closing ipc connection to \" + server + \": \" +\n              closeException.getMessage(),closeException);\n        }\n\n        // cleanup calls\n        cleanupCalls();\n      }\n      closeConnection();\n      if (LOG.isDebugEnabled())\n        LOG.debug(getName() + \": closed\");\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupSaslConnection": "    private synchronized AuthMethod setupSaslConnection(final InputStream in2, \n        final OutputStream out2) throws IOException, InterruptedException {\n      saslRpcClient = new SaslRpcClient(remoteId.getTicket(),\n          remoteId.getProtocol(), remoteId.getAddress(), conf);\n      return saslRpcClient.saslConnect(in2, out2);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.touch": "    private void touch() {\n      lastActivity.set(Time.now());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.writeConnectionHeader": "    private void writeConnectionHeader(OutputStream outStream)\n        throws IOException {\n      DataOutputStream out = new DataOutputStream(new BufferedOutputStream(outStream));\n      // Write out the header, version and authentication method\n      out.write(RpcConstants.HEADER.array());\n      out.write(RpcConstants.CURRENT_VERSION);\n      out.write(serviceClass);\n      out.write(authProtocol.callId);\n      out.flush();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.writeConnectionContext": "    private void writeConnectionContext(ConnectionId remoteId,\n                                        AuthMethod authMethod)\n                                            throws IOException {\n      // Write out the ConnectionHeader\n      IpcConnectionContextProto message = ProtoUtil.makeIpcConnectionContext(\n          RPC.getProtocolName(remoteId.getProtocol()),\n          remoteId.getTicket(),\n          authMethod);\n      RpcRequestHeaderProto connectionContextHeader = ProtoUtil\n          .makeRpcRequestHeader(RpcKind.RPC_PROTOCOL_BUFFER,\n              OperationProto.RPC_FINAL_PACKET, CONNECTION_CONTEXT_CALL_ID,\n              RpcConstants.INVALID_RETRY_COUNT, clientId);\n      RpcRequestMessageWrapper request =\n          new RpcRequestMessageWrapper(connectionContextHeader, message);\n      \n      // Write out the packet length\n      out.writeInt(request.getLength());\n      request.write(out);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.handleSaslConnectionFailure": "    private synchronized void handleSaslConnectionFailure(\n        final int currRetries, final int maxRetries, final Exception ex,\n        final Random rand, final UserGroupInformation ugi) throws IOException,\n        InterruptedException {\n      ugi.doAs(new PrivilegedExceptionAction<Object>() {\n        @Override\n        public Object run() throws IOException, InterruptedException {\n          final short MAX_BACKOFF = 5000;\n          closeConnection();\n          disposeSasl();\n          if (shouldAuthenticateOverKrb()) {\n            if (currRetries < maxRetries) {\n              if(LOG.isDebugEnabled()) {\n                LOG.debug(\"Exception encountered while connecting to \"\n                    + \"the server : \" + ex);\n              }\n              // try re-login\n              if (UserGroupInformation.isLoginKeytabBased()) {\n                UserGroupInformation.getLoginUser().reloginFromKeytab();\n              } else {\n                UserGroupInformation.getLoginUser().reloginFromTicketCache();\n              }\n              // have granularity of milliseconds\n              //we are sleeping with the Connection lock held but since this\n              //connection instance is being used for connecting to the server\n              //in question, it is okay\n              Thread.sleep((rand.nextInt(MAX_BACKOFF) + 1));\n              return null;\n            } else {\n              String msg = \"Couldn't setup connection for \"\n                  + UserGroupInformation.getLoginUser().getUserName() + \" to \"\n                  + remoteId;\n              LOG.warn(msg);\n              throw (IOException) new IOException(msg).initCause(ex);\n            }\n          } else {\n            LOG.warn(\"Exception encountered while connecting to \"\n                + \"the server : \" + ex);\n          }\n          if (ex instanceof RemoteException)\n            throw (RemoteException) ex;\n          throw new IOException(ex);\n        }\n      });\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.markClosed": "    private synchronized void markClosed(IOException e) {\n      if (shouldCloseConnection.compareAndSet(false, true)) {\n        closeException = e;\n        notifyAll();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceOperations.stopQuietly": "  public static Exception stopQuietly(Log log, Service service) {\n    try {\n      stop(service);\n    } catch (Exception e) {\n      log.warn(\"When stopping the service \" + service.getName()\n               + \" : \" + e,\n               e);\n      return e;\n    }\n    return null;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceOperations.stop": "  public static void stop(Service service) {\n    if (service != null) {\n      service.stop();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceStateException.convert": "  public static RuntimeException convert(String text, Throwable fault) {\n    if (fault instanceof RuntimeException) {\n      return (RuntimeException) fault;\n    } else {\n      return new ServiceStateException(text, fault);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.Service.init": "  void init(Configuration config);\n\n\n  /**\n   * Start the service.\n   *\n   * The transition MUST be from {@link STATE#INITED} to {@link STATE#STARTED}",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.AbstractFileSystem.getLinkTarget": "  public Path getLinkTarget(final Path f) throws IOException {\n    /* We should never get here. Any file system that threw an\n     * UnresolvedLinkException, causing this function to be called,\n     * needs to override this method.\n     */\n    throw new AssertionError();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.AbstractFileSystem.getUri": "  public URI getUri() {\n    return myUri;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.AbstractFileSystem.checkScheme": "  public void checkScheme(URI uri, String supportedScheme) {\n    String scheme = uri.getScheme();\n    if (scheme == null) {\n      throw new HadoopIllegalArgumentException(\"Uri without scheme: \" + uri);\n    }\n    if (!scheme.equals(supportedScheme)) {\n      throw new HadoopIllegalArgumentException(\"Uri scheme \" + uri\n          + \" does not match the scheme \" + supportedScheme);\n    }\n  }"
        },
        "bug_report": {
            "Title": "JobHistoryServer does not start if HDFS is not running",
            "Description": "Starting JHS without HDFS running fails with the following error:\n\n{code}\nSTARTUP_MSG:   build = git://git.apache.org/hadoop-common.git -r ad74e8850b99e03b0b6435b04f5b3e9995bc3956; compiled by 'tucu' on 2014-01-14T22:40Z\nSTARTUP_MSG:   java = 1.7.0_45\n************************************************************/\n2014-01-14 16:47:40,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer: registered UNIX signal handlers for [TERM, HUP, INT]\n2014-01-14 16:47:40,883 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2014-01-14 16:47:41,101 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: JobHistory Init\n2014-01-14 16:47:41,710 INFO org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager failed in state INITED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://localhost:8020/tmp/hadoop-yarn/staging/history/done]\norg.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://localhost:8020/tmp/hadoop-yarn/staging/history/done]\n\tat org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:505)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistory.serviceInit(JobHistory.java:94)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n\tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:143)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.launchJobHistoryServer(JobHistoryServer.java:207)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:217)\nCaused by: java.net.ConnectException: Call From dontknow.local/172.20.10.4 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1410)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1359)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n\tat com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:185)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:101)\n\tat com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:671)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1722)\n\tat org.apache.hadoop.fs.Hdfs.getFileStatus(Hdfs.java:124)\n\tat org.apache.hadoop.fs.FileContext$14.next(FileContext.java:1106)\n\tat org.apache.hadoop.fs.FileContext$14.next(FileContext.java:1102)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.getFileStatus(FileContext.java:1102)\n\tat org.apache.hadoop.fs.FileContext$Util.exists(FileContext.java:1514)\n\tat org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.mkdir(HistoryFileManager.java:561)\n\tat org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:502)\n\t... 8 more\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:601)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:696)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2700(Client.java:367)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1458)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1377)\n\t... 28 more\n2014-01-14 16:47:41,713 INFO org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.mapreduce.v2.hs.JobHistory failed in state INITED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://localhost:8020/tmp/hadoop-yarn/staging/history/done]\norg.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://localhost:8020/tmp/hadoop-yarn/staging/history/done]\n\tat org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:505)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistory.serviceInit(JobHistory.java:94)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n\tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:143)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.launchJobHistoryServer(JobHistoryServer.java:207)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:217)\nCaused by: java.net.ConnectException: Call From dontknow.local/172.20.10.4 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1410)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1359)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n\tat com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:185)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:101)\n\tat com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:671)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1722)\n\tat org.apache.hadoop.fs.Hdfs.getFileStatus(Hdfs.java:124)\n\tat org.apache.hadoop.fs.FileContext$14.next(FileContext.java:1106)\n\tat org.apache.hadoop.fs.FileContext$14.next(FileContext.java:1102)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.getFileStatus(FileContext.java:1102)\n\tat org.apache.hadoop.fs.FileContext$Util.exists(FileContext.java:1514)\n\tat org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.mkdir(HistoryFileManager.java:561)\n\tat org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:502)\n\t... 8 more\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:601)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:696)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2700(Client.java:367)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1458)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1377)\n\t... 28 more\n2014-01-14 16:47:41,714 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Stopping JobHistory\n2014-01-14 16:47:41,714 INFO org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer failed in state INITED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://localhost:8020/tmp/hadoop-yarn/staging/history/done]\n{code}\n"
        }
    },
    {
        "filename": "MAPREDUCE-5358.json",
        "creation_time": "2013-06-28T05:40:45.000+0000",
        "stack_trace": "```\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at SUCCEEDED\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)\n\tat java.lang.Thread.run(Thread.java:662)\n\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_MAP_TASK_RESCHEDULED at SUCCEEDED\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)\n\tat java.lang.Thread.run(Thread.java:662)\n```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle": "  public void handle(JobEvent event) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Processing \" + event.getJobId() + \" of type \"\n          + event.getType());\n    }\n    try {\n      writeLock.lock();\n      JobStateInternal oldState = getInternalState();\n      try {\n         getStateMachine().doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        addDiagnostic(\"Invalid event \" + event.getType() + \n            \" on Job \" + this.jobId);\n        eventHandler.handle(new JobEvent(this.jobId,\n            JobEventType.INTERNAL_ERROR));\n      }\n      //notify the eventhandler of state change\n      if (oldState != getInternalState()) {\n        LOG.info(jobId + \"Job Transitioned from \" + oldState + \" to \"\n                 + getInternalState());\n      }\n    }\n    \n    finally {\n      writeLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getStateMachine": "  protected StateMachine<JobStateInternal, JobEventType, JobEvent> getStateMachine() {\n    return stateMachine;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getInternalState": "  public JobStateInternal getInternalState() {\n    readLock.lock();\n    try {\n      if(forcedState != null) {\n        return forcedState;\n      }\n     return getStateMachine().getCurrentState();\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.addDiagnostic": "  protected void addDiagnostic(String diag) {\n    diagnostics.add(diag);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.handle": "    public void handle(Event event) {\n      //Empty\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJob": "    public Job getJob(JobId jobID) {\n      return jobs.get(jobID);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob": "  public void shutDownJob() {\n    // job has finished\n    // this is the only job, so shut down the Appmaster\n    // note in a workflow scenario, this may lead to creation of a new\n    // job (FIXME?)\n    // Send job-end notification\n    if (getConfig().get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL) != null) {\n      try {\n        LOG.info(\"Job end notification started for jobID : \"\n            + job.getReport().getJobId());\n        JobEndNotifier notifier = new JobEndNotifier();\n        notifier.setConf(getConfig());\n        notifier.notify(job.getReport());\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Job end notification interrupted for jobID : \"\n            + job.getReport().getJobId(), ie);\n      }\n    }\n\n    // TODO:currently just wait for some time so clients can know the\n    // final states. Will be removed once RM come on.\n    try {\n      Thread.sleep(5000);\n    } catch (InterruptedException e) {\n      e.printStackTrace();\n    }\n\n    try {\n      //if isLastAMRetry comes as true, should never set it to false\n      if ( !isLastAMRetry){\n        if (((JobImpl)job).getInternalState() != JobStateInternal.REBOOT) {\n          LOG.info(\"We are finishing cleanly so this is the last retry\");\n          isLastAMRetry = true;\n        }\n      }\n      notifyIsLastAMRetry(isLastAMRetry);\n      // Stop all services\n      // This will also send the final report to the ResourceManager\n      LOG.info(\"Calling stop for all the services\");\n      MRAppMaster.this.stop();\n\n    } catch (Throwable t) {\n      LOG.warn(\"Graceful stop failed \", t);\n    }\n\n    //Bring the process down by force.\n    //Not needed after HADOOP-7140\n    LOG.info(\"Exiting MR AppMaster..GoodBye!\");\n    sysexit();   \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJobId": "  public JobId getJobId() {\n    return jobId;\n  }"
        },
        "bug_report": {
            "Title": "MRAppMaster throws invalid transitions for JobImpl",
            "Description": "{code:xml}\n2013-06-26 11:39:50,128 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at SUCCEEDED\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)\n\tat java.lang.Thread.run(Thread.java:662)\n{code}\n\n{code:xml}\n2013-06-26 11:39:50,129 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_MAP_TASK_RESCHEDULED at SUCCEEDED\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)\n\tat java.lang.Thread.run(Thread.java:662)\n{code}"
        }
    },
    {
        "filename": "MAPREDUCE-5837.json",
        "creation_time": "2014-04-15T20:24:58.000+0000",
        "stack_trace": "```\njava.lang.NoClassDefFoundError: scala/Function1\n        at java.lang.Class.forName0(Native Method)\n        at java.lang.Class.forName(Class.java:190)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.isChainJob(JobImpl.java:1282)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.makeUberDecision(JobImpl.java:1224)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.access$3700(JobImpl.java:136)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1425)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1363)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:976)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:135)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1263)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1063)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$1.run(MRAppMaster.java:1480)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1606)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1476)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1409)\nCaused by: java.lang.ClassNotFoundException: scala.Function1\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.isChainJob": "  private boolean isChainJob(Configuration conf) {\n    boolean isChainJob = false;\n    try {\n      String mapClassName = conf.get(MRJobConfig.MAP_CLASS_ATTR);\n      if (mapClassName != null) {\n        Class<?> mapClass = Class.forName(mapClassName);\n        if (ChainMapper.class.isAssignableFrom(mapClass))\n          isChainJob = true;\n      }\n    } catch (ClassNotFoundException cnfe) {\n      // don't care; assume it's not derived from ChainMapper\n    }\n    try {\n      String reduceClassName = conf.get(MRJobConfig.REDUCE_CLASS_ATTR);\n      if (reduceClassName != null) {\n        Class<?> reduceClass = Class.forName(reduceClassName);\n        if (ChainReducer.class.isAssignableFrom(reduceClass))\n          isChainJob = true;\n      }\n    } catch (ClassNotFoundException cnfe) {\n      // don't care; assume it's not derived from ChainReducer\n    }\n    return isChainJob;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.makeUberDecision": "  private void makeUberDecision(long dataInputLength) {\n    //FIXME:  need new memory criterion for uber-decision (oops, too late here;\n    // until AM-resizing supported,\n    // must depend on job client to pass fat-slot needs)\n    // these are no longer \"system\" settings, necessarily; user may override\n    int sysMaxMaps = conf.getInt(MRJobConfig.JOB_UBERTASK_MAXMAPS, 9);\n\n    int sysMaxReduces = conf.getInt(MRJobConfig.JOB_UBERTASK_MAXREDUCES, 1);\n\n    long sysMaxBytes = conf.getLong(MRJobConfig.JOB_UBERTASK_MAXBYTES,\n        fs.getDefaultBlockSize(this.remoteJobSubmitDir)); // FIXME: this is wrong; get FS from\n                                   // [File?]InputFormat and default block size\n                                   // from that\n\n    long sysMemSizeForUberSlot =\n        conf.getInt(MRJobConfig.MR_AM_VMEM_MB,\n            MRJobConfig.DEFAULT_MR_AM_VMEM_MB);\n\n    long sysCPUSizeForUberSlot =\n        conf.getInt(MRJobConfig.MR_AM_CPU_VCORES,\n            MRJobConfig.DEFAULT_MR_AM_CPU_VCORES);\n\n    boolean uberEnabled =\n        conf.getBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\n    boolean smallNumMapTasks = (numMapTasks <= sysMaxMaps);\n    boolean smallNumReduceTasks = (numReduceTasks <= sysMaxReduces);\n    boolean smallInput = (dataInputLength <= sysMaxBytes);\n    // ignoring overhead due to UberAM and statics as negligible here:\n    boolean smallMemory =\n        ( (Math.max(conf.getLong(MRJobConfig.MAP_MEMORY_MB, 0),\n            conf.getLong(MRJobConfig.REDUCE_MEMORY_MB, 0))\n            <= sysMemSizeForUberSlot)\n            || (sysMemSizeForUberSlot == JobConf.DISABLED_MEMORY_LIMIT));\n    boolean smallCpu =\n        (\n            Math.max(\n                conf.getInt(\n                    MRJobConfig.MAP_CPU_VCORES, \n                    MRJobConfig.DEFAULT_MAP_CPU_VCORES), \n                conf.getInt(\n                    MRJobConfig.REDUCE_CPU_VCORES, \n                    MRJobConfig.DEFAULT_REDUCE_CPU_VCORES)) \n             <= sysCPUSizeForUberSlot\n        );\n    boolean notChainJob = !isChainJob(conf);\n\n    // User has overall veto power over uberization, or user can modify\n    // limits (overriding system settings and potentially shooting\n    // themselves in the head).  Note that ChainMapper/Reducer are\n    // fundamentally incompatible with MR-1220; they employ a blocking\n    // queue between the maps/reduces and thus require parallel execution,\n    // while \"uber-AM\" (MR AM + LocalContainerLauncher) loops over tasks\n    // and thus requires sequential execution.\n    isUber = uberEnabled && smallNumMapTasks && smallNumReduceTasks\n        && smallInput && smallMemory && smallCpu \n        && notChainJob;\n\n    if (isUber) {\n      LOG.info(\"Uberizing job \" + jobId + \": \" + numMapTasks + \"m+\"\n          + numReduceTasks + \"r tasks (\" + dataInputLength\n          + \" input bytes) will run sequentially on single node.\");\n\n      // make sure reduces are scheduled only after all map are completed\n      conf.setFloat(MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART,\n                        1.0f);\n      // uber-subtask attempts all get launched on same node; if one fails,\n      // probably should retry elsewhere, i.e., move entire uber-AM:  ergo,\n      // limit attempts to 1 (or at most 2?  probably not...)\n      conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS, 1);\n      conf.setInt(MRJobConfig.REDUCE_MAX_ATTEMPTS, 1);\n\n      // disable speculation\n      conf.setBoolean(MRJobConfig.MAP_SPECULATIVE, false);\n      conf.setBoolean(MRJobConfig.REDUCE_SPECULATIVE, false);\n    } else {\n      StringBuilder msg = new StringBuilder();\n      msg.append(\"Not uberizing \").append(jobId).append(\" because:\");\n      if (!uberEnabled)\n        msg.append(\" not enabled;\");\n      if (!smallNumMapTasks)\n        msg.append(\" too many maps;\");\n      if (!smallNumReduceTasks)\n        msg.append(\" too many reduces;\");\n      if (!smallInput)\n        msg.append(\" too much input;\");\n      if (!smallMemory)\n        msg.append(\" too much RAM;\");\n      if (!notChainJob)\n        msg.append(\" chainjob;\");\n      LOG.info(msg.toString());\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.transition": "    public void transition(JobImpl job, JobEvent event) {\n      //TODO Is this JH event required.\n      job.setFinishTime();\n      JobUnsuccessfulCompletionEvent failedEvent =\n          new JobUnsuccessfulCompletionEvent(job.oldJobId,\n              job.finishTime, 0, 0,\n              jobHistoryString, job.diagnostics);\n      job.eventHandler.handle(new JobHistoryEvent(job.jobId, failedEvent));\n      job.finished(terminationState);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.taskKilled": "    private void taskKilled(JobImpl job, Task task) {\n      if (task.getType() == TaskType.MAP) {\n        job.killedMapTaskCount++;\n      } else if (task.getType() == TaskType.REDUCE) {\n        job.killedReduceTaskCount++;\n      }\n      job.metrics.killedTask(task);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.actOnUnusableNode": "  private void actOnUnusableNode(NodeId nodeId, NodeState nodeState) {\n    // rerun previously successful map tasks\n    List<TaskAttemptId> taskAttemptIdList = nodesToSucceededTaskAttempts.get(nodeId);\n    if(taskAttemptIdList != null) {\n      String mesg = \"TaskAttempt killed because it ran on unusable node \"\n          + nodeId;\n      for(TaskAttemptId id : taskAttemptIdList) {\n        if(TaskType.MAP == id.getTaskId().getTaskType()) {\n          // reschedule only map tasks because their outputs maybe unusable\n          LOG.info(mesg + \". AttemptId:\" + id);\n          eventHandler.handle(new TaskAttemptKillEvent(id, mesg));\n        }\n      }\n    }\n    // currently running task attempts on unusable nodes are handled in\n    // RMContainerAllocator\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getState": "  public JobState getState() {\n    readLock.lock();\n    try {\n      JobState state = getExternalState(getInternalState());\n      if (!appContext.hasSuccessfullyUnregistered()\n          && (state == JobState.SUCCEEDED || state == JobState.FAILED\n          || state == JobState.KILLED || state == JobState.ERROR)) {\n        return lastNonFinalState;\n      } else {\n        return state;\n      }\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.checkReadyForCommit": "  protected JobStateInternal checkReadyForCommit() {\n    JobStateInternal currentState = getInternalState();\n    if (completedTaskCount == tasks.size()\n        && currentState == JobStateInternal.RUNNING) {\n      eventHandler.handle(new CommitterJobCommitEvent(jobId, getJobContext()));\n      return JobStateInternal.COMMITTING;\n    }\n    // return the current state as job not ready to commit yet\n    return getInternalState();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.scheduleTasks": "  protected void scheduleTasks(Set<TaskId> taskIDs,\n      boolean recoverTaskOutput) {\n    for (TaskId taskID : taskIDs) {\n      TaskInfo taskInfo = completedTasksFromPreviousRun.remove(taskID);\n      if (taskInfo != null) {\n        eventHandler.handle(new TaskRecoverEvent(taskID, taskInfo,\n            committer, recoverTaskOutput));\n      } else {\n        eventHandler.handle(new TaskEvent(taskID, TaskEventType.T_SCHEDULE));\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.logJobHistoryFinishedEvent": "  void logJobHistoryFinishedEvent() {\n    this.setFinishTime();\n    JobFinishedEvent jfe = createJobFinishedEvent(this);\n    LOG.info(\"Calling handler for JobFinishedEvent \");\n    this.getEventHandler().handle(new JobHistoryEvent(this.jobId, jfe));    \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.setup": "    protected void setup(JobImpl job) throws IOException {\n\n      String oldJobIDString = job.oldJobId.toString();\n      String user = \n        UserGroupInformation.getCurrentUser().getShortUserName();\n      Path path = MRApps.getStagingAreaDir(job.conf, user);\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"startJobs: parent=\" + path + \" child=\" + oldJobIDString);\n      }\n\n      job.remoteJobSubmitDir =\n          FileSystem.get(job.conf).makeQualified(\n              new Path(path, oldJobIDString));\n      job.remoteJobConfFile =\n          new Path(job.remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n\n      // Prepare the TaskAttemptListener server for authentication of Containers\n      // TaskAttemptListener gets the information via jobTokenSecretManager.\n      JobTokenIdentifier identifier =\n          new JobTokenIdentifier(new Text(oldJobIDString));\n      job.jobToken =\n          new Token<JobTokenIdentifier>(identifier, job.jobTokenSecretManager);\n      job.jobToken.setService(identifier.getJobId());\n      // Add it to the jobTokenSecretManager so that TaskAttemptListener server\n      // can authenticate containers(tasks)\n      job.jobTokenSecretManager.addTokenForJob(oldJobIDString, job.jobToken);\n      LOG.info(\"Adding job token for \" + oldJobIDString\n          + \" to jobTokenSecretManager\");\n\n      // If the job client did not setup the shuffle secret then reuse\n      // the job token secret for the shuffle.\n      if (TokenCache.getShuffleSecretKey(job.jobCredentials) == null) {\n        LOG.warn(\"Shuffle secret key missing from job credentials.\"\n            + \" Using job token secret as shuffle secret.\");\n        TokenCache.setShuffleSecretKey(job.jobToken.getPassword(),\n            job.jobCredentials);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.setFinishTime": "  void setFinishTime() {\n    finishTime = clock.getTime();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getFileSystem": "  protected FileSystem getFileSystem(Configuration conf) throws IOException {\n    return FileSystem.get(conf);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getWorkflowAdjacencies": "  private static String getWorkflowAdjacencies(Configuration conf) {\n    int prefixLen = MRJobConfig.WORKFLOW_ADJACENCY_PREFIX_STRING.length();\n    Map<String,String> adjacencies = \n        conf.getValByRegex(MRJobConfig.WORKFLOW_ADJACENCY_PREFIX_PATTERN);\n    if (adjacencies.isEmpty()) {\n      return \"\";\n    }\n    int size = 0;\n    for (Entry<String,String> entry : adjacencies.entrySet()) {\n      int keyLen = entry.getKey().length();\n      size += keyLen - prefixLen;\n      size += entry.getValue().length() + 6;\n    }\n    StringBuilder sb = new StringBuilder(size);\n    for (Entry<String,String> entry : adjacencies.entrySet()) {\n      int keyLen = entry.getKey().length();\n      sb.append(\"\\\"\");\n      sb.append(escapeString(entry.getKey().substring(prefixLen, keyLen)));\n      sb.append(\"\\\"=\\\"\");\n      sb.append(escapeString(entry.getValue()));\n      sb.append(\"\\\" \");\n    }\n    return sb.toString();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.taskSucceeded": "    private void taskSucceeded(JobImpl job, Task task) {\n      if (task.getType() == TaskType.MAP) {\n        job.succeededMapTaskCount++;\n      } else {\n        job.succeededReduceTaskCount++;\n      }\n      job.metrics.completedTask(task);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.unsuccessfulFinish": "  private void unsuccessfulFinish(JobStateInternal finalState) {\n      if (finishTime == 0) setFinishTime();\n      cleanupProgress = 1.0f;\n      JobUnsuccessfulCompletionEvent unsuccessfulJobEvent =\n          new JobUnsuccessfulCompletionEvent(oldJobId,\n              finishTime,\n              succeededMapTaskCount,\n              succeededReduceTaskCount,\n              finalState.toString(),\n              diagnostics);\n      eventHandler.handle(new JobHistoryEvent(jobId,\n          unsuccessfulJobEvent));\n      finished(finalState);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.finished": "  JobStateInternal finished(JobStateInternal finalState) {\n    if (getInternalState() == JobStateInternal.RUNNING) {\n      metrics.endRunningJob(this);\n    }\n    if (finishTime == 0) setFinishTime();\n    eventHandler.handle(new JobFinishEvent(jobId));\n\n    switch (finalState) {\n      case KILLED:\n        metrics.killedJob(this);\n        break;\n      case REBOOT:\n      case ERROR:\n      case FAILED:\n        metrics.failedJob(this);\n        break;\n      case SUCCEEDED:\n        metrics.completedJob(this);\n        break;\n      default:\n        throw new IllegalArgumentException(\"Illegal job state: \" + finalState);\n    }\n    return finalState;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.checkJobAfterTaskCompletion": "    protected JobStateInternal checkJobAfterTaskCompletion(JobImpl job) {\n      if (job.completedTaskCount == job.tasks.size()) {\n        job.setFinishTime();\n        job.eventHandler.handle(new CommitterJobAbortEvent(job.jobId,\n            job.jobContext,\n            org.apache.hadoop.mapreduce.JobStatus.State.KILLED));\n        return JobStateInternal.KILL_ABORT;\n      }\n      //return the current state, Job not finished yet\n      return job.getInternalState();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.createMapTasks": "    private void createMapTasks(JobImpl job, long inputLength,\n                                TaskSplitMetaInfo[] splits) {\n      for (int i=0; i < job.numMapTasks; ++i) {\n        TaskImpl task =\n            new MapTaskImpl(job.jobId, i,\n                job.eventHandler, \n                job.remoteJobConfFile, \n                job.conf, splits[i], \n                job.taskAttemptListener, \n                job.jobToken, job.jobCredentials,\n                job.clock,\n                job.applicationAttemptId.getAttemptId(),\n                job.metrics, job.appContext);\n        job.addTask(task);\n      }\n      LOG.info(\"Input size for job \" + job.jobId + \" = \" + inputLength\n          + \". Number of splits = \" + splits.length);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getID": "  public JobId getID() {\n    return jobId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.taskFailed": "    private void taskFailed(JobImpl job, Task task) {\n      if (task.getType() == TaskType.MAP) {\n        job.failedMapTaskCount++;\n      } else if (task.getType() == TaskType.REDUCE) {\n        job.failedReduceTaskCount++;\n      }\n      job.addDiagnostic(\"Task failed \" + task.getID());\n      job.metrics.failedTask(task);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.checkTaskLimits": "    private void checkTaskLimits() {\n      // no code, for now\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.isUber": "  public boolean isUber() {\n    return isUber;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.addDiagnostic": "  protected void addDiagnostic(String diag) {\n    diagnostics.add(diag);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.createSplits": "    protected TaskSplitMetaInfo[] createSplits(JobImpl job, JobId jobId) {\n      TaskSplitMetaInfo[] allTaskSplitMetaInfo;\n      try {\n        allTaskSplitMetaInfo = SplitMetaInfoReader.readSplitMetaInfo(\n            job.oldJobId, job.fs, \n            job.conf, \n            job.remoteJobSubmitDir);\n      } catch (IOException e) {\n        throw new YarnRuntimeException(e);\n      }\n      return allTaskSplitMetaInfo;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.createReduceTasks": "    private void createReduceTasks(JobImpl job) {\n      for (int i = 0; i < job.numReduceTasks; i++) {\n        TaskImpl task =\n            new ReduceTaskImpl(job.jobId, i,\n                job.eventHandler, \n                job.remoteJobConfFile, \n                job.conf, job.numMapTasks, \n                job.taskAttemptListener, job.jobToken,\n                job.jobCredentials, job.clock,\n                job.applicationAttemptId.getAttemptId(),\n                job.metrics, job.appContext);\n        job.addTask(task);\n      }\n      LOG.info(\"Number of reduces for job \" + job.jobId + \" = \"\n          + job.numReduceTasks);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle": "  public void handle(JobEvent event) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Processing \" + event.getJobId() + \" of type \"\n          + event.getType());\n    }\n    try {\n      writeLock.lock();\n      JobStateInternal oldState = getInternalState();\n      try {\n         getStateMachine().doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        addDiagnostic(\"Invalid event \" + event.getType() + \n            \" on Job \" + this.jobId);\n        eventHandler.handle(new JobEvent(this.jobId,\n            JobEventType.INTERNAL_ERROR));\n      }\n      //notify the eventhandler of state change\n      if (oldState != getInternalState()) {\n        LOG.info(jobId + \"Job Transitioned from \" + oldState + \" to \"\n                 + getInternalState());\n        rememberLastNonFinalState(oldState);\n      }\n    }\n    \n    finally {\n      writeLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.rememberLastNonFinalState": "  private void rememberLastNonFinalState(JobStateInternal stateInternal) {\n    JobState state = getExternalState(stateInternal);\n    // if state is not the final state, set lastNonFinalState\n    if (state != JobState.SUCCEEDED && state != JobState.FAILED\n        && state != JobState.KILLED && state != JobState.ERROR) {\n      lastNonFinalState = state;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getStateMachine": "  protected StateMachine<JobStateInternal, JobEventType, JobEvent> getStateMachine() {\n    return stateMachine;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getInternalState": "  public JobStateInternal getInternalState() {\n    readLock.lock();\n    try {\n      if(forcedState != null) {\n        return forcedState;\n      }\n     return getStateMachine().getCurrentState();\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.handle": "    public void handle(Event event) {\n      //Empty\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJob": "    public Job getJob(JobId jobID) {\n      return jobs.get(jobID);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob": "  public void shutDownJob() {\n    // job has finished\n    // this is the only job, so shut down the Appmaster\n    // note in a workflow scenario, this may lead to creation of a new\n    // job (FIXME?)\n\n    try {\n      //if isLastAMRetry comes as true, should never set it to false\n      if ( !isLastAMRetry){\n        if (((JobImpl)job).getInternalState() != JobStateInternal.REBOOT) {\n          LOG.info(\"We are finishing cleanly so this is the last retry\");\n          isLastAMRetry = true;\n        }\n      }\n      notifyIsLastAMRetry(isLastAMRetry);\n      // Stop all services\n      // This will also send the final report to the ResourceManager\n      LOG.info(\"Calling stop for all the services\");\n      MRAppMaster.this.stop();\n\n      if (isLastAMRetry) {\n        // Send job-end notification when it is safe to report termination to\n        // users and it is the last AM retry\n        if (getConfig().get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL) != null) {\n          try {\n            LOG.info(\"Job end notification started for jobID : \"\n                + job.getReport().getJobId());\n            JobEndNotifier notifier = new JobEndNotifier();\n            notifier.setConf(getConfig());\n            JobReport report = job.getReport();\n            // If unregistration fails, the final state is unavailable. However,\n            // at the last AM Retry, the client will finally be notified FAILED\n            // from RM, so we should let users know FAILED via notifier as well\n            if (!context.hasSuccessfullyUnregistered()) {\n              report.setJobState(JobState.FAILED);\n            }\n            notifier.notify(report);\n          } catch (InterruptedException ie) {\n            LOG.warn(\"Job end notification interrupted for jobID : \"\n                + job.getReport().getJobId(), ie);\n          }\n        }\n      }\n\n      try {\n        Thread.sleep(5000);\n      } catch (InterruptedException e) {\n        e.printStackTrace();\n      }\n      clientService.stop();\n    } catch (Throwable t) {\n      LOG.warn(\"Graceful stop failed \", t);\n    }\n\n    //Bring the process down by force.\n    //Not needed after HADOOP-7140\n    LOG.info(\"Exiting MR AppMaster..GoodBye!\");\n    sysexit();   \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getJobId": "  public JobId getJobId() {\n    return jobId;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart": "  protected void serviceStart() throws Exception {\n\n    amInfos = new LinkedList<AMInfo>();\n    completedTasksFromPreviousRun = new HashMap<TaskId, TaskInfo>();\n    processRecovery();\n\n    // Current an AMInfo for the current AM generation.\n    AMInfo amInfo =\n        MRBuilderUtils.newAMInfo(appAttemptID, startTime, containerID, nmHost,\n            nmPort, nmHttpPort);\n\n    // /////////////////// Create the job itself.\n    job = createJob(getConfig(), forcedState, shutDownMessage);\n\n    // End of creating the job.\n\n    // Send out an MR AM inited event for all previous AMs.\n    for (AMInfo info : amInfos) {\n      dispatcher.getEventHandler().handle(\n          new JobHistoryEvent(job.getID(), new AMStartedEvent(info\n              .getAppAttemptId(), info.getStartTime(), info.getContainerId(),\n              info.getNodeManagerHost(), info.getNodeManagerPort(), info\n                  .getNodeManagerHttpPort())));\n    }\n\n    // Send out an MR AM inited event for this AM.\n    dispatcher.getEventHandler().handle(\n        new JobHistoryEvent(job.getID(), new AMStartedEvent(amInfo\n            .getAppAttemptId(), amInfo.getStartTime(), amInfo.getContainerId(),\n            amInfo.getNodeManagerHost(), amInfo.getNodeManagerPort(), amInfo\n                .getNodeManagerHttpPort(), this.forcedState == null ? null\n                    : this.forcedState.toString())));\n    amInfos.add(amInfo);\n\n    // metrics system init is really init & start.\n    // It's more test friendly to put it here.\n    DefaultMetricsSystem.initialize(\"MRAppMaster\");\n\n    if (!errorHappenedShutDown) {\n      // create a job event for job intialization\n      JobEvent initJobEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT);\n      // Send init to the job (this does NOT trigger job execution)\n      // This is a synchronous call, not an event through dispatcher. We want\n      // job-init to be done completely here.\n      jobEventDispatcher.handle(initJobEvent);\n\n\n      // JobImpl's InitTransition is done (call above is synchronous), so the\n      // \"uber-decision\" (MR-1220) has been made.  Query job and switch to\n      // ubermode if appropriate (by registering different container-allocator\n      // and container-launcher services/event-handlers).\n\n      if (job.isUber()) {\n        speculatorEventDispatcher.disableSpeculation();\n        LOG.info(\"MRAppMaster uberizing job \" + job.getID()\n            + \" in local container (\\\"uber-AM\\\") on node \"\n            + nmHost + \":\" + nmPort + \".\");\n      } else {\n        // send init to speculator only for non-uber jobs. \n        // This won't yet start as dispatcher isn't started yet.\n        dispatcher.getEventHandler().handle(\n            new SpeculatorEvent(job.getID(), clock.getTime()));\n        LOG.info(\"MRAppMaster launching normal, non-uberized, multi-container \"\n            + \"job \" + job.getID() + \".\");\n      }\n      // Start ClientService here, since it's not initialized if\n      // errorHappenedShutDown is true\n      clientService.start();\n    }\n    //start all the components\n    super.serviceStart();\n\n    // set job classloader if configured\n    MRApps.setJobClassLoader(getConfig());\n    // All components have started, start the job.\n    startJobs();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.processRecovery": "  private void processRecovery() {\n    if (appAttemptID.getAttemptId() == 1) {\n      return;  // no need to recover on the first attempt\n    }\n\n    boolean recoveryEnabled = getConfig().getBoolean(\n        MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE,\n        MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE_DEFAULT);\n    boolean recoverySupportedByCommitter =\n        committer != null && committer.isRecoverySupported();\n\n    // If a shuffle secret was not provided by the job client then this app\n    // attempt will generate one.  However that disables recovery if there\n    // are reducers as the shuffle secret would be app attempt specific.\n    int numReduceTasks = getConfig().getInt(MRJobConfig.NUM_REDUCES, 0);\n    boolean shuffleKeyValidForRecovery =\n        TokenCache.getShuffleSecretKey(jobCredentials) != null;\n\n    if (recoveryEnabled && recoverySupportedByCommitter\n        && (numReduceTasks <= 0 || shuffleKeyValidForRecovery)) {\n      LOG.info(\"Recovery is enabled. \"\n          + \"Will try to recover from previous life on best effort basis.\");\n      try {\n        parsePreviousJobHistory();\n      } catch (IOException e) {\n        LOG.warn(\"Unable to parse prior job history, aborting recovery\", e);\n        // try to get just the AMInfos\n        amInfos.addAll(readJustAMInfos());\n      }\n    } else {\n      LOG.info(\"Will not try to recover. recoveryEnabled: \"\n            + recoveryEnabled + \" recoverySupportedByCommitter: \"\n            + recoverySupportedByCommitter + \" numReduceTasks: \"\n            + numReduceTasks + \" shuffleKeyValidForRecovery: \"\n            + shuffleKeyValidForRecovery + \" ApplicationAttemptID: \"\n            + appAttemptID.getAttemptId());\n      // Get the amInfos anyways whether recovery is enabled or not\n      amInfos.addAll(readJustAMInfos());\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.startJobs": "  protected void startJobs() {\n    /** create a job-start event to get this ball rolling */\n    JobEvent startJobEvent = new JobStartEvent(job.getID(),\n        recoveredJobStartTime);\n    /** send the job-start event. this triggers the job execution. */\n    dispatcher.getEventHandler().handle(startJobEvent);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.disableSpeculation": "    public void disableSpeculation() {\n      disabled = true;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getStartTime": "    public long getStartTime() {\n      return startTime;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getEventHandler": "    public EventHandler getEventHandler() {\n      return dispatcher.getEventHandler();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createJob": "  protected Job createJob(Configuration conf, JobStateInternal forcedState, \n      String diagnostic) {\n\n    // create single job\n    Job newJob =\n        new JobImpl(jobId, appAttemptID, conf, dispatcher.getEventHandler(),\n            taskAttemptListener, jobTokenSecretManager, jobCredentials, clock,\n            completedTasksFromPreviousRun, metrics,\n            committer, newApiCommitter,\n            currentUser.getUserName(), appSubmitTime, amInfos, context, \n            forcedState, diagnostic);\n    ((RunningAppContext) context).jobs.put(newJob.getID(), newJob);\n\n    dispatcher.register(JobFinishEvent.Type.class,\n        createJobFinishEventHandler());     \n    return newJob;\n  } // end createJob()",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.start": "  public void start() {\n    if (isInState(STATE.STARTED)) {\n      return;\n    }\n    //enter the started state\n    synchronized (stateChangeLock) {\n      if (stateModel.enterState(STATE.STARTED) != STATE.STARTED) {\n        try {\n          startTime = System.currentTimeMillis();\n          serviceStart();\n          if (isInState(STATE.STARTED)) {\n            //if the service started (and isn't now in a later state), notify\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Service \" + getName() + \" is started\");\n            }\n            notifyListeners();\n          }\n        } catch (Exception e) {\n          noteFailure(e);\n          ServiceOperations.stopQuietly(LOG, this);\n          throw ServiceStateException.convert(e);\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.isInState": "  public final boolean isInState(Service.STATE expected) {\n    return stateModel.isInState(expected);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.getName": "  public String getName() {\n    return name;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.notifyListeners": "  private void notifyListeners() {\n    try {\n      listeners.notifyListeners(this);\n      globalListeners.notifyListeners(this);\n    } catch (Throwable e) {\n      LOG.warn(\"Exception while notifying listeners of \" + this + \": \" + e,\n               e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.enterState": "  private STATE enterState(STATE newState) {\n    assert stateModel != null : \"null state in \" + name + \" \" + this.getClass();\n    STATE oldState = stateModel.enterState(newState);\n    if (oldState != newState) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\n          \"Service: \" + getName() + \" entered state \" + getServiceState());\n      }\n      recordLifecycleEvent();\n    }\n    return oldState;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.serviceStart": "  protected void serviceStart() throws Exception {\n\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.noteFailure": "  protected final void noteFailure(Exception exception) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"noteFailure \" + exception, null);\n    }\n    if (exception == null) {\n      //make sure failure logic doesn't itself cause problems\n      return;\n    }\n    //record the failure details, and log it\n    synchronized (this) {\n      if (failureCause == null) {\n        failureCause = exception;\n        failureState = getServiceState();\n        LOG.info(\"Service \" + getName()\n                 + \" failed in state \" + failureState\n                 + \"; cause: \" + exception,\n                 exception);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.run": "      public Object run() throws Exception {\n        appMaster.init(conf);\n        appMaster.start();\n        if(appMaster.errorHappenedShutDown) {\n          throw new IOException(\"Was asked to shut down.\");\n        }\n        return null;\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.notifyIsLastAMRetry": "  public void notifyIsLastAMRetry(boolean isLastAMRetry){\n    if(containerAllocator instanceof ContainerAllocatorRouter) {\n      LOG.info(\"Notify RMCommunicator isAMLastRetry: \" + isLastAMRetry);\n      ((ContainerAllocatorRouter) containerAllocator)\n        .setShouldUnregister(isLastAMRetry);\n    }\n    if(jobHistoryEventHandler != null) {\n      LOG.info(\"Notify JHEH isAMLastRetry: \" + isLastAMRetry);\n      jobHistoryEventHandler.setForcejobCompletion(isLastAMRetry);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop": "  public void stop() {\n    super.stop();\n    TaskLog.syncLogsShutdown(logSyncer);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster": "  protected static void initAndStartAppMaster(final MRAppMaster appMaster,\n      final JobConf conf, String jobUserName) throws IOException,\n      InterruptedException {\n    UserGroupInformation.setConfiguration(conf);\n    // Security framework already loaded the tokens into current UGI, just use\n    // them\n    Credentials credentials =\n        UserGroupInformation.getCurrentUser().getCredentials();\n    LOG.info(\"Executing with tokens:\");\n    for (Token<?> token : credentials.getAllTokens()) {\n      LOG.info(token);\n    }\n    \n    UserGroupInformation appMasterUgi = UserGroupInformation\n        .createRemoteUser(jobUserName);\n    appMasterUgi.addCredentials(credentials);\n\n    // Now remove the AM->RM token so tasks don't have it\n    Iterator<Token<?>> iter = credentials.getAllTokens().iterator();\n    while (iter.hasNext()) {\n      Token<?> token = iter.next();\n      if (token.getKind().equals(AMRMTokenIdentifier.KIND_NAME)) {\n        iter.remove();\n      }\n    }\n    conf.getCredentials().addAll(credentials);\n    appMasterUgi.doAs(new PrivilegedExceptionAction<Object>() {\n      @Override\n      public Object run() throws Exception {\n        appMaster.init(conf);\n        appMaster.start();\n        if(appMaster.errorHappenedShutDown) {\n          throw new IOException(\"Was asked to shut down.\");\n        }\n        return null;\n      }\n    });\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getCredentials": "  protected Credentials getCredentials() {\n    return jobCredentials;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main": "  public static void main(String[] args) {\n    try {\n      Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\n      String containerIdStr =\n          System.getenv(Environment.CONTAINER_ID.name());\n      String nodeHostString = System.getenv(Environment.NM_HOST.name());\n      String nodePortString = System.getenv(Environment.NM_PORT.name());\n      String nodeHttpPortString =\n          System.getenv(Environment.NM_HTTP_PORT.name());\n      String appSubmitTimeStr =\n          System.getenv(ApplicationConstants.APP_SUBMIT_TIME_ENV);\n      String maxAppAttempts =\n          System.getenv(ApplicationConstants.MAX_APP_ATTEMPTS_ENV);\n      \n      validateInputParam(containerIdStr,\n          Environment.CONTAINER_ID.name());\n      validateInputParam(nodeHostString, Environment.NM_HOST.name());\n      validateInputParam(nodePortString, Environment.NM_PORT.name());\n      validateInputParam(nodeHttpPortString,\n          Environment.NM_HTTP_PORT.name());\n      validateInputParam(appSubmitTimeStr,\n          ApplicationConstants.APP_SUBMIT_TIME_ENV);\n      validateInputParam(maxAppAttempts,\n          ApplicationConstants.MAX_APP_ATTEMPTS_ENV);\n\n      ContainerId containerId = ConverterUtils.toContainerId(containerIdStr);\n      ApplicationAttemptId applicationAttemptId =\n          containerId.getApplicationAttemptId();\n      long appSubmitTime = Long.parseLong(appSubmitTimeStr);\n      \n      \n      MRAppMaster appMaster =\n          new MRAppMaster(applicationAttemptId, containerId, nodeHostString,\n              Integer.parseInt(nodePortString),\n              Integer.parseInt(nodeHttpPortString), appSubmitTime,\n              Integer.parseInt(maxAppAttempts));\n      ShutdownHookManager.get().addShutdownHook(\n        new MRAppMasterShutdownHook(appMaster), SHUTDOWN_HOOK_PRIORITY);\n      JobConf conf = new JobConf(new YarnConfiguration());\n      conf.addResource(new Path(MRJobConfig.JOB_CONF_FILE));\n      \n      MRWebAppUtil.initialize(conf);\n      // log the system properties\n      String systemPropsToLog = MRApps.getSystemPropertiesToLog(conf);\n      if (systemPropsToLog != null) {\n        LOG.info(systemPropsToLog);\n      }\n\n      String jobUserName = System\n          .getenv(ApplicationConstants.Environment.USER.name());\n      conf.set(MRJobConfig.USER_NAME, jobUserName);\n      // Do not automatically close FileSystem objects so that in case of\n      // SIGTERM I have a chance to write out the job history. I'll be closing\n      // the objects myself.\n      conf.setBoolean(\"fs.automatic.close\", false);\n      initAndStartAppMaster(appMaster, conf, jobUserName);\n    } catch (Throwable t) {\n      LOG.fatal(\"Error starting MRAppMaster\", t);\n      ExitUtil.terminate(1, t);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getApplicationAttemptId": "    public ApplicationAttemptId getApplicationAttemptId() {\n      return appAttemptID;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.validateInputParam": "  private static void validateInputParam(String value, String param)\n      throws IOException {\n    if (value == null) {\n      String msg = param + \" is null\";\n      LOG.error(msg);\n      throw new IOException(msg);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.getAttempt": "  public TaskAttempt getAttempt(TaskAttemptId attemptID) {\n    readLock.lock();\n    try {\n      return attempts.get(attemptID);\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.getAttempts": "  public Map<TaskAttemptId, TaskAttempt> getAttempts() {\n    readLock.lock();\n\n    try {\n      if (attempts.size() <= 1) {\n        return attempts;\n      }\n      \n      Map<TaskAttemptId, TaskAttempt> result\n          = new LinkedHashMap<TaskAttemptId, TaskAttempt>();\n      result.putAll(attempts);\n\n      return result;\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.isFinished": "  public boolean isFinished() {\n    readLock.lock();\n    try {\n     // TODO: Use stateMachine level method?\n      return (getInternalState() == TaskStateInternal.SUCCEEDED ||\n              getInternalState() == TaskStateInternal.FAILED ||\n              getInternalState() == TaskStateInternal.KILLED);\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.getInternalState": "  public TaskStateInternal getInternalState() {\n    readLock.lock();\n    try {\n      return stateMachine.getCurrentState();\n    } finally {\n      readLock.unlock();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceOperations.stopQuietly": "  public static Exception stopQuietly(Log log, Service service) {\n    try {\n      stop(service);\n    } catch (Exception e) {\n      log.warn(\"When stopping the service \" + service.getName()\n               + \" : \" + e,\n               e);\n      return e;\n    }\n    return null;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceOperations.stop": "  public static void stop(Service service) {\n    if (service != null) {\n      service.stop();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceStateException.convert": "  public static RuntimeException convert(String text, Throwable fault) {\n    if (fault instanceof RuntimeException) {\n      return (RuntimeException) fault;\n    } else {\n      return new ServiceStateException(text, fault);\n    }\n  }"
        },
        "bug_report": {
            "Title": "MRAppMaster fails when checking on uber mode",
            "Description": "When the MRAppMaster determines whether the job should run in the uber mode, it call {{Class.forName()}} to check whether the class is derived from {{ChainMapper}}:\n\n{code}\n try {\n      String mapClassName = conf.get(MRJobConfig.MAP_CLASS_ATTR);\n      if (mapClassName != null) {\n        Class<?> mapClass = Class.forName(mapClassName);\n        if (ChainMapper.class.isAssignableFrom(mapClass))\n          isChainJob = true;\n      }\n    } catch (ClassNotFoundException cnfe) {\n      // don't care; assume it's not derived from ChainMapper\n    }\n{code}\n\nThe problem here is that {{Class.forName()}} can also throw {{NoClassDefError}}. It happens when the additional dependent jar is unavailable to the MRAppMaster. For example, the MRAppMaster complains about a MR job on Scala:\n\n{noformat}\n2014-04-15 11:52:55,877 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster\njava.lang.NoClassDefFoundError: scala/Function1\n        at java.lang.Class.forName0(Native Method)\n        at java.lang.Class.forName(Class.java:190)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.isChainJob(JobImpl.java:1282)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.makeUberDecision(JobImpl.java:1224)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.access$3700(JobImpl.java:136)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1425)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1363)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:976)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:135)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1263)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1063)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$1.run(MRAppMaster.java:1480)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1606)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1476)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1409)\nCaused by: java.lang.ClassNotFoundException: scala.Function1\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n        ... 22 more\n{noformat}\n \nThe proposed fix is to catch {{NoClassDefError}} at the corresponding places."
        }
    },
    {
        "filename": "MAPREDUCE-3058.json",
        "creation_time": "2011-09-21T12:53:39.000+0000",
        "stack_trace": "```\njava.io.EOFException: Premature EOF: no length prefix available\n        at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:158)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:860)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:838)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:929)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:740)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:415)\n\njava.io.EOFException: Premature EOF: no length prefix available\n        at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:158)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:860)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:838)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:929)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:740)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:415)\n```",
        "source_code": {},
        "bug_report": {
            "Title": "Sometimes task keeps on running while its Syslog says that it is shutdown",
            "Description": "While running GridMixV3, one of the jobs got stuck for 15 hrs. After clicking on the Job-page, found one of its reduces to be stuck. Looking at syslog of the stuck reducer, found this:\nTask-logs' head:\n\n{code}\n2011-09-19 17:57:22,002 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2011-09-19 17:57:22,002 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started\n{code}\n\nTask-logs' tail:\n{code}\n2011-09-19 18:06:49,818 INFO org.apache.hadoop.hdfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: Bad connect ack with firstBadLink as <DATANODE1>\n2011-09-19 18:06:49,818 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block BP-1405370709-<NAMENODE>-1316452621953:blk_-7004355226367468317_79871 in pipeline  <DATANODE2>,  <DATANODE1>: bad datanode  <DATANODE1>\n2011-09-19 18:06:49,818 DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol: lastAckedSeqno = 26870\n2011-09-19 18:06:49,820 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <NAMENODE> from gridperf sending #454\n2011-09-19 18:06:49,826 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <<NAMENODE> from gridperf got value #454\n2011-09-19 18:06:49,827 DEBUG org.apache.hadoop.ipc.RPC: Call: getAdditionalDatanode 8\n2011-09-19 18:06:49,827 DEBUG org.apache.hadoop.hdfs.DFSClient: Connecting to datanode <DATANODE2>\n2011-09-19 18:06:49,827 DEBUG org.apache.hadoop.hdfs.DFSClient: Send buf size 131071\n2011-09-19 18:06:49,833 WARN org.apache.hadoop.hdfs.DFSClient: DataStreamer Exception\njava.io.EOFException: Premature EOF: no length prefix available\n        at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:158)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:860)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:838)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:929)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:740)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:415)\n2011-09-19 18:06:49,837 WARN org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.EOFException: Premature EOF: no length prefix available\n        at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:158)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:860)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:838)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:929)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:740)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:415)\n\n2011-09-19 18:06:49,837 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <APPMASTER> from job_1316452677984_0862 sending #455\n2011-09-19 18:06:49,839 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <APPMASTER> from job_1316452677984_0862 got value #455\n2011-09-19 18:06:49,840 DEBUG org.apache.hadoop.ipc.RPC: Call: statusUpdate 3\n2011-09-19 18:06:49,840 INFO org.apache.hadoop.mapred.Task: Runnning cleanup for the task\n2011-09-19 18:06:49,840 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <NAMENODE> from gridperf sending #456\n2011-09-19 18:06:49,858 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <NAMENODE> from gridperf got value #456\n2011-09-19 18:06:49,858 DEBUG org.apache.hadoop.ipc.RPC: Call: delete 18\n2011-09-19 18:06:49,858 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <APPMASTER> from job_1316452677984_0862 sending #457\n2011-09-19 18:06:49,859 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <APPMASTER> from job_1316452677984_0862 got value #457\n2011-09-19 18:06:49,859 DEBUG org.apache.hadoop.ipc.RPC: Call: reportDiagnosticInfo 1\n2011-09-19 18:06:49,859 DEBUG org.apache.hadoop.metrics2.impl.MetricsSystemImpl: refCount=1\n2011-09-19 18:06:49,859 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping ReduceTask metrics system...\n2011-09-19 18:06:49,859 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping metrics source UgiMetrics\n2011-09-19 18:06:49,859 DEBUG org.apache.hadoop.metrics2.impl.MetricsSystemImpl: class org.apache.hadoop.metrics2.lib.MetricsSourceBuilder$1\n2011-09-19 18:06:49,860 DEBUG org.apache.hadoop.metrics2.util.MBeans: Unregistering Hadoop:service=ReduceTask,name=UgiMetrics\n2011-09-19 18:06:49,860 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping metrics source JvmMetrics\n2011-09-19 18:06:49,860 DEBUG org.apache.hadoop.metrics2.impl.MetricsSystemImpl: class org.apache.hadoop.metrics2.source.JvmMetrics\n2011-09-19 18:06:49,860 DEBUG org.apache.hadoop.metrics2.util.MBeans: Unregistering Hadoop:service=ReduceTask,name=JvmMetrics\n2011-09-19 18:06:49,860 DEBUG org.apache.hadoop.metrics2.util.MBeans: Unregistering Hadoop:service=ReduceTask,name=MetricsSystem,sub=Stats\n2011-09-19 18:06:49,860 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ReduceTask metrics system stopped.\n2011-09-19 18:06:49,860 DEBUG org.apache.hadoop.metrics2.util.MBeans: Unregistering Hadoop:service=ReduceTask,name=MetricsSystem,sub=Control\n2011-09-19 18:06:49,860 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ReduceTask metrics system shutdown complete.\n{code}\n\nWhich means that tasks is supposed to have stopped within 20 secs, whereas the process itself is stuck for more than 15 hours. From AM log, also found that this task was sending its update regularly. ps -ef | grep java was also showing that process is still alive.\n"
        }
    },
    {
        "filename": "MAPREDUCE-5088.json",
        "creation_time": "2013-03-15T17:55:08.000+0000",
        "stack_trace": "```\norg.apache.oozie.action.ActionExecutorException: UninitializedMessageException: Message missing required fields: renewer\n\tat org.apache.oozie.action.ActionExecutor.convertException(ActionExecutor.java:401)\n\tat org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:738)\n\tat org.apache.oozie.action.hadoop.JavaActionExecutor.start(JavaActionExecutor.java:889)\n\tat org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:211)\n\tat org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:59)\n\tat org.apache.oozie.command.XCommand.call(XCommand.java:277)\n\tat org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:326)\n\tat org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:255)\n\tat org.apache.oozie.service.CallableQueueService$CallableWrapper.run(CallableQueueService.java:175)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:662)\nCaused by: com.google.protobuf.UninitializedMessageException: Message missing required fields: renewer\n\tat com.google.protobuf.AbstractMessage$Builder.newUninitializedMessageException(AbstractMessage.java:605)\n\tat org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenRequestProto$Builder.build(SecurityProtos.java:973)\n\tat org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.mergeLocalToProto(GetDelegationTokenRequestPBImpl.java:84)\n\tat org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.getProto(GetDelegationTokenRequestPBImpl.java:67)\n\tat org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getDelegationToken(MRClientProtocolPBClientImpl.java:200)\n\tat org.apache.hadoop.mapred.YARNRunner.getDelegationTokenFromHS(YARNRunner.java:194)\n\tat org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:273)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:392)\n\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1218)\n\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1215)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1439)\n\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1215)\n\tat org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:581)\n\tat org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:576)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1439)\n\tat org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:576)\n\tat org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:723)```",
        "source_code": {
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.mergeLocalToProto": "  private void mergeLocalToProto() {\n    if (viaProto) \n      maybeInitBuilder();\n    mergeLocalToBuilder();\n    proto = builder.build();\n    viaProto = true;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.mergeLocalToBuilder": "  private void mergeLocalToBuilder() {\n    if (renewer != null) {\n      builder.setRenewer(this.renewer);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.maybeInitBuilder": "  private void maybeInitBuilder() {\n    if (viaProto || builder == null) {\n      builder = GetDelegationTokenRequestProto.newBuilder(proto);\n    }\n    viaProto = false;\n  }   ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.getProto": "  public GetDelegationTokenRequestProto getProto() {\n    mergeLocalToProto();\n    proto = viaProto ? proto : builder.build();\n    viaProto = true;\n    return proto;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getDelegationToken": "  public GetDelegationTokenResponse getDelegationToken(\n      GetDelegationTokenRequest request) throws YarnRemoteException {\n    GetDelegationTokenRequestProto requestProto = ((GetDelegationTokenRequestPBImpl)\n        request).getProto();\n    try {\n      return new GetDelegationTokenResponsePBImpl(proxy.getDelegationToken(\n          null, requestProto));\n    } catch (ServiceException e) {\n      throw YarnRemoteExceptionPBImpl.unwrapAndThrowException(e);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.YARNRunner.getDelegationTokenFromHS": "  Token<?> getDelegationTokenFromHS(MRClientProtocol hsProxy)\n      throws IOException, InterruptedException {\n    GetDelegationTokenRequest request = recordFactory\n      .newRecordInstance(GetDelegationTokenRequest.class);\n    request.setRenewer(Master.getMasterPrincipal(conf));\n    DelegationToken mrDelegationToken = hsProxy.getDelegationToken(request)\n      .getDelegationToken();\n    return ProtoUtils.convertFromProtoFormat(mrDelegationToken,\n                                             hsProxy.getConnectAddress());\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.YARNRunner.getDelegationToken": "  public Token<DelegationTokenIdentifier> getDelegationToken(Text renewer)\n      throws IOException, InterruptedException {\n    // The token is only used for serialization. So the type information\n    // mismatch should be fine.\n    return resMgrDelegate.getDelegationToken(renewer);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.YARNRunner.submitJob": "  public JobStatus submitJob(JobID jobId, String jobSubmitDir, Credentials ts)\n  throws IOException, InterruptedException {\n    \n    /* check if we have a hsproxy, if not, no need */\n    MRClientProtocol hsProxy = clientCache.getInitializedHSProxy();\n    if (hsProxy != null) {\n      // JobClient will set this flag if getDelegationToken is called, if so, get\n      // the delegation tokens for the HistoryServer also.\n      if (conf.getBoolean(JobClient.HS_DELEGATION_TOKEN_REQUIRED, \n          DEFAULT_HS_DELEGATION_TOKEN_REQUIRED)) {\n        Token hsDT = getDelegationTokenFromHS(hsProxy);\n        ts.addToken(hsDT.getService(), hsDT);\n      }\n    }\n\n    // Upload only in security mode: TODO\n    Path applicationTokensFile =\n        new Path(jobSubmitDir, MRJobConfig.APPLICATION_TOKENS_FILE);\n    try {\n      ts.writeTokenStorageFile(applicationTokensFile, conf);\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Construct necessary information to start the MR AM\n    ApplicationSubmissionContext appContext =\n      createApplicationSubmissionContext(conf, jobSubmitDir, ts);\n\n    // Submit to ResourceManager\n    ApplicationId applicationId = resMgrDelegate.submitApplication(appContext);\n\n    ApplicationReport appMaster = resMgrDelegate\n        .getApplicationReport(applicationId);\n    String diagnostics =\n        (appMaster == null ?\n            \"application report is null\" : appMaster.getDiagnostics());\n    if (appMaster == null || appMaster.getYarnApplicationState() == YarnApplicationState.FAILED\n        || appMaster.getYarnApplicationState() == YarnApplicationState.KILLED) {\n      throw new IOException(\"Failed to run job : \" +\n        diagnostics);\n    }\n    return clientCache.getClient(jobId).getJobStatus(jobId);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.YARNRunner.createApplicationSubmissionContext": "  public ApplicationSubmissionContext createApplicationSubmissionContext(\n      Configuration jobConf,\n      String jobSubmitDir, Credentials ts) throws IOException {\n    ApplicationId applicationId = resMgrDelegate.getApplicationId();\n\n    // Setup resource requirements\n    Resource capability = recordFactory.newRecordInstance(Resource.class);\n    capability.setMemory(\n        conf.getInt(\n            MRJobConfig.MR_AM_VMEM_MB, MRJobConfig.DEFAULT_MR_AM_VMEM_MB\n            )\n        );\n    capability.setVirtualCores(\n        conf.getInt(\n            MRJobConfig.MR_AM_CPU_VCORES, MRJobConfig.DEFAULT_MR_AM_CPU_VCORES\n            )\n        );\n    LOG.debug(\"AppMaster capability = \" + capability);\n\n    // Setup LocalResources\n    Map<String, LocalResource> localResources =\n        new HashMap<String, LocalResource>();\n\n    Path jobConfPath = new Path(jobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n\n    URL yarnUrlForJobSubmitDir = ConverterUtils\n        .getYarnUrlFromPath(defaultFileContext.getDefaultFileSystem()\n            .resolvePath(\n                defaultFileContext.makeQualified(new Path(jobSubmitDir))));\n    LOG.debug(\"Creating setup context, jobSubmitDir url is \"\n        + yarnUrlForJobSubmitDir);\n\n    localResources.put(MRJobConfig.JOB_CONF_FILE,\n        createApplicationResource(defaultFileContext,\n            jobConfPath, LocalResourceType.FILE));\n    if (jobConf.get(MRJobConfig.JAR) != null) {\n      Path jobJarPath = new Path(jobConf.get(MRJobConfig.JAR));\n      LocalResource rc = createApplicationResource(defaultFileContext,\n          jobJarPath, \n          LocalResourceType.PATTERN);\n      String pattern = conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n          JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n      rc.setPattern(pattern);\n      localResources.put(MRJobConfig.JOB_JAR, rc);\n    } else {\n      // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n      // mapreduce jar itself which is already on the classpath.\n      LOG.info(\"Job jar is not present. \"\n          + \"Not adding any jar to the list of resources.\");\n    }\n\n    // TODO gross hack\n    for (String s : new String[] {\n        MRJobConfig.JOB_SPLIT,\n        MRJobConfig.JOB_SPLIT_METAINFO,\n        MRJobConfig.APPLICATION_TOKENS_FILE }) {\n      localResources.put(\n          MRJobConfig.JOB_SUBMIT_DIR + \"/\" + s,\n          createApplicationResource(defaultFileContext,\n              new Path(jobSubmitDir, s), LocalResourceType.FILE));\n    }\n\n    // Setup security tokens\n    DataOutputBuffer dob = new DataOutputBuffer();\n    ts.writeTokenStorageToStream(dob);\n    ByteBuffer securityTokens  = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());\n\n    // Setup the command to run the AM\n    List<String> vargs = new ArrayList<String>(8);\n    vargs.add(Environment.JAVA_HOME.$() + \"/bin/java\");\n\n    // TODO: why do we use 'conf' some places and 'jobConf' others?\n    long logSize = TaskLog.getTaskLogLength(new JobConf(conf));\n    String logLevel = jobConf.get(\n        MRJobConfig.MR_AM_LOG_LEVEL, MRJobConfig.DEFAULT_MR_AM_LOG_LEVEL);\n    MRApps.addLog4jSystemProperties(logLevel, logSize, vargs);\n\n    // Check for Java Lib Path usage in MAP and REDUCE configs\n    warnForJavaLibPath(conf.get(MRJobConfig.MAP_JAVA_OPTS,\"\"), \"map\", \n        MRJobConfig.MAP_JAVA_OPTS, MRJobConfig.MAP_ENV);\n    warnForJavaLibPath(conf.get(MRJobConfig.MAPRED_MAP_ADMIN_JAVA_OPTS,\"\"), \"map\", \n        MRJobConfig.MAPRED_MAP_ADMIN_JAVA_OPTS, MRJobConfig.MAPRED_ADMIN_USER_ENV);\n    warnForJavaLibPath(conf.get(MRJobConfig.REDUCE_JAVA_OPTS,\"\"), \"reduce\", \n        MRJobConfig.REDUCE_JAVA_OPTS, MRJobConfig.REDUCE_ENV);\n    warnForJavaLibPath(conf.get(MRJobConfig.MAPRED_REDUCE_ADMIN_JAVA_OPTS,\"\"), \"reduce\", \n        MRJobConfig.MAPRED_REDUCE_ADMIN_JAVA_OPTS, MRJobConfig.MAPRED_ADMIN_USER_ENV);   \n\n    // Add AM admin command opts before user command opts\n    // so that it can be overridden by user\n    String mrAppMasterAdminOptions = conf.get(MRJobConfig.MR_AM_ADMIN_COMMAND_OPTS,\n        MRJobConfig.DEFAULT_MR_AM_ADMIN_COMMAND_OPTS);\n    warnForJavaLibPath(mrAppMasterAdminOptions, \"app master\", \n        MRJobConfig.MR_AM_ADMIN_COMMAND_OPTS, MRJobConfig.MR_AM_ADMIN_USER_ENV);\n    vargs.add(mrAppMasterAdminOptions);\n    \n    // Add AM user command opts\n    String mrAppMasterUserOptions = conf.get(MRJobConfig.MR_AM_COMMAND_OPTS,\n        MRJobConfig.DEFAULT_MR_AM_COMMAND_OPTS);\n    warnForJavaLibPath(mrAppMasterUserOptions, \"app master\", \n        MRJobConfig.MR_AM_COMMAND_OPTS, MRJobConfig.MR_AM_ENV);\n    vargs.add(mrAppMasterUserOptions);\n    \n    vargs.add(MRJobConfig.APPLICATION_MASTER_CLASS);\n    vargs.add(\"1>\" + ApplicationConstants.LOG_DIR_EXPANSION_VAR +\n        Path.SEPARATOR + ApplicationConstants.STDOUT);\n    vargs.add(\"2>\" + ApplicationConstants.LOG_DIR_EXPANSION_VAR +\n        Path.SEPARATOR + ApplicationConstants.STDERR);\n\n\n    Vector<String> vargsFinal = new Vector<String>(8);\n    // Final command\n    StringBuilder mergedCommand = new StringBuilder();\n    for (CharSequence str : vargs) {\n      mergedCommand.append(str).append(\" \");\n    }\n    vargsFinal.add(mergedCommand.toString());\n\n    LOG.debug(\"Command to launch container for ApplicationMaster is : \"\n        + mergedCommand);\n\n    // Setup the CLASSPATH in environment\n    // i.e. add { Hadoop jars, job jar, CWD } to classpath.\n    Map<String, String> environment = new HashMap<String, String>();\n    MRApps.setClasspath(environment, conf);\n    \n    // Setup the environment variables for Admin first\n    MRApps.setEnvFromInputString(environment, \n        conf.get(MRJobConfig.MR_AM_ADMIN_USER_ENV));\n    // Setup the environment variables (LD_LIBRARY_PATH, etc)\n    MRApps.setEnvFromInputString(environment, \n        conf.get(MRJobConfig.MR_AM_ENV));\n\n    // Parse distributed cache\n    MRApps.setupDistributedCache(jobConf, localResources);\n\n    Map<ApplicationAccessType, String> acls\n        = new HashMap<ApplicationAccessType, String>(2);\n    acls.put(ApplicationAccessType.VIEW_APP, jobConf.get(\n        MRJobConfig.JOB_ACL_VIEW_JOB, MRJobConfig.DEFAULT_JOB_ACL_VIEW_JOB));\n    acls.put(ApplicationAccessType.MODIFY_APP, jobConf.get(\n        MRJobConfig.JOB_ACL_MODIFY_JOB,\n        MRJobConfig.DEFAULT_JOB_ACL_MODIFY_JOB));\n\n    // Setup ContainerLaunchContext for AM container\n    ContainerLaunchContext amContainer = BuilderUtils\n        .newContainerLaunchContext(null, UserGroupInformation\n            .getCurrentUser().getShortUserName(), capability, localResources,\n            environment, vargsFinal, null, securityTokens, acls);\n\n    // Set up the ApplicationSubmissionContext\n    ApplicationSubmissionContext appContext =\n        recordFactory.newRecordInstance(ApplicationSubmissionContext.class);\n    appContext.setApplicationId(applicationId);                // ApplicationId\n    appContext.setUser(                                        // User name\n        UserGroupInformation.getCurrentUser().getShortUserName());\n    appContext.setQueue(                                       // Queue name\n        jobConf.get(JobContext.QUEUE_NAME,\n        YarnConfiguration.DEFAULT_QUEUE_NAME));\n    appContext.setApplicationName(                             // Job name\n        jobConf.get(JobContext.JOB_NAME,\n        YarnConfiguration.DEFAULT_APPLICATION_NAME));\n    appContext.setCancelTokensWhenComplete(\n        conf.getBoolean(MRJobConfig.JOB_CANCEL_DELEGATION_TOKEN, true));\n    appContext.setAMContainerSpec(amContainer);         // AM Container\n\n    return appContext;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.YARNRunner.getJobStatus": "  public JobStatus getJobStatus(JobID jobID) throws IOException,\n      InterruptedException {\n    JobStatus status = clientCache.getClient(jobID).getJobStatus(jobID);\n    return status;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    //validate the jobs output specs \n    checkSpecs(job);\n    \n    Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, \n                                                     job.getConfiguration());\n    //configure the command line options correctly on the submitting dfs\n    Configuration conf = job.getConfiguration();\n    InetAddress ip = InetAddress.getLocalHost();\n    if (ip != null) {\n      submitHostAddress = ip.getHostAddress();\n      submitHostName = ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId = submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir = new Path(jobStagingArea, jobId.toString());\n    JobStatus status = null;\n    try {\n      conf.set(\"hadoop.http.filter.initializers\", \n          \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n      conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      copyAndConfigureFiles(job, submitJobDir);\n      Path submitJobFile = JobSubmissionFiles.getJobConfPath(submitJobDir);\n      \n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps = writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue = conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl = submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // removing jobtoken referrals before copying the jobconf to HDFS\n      // as the tasks don't need this setting, actually they may break\n      // because of it if present as the referral will point to a\n      // different job.\n      TokenCache.cleanUpTokenReferral(conf);\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status = submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status != null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status == null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs != null && submitJobDir != null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmitter.writeConf": "  private void writeConf(Configuration conf, Path jobFile) \n      throws IOException {\n    // Write job file to JobTracker's fs        \n    FSDataOutputStream out = \n      FileSystem.create(jtFs, jobFile, \n                        new FsPermission(JobSubmissionFiles.JOB_FILE_PERMISSION));\n    try {\n      conf.writeXml(out);\n    } finally {\n      out.close();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles": "  private void copyAndConfigureFiles(Job job, Path jobSubmitDir) \n  throws IOException {\n    Configuration conf = job.getConfiguration();\n    short replication = (short)conf.getInt(Job.SUBMIT_REPLICATION, 10);\n    copyAndConfigureFiles(job, jobSubmitDir, replication);\n\n    // Set the working directory\n    if (job.getWorkingDirectory() == null) {\n      job.setWorkingDirectory(jtFs.getWorkingDirectory());          \n    }\n\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmitter.printTokens": "  private void printTokens(JobID jobId,\n      Credentials credentials) throws IOException {\n    LOG.info(\"Submitting tokens for job: \" + jobId);\n    for (Token<?> token: credentials.getAllTokens()) {\n      LOG.info(token);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmitter.writeSplits": "  private int writeSplits(org.apache.hadoop.mapreduce.JobContext job,\n      Path jobSubmitDir) throws IOException,\n      InterruptedException, ClassNotFoundException {\n    JobConf jConf = (JobConf)job.getConfiguration();\n    int maps;\n    if (jConf.getUseNewMapper()) {\n      maps = writeNewSplits(job, jobSubmitDir);\n    } else {\n      maps = writeOldSplits(jConf, jobSubmitDir);\n    }\n    return maps;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs": "  private void checkSpecs(Job job) throws ClassNotFoundException, \n      InterruptedException, IOException {\n    JobConf jConf = (JobConf)job.getConfiguration();\n    // Check the output specification\n    if (jConf.getNumReduceTasks() == 0 ? \n        jConf.getUseNewMapper() : jConf.getUseNewReducer()) {\n      org.apache.hadoop.mapreduce.OutputFormat<?, ?> output =\n        ReflectionUtils.newInstance(job.getOutputFormatClass(),\n          job.getConfiguration());\n      output.checkOutputSpecs(job);\n    } else {\n      jConf.getOutputFormat().checkOutputSpecs(jtFs, jConf);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmitter.populateTokenCache": "  private void populateTokenCache(Configuration conf, Credentials credentials) \n  throws IOException{\n    readTokensFromFiles(conf, credentials);\n    // add the delegation tokens from configuration\n    String [] nameNodes = conf.getStrings(MRJobConfig.JOB_NAMENODES);\n    LOG.debug(\"adding the following namenodes' delegation tokens:\" + \n        Arrays.toString(nameNodes));\n    if(nameNodes != null) {\n      Path [] ps = new Path[nameNodes.length];\n      for(int i=0; i< nameNodes.length; i++) {\n        ps[i] = new Path(nameNodes[i]);\n      }\n      TokenCache.obtainTokensForNamenodes(credentials, ps, conf);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      LOG.error(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobClient.run": "      public Token<DelegationTokenIdentifier> run() throws IOException, \n      InterruptedException {\n        return cluster.getDelegationToken(renewer);\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobClient.arrayToStringList": "  private  Collection<String> arrayToStringList(TaskTrackerInfo[] objs) {\n    Collection<String> list = new ArrayList<String>();\n    for (TaskTrackerInfo info: objs) {\n      list.add(info.getTaskTrackerName());\n    }\n    return list;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobClient.getClusterStatus": "  public ClusterStatus getClusterStatus(boolean detailed) throws IOException {\n    try {\n      return clientUgi.doAs(new PrivilegedExceptionAction<ClusterStatus>() {\n        public ClusterStatus run() throws IOException, InterruptedException {\n        ClusterMetrics metrics = cluster.getClusterStatus();\n        return new ClusterStatus(arrayToStringList(cluster.getActiveTaskTrackers()),\n          arrayToBlackListInfo(cluster.getBlackListedTaskTrackers()),\n          cluster.getTaskTrackerExpiryInterval(), metrics.getOccupiedMapSlots(),\n          metrics.getOccupiedReduceSlots(), metrics.getMapSlotCapacity(),\n          metrics.getReduceSlotCapacity(), \n          cluster.getJobTrackerStatus());\n        }\n      });\n    } catch (InterruptedException ie) {\n      throw new IOException(ie);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobClient.getJob": "  public RunningJob getJob(String jobid) throws IOException {\n    return getJob(JobID.forName(jobid));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobClient.getChildQueues": "  public JobQueueInfo[] getChildQueues(final String queueName) throws IOException {\n    try {\n      return clientUgi.doAs(new PrivilegedExceptionAction<JobQueueInfo[]>() {\n        public JobQueueInfo[] run() throws IOException, InterruptedException {\n          return getJobQueueInfoArray(cluster.getChildQueues(queueName));\n        }\n      });\n    } catch (InterruptedException ie) {\n      throw new IOException(ie);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobClient.getJobQueueInfoArray": "  private JobQueueInfo[] getJobQueueInfoArray(QueueInfo[] queues)\n      throws IOException {\n    JobQueueInfo[] ret = new JobQueueInfo[queues.length];\n    for (int i = 0; i < queues.length; i++) {\n      ret[i] = getJobQueueInfo(queues[i]);\n    }\n    return ret;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobClient.getRootQueues": "  public JobQueueInfo[] getRootQueues() throws IOException {\n    try {\n      return clientUgi.doAs(new PrivilegedExceptionAction<JobQueueInfo[]>() {\n        public JobQueueInfo[] run() throws IOException, InterruptedException {\n          return getJobQueueInfoArray(cluster.getRootQueues());\n        }\n      });\n    } catch (InterruptedException ie) {\n      throw new IOException(ie);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobClient.arrayToBlackListInfo": "  private  Collection<BlackListInfo> arrayToBlackListInfo(TaskTrackerInfo[] objs) {\n    Collection<BlackListInfo> list = new ArrayList<BlackListInfo>();\n    for (TaskTrackerInfo info: objs) {\n      BlackListInfo binfo = new BlackListInfo();\n      binfo.setTrackerName(info.getTaskTrackerName());\n      binfo.setReasonForBlackListing(info.getReasonForBlacklist());\n      binfo.setBlackListReport(info.getBlacklistReport());\n      list.add(binfo);\n    }\n    return list;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobClient.getDelegationToken": "  public Token<DelegationTokenIdentifier> \n    getDelegationToken(final Text renewer) throws IOException, InterruptedException {\n    getDelegationTokenCalled = true;\n    return clientUgi.doAs(new \n        PrivilegedExceptionAction<Token<DelegationTokenIdentifier>>() {\n      public Token<DelegationTokenIdentifier> run() throws IOException, \n      InterruptedException {\n        return cluster.getDelegationToken(renewer);\n      }\n    });\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobClient.getSystemDir": "  public Path getSystemDir() {\n    try {\n      return clientUgi.doAs(new PrivilegedExceptionAction<Path>() {\n        @Override\n        public Path run() throws IOException, InterruptedException {\n          return cluster.getSystemDir();\n        }\n      });\n      } catch (IOException ioe) {\n      return null;\n    } catch (InterruptedException ie) {\n      return null;\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobClient.getQueues": "  public JobQueueInfo[] getQueues() throws IOException {\n    try {\n      return clientUgi.doAs(new PrivilegedExceptionAction<JobQueueInfo[]>() {\n        public JobQueueInfo[] run() throws IOException, InterruptedException {\n          return getJobQueueInfoArray(cluster.getQueues());\n        }\n      });\n    } catch (InterruptedException ie) {\n      throw new IOException(ie);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobClient.getQueueAclsForCurrentUser": "  public QueueAclsInfo[] getQueueAclsForCurrentUser() throws IOException {\n    try {\n      org.apache.hadoop.mapreduce.QueueAclsInfo[] acls = \n        clientUgi.doAs(new \n            PrivilegedExceptionAction\n            <org.apache.hadoop.mapreduce.QueueAclsInfo[]>() {\n              public org.apache.hadoop.mapreduce.QueueAclsInfo[] run() \n              throws IOException, InterruptedException {\n                return cluster.getQueueAclsForCurrentUser();\n              }\n        });\n      QueueAclsInfo[] ret = new QueueAclsInfo[acls.length];\n      for (int i = 0 ; i < acls.length; i++ ) {\n        ret[i] = QueueAclsInfo.downgrade(acls[i]);\n      }\n      return ret;\n    } catch (InterruptedException ie) {\n      throw new IOException(ie);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobClient.submitJob": "  public RunningJob submitJob(final JobConf conf) throws FileNotFoundException,\n                                                  IOException {\n    try {\n      conf.setBooleanIfUnset(\"mapred.mapper.new-api\", false);\n      conf.setBooleanIfUnset(\"mapred.reducer.new-api\", false);\n      if (getDelegationTokenCalled) {\n        conf.setBoolean(HS_DELEGATION_TOKEN_REQUIRED, getDelegationTokenCalled);\n        getDelegationTokenCalled = false;\n      }\n      Job job = clientUgi.doAs(new PrivilegedExceptionAction<Job> () {\n        @Override\n        public Job run() throws IOException, ClassNotFoundException, \n          InterruptedException {\n          Job job = Job.getInstance(conf);\n          job.submit();\n          return job;\n        }\n      });\n      // update our Cluster instance with the one created by Job for submission\n      // (we can't pass our Cluster instance to Job, since Job wraps the config\n      // instance, and the two configs would then diverge)\n      cluster = job.getCluster();\n      return new NetworkedJob(job);\n    } catch (InterruptedException ie) {\n      throw new IOException(\"interrupted\", ie);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Master.getMasterPrincipal": "  public static String getMasterPrincipal(Configuration conf) \n  throws IOException {\n    String masterHostname = getMasterAddress(conf).getHostName();\n    // get kerberos principal for use as delegation token renewer\n    return SecurityUtil.getServerPrincipal(getMasterUserName(conf), masterHostname);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Master.getMasterUserName": "  public static String getMasterUserName(Configuration conf) {\n    String framework = conf.get(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\n    if (framework.equals(MRConfig.CLASSIC_FRAMEWORK_NAME)) {    \n      return conf.get(MRConfig.MASTER_USER_NAME);\n    } \n    else {\n      return conf.get(YarnConfiguration.RM_PRINCIPAL);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Master.getMasterAddress": "  public static InetSocketAddress getMasterAddress(Configuration conf) {\n    String masterAddress;\n    String framework = conf.get(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\n    if (framework.equals(MRConfig.CLASSIC_FRAMEWORK_NAME)) {\n      masterAddress = conf.get(MRConfig.MASTER_ADDRESS, \"localhost:8012\");\n      return NetUtils.createSocketAddr(masterAddress, 8012, MRConfig.MASTER_ADDRESS);\n    } \n    else {\n      return conf.getSocketAddr(\n          YarnConfiguration.RM_ADDRESS,\n          YarnConfiguration.DEFAULT_RM_ADDRESS,\n          YarnConfiguration.DEFAULT_RM_PORT);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.clientCache.getInitializedHSProxy": "  protected synchronized MRClientProtocol getInitializedHSProxy()\n      throws IOException {\n    if (this.hsProxy == null) {\n      hsProxy = instantiateHistoryProxy();\n    }\n    return this.hsProxy;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.clientCache.instantiateHistoryProxy": "  protected MRClientProtocol instantiateHistoryProxy()\n      throws IOException {\n    final String serviceAddr = conf.get(JHAdminConfig.MR_HISTORY_ADDRESS);\n    if (StringUtils.isEmpty(serviceAddr)) {\n      return null;\n    }\n    LOG.debug(\"Connecting to HistoryServer at: \" + serviceAddr);\n    final YarnRPC rpc = YarnRPC.create(conf);\n    LOG.debug(\"Connected to HistoryServer at: \" + serviceAddr);\n    UserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\n    return currentUser.doAs(new PrivilegedAction<MRClientProtocol>() {\n      @Override\n      public MRClientProtocol run() {\n        return (MRClientProtocol) rpc.getProxy(HSClientProtocol.class,\n            NetUtils.createSocketAddr(serviceAddr), conf);\n      }\n    });\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.main.java.org.apache.hadoop.mapred.clientCache.getClient": "  public synchronized ClientServiceDelegate getClient(JobID jobId) {\n    if (hsProxy == null) {\n      try {\n        hsProxy = instantiateHistoryProxy();\n      } catch (IOException e) {\n        LOG.warn(\"Could not connect to History server.\", e);\n        throw new YarnException(\"Could not connect to History server.\", e);\n      }\n    }\n    ClientServiceDelegate client = cache.get(jobId);\n    if (client == null) {\n      client = new ClientServiceDelegate(conf, rm, jobId, hsProxy);\n      cache.put(jobId, client);\n    }\n    return client;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmissionFiles.getJobConfPath": "  public static Path getJobConfPath(Path jobSubmitDir) {\n    return new Path(jobSubmitDir, \"job.xml\");\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobID.toString": "  public String toString() {\n    return appendTo(new StringBuilder(JOB)).toString();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobID.appendTo": "  public StringBuilder appendTo(StringBuilder builder) {\n    builder.append(SEPARATOR);\n    builder.append(jtIdentifier);\n    builder.append(SEPARATOR);\n    builder.append(idFormat.format(id));\n    return builder;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir": "  public static Path getStagingDir(Cluster cluster, Configuration conf) \n  throws IOException,InterruptedException {\n    Path stagingArea = cluster.getStagingAreaDir();\n    FileSystem fs = stagingArea.getFileSystem(conf);\n    String realUser;\n    String currentUser;\n    UserGroupInformation ugi = UserGroupInformation.getLoginUser();\n    realUser = ugi.getShortUserName();\n    currentUser = UserGroupInformation.getCurrentUser().getShortUserName();\n    if (fs.exists(stagingArea)) {\n      FileStatus fsStatus = fs.getFileStatus(stagingArea);\n      String owner = fsStatus.getOwner();\n      if (!(owner.equals(currentUser) || owner.equals(realUser))) {\n         throw new IOException(\"The ownership on the staging directory \" +\n                      stagingArea + \" is not as expected. \" +\n                      \"It is owned by \" + owner + \". The directory must \" +\n                      \"be owned by the submitter \" + currentUser + \" or \" +\n                      \"by \" + realUser);\n      }\n      if (!fsStatus.getPermission().equals(JOB_DIR_PERMISSION)) {\n        LOG.info(\"Permissions on staging directory \" + stagingArea + \" are \" +\n          \"incorrect: \" + fsStatus.getPermission() + \". Fixing permissions \" +\n          \"to correct value \" + JOB_DIR_PERMISSION);\n        fs.setPermission(stagingArea, JOB_DIR_PERMISSION);\n      }\n    } else {\n      fs.mkdirs(stagingArea, \n          new FsPermission(JOB_DIR_PERMISSION));\n    }\n    return stagingArea;\n  }"
        },
        "bug_report": {
            "Title": "MR Client gets an renewer token exception while Oozie is submitting a job",
            "Description": "After the fix for HADOOP-9299 I'm now getting the following bizzare exception in Oozie while trying to submit a job. This also seems to be KRB related:\n\n{noformat}\n2013-03-15 13:34:16,555  WARN ActionStartXCommand:542 - USER[hue] GROUP[-] TOKEN[] APP[MapReduce] JOB[0000001-130315123130987-oozie-oozi-W] ACTION[0000001-130315123130987-oozie-oozi-W@Sleep] Error starting action [Sleep]. ErrorType [ERROR], ErrorCode [UninitializedMessageException], Message [UninitializedMessageException: Message missing required fields: renewer]\norg.apache.oozie.action.ActionExecutorException: UninitializedMessageException: Message missing required fields: renewer\n\tat org.apache.oozie.action.ActionExecutor.convertException(ActionExecutor.java:401)\n\tat org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:738)\n\tat org.apache.oozie.action.hadoop.JavaActionExecutor.start(JavaActionExecutor.java:889)\n\tat org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:211)\n\tat org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:59)\n\tat org.apache.oozie.command.XCommand.call(XCommand.java:277)\n\tat org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:326)\n\tat org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:255)\n\tat org.apache.oozie.service.CallableQueueService$CallableWrapper.run(CallableQueueService.java:175)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:662)\nCaused by: com.google.protobuf.UninitializedMessageException: Message missing required fields: renewer\n\tat com.google.protobuf.AbstractMessage$Builder.newUninitializedMessageException(AbstractMessage.java:605)\n\tat org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenRequestProto$Builder.build(SecurityProtos.java:973)\n\tat org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.mergeLocalToProto(GetDelegationTokenRequestPBImpl.java:84)\n\tat org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.getProto(GetDelegationTokenRequestPBImpl.java:67)\n\tat org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getDelegationToken(MRClientProtocolPBClientImpl.java:200)\n\tat org.apache.hadoop.mapred.YARNRunner.getDelegationTokenFromHS(YARNRunner.java:194)\n\tat org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:273)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:392)\n\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1218)\n\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1215)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1439)\n\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1215)\n\tat org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:581)\n\tat org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:576)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1439)\n\tat org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:576)\n\tat org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:723)\n\t... 10 more\n2013-03-15 13:34:16,555  WARN ActionStartXCommand:542 - USER[hue] GROUP[-] TOKEN[] APP[MapReduce] JOB[0000001-13031512313\n{noformat}"
        }
    }
]