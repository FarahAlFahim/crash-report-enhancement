[
    {
        "filename": "HADOOP-6989.json",
        "creation_time": "2010-10-04T23:55:16.000+0000",
        "stack_trace": "```\nCaused by: \njava.lang.IllegalArgumentException: key class or comparator option must be set\n\tat org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:247)\n\tat org.apache.hadoop.io.SetFile$Writer.<init>(SetFile.java:60)\n\tat org.apache.hadoop.io.TestSetFile.writeTest(TestSetFile.java:73)\n\tat org.apache.hadoop.io.TestSetFile.testSetFile(TestSetFile.java:45)\n```",
        "source_code": {},
        "bug_report": {
            "Title": "TestSetFile is failing on trunk",
            "Description": "Testsuite: org.apache.hadoop.io.TestSetFile\nTests run: 1, Failures: 0, Errors: 1, Time elapsed: 1.015 sec\n------------- Standard Output ---------------\n2010-10-04 16:32:01,030 INFO  io.TestSetFile (TestSetFile.java:generate(56)) - generating 10000 records in memory\n2010-10-04 16:32:01,249 INFO  io.TestSetFile (TestSetFile.java:generate(63)) - sorting 10000 records\n2010-10-04 16:32:01,350 INFO  io.TestSetFile (TestSetFile.java:writeTest(72)) - creating with 10000 records\n------------- ---------------- ---------------\n\nTestcase: testSetFile took 0.964 sec\n\tCaused an ERROR\nkey class or comparator option must be set\njava.lang.IllegalArgumentException: key class or comparator option must be set\n\tat org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:247)\n\tat org.apache.hadoop.io.SetFile$Writer.<init>(SetFile.java:60)\n\tat org.apache.hadoop.io.TestSetFile.writeTest(TestSetFile.java:73)\n\tat org.apache.hadoop.io.TestSetFile.testSetFile(TestSetFile.java:45)"
        }
    },
    {
        "filename": "HADOOP-10823.json",
        "creation_time": "2014-07-15T07:20:00.000+0000",
        "stack_trace": "```\njunit.framework.AssertionFailedError: expected:<2> but was:<1>\n\tat junit.framework.Assert.fail(Assert.java:50)\n\tat junit.framework.Assert.failNotEquals(Assert.java:287)\n\tat junit.framework.Assert.assertEquals(Assert.java:67)\n\tat junit.framework.Assert.assertEquals(Assert.java:199)\n\tat junit.framework.Assert.assertEquals(Assert.java:205)\n\tat org.apache.hadoop.security.ssl.TestReloadingX509TrustManager.testReload(TestReloadingX509TrustManager.java:112)\n\njava.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:375)\n\tat sun.security.provider.JavaKeyStore.engineLoad(JavaKeyStore.java:628)\n\tat sun.security.provider.JavaKeyStore$JKS.engineLoad(JavaKeyStore.java:38)\n\tat java.security.KeyStore.load(KeyStore.java:1185)\n\tat org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager(ReloadingX509TrustManager.java:166)\n\tat org.apache.hadoop.security.ssl.ReloadingX509TrustManager.run(ReloadingX509TrustManager.java:195)\n\tat java.lang.Thread.run(Thread.java:662)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager": "  X509TrustManager loadTrustManager()\n  throws IOException, GeneralSecurityException {\n    X509TrustManager trustManager = null;\n    KeyStore ks = KeyStore.getInstance(type);\n    lastLoaded = file.lastModified();\n    FileInputStream in = new FileInputStream(file);\n    try {\n      ks.load(in, password.toCharArray());\n      LOG.debug(\"Loaded truststore '\" + file + \"'\");\n    } finally {\n      in.close();\n    }\n\n    TrustManagerFactory trustManagerFactory = \n      TrustManagerFactory.getInstance(SSLFactory.SSLCERTIFICATE);\n    trustManagerFactory.init(ks);\n    TrustManager[] trustManagers = trustManagerFactory.getTrustManagers();\n    for (TrustManager trustManager1 : trustManagers) {\n      if (trustManager1 instanceof X509TrustManager) {\n        trustManager = (X509TrustManager) trustManager1;\n        break;\n      }\n    }\n    return trustManager;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ssl.ReloadingX509TrustManager.init": "  public void init() {\n    reloader = new Thread(this, \"Truststore reloader thread\");\n    reloader.setDaemon(true);\n    running =  true;\n    reloader.start();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ssl.ReloadingX509TrustManager.run": "  public void run() {\n    while (running) {\n      try {\n        Thread.sleep(reloadInterval);\n      } catch (InterruptedException e) {\n        //NOP\n      }\n      if (running && needsReload()) {\n        try {\n          trustManagerRef.set(loadTrustManager());\n        } catch (Exception ex) {\n          LOG.warn(\"Could not load truststore (keep using existing one) : \" +\n                   ex.toString(), ex);\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ssl.ReloadingX509TrustManager.needsReload": "  boolean needsReload() {\n    boolean reload = true;\n    if (file.exists()) {\n      if (file.lastModified() == lastLoaded) {\n        reload = false;\n      }\n    } else {\n      lastLoaded = 0;\n    }\n    return reload;\n  }"
        },
        "bug_report": {
            "Title": "TestReloadingX509TrustManager is flaky",
            "Description": "Pasting the log\n{quote}\nError Message\n\nexpected:<2> but was:<1>\nStacktrace\n\njunit.framework.AssertionFailedError: expected:<2> but was:<1>\n\tat junit.framework.Assert.fail(Assert.java:50)\n\tat junit.framework.Assert.failNotEquals(Assert.java:287)\n\tat junit.framework.Assert.assertEquals(Assert.java:67)\n\tat junit.framework.Assert.assertEquals(Assert.java:199)\n\tat junit.framework.Assert.assertEquals(Assert.java:205)\n\tat org.apache.hadoop.security.ssl.TestReloadingX509TrustManager.testReload(TestReloadingX509TrustManager.java:112)\nStandard Output\n\n2014-07-06 06:12:21,170 WARN  ssl.ReloadingX509TrustManager (ReloadingX509TrustManager.java:run(197)) - Could not load truststore (keep using existing one) : java.io.EOFException\njava.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:375)\n\tat sun.security.provider.JavaKeyStore.engineLoad(JavaKeyStore.java:628)\n\tat sun.security.provider.JavaKeyStore$JKS.engineLoad(JavaKeyStore.java:38)\n\tat java.security.KeyStore.load(KeyStore.java:1185)\n\tat org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager(ReloadingX509TrustManager.java:166)\n\tat org.apache.hadoop.security.ssl.ReloadingX509TrustManager.run(ReloadingX509TrustManager.java:195)\n\tat java.lang.Thread.run(Thread.java:662)\n{quote}"
        }
    },
    {
        "filename": "HADOOP-9125.json",
        "creation_time": "2012-12-10T02:07:52.000+0000",
        "stack_trace": "```\njavax.naming.CommunicationException: connection closed [Root exception is java.io.IOException: connection closed]; remaining name 'CN=Users,DC=EXAMPLE,DC=COM'\n        at com.sun.jndi.ldap.LdapCtx.doSearch(LdapCtx.java:1983)\n        at com.sun.jndi.ldap.LdapCtx.searchAux(LdapCtx.java:1827)\n        at com.sun.jndi.ldap.LdapCtx.c_search(LdapCtx.java:1752)\n        at com.sun.jndi.ldap.LdapCtx.c_search(LdapCtx.java:1769)\n        at com.sun.jndi.toolkit.ctx.ComponentDirContext.p_search(ComponentDirContext.java:394)\n        at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:376)\n        at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:358)\n        at javax.naming.directory.InitialDirContext.search(InitialDirContext.java:267)\n        at org.apache.hadoop.security.LdapGroupsMapping.getGroups(LdapGroupsMapping.java:187)\n        at org.apache.hadoop.security.CompositeGroupsMapping.getGroups(CompositeGroupsMapping.java:97)\n        at org.apache.hadoop.security.Groups.doGetGroups(Groups.java:103)\n        at org.apache.hadoop.security.Groups.getGroups(Groups.java:70)\n        at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1035)\n        at org.apache.hadoop.hbase.security.User.getGroupNames(User.java:90)\n        at org.apache.hadoop.hbase.security.access.TableAuthManager.authorize(TableAuthManager.java:355)\n        at org.apache.hadoop.hbase.security.access.AccessController.requirePermission(AccessController.java:379)\n        at org.apache.hadoop.hbase.security.access.AccessController.getUserPermissions(AccessController.java:1051)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.hbase.regionserver.HRegion.exec(HRegion.java:4914)\n        at org.apache.hadoop.hbase.regionserver.HRegionServer.execCoprocessor(HRegionServer.java:3546)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.hbase.ipc.SecureRpcEngine$Server.call(SecureRpcEngine.java:372)\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1399)\nCaused by: java.io.IOException: connection closed\n        at com.sun.jndi.ldap.LdapClient.ensureOpen(LdapClient.java:1558)\n        at com.sun.jndi.ldap.LdapClient.search(LdapClient.java:503)\n        at com.sun.jndi.ldap.LdapCtx.doSearch(LdapCtx.java:1965)\n        ... 28 more\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.LdapGroupsMapping.getGroups": "  public synchronized List<String> getGroups(String user) throws IOException {\n    List<String> groups = new ArrayList<String>();\n\n    try {\n      DirContext ctx = getDirContext();\n\n      // Search for the user. We'll only ever need to look at the first result\n      NamingEnumeration<SearchResult> results = ctx.search(baseDN,\n                                                           userSearchFilter,\n                                                           new Object[]{user},\n                                                           SEARCH_CONTROLS);\n      if (results.hasMoreElements()) {\n        SearchResult result = results.nextElement();\n        String userDn = result.getNameInNamespace();\n\n        NamingEnumeration<SearchResult> groupResults =\n          ctx.search(baseDN,\n                     \"(&\" + groupSearchFilter + \"(\" + groupMemberAttr + \"={0}))\",\n                     new Object[]{userDn},\n                     SEARCH_CONTROLS);\n        while (groupResults.hasMoreElements()) {\n          SearchResult groupResult = groupResults.nextElement();\n          Attribute groupName = groupResult.getAttributes().get(groupNameAttr);\n          groups.add(groupName.get().toString());\n        }\n      }\n    } catch (NamingException e) {\n      LOG.warn(\"Exception trying to get groups for user \" + user, e);\n      return new ArrayList<String>();\n    }\n\n    return groups;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.LdapGroupsMapping.getDirContext": "  DirContext getDirContext() throws NamingException {\n    if (ctx == null) {\n      // Set up the initial environment for LDAP connectivity\n      Hashtable<String, String> env = new Hashtable<String, String>();\n      env.put(Context.INITIAL_CONTEXT_FACTORY,\n          com.sun.jndi.ldap.LdapCtxFactory.class.getName());\n      env.put(Context.PROVIDER_URL, ldapUrl);\n      env.put(Context.SECURITY_AUTHENTICATION, \"simple\");\n\n      // Set up SSL security, if necessary\n      if (useSsl) {\n        env.put(Context.SECURITY_PROTOCOL, \"ssl\");\n        System.setProperty(\"javax.net.ssl.keyStore\", keystore);\n        System.setProperty(\"javax.net.ssl.keyStorePassword\", keystorePass);\n      }\n\n      env.put(Context.SECURITY_PRINCIPAL, bindUser);\n      env.put(Context.SECURITY_CREDENTIALS, bindPassword);\n\n      ctx = new InitialDirContext(env);\n    }\n\n    return ctx;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.Groups.getGroups": "    public List<String> getGroups() {\n      return groups;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.Groups.getTimestamp": "    public long getTimestamp() {\n      return timestamp;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getGroupNames": "  public synchronized String[] getGroupNames() {\n    ensureInitialized();\n    try {\n      List<String> result = groups.getGroups(getShortUserName());\n      return result.toArray(new String[result.size()]);\n    } catch (IOException ie) {\n      LOG.warn(\"No groups available for user \" + getShortUserName());\n      return new String[0];\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getShortUserName": "  public String getShortUserName() {\n    for (User p: subject.getPrincipals(User.class)) {\n      return p.getShortName();\n    }\n    return null;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.ensureInitialized": "  private static synchronized void ensureInitialized() {\n    if (!isInitialized) {\n        initialize(new Configuration(), KerberosName.hasRulesBeenSet());\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getGroups": "    public List<String> getGroups(String user) throws IOException {\n      List<String> result = userToGroupsMapping.get(user);\n      \n      if (result == null) {\n        result = underlyingImplementation.getGroups(user);\n      }\n\n      return result;\n    }"
        },
        "bug_report": {
            "Title": "LdapGroupsMapping threw CommunicationException after some idle time",
            "Description": "LdapGroupsMapping threw exception as below after some idle time. During the idle time no call to the group mapping provider should be made to repeat it.\n\n2012-12-07 02:20:59,738 WARN org.apache.hadoop.security.LdapGroupsMapping: Exception trying to get groups for user aduser2\njavax.naming.CommunicationException: connection closed [Root exception is java.io.IOException: connection closed]; remaining name 'CN=Users,DC=EXAMPLE,DC=COM'\n        at com.sun.jndi.ldap.LdapCtx.doSearch(LdapCtx.java:1983)\n        at com.sun.jndi.ldap.LdapCtx.searchAux(LdapCtx.java:1827)\n        at com.sun.jndi.ldap.LdapCtx.c_search(LdapCtx.java:1752)\n        at com.sun.jndi.ldap.LdapCtx.c_search(LdapCtx.java:1769)\n        at com.sun.jndi.toolkit.ctx.ComponentDirContext.p_search(ComponentDirContext.java:394)\n        at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:376)\n        at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:358)\n        at javax.naming.directory.InitialDirContext.search(InitialDirContext.java:267)\n        at org.apache.hadoop.security.LdapGroupsMapping.getGroups(LdapGroupsMapping.java:187)\n        at org.apache.hadoop.security.CompositeGroupsMapping.getGroups(CompositeGroupsMapping.java:97)\n        at org.apache.hadoop.security.Groups.doGetGroups(Groups.java:103)\n        at org.apache.hadoop.security.Groups.getGroups(Groups.java:70)\n        at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1035)\n        at org.apache.hadoop.hbase.security.User.getGroupNames(User.java:90)\n        at org.apache.hadoop.hbase.security.access.TableAuthManager.authorize(TableAuthManager.java:355)\n        at org.apache.hadoop.hbase.security.access.AccessController.requirePermission(AccessController.java:379)\n        at org.apache.hadoop.hbase.security.access.AccessController.getUserPermissions(AccessController.java:1051)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.hbase.regionserver.HRegion.exec(HRegion.java:4914)\n        at org.apache.hadoop.hbase.regionserver.HRegionServer.execCoprocessor(HRegionServer.java:3546)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.hbase.ipc.SecureRpcEngine$Server.call(SecureRpcEngine.java:372)\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1399)\nCaused by: java.io.IOException: connection closed\n        at com.sun.jndi.ldap.LdapClient.ensureOpen(LdapClient.java:1558)\n        at com.sun.jndi.ldap.LdapClient.search(LdapClient.java:503)\n        at com.sun.jndi.ldap.LdapCtx.doSearch(LdapCtx.java:1965)\n        ... 28 more\n2012-12-07 02:20:59,739 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user aduser2\n"
        }
    },
    {
        "filename": "HADOOP-10252.json",
        "creation_time": "2014-01-22T16:43:27.000+0000",
        "stack_trace": "```\njava.lang.IllegalArgumentException: Property value must not be null\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:92)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:958)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:940)\n\tat org.apache.hadoop.http.HttpServer.initializeWebServer(HttpServer.java:510)\n\tat org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:470)\n\tat org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:458)\n\tat org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:412)\n\tat org.apache.hadoop.hbase.util.InfoServer.<init>(InfoServer.java:59)\n\tat org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:584)\n\tat java.lang.Thread.run(Thread.java:722)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.set": "  public void set(String name, String value, String source) {\n    Preconditions.checkArgument(\n        name != null,\n        \"Property name must not be null\");\n    Preconditions.checkArgument(\n        value != null,\n        \"Property value must not be null\");\n    DeprecationContext deprecations = deprecationContext.get();\n    if (deprecations.getDeprecatedKeyMap().isEmpty()) {\n      getProps();\n    }\n    getOverlay().setProperty(name, value);\n    getProps().setProperty(name, value);\n    String newSource = (source == null ? \"programatically\" : source);\n\n    if (!isDeprecated(name)) {\n      updatingResource.put(name, new String[] {newSource});\n      String[] altNames = getAlternativeNames(name);\n      if(altNames != null) {\n        for(String n: altNames) {\n          if(!n.equals(name)) {\n            getOverlay().setProperty(n, value);\n            getProps().setProperty(n, value);\n            updatingResource.put(n, new String[] {newSource});\n          }\n        }\n      }\n    }\n    else {\n      String[] names = handleDeprecation(deprecationContext.get(), name);\n      String altSource = \"because \" + name + \" is deprecated\";\n      for(String n : names) {\n        getOverlay().setProperty(n, value);\n        getProps().setProperty(n, value);\n        updatingResource.put(n, new String[] {altSource});\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getOverlay": "  private synchronized Properties getOverlay() {\n    if (overlay==null){\n      overlay=new Properties();\n    }\n    return overlay;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.get": "  public String get(String name, String defaultValue) {\n    String[] names = handleDeprecation(deprecationContext.get(), name);\n    String result = null;\n    for(String n : names) {\n      result = substituteVars(getProps().getProperty(n, defaultValue));\n    }\n    return result;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getProps": "  protected synchronized Properties getProps() {\n    if (properties == null) {\n      properties = new Properties();\n      HashMap<String, String[]> backup = \n        new HashMap<String, String[]>(updatingResource);\n      loadResources(properties, resources, quietmode);\n      if (overlay!= null) {\n        properties.putAll(overlay);\n        for (Map.Entry<Object,Object> item: overlay.entrySet()) {\n          String key = (String)item.getKey();\n          updatingResource.put(key, backup.get(key));\n        }\n      }\n    }\n    return properties;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.isEmpty": "    public boolean isEmpty() {\n      return ranges == null || ranges.isEmpty();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.isDeprecated": "  public static boolean isDeprecated(String key) {\n    return deprecationContext.get().getDeprecatedKeyMap().containsKey(key);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.handleDeprecation": "  private void handleDeprecation() {\n    LOG.debug(\"Handling deprecation for all properties in config...\");\n    DeprecationContext deprecations = deprecationContext.get();\n    Set<Object> keys = new HashSet<Object>();\n    keys.addAll(getProps().keySet());\n    for (Object item: keys) {\n      LOG.debug(\"Handling deprecation for \" + (String)item);\n      handleDeprecation(deprecations, (String)item);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getAlternativeNames": "  private String[] getAlternativeNames(String name) {\n    String altNames[] = null;\n    DeprecatedKeyInfo keyInfo = null;\n    DeprecationContext cur = deprecationContext.get();\n    String depKey = cur.getReverseDeprecatedKeyMap().get(name);\n    if(depKey != null) {\n      keyInfo = cur.getDeprecatedKeyMap().get(depKey);\n      if(keyInfo.newKeys.length > 0) {\n        if(getProps().containsKey(depKey)) {\n          //if deprecated key is previously set explicitly\n          List<String> list = new ArrayList<String>();\n          list.addAll(Arrays.asList(keyInfo.newKeys));\n          list.add(depKey);\n          altNames = list.toArray(new String[list.size()]);\n        }\n        else {\n          altNames = keyInfo.newKeys;\n        }\n      }\n    }\n    return altNames;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getDeprecatedKeyMap": "    Map<String, DeprecatedKeyInfo> getDeprecatedKeyMap() {\n      return deprecatedKeyMap;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer.initializeWebServer": "  private void initializeWebServer(String name, String hostName,\n      Configuration conf, String[] pathSpecs)\n      throws FileNotFoundException, IOException {\n\n    Preconditions.checkNotNull(webAppContext);\n\n    int maxThreads = conf.getInt(HTTP_MAX_THREADS, -1);\n    // If HTTP_MAX_THREADS is not configured, QueueThreadPool() will use the\n    // default value (currently 250).\n    QueuedThreadPool threadPool = maxThreads == -1 ? new QueuedThreadPool()\n        : new QueuedThreadPool(maxThreads);\n    threadPool.setDaemon(true);\n    webServer.setThreadPool(threadPool);\n\n    ContextHandlerCollection contexts = new ContextHandlerCollection();\n    RequestLog requestLog = HttpRequestLog.getRequestLog(name);\n\n    if (requestLog != null) {\n      RequestLogHandler requestLogHandler = new RequestLogHandler();\n      requestLogHandler.setRequestLog(requestLog);\n      HandlerCollection handlers = new HandlerCollection();\n      handlers.setHandlers(new Handler[] { requestLogHandler, contexts });\n      webServer.setHandler(handlers);\n    } else {\n      webServer.setHandler(contexts);\n    }\n\n    final String appDir = getWebAppsPath(name);\n\n    webServer.addHandler(webAppContext);\n\n    addDefaultApps(contexts, appDir, conf);\n\n    addGlobalFilter(\"safety\", QuotingInputFilter.class.getName(), null);\n    final FilterInitializer[] initializers = getFilterInitializers(conf);\n    if (initializers != null) {\n      conf = new Configuration(conf);\n      conf.set(BIND_ADDRESS, hostName);\n      for (FilterInitializer c : initializers) {\n        c.initFilter(this, conf);\n      }\n    }\n\n    addDefaultServlets();\n\n    if (pathSpecs != null) {\n      for (String path : pathSpecs) {\n        LOG.info(\"adding path spec: \" + path);\n        addFilterPathMapping(path, webAppContext);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer.addDefaultApps": "  protected void addDefaultApps(ContextHandlerCollection parent,\n      final String appDir, Configuration conf) throws IOException {\n    // set up the context for \"/logs/\" if \"hadoop.log.dir\" property is defined. \n    String logDir = System.getProperty(\"hadoop.log.dir\");\n    if (logDir != null) {\n      Context logContext = new Context(parent, \"/logs\");\n      logContext.setResourceBase(logDir);\n      logContext.addServlet(AdminAuthorizedServlet.class, \"/*\");\n      if (conf.getBoolean(\n          CommonConfigurationKeys.HADOOP_JETTY_LOGS_SERVE_ALIASES,\n          CommonConfigurationKeys.DEFAULT_HADOOP_JETTY_LOGS_SERVE_ALIASES)) {\n        @SuppressWarnings(\"unchecked\")\n        Map<String, String> params = logContext.getInitParams();\n        params.put(\n            \"org.mortbay.jetty.servlet.Default.aliases\", \"true\");\n      }\n      logContext.setDisplayName(\"logs\");\n      setContextAttributes(logContext, conf);\n      addNoCacheFilter(webAppContext);\n      defaultContexts.put(logContext, true);\n    }\n    // set up the context for \"/static/*\"\n    Context staticContext = new Context(parent, \"/static\");\n    staticContext.setResourceBase(appDir + \"/static\");\n    staticContext.addServlet(DefaultServlet.class, \"/*\");\n    staticContext.setDisplayName(\"static\");\n    setContextAttributes(staticContext, conf);\n    defaultContexts.put(staticContext, true);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer.addGlobalFilter": "  public void addGlobalFilter(String name, String classname,\n      Map<String, String> parameters) {\n    final String[] ALL_URLS = { \"/*\" };\n    defineFilter(webAppContext, name, classname, parameters, ALL_URLS);\n    for (Context ctx : defaultContexts.keySet()) {\n      defineFilter(ctx, name, classname, parameters, ALL_URLS);\n    }\n    LOG.info(\"Added global filter '\" + name + \"' (class=\" + classname + \")\");\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer.addDefaultServlets": "  protected void addDefaultServlets() {\n    // set up default servlets\n    addServlet(\"stacks\", \"/stacks\", StackServlet.class);\n    addServlet(\"logLevel\", \"/logLevel\", LogLevel.Servlet.class);\n    addServlet(\"metrics\", \"/metrics\", MetricsServlet.class);\n    addServlet(\"jmx\", \"/jmx\", JMXJsonServlet.class);\n    addServlet(\"conf\", \"/conf\", ConfServlet.class);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer.getWebAppsPath": "  protected String getWebAppsPath(String appName) throws FileNotFoundException {\n    URL url = getClass().getClassLoader().getResource(\"webapps/\" + appName);\n    if (url == null) \n      throw new FileNotFoundException(\"webapps/\" + appName\n          + \" not found in CLASSPATH\");\n    String urlString = url.toString();\n    return urlString.substring(0, urlString.lastIndexOf('/'));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer.getFilterInitializers": "  private static FilterInitializer[] getFilterInitializers(Configuration conf) {\n    if (conf == null) {\n      return null;\n    }\n\n    Class<?>[] classes = conf.getClasses(FILTER_INITIALIZER_PROPERTY);\n    if (classes == null) {\n      return null;\n    }\n\n    FilterInitializer[] initializers = new FilterInitializer[classes.length];\n    for(int i = 0; i < classes.length; i++) {\n      initializers[i] = (FilterInitializer)ReflectionUtils.newInstance(\n          classes[i], conf);\n    }\n    return initializers;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer.addFilterPathMapping": "  protected void addFilterPathMapping(String pathSpec,\n      Context webAppCtx) {\n    ServletHandler handler = webAppCtx.getServletHandler();\n    for(String name : filterNames) {\n      FilterMapping fmap = new FilterMapping();\n      fmap.setPathSpec(pathSpec);\n      fmap.setFilterName(name);\n      fmap.setDispatches(Handler.ALL);\n      handler.addFilterMapping(fmap);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpRequestLog.getRequestLog": "  public static RequestLog getRequestLog(String name) {\n\n    String lookup = serverToComponent.get(name);\n    if (lookup != null) {\n      name = lookup;\n    }\n    String loggerName = \"http.requests.\" + name;\n    String appenderName = name + \"requestlog\";\n    Log logger = LogFactory.getLog(loggerName);\n\n    if (logger instanceof Log4JLogger) {\n      Log4JLogger httpLog4JLog = (Log4JLogger)logger;\n      Logger httpLogger = httpLog4JLog.getLogger();\n      Appender appender = null;\n\n      try {\n        appender = httpLogger.getAppender(appenderName);\n      } catch (LogConfigurationException e) {\n        LOG.warn(\"Http request log for \" + loggerName\n            + \" could not be created\");\n        throw e;\n      }\n\n      if (appender == null) {\n        LOG.info(\"Http request log for \" + loggerName\n            + \" is not defined\");\n        return null;\n      }\n\n      if (appender instanceof HttpRequestLogAppender) {\n        HttpRequestLogAppender requestLogAppender\n          = (HttpRequestLogAppender)appender;\n        NCSARequestLog requestLog = new NCSARequestLog();\n        requestLog.setFilename(requestLogAppender.getFilename());\n        requestLog.setRetainDays(requestLogAppender.getRetainDays());\n        return requestLog;\n      }\n      else {\n        LOG.warn(\"Jetty request log for \" + loggerName\n            + \" was of the wrong class\");\n        return null;\n      }\n    }\n    else {\n      LOG.warn(\"Jetty request log can only be enabled using Log4j\");\n      return null;\n    }\n  }"
        },
        "bug_report": {
            "Title": "HttpServer can't start if hostname is not specified",
            "Description": "HADOOP-8362 added a checking to make sure configuration values are not null. By default, we don't specify the hostname for the HttpServer. So we could not start info server due to\n\n{noformat}\n2014-01-22 08:43:05,969 FATAL [M:0;localhost:48573] master.HMaster(2187): Unhandled exception. Starting shutdown.\njava.lang.IllegalArgumentException: Property value must not be null\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:92)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:958)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:940)\n\tat org.apache.hadoop.http.HttpServer.initializeWebServer(HttpServer.java:510)\n\tat org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:470)\n\tat org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:458)\n\tat org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:412)\n\tat org.apache.hadoop.hbase.util.InfoServer.<init>(InfoServer.java:59)\n\tat org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:584)\n\tat java.lang.Thread.run(Thread.java:722){noformat}"
        }
    },
    {
        "filename": "HADOOP-12239.json",
        "creation_time": "2015-07-15T18:06:14.000+0000",
        "stack_trace": "```\njava.io.IOException: failed log splitting for workernode12.xxx.b6.internal.cloudapp.net,60020,1436448556374, will retry\n\tat org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.resubmit(ServerShutdownHandler.java:343)\n\tat org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:211)\n\tat org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Unable to write RenamePending file for folder rename from hbase/WALs/workernode12.xxx.b6.internal.cloudapp.net,60020,1436448566374 to hbase/WALs/workernode12.xxx.b6.internal.cloudapp.net,60020,1436448566374-splitting\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending.writeFile(NativeAzureFileSystem.java:258)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.prepareAtomicFolderRename(NativeAzureFileSystem.java:2110)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1998)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.getLogDirs(MasterFileSystem.java:325)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:412)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:390)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:288)\n\tat org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:204)\n\t... 4 more\nCaused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2598)\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2609)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1366)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:775)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending.writeFile(NativeAzureFileSystem.java:255)\n\t... 11 more\nCaused by: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat com.microsoft.azure.storage.StorageException.translateException(StorageException.java:89)\n\tat com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:307)\n\tat com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:182)\n\tat com.microsoft.azure.storage.blob.CloudBlob.uploadProperties(CloudBlob.java:2892)\n\tat org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobWrapperImpl.uploadProperties(StorageInterfaceImpl.java:372)\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2593)```",
        "source_code": {
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.writeFile": "    public void writeFile(FileSystem fs) throws IOException {\n      Path path = getRenamePendingFilePath();\n      if (LOG.isDebugEnabled()){\n        LOG.debug(\"Preparing to write atomic rename state to \" + path.toString());\n      }\n      OutputStream output = null;\n\n      String contents = makeRenamePendingFileContents();\n\n      // Write file.\n      try {\n        output = fs.create(path);\n        output.write(contents.getBytes(Charset.forName(\"UTF-8\")));\n      } catch (IOException e) {\n        throw new IOException(\"Unable to write RenamePending file for folder rename from \"\n            + srcKey + \" to \" + dstKey, e);\n      } finally {\n        IOUtils.cleanup(LOG, output);\n      }\n    }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.write": "    public void write(byte[] b, int off, int len) throws IOException {\n      out.write(b, off, len);\n    }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.makeRenamePendingFileContents": "    public String makeRenamePendingFileContents() {\n      SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS\");\n      sdf.setTimeZone(TimeZone.getTimeZone(\"UTC\"));\n      String time = sdf.format(new Date());\n\n      // Make file list string\n      StringBuilder builder = new StringBuilder();\n      builder.append(\"[\\n\");\n      for (int i = 0; i != fileMetadata.length; i++) {\n        if (i > 0) {\n          builder.append(\",\\n\");\n        }\n        builder.append(\"    \");\n        String noPrefix = StringUtils.removeStart(fileMetadata[i].getKey(), srcKey + \"/\");\n\n        // Quote string file names, escaping any possible \" characters or other\n        // necessary characters in the name.\n        builder.append(quote(noPrefix));\n        if (builder.length() >=\n            MAX_RENAME_PENDING_FILE_SIZE - FORMATTING_BUFFER) {\n\n          // Give up now to avoid using too much memory.\n          LOG.error(\"Internal error: Exceeded maximum rename pending file size of \"\n              + MAX_RENAME_PENDING_FILE_SIZE + \" bytes.\");\n\n          // return some bad JSON with an error message to make it human readable\n          return \"exceeded maximum rename pending file size\";\n        }\n      }\n      builder.append(\"\\n  ]\");\n      String fileList = builder.toString();\n\n      // Make file contents as a string. Again, quote file names, escaping\n      // characters as appropriate.\n      String contents = \"{\\n\"\n          + \"  FormatVersion: \\\"1.0\\\",\\n\"\n          + \"  OperationUTCTime: \\\"\" + time + \"\\\",\\n\"\n          + \"  OldFolderName: \" + quote(srcKey) + \",\\n\"\n          + \"  NewFolderName: \" + quote(dstKey) + \",\\n\"\n          + \"  FileList: \" + fileList + \"\\n\"\n          + \"}\\n\";\n\n      return contents;\n    }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.cleanup": "    public void cleanup() throws IOException {\n\n      if (fs.getStoreInterface().isAtomicRenameKey(srcKey)) {\n\n        // Remove RenamePending file\n        fs.delete(getRenamePendingFilePath(), false);\n\n        // Freeing source folder lease is not necessary since the source\n        // folder file was deleted.\n      }\n    }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.create": "  private FSDataOutputStream create(Path f, FsPermission permission,\n      boolean overwrite, boolean createParent, int bufferSize,\n      short replication, long blockSize, Progressable progress,\n      SelfRenewingLease parentFolderLease)\n          throws IOException {\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Creating file: \" + f.toString());\n    }\n\n    if (containsColon(f)) {\n      throw new IOException(\"Cannot create file \" + f\n          + \" through WASB that has colons in the name\");\n    }\n\n    Path absolutePath = makeAbsolute(f);\n    String key = pathToKey(absolutePath);\n\n    FileMetadata existingMetadata = store.retrieveMetadata(key);\n    if (existingMetadata != null) {\n      if (existingMetadata.isDir()) {\n        throw new IOException(\"Cannot create file \" + f\n            + \"; already exists as a directory.\");\n      }\n      if (!overwrite) {\n        throw new IOException(\"File already exists:\" + f);\n      }\n    }\n\n    Path parentFolder = absolutePath.getParent();\n    if (parentFolder != null && parentFolder.getParent() != null) { // skip root\n      // Update the parent folder last modified time if the parent folder\n      // already exists.\n      String parentKey = pathToKey(parentFolder);\n      FileMetadata parentMetadata = store.retrieveMetadata(parentKey);\n      if (parentMetadata != null && parentMetadata.isDir() &&\n          parentMetadata.getBlobMaterialization() == BlobMaterialization.Explicit) {\n        store.updateFolderLastModifiedTime(parentKey, parentFolderLease);\n      } else {\n        // Make sure that the parent folder exists.\n        // Create it using inherited permissions from the first existing directory going up the path\n        Path firstExisting = parentFolder.getParent();\n        FileMetadata metadata = store.retrieveMetadata(pathToKey(firstExisting));\n        while(metadata == null) {\n          // Guaranteed to terminate properly because we will eventually hit root, which will return non-null metadata\n          firstExisting = firstExisting.getParent();\n          metadata = store.retrieveMetadata(pathToKey(firstExisting));\n        }\n        mkdirs(parentFolder, metadata.getPermissionStatus().getPermission(), true);\n      }\n    }\n\n    // Mask the permission first (with the default permission mask as well).\n    FsPermission masked = applyUMask(permission, UMaskApplyMode.NewFile);\n    PermissionStatus permissionStatus = createPermissionStatus(masked);\n\n    OutputStream bufOutStream;\n    if (store.isPageBlobKey(key)) {\n      // Store page blobs directly in-place without renames.\n      bufOutStream = store.storefile(key, permissionStatus);\n    } else {\n      // This is a block blob, so open the output blob stream based on the\n      // encoded key.\n      //\n      String keyEncoded = encodeKey(key);\n\n\n      // First create a blob at the real key, pointing back to the temporary file\n      // This accomplishes a few things:\n      // 1. Makes sure we can create a file there.\n      // 2. Makes it visible to other concurrent threads/processes/nodes what\n      // we're\n      // doing.\n      // 3. Makes it easier to restore/cleanup data in the event of us crashing.\n      store.storeEmptyLinkFile(key, keyEncoded, permissionStatus);\n\n      // The key is encoded to point to a common container at the storage server.\n      // This reduces the number of splits on the server side when load balancing.\n      // Ingress to Azure storage can take advantage of earlier splits. We remove\n      // the root path to the key and prefix a random GUID to the tail (or leaf\n      // filename) of the key. Keys are thus broadly and randomly distributed over\n      // a single container to ease load balancing on the storage server. When the\n      // blob is committed it is renamed to its earlier key. Uncommitted blocks\n      // are not cleaned up and we leave it to Azure storage to garbage collect\n      // these\n      // blocks.\n      bufOutStream = new NativeAzureFsOutputStream(store.storefile(\n          keyEncoded, permissionStatus), key, keyEncoded);\n    }\n    // Construct the data output stream from the buffered output stream.\n    FSDataOutputStream fsOut = new FSDataOutputStream(bufOutStream, statistics);\n\n    \n    // Increment the counter\n    instrumentation.fileCreated();\n    \n    // Return data output stream to caller.\n    return fsOut;\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.getRenamePendingFilePath": "    private Path getRenamePendingFilePath() {\n      String fileName = srcKey + SUFFIX;\n      Path fileNamePath = keyToPath(fileName);\n      Path path = fs.makeAbsolute(fileNamePath);\n      return path;\n    }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.prepareAtomicFolderRename": "  private FolderRenamePending prepareAtomicFolderRename(\n      String srcKey, String dstKey) throws IOException {\n\n    if (store.isAtomicRenameKey(srcKey)) {\n\n      // Block unwanted concurrent access to source folder.\n      SelfRenewingLease lease = leaseSourceFolder(srcKey);\n\n      // Prepare in-memory information needed to do or redo a folder rename.\n      FolderRenamePending renamePending =\n          new FolderRenamePending(srcKey, dstKey, lease, this);\n\n      // Save it to persistent storage to help recover if the operation fails.\n      renamePending.writeFile(this);\n      return renamePending;\n    } else {\n      FolderRenamePending renamePending =\n          new FolderRenamePending(srcKey, dstKey, null, this);\n      return renamePending;\n    }\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.leaseSourceFolder": "  private SelfRenewingLease leaseSourceFolder(String srcKey)\n      throws AzureException {\n    return store.acquireLease(srcKey);\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.rename": "  public boolean rename(Path src, Path dst) throws IOException {\n\n    FolderRenamePending renamePending = null;\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Moving \" + src + \" to \" + dst);\n    }\n\n    if (containsColon(dst)) {\n      throw new IOException(\"Cannot rename to file \" + dst\n          + \" through WASB that has colons in the name\");\n    }\n\n    String srcKey = pathToKey(makeAbsolute(src));\n\n    if (srcKey.length() == 0) {\n      // Cannot rename root of file system\n      return false;\n    }\n\n    // Figure out the final destination\n    Path absoluteDst = makeAbsolute(dst);\n    String dstKey = pathToKey(absoluteDst);\n    FileMetadata dstMetadata = store.retrieveMetadata(dstKey);\n    if (dstMetadata != null && dstMetadata.isDir()) {\n      // It's an existing directory.\n      dstKey = pathToKey(makeAbsolute(new Path(dst, src.getName())));\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Destination \" + dst\n            + \" is a directory, adjusted the destination to be \" + dstKey);\n      }\n    } else if (dstMetadata != null) {\n      // Attempting to overwrite a file using rename()\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Destination \" + dst\n            + \" is an already existing file, failing the rename.\");\n      }\n      return false;\n    } else {\n      // Check that the parent directory exists.\n      FileMetadata parentOfDestMetadata =\n          store.retrieveMetadata(pathToKey(absoluteDst.getParent()));\n      if (parentOfDestMetadata == null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Parent of the destination \" + dst\n              + \" doesn't exist, failing the rename.\");\n        }\n        return false;\n      } else if (!parentOfDestMetadata.isDir()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Parent of the destination \" + dst\n              + \" is a file, failing the rename.\");\n        }\n        return false;\n      }\n    }\n    FileMetadata srcMetadata = store.retrieveMetadata(srcKey);\n    if (srcMetadata == null) {\n      // Source doesn't exist\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Source \" + src + \" doesn't exist, failing the rename.\");\n      }\n      return false;\n    } else if (!srcMetadata.isDir()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Source \" + src + \" found as a file, renaming.\");\n      }\n      store.rename(srcKey, dstKey);\n    } else {\n\n      // Prepare for, execute and clean up after of all files in folder, and\n      // the root file, and update the last modified time of the source and\n      // target parent folders. The operation can be redone if it fails part\n      // way through, by applying the \"Rename Pending\" file.\n\n      // The following code (internally) only does atomic rename preparation\n      // and lease management for page blob folders, limiting the scope of the\n      // operation to HBase log file folders, where atomic rename is required.\n      // In the future, we could generalize it easily to all folders.\n      renamePending = prepareAtomicFolderRename(srcKey, dstKey);\n      renamePending.execute();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Renamed \" + src + \" to \" + dst + \" successfully.\");\n      }\n      renamePending.cleanup();\n      return true;\n    }\n\n    // Update the last-modified time of the parent folders of both source\n    // and destination.\n    updateParentFolderLastModifiedTime(srcKey);\n    updateParentFolderLastModifiedTime(dstKey);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Renamed \" + src + \" to \" + dst + \" successfully.\");\n    }\n    return true;\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.makeAbsolute": "  public Path makeAbsolute(Path path) {\n    if (path.isAbsolute()) {\n      return path;\n    }\n    return new Path(workingDir, path);\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.execute": "    public void execute() throws IOException {\n\n      for (FileMetadata file : this.getFiles()) {\n\n        // Rename all materialized entries under the folder to point to the\n        // final destination.\n        if (file.getBlobMaterialization() == BlobMaterialization.Explicit) {\n          String srcName = file.getKey();\n          String suffix  = srcName.substring((this.getSrcKey()).length());\n          String dstName = this.getDstKey() + suffix;\n\n          // Rename gets exclusive access (via a lease) for files\n          // designated for atomic rename.\n          // The main use case is for HBase write-ahead log (WAL) and data\n          // folder processing correctness.  See the rename code for details.\n          boolean acquireLease = fs.getStoreInterface().isAtomicRenameKey(srcName);\n          fs.getStoreInterface().rename(srcName, dstName, acquireLease, null);\n        }\n      }\n\n      // Rename the source folder 0-byte root file itself.\n      FileMetadata srcMetadata2 = this.getSourceMetadata();\n      if (srcMetadata2.getBlobMaterialization() ==\n          BlobMaterialization.Explicit) {\n\n        // It already has a lease on it from the \"prepare\" phase so there's no\n        // need to get one now. Pass in existing lease to allow file delete.\n        fs.getStoreInterface().rename(this.getSrcKey(), this.getDstKey(),\n            false, folderLease);\n      }\n\n      // Update the last-modified time of the parent folders of both source and\n      // destination.\n      fs.updateParentFolderLastModifiedTime(srcKey);\n      fs.updateParentFolderLastModifiedTime(dstKey);\n    }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.pathToKey": "  public String pathToKey(Path path) {\n    // Convert the path to a URI to parse the scheme, the authority, and the\n    // path from the path object.\n    URI tmpUri = path.toUri();\n    String pathUri = tmpUri.getPath();\n\n    // The scheme and authority is valid. If the path does not exist add a \"/\"\n    // separator to list the root of the container.\n    Path newPath = path;\n    if (\"\".equals(pathUri)) {\n      newPath = new Path(tmpUri.toString() + Path.SEPARATOR);\n    }\n\n    // Verify path is absolute if the path refers to a windows drive scheme.\n    if (!newPath.isAbsolute()) {\n      throw new IllegalArgumentException(\"Path must be absolute: \" + path);\n    }\n\n    String key = null;\n    key = newPath.toUri().getPath();\n    key = removeTrailingSlash(key);\n    key = encodeTrailingPeriod(key);\n    if (key.length() == 1) {\n      return key;\n    } else {\n      return key.substring(1); // remove initial slash\n    }\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.updateParentFolderLastModifiedTime": "  private void updateParentFolderLastModifiedTime(String key)\n      throws IOException {\n    Path parent = makeAbsolute(keyToPath(key)).getParent();\n    if (parent != null && parent.getParent() != null) { // not root\n      String parentKey = pathToKey(parent);\n\n      // ensure the parent is a materialized folder\n      FileMetadata parentMetadata = store.retrieveMetadata(parentKey);\n      // The metadata could be null if the implicit folder only contains a\n      // single file. In this case, the parent folder no longer exists if the\n      // file is renamed; so we can safely ignore the null pointer case.\n      if (parentMetadata != null) {\n        if (parentMetadata.isDir()\n            && parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {\n          store.storeEmptyFolder(parentKey,\n              createPermissionStatus(FsPermission.getDefault()));\n        }\n\n        if (store.isAtomicRenameKey(parentKey)) {\n          SelfRenewingLease lease = null;\n          try {\n            lease = leaseSourceFolder(parentKey);\n            store.updateFolderLastModifiedTime(parentKey, lease);\n          } catch (AzureException e) {\n            String errorCode = \"\";\n            try {\n              StorageException e2 = (StorageException) e.getCause();\n              errorCode = e2.getErrorCode();\n            } catch (Exception e3) {\n              // do nothing if cast fails\n            }\n            if (errorCode.equals(\"BlobNotFound\")) {\n              throw new FileNotFoundException(\"Folder does not exist: \" + parentKey);\n            }\n            LOG.warn(\"Got unexpected exception trying to get lease on \"\n                + parentKey + \". \" + e.getMessage());\n            throw e;\n          } finally {\n            try {\n              if (lease != null) {\n                lease.free();\n              }\n            } catch (Exception e) {\n              LOG.error(\"Unable to free lease on \" + parentKey, e);\n            }\n          }\n        } else {\n          store.updateFolderLastModifiedTime(parentKey, null);\n        }\n      }\n    }\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.containsColon": "  private boolean containsColon(Path p) {\n    return p.toUri().getPath().toString().contains(\":\");\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime": "  public void updateFolderLastModifiedTime(String key,\n      SelfRenewingLease folderLease) throws AzureException {\n    final Calendar lastModifiedCalendar = Calendar\n        .getInstance(Utility.LOCALE_US);\n    lastModifiedCalendar.setTimeZone(Utility.UTC_ZONE);\n    Date lastModified = lastModifiedCalendar.getTime();\n    updateFolderLastModifiedTime(key, lastModified, folderLease);\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.getBlobReference": "  private CloudBlobWrapper getBlobReference(String aKey)\n      throws StorageException, URISyntaxException {\n\n    CloudBlobWrapper blob = null;\n    if (isPageBlobKey(aKey)) {\n      blob = this.container.getPageBlobReference(aKey);\n    } else {\n      blob = this.container.getBlockBlobReference(aKey);\n    blob.setStreamMinimumReadSizeInBytes(downloadBlockSizeBytes);\n    blob.setWriteBlockSizeInBytes(uploadBlockSizeBytes);\n    }\n\n    return blob;\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.getInstrumentedContext": "  private OperationContext getInstrumentedContext(boolean bindConcurrentOOBIo) {\n\n    OperationContext operationContext = new OperationContext();\n\n    if (selfThrottlingEnabled) {\n      SelfThrottlingIntercept.hook(operationContext, selfThrottlingReadFactor,\n          selfThrottlingWriteFactor);\n    }\n\n    if(bandwidthGaugeUpdater != null) {\n      //bandwidthGaugeUpdater is null when we config to skip azure metrics\n      ResponseReceivedMetricUpdater.hook(\n         operationContext,\n         instrumentation,\n         bandwidthGaugeUpdater);\n    }\n\n    // Bind operation context to receive send request callbacks on this operation.\n    // If reads concurrent to OOB writes are allowed, the interception will reset\n    // the conditional header on all Azure blob storage read requests.\n    if (bindConcurrentOOBIo) {\n      SendRequestIntercept.bind(storageInteractionLayer.getCredentials(),\n          operationContext, true);\n    }\n\n    if (testHookOperationContext != null) {\n      operationContext =\n          testHookOperationContext.modifyOperationContext(operationContext);\n    }\n\n    ErrorMetricUpdater.hook(operationContext, instrumentation);\n\n    // Return the operation context.\n    return operationContext;\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.checkContainer": "  private ContainerState checkContainer(ContainerAccessType accessType)\n      throws StorageException, AzureException {\n    synchronized (containerStateLock) {\n      if (isOkContainerState(accessType)) {\n        return currentKnownContainerState;\n      }\n      if (currentKnownContainerState == ContainerState.ExistsAtWrongVersion) {\n        String containerVersion = retrieveVersionAttribute(container);\n        throw wrongVersionException(containerVersion);\n      }\n      // This means I didn't check it before or it didn't exist or\n      // we need to stamp the version. Since things may have changed by\n      // other machines since then, do the check again and don't depend\n      // on past information.\n\n      // Sanity check: we don't expect this at this point.\n      if (currentKnownContainerState == ContainerState.ExistsAtRightVersion) {\n        throw new AssertionError(\"Unexpected state: \"\n            + currentKnownContainerState);\n      }\n\n      // Download the attributes - doubles as an existence check with just\n      // one service call\n      try {\n        container.downloadAttributes(getInstrumentedContext());\n        currentKnownContainerState = ContainerState.Unknown;\n      } catch (StorageException ex) {\n        if (ex.getErrorCode().equals(\n            StorageErrorCode.RESOURCE_NOT_FOUND.toString())) {\n          currentKnownContainerState = ContainerState.DoesntExist;\n        } else {\n          throw ex;\n        }\n      }\n\n      if (currentKnownContainerState == ContainerState.DoesntExist) {\n        // If the container doesn't exist and we intend to write to it,\n        // create it now.\n        if (needToCreateContainer(accessType)) {\n          storeVersionAttribute(container);\n          container.create(getInstrumentedContext());\n          currentKnownContainerState = ContainerState.ExistsAtRightVersion;\n        }\n      } else {\n        // The container exists, check the version.\n        String containerVersion = retrieveVersionAttribute(container);\n        if (containerVersion != null) {\n          if (containerVersion.equals(FIRST_WASB_VERSION)) {\n            // It's the version from when WASB was called ASV, just\n            // fix the version attribute if needed and proceed.\n            // We should be good otherwise.\n            if (needToStampVersion(accessType)) {\n              storeVersionAttribute(container);\n              container.uploadMetadata(getInstrumentedContext());\n            }\n          } else if (!containerVersion.equals(CURRENT_WASB_VERSION)) {\n            // Don't know this version - throw.\n            currentKnownContainerState = ContainerState.ExistsAtWrongVersion;\n            throw wrongVersionException(containerVersion);\n          } else {\n            // It's our correct version.\n            currentKnownContainerState = ContainerState.ExistsAtRightVersion;\n          }\n        } else {\n          // No version info exists.\n          currentKnownContainerState = ContainerState.ExistsNoVersion;\n          if (needToStampVersion(accessType)) {\n            // Need to stamp the version\n            storeVersionAttribute(container);\n            container.uploadMetadata(getInstrumentedContext());\n            currentKnownContainerState = ContainerState.ExistsAtRightVersion;\n          }\n        }\n      }\n      return currentKnownContainerState;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.create": "  public FSDataOutputStream create(Path f,\n      FsPermission permission,\n      EnumSet<CreateFlag> flags,\n      int bufferSize,\n      short replication,\n      long blockSize,\n      Progressable progress,\n      ChecksumOpt checksumOpt) throws IOException {\n    // Checksum options are ignored by default. The file systems that\n    // implement checksum need to override this method. The full\n    // support is currently only available in DFS.\n    return create(f, permission, flags.contains(CreateFlag.OVERWRITE), \n        bufferSize, replication, blockSize, progress);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.setPermission": "  public void setPermission(Path p, FsPermission permission\n      ) throws IOException {\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getDefaultReplication": "  public short getDefaultReplication(Path path) {\n    return getDefaultReplication();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getDefaultBlockSize": "  public long getDefaultBlockSize(Path f) {\n    return getDefaultBlockSize();\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.StorageInterfaceImpl.uploadProperties": "    public void uploadProperties(OperationContext opContext)\n        throws StorageException {\n      getBlob().uploadProperties(null, null, opContext);\n    }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.StorageInterfaceImpl.getBlob": "    public CloudBlob getBlob() {\n      return blob;\n    }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.StorageInterfaceImpl.getLeaseCondition": "    private AccessCondition getLeaseCondition(SelfRenewingLease lease) {\n      AccessCondition leaseCondition = null;\n      if (lease != null) {\n        leaseCondition = AccessCondition.generateLeaseCondition(lease.getLeaseID());\n      }\n      return leaseCondition;\n    }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.FileMetadata.isDir": "  public boolean isDir() {\n    return isDir;\n  }"
        },
        "bug_report": {
            "Title": "StorageException complaining \" no lease ID\" when updating FolderLastModifiedTime in WASB",
            "Description": " This is a similar issue as HADOOP-11523 and HADOOP-12089, which I found in a customer's HBase cluster logs, but the piece of code is in a different place.\n{code}\n2015-07-09 13:38:57,388 INFO org.apache.hadoop.hbase.master.SplitLogManager: dead splitlog workers [workernode3.xxx.b6.internal.cloudapp.net,60020,1436448555180]\n2015-07-09 13:38:57,466 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN\njava.io.IOException: failed log splitting for workernode12.xxx.b6.internal.cloudapp.net,60020,1436 448566374, will retry\n\tat org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.resubmit(ServerShutdownHandler.java:343)\n\tat org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:211)\n\tat org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Unable to write RenamePending file for folder rename from hbase/WALs/workernode12.xxx.b6.internal.cloudapp.net,60020,1436448566374 to hbase/WALs/workernode12.xxx.b6.internal.cloudapp.net,60020,1436448566374-splitting\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending.writeFile(NativeAzureFileSystem.java:258)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.prepareAtomicFolderRename(NativeAzureFileSystem.java:2110)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1998)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.getLogDirs(MasterFileSystem.java:325)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:412)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:390)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:288)\n\tat org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:204)\n\t... 4 more\nCaused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2598)\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2609)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1366)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:775)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending.writeFile(NativeAzureFileSystem.java:255)\n\t... 11 more\nCaused by: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat com.microsoft.azure.storage.StorageException.translateException(StorageException.java:89)\n\tat com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:307)\n\tat com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:182)\n\tat com.microsoft.azure.storage.blob.CloudBlob.uploadProperties(CloudBlob.java:2892)\n\tat org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobWrapperImpl.uploadProperties(StorageInterfaceImpl.java:372)\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2593)\n\t... 19 more\n{code}"
        }
    },
    {
        "filename": "HADOOP-11878.json",
        "creation_time": "2015-04-27T08:06:22.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.fs.FileContext.fixRelativePart(FileContext.java:274)\n        at org.apache.hadoop.fs.FileContext.delete(FileContext.java:761)\n        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.deleteAsUser(DefaultContainerExecutor.java:457)\n        at org.apache.hadoop.yarn.server.nodemanager.DeletionService$FileDeletionTask.run(DeletionService.java:293)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.fixRelativePart": "  Path fixRelativePart(Path p) {\n    if (p.isUriPathAbsolute()) {\n      return p;\n    } else {\n      return new Path(workingDir, p);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.delete": "  public boolean delete(final Path f, final boolean recursive)\n      throws AccessControlException, FileNotFoundException,\n      UnsupportedFileSystemException, IOException {\n    Path absF = fixRelativePart(f);\n    return new FSLinkResolver<Boolean>() {\n      @Override\n      public Boolean next(final AbstractFileSystem fs, final Path p) \n        throws IOException, UnresolvedLinkException {\n        return Boolean.valueOf(fs.delete(p, recursive));\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.resolve": "  protected Path resolve(final Path f) throws FileNotFoundException,\n      UnresolvedLinkException, AccessControlException, IOException {\n    return new FSLinkResolver<Path>() {\n      @Override\n      public Path next(final AbstractFileSystem fs, final Path p) \n        throws IOException, UnresolvedLinkException {\n        return fs.resolvePath(p);\n      }\n    }.resolve(this, f);\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.deleteAsUser": "  public void deleteAsUser(String user, Path subDir, Path... baseDirs)\n      throws IOException, InterruptedException {\n    if (baseDirs == null || baseDirs.length == 0) {\n      LOG.info(\"Deleting absolute path : \" + subDir);\n      if (!lfs.delete(subDir, true)) {\n        //Maybe retry\n        LOG.warn(\"delete returned false for path: [\" + subDir + \"]\");\n      }\n      return;\n    }\n    for (Path baseDir : baseDirs) {\n      Path del = subDir == null ? baseDir : new Path(baseDir, subDir);\n      LOG.info(\"Deleting path : \" + del);\n      if (!lfs.delete(del, true)) {\n        LOG.warn(\"delete returned false for path: [\" + del + \"]\");\n      }\n    }\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.DeletionService.run": "    public void run() {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(this);\n      }\n      boolean error = false;\n      if (null == user) {\n        if (baseDirs == null || baseDirs.size() == 0) {\n          LOG.debug(\"NM deleting absolute path : \" + subDir);\n          try {\n            lfs.delete(subDir, true);\n          } catch (IOException e) {\n            error = true;\n            LOG.warn(\"Failed to delete \" + subDir);\n          }\n        } else {\n          for (Path baseDir : baseDirs) {\n            Path del = subDir == null? baseDir : new Path(baseDir, subDir);\n            LOG.debug(\"NM deleting path : \" + del);\n            try {\n              lfs.delete(del, true);\n            } catch (IOException e) {\n              error = true;\n              LOG.warn(\"Failed to delete \" + subDir);\n            }\n          }\n        }\n      } else {\n        try {\n          LOG.debug(\"Deleting path: [\" + subDir + \"] as user: [\" + user + \"]\");\n          if (baseDirs == null || baseDirs.size() == 0) {\n            delService.exec.deleteAsUser(user, subDir, (Path[])null);\n          } else {\n            delService.exec.deleteAsUser(user, subDir,\n              baseDirs.toArray(new Path[0]));\n          }\n        } catch (IOException e) {\n          error = true;\n          LOG.warn(\"Failed to delete as user \" + user, e);\n        } catch (InterruptedException e) {\n          error = true;\n          LOG.warn(\"Failed to delete as user \" + user, e);\n        }\n      }\n      if (error) {\n        setSuccess(!error);        \n      }\n      fileDeletionTaskFinished();\n    }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.DeletionService.fileDeletionTaskFinished": "    private synchronized void fileDeletionTaskFinished() {\n      try {\n        delService.stateStore.removeDeletionTask(taskId);\n      } catch (IOException e) {\n        LOG.error(\"Unable to remove deletion task \" + taskId\n            + \" from state store\", e);\n      }\n      Iterator<FileDeletionTask> successorTaskI =\n          this.successorTaskSet.iterator();\n      while (successorTaskI.hasNext()) {\n        FileDeletionTask successorTask = successorTaskI.next();\n        if (!success) {\n          successorTask.setSuccess(success);\n        }\n        int count = successorTask.decrementAndGetPendingPredecessorTasks();\n        if (count == 0) {\n          if (successorTask.getSucess()) {\n            successorTask.delService.scheduleFileDeletionTask(successorTask);\n          } else {\n            successorTask.fileDeletionTaskFinished();\n          }\n        }\n      }\n    }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.DeletionService.delete": "  public void delete(String user, Path subDir, Path... baseDirs) {\n    // TODO if parent owned by NM, rename within parent inline\n    if (debugDelay != -1) {\n      List<Path> baseDirList = null;\n      if (baseDirs != null && baseDirs.length != 0) {\n        baseDirList = Arrays.asList(baseDirs);\n      }\n      FileDeletionTask task =\n          new FileDeletionTask(this, user, subDir, baseDirList);\n      recordDeletionTaskInStateStore(task);\n      sched.schedule(task, debugDelay, TimeUnit.SECONDS);\n    }\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.DeletionService.setSuccess": "    public synchronized void setSuccess(boolean success) {\n      this.success = success;\n    }"
        },
        "bug_report": {
            "Title": "FileContext.java # fixRelativePart should check for not null for a more informative exception",
            "Description": "Following will come when job failed and deletion service trying to delete the log fiels\n\n2015-04-27 14:56:17,113 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : null\n2015-04-27 14:56:17,113 ERROR org.apache.hadoop.yarn.server.nodemanager.DeletionService: Exception during execution of task in DeletionService\njava.lang.NullPointerException\n        at org.apache.hadoop.fs.FileContext.fixRelativePart(FileContext.java:274)\n        at org.apache.hadoop.fs.FileContext.delete(FileContext.java:761)\n        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.deleteAsUser(DefaultContainerExecutor.java:457)\n        at org.apache.hadoop.yarn.server.nodemanager.DeletionService$FileDeletionTask.run(DeletionService.java:293)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "HADOOP-14949.json",
        "creation_time": "2017-10-13T23:44:09.000+0000",
        "stack_trace": "```\njava.lang.AssertionError: Should not have been able to reencryptEncryptedKey\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.apache.hadoop.crypto.key.kms.server.TestKMS$11$15.run(TestKMS.java:1616)\n\tat org.apache.hadoop.crypto.key.kms.server.TestKMS$11$15.run(TestKMS.java:1608)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\n\tat org.apache.hadoop.crypto.key.kms.server.TestKMS.doAs(TestKMS.java:313)\n\tat org.apache.hadoop.crypto.key.kms.server.TestKMS.access$100(TestKMS.java:97)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause == null) {\n        throw new RuntimeException(\"PrivilegedActionException with no \" +\n                \"underlying cause. UGI [\" + this + \"]\" +\": \" + pae, pae);\n      } else if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }"
        },
        "bug_report": {
            "Title": "TestKMS#testACLs fails intermittently",
            "Description": "We have seen some intermittent failures of this test:\r\n\r\nError Message\r\n{noformat}\r\njava.lang.AssertionError\r\n{noformat}\r\nStack Trace\r\n\r\n{noformat}java.lang.AssertionError: Should not have been able to reencryptEncryptedKey\r\n\tat org.junit.Assert.fail(Assert.java:88)\r\n\tat org.apache.hadoop.crypto.key.kms.server.TestKMS$11$15.run(TestKMS.java:1616)\r\n\tat org.apache.hadoop.crypto.key.kms.server.TestKMS$11$15.run(TestKMS.java:1608)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\r\n\tat org.apache.hadoop.crypto.key.kms.server.TestKMS.doAs(TestKMS.java:313)\r\n\tat org.apache.hadoop.crypto.key.kms.server.TestKMS.access$100(TestKMS.java:97)\r\n{noformat}\r\nStandard Output\r\n{noformat}\r\n2017-10-07 09:44:11,112 INFO  log - jetty-6.1.26.cloudera.4\r\n2017-10-07 09:44:11,131 INFO  KMSWebApp - -------------------------------------------------------------\r\n2017-10-07 09:44:11,131 INFO  KMSWebApp -   Java runtime version : 1.7.0_121-b00\r\n2017-10-07 09:44:11,131 INFO  KMSWebApp -   User: slave\r\n2017-10-07 09:44:11,131 INFO  KMSWebApp -   KMS Hadoop Version: 2.6.0-cdh5.14.0-SNAPSHOT\r\n2017-10-07 09:44:11,131 INFO  KMSWebApp - -------------------------------------------------------------\r\n2017-10-07 09:44:11,134 INFO  KMSACLs - 'CREATE' ACL 'CREATE,SET_KEY_MATERIAL'\r\n2017-10-07 09:44:11,134 INFO  KMSACLs - 'DELETE' ACL 'DELETE'\r\n2017-10-07 09:44:11,134 INFO  KMSACLs - 'ROLLOVER' ACL 'ROLLOVER,SET_KEY_MATERIAL'\r\n2017-10-07 09:44:11,134 INFO  KMSACLs - 'GET' ACL 'GET'\r\n2017-10-07 09:44:11,135 INFO  KMSACLs - 'GET_KEYS' ACL 'GET_KEYS'\r\n2017-10-07 09:44:11,135 INFO  KMSACLs - 'GET_METADATA' ACL 'GET_METADATA'\r\n2017-10-07 09:44:11,135 INFO  KMSACLs - 'SET_KEY_MATERIAL' ACL 'SET_KEY_MATERIAL'\r\n2017-10-07 09:44:11,135 INFO  KMSACLs - 'GENERATE_EEK' ACL 'GENERATE_EEK'\r\n2017-10-07 09:44:11,135 INFO  KMSACLs - 'DECRYPT_EEK' ACL 'DECRYPT_EEK'\r\n2017-10-07 09:44:11,135 INFO  KMSACLs - KEY_NAME 'k0' KEY_OP 'ALL' ACL '*'\r\n2017-10-07 09:44:11,135 INFO  KMSACLs - KEY_NAME 'k1' KEY_OP 'ALL' ACL '*'\r\n2017-10-07 09:44:11,136 INFO  KMSAudit - No audit logger configured, using default.\r\n2017-10-07 09:44:11,137 INFO  KMSAudit - Initializing audit logger class org.apache.hadoop.crypto.key.kms.server.SimpleKMSAuditLogger\r\n2017-10-07 09:44:11,137 INFO  KMSWebApp - Initialized KeyProvider CachingKeyProvider: jceks://file@/tmp/run_tha_testUYG3Cl/hadoop-common-project/hadoop-kms/target/ddbffdf2-e7d8-4e75-982a-debebb227075/kms.keystore\r\n2017-10-07 09:44:11,138 INFO  KMSWebApp - Initialized KeyProviderCryptoExtension EagerKeyGeneratorKeyProviderCryptoExtension: KeyProviderCryptoExtension: CachingKeyProvider: jceks://file@/tmp/run_tha_testUYG3Cl/hadoop-common-project/hadoop-kms/target/ddbffdf2-e7d8-4e75-982a-debebb227075/kms.keystore\r\n2017-10-07 09:44:11,138 INFO  KMSWebApp - Default key bitlength is 128\r\n2017-10-07 09:44:11,138 INFO  KMSWebApp - KMS Started\r\n2017-10-07 09:44:11,141 INFO  PackagesResourceConfig - Scanning for root resource and provider classes in the packages:\r\n  org.apache.hadoop.crypto.key.kms.server\r\n2017-10-07 09:44:11,146 INFO  ScanningResourceConfig - Root resource classes found:\r\n  class org.apache.hadoop.crypto.key.kms.server.KMS\r\n2017-10-07 09:44:11,146 INFO  ScanningResourceConfig - Provider classes found:\r\n  class org.apache.hadoop.crypto.key.kms.server.KMSJSONWriter\r\n  class org.apache.hadoop.crypto.key.kms.server.KMSExceptionsProvider\r\n  class org.apache.hadoop.crypto.key.kms.server.KMSJSONReader\r\n2017-10-07 09:44:11,147 INFO  WebApplicationImpl - Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\r\n2017-10-07 09:44:11,224 INFO  log - Started SocketConnector@localhost:46764\r\nTest KMS running at: http://localhost:46764/kms\r\n2017-10-07 09:44:11,254 INFO  kms-audit - UNAUTHORIZED[op=CREATE_KEY, key=k, user=client] \r\n2017-10-07 09:44:11,255 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request POST http://localhost:46764/kms/v1/keys caused exception.\r\n2017-10-07 09:44:11,256 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'CREATE_KEY' on 'k']!!\r\n2017-10-07 09:44:11,256 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,270 INFO  kms-audit - UNAUTHORIZED[op=CREATE_KEY, key=k, user=client] \r\n2017-10-07 09:44:11,270 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request POST http://localhost:46764/kms/v1/keys caused exception.\r\n2017-10-07 09:44:11,271 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'CREATE_KEY' on 'k']!!\r\n2017-10-07 09:44:11,271 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,284 INFO  kms-audit - UNAUTHORIZED[op=ROLL_NEW_VERSION, key=k, user=client] \r\n2017-10-07 09:44:11,284 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request POST http://localhost:46764/kms/v1/key/k caused exception.\r\n2017-10-07 09:44:11,285 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'ROLL_NEW_VERSION' on 'k']!!\r\n2017-10-07 09:44:11,285 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,300 INFO  kms-audit - UNAUTHORIZED[op=ROLL_NEW_VERSION, key=k, user=client] \r\n2017-10-07 09:44:11,300 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request POST http://localhost:46764/kms/v1/key/k caused exception.\r\n2017-10-07 09:44:11,301 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'ROLL_NEW_VERSION' on 'k']!!\r\n2017-10-07 09:44:11,301 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,315 INFO  kms-audit - UNAUTHORIZED[op=GET_KEYS, user=client] \r\n2017-10-07 09:44:11,315 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/keys/names caused exception.\r\n2017-10-07 09:44:11,316 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'GET_KEYS']!!\r\n2017-10-07 09:44:11,316 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,330 INFO  kms-audit - UNAUTHORIZED[op=GET_KEYS_METADATA, user=client] \r\n2017-10-07 09:44:11,330 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/keys/metadata?key=k caused exception.\r\n2017-10-07 09:44:11,331 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'GET_KEYS_METADATA']!!\r\n2017-10-07 09:44:11,331 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,344 INFO  kms-audit - UNAUTHORIZED[op=GET_KEY_VERSION, user=client] \r\n2017-10-07 09:44:11,344 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/keyversion/k%400 caused exception.\r\n2017-10-07 09:44:11,345 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'GET_KEY_VERSION']!!\r\n2017-10-07 09:44:11,345 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,358 INFO  kms-audit - UNAUTHORIZED[op=GET_CURRENT_KEY, key=k, user=client] \r\n2017-10-07 09:44:11,358 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/key/k/_currentversion caused exception.\r\n2017-10-07 09:44:11,359 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'GET_CURRENT_KEY' on 'k']!!\r\n2017-10-07 09:44:11,359 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,372 INFO  kms-audit - UNAUTHORIZED[op=GET_METADATA, key=k, user=client] \r\n2017-10-07 09:44:11,372 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/key/k/_metadata caused exception.\r\n2017-10-07 09:44:11,373 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'GET_METADATA' on 'k']!!\r\n2017-10-07 09:44:11,373 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,386 INFO  kms-audit - UNAUTHORIZED[op=GET_KEY_VERSIONS, key=k, user=client] \r\n2017-10-07 09:44:11,386 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/key/k/_versions caused exception.\r\n2017-10-07 09:44:11,387 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'GET_KEY_VERSIONS' on 'k']!!\r\n2017-10-07 09:44:11,387 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,407 INFO  kms-audit - UNAUTHORIZED[op=MANAGEMENT, key=k0, user=CREATE] \r\n2017-10-07 09:44:11,409 INFO  kms-audit - OK[op=CREATE_KEY, key=k0, user=CREATE] UserProvidedMaterial:false Description:null\r\n2017-10-07 09:44:11,435 INFO  kms-audit - UNAUTHORIZED[op=MANAGEMENT, key=k0, user=DELETE] \r\n2017-10-07 09:44:11,437 INFO  kms-audit - OK[op=DELETE_KEY, key=k0, user=DELETE] \r\n2017-10-07 09:44:11,457 INFO  kms-audit - UNAUTHORIZED[op=MANAGEMENT, key=k1, user=SET_KEY_MATERIAL] \r\n2017-10-07 09:44:11,459 INFO  kms-audit - OK[op=CREATE_KEY, key=k1, user=SET_KEY_MATERIAL] UserProvidedMaterial:true Description:null\r\n2017-10-07 09:44:11,482 INFO  kms-audit - UNAUTHORIZED[op=MANAGEMENT, key=k1, user=ROLLOVER] \r\n2017-10-07 09:44:11,487 INFO  kms-audit - OK[op=ROLL_NEW_VERSION, key=k1, user=ROLLOVER] UserProvidedMaterial:false NewVersion:k1@1\r\n2017-10-07 09:44:11,489 INFO  kms-audit - UNAUTHORIZED[op=MANAGEMENT, key=k1, user=ROLLOVER] \r\n2017-10-07 09:44:11,489 INFO  kms-audit - OK[op=INVALIDATE_CACHE, key=k1, user=ROLLOVER] \r\n2017-10-07 09:44:11,491 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=ROLLOVER] \r\n2017-10-07 09:44:11,491 WARN  KMS - User ROLLOVER@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/key/k1/_eek?num_keys=150&eek_op=generate caused exception.\r\n2017-10-07 09:44:11,506 INFO  kms-audit - UNAUTHORIZED[op=MANAGEMENT, key=k1, user=ROLLOVER] \r\n2017-10-07 09:44:11,506 INFO  kms-audit - OK[op=INVALIDATE_CACHE, key=k1, user=ROLLOVER] \r\n2017-10-07 09:44:11,507 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=ROLLOVER] \r\n2017-10-07 09:44:11,507 WARN  KMS - User ROLLOVER@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/key/k1/_eek?num_keys=150&eek_op=generate caused exception.\r\n2017-10-07 09:44:11,530 INFO  kms-audit - UNAUTHORIZED[op=MANAGEMENT, key=k1, user=SET_KEY_MATERIAL] \r\n2017-10-07 09:44:11,532 INFO  kms-audit - OK[op=ROLL_NEW_VERSION, key=k1, user=SET_KEY_MATERIAL] UserProvidedMaterial:true NewVersion:k1@2\r\n2017-10-07 09:44:11,534 INFO  kms-audit - UNAUTHORIZED[op=MANAGEMENT, key=k1, user=SET_KEY_MATERIAL] \r\n2017-10-07 09:44:11,534 INFO  kms-audit - OK[op=INVALIDATE_CACHE, key=k1, user=SET_KEY_MATERIAL] \r\n2017-10-07 09:44:11,535 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=SET_KEY_MATERIAL] \r\n2017-10-07 09:44:11,535 WARN  KMS - User SET_KEY_MATERIAL@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/key/k1/_eek?num_keys=150&eek_op=generate caused exception.\r\n2017-10-07 09:44:11,550 INFO  kms-audit - UNAUTHORIZED[op=MANAGEMENT, key=k1, user=SET_KEY_MATERIAL] \r\n2017-10-07 09:44:11,550 INFO  kms-audit - OK[op=INVALIDATE_CACHE, key=k1, user=SET_KEY_MATERIAL] \r\n2017-10-07 09:44:11,551 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=SET_KEY_MATERIAL] \r\n2017-10-07 09:44:11,551 WARN  KMS - User SET_KEY_MATERIAL@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/key/k1/_eek?num_keys=150&eek_op=generate caused exception.\r\n2017-10-07 09:44:11,571 INFO  kms-audit - UNAUTHORIZED[op=READ, key=k1, user=GET] \r\n2017-10-07 09:44:11,571 INFO  kms-audit - OK[op=GET_KEY_VERSION, key=k1, user=GET, accessCount=1, interval=0ms] \r\n2017-10-07 09:44:11,574 INFO  kms-audit - UNAUTHORIZED[op=READ, key=k1, user=GET] \r\n2017-10-07 09:44:11,575 INFO  kms-audit - OK[op=GET_CURRENT_KEY, key=k1, user=GET, accessCount=1, interval=0ms] \r\n2017-10-07 09:44:11,598 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,599 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,613 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,613 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,616 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,616 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,616 INFO  kms-audit - OK[op=GENERATE_EEK, key=k1, user=GENERATE_EEK, accessCount=1, interval=0ms] \r\n2017-10-07 09:44:11,645 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,645 INFO  kms-audit - OK[op=REENCRYPT_EEK, key=k1, user=GENERATE_EEK, accessCount=1, interval=0ms] \r\n2017-10-07 09:44:11,648 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,648 INFO  kms-audit - OK[op=REENCRYPT_EEK_BATCH, key=k1, user=GENERATE_EEK] reencrypted 2 keys\r\n2017-10-07 09:44:11,648 INFO  KMS - reencryptEncryptedKeys 2 keys for key k1 took 347.5 ?s\r\n2017-10-07 09:44:11,674 INFO  kms-audit - UNAUTHORIZED[op=DECRYPT_EEK, key=k1, user=DECRYPT_EEK] \r\n2017-10-07 09:44:11,674 INFO  kms-audit - OK[op=DECRYPT_EEK, key=k1, user=DECRYPT_EEK, accessCount=1, interval=0ms] \r\n2017-10-07 09:44:11,698 INFO  kms-audit - OK[op=GET_KEYS, user=GET_KEYS] \r\n2017-10-07 09:44:11,721 INFO  kms-audit - UNAUTHORIZED[op=READ, key=k1, user=GET_METADATA] \r\n2017-10-07 09:44:11,722 INFO  kms-audit - OK[op=GET_METADATA, key=k1, user=GET_METADATA] \r\n2017-10-07 09:44:11,725 INFO  kms-audit - UNAUTHORIZED[op=READ, key=k1, user=GET_METADATA] \r\n2017-10-07 09:44:11,725 INFO  kms-audit - OK[op=GET_KEYS_METADATA, user=GET_METADATA] \r\n2017-10-07 09:44:12,000 INFO  SessionTrackerImpl - SessionTrackerImpl exited loop!\r\n2017-10-07 09:44:12,771 WARN  KMS - User CREATE@EXAMPLE.COM (auth:KERBEROS) request POST http://localhost:46764/kms/v1/keys caused exception.\r\n2017-10-07 09:44:12,772 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User [CREATE] is not authorized to create key !!]!!\r\n2017-10-07 09:44:12,772 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,797 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,837 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,838 INFO  log - Stopped SocketConnector@localhost:46764\r\n2017-10-07 09:44:12,840 INFO  KMSWebApp - KMS Stopped\r\n\r\n{noformat}"
        }
    },
    {
        "filename": "HADOOP-10540.json",
        "creation_time": "2014-04-11T05:14:07.000+0000",
        "stack_trace": "```\njava.io.IOException: Usage: hardlink create [LINKNAME] [FILENAME] |Incorrect command line arguments.\n\tat org.apache.hadoop.fs.HardLink.createHardLinkMult(HardLink.java:479)\n\tat org.apache.hadoop.fs.HardLink.createHardLinkMult(HardLink.java:416)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.linkBlocks(DataStorage.java:816)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.linkAllBlocks(DataStorage.java:759)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.doUpgrade(DataStorage.java:566)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:486)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:225)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:249)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:929)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:900)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:274)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:220)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:815)\n\tat java.lang.Thread.run(Thread.java:722)\n\njava.lang.Exception: trace\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:143)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.remove(BlockPoolManager.java:91)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:859)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:350)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:619)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:837)\n\tat java.lang.Thread.run(Thread.java:722)\n\njava.lang.Exception: trace\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:143)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:861)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:350)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:619)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:837)\n\tat java.lang.Thread.run(Thread.java:722)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.HardLink.createHardLinkMult": "  protected static int createHardLinkMult(File parentDir, \n      String[] fileBaseNames, File linkDir, int maxLength) \n  throws IOException {\n    if (parentDir == null) {\n      throw new IOException(\n          \"invalid arguments to createHardLinkMult: parent directory is null\");\n    }\n    if (linkDir == null) {\n      throw new IOException(\n          \"invalid arguments to createHardLinkMult: link directory is null\");\n    }\n    if (fileBaseNames == null) {\n      throw new IOException(\n          \"invalid arguments to createHardLinkMult: \"\n          + \"filename list can be empty but not null\");\n    }\n    if (fileBaseNames.length == 0) {\n      //the OS cmds can't handle empty list of filenames, \n      //but it's legal, so just return.\n      return 0; \n    }\n    if (!linkDir.exists()) {\n      throw new FileNotFoundException(linkDir + \" not found.\");\n    }\n\n    //if the list is too long, split into multiple invocations\n    int callCount = 0;\n    if (getLinkMultArgLength(parentDir, fileBaseNames, linkDir) > maxLength\n          && fileBaseNames.length > 1) {\n      String[] list1 = Arrays.copyOf(fileBaseNames, fileBaseNames.length/2);\n      callCount += createHardLinkMult(parentDir, list1, linkDir, maxLength);\n      String[] list2 = Arrays.copyOfRange(fileBaseNames, fileBaseNames.length/2,\n          fileBaseNames.length);\n      callCount += createHardLinkMult(parentDir, list2, linkDir, maxLength);  \n      return callCount;\n    } else {\n      callCount = 1;\n    }\n    \n    // construct and execute shell command\n    String[] hardLinkCommand = getHardLinkCommand.linkMult(fileBaseNames, \n        linkDir);\n    Process process = Runtime.getRuntime().exec(hardLinkCommand, null, \n        parentDir);\n    try {\n      if (process.waitFor() != 0) {\n        String errMsg = new BufferedReader(new InputStreamReader(\n            process.getInputStream())).readLine();\n        if (errMsg == null)  errMsg = \"\";\n        String inpMsg = new BufferedReader(new InputStreamReader(\n            process.getErrorStream())).readLine();\n        if (inpMsg == null)  inpMsg = \"\";\n        throw new IOException(errMsg + inpMsg);\n      }\n    } catch (InterruptedException e) {\n      throw new IOException(e);\n    } finally {\n      process.destroy();\n    }\n    return callCount;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.HardLink.getMaxAllowedCmdArgLength": "  protected static int getMaxAllowedCmdArgLength() {\n    return getHardLinkCommand.getMaxAllowedCmdArgLength();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.HardLink.linkMult": "    String[] linkMult(String[] fileBaseNames, File linkDir) \n    throws IOException {\n      String[] buf = new String[fileBaseNames.length \n                                + hardLinkMultPrefix.length \n                                + hardLinkMultSuffix.length];\n      String td = linkDir.getCanonicalPath() + hardLinkMultDir;\n      int mark=0;\n      System.arraycopy(hardLinkMultPrefix, 0, buf, mark, \n                       hardLinkMultPrefix.length);\n      mark += hardLinkMultPrefix.length;\n      System.arraycopy(fileBaseNames, 0, buf, mark, fileBaseNames.length);\n      mark += fileBaseNames.length;\n      System.arraycopy(hardLinkMultSuffix, 0, buf, mark, \n                       hardLinkMultSuffix.length);\n      mark += hardLinkMultSuffix.length;\n      buf[mark - 3] = td;\n      return buf;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.HardLink.getLinkMultArgLength": "  protected static int getLinkMultArgLength(\n          File fileDir, String[] fileBaseNames, File linkDir) \n  throws IOException {\n    return getHardLinkCommand.getLinkMultArgLength(fileDir, \n          fileBaseNames, linkDir);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataStorage.linkBlocks": "  static void linkBlocks(File from, File to, int oldLV, HardLink hl) \n  throws IOException {\n    if (!from.exists()) {\n      return;\n    }\n    if (!from.isDirectory()) {\n      if (from.getName().startsWith(COPY_FILE_PREFIX)) {\n        FileInputStream in = new FileInputStream(from);\n        try {\n          FileOutputStream out = new FileOutputStream(to);\n          try {\n            IOUtils.copyBytes(in, out, 16*1024);\n            hl.linkStats.countPhysicalFileCopies++;\n          } finally {\n            out.close();\n          }\n        } finally {\n          in.close();\n        }\n      } else {\n        HardLink.createHardLink(from, to);\n        hl.linkStats.countSingleLinks++;\n      }\n      return;\n    }\n    // from is a directory\n    hl.linkStats.countDirs++;\n    \n    if (!to.mkdirs())\n      throw new IOException(\"Cannot create directory \" + to);\n    \n    String[] blockNames = from.list(new java.io.FilenameFilter() {\n      @Override\n      public boolean accept(File dir, String name) {\n        return name.startsWith(BLOCK_FILE_PREFIX);\n      }\n    });\n\n    // Block files just need hard links with the same file names\n    // but a different directory\n    if (blockNames.length > 0) {\n      HardLink.createHardLinkMult(from, blockNames, to);\n      hl.linkStats.countMultLinks++;\n      hl.linkStats.countFilesMultLinks += blockNames.length;\n    } else {\n      hl.linkStats.countEmptyDirs++;\n    }\n    \n    // Now take care of the rest of the files and subdirectories\n    String[] otherNames = from.list(new java.io.FilenameFilter() {\n        @Override\n        public boolean accept(File dir, String name) {\n          return name.startsWith(BLOCK_SUBDIR_PREFIX) \n            || name.startsWith(COPY_FILE_PREFIX);\n        }\n      });\n    for(int i = 0; i < otherNames.length; i++)\n      linkBlocks(new File(from, otherNames[i]), \n          new File(to, otherNames[i]), oldLV, hl);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataStorage.linkAllBlocks": "  private void linkAllBlocks(File fromDir, File fromBbwDir, File toDir)\n      throws IOException {\n    HardLink hardLink = new HardLink();\n    // do the link\n    int diskLayoutVersion = this.getLayoutVersion();\n    if (DataNodeLayoutVersion.supports(\n        LayoutVersion.Feature.APPEND_RBW_DIR, diskLayoutVersion)) {\n      // hardlink finalized blocks in tmpDir/finalized\n      linkBlocks(new File(fromDir, STORAGE_DIR_FINALIZED), \n          new File(toDir, STORAGE_DIR_FINALIZED), diskLayoutVersion, hardLink);\n      // hardlink rbw blocks in tmpDir/rbw\n      linkBlocks(new File(fromDir, STORAGE_DIR_RBW), \n          new File(toDir, STORAGE_DIR_RBW), diskLayoutVersion, hardLink);\n    } else { // pre-RBW version\n      // hardlink finalized blocks in tmpDir\n      linkBlocks(fromDir, new File(toDir, STORAGE_DIR_FINALIZED), \n          diskLayoutVersion, hardLink);      \n      if (fromBbwDir.exists()) {\n        /*\n         * We need to put the 'blocksBeingWritten' from HDFS 1.x into the rbw\n         * directory.  It's a little messy, because the blocksBeingWriten was\n         * NOT underneath the 'current' directory in those releases.  See\n         * HDFS-3731 for details.\n         */\n        linkBlocks(fromBbwDir,\n            new File(toDir, STORAGE_DIR_RBW), diskLayoutVersion, hardLink);\n      }\n    } \n    LOG.info( hardLink.linkStats.report() );\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataStorage.doUpgrade": "  void doUpgrade(StorageDirectory sd, NamespaceInfo nsInfo) throws IOException {\n    // If the existing on-disk layout version supportes federation, simply\n    // update its layout version.\n    if (DataNodeLayoutVersion.supports(\n        LayoutVersion.Feature.FEDERATION, layoutVersion)) {\n      // The VERSION file is already read in. Override the layoutVersion \n      // field and overwrite the file.\n      LOG.info(\"Updating layout version from \" + layoutVersion + \" to \"\n          + HdfsConstants.DATANODE_LAYOUT_VERSION + \" for storage \"\n          + sd.getRoot());\n      layoutVersion = HdfsConstants.DATANODE_LAYOUT_VERSION;\n      writeProperties(sd);\n      return;\n    }\n    \n    LOG.info(\"Upgrading storage directory \" + sd.getRoot()\n             + \".\\n   old LV = \" + this.getLayoutVersion()\n             + \"; old CTime = \" + this.getCTime()\n             + \".\\n   new LV = \" + HdfsConstants.DATANODE_LAYOUT_VERSION\n             + \"; new CTime = \" + nsInfo.getCTime());\n    \n    File curDir = sd.getCurrentDir();\n    File prevDir = sd.getPreviousDir();\n    File bbwDir = new File(sd.getRoot(), Storage.STORAGE_1_BBW);\n\n    assert curDir.exists() : \"Data node current directory must exist.\";\n    // Cleanup directory \"detach\"\n    cleanupDetachDir(new File(curDir, STORAGE_DIR_DETACHED));\n    \n    // 1. delete <SD>/previous dir before upgrading\n    if (prevDir.exists())\n      deleteDir(prevDir);\n    // get previous.tmp directory, <SD>/previous.tmp\n    File tmpDir = sd.getPreviousTmp();\n    assert !tmpDir.exists() : \n      \"Data node previous.tmp directory must not exist.\";\n    \n    // 2. Rename <SD>/current to <SD>/previous.tmp\n    rename(curDir, tmpDir);\n    \n    // 3. Format BP and hard link blocks from previous directory\n    File curBpDir = BlockPoolSliceStorage.getBpRoot(nsInfo.getBlockPoolID(), curDir);\n    BlockPoolSliceStorage bpStorage = new BlockPoolSliceStorage(nsInfo.getNamespaceID(), \n        nsInfo.getBlockPoolID(), nsInfo.getCTime(), nsInfo.getClusterID());\n    bpStorage.format(curDir, nsInfo);\n    linkAllBlocks(tmpDir, bbwDir, new File(curBpDir, STORAGE_DIR_CURRENT));\n    \n    // 4. Write version file under <SD>/current\n    layoutVersion = HdfsConstants.DATANODE_LAYOUT_VERSION;\n    clusterID = nsInfo.getClusterID();\n    writeProperties(sd);\n    \n    // 5. Rename <SD>/previous.tmp to <SD>/previous\n    rename(tmpDir, prevDir);\n    LOG.info(\"Upgrade of \" + sd.getRoot()+ \" is complete\");\n    addBlockPoolStorage(nsInfo.getBlockPoolID(), bpStorage);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataStorage.cleanupDetachDir": "  private void cleanupDetachDir(File detachDir) throws IOException {\n    if (!DataNodeLayoutVersion.supports(\n        LayoutVersion.Feature.APPEND_RBW_DIR, layoutVersion) &&\n        detachDir.exists() && detachDir.isDirectory() ) {\n      \n        if (FileUtil.list(detachDir).length != 0 ) {\n          throw new IOException(\"Detached directory \" + detachDir +\n              \" is not empty. Please manually move each file under this \" +\n              \"directory to the finalized directory if the finalized \" +\n              \"directory tree does not have the file.\");\n        } else if (!detachDir.delete()) {\n          throw new IOException(\"Cannot remove directory \" + detachDir);\n        }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataStorage.addBlockPoolStorage": "  private void addBlockPoolStorage(String bpID, BlockPoolSliceStorage bpStorage\n      ) {\n    if (!this.bpStorageMap.containsKey(bpID)) {\n      this.bpStorageMap.put(bpID, bpStorage);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataStorage.format": "  void format(StorageDirectory sd, NamespaceInfo nsInfo,\n              String datanodeUuid) throws IOException {\n    sd.clearDirectory(); // create directory\n    this.layoutVersion = HdfsConstants.DATANODE_LAYOUT_VERSION;\n    this.clusterID = nsInfo.getClusterID();\n    this.namespaceID = nsInfo.getNamespaceID();\n    this.cTime = 0;\n    this.datanodeUuid = datanodeUuid;\n\n    if (sd.getStorageUuid() == null) {\n      // Assign a new Storage UUID.\n      sd.setStorageUuid(DatanodeStorage.generateUuid());\n    }\n\n    writeProperties(sd);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition": "  private void doTransition( DataNode datanode,\n                             StorageDirectory sd, \n                             NamespaceInfo nsInfo, \n                             StartupOption startOpt\n                             ) throws IOException {\n    if (startOpt == StartupOption.ROLLBACK) {\n      doRollback(sd, nsInfo); // rollback if applicable\n    }\n    readProperties(sd);\n    checkVersionUpgradable(this.layoutVersion);\n    assert this.layoutVersion >= HdfsConstants.DATANODE_LAYOUT_VERSION :\n      \"Future version is not allowed\";\n    \n    boolean federationSupported = \n      DataNodeLayoutVersion.supports(\n          LayoutVersion.Feature.FEDERATION, layoutVersion);\n    // For pre-federation version - validate the namespaceID\n    if (!federationSupported &&\n        getNamespaceID() != nsInfo.getNamespaceID()) {\n      throw new IOException(\"Incompatible namespaceIDs in \"\n          + sd.getRoot().getCanonicalPath() + \": namenode namespaceID = \"\n          + nsInfo.getNamespaceID() + \"; datanode namespaceID = \"\n          + getNamespaceID());\n    }\n    \n    // For version that supports federation, validate clusterID\n    if (federationSupported\n        && !getClusterID().equals(nsInfo.getClusterID())) {\n      throw new IOException(\"Incompatible clusterIDs in \"\n          + sd.getRoot().getCanonicalPath() + \": namenode clusterID = \"\n          + nsInfo.getClusterID() + \"; datanode clusterID = \" + getClusterID());\n    }\n    \n    // After addition of the federation feature, ctime check is only \n    // meaningful at BlockPoolSliceStorage level. \n\n    // regular start up. \n    if (this.layoutVersion == HdfsConstants.DATANODE_LAYOUT_VERSION)\n      return; // regular startup\n    \n    // do upgrade\n    if (this.layoutVersion > HdfsConstants.DATANODE_LAYOUT_VERSION) {\n      doUpgrade(sd, nsInfo);  // upgrade\n      return;\n    }\n    \n    // layoutVersion < DATANODE_LAYOUT_VERSION. I.e. stored layout version is newer\n    // than the version supported by datanode. This should have been caught\n    // in readProperties(), even if rollback was not carried out or somehow\n    // failed.\n    throw new IOException(\"BUG: The stored LV = \" + this.getLayoutVersion()\n        + \" is newer than the supported LV = \"\n        + HdfsConstants.DATANODE_LAYOUT_VERSION);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataStorage.readProperties": "  void readProperties(StorageDirectory sd, int rollbackLayoutVersion)\n      throws IOException {\n    Properties props = readPropertiesFile(sd.getVersionFile());\n    setFieldsFromProperties(props, sd, true, rollbackLayoutVersion);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataStorage.doRollback": "  void doRollback( StorageDirectory sd,\n                   NamespaceInfo nsInfo\n                   ) throws IOException {\n    File prevDir = sd.getPreviousDir();\n    // This is a regular startup or a post-federation rollback\n    if (!prevDir.exists()) {\n      if (DataNodeLayoutVersion.supports(LayoutVersion.Feature.FEDERATION,\n          HdfsConstants.DATANODE_LAYOUT_VERSION)) {\n        readProperties(sd, HdfsConstants.DATANODE_LAYOUT_VERSION);\n        writeProperties(sd);\n        LOG.info(\"Layout version rolled back to \"\n            + HdfsConstants.DATANODE_LAYOUT_VERSION + \" for storage \"\n            + sd.getRoot());\n      }\n      return;\n    }\n    DataStorage prevInfo = new DataStorage();\n    prevInfo.readPreviousVersionProperties(sd);\n\n    // We allow rollback to a state, which is either consistent with\n    // the namespace state or can be further upgraded to it.\n    if (!(prevInfo.getLayoutVersion() >= HdfsConstants.DATANODE_LAYOUT_VERSION\n          && prevInfo.getCTime() <= nsInfo.getCTime()))  // cannot rollback\n      throw new InconsistentFSStateException(sd.getRoot(),\n          \"Cannot rollback to a newer state.\\nDatanode previous state: LV = \"\n              + prevInfo.getLayoutVersion() + \" CTime = \" + prevInfo.getCTime()\n              + \" is newer than the namespace state: LV = \"\n              + HdfsConstants.DATANODE_LAYOUT_VERSION + \" CTime = \"\n              + nsInfo.getCTime());\n    LOG.info(\"Rolling back storage directory \" + sd.getRoot()\n        + \".\\n   target LV = \" + HdfsConstants.DATANODE_LAYOUT_VERSION\n        + \"; target CTime = \" + nsInfo.getCTime());\n    File tmpDir = sd.getRemovedTmp();\n    assert !tmpDir.exists() : \"removed.tmp directory must not exist.\";\n    // rename current to tmp\n    File curDir = sd.getCurrentDir();\n    assert curDir.exists() : \"Current directory must exist.\";\n    rename(curDir, tmpDir);\n    // rename previous to current\n    rename(prevDir, curDir);\n    // delete tmp dir\n    deleteDir(tmpDir);\n    LOG.info(\"Rollback of \" + sd.getRoot() + \" is complete\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead": "  void recoverTransitionRead(DataNode datanode, String bpID, NamespaceInfo nsInfo,\n      Collection<StorageLocation> dataDirs, StartupOption startOpt) throws IOException {\n    // First ensure datanode level format/snapshot/rollback is completed\n    recoverTransitionRead(datanode, nsInfo, dataDirs, startOpt);\n\n    // Create list of storage directories for the block pool\n    Collection<File> bpDataDirs = new ArrayList<File>();\n    for(StorageLocation dir : dataDirs) {\n      File dnRoot = dir.getFile();\n      File bpRoot = BlockPoolSliceStorage.getBpRoot(bpID, new File(dnRoot,\n          STORAGE_DIR_CURRENT));\n      bpDataDirs.add(bpRoot);\n    }\n    // mkdir for the list of BlockPoolStorage\n    makeBlockPoolDataDir(bpDataDirs, null);\n    BlockPoolSliceStorage bpStorage = new BlockPoolSliceStorage(\n        nsInfo.getNamespaceID(), bpID, nsInfo.getCTime(), nsInfo.getClusterID());\n    \n    bpStorage.recoverTransitionRead(datanode, nsInfo, bpDataDirs, startOpt);\n    addBlockPoolStorage(bpID, bpStorage);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataStorage.makeBlockPoolDataDir": "  static void makeBlockPoolDataDir(Collection<File> dataDirs,\n      Configuration conf) throws IOException {\n    if (conf == null)\n      conf = new HdfsConfiguration();\n\n    LocalFileSystem localFS = FileSystem.getLocal(conf);\n    FsPermission permission = new FsPermission(conf.get(\n        DFSConfigKeys.DFS_DATANODE_DATA_DIR_PERMISSION_KEY,\n        DFSConfigKeys.DFS_DATANODE_DATA_DIR_PERMISSION_DEFAULT));\n    for (File data : dataDirs) {\n      try {\n        DiskChecker.checkDir(localFS, new Path(data.toURI()), permission);\n      } catch ( IOException e ) {\n        LOG.warn(\"Invalid directory in: \" + data.getCanonicalPath() + \": \"\n            + e.getMessage());\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataStorage.createStorageID": "  public synchronized void createStorageID(StorageDirectory sd) {\n    if (sd.getStorageUuid() == null) {\n      sd.setStorageUuid(DatanodeStorage.generateUuid());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataStorage.getDatanodeUuid": "  public synchronized String getDatanodeUuid() {\n    return datanodeUuid;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage": "  private void initStorage(final NamespaceInfo nsInfo) throws IOException {\n    final FsDatasetSpi.Factory<? extends FsDatasetSpi<?>> factory\n        = FsDatasetSpi.Factory.getFactory(conf);\n    \n    if (!factory.isSimulated()) {\n      final StartupOption startOpt = getStartupOption(conf);\n      if (startOpt == null) {\n        throw new IOException(\"Startup option not set.\");\n      }\n      final String bpid = nsInfo.getBlockPoolID();\n      //read storage info, lock data dirs and transition fs state if necessary\n      storage.recoverTransitionRead(this, bpid, nsInfo, dataDirs, startOpt);\n      final StorageInfo bpStorage = storage.getBPStorage(bpid);\n      LOG.info(\"Setting up storage: nsid=\" + bpStorage.getNamespaceID()\n          + \";bpid=\" + bpid + \";lv=\" + storage.getLayoutVersion()\n          + \";nsInfo=\" + nsInfo + \";dnuuid=\" + storage.getDatanodeUuid());\n    }\n\n    // If this is a newly formatted DataNode then assign a new DatanodeUuid.\n    checkDatanodeUuid();\n\n    synchronized(this)  {\n      if (data == null) {\n        data = factory.newInstance(this, storage, conf);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getDatanodeUuid": "  public String getDatanodeUuid() {\n    return id == null ? null : id.getDatanodeUuid();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.checkDatanodeUuid": "  private synchronized void checkDatanodeUuid() throws IOException {\n    if (storage.getDatanodeUuid() == null) {\n      storage.setDatanodeUuid(generateUuid());\n      storage.writeAll();\n      LOG.info(\"Generated and persisted new Datanode UUID \" +\n               storage.getDatanodeUuid());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getStartupOption": "  static StartupOption getStartupOption(Configuration conf) {\n    String value = conf.get(DFS_DATANODE_STARTUP_KEY,\n                            StartupOption.REGULAR.toString());\n    return StartupOption.getEnum(value);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool": "  void initBlockPool(BPOfferService bpos) throws IOException {\n    NamespaceInfo nsInfo = bpos.getNamespaceInfo();\n    if (nsInfo == null) {\n      throw new IOException(\"NamespaceInfo not found: Block pool \" + bpos\n          + \" should have retrieved namespace info before initBlockPool.\");\n    }\n    \n    // Register the new block pool with the BP manager.\n    blockPoolManager.addBlockPool(bpos);\n\n    setClusterId(nsInfo.clusterID, nsInfo.getBlockPoolID());\n    \n    // In the case that this is the first block pool to connect, initialize\n    // the dataset, block scanners, etc.\n    initStorage(nsInfo);\n    initPeriodicScanners(conf);\n    \n    data.addBlockPool(nsInfo.getBlockPoolID(), conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.initPeriodicScanners": "  private void initPeriodicScanners(Configuration conf) {\n    initDataBlockScanner(conf);\n    initDirectoryScanner(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.setClusterId": "  private synchronized void setClusterId(final String nsCid, final String bpid\n      ) throws IOException {\n    if(clusterId != null && !clusterId.equals(nsCid)) {\n      throw new IOException (\"Cluster IDs not matched: dn cid=\" + clusterId \n          + \" but ns cid=\"+ nsCid + \"; bpid=\" + bpid);\n    }\n    // else\n    clusterId = nsCid;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo": "  synchronized void verifyAndSetNamespaceInfo(NamespaceInfo nsInfo) throws IOException {\n    if (this.bpNSInfo == null) {\n      this.bpNSInfo = nsInfo;\n      boolean success = false;\n\n      // Now that we know the namespace ID, etc, we can pass this to the DN.\n      // The DN can now initialize its local storage if we are the\n      // first BP to handshake, etc.\n      try {\n        dn.initBlockPool(this);\n        success = true;\n      } finally {\n        if (!success) {\n          // The datanode failed to initialize the BP. We need to reset\n          // the namespace info so that other BPService actors still have\n          // a chance to set it, and re-initialize the datanode.\n          this.bpNSInfo = null;\n        }\n      }\n    } else {\n      checkNSEquality(bpNSInfo.getBlockPoolID(), nsInfo.getBlockPoolID(),\n          \"Blockpool ID\");\n      checkNSEquality(bpNSInfo.getNamespaceID(), nsInfo.getNamespaceID(),\n          \"Namespace ID\");\n      checkNSEquality(bpNSInfo.getClusterID(), nsInfo.getClusterID(),\n          \"Cluster ID\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPOfferService.checkNSEquality": "  private static void checkNSEquality(\n      Object ourID, Object theirID,\n      String idHelpText) throws IOException {\n    if (!ourID.equals(theirID)) {\n      throw new IOException(idHelpText + \" mismatch: \" +\n          \"previously connected to \" + idHelpText + \" \" + ourID + \n          \" but now connected to \" + idHelpText + \" \" + theirID);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake": "  private void connectToNNAndHandshake() throws IOException {\n    // get NN proxy\n    bpNamenode = dn.connectToNN(nnAddr);\n\n    // First phase of the handshake with NN - get the namespace\n    // info.\n    NamespaceInfo nsInfo = retrieveNamespaceInfo();\n    \n    // Verify that this matches the other NN in this HA pair.\n    // This also initializes our block pool in the DN if we are\n    // the first NN connection for this BP.\n    bpos.verifyAndSetNamespaceInfo(nsInfo);\n    \n    // Second phase of the handshake with the NN.\n    register();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo": "  NamespaceInfo retrieveNamespaceInfo() throws IOException {\n    NamespaceInfo nsInfo = null;\n    while (shouldRun()) {\n      try {\n        nsInfo = bpNamenode.versionRequest();\n        LOG.debug(this + \" received versionRequest response: \" + nsInfo);\n        break;\n      } catch(SocketTimeoutException e) {  // namenode is busy\n        LOG.warn(\"Problem connecting to server: \" + nnAddr);\n      } catch(IOException e ) {  // namenode is not available\n        LOG.warn(\"Problem connecting to server: \" + nnAddr);\n      }\n      \n      // try again in a second\n      sleepAndLogInterrupts(5000, \"requesting version info from NN\");\n    }\n    \n    if (nsInfo != null) {\n      checkNNVersion(nsInfo);\n    } else {\n      throw new IOException(\"DN shut down before block pool connected\");\n    }\n    return nsInfo;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.register": "  void register() throws IOException {\n    // The handshake() phase loaded the block pool storage\n    // off disk - so update the bpRegistration object from that info\n    bpRegistration = bpos.createRegistration();\n\n    LOG.info(this + \" beginning handshake with NN\");\n\n    while (shouldRun()) {\n      try {\n        // Use returned registration from namenode with updated fields\n        bpRegistration = bpNamenode.registerDatanode(bpRegistration);\n        break;\n      } catch(SocketTimeoutException e) {  // namenode is busy\n        LOG.info(\"Problem connecting to server: \" + nnAddr);\n        sleepAndLogInterrupts(1000, \"connecting to server\");\n      }\n    }\n    \n    LOG.info(\"Block pool \" + this + \" successfully registered with NN\");\n    bpos.registrationSucceeded(this, bpRegistration);\n\n    // random short delay - helps scatter the BR from all DNs\n    scheduleBlockReport(dnConf.initialBlockReportDelay);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run": "  public void run() {\n    LOG.info(this + \" starting to offer service\");\n\n    try {\n      // init stuff\n      try {\n        // setup storage\n        connectToNNAndHandshake();\n      } catch (IOException ioe) {\n        // Initial handshake, storage recovery or registration failed\n        // End BPOfferService thread\n        LOG.fatal(\"Initialization failed for block pool \" + this, ioe);\n        return;\n      }\n\n      initialized = true; // bp is initialized;\n      \n      while (shouldRun()) {\n        try {\n          offerService();\n        } catch (Exception ex) {\n          LOG.error(\"Exception in BPOfferService for \" + this, ex);\n          sleepAndLogInterrupts(5000, \"offering service\");\n        }\n      }\n    } catch (Throwable ex) {\n      LOG.warn(\"Unexpected exception in block pool \" + this, ex);\n    } finally {\n      LOG.warn(\"Ending block pool service for: \" + this);\n      cleanUp();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp": "  private synchronized void cleanUp() {\n    \n    shouldServiceRun = false;\n    IOUtils.cleanup(LOG, bpNamenode);\n    bpos.shutdownActor(this);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.shouldRun": "  private boolean shouldRun() {\n    return shouldServiceRun && dn.shouldRun();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sleepAndLogInterrupts": "  private void sleepAndLogInterrupts(int millis,\n      String stateString) {\n    try {\n      Thread.sleep(millis);\n    } catch (InterruptedException ie) {\n      LOG.info(\"BPOfferService \" + this + \" interrupted while \" + stateString);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService": "  private void offerService() throws Exception {\n    LOG.info(\"For namenode \" + nnAddr + \" using\"\n        + \" DELETEREPORT_INTERVAL of \" + dnConf.deleteReportInterval + \" msec \"\n        + \" BLOCKREPORT_INTERVAL of \" + dnConf.blockReportInterval + \"msec\"\n        + \" CACHEREPORT_INTERVAL of \" + dnConf.cacheReportInterval + \"msec\"\n        + \" Initial delay: \" + dnConf.initialBlockReportDelay + \"msec\"\n        + \"; heartBeatInterval=\" + dnConf.heartBeatInterval);\n\n    //\n    // Now loop for a long time....\n    //\n    while (shouldRun()) {\n      try {\n        final long startTime = now();\n\n        //\n        // Every so often, send heartbeat or block-report\n        //\n        if (startTime - lastHeartbeat >= dnConf.heartBeatInterval) {\n          //\n          // All heartbeat messages include following info:\n          // -- Datanode name\n          // -- data transfer port\n          // -- Total capacity\n          // -- Bytes remaining\n          //\n          lastHeartbeat = startTime;\n          if (!dn.areHeartbeatsDisabledForTests()) {\n            HeartbeatResponse resp = sendHeartBeat();\n            assert resp != null;\n            dn.getMetrics().addHeartbeat(now() - startTime);\n\n            // If the state of this NN has changed (eg STANDBY->ACTIVE)\n            // then let the BPOfferService update itself.\n            //\n            // Important that this happens before processCommand below,\n            // since the first heartbeat to a new active might have commands\n            // that we should actually process.\n            bpos.updateActorStatesFromHeartbeat(\n                this, resp.getNameNodeHaState());\n            state = resp.getNameNodeHaState().getState();\n\n            if (state == HAServiceState.ACTIVE) {\n              handleRollingUpgradeStatus(resp);\n            }\n\n            long startProcessCommands = now();\n            if (!processCommand(resp.getCommands()))\n              continue;\n            long endProcessCommands = now();\n            if (endProcessCommands - startProcessCommands > 2000) {\n              LOG.info(\"Took \" + (endProcessCommands - startProcessCommands)\n                  + \"ms to process \" + resp.getCommands().length\n                  + \" commands from NN\");\n            }\n          }\n        }\n        if (sendImmediateIBR ||\n            (startTime - lastDeletedReport > dnConf.deleteReportInterval)) {\n          reportReceivedDeletedBlocks();\n          lastDeletedReport = startTime;\n        }\n\n        List<DatanodeCommand> cmds = blockReport();\n        processCommand(cmds == null ? null : cmds.toArray(new DatanodeCommand[cmds.size()]));\n\n        DatanodeCommand cmd = cacheReport();\n        processCommand(new DatanodeCommand[]{ cmd });\n\n        // Now safe to start scanning the block pool.\n        // If it has already been started, this is a no-op.\n        if (dn.blockScanner != null) {\n          dn.blockScanner.addBlockPool(bpos.getBlockPoolId());\n        }\n\n        //\n        // There is no work to do;  sleep until hearbeat timer elapses, \n        // or work arrives, and then iterate again.\n        //\n        long waitTime = dnConf.heartBeatInterval - \n        (Time.now() - lastHeartbeat);\n        synchronized(pendingIncrementalBRperStorage) {\n          if (waitTime > 0 && !sendImmediateIBR) {\n            try {\n              pendingIncrementalBRperStorage.wait(waitTime);\n            } catch (InterruptedException ie) {\n              LOG.warn(\"BPOfferService for \" + this + \" interrupted\");\n            }\n          }\n        } // synchronized\n      } catch(RemoteException re) {\n        String reClass = re.getClassName();\n        if (UnregisteredNodeException.class.getName().equals(reClass) ||\n            DisallowedDatanodeException.class.getName().equals(reClass) ||\n            IncorrectVersionException.class.getName().equals(reClass)) {\n          LOG.warn(this + \" is shutting down\", re);\n          shouldServiceRun = false;\n          return;\n        }\n        LOG.warn(\"RemoteException in offerService\", re);\n        try {\n          long sleepTime = Math.min(1000, dnConf.heartBeatInterval);\n          Thread.sleep(sleepTime);\n        } catch (InterruptedException ie) {\n          Thread.currentThread().interrupt();\n        }\n      } catch (IOException e) {\n        LOG.warn(\"IOException in offerService\", e);\n      }\n    } // while (shouldRun())\n  } // offerService",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId": "  synchronized String getBlockPoolId() {\n    if (bpNSInfo != null) {\n      return bpNSInfo.getBlockPoolID();\n    } else {\n      LOG.warn(\"Block pool ID needed, but service not yet registered with NN\",\n          new Exception(\"trace\"));\n      return null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.remove": "  synchronized void remove(BPOfferService t) {\n    offerServices.remove(t);\n    bpByBlockPoolId.remove(t.getBlockPoolId());\n    \n    boolean removed = false;\n    for (Iterator<BPOfferService> it = bpByNameserviceId.values().iterator();\n         it.hasNext() && !removed;) {\n      BPOfferService bpos = it.next();\n      if (bpos == t) {\n        it.remove();\n        LOG.info(\"Removed \" + bpos);\n        removed = true;\n      }\n    }\n    \n    if (!removed) {\n      LOG.warn(\"Couldn't remove BPOS \" + t + \" from bpByNameserviceId map\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool": "  void shutdownBlockPool(BPOfferService bpos) {\n    blockPoolManager.remove(bpos);\n\n    String bpId = bpos.getBlockPoolId();\n    if (blockScanner != null) {\n      blockScanner.removeBlockPool(bpId);\n    }\n  \n    if (data != null) { \n      data.shutdownBlockPool(bpId);\n    }\n\n    if (storage != null) {\n      storage.removeBlockPoolStorage(bpId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor": "  synchronized void shutdownActor(BPServiceActor actor) {\n    if (bpServiceToActive == actor) {\n      bpServiceToActive = null;\n    }\n\n    bpServices.remove(actor);\n\n    if (bpServices.isEmpty()) {\n      dn.shutdownBlockPool(this);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion.supports": "  public static boolean supports(final LayoutFeature f, final int lv) {\n    return LayoutVersion.supports(FEATURES, f, lv);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.getBpRoot": "  public static File getBpRoot(String bpID, File dnCurDir) {\n    return new File(dnCurDir, bpID);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockPoolManager.addBlockPool": "  synchronized void addBlockPool(BPOfferService bpos) {\n    Preconditions.checkArgument(offerServices.contains(bpos),\n        \"Unknown BPOS: %s\", bpos);\n    if (bpos.getBlockPoolId() == null) {\n      throw new IllegalArgumentException(\"Null blockpool id\");\n    }\n    bpByBlockPoolId.put(bpos.getBlockPoolId(), bpos);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPOfferService.getNamespaceInfo": "  synchronized NamespaceInfo getNamespaceInfo() {\n    return bpNSInfo;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockPoolManager.remove": "  synchronized void remove(BPOfferService t) {\n    offerServices.remove(t);\n    bpByBlockPoolId.remove(t.getBlockPoolId());\n    \n    boolean removed = false;\n    for (Iterator<BPOfferService> it = bpByNameserviceId.values().iterator();\n         it.hasNext() && !removed;) {\n      BPOfferService bpos = it.next();\n      if (bpos == t) {\n        it.remove();\n        LOG.info(\"Removed \" + bpos);\n        removed = true;\n      }\n    }\n    \n    if (!removed) {\n      LOG.warn(\"Couldn't remove BPOS \" + t + \" from bpByNameserviceId map\");\n    }\n  }"
        },
        "bug_report": {
            "Title": "Datanode upgrade in Windows fails with hardlink error.",
            "Description": "I try to upgrade Hadoop from 1.x and 2.4, but DataNode failed to start due to hard link exception.\nRepro steps:\n*Installed Hadoop 1.x\n*hadoop dfsadmin -safemode enter\n*hadoop dfsadmin -saveNamespace\n*hadoop namenode -finalize\n*Stop all services\n*Uninstall Hadoop 1.x \n*Install Hadoop 2.4 \n*Start namenode with -upgrade option\n*Try to start datanode, begin to see Hardlink exception in datanode service log.\n\n{code}\n\n2014-04-10 22:47:11,655 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8010: starting\n2014-04-10 22:47:11,656 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting\n2014-04-10 22:47:11,999 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -56\n2014-04-10 22:47:12,008 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on d:\\hadoop\\data\\hdfs\\dn\\in_use.lock acquired by nodename 7268@myhost\n2014-04-10 22:47:12,011 INFO org.apache.hadoop.hdfs.server.common.Storage: Recovering storage directory D:\\hadoop\\data\\hdfs\\dn from previous upgrade\n2014-04-10 22:47:12,017 INFO org.apache.hadoop.hdfs.server.common.Storage: Upgrading storage directory d:\\hadoop\\data\\hdfs\\dn.\n   old LV = -44; old CTime = 0.\n   new LV = -55; new CTime = 1397168400373\n2014-04-10 22:47:12,021 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-39008719-10.0.0.1-1397168400092 directory d:\\hadoop\\data\\hdfs\\dn\\current\\BP-39008719-10.0.0.1-1397168400092\\current\n2014-04-10 22:47:12,254 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool <registering> (Datanode Uuid unassigned) service to myhost/10.0.0.1:8020\njava.io.IOException: Usage: hardlink create [LINKNAME] [FILENAME] |Incorrect command line arguments.\n\tat org.apache.hadoop.fs.HardLink.createHardLinkMult(HardLink.java:479)\n\tat org.apache.hadoop.fs.HardLink.createHardLinkMult(HardLink.java:416)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.linkBlocks(DataStorage.java:816)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.linkAllBlocks(DataStorage.java:759)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.doUpgrade(DataStorage.java:566)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:486)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:225)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:249)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:929)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:900)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:274)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:220)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:815)\n\tat java.lang.Thread.run(Thread.java:722)\n2014-04-10 22:47:12,258 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to myhost/10.0.0.1:8020\n2014-04-10 22:47:12,359 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN\njava.lang.Exception: trace\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:143)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.remove(BlockPoolManager.java:91)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:859)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:350)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:619)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:837)\n\tat java.lang.Thread.run(Thread.java:722)\n2014-04-10 22:47:12,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)\n2014-04-10 22:47:12,360 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN\njava.lang.Exception: trace\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:143)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:861)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:350)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:619)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:837)\n\tat java.lang.Thread.run(Thread.java:722)\n2014-04-10 22:47:14,360 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode\n2014-04-10 22:47:14,361 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0\n2014-04-10 22:47:14,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at myhost/10.0.0.1\n************************************************************/\n{code}"
        }
    },
    {
        "filename": "HADOOP-7629.json",
        "creation_time": "2011-09-09T17:45:14.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:115)\n        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:53)\n        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:236)\n        at org.apache.hadoop.ipc.RPC$Invocation.readFields(RPC.java:104)\n        at org.apache.hadoop.ipc.Server$Connection.processData(Server.java:1337)\n        at org.apache.hadoop.ipc.Server$Connection.processOneRpc(Server.java:1315)\n        at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1215)\n        at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:566)\n        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:363)\nCaused by: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()\n        at java.lang.Class.getConstructor0(Class.java:2706)\n        at java.lang.Class.getDeclaredConstructor(Class.java:1985)\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:109)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ReflectionUtils.newInstance": "  public static <T> T newInstance(Class<T> theClass, Configuration conf) {\n    T result;\n    try {\n      Constructor<T> meth = (Constructor<T>) CONSTRUCTOR_CACHE.get(theClass);\n      if (meth == null) {\n        meth = theClass.getDeclaredConstructor(EMPTY_ARRAY);\n        meth.setAccessible(true);\n        CONSTRUCTOR_CACHE.put(theClass, meth);\n      }\n      result = meth.newInstance();\n    } catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n    setConf(result, conf);\n    return result;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ReflectionUtils.setConf": "  public static void setConf(Object theObject, Configuration conf) {\n    if (conf != null) {\n      if (theObject instanceof Configurable) {\n        ((Configurable) theObject).setConf(conf);\n      }\n      setJobConf(theObject, conf);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.WritableFactories.newInstance": "  public static Writable newInstance(Class<? extends Writable> c) {\n    return newInstance(c, null);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.WritableFactories.getFactory": "  public static synchronized WritableFactory getFactory(Class c) {\n    return CLASS_TO_FACTORY.get(c);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.ObjectWritable.readObject": "  public static Object readObject(DataInput in, ObjectWritable objectWritable, Configuration conf)\n    throws IOException {\n    String className = UTF8.readString(in);\n    Class<?> declaredClass = PRIMITIVE_NAMES.get(className);\n    if (declaredClass == null) {\n      declaredClass = loadClass(conf, className);\n    }\n    \n    Object instance;\n    \n    if (declaredClass.isPrimitive()) {            // primitive types\n\n      if (declaredClass == Boolean.TYPE) {             // boolean\n        instance = Boolean.valueOf(in.readBoolean());\n      } else if (declaredClass == Character.TYPE) {    // char\n        instance = Character.valueOf(in.readChar());\n      } else if (declaredClass == Byte.TYPE) {         // byte\n        instance = Byte.valueOf(in.readByte());\n      } else if (declaredClass == Short.TYPE) {        // short\n        instance = Short.valueOf(in.readShort());\n      } else if (declaredClass == Integer.TYPE) {      // int\n        instance = Integer.valueOf(in.readInt());\n      } else if (declaredClass == Long.TYPE) {         // long\n        instance = Long.valueOf(in.readLong());\n      } else if (declaredClass == Float.TYPE) {        // float\n        instance = Float.valueOf(in.readFloat());\n      } else if (declaredClass == Double.TYPE) {       // double\n        instance = Double.valueOf(in.readDouble());\n      } else if (declaredClass == Void.TYPE) {         // void\n        instance = null;\n      } else {\n        throw new IllegalArgumentException(\"Not a primitive: \"+declaredClass);\n      }\n\n    } else if (declaredClass.isArray()) {              // array\n      int length = in.readInt();\n      instance = Array.newInstance(declaredClass.getComponentType(), length);\n      for (int i = 0; i < length; i++) {\n        Array.set(instance, i, readObject(in, conf));\n      }\n      \n    } else if (declaredClass == ArrayPrimitiveWritable.Internal.class) {\n      // Read and unwrap ArrayPrimitiveWritable$Internal array.\n      // Always allow the read, even if write is disabled by allowCompactArrays.\n      ArrayPrimitiveWritable.Internal temp = \n          new ArrayPrimitiveWritable.Internal();\n      temp.readFields(in);\n      instance = temp.get();\n      declaredClass = instance.getClass();\n\n    } else if (declaredClass == String.class) {        // String\n      instance = UTF8.readString(in);\n    } else if (declaredClass.isEnum()) {         // enum\n      instance = Enum.valueOf((Class<? extends Enum>) declaredClass, UTF8.readString(in));\n    } else if (Message.class.isAssignableFrom(declaredClass)) {\n      instance = tryInstantiateProtobuf(declaredClass, in);\n    } else {                                      // Writable\n      Class instanceClass = null;\n      String str = UTF8.readString(in);\n      instanceClass = loadClass(conf, str);\n      \n      Writable writable = WritableFactories.newInstance(instanceClass, conf);\n      writable.readFields(in);\n      instance = writable;\n\n      if (instanceClass == NullInstance.class) {  // null\n        declaredClass = ((NullInstance)instance).declaredClass;\n        instance = null;\n      }\n    }\n\n    if (objectWritable != null) {                 // store values\n      objectWritable.declaredClass = declaredClass;\n      objectWritable.instance = instance;\n    }\n\n    return instance;\n      \n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.ObjectWritable.loadClass": "  public static Class<?> loadClass(Configuration conf, String className) {\n    Class<?> declaredClass = null;\n    try {\n      if (conf != null)\n        declaredClass = conf.getClassByName(className);\n      else\n        declaredClass = Class.forName(className);\n    } catch (ClassNotFoundException e) {\n      throw new RuntimeException(\"readObject can't find class \" + className,\n          e);\n    }\n    return declaredClass;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.ObjectWritable.set": "  public void set(Object instance) {\n    this.declaredClass = instance.getClass();\n    this.instance = instance;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.ObjectWritable.readFields": "    public void readFields(DataInput in) throws IOException {\n      String className = UTF8.readString(in);\n      declaredClass = PRIMITIVE_NAMES.get(className);\n      if (declaredClass == null) {\n        try {\n          declaredClass = getConf().getClassByName(className);\n        } catch (ClassNotFoundException e) {\n          throw new RuntimeException(e.toString());\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.ObjectWritable.get": "  public Object get() { return instance; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.ObjectWritable.tryInstantiateProtobuf": "  private static Message tryInstantiateProtobuf(\n      Class<?> protoClass,\n      DataInput dataIn) throws IOException {\n\n    try {\n      if (dataIn instanceof InputStream) {\n        // We can use the built-in parseDelimitedFrom and not have to re-copy\n        // the data\n        Method parseMethod = getStaticProtobufMethod(protoClass,\n            \"parseDelimitedFrom\", InputStream.class);\n        return (Message)parseMethod.invoke(null, (InputStream)dataIn);\n      } else {\n        // Have to read it into a buffer first, since protobuf doesn't deal\n        // with the DataInput interface directly.\n        \n        // Read the size delimiter that writeDelimitedTo writes\n        int size = ProtoUtil.readRawVarint32(dataIn);\n        if (size < 0) {\n          throw new IOException(\"Invalid size: \" + size);\n        }\n      \n        byte[] data = new byte[size];\n        dataIn.readFully(data);\n        Method parseMethod = getStaticProtobufMethod(protoClass,\n            \"parseFrom\", byte[].class);\n        return (Message)parseMethod.invoke(null, data);\n      }\n    } catch (InvocationTargetException e) {\n      \n      if (e.getCause() instanceof IOException) {\n        throw (IOException)e.getCause();\n      } else {\n        throw new IOException(e.getCause());\n      }\n    } catch (IllegalAccessException iae) {\n      throw new AssertionError(\"Could not access parse method in \" +\n          protoClass);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.processData": "    private void processData(byte[] buf) throws  IOException, InterruptedException {\n      DataInputStream dis =\n        new DataInputStream(new ByteArrayInputStream(buf));\n      int id = dis.readInt();                    // try to read an id\n        \n      if (LOG.isDebugEnabled())\n        LOG.debug(\" got #\" + id);\n      Writable param;\n      try {\n        param = ReflectionUtils.newInstance(paramClass, conf);//read param\n        param.readFields(dis);\n      } catch (Throwable t) {\n        LOG.warn(\"Unable to read call parameters for client \" +\n                 getHostAddress(), t);\n        final Call readParamsFailedCall = new Call(id, null, this);\n        ByteArrayOutputStream responseBuffer = new ByteArrayOutputStream();\n\n        setupResponse(responseBuffer, readParamsFailedCall, Status.FATAL, null,\n            t.getClass().getName(),\n            \"IPC server unable to read call parameters: \" + t.getMessage());\n        responder.doRespond(readParamsFailedCall);\n        return;\n      }\n        \n      Call call = new Call(id, param, this);\n      callQueue.put(call);              // queue the call; maybe blocked here\n      incRpcCount();  // Increment the rpc count\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRespond": "    void doRespond(Call call) throws IOException {\n      synchronized (call.connection.responseQueue) {\n        call.connection.responseQueue.addLast(call);\n        if (call.connection.responseQueue.size() == 1) {\n          processResponse(call.connection.responseQueue, true);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.incRpcCount": "    private void incRpcCount() {\n      rpcCount++;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(ByteArrayOutputStream response, \n                             Call call, Status status, \n                             Writable rv, String errorClass, String error) \n  throws IOException {\n    response.reset();\n    DataOutputStream out = new DataOutputStream(response);\n    out.writeInt(call.id);                // write call id\n    out.writeInt(status.state);           // write status\n\n    if (status == Status.SUCCESS) {\n      try {\n        rv.write(out);\n      } catch (Throwable t) {\n        LOG.warn(\"Error serializing call response for call \" + call, t);\n        // Call back to same function - this is OK since the\n        // buffer is reset at the top, and since status is changed\n        // to ERROR it won't infinite loop.\n        setupResponse(response, call, Status.ERROR,\n            null, t.getClass().getName(),\n            StringUtils.stringifyException(t));\n        return;\n      }\n    } else {\n      WritableUtils.writeString(out, errorClass);\n      WritableUtils.writeString(out, error);\n    }\n    if (call.connection.useWrap) {\n      wrapWithSasl(response, call);\n    }\n    call.setResponse(ByteBuffer.wrap(response.toByteArray()));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getHostAddress": "    public String getHostAddress() {\n      return hostAddress;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.processOneRpc": "    private void processOneRpc(byte[] buf) throws IOException,\n        InterruptedException {\n      if (headerRead) {\n        processData(buf);\n      } else {\n        processHeader(buf);\n        headerRead = true;\n        if (!authorizeConnection()) {\n          throw new AccessControlException(\"Connection from \" + this\n              + \" for protocol \" + header.getProtocol()\n              + \" is unauthorized for user \" + user);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.processHeader": "    private void processHeader(byte[] buf) throws IOException {\n      DataInputStream in =\n        new DataInputStream(new ByteArrayInputStream(buf));\n      header.readFields(in);\n      protocolName = header.getProtocol();\n\n      \n      UserGroupInformation protocolUser = header.getUgi();\n      if (!useSasl) {\n        user = protocolUser;\n        if (user != null) {\n          user.setAuthenticationMethod(AuthMethod.SIMPLE.authenticationMethod);\n        }\n      } else {\n        // user is authenticated\n        user.setAuthenticationMethod(authMethod.authenticationMethod);\n        //Now we check if this is a proxy user case. If the protocol user is\n        //different from the 'user', it is a proxy user scenario. However, \n        //this is not allowed if user authenticated with DIGEST.\n        if ((protocolUser != null)\n            && (!protocolUser.getUserName().equals(user.getUserName()))) {\n          if (authMethod == AuthMethod.DIGEST) {\n            // Not allowed to doAs if token authentication is used\n            throw new AccessControlException(\"Authenticated user (\" + user\n                + \") doesn't match what the client claims to be (\"\n                + protocolUser + \")\");\n          } else {\n            // Effective user can be different from authenticated user\n            // for simple auth or kerberos auth\n            // The user is the real user. Now we create a proxy user\n            UserGroupInformation realUser = user;\n            user = UserGroupInformation.createProxyUser(protocolUser\n                .getUserName(), realUser);\n            // Now the user is a proxy user, set Authentication method Proxy.\n            user.setAuthenticationMethod(AuthenticationMethod.PROXY);\n          }\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.authorizeConnection": "    private boolean authorizeConnection() throws IOException {\n      try {\n        // If auth method is DIGEST, the token was obtained by the\n        // real user for the effective user, therefore not required to\n        // authorize real user. doAs is allowed only for simple or kerberos\n        // authentication\n        if (user != null && user.getRealUser() != null\n            && (authMethod != AuthMethod.DIGEST)) {\n          ProxyUsers.authorize(user, this.getHostAddress(), conf);\n        }\n        authorize(user, header, getHostInetAddress());\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Successfully authorized \" + header);\n        }\n        rpcMetrics.incrAuthorizationSuccesses();\n      } catch (AuthorizationException ae) {\n        rpcMetrics.incrAuthorizationFailures();\n        setupResponse(authFailedResponse, authFailedCall, Status.FATAL, null,\n            ae.getClass().getName(), ae.getMessage());\n        responder.doRespond(authFailedCall);\n        return false;\n      }\n      return true;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.readAndProcess": "    public int readAndProcess() throws IOException, InterruptedException {\n      while (true) {\n        /* Read at most one RPC. If the header is not read completely yet\n         * then iterate until we read first RPC or until there is no data left.\n         */    \n        int count = -1;\n        if (dataLengthBuffer.remaining() > 0) {\n          count = channelRead(channel, dataLengthBuffer);       \n          if (count < 0 || dataLengthBuffer.remaining() > 0) \n            return count;\n        }\n      \n        if (!rpcHeaderRead) {\n          //Every connection is expected to send the header.\n          if (rpcHeaderBuffer == null) {\n            rpcHeaderBuffer = ByteBuffer.allocate(2);\n          }\n          count = channelRead(channel, rpcHeaderBuffer);\n          if (count < 0 || rpcHeaderBuffer.remaining() > 0) {\n            return count;\n          }\n          int version = rpcHeaderBuffer.get(0);\n          byte[] method = new byte[] {rpcHeaderBuffer.get(1)};\n          authMethod = AuthMethod.read(new DataInputStream(\n              new ByteArrayInputStream(method)));\n          dataLengthBuffer.flip();          \n          if (!HEADER.equals(dataLengthBuffer) || version != CURRENT_VERSION) {\n            //Warning is ok since this is not supposed to happen.\n            LOG.warn(\"Incorrect header or version mismatch from \" + \n                     hostAddress + \":\" + remotePort +\n                     \" got version \" + version + \n                     \" expected version \" + CURRENT_VERSION);\n            setupBadVersionResponse(version);\n            return -1;\n          }\n          dataLengthBuffer.clear();\n          if (authMethod == null) {\n            throw new IOException(\"Unable to read authentication method\");\n          }\n          if (isSecurityEnabled && authMethod == AuthMethod.SIMPLE) {\n            AccessControlException ae = new AccessControlException(\n                \"Authentication is required\");\n            setupResponse(authFailedResponse, authFailedCall, Status.FATAL,\n                null, ae.getClass().getName(), ae.getMessage());\n            responder.doRespond(authFailedCall);\n            throw ae;\n          }\n          if (!isSecurityEnabled && authMethod != AuthMethod.SIMPLE) {\n            doSaslReply(SaslStatus.SUCCESS, new IntWritable(\n                SaslRpcServer.SWITCH_TO_SIMPLE_AUTH), null, null);\n            authMethod = AuthMethod.SIMPLE;\n            // client has already sent the initial Sasl message and we\n            // should ignore it. Both client and server should fall back\n            // to simple auth from now on.\n            skipInitialSaslHandshake = true;\n          }\n          if (authMethod != AuthMethod.SIMPLE) {\n            useSasl = true;\n          }\n          \n          rpcHeaderBuffer = null;\n          rpcHeaderRead = true;\n          continue;\n        }\n        \n        if (data == null) {\n          dataLengthBuffer.flip();\n          dataLength = dataLengthBuffer.getInt();\n       \n          if ((dataLength == Client.PING_CALL_ID) && (!useWrap)) {\n            // covers the !useSasl too\n            dataLengthBuffer.clear();\n            return 0; // ping message\n          }\n          \n          if (dataLength < 0) {\n            LOG.warn(\"Unexpected data length \" + dataLength + \"!! from \" + \n                getHostAddress());\n          }\n          data = ByteBuffer.allocate(dataLength);\n        }\n        \n        count = channelRead(channel, data);\n        \n        if (data.remaining() == 0) {\n          dataLengthBuffer.clear();\n          data.flip();\n          if (skipInitialSaslHandshake) {\n            data = null;\n            skipInitialSaslHandshake = false;\n            continue;\n          }\n          boolean isHeaderRead = headerRead;\n          if (useSasl) {\n            saslReadAndProcess(data.array());\n          } else {\n            processOneRpc(data.array());\n          }\n          data = null;\n          if (!isHeaderRead) {\n            continue;\n          }\n        } \n        return count;\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doSaslReply": "    private void doSaslReply(SaslStatus status, Writable rv,\n        String errorClass, String error) throws IOException {\n      saslResponse.reset();\n      DataOutputStream out = new DataOutputStream(saslResponse);\n      out.writeInt(status.state); // write status\n      if (status == SaslStatus.SUCCESS) {\n        rv.write(out);\n      } else {\n        WritableUtils.writeString(out, errorClass);\n        WritableUtils.writeString(out, error);\n      }\n      saslCall.setResponse(ByteBuffer.wrap(saslResponse.toByteArray()));\n      responder.doRespond(saslCall);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupBadVersionResponse": "    private void setupBadVersionResponse(int clientVersion) throws IOException {\n      String errMsg = \"Server IPC version \" + CURRENT_VERSION +\n      \" cannot communicate with client version \" + clientVersion;\n      ByteArrayOutputStream buffer = new ByteArrayOutputStream();\n      \n      if (clientVersion >= 3) {\n        Call fakeCall =  new Call(-1, null, this);\n        // Versions 3 and greater can interpret this exception\n        // response in the same manner\n        setupResponse(buffer, fakeCall, Status.FATAL,\n            null, VersionMismatch.class.getName(), errMsg);\n\n        responder.doRespond(fakeCall);\n      } else if (clientVersion == 2) { // Hadoop 0.18.3\n        Call fakeCall =  new Call(0, null, this);\n        DataOutputStream out = new DataOutputStream(buffer);\n        out.writeInt(0); // call ID\n        out.writeBoolean(true); // error\n        WritableUtils.writeString(out, VersionMismatch.class.getName());\n        WritableUtils.writeString(out, errMsg);\n        fakeCall.setResponse(ByteBuffer.wrap(buffer.toByteArray()));\n        \n        responder.doRespond(fakeCall);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.get": "  public static Server get() {\n    return SERVER.get();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.saslReadAndProcess": "    private void saslReadAndProcess(byte[] saslToken) throws IOException,\n        InterruptedException {\n      if (!saslContextEstablished) {\n        byte[] replyToken = null;\n        try {\n          if (saslServer == null) {\n            switch (authMethod) {\n            case DIGEST:\n              if (secretManager == null) {\n                throw new AccessControlException(\n                    \"Server is not configured to do DIGEST authentication.\");\n              }\n              saslServer = Sasl.createSaslServer(AuthMethod.DIGEST\n                  .getMechanismName(), null, SaslRpcServer.SASL_DEFAULT_REALM,\n                  SaslRpcServer.SASL_PROPS, new SaslDigestCallbackHandler(\n                      secretManager, this));\n              break;\n            default:\n              UserGroupInformation current = UserGroupInformation\n                  .getCurrentUser();\n              String fullName = current.getUserName();\n              if (LOG.isDebugEnabled())\n                LOG.debug(\"Kerberos principal name is \" + fullName);\n              final String names[] = SaslRpcServer.splitKerberosName(fullName);\n              if (names.length != 3) {\n                throw new AccessControlException(\n                    \"Kerberos principal name does NOT have the expected \"\n                        + \"hostname part: \" + fullName);\n              }\n              current.doAs(new PrivilegedExceptionAction<Object>() {\n                @Override\n                public Object run() throws SaslException {\n                  saslServer = Sasl.createSaslServer(AuthMethod.KERBEROS\n                      .getMechanismName(), names[0], names[1],\n                      SaslRpcServer.SASL_PROPS, new SaslGssCallbackHandler());\n                  return null;\n                }\n              });\n            }\n            if (saslServer == null)\n              throw new AccessControlException(\n                  \"Unable to find SASL server implementation for \"\n                      + authMethod.getMechanismName());\n            if (LOG.isDebugEnabled())\n              LOG.debug(\"Created SASL server with mechanism = \"\n                  + authMethod.getMechanismName());\n          }\n          if (LOG.isDebugEnabled())\n            LOG.debug(\"Have read input token of size \" + saslToken.length\n                + \" for processing by saslServer.evaluateResponse()\");\n          replyToken = saslServer.evaluateResponse(saslToken);\n        } catch (IOException e) {\n          IOException sendToClient = e;\n          Throwable cause = e;\n          while (cause != null) {\n            if (cause instanceof InvalidToken) {\n              sendToClient = (InvalidToken) cause;\n              break;\n            }\n            cause = cause.getCause();\n          }\n          doSaslReply(SaslStatus.ERROR, null, sendToClient.getClass().getName(), \n              sendToClient.getLocalizedMessage());\n          rpcMetrics.incrAuthenticationFailures();\n          String clientIP = this.toString();\n          // attempting user could be null\n          AUDITLOG.warn(AUTH_FAILED_FOR + clientIP + \":\" + attemptingUser);\n          throw e;\n        }\n        if (replyToken != null) {\n          if (LOG.isDebugEnabled())\n            LOG.debug(\"Will send token of size \" + replyToken.length\n                + \" from saslServer.\");\n          doSaslReply(SaslStatus.SUCCESS, new BytesWritable(replyToken), null,\n              null);\n        }\n        if (saslServer.isComplete()) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"SASL server context established. Negotiated QoP is \"\n                + saslServer.getNegotiatedProperty(Sasl.QOP));\n          }\n          String qop = (String) saslServer.getNegotiatedProperty(Sasl.QOP);\n          useWrap = qop != null && !\"auth\".equalsIgnoreCase(qop);\n          user = getAuthorizedUgi(saslServer.getAuthorizationID());\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"SASL server successfully authenticated client: \" + user);\n          }\n          rpcMetrics.incrAuthenticationSuccesses();\n          AUDITLOG.info(AUTH_SUCCESSFULL_FOR + user);\n          saslContextEstablished = true;\n        }\n      } else {\n        if (LOG.isDebugEnabled())\n          LOG.debug(\"Have read input token of size \" + saslToken.length\n              + \" for processing by saslServer.unwrap()\");\n        \n        if (!useWrap) {\n          processOneRpc(saslToken);\n        } else {\n          byte[] plaintextData = saslServer.unwrap(saslToken, 0,\n              saslToken.length);\n          processUnwrappedData(plaintextData);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.channelRead": "  private int channelRead(ReadableByteChannel channel, \n                          ByteBuffer buffer) throws IOException {\n    \n    int count = (buffer.remaining() <= NIO_BUFFER_LIMIT) ?\n                channel.read(buffer) : channelIO(channel, null, buffer);\n    if (count > 0) {\n      rpcMetrics.incrReceivedBytes(count);\n    }\n    return count;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRead": "    void doRead(SelectionKey key) throws InterruptedException {\n      int count = 0;\n      Connection c = (Connection)key.attachment();\n      if (c == null) {\n        return;  \n      }\n      c.setLastContact(System.currentTimeMillis());\n      \n      try {\n        count = c.readAndProcess();\n      } catch (InterruptedException ieo) {\n        LOG.info(getName() + \": readAndProcess caught InterruptedException\", ieo);\n        throw ieo;\n      } catch (Exception e) {\n        LOG.info(getName() + \": readAndProcess threw exception \" + e +\n            \" from client \" + c.getHostAddress() +\n            \". Count of bytes read: \" + count, e);\n        count = -1; //so that the (count < 0) block is executed\n      }\n      if (count < 0) {\n        if (LOG.isDebugEnabled())\n          LOG.debug(getName() + \": disconnecting client \" + \n                    c + \". Number of active connections: \"+\n                    numConnections);\n        closeConnection(c);\n        c = null;\n      }\n      else {\n        c.setLastContact(System.currentTimeMillis());\n      }\n    }   ",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setLastContact": "    public void setLastContact(long lastContact) {\n      this.lastContact = lastContact;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeConnection": "  private void closeConnection(Connection connection) {\n    synchronized (connectionList) {\n      if (connectionList.remove(connection))\n        numConnections--;\n    }\n    try {\n      connection.close();\n    } catch (IOException e) {\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "                     public Writable run() throws Exception {\n                       // make the call\n                       return call(call.connection.protocolName, \n                                   call.param, call.timestamp);\n\n                     }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(String protocol,\n                               Writable param, long receiveTime)\n  throws IOException;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param connection incoming connection\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  public void authorize(UserGroupInformation user, \n                        ConnectionHeader connection,\n                        InetAddress addr\n                        ) throws AuthorizationException {\n    if (authorize) {\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(connection.getProtocol(), getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         connection.getProtocol());\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          if (LOG.isDebugEnabled())\n            LOG.debug(getName() + \": disconnecting client \" + c.getHostAddress());\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws IOException,  OutOfMemoryError {\n      Connection c = null;\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        \n        Reader reader = getReader();\n        try {\n          reader.startAdd();\n          SelectionKey readKey = reader.registerChannel(channel);\n          c = new Connection(readKey, channel, System.currentTimeMillis());\n          readKey.attach(c);\n          synchronized (connectionList) {\n            connectionList.add(numConnections, c);\n            numConnections++;\n          }\n          if (LOG.isDebugEnabled())\n            LOG.debug(\"Server connection from \" + c.toString() +\n                \"; # active connections: \" + numConnections +\n                \"; # queued calls: \" + callQueue.size());          \n        } finally {\n          reader.finishAdd(); \n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.cleanupConnections": "    private void cleanupConnections(boolean force) {\n      if (force || numConnections > thresholdIdleConnections) {\n        long currentTime = System.currentTimeMillis();\n        if (!force && (currentTime - lastCleanupRunTime) < cleanupInterval) {\n          return;\n        }\n        int start = 0;\n        int end = numConnections - 1;\n        if (!force) {\n          start = rand.nextInt() % numConnections;\n          end = rand.nextInt() % numConnections;\n          int temp;\n          if (end < start) {\n            temp = start;\n            start = end;\n            end = temp;\n          }\n        }\n        int i = start;\n        int numNuked = 0;\n        while (i <= end) {\n          Connection c;\n          synchronized (connectionList) {\n            try {\n              c = connectionList.get(i);\n            } catch (Exception e) {return;}\n          }\n          if (c.timedOut(currentTime)) {\n            if (LOG.isDebugEnabled())\n              LOG.debug(getName() + \": disconnecting client \" + c.getHostAddress());\n            closeConnection(c);\n            numNuked++;\n            end--;\n            c = null;\n            if (!force && numNuked == maxConnectionsToNuke) break;\n          }\n          else i++;\n        }\n        lastCleanupRunTime = System.currentTimeMillis();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    private synchronized void close() throws IOException {\n      disposeSasl();\n      data = null;\n      dataLengthBuffer = null;\n      if (!channel.isOpen())\n        return;\n      try {socket.shutdownOutput();} catch(Exception e) {\n        LOG.warn(\"Ignoring socket shutdown exception\");\n      }\n      if (channel.isOpen()) {\n        try {channel.close();} catch(Exception e) {}\n      }\n      try {socket.close();} catch(Exception e) {}\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.toString": "    public String toString() {\n      return getHostAddress() + \":\" + remotePort; \n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isValid() && key.isWritable()) {\n                  doAsyncWrite(key);\n              }\n            } catch (IOException e) {\n              LOG.info(getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = System.currentTimeMillis();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<Call> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<Call>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              Call call = (Call)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n          \n          for(Call call : calls) {\n            try {\n              doPurge(call, now);\n            } catch (IOException e) {\n              LOG.warn(\"Error in purging old calls \" + e);\n            }\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.UTF8.readString": "  public static String readString(DataInput in) throws IOException {\n    int bytes = in.readUnsignedShort();\n    StringBuilder buffer = new StringBuilder(bytes);\n    readChars(in, buffer, bytes);\n    return buffer.toString();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.UTF8.readChars": "  private static void readChars(DataInput in, StringBuilder buffer, int nBytes)\n    throws IOException {\n    DataOutputBuffer obuf = OBUF_FACTORY.get();\n    obuf.reset();\n    obuf.write(in, nBytes);\n    byte[] bytes = obuf.getData();\n    int i = 0;\n    while (i < nBytes) {\n      byte b = bytes[i++];\n      if ((b & 0x80) == 0) {\n        buffer.append((char)(b & 0x7F));\n      } else if ((b & 0xE0) != 0xE0) {\n        buffer.append((char)(((b & 0x1F) << 6)\n            | (bytes[i++] & 0x3F)));\n      } else {\n        buffer.append((char)(((b & 0x0F) << 12)\n            | ((bytes[i++] & 0x3F) << 6)\n            |  (bytes[i++] & 0x3F)));\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.UTF8.toString": "  public String toString() {\n    StringBuilder buffer = new StringBuilder(length);\n    try {\n      synchronized (IBUF) {\n        IBUF.reset(bytes, length);\n        readChars(IBUF, buffer, length);\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    return buffer.toString();\n  }"
        },
        "bug_report": {
            "Title": "regression with MAPREDUCE-2289 - setPermission passed immutable FsPermission (rpc failure)",
            "Description": "MAPREDUCE-2289 introduced the following change:\n\n{noformat}\n+        fs.setPermission(stagingArea, JOB_DIR_PERMISSION);\n{noformat}\n\nJOB_DIR_PERMISSION is an immutable FsPermission which cannot be used in RPC calls, it results in the following exception:\n\n{noformat}\n2011-09-08 16:31:45,187 WARN org.apache.hadoop.ipc.Server: Unable to read call parameters for client 127.0.0.1\njava.lang.RuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:115)\n        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:53)\n        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:236)\n        at org.apache.hadoop.ipc.RPC$Invocation.readFields(RPC.java:104)\n        at org.apache.hadoop.ipc.Server$Connection.processData(Server.java:1337)\n        at org.apache.hadoop.ipc.Server$Connection.processOneRpc(Server.java:1315)\n        at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1215)\n        at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:566)\n        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:363)\nCaused by: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()\n        at java.lang.Class.getConstructor0(Class.java:2706)\n        at java.lang.Class.getDeclaredConstructor(Class.java:1985)\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:109)\n        ... 8 more\n{noformat}\n"
        }
    },
    {
        "filename": "HADOOP-15060.json",
        "creation_time": "2017-11-22T00:18:55.000+0000",
        "stack_trace": "```\njava.lang.AssertionError: \nExpected the logs to carry a message about command timeout but was: 2017-11-22 00:10:57,523 WARN  security.ShellBasedUnixGroupsMapping (ShellBasedUnixGroupsMapping.java:getUnixGroups(181)) - unable to return groups for user foobarnonexistinguser\nPartialGroupNameException The user name 'foobarnonexistinguser' is not found. \n\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:275)\n\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:178)\n\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97)\n\tat org.apache.hadoop.security.TestShellBasedUnixGroupsMapping.testFiniteGroupResolutionTime(TestShellBasedUnixGroupsMapping.java:278)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames": "  private List<String> resolvePartialGroupNames(String userName,\n      String errMessage, String groupNames) throws PartialGroupNameException {\n    // Exception may indicate that some group names are not resolvable.\n    // Shell-based implementation should tolerate unresolvable groups names,\n    // and return resolvable ones, similar to what JNI-based implementation\n    // does.\n    if (Shell.WINDOWS) {\n      throw new PartialGroupNameException(\"Does not support partial group\"\n      + \" name resolution on Windows. \" + errMessage);\n    }\n    if (groupNames.isEmpty()) {\n      throw new PartialGroupNameException(\"The user name '\" + userName\n          + \"' is not found. \" + errMessage);\n    } else {\n      LOG.warn(\"Some group names for '{}' are not resolvable. {}\",\n          userName, errMessage);\n      // attempt to partially resolve group names\n      ShellCommandExecutor partialResolver = createGroupIDExecutor(userName);\n      try {\n        partialResolver.execute();\n        return parsePartialGroupNames(\n            groupNames, partialResolver.getOutput());\n      } catch (ExitCodeException ece) {\n        // If exception is thrown trying to get group id list,\n        // something is terribly wrong, so give up.\n        throw new PartialGroupNameException(\n            \"failed to get group id list for user '\" + userName + \"'\", ece);\n      } catch (IOException ioe) {\n        String message =\n            \"Can't execute the shell command to \" +\n            \"get the list of group id for user '\" + userName + \"'\";\n        if (partialResolver.isTimedOut()) {\n          message +=\n              \" because of the command taking longer than \" +\n              \"the configured timeout: \" + timeout + \" seconds\";\n        }\n        throw new PartialGroupNameException(message, ioe);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ShellBasedUnixGroupsMapping.createGroupIDExecutor": "  protected ShellCommandExecutor createGroupIDExecutor(String userName) {\n    return new ShellCommandExecutor(\n        getGroupsIDForUserCommand(userName), null, null, timeout);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ShellBasedUnixGroupsMapping.parsePartialGroupNames": "  private List<String> parsePartialGroupNames(String groupNames,\n      String groupIDs) throws PartialGroupNameException {\n    StringTokenizer nameTokenizer =\n        new StringTokenizer(groupNames, Shell.TOKEN_SEPARATOR_REGEX);\n    StringTokenizer idTokenizer =\n        new StringTokenizer(groupIDs, Shell.TOKEN_SEPARATOR_REGEX);\n    List<String> groups = new LinkedList<String>();\n    while (nameTokenizer.hasMoreTokens()) {\n      // check for unresolvable group names.\n      if (!idTokenizer.hasMoreTokens()) {\n        throw new PartialGroupNameException(\"Number of group names and ids do\"\n        + \" not match. group name =\" + groupNames + \", group id = \" + groupIDs);\n      }\n      String groupName = nameTokenizer.nextToken();\n      String groupID = idTokenizer.nextToken();\n      if (!StringUtils.isNumeric(groupName) ||\n          !groupName.equals(groupID)) {\n        // if the group name is non-numeric, it is resolved.\n        // if the group name is numeric, but is not the same as group id,\n        // regard it as a group name.\n        // if unfortunately, some group names are not resolvable, and\n        // the group name is the same as the group id, regard it as not\n        // resolved.\n        groups.add(groupName);\n      }\n    }\n    return groups;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups": "  private List<String> getUnixGroups(String user) throws IOException {\n    ShellCommandExecutor executor = createGroupExecutor(user);\n\n    List<String> groups;\n    try {\n      executor.execute();\n      groups = resolveFullGroupNames(executor.getOutput());\n    } catch (ExitCodeException e) {\n      try {\n        groups = resolvePartialGroupNames(user, e.getMessage(),\n            executor.getOutput());\n      } catch (PartialGroupNameException pge) {\n        LOG.warn(\"unable to return groups for user {}\", user, pge);\n        return EMPTY_GROUPS;\n      }\n    } catch (IOException ioe) {\n      // If its a shell executor timeout, indicate so in the message\n      // but treat the result as empty instead of throwing it up,\n      // similar to how partial resolution failures are handled above\n      if (executor.isTimedOut()) {\n        LOG.warn(\n            \"Unable to return groups for user '{}' as shell group lookup \" +\n            \"command '{}' ran longer than the configured timeout limit of \" +\n            \"{} seconds.\",\n            user,\n            Joiner.on(' ').join(executor.getExecString()),\n            timeout\n        );\n        return EMPTY_GROUPS;\n      } else {\n        // If its not an executor timeout, we should let the caller handle it\n        throw ioe;\n      }\n    }\n\n    // remove duplicated primary group\n    if (!Shell.WINDOWS) {\n      for (int i = 1; i < groups.size(); i++) {\n        if (groups.get(i).equals(groups.get(0))) {\n          groups.remove(i);\n          break;\n        }\n      }\n    }\n\n    return groups;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ShellBasedUnixGroupsMapping.createGroupExecutor": "  protected ShellCommandExecutor createGroupExecutor(String userName) {\n    return new ShellCommandExecutor(\n        getGroupsForUserCommand(userName), null, null, timeout);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolveFullGroupNames": "  protected List<String> resolveFullGroupNames(String groupNames) {\n    StringTokenizer tokenizer =\n        new StringTokenizer(groupNames, Shell.TOKEN_SEPARATOR_REGEX);\n    List<String> groups = new LinkedList<String>();\n    while (tokenizer.hasMoreTokens()) {\n      groups.add(tokenizer.nextToken());\n    }\n\n    return groups;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups": "  public List<String> getGroups(String userName) throws IOException {\n    return getUnixGroups(userName);\n  }"
        },
        "bug_report": {
            "Title": "TestShellBasedUnixGroupsMapping.testFiniteGroupResolutionTime flaky",
            "Description": "{code}\r\n[ERROR] testFiniteGroupResolutionTime(org.apache.hadoop.security.TestShellBasedUnixGroupsMapping)  Time elapsed: 61.975 s  <<< FAILURE!\r\njava.lang.AssertionError: \r\nExpected the logs to carry a message about command timeout but was: 2017-11-22 00:10:57,523 WARN  security.ShellBasedUnixGroupsMapping (ShellBasedUnixGroupsMapping.java:getUnixGroups(181)) - unable to return groups for user foobarnonexistinguser\r\nPartialGroupNameException The user name 'foobarnonexistinguser' is not found. \r\n\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:275)\r\n\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:178)\r\n\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97)\r\n\tat org.apache.hadoop.security.TestShellBasedUnixGroupsMapping.testFiniteGroupResolutionTime(TestShellBasedUnixGroupsMapping.java:278)\r\n{code}"
        }
    },
    {
        "filename": "HADOOP-10937.json",
        "creation_time": "2014-08-04T21:22:55.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n\tat org.apache.hadoop.crypto.key.kms.KMSClientProvider.decryptEncryptedKey(KMSClientProvider.java:652)\n\tat org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:342)\n\tat org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1319)\n\tat org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1364)\n\tat org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1352)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:391)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:384)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:328)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.kms.KMSClientProvider.decryptEncryptedKey": "  public KeyVersion decryptEncryptedKey(\n      EncryptedKeyVersion encryptedKeyVersion) throws IOException,\n                                                      GeneralSecurityException {\n    checkNotNull(encryptedKeyVersion.getEncryptionKeyVersionName(),\n        \"versionName\");\n    checkNotNull(encryptedKeyVersion.getEncryptedKeyIv(), \"iv\");\n    Preconditions.checkArgument(\n        encryptedKeyVersion.getEncryptedKeyVersion().getVersionName()\n            .equals(KeyProviderCryptoExtension.EEK),\n        \"encryptedKey version name must be '%s', is '%s'\",\n        KeyProviderCryptoExtension.EK,\n        encryptedKeyVersion.getEncryptedKeyVersion().getVersionName()\n    );\n    checkNotNull(encryptedKeyVersion.getEncryptedKeyVersion(), \"encryptedKey\");\n    Map<String, String> params = new HashMap<String, String>();\n    params.put(KMSRESTConstants.EEK_OP, KMSRESTConstants.EEK_DECRYPT);\n    Map<String, Object> jsonPayload = new HashMap<String, Object>();\n    jsonPayload.put(KMSRESTConstants.NAME_FIELD,\n        encryptedKeyVersion.getEncryptionKeyName());\n    jsonPayload.put(KMSRESTConstants.IV_FIELD, Base64.encodeBase64String(\n        encryptedKeyVersion.getEncryptedKeyIv()));\n    jsonPayload.put(KMSRESTConstants.MATERIAL_FIELD, Base64.encodeBase64String(\n            encryptedKeyVersion.getEncryptedKeyVersion().getMaterial()));\n    URL url = createURL(KMSRESTConstants.KEY_VERSION_RESOURCE,\n        encryptedKeyVersion.getEncryptionKeyVersionName(),\n        KMSRESTConstants.EEK_SUB_RESOURCE, params);\n    HttpURLConnection conn = createConnection(url, HTTP_POST);\n    conn.setRequestProperty(CONTENT_TYPE, APPLICATION_JSON_MIME);\n    Map response =\n        call(conn, jsonPayload, HttpURLConnection.HTTP_OK, Map.class);\n    return parseJSONKeyVersion(response);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.kms.KMSClientProvider.call": "  private static <T> T call(HttpURLConnection conn, Map jsonOutput,\n      int expectedResponse, Class<T> klass)\n      throws IOException {\n    T ret = null;\n    try {\n      if (jsonOutput != null) {\n        writeJson(jsonOutput, conn.getOutputStream());\n      }\n    } catch (IOException ex) {\n      conn.getInputStream().close();\n      throw ex;\n    }\n    validateResponse(conn, expectedResponse);\n    if (APPLICATION_JSON_MIME.equalsIgnoreCase(conn.getContentType())\n        && klass != null) {\n      ObjectMapper mapper = new ObjectMapper();\n      InputStream is = null;\n      try {\n        is = conn.getInputStream();\n        ret = mapper.readValue(is, klass);\n      } catch (IOException ex) {\n        if (is != null) {\n          is.close();\n        }\n        throw ex;\n      } finally {\n        if (is != null) {\n          is.close();\n        }\n      }\n    }\n    return ret;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.kms.KMSClientProvider.checkNotNull": "  public static <T> T checkNotNull(T o, String name)\n      throws IllegalArgumentException {\n    if (o == null) {\n      throw new IllegalArgumentException(\"Parameter '\" + name +\n          \"' cannot be null\");\n    }\n    return o;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.kms.KMSClientProvider.createConnection": "  private HttpURLConnection createConnection(URL url, String method)\n      throws IOException {\n    HttpURLConnection conn;\n    try {\n      AuthenticatedURL authUrl = new AuthenticatedURL(new PseudoAuthenticator(),\n          configurator);\n      conn = authUrl.openConnection(url, new AuthenticatedURL.Token());\n    } catch (AuthenticationException ex) {\n      throw new IOException(ex);\n    }\n    conn.setUseCaches(false);\n    conn.setRequestMethod(method);\n    if (method.equals(HTTP_POST) || method.equals(HTTP_PUT)) {\n      conn.setDoOutput(true);\n    }\n    conn = configureConnection(conn);\n    return conn;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.kms.KMSClientProvider.parseJSONKeyVersion": "  private static KeyVersion parseJSONKeyVersion(Map valueMap) {\n    KeyVersion keyVersion = null;\n    if (!valueMap.isEmpty()) {\n      byte[] material = (valueMap.containsKey(KMSRESTConstants.MATERIAL_FIELD))\n          ? Base64.decodeBase64((String) valueMap.get(KMSRESTConstants.MATERIAL_FIELD))\n          : null;\n      String versionName = (String)valueMap.get(KMSRESTConstants.VERSION_NAME_FIELD);\n      String keyName = (String)valueMap.get(KMSRESTConstants.NAME_FIELD);\n      keyVersion = new KMSKeyVersion(keyName, versionName, material);\n    }\n    return keyVersion;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.kms.KMSClientProvider.createURL": "  private URL createURL(String collection, String resource, String subResource,\n      Map<String, ?> parameters) throws IOException {\n    try {\n      StringBuilder sb = new StringBuilder();\n      sb.append(kmsUrl);\n      sb.append(collection);\n      if (resource != null) {\n        sb.append(\"/\").append(URLEncoder.encode(resource, UTF8));\n      }\n      if (subResource != null) {\n        sb.append(\"/\").append(subResource);\n      }\n      URIBuilder uriBuilder = new URIBuilder(sb.toString());\n      if (parameters != null) {\n        for (Map.Entry<String, ?> param : parameters.entrySet()) {\n          Object value = param.getValue();\n          if (value instanceof String) {\n            uriBuilder.addParameter(param.getKey(), (String) value);\n          } else {\n            for (String s : (String[]) value) {\n              uriBuilder.addParameter(param.getKey(), s);\n            }\n          }\n        }\n      }\n      return uriBuilder.build().toURL();\n    } catch (URISyntaxException ex) {\n      throw new IOException(ex);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey": "  public KeyVersion decryptEncryptedKey(EncryptedKeyVersion encryptedKey) \n      throws IOException, GeneralSecurityException {\n    return getExtension().decryptEncryptedKey(encryptedKey);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.getEncryptedKeyVersion": "    public KeyVersion getEncryptedKeyVersion() {\n      return encryptedKeyVersion;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.getEncryptionKeyVersionName": "    public String getEncryptionKeyVersionName() {\n      return encryptionKeyVersionName;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.deriveIV": "    protected static byte[] deriveIV(byte[] encryptedKeyIV) {\n      byte[] rIv = new byte[encryptedKeyIV.length];\n      // Do a simple XOR transformation to flip all the bits\n      for (int i = 0; i < encryptedKeyIV.length; i++) {\n        rIv[i] = (byte) (encryptedKeyIV[i] ^ 0xff);\n      }\n      return rIv;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.getEncryptedKeyIv": "    public byte[] getEncryptedKeyIv() {\n      return encryptedKeyIv;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.doCall": "      public Void doCall(final Path p) throws IOException {\n        dfs.checkAccess(getPathName(p), mode);\n        return null;\n      }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.listXAttrs": "  public List<String> listXAttrs(Path path)\n          throws IOException {\n    final Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<List<String>>() {\n      @Override\n      public List<String> doCall(final Path p) throws IOException {\n        return dfs.listXAttrs(getPathName(p));\n      }\n      @Override\n      public List<String> next(final FileSystem fs, final Path p)\n              throws IOException, UnresolvedLinkException {\n        return fs.listXAttrs(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getPathName": "  private String getPathName(Path file) {\n    checkPath(file);\n    String result = file.toUri().getPath();\n    if (!DFSUtil.isValidName(result)) {\n      throw new IllegalArgumentException(\"Pathname \" + result + \" from \" +\n                                         file+\" is not a valid DFS filename.\");\n    }\n    return result;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.deleteSnapshot": "  public void deleteSnapshot(final Path snapshotDir, final String snapshotName)\n      throws IOException {\n    Path absF = fixRelativePart(snapshotDir);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.deleteSnapshot(getPathName(p), snapshotName);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.deleteSnapshot(p, snapshotName);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + snapshotDir + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.recoverLease": "  public boolean recoverLease(final Path f) throws IOException {\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.recoverLease(getPathName(p));\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.recoverLease(p);\n        }\n        throw new UnsupportedOperationException(\"Cannot recoverLease through\" +\n            \" a symlink to a non-DistributedFileSystem: \" + f + \" -> \" + p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getSnapshotDiffReport": "  public SnapshotDiffReport getSnapshotDiffReport(final Path snapshotDir,\n      final String fromSnapshot, final String toSnapshot) throws IOException {\n    Path absF = fixRelativePart(snapshotDir);\n    return new FileSystemLinkResolver<SnapshotDiffReport>() {\n      @Override\n      public SnapshotDiffReport doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.getSnapshotDiffReport(getPathName(p), fromSnapshot,\n            toSnapshot);\n      }\n\n      @Override\n      public SnapshotDiffReport next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.getSnapshotDiffReport(p, fromSnapshot, toSnapshot);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + snapshotDir + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.createSnapshot": "  public Path createSnapshot(final Path path, final String snapshotName) \n      throws IOException {\n    Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<Path>() {\n      @Override\n      public Path doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return new Path(dfs.createSnapshot(getPathName(p), snapshotName));\n      }\n\n      @Override\n      public Path next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.createSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.rename": "  public void rename(Path src, Path dst, final Options.Rename... options)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    final Path absSrc = fixRelativePart(src);\n    final Path absDst = fixRelativePart(dst);\n    // Try the rename without resolving first\n    try {\n      dfs.rename(getPathName(absSrc), getPathName(absDst), options);\n    } catch (UnresolvedLinkException e) {\n      // Fully resolve the source\n      final Path source = getFileLinkStatus(absSrc).getPath();\n      // Keep trying to resolve the destination\n      new FileSystemLinkResolver<Void>() {\n        @Override\n        public Void doCall(final Path p)\n            throws IOException, UnresolvedLinkException {\n          dfs.rename(getPathName(source), getPathName(p), options);\n          return null;\n        }\n        @Override\n        public Void next(final FileSystem fs, final Path p)\n            throws IOException {\n          // Should just throw an error in FileSystem#checkPath\n          return doCall(p);\n        }\n      }.resolve(this, absDst);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getUri": "  public URI getUri() { return uri; }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.createSymlink": "  public void createSymlink(final Path target, final Path link,\n      final boolean createParent) throws AccessControlException,\n      FileAlreadyExistsException, FileNotFoundException,\n      ParentNotDirectoryException, UnsupportedFileSystemException, \n      IOException {\n    statistics.incrementWriteOps(1);\n    final Path absF = fixRelativePart(link);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException,\n          UnresolvedLinkException {\n        dfs.createSymlink(target.toString(), getPathName(p), createParent);\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException, UnresolvedLinkException {\n        fs.createSymlink(target, p, createParent);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.allowSnapshot": "  public void allowSnapshot(final Path path) throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.allowSnapshot(getPathName(p));\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.allowSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.removeXAttr": "  public void removeXAttr(Path path, final String name) throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.removeXAttr(getPathName(p), name);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.removeXAttr(p, name);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.modifyAclEntries": "  public void modifyAclEntries(Path path, final List<AclEntry> aclSpec)\n      throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.modifyAclEntries(getPathName(p), aclSpec);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.modifyAclEntries(p, aclSpec);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.removeAcl": "  public void removeAcl(Path path) throws IOException {\n    final Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.removeAcl(getPathName(p));\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n        throws IOException, UnresolvedLinkException {\n        fs.removeAcl(p);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.removeAclEntries": "  public void removeAclEntries(Path path, final List<AclEntry> aclSpec)\n      throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.removeAclEntries(getPathName(p), aclSpec);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.removeAclEntries(p, aclSpec);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setOwner": "  public void setOwner(Path p, final String username, final String groupname\n      ) throws IOException {\n    if (username == null && groupname == null) {\n      throw new IOException(\"username == null && groupname == null\");\n    }\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(p);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.setOwner(getPathName(p), username, groupname);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        fs.setOwner(p, username, groupname);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setPermission": "  public void setPermission(Path p, final FsPermission permission\n      ) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(p);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.setPermission(getPathName(p), permission);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        fs.setPermission(p, permission);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.open": "  public FSDataInputStream open(Path f, final int bufferSize)\n      throws IOException {\n    statistics.incrementReadOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FSDataInputStream>() {\n      @Override\n      public FSDataInputStream doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return new HdfsDataInputStream(\n            dfs.open(getPathName(p), bufferSize, verifyChecksum));\n      }\n      @Override\n      public FSDataInputStream next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.open(p, bufferSize);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.append": "  public FSDataOutputStream append(Path f, final int bufferSize,\n      final Progressable progress) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FSDataOutputStream>() {\n      @Override\n      public FSDataOutputStream doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.append(getPathName(p), bufferSize, progress, statistics);\n      }\n      @Override\n      public FSDataOutputStream next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.append(p, bufferSize);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.disallowSnapshot": "  public void disallowSnapshot(final Path path) throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.disallowSnapshot(getPathName(p));\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.disallowSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setTimes": "  public void setTimes(Path p, final long mtime, final long atime\n      ) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(p);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.setTimes(getPathName(p), mtime, atime);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        fs.setTimes(p, mtime, atime);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.isFileClosed": "  public boolean isFileClosed(final Path src) throws IOException {\n    Path absF = fixRelativePart(src);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.isFileClosed(getPathName(p));\n      }\n\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.isFileClosed(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot call isFileClosed\"\n              + \" on a symlink to a non-DistributedFileSystem: \"\n              + src + \" -> \" + p);\n        }\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.renameSnapshot": "  public void renameSnapshot(final Path path, final String snapshotOldName,\n      final String snapshotNewName) throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.renameSnapshot(getPathName(p), snapshotOldName, snapshotNewName);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.renameSnapshot(p, snapshotOldName, snapshotNewName);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getXAttr": "  public byte[] getXAttr(Path path, final String name) throws IOException {\n    final Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<byte[]>() {\n      @Override\n      public byte[] doCall(final Path p) throws IOException {\n        return dfs.getXAttr(getPathName(p), name);\n      }\n      @Override\n      public byte[] next(final FileSystem fs, final Path p)\n        throws IOException, UnresolvedLinkException {\n        return fs.getXAttr(p, name);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getAclStatus": "  public AclStatus getAclStatus(Path path) throws IOException {\n    final Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<AclStatus>() {\n      @Override\n      public AclStatus doCall(final Path p) throws IOException {\n        return dfs.getAclStatus(getPathName(p));\n      }\n      @Override\n      public AclStatus next(final FileSystem fs, final Path p)\n        throws IOException, UnresolvedLinkException {\n        return fs.getAclStatus(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.create": "  public FSDataOutputStream create(final Path f, final FsPermission permission,\n    final EnumSet<CreateFlag> cflags, final int bufferSize,\n    final short replication, final long blockSize, final Progressable progress,\n    final ChecksumOpt checksumOpt) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FSDataOutputStream>() {\n      @Override\n      public FSDataOutputStream doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return new HdfsDataOutputStream(dfs.create(getPathName(p), permission,\n            cflags, replication, blockSize, progress, bufferSize, checksumOpt),\n            statistics);\n      }\n      @Override\n      public FSDataOutputStream next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.create(p, permission, cflags, bufferSize,\n            replication, blockSize, progress, checksumOpt);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setReplication": "  public boolean setReplication(Path src, \n                                final short replication\n                               ) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(src);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.setReplication(getPathName(p), replication);\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.setReplication(p, replication);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setXAttr": "  public void setXAttr(Path path, final String name, final byte[] value, \n      final EnumSet<XAttrSetFlag> flag) throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.setXAttr(getPathName(p), name, value, flag);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.setXAttr(p, name, value, flag);\n        return null;\n      }      \n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setQuota": "  public void setQuota(Path src, final long namespaceQuota,\n      final long diskspaceQuota) throws IOException {\n    Path absF = fixRelativePart(src);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.setQuota(getPathName(p), namespaceQuota, diskspaceQuota);\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        // setQuota is not defined in FileSystem, so we only can resolve\n        // within this DFS\n        return doCall(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal": "  private FileStatus[] listStatusInternal(Path p) throws IOException {\n    String src = getPathName(p);\n\n    // fetch the first batch of entries in the directory\n    DirectoryListing thisListing = dfs.listPaths(\n        src, HdfsFileStatus.EMPTY_NAME);\n\n    if (thisListing == null) { // the directory does not exist\n      throw new FileNotFoundException(\"File \" + p + \" does not exist.\");\n    }\n    \n    HdfsFileStatus[] partialListing = thisListing.getPartialListing();\n    if (!thisListing.hasMore()) { // got all entries of the directory\n      FileStatus[] stats = new FileStatus[partialListing.length];\n      for (int i = 0; i < partialListing.length; i++) {\n        stats[i] = partialListing[i].makeQualified(getUri(), p);\n      }\n      statistics.incrementReadOps(1);\n      return stats;\n    }\n\n    // The directory size is too big that it needs to fetch more\n    // estimate the total number of entries in the directory\n    int totalNumEntries =\n      partialListing.length + thisListing.getRemainingEntries();\n    ArrayList<FileStatus> listing =\n      new ArrayList<FileStatus>(totalNumEntries);\n    // add the first batch of entries to the array list\n    for (HdfsFileStatus fileStatus : partialListing) {\n      listing.add(fileStatus.makeQualified(getUri(), p));\n    }\n    statistics.incrementLargeReadOps(1);\n \n    // now fetch more entries\n    do {\n      thisListing = dfs.listPaths(src, thisListing.getLastName());\n \n      if (thisListing == null) { // the directory is deleted\n        throw new FileNotFoundException(\"File \" + p + \" does not exist.\");\n      }\n \n      partialListing = thisListing.getPartialListing();\n      for (HdfsFileStatus fileStatus : partialListing) {\n        listing.add(fileStatus.makeQualified(getUri(), p));\n      }\n      statistics.incrementLargeReadOps(1);\n    } while (thisListing.hasMore());\n \n    return listing.toArray(new FileStatus[listing.size()]);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getContentSummary": "  public ContentSummary getContentSummary(Path f) throws IOException {\n    statistics.incrementReadOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<ContentSummary>() {\n      @Override\n      public ContentSummary doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.getContentSummary(getPathName(p));\n      }\n      @Override\n      public ContentSummary next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.getContentSummary(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum": "  public FileChecksum getFileChecksum(Path f, final long length)\n      throws IOException {\n    statistics.incrementReadOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FileChecksum>() {\n      @Override\n      public FileChecksum doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.getFileChecksum(getPathName(p), length);\n      }\n\n      @Override\n      public FileChecksum next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          return ((DistributedFileSystem) fs).getFileChecksum(p, length);\n        } else {\n          throw new UnsupportedFileSystemException(\n              \"getFileChecksum(Path, long) is not supported by \"\n                  + fs.getClass().getSimpleName()); \n        }\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.removeDefaultAcl": "  public void removeDefaultAcl(Path path) throws IOException {\n    final Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.removeDefaultAcl(getPathName(p));\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n        throws IOException, UnresolvedLinkException {\n        fs.removeDefaultAcl(p);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setAcl": "  public void setAcl(Path path, final List<AclEntry> aclSpec) throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.setAcl(getPathName(p), aclSpec);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.setAcl(p, aclSpec);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getXAttrs": "  public Map<String, byte[]> getXAttrs(Path path, final List<String> names) \n      throws IOException {\n    final Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<Map<String, byte[]>>() {\n      @Override\n      public Map<String, byte[]> doCall(final Path p) throws IOException {\n        return dfs.getXAttrs(getPathName(p), names);\n      }\n      @Override\n      public Map<String, byte[]> next(final FileSystem fs, final Path p)\n        throws IOException, UnresolvedLinkException {\n        return fs.getXAttrs(p, names);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.delete": "  public boolean delete(Path f, final boolean recursive) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.delete(getPathName(p), recursive);\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.delete(p, recursive);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs": "  public boolean mkdirs(Path f, FsPermission permission) throws IOException {\n    return mkdirsInternal(f, permission, true);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.toString": "  public String toString() {\n    return \"DFS[\" + dfs + \"]\";\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystemLinkResolver.resolve": "  public T resolve(final FileSystem filesys, final Path path)\n      throws IOException {\n    int count = 0;\n    T in = null;\n    Path p = path;\n    // Assumes path belongs to this FileSystem.\n    // Callers validate this by passing paths through FileSystem#checkPath\n    FileSystem fs = filesys;\n    for (boolean isLink = true; isLink;) {\n      try {\n        in = doCall(p);\n        isLink = false;\n      } catch (UnresolvedLinkException e) {\n        if (!filesys.resolveSymlinks) {\n          throw new IOException(\"Path \" + path + \" contains a symlink\"\n              + \" and symlink resolution is disabled (\"\n              + CommonConfigurationKeys.FS_CLIENT_RESOLVE_REMOTE_SYMLINKS_KEY\n              + \").\", e);\n        }\n        if (count++ > FsConstants.MAX_PATH_LINKS) {\n          throw new IOException(\"Possible cyclic loop while \" +\n                                \"following symbolic link \" + path);\n        }\n        // Resolve the first unresolved path component\n        p = FSLinkResolver.qualifySymlinkTarget(fs.getUri(), p,\n            filesys.resolveLink(p));\n        fs = FileSystem.getFSofPath(p, filesys.getConf());\n        // Have to call next if it's a new FS\n        if (!fs.equals(filesys)) {\n          return next(fs, p);\n        }\n        // Else, we keep resolving with this filesystem\n      }\n    }\n    // Successful call, path was fully resolved\n    return in;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystemLinkResolver.next": "  abstract public T next(final FileSystem fs, final Path p) throws IOException;\n\n  /**\n   * Attempt calling overridden {@link #doCall(Path)} method with",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystemLinkResolver.doCall": "  abstract public T doCall(final Path p) throws IOException,\n      UnresolvedLinkException;\n\n  /**\n   * Calls the abstract FileSystem call equivalent to the specialized subclass\n   * implementation in {@link #doCall(Path)}. This is used when retrying the",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.create": "  public FSDataOutputStream create(Path f,\n      FsPermission permission,\n      EnumSet<CreateFlag> flags,\n      int bufferSize,\n      short replication,\n      long blockSize,\n      Progressable progress,\n      ChecksumOpt checksumOpt) throws IOException {\n    // Checksum options are ignored by default. The file systems that\n    // implement checksum need to override this method. The full\n    // support is currently only available in DFS.\n    return create(f, permission, flags.contains(CreateFlag.OVERWRITE), \n        bufferSize, replication, blockSize, progress);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.setPermission": "  public void setPermission(Path p, FsPermission permission\n      ) throws IOException {\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getDefaultReplication": "  public short getDefaultReplication(Path path) {\n    return getDefaultReplication();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getDefaultBlockSize": "  public long getDefaultBlockSize(Path f) {\n    return getDefaultBlockSize();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.keyProvider.getKeyVersion": "  public abstract KeyVersion getKeyVersion(String versionName\n                                            ) throws IOException;\n\n  /**\n   * Get the key names for all keys.\n   * @return the list of key names\n   * @throws IOException\n   */\n  public abstract List<String> getKeys() throws IOException;\n\n  /**\n   * Get key metadata in bulk.\n   * @param names the names of the keys to get\n   * @throws IOException\n   */\n  public Metadata[] getKeysMetadata(String... names) throws IOException {\n    Metadata[] result = new Metadata[names.length];\n    for (int i=0; i < names.length; ++i) {\n      result[i] = getMetadata(names[i]);\n    }\n    return result;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getFSofPath": "  protected static FileSystem getFSofPath(final Path absOrFqPath,\n      final Configuration conf)\n      throws UnsupportedFileSystemException, IOException {\n    absOrFqPath.checkNotSchemeWithRelative();\n    absOrFqPath.checkNotRelative();\n\n    // Uses the default file system if not fully qualified\n    return get(absOrFqPath.toUri(), conf);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.get": "    FileSystem get(URI uri, Configuration conf) throws IOException{\n      Key key = new Key(uri, conf);\n      return getInternal(uri, conf, key);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getUri": "  public abstract URI getUri();\n  \n  /**\n   * Return a canonicalized form of this FileSystem's URI.\n   * \n   * The default implementation simply calls {@link #canonicalizeUri(URI)}",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.equals": "      public boolean equals(Object obj) {\n        if (obj == this) {\n          return true;\n        }\n        if (obj != null && obj instanceof Key) {\n          Key that = (Key)obj;\n          return isEqual(this.scheme, that.scheme)\n                 && isEqual(this.authority, that.authority)\n                 && isEqual(this.ugi, that.ugi)\n                 && (this.unique == that.unique);\n        }\n        return false;        \n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.isEqual": "      static boolean isEqual(Object a, Object b) {\n        return a == b || (a != null && a.equals(b));        \n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FSLinkResolver.qualifySymlinkTarget": "  public static Path qualifySymlinkTarget(final URI pathURI,\n      Path pathWithLink, Path target) {\n    // NB: makeQualified uses the target's scheme and authority, if\n    // specified, and the scheme and authority of pathURI, if not.\n    final URI targetUri = target.toUri();\n    final String scheme = targetUri.getScheme();\n    final String auth = targetUri.getAuthority();\n    return (scheme == null && auth == null) ? target.makeQualified(pathURI,\n        pathWithLink.getParent()) : target;\n  }"
        },
        "bug_report": {
            "Title": "Need to set version name correctly before decrypting EEK",
            "Description": "Touchz-ing a file results in a Null Pointer Exception\n\n{noformat}\n[hdfs@mynode hadoop-common]$ hdfs dfs -touchz /enc3/touchFIle\n2014-08-01 08:45:10,148 INFO  [main] hdfs.DFSClient (DFSClient.java:<init>(605)) - Found KeyProvider: KeyProviderCryptoExtension: KMSClientProvider[http://mynode.myhost.com:16000/kms/v1/]\n-touchz: Fatal internal error\njava.lang.NullPointerException\n\tat org.apache.hadoop.crypto.key.kms.KMSClientProvider.decryptEncryptedKey(KMSClientProvider.java:652)\n\tat org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:342)\n\tat org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1319)\n\tat org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1364)\n\tat org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1352)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:391)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:384)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:328)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)\n{noformat}"
        }
    },
    {
        "filename": "HADOOP-9103.json",
        "creation_time": "2012-04-20T01:07:25.000+0000",
        "stack_trace": "```\njava.io.IOException: Found lease for\n non-existent file /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/????@???????????????\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFilesUnderConstruction(FSImage.java:1211)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:959)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.doMerge(SecondaryNameNode.java:589)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.access$000(SecondaryNameNode.java:473)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doMerge(SecondaryNameNode.java:350)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:314)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:225)\n        at java.lang.Thread.run(Thread.java:619)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage": "  private void loadFSImage(File curFile, MD5Hash expectedMd5,\n      FSNamesystem target, MetaRecoveryContext recovery) throws IOException {\n    FSImageFormat.Loader loader = new FSImageFormat.Loader(\n        conf, target);\n    loader.load(curFile);\n    target.setBlockPoolId(this.getBlockPoolID());\n\n    // Check that the image digest we loaded matches up with what\n    // we expected\n    MD5Hash readImageMd5 = loader.getLoadedImageMd5();\n    if (expectedMd5 != null &&\n        !expectedMd5.equals(readImageMd5)) {\n      throw new IOException(\"Image file \" + curFile +\n          \" is corrupt with MD5 checksum of \" + readImageMd5 +\n          \" but expecting \" + expectedMd5);\n    }\n\n    long txId = loader.getLoadedImageTxId();\n    LOG.info(\"Loaded image for txid \" + txId + \" from \" + curFile);\n    lastAppliedTxId = txId;\n    storage.setMostRecentCheckpointInfo(txId, curFile.lastModified());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits": "  public long loadEdits(Iterable<EditLogInputStream> editStreams,\n      FSNamesystem target, MetaRecoveryContext recovery) throws IOException {\n    LOG.debug(\"About to load edits:\\n  \" + Joiner.on(\"\\n  \").join(editStreams));\n    \n    long prevLastAppliedTxId = lastAppliedTxId;  \n    try {    \n      FSEditLogLoader loader = new FSEditLogLoader(target, lastAppliedTxId);\n      \n      // Load latest edits\n      for (EditLogInputStream editIn : editStreams) {\n        LOG.info(\"Reading \" + editIn + \" expecting start txid #\" +\n              (lastAppliedTxId + 1));\n        try {\n          loader.loadFSEdits(editIn, lastAppliedTxId + 1, recovery);\n        } finally {\n          // Update lastAppliedTxId even in case of error, since some ops may\n          // have been successfully applied before the error.\n          lastAppliedTxId = loader.getLastAppliedTxId();\n        }\n        // If we are in recovery mode, we may have skipped over some txids.\n        if (editIn.getLastTxId() != HdfsConstants.INVALID_TXID) {\n          lastAppliedTxId = editIn.getLastTxId();\n        }\n      }\n    } finally {\n      FSEditLog.closeAllStreams(editStreams);\n      // update the counts\n      target.dir.updateCountForINodeWithQuota();   \n    }\n    return lastAppliedTxId - prevLastAppliedTxId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.isUpgradeFinalized": "  boolean isUpgradeFinalized() {\n    return isUpgradeFinalized;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getLayoutVersion": "  public int getLayoutVersion() {\n    return storage.getLayoutVersion();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.needsResaveBasedOnStaleCheckpoint": "  private boolean needsResaveBasedOnStaleCheckpoint(\n      File imageFile, long numEditsLoaded) {\n    final long checkpointPeriod = conf.getLong(\n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_KEY, \n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_DEFAULT);\n    final long checkpointTxnCount = conf.getLong(\n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_KEY, \n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_DEFAULT);\n    long checkpointAge = System.currentTimeMillis() - imageFile.lastModified();\n\n    return (checkpointAge > checkpointPeriod * 1000) ||\n           (numEditsLoaded > checkpointTxnCount);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getBlockPoolID": "  public String getBlockPoolID() {\n    return storage.getBlockPoolID();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doMerge": "  static void doMerge(\n      CheckpointSignature sig, RemoteEditLogManifest manifest,\n      boolean loadImage, FSImage dstImage, FSNamesystem dstNamesystem)\n      throws IOException {   \n    NNStorage dstStorage = dstImage.getStorage();\n    \n    dstStorage.setStorageInfo(sig);\n    if (loadImage) {\n      File file = dstStorage.findImageFile(sig.mostRecentCheckpointTxId);\n      if (file == null) {\n        throw new IOException(\"Couldn't find image file at txid \" + \n            sig.mostRecentCheckpointTxId + \" even though it should have \" +\n            \"just been downloaded\");\n      }\n      dstImage.reloadFromImageFile(file, dstNamesystem);\n    }\n    \n    Checkpointer.rollForwardByApplyingLogs(manifest, dstImage, dstNamesystem);\n    dstImage.saveFSImageInAllDirs(dstNamesystem, dstImage.getLastAppliedTxId());\n    dstStorage.writeAll();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint": "  boolean doCheckpoint() throws IOException {\n    checkpointImage.ensureCurrentDirExists();\n    NNStorage dstStorage = checkpointImage.getStorage();\n    \n    // Tell the namenode to start logging transactions in a new edit file\n    // Returns a token that would be used to upload the merged image.\n    CheckpointSignature sig = namenode.rollEditLog();\n    \n    // Make sure we're talking to the same NN!\n    if (checkpointImage.getNamespaceID() != 0) {\n      // If the image actually has some data, make sure we're talking\n      // to the same NN as we did before.\n      sig.validateStorageInfo(checkpointImage);\n    } else {\n      // if we're a fresh 2NN, just take the storage info from the server\n      // we first talk to.\n      dstStorage.setStorageInfo(sig);\n      dstStorage.setClusterID(sig.getClusterID());\n      dstStorage.setBlockPoolID(sig.getBlockpoolID());\n    }\n\n    // error simulation code for junit test\n    if (ErrorSimulator.getErrorSimulation(0)) {\n      throw new IOException(\"Simulating error0 \" +\n                            \"after creating edits.new\");\n    }\n\n    RemoteEditLogManifest manifest =\n      namenode.getEditLogManifest(sig.mostRecentCheckpointTxId + 1);\n\n    boolean loadImage = downloadCheckpointFiles(\n        fsName, checkpointImage, sig, manifest);   // Fetch fsimage and edits\n    doMerge(sig, manifest, loadImage, checkpointImage, namesystem);\n    \n    //\n    // Upload the new image into the NameNode. Then tell the Namenode\n    // to make this new uploaded image as the most current image.\n    //\n    long txid = checkpointImage.getLastAppliedTxId();\n    TransferFsImage.uploadImageFromStorage(fsName, getImageListenAddress(),\n        dstStorage, txid);\n\n    // error simulation code for junit test\n    if (ErrorSimulator.getErrorSimulation(1)) {\n      throw new IOException(\"Simulating error1 \" +\n                            \"after uploading new image to NameNode\");\n    }\n\n    LOG.warn(\"Checkpoint done. New Image Size: \" \n             + dstStorage.getFsImageName(txid).length());\n    \n    // Since we've successfully checkpointed, we can remove some old\n    // image files\n    checkpointImage.purgeOldStorage();\n    \n    return loadImage;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.ensureCurrentDirExists": "    void ensureCurrentDirExists() throws IOException {\n      for (Iterator<StorageDirectory> it\n             = storage.dirIterator(); it.hasNext();) {\n        StorageDirectory sd = it.next();\n        File curDir = sd.getCurrentDir();\n        if (!curDir.exists() && !curDir.mkdirs()) {\n          throw new IOException(\"Could not create directory \" + curDir);\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.getImageListenAddress": "  private InetSocketAddress getImageListenAddress() {\n    return new InetSocketAddress(infoBindAddress, imagePort);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.downloadCheckpointFiles": "  static boolean downloadCheckpointFiles(\n      final String nnHostPort,\n      final FSImage dstImage,\n      final CheckpointSignature sig,\n      final RemoteEditLogManifest manifest\n  ) throws IOException {\n    \n    // Sanity check manifest - these could happen if, eg, someone on the\n    // NN side accidentally rmed the storage directories\n    if (manifest.getLogs().isEmpty()) {\n      throw new IOException(\"Found no edit logs to download on NN since txid \" \n          + sig.mostRecentCheckpointTxId);\n    }\n    \n    long expectedTxId = sig.mostRecentCheckpointTxId + 1;\n    if (manifest.getLogs().get(0).getStartTxId() != expectedTxId) {\n      throw new IOException(\"Bad edit log manifest (expected txid = \" +\n          expectedTxId + \": \" + manifest);\n    }\n\n    try {\n        Boolean b = UserGroupInformation.getCurrentUser().doAs(\n            new PrivilegedExceptionAction<Boolean>() {\n  \n          @Override\n          public Boolean run() throws Exception {\n            dstImage.getStorage().cTime = sig.cTime;\n\n            // get fsimage\n            boolean downloadImage = true;\n            if (sig.mostRecentCheckpointTxId ==\n                dstImage.getStorage().getMostRecentCheckpointTxId()) {\n              downloadImage = false;\n              LOG.info(\"Image has not changed. Will not download image.\");\n            } else {\n              MD5Hash downloadedHash = TransferFsImage.downloadImageToStorage(\n                  nnHostPort, sig.mostRecentCheckpointTxId, dstImage.getStorage(), true);\n              dstImage.saveDigestAndRenameCheckpointImage(\n                  sig.mostRecentCheckpointTxId, downloadedHash);\n            }\n        \n            // get edits file\n            for (RemoteEditLog log : manifest.getLogs()) {\n              TransferFsImage.downloadEditsToStorage(\n                  nnHostPort, log, dstImage.getStorage());\n            }\n        \n            return Boolean.valueOf(downloadImage);\n          }\n        });\n        return b.booleanValue();\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run": "          public Boolean run() throws Exception {\n            dstImage.getStorage().cTime = sig.cTime;\n\n            // get fsimage\n            boolean downloadImage = true;\n            if (sig.mostRecentCheckpointTxId ==\n                dstImage.getStorage().getMostRecentCheckpointTxId()) {\n              downloadImage = false;\n              LOG.info(\"Image has not changed. Will not download image.\");\n            } else {\n              MD5Hash downloadedHash = TransferFsImage.downloadImageToStorage(\n                  nnHostPort, sig.mostRecentCheckpointTxId, dstImage.getStorage(), true);\n              dstImage.saveDigestAndRenameCheckpointImage(\n                  sig.mostRecentCheckpointTxId, downloadedHash);\n            }\n        \n            // get edits file\n            for (RemoteEditLog log : manifest.getLogs()) {\n              TransferFsImage.downloadEditsToStorage(\n                  nnHostPort, log, dstImage.getStorage());\n            }\n        \n            return Boolean.valueOf(downloadImage);\n          }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork": "  public void doWork() {\n    //\n    // Poll the Namenode (once every checkpointCheckPeriod seconds) to find the\n    // number of transactions in the edit log that haven't yet been checkpointed.\n    //\n    long period = checkpointConf.getCheckPeriod();\n\n    while (shouldRun) {\n      try {\n        Thread.sleep(1000 * period);\n      } catch (InterruptedException ie) {\n        // do nothing\n      }\n      if (!shouldRun) {\n        break;\n      }\n      try {\n        // We may have lost our ticket since last checkpoint, log in again, just in case\n        if(UserGroupInformation.isSecurityEnabled())\n          UserGroupInformation.getCurrentUser().reloginFromKeytab();\n        \n        long now = System.currentTimeMillis();\n\n        if (shouldCheckpointBasedOnCount() ||\n            now >= lastCheckpointTime + 1000 * checkpointConf.getPeriod()) {\n          doCheckpoint();\n          lastCheckpointTime = now;\n        }\n      } catch (IOException e) {\n        LOG.error(\"Exception in doCheckpoint\", e);\n        e.printStackTrace();\n      } catch (Throwable e) {\n        LOG.error(\"Throwable Exception in doCheckpoint\", e);\n        e.printStackTrace();\n        Runtime.getRuntime().exit(-1);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.closeAllStreams": "  static void closeAllStreams(Iterable<EditLogInputStream> streams) {\n    for (EditLogInputStream s : streams) {\n      IOUtils.closeStream(s);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getFile": "    File getFile() {\n      return file;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getCheckpointTxId": "    public long getCheckpointTxId() {\n      return txId;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getLatestImage": "  abstract FSImageFile getLatestImage() throws IOException;\n\n  /** \n   * Get the minimum tx id which should be loaded with this set of images.\n   */\n  abstract long getMaxSeenTxId();\n\n  /**\n   * @return true if the directories are in such a state that the image should be re-saved\n   * following the load\n   */\n  abstract boolean needToSave();\n\n  /**\n   * Record of an image that has been located and had its filename parsed.\n   */\n  static class FSImageFile {\n    final StorageDirectory sd;    \n    final long txId;\n    private final File file;\n    \n    FSImageFile(StorageDirectory sd, File file, long txId) {\n      assert txId >= 0 || txId == HdfsConstants.INVALID_TXID \n        : \"Invalid txid on \" + file +\": \" + txId;\n      \n      this.sd = sd;\n      this.txId = txId;\n      this.file = file;\n    } \n    \n    File getFile() {\n      return file;\n    }\n\n    public long getCheckpointTxId() {\n      return txId;\n    }\n    \n    @Override\n    public String toString() {\n      return String.format(\"FSImageFile(file=%s, cpktTxId=%019d)\", \n                           file.toString(), txId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.needToSave": "  abstract boolean needToSave();\n\n  /**\n   * Record of an image that has been located and had its filename parsed.\n   */\n  static class FSImageFile {\n    final StorageDirectory sd;    \n    final long txId;\n    private final File file;\n    \n    FSImageFile(StorageDirectory sd, File file, long txId) {\n      assert txId >= 0 || txId == HdfsConstants.INVALID_TXID \n        : \"Invalid txid on \" + file +\": \" + txId;\n      \n      this.sd = sd;\n      this.txId = txId;\n      this.file = file;\n    } \n    \n    File getFile() {\n      return file;\n    }\n\n    public long getCheckpointTxId() {\n      return txId;\n    }\n    \n    @Override\n    public String toString() {\n      return String.format(\"FSImageFile(file=%s, cpktTxId=%019d)\", \n                           file.toString(), txId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getMaxSeenTxId": "  abstract long getMaxSeenTxId();\n\n  /**\n   * @return true if the directories are in such a state that the image should be re-saved\n   * following the load\n   */\n  abstract boolean needToSave();\n\n  /**\n   * Record of an image that has been located and had its filename parsed.\n   */\n  static class FSImageFile {\n    final StorageDirectory sd;    \n    final long txId;\n    private final File file;\n    \n    FSImageFile(StorageDirectory sd, File file, long txId) {\n      assert txId >= 0 || txId == HdfsConstants.INVALID_TXID \n        : \"Invalid txid on \" + file +\": \" + txId;\n      \n      this.sd = sd;\n      this.txId = txId;\n      this.file = file;\n    } \n    \n    File getFile() {\n      return file;\n    }\n\n    public long getCheckpointTxId() {\n      return txId;\n    }\n    \n    @Override\n    public String toString() {\n      return String.format(\"FSImageFile(file=%s, cpktTxId=%019d)\", \n                           file.toString(), txId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditLogStreams": "  static Iterable<EditLogInputStream> getEditLogStreams(NNStorage storage)\n      throws IOException {\n    FSImagePreTransactionalStorageInspector inspector \n      = new FSImagePreTransactionalStorageInspector();\n    storage.inspectStorageDirs(inspector);\n\n    List<EditLogInputStream> editStreams = new ArrayList<EditLogInputStream>();\n    for (File f : inspector.getLatestEditsFiles()) {\n      editStreams.add(new EditLogFileInputStream(f));\n    }\n    return editStreams;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getLatestEditsFiles": "  private List<File> getLatestEditsFiles() {\n    if (latestNameCheckpointTime > latestEditsCheckpointTime) {\n      // the image is already current, discard edits\n      LOG.debug(\n          \"Name checkpoint time is newer than edits, not loading edits.\");\n      return Collections.<File>emptyList();\n    }\n    \n    return getEditsInStorageDir(latestEditsSD);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.findImageFile": "  File findImageFile(long txid) throws IOException {\n    return findFile(NameNodeDirType.IMAGE,\n        getImageFileName(txid));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.getImageFileName": "  public static String getImageFileName(long txid) {\n    return String.format(\"%s_%019d\",\n                         NameNodeFile.IMAGE.getName(), txid);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.findFile": "  private File findFile(NameNodeDirType dirType, String name) {\n    for (StorageDirectory sd : dirIterable(dirType)) {\n      File candidate = new File(sd.getCurrentDir(), name);\n      if (sd.getCurrentDir().canRead() &&\n          candidate.exists()) {\n        return candidate;\n      }\n    }\n    return null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.Checkpointer.rollForwardByApplyingLogs": "  static void rollForwardByApplyingLogs(\n      RemoteEditLogManifest manifest,\n      FSImage dstImage,\n      FSNamesystem dstNamesystem) throws IOException {\n    NNStorage dstStorage = dstImage.getStorage();\n  \n    List<EditLogInputStream> editsStreams = Lists.newArrayList();    \n    for (RemoteEditLog log : manifest.getLogs()) {\n      File f = dstStorage.findFinalizedEditsFile(\n          log.getStartTxId(), log.getEndTxId());\n      if (log.getStartTxId() > dstImage.getLastAppliedTxId()) {\n        editsStreams.add(new EditLogFileInputStream(f, log.getStartTxId(), \n                                                    log.getEndTxId(), true));\n       }\n    }\n    LOG.info(\"Checkpointer about to load edits from \" +\n        editsStreams.size() + \" stream(s).\");\n    dstImage.loadEdits(editsStreams, dstNamesystem, null);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.TransferFsImage.uploadImageFromStorage": "  public static void uploadImageFromStorage(String fsName,\n      InetSocketAddress imageListenAddress,\n      NNStorage storage, long txid) throws IOException {\n    \n    String fileid = GetImageServlet.getParamStringToPutImage(\n        txid, imageListenAddress, storage);\n    // this doesn't directly upload an image, but rather asks the NN\n    // to connect back to the 2NN to download the specified image.\n    try {\n      TransferFsImage.getFileClient(fsName, fileid, null, null, false);\n    } catch (HttpGetFailedException e) {\n      if (e.getResponseCode() == HttpServletResponse.SC_CONFLICT) {\n        // this is OK - this means that a previous attempt to upload\n        // this checkpoint succeeded even though we thought it failed.\n        LOG.info(\"Image upload with txid \" + txid + \n            \" conflicted with a previous image upload to the \" +\n            \"same NameNode. Continuing...\", e);\n        return;\n      } else {\n        throw e;\n      }\n    }\n    LOG.info(\"Uploaded image with txid \" + txid + \" to namenode at \" +\n    \t\tfsName);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getResponseCode": "    public int getResponseCode() {\n      return responseCode;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient": "  static MD5Hash getFileClient(String nnHostPort,\n      String queryString, List<File> localPaths,\n      NNStorage dstStorage, boolean getChecksum) throws IOException {\n    byte[] buf = new byte[HdfsConstants.IO_FILE_BUFFER_SIZE];\n    String proto = UserGroupInformation.isSecurityEnabled() ? \"https://\" : \"http://\";\n    StringBuilder str = new StringBuilder(proto+nnHostPort+\"/getimage?\");\n    str.append(queryString);\n\n    //\n    // open connection to remote server\n    //\n    URL url = new URL(str.toString());\n    \n    // Avoid Krb bug with cross-realm hosts\n    SecurityUtil.fetchServiceTicket(url);\n    HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n    \n    if (connection.getResponseCode() != HttpURLConnection.HTTP_OK) {\n      throw new HttpGetFailedException(\n          \"Image transfer servlet at \" + url +\n          \" failed with status code \" + connection.getResponseCode() +\n          \"\\nResponse message:\\n\" + connection.getResponseMessage(),\n          connection);\n    }\n    \n    long advertisedSize;\n    String contentLength = connection.getHeaderField(CONTENT_LENGTH);\n    if (contentLength != null) {\n      advertisedSize = Long.parseLong(contentLength);\n    } else {\n      throw new IOException(CONTENT_LENGTH + \" header is not provided \" +\n                            \"by the namenode when trying to fetch \" + str);\n    }\n    \n    if (localPaths != null) {\n      String fsImageName = connection.getHeaderField(\n          GetImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n      // If the local paths refer to directories, use the server-provided header\n      // as the filename within that directory\n      List<File> newLocalPaths = new ArrayList<File>();\n      for (File localPath : localPaths) {\n        if (localPath.isDirectory()) {\n          if (fsImageName == null) {\n            throw new IOException(\"No filename header provided by server\");\n          }\n          newLocalPaths.add(new File(localPath, fsImageName));\n        } else {\n          newLocalPaths.add(localPath);\n        }\n      }\n      localPaths = newLocalPaths;\n    }\n    \n    MD5Hash advertisedDigest = parseMD5Header(connection);\n\n    long received = 0;\n    InputStream stream = connection.getInputStream();\n    MessageDigest digester = null;\n    if (getChecksum) {\n      digester = MD5Hash.getDigester();\n      stream = new DigestInputStream(stream, digester);\n    }\n    boolean finishedReceiving = false;\n\n    List<FileOutputStream> outputStreams = Lists.newArrayList();\n\n    try {\n      if (localPaths != null) {\n        for (File f : localPaths) {\n          try {\n            if (f.exists()) {\n              LOG.warn(\"Overwriting existing file \" + f\n                  + \" with file downloaded from \" + str);\n            }\n            outputStreams.add(new FileOutputStream(f));\n          } catch (IOException ioe) {\n            LOG.warn(\"Unable to download file \" + f, ioe);\n            // This will be null if we're downloading the fsimage to a file\n            // outside of an NNStorage directory.\n            if (dstStorage != null) {\n              dstStorage.reportErrorOnFile(f);\n            }\n          }\n        }\n        \n        if (outputStreams.isEmpty()) {\n          throw new IOException(\n              \"Unable to download to any storage directory\");\n        }\n      }\n      \n      int num = 1;\n      while (num > 0) {\n        num = stream.read(buf);\n        if (num > 0) {\n          received += num;\n          for (FileOutputStream fos : outputStreams) {\n            fos.write(buf, 0, num);\n          }\n        }\n      }\n      finishedReceiving = true;\n    } finally {\n      stream.close();\n      for (FileOutputStream fos : outputStreams) {\n        fos.getChannel().force(true);\n        fos.close();\n      }\n      if (finishedReceiving && received != advertisedSize) {\n        // only throw this exception if we think we read all of it on our end\n        // -- otherwise a client-side IOException would be masked by this\n        // exception that makes it look like a server-side problem!\n        throw new IOException(\"File \" + str + \" received length \" + received +\n                              \" is not of the advertised size \" +\n                              advertisedSize);\n      }\n    }\n\n    if (digester != null) {\n      MD5Hash computedDigest = new MD5Hash(digester.digest());\n      \n      if (advertisedDigest != null &&\n          !computedDigest.equals(advertisedDigest)) {\n        throw new IOException(\"File \" + str + \" computed digest \" +\n            computedDigest + \" does not match advertised digest \" + \n            advertisedDigest);\n      }\n      return computedDigest;\n    } else {\n      return null;\n    }    \n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.getBlockpoolID": "  public String getBlockpoolID() {\n    return blockpoolID;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.setBlockPoolID": "  private void setBlockPoolID(File storage, String bpid)\n      throws InconsistentFSStateException {\n    if (bpid == null || bpid.equals(\"\")) {\n      throw new InconsistentFSStateException(storage, \"file \"\n          + Storage.STORAGE_FILE_VERSION + \" has no block pool Id.\");\n    }\n    \n    if (!blockpoolID.equals(\"\") && !blockpoolID.equals(bpid)) {\n      throw new InconsistentFSStateException(storage,\n          \"Unexepcted blockpoolID \" + bpid + \" . Expected \" + blockpoolID);\n    }\n    setBlockPoolID(bpid);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.getFsImageName": "  public File getFsImageName(long txid) {\n    StorageDirectory sd = null;\n    for (Iterator<StorageDirectory> it =\n      dirIterator(NameNodeDirType.IMAGE); it.hasNext();) {\n      sd = it.next();\n      File fsImage = getStorageFile(sd, NameNodeFile.IMAGE, txid);\n      if(sd.getRoot().canRead() && fsImage.exists())\n        return fsImage;\n    }\n    return null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.getStorageFile": "  static File getStorageFile(StorageDirectory sd, NameNodeFile type) {\n    return new File(sd.getCurrentDir(), type.getName());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.getClusterID": "  public String getClusterID() {\n    return clusterID;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo": "  void validateStorageInfo(FSImage si) throws IOException {\n    if(layoutVersion != si.getStorage().layoutVersion\n       || namespaceID != si.getStorage().namespaceID \n       || cTime != si.getStorage().cTime\n       || !clusterID.equals(si.getClusterID())\n       || !blockpoolID.equals(si.getBlockPoolID())) {\n      throw new IOException(\"Inconsistent checkpoint fields.\\n\"\n          + \"LV = \" + layoutVersion + \" namespaceID = \" + namespaceID\n          + \" cTime = \" + cTime\n          + \" ; clusterId = \" + clusterID\n          + \" ; blockpoolId = \" + blockpoolID\n          + \".\\nExpecting respectively: \"\n          + si.getStorage().layoutVersion + \"; \" \n          + si.getStorage().namespaceID + \"; \" + si.getStorage().cTime\n          + \"; \" + si.getClusterID() + \"; \" \n          + si.getBlockPoolID() + \".\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.equals": "  public boolean equals(Object o) {\n    if (!(o instanceof CheckpointSignature)) {\n      return false;\n    }\n    return compareTo((CheckpointSignature)o) == 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.setClusterID": "  void setClusterID(String cid) {\n    clusterID = cid;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.TransferFsImage.downloadEditsToStorage": "  static void downloadEditsToStorage(String fsName, RemoteEditLog log,\n      NNStorage dstStorage) throws IOException {\n    assert log.getStartTxId() > 0 && log.getEndTxId() > 0 :\n      \"bad log: \" + log;\n    String fileid = GetImageServlet.getParamStringForLog(\n        log, dstStorage);\n    String fileName = NNStorage.getFinalizedEditsFileName(\n        log.getStartTxId(), log.getEndTxId());\n\n    List<File> dstFiles = dstStorage.getFiles(NameNodeDirType.EDITS, fileName);\n    assert !dstFiles.isEmpty() : \"No checkpoint targets.\";\n    \n    for (File f : dstFiles) {\n      if (f.exists() && f.canRead()) {\n        LOG.info(\"Skipping download of remote edit log \" +\n            log + \" since it already is stored locally at \" + f);\n        return;\n      } else {\n        LOG.debug(\"Dest file: \" + f);\n      }\n    }\n\n    getFileClient(fsName, fileid, dstFiles, dstStorage, false);\n    LOG.info(\"Downloaded file \" + dstFiles.get(0).getName() + \" size \" +\n        dstFiles.get(0).length() + \" bytes.\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.TransferFsImage.downloadImageToStorage": "  public static MD5Hash downloadImageToStorage(\n      String fsName, long imageTxId, NNStorage dstStorage, boolean needDigest)\n      throws IOException {\n    String fileid = GetImageServlet.getParamStringForImage(\n        imageTxId, dstStorage);\n    String fileName = NNStorage.getCheckpointImageFileName(imageTxId);\n    \n    List<File> dstFiles = dstStorage.getFiles(\n        NameNodeDirType.IMAGE, fileName);\n    if (dstFiles.isEmpty()) {\n      throw new IOException(\"No targets in destination storage!\");\n    }\n    \n    MD5Hash hash = getFileClient(fsName, fileid, dstFiles, dstStorage, needDigest);\n    LOG.info(\"Downloaded file \" + dstFiles.get(0).getName() + \" size \" +\n        dstFiles.get(0).length() + \" bytes.\");\n    return hash;\n  }"
        },
        "bug_report": {
            "Title": "UTF8 class does not properly decode Unicode characters outside the basic multilingual plane",
            "Description": "this the log information  of the  exception  from the SecondaryNameNode: \n2012-03-28 00:48:42,553 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.io.IOException: Found lease for\n non-existent file /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/????@???????????????\n??????????tor.qzone.qq.com/keypart-00174\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFilesUnderConstruction(FSImage.java:1211)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:959)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.doMerge(SecondaryNameNode.java:589)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.access$000(SecondaryNameNode.java:473)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doMerge(SecondaryNameNode.java:350)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:314)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:225)\n        at java.lang.Thread.run(Thread.java:619)\n\nthis is the log information  about the file from namenode:\n2012-03-28 00:32:26,528 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=boss,boss\tip=/10.131.16.34\tcmd=create\tsrc=/user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174\tdst=null\tperm=boss:boss:rw-r--r--\n2012-03-28 00:37:42,387 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174. blk_2751836614265659170_184668759\n2012-03-28 00:37:42,696 INFO org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.completeFile: file /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174 is closed by DFSClient_attempt_201203271849_0016_r_000174_0\n2012-03-28 00:37:50,315 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=boss,boss\tip=/10.131.16.34\tcmd=rename\tsrc=/user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174\tdst=/user/boss/pgv/fission/task16/split/  @?            tor.qzone.qq.com/keypart-00174\tperm=boss:boss:rw-r--r--\n\n\nafter check the code that save FSImage,I found there are a problem that maybe a bug of HDFS Code,I past below:\n-------------this is the saveFSImage method  in  FSImage.java, I make some mark at the problem code------------\n\n/**\n   * Save the contents of the FS image to the file.\n   */\n  void saveFSImage(File newFile) throws IOException {\n    FSNamesystem fsNamesys = FSNamesystem.getFSNamesystem();\n    FSDirectory fsDir = fsNamesys.dir;\n    long startTime = FSNamesystem.now();\n    //\n    // Write out data\n    //\n    DataOutputStream out = new DataOutputStream(\n                                                new BufferedOutputStream(\n                                                                         new FileOutputStream(newFile)));\n    try {\n      .........\n    \n      // save the rest of the nodes\n      saveImage(strbuf, 0, fsDir.rootDir, out);------------------problem\n      fsNamesys.saveFilesUnderConstruction(out);------------------problem  detail is below\n      strbuf = null;\n    } finally {\n      out.close();\n    }\n\n    LOG.info(\"Image file of size \" + newFile.length() + \" saved in \" \n        + (FSNamesystem.now() - startTime)/1000 + \" seconds.\");\n  }\n\n /**\n   * Save file tree image starting from the given root.\n   * This is a recursive procedure, which first saves all children of\n   * a current directory and then moves inside the sub-directories.\n   */\n  private static void saveImage(ByteBuffer parentPrefix,\n                                int prefixLength,\n                                INodeDirectory current,\n                                DataOutputStream out) throws IOException {\n    int newPrefixLength = prefixLength;\n    if (current.getChildrenRaw() == null)\n      return;\n    for(INode child : current.getChildren()) {\n      // print all children first\n      parentPrefix.position(prefixLength);\n      parentPrefix.put(PATH_SEPARATOR).put(child.getLocalNameBytes());------------------problem\n      saveINode2Image(parentPrefix, child, out);\n    }\n   ..........\n  }\n\n\n // Helper function that writes an INodeUnderConstruction\n  // into the input stream\n  //\n  static void writeINodeUnderConstruction(DataOutputStream out,\n                                           INodeFileUnderConstruction cons,\n                                           String path) \n                                           throws IOException {\n    writeString(path, out);------------------problem\n    ..........\n  }\n  \n  static private final UTF8 U_STR = new UTF8();\n  static void writeString(String str, DataOutputStream out) throws IOException {\n    U_STR.set(str);\n    U_STR.write(out);------------------problem \n  }\n\n  /**\n   * Converts a string to a byte array using UTF8 encoding.\n   */\n  static byte[] string2Bytes(String str) {\n    try {\n      return str.getBytes(\"UTF8\");------------------problem \n    } catch(UnsupportedEncodingException e) {\n      assert false : \"UTF8 encoding is not supported \";\n    }\n    return null;\n  }\n------------------------------------------below is the explain------------------------\nin  saveImage method:  child.getLocalNameBytes(),the  bytes use the method of str.getBytes(\"UTF8\");\n\nbut in writeINodeUnderConstruction, the bytes user the method of Class  UTF8 to get the bytes.\n\nI make a test use our messy code file name , found the the two bytes arrsy are not equal. so I both use the class UTF8,then the problem desappare.\n\nI think this is a bug of HDFS or UTF8."
        }
    },
    {
        "filename": "HADOOP-11151.json",
        "creation_time": "2014-09-29T08:18:04.000+0000",
        "stack_trace": "```\norg.apache.hadoop.security.authentication.client.AuthenticationException: Anonymous requests are disallowed\n        at org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler.authenticate(PseudoAuthenticationHandler.java:184)\n        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.authenticate(DelegationTokenAuthenticationHandler.java:331)\n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:507)\n        at org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter.doFilter(KMSAuthenticationFilter.java:129)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:861)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:606)\n        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)\n        at java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler.authenticate": "  public AuthenticationToken authenticate(HttpServletRequest request, HttpServletResponse response)\n    throws IOException, AuthenticationException {\n    AuthenticationToken token;\n    String userName = getUserName(request);\n    if (userName == null) {\n      if (getAcceptAnonymous()) {\n        token = AuthenticationToken.ANONYMOUS;\n      } else {\n        throw new AuthenticationException(\"Anonymous requests are disallowed\");\n      }\n    } else {\n      token = new AuthenticationToken(userName, userName, getType());\n    }\n    return token;\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler.getUserName": "  private String getUserName(HttpServletRequest request) {\n    List<NameValuePair> list = URLEncodedUtils.parse(request.getQueryString(), UTF8_CHARSET);\n    if (list != null) {\n      for (NameValuePair nv : list) {\n        if (PseudoAuthenticator.USER_NAME.equals(nv.getName())) {\n          return nv.getValue();\n        }\n      }\n    }\n    return null;\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler.getAcceptAnonymous": "  protected boolean getAcceptAnonymous() {\n    return acceptAnonymous;\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler.getType": "  public String getType() {\n    return type;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.authenticate": "  public AuthenticationToken authenticate(HttpServletRequest request,\n      HttpServletResponse response)\n      throws IOException, AuthenticationException {\n    AuthenticationToken token;\n    String delegationParam = getDelegationToken(request);\n    if (delegationParam != null) {\n      try {\n        Token<DelegationTokenIdentifier> dt =\n            new Token<DelegationTokenIdentifier>();\n        dt.decodeFromUrlString(delegationParam);\n        UserGroupInformation ugi = tokenManager.verifyToken(dt);\n        final String shortName = ugi.getShortUserName();\n\n        // creating a ephemeral token\n        token = new AuthenticationToken(shortName, ugi.getUserName(),\n            getType());\n        token.setExpires(0);\n        request.setAttribute(DELEGATION_TOKEN_UGI_ATTRIBUTE, ugi);\n      } catch (Throwable ex) {\n        token = null;\n        HttpExceptionUtils.createServletExceptionResponse(response,\n            HttpServletResponse.SC_FORBIDDEN, new AuthenticationException(ex));\n      }\n    } else {\n      token = authHandler.authenticate(request, response);\n    }\n    return token;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.getDelegationToken": "  private String getDelegationToken(HttpServletRequest request)\n      throws IOException {\n    String dToken = request.getHeader(\n        DelegationTokenAuthenticator.DELEGATION_TOKEN_HEADER);\n    if (dToken == null) {\n      dToken = ServletUtils.getParameter(request,\n          KerberosDelegationTokenAuthenticator.DELEGATION_PARAM);\n    }\n    return dToken;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.getType": "  public String getType() {\n    return authType;\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter": "  protected void doFilter(FilterChain filterChain, HttpServletRequest request,\n      HttpServletResponse response) throws IOException, ServletException {\n    filterChain.doFilter(request, response);\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.getRequestURL": "  protected String getRequestURL(HttpServletRequest request) {\n    StringBuffer sb = request.getRequestURL();\n    if (request.getQueryString() != null) {\n      sb.append(\"?\").append(request.getQueryString());\n    }\n    return sb.toString();\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.getCookiePath": "  protected String getCookiePath() {\n    return cookiePath;\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.createAuthCookie": "  public static void createAuthCookie(HttpServletResponse resp, String token,\n                                      String domain, String path, long expires,\n                                      boolean isSecure) {\n    StringBuilder sb = new StringBuilder(AuthenticatedURL.AUTH_COOKIE)\n                           .append(\"=\");\n    if (token != null && token.length() > 0) {\n      sb.append(token);\n    }\n    sb.append(\"; Version=1\");\n\n    if (path != null) {\n      sb.append(\"; Path=\").append(path);\n    }\n\n    if (domain != null) {\n      sb.append(\"; Domain=\").append(domain);\n    }\n\n    if (expires >= 0) {\n      Date date = new Date(expires);\n      SimpleDateFormat df = new SimpleDateFormat(\"EEE, \" +\n              \"dd-MMM-yyyy HH:mm:ss zzz\");\n      df.setTimeZone(TimeZone.getTimeZone(\"GMT\"));\n      sb.append(\"; Expires=\").append(df.format(date));\n    }\n\n    if (isSecure) {\n      sb.append(\"; Secure\");\n    }\n\n    sb.append(\"; HttpOnly\");\n    resp.addHeader(\"Set-Cookie\", sb.toString());\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.getCookieDomain": "  protected String getCookieDomain() {\n    return cookieDomain;\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.getToken": "  protected AuthenticationToken getToken(HttpServletRequest request) throws IOException, AuthenticationException {\n    AuthenticationToken token = null;\n    String tokenStr = null;\n    Cookie[] cookies = request.getCookies();\n    if (cookies != null) {\n      for (Cookie cookie : cookies) {\n        if (cookie.getName().equals(AuthenticatedURL.AUTH_COOKIE)) {\n          tokenStr = cookie.getValue();\n          try {\n            tokenStr = signer.verifyAndExtract(tokenStr);\n          } catch (SignerException ex) {\n            throw new AuthenticationException(ex);\n          }\n          break;\n        }\n      }\n    }\n    if (tokenStr != null) {\n      token = AuthenticationToken.parse(tokenStr);\n      if (!token.getType().equals(authHandler.getType())) {\n        throw new AuthenticationException(\"Invalid AuthenticationToken type\");\n      }\n      if (token.isExpired()) {\n        throw new AuthenticationException(\"AuthenticationToken expired\");\n      }\n    }\n    return token;\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.getValidity": "  protected long getValidity() {\n    return validity / 1000;\n  }",
            "hadoop-common-project.hadoop-kms.src.main.java.org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter.doFilter": "  public void doFilter(ServletRequest request, ServletResponse response,\n      FilterChain filterChain) throws IOException, ServletException {\n    KMSResponse kmsResponse = new KMSResponse(response);\n    super.doFilter(request, kmsResponse, filterChain);\n\n    if (kmsResponse.statusCode != HttpServletResponse.SC_OK &&\n        kmsResponse.statusCode != HttpServletResponse.SC_CREATED &&\n        kmsResponse.statusCode != HttpServletResponse.SC_UNAUTHORIZED) {\n      KMSWebApp.getInvalidCallsMeter().mark();\n    }\n\n    // HttpServletResponse.SC_UNAUTHORIZED is because the request does not\n    // belong to an authenticated user.\n    if (kmsResponse.statusCode == HttpServletResponse.SC_UNAUTHORIZED) {\n      KMSWebApp.getUnauthenticatedCallsMeter().mark();\n      String method = ((HttpServletRequest) request).getMethod();\n      StringBuffer requestURL = ((HttpServletRequest) request).getRequestURL();\n      String queryString = ((HttpServletRequest) request).getQueryString();\n      if (queryString != null) {\n        requestURL.append(\"?\").append(queryString);\n      }\n\n      KMSWebApp.getKMSAudit().unauthenticated(\n          request.getRemoteHost(), method, requestURL.toString(),\n          kmsResponse.msg);\n    }\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationToken.getExpires": "  public long getExpires() {\n    return expires;\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationToken.isExpired": "  public boolean isExpired() {\n    return getExpires() != -1 && System.currentTimeMillis() > getExpires();\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationToken.getUserName": "  public String getUserName() {\n    return userName;\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationToken.toString": "  public String toString() {\n    return token;\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationToken.setExpires": "  public void setExpires(long expires) {\n    if (this != AuthenticationToken.ANONYMOUS) {\n      this.expires = expires;\n      generateToken();\n    }\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationToken.generateToken": "  private void generateToken() {\n    StringBuffer sb = new StringBuffer();\n    sb.append(USER_NAME).append(\"=\").append(getUserName()).append(ATTR_SEPARATOR);\n    sb.append(PRINCIPAL).append(\"=\").append(getName()).append(ATTR_SEPARATOR);\n    sb.append(TYPE).append(\"=\").append(getType()).append(ATTR_SEPARATOR);\n    sb.append(EXPIRES).append(\"=\").append(getExpires());\n    token = sb.toString();\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationToken.getType": "  public String getType() {\n    return type;\n  }",
            "hadoop-common-project.hadoop-kms.src.main.java.org.apache.hadoop.crypto.key.kms.server.KMSWebApp.getInvalidCallsMeter": "  public static Meter getInvalidCallsMeter() {\n    return invalidCallsMeter;\n  }",
            "hadoop-common-project.hadoop-kms.src.main.java.org.apache.hadoop.crypto.key.kms.server.KMSWebApp.getKMSAudit": "  public static KMSAudit getKMSAudit() {\n    return kmsAudit;\n  }",
            "hadoop-common-project.hadoop-kms.src.main.java.org.apache.hadoop.crypto.key.kms.server.KMSWebApp.getUnauthenticatedCallsMeter": "  public static Meter getUnauthenticatedCallsMeter() {\n    return unauthenticatedCallsMeter;\n  }"
        },
        "bug_report": {
            "Title": "Automatically refresh auth token and retry on auth failure",
            "Description": "Enable CFS and KMS service in the cluster, initially it worked to put/copy file into encryption zone. But after a while (might be one day), it fails to put/copy file into the encryption zone with the error\njava.util.concurrent.ExecutionException: java.io.IOException: HTTP status [403], message [Forbidden]\n\nThe kms.log shows below\nAbstractDelegationTokenSecretManager - Updating the current master key for generating delegation tokens\n2014-09-29 13:18:46,599 WARN  AuthenticationFilter - AuthenticationToken ignored: org.apache.hadoop.security.authentication.util.SignerException: Invalid signature\n2014-09-29 13:18:46,599 WARN  AuthenticationFilter - Authentication exception: Anonymous requests are disallowed\norg.apache.hadoop.security.authentication.client.AuthenticationException: Anonymous requests are disallowed\n        at org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler.authenticate(PseudoAuthenticationHandler.java:184)\n        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.authenticate(DelegationTokenAuthenticationHandler.java:331)\n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:507)\n        at org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter.doFilter(KMSAuthenticationFilter.java:129)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:861)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:606)\n        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)\n        at java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "HADOOP-8031.json",
        "creation_time": "2012-02-07T20:22:24.000+0000",
        "stack_trace": "```\norg.rhq.core.clientapi.agent.PluginContainerException: Failed to start component for resource Resource[id=16290, type=NameNode, key=NameNode:/usr/lib/hadoop-0.20, name=NameNode, parent=vg61l01ad-hadoop002.apple.com].\nCaused by: java.lang.RuntimeException: core-site.xml not found\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1308)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1228)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1169)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:438)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.loadResource": "  private void loadResource(Properties properties, Object name, boolean quiet) {\n    try {\n      DocumentBuilderFactory docBuilderFactory \n        = DocumentBuilderFactory.newInstance();\n      //ignore all comments inside the xml file\n      docBuilderFactory.setIgnoringComments(true);\n\n      //allow includes in the xml file\n      docBuilderFactory.setNamespaceAware(true);\n      try {\n          docBuilderFactory.setXIncludeAware(true);\n      } catch (UnsupportedOperationException e) {\n        LOG.error(\"Failed to set setXIncludeAware(true) for parser \"\n                + docBuilderFactory\n                + \":\" + e,\n                e);\n      }\n      DocumentBuilder builder = docBuilderFactory.newDocumentBuilder();\n      Document doc = null;\n      Element root = null;\n\n      if (name instanceof URL) {                  // an URL resource\n        URL url = (URL)name;\n        if (url != null) {\n          if (!quiet) {\n            LOG.info(\"parsing \" + url);\n          }\n          doc = builder.parse(url.toString());\n        }\n      } else if (name instanceof String) {        // a CLASSPATH resource\n        URL url = getResource((String)name);\n        if (url != null) {\n          if (!quiet) {\n            LOG.info(\"parsing \" + url);\n          }\n          doc = builder.parse(url.toString());\n        }\n      } else if (name instanceof Path) {          // a file resource\n        // Can't use FileSystem API or we get an infinite loop\n        // since FileSystem uses Configuration API.  Use java.io.File instead.\n        File file = new File(((Path)name).toUri().getPath())\n          .getAbsoluteFile();\n        if (file.exists()) {\n          if (!quiet) {\n            LOG.info(\"parsing \" + file);\n          }\n          InputStream in = new BufferedInputStream(new FileInputStream(file));\n          try {\n            doc = builder.parse(in);\n          } finally {\n            in.close();\n          }\n        }\n      } else if (name instanceof InputStream) {\n        try {\n          doc = builder.parse((InputStream)name);\n        } finally {\n          ((InputStream)name).close();\n        }\n      } else if (name instanceof Element) {\n        root = (Element)name;\n      }\n\n      if (doc == null && root == null) {\n        if (quiet)\n          return;\n        throw new RuntimeException(name + \" not found\");\n      }\n\n      if (root == null) {\n        root = doc.getDocumentElement();\n      }\n      if (!\"configuration\".equals(root.getTagName()))\n        LOG.fatal(\"bad conf file: top-level element not <configuration>\");\n      NodeList props = root.getChildNodes();\n      for (int i = 0; i < props.getLength(); i++) {\n        Node propNode = props.item(i);\n        if (!(propNode instanceof Element))\n          continue;\n        Element prop = (Element)propNode;\n        if (\"configuration\".equals(prop.getTagName())) {\n          loadResource(properties, prop, quiet);\n          continue;\n        }\n        if (!\"property\".equals(prop.getTagName()))\n          LOG.warn(\"bad conf file: element not <property>\");\n        NodeList fields = prop.getChildNodes();\n        String attr = null;\n        String value = null;\n        boolean finalParameter = false;\n        for (int j = 0; j < fields.getLength(); j++) {\n          Node fieldNode = fields.item(j);\n          if (!(fieldNode instanceof Element))\n            continue;\n          Element field = (Element)fieldNode;\n          if (\"name\".equals(field.getTagName()) && field.hasChildNodes())\n            attr = ((Text)field.getFirstChild()).getData().trim();\n          if (\"value\".equals(field.getTagName()) && field.hasChildNodes())\n            value = ((Text)field.getFirstChild()).getData();\n          if (\"final\".equals(field.getTagName()) && field.hasChildNodes())\n            finalParameter = \"true\".equals(((Text)field.getFirstChild()).getData());\n        }\n        \n        // Ignore this parameter if it has already been marked as 'final'\n        if (attr != null) {\n          if (deprecatedKeyMap.containsKey(attr)) {\n            DeprecatedKeyInfo keyInfo = deprecatedKeyMap.get(attr);\n            keyInfo.accessed = false;\n            for (String key:keyInfo.newKeys) {\n              // update new keys with deprecated key's value \n              loadProperty(properties, name, key, value, finalParameter);\n            }\n          }\n          else {\n            loadProperty(properties, name, attr, value, finalParameter);\n          }\n        }\n      }\n        \n    } catch (IOException e) {\n      LOG.fatal(\"error parsing conf file: \" + e);\n      throw new RuntimeException(e);\n    } catch (DOMException e) {\n      LOG.fatal(\"error parsing conf file: \" + e);\n      throw new RuntimeException(e);\n    } catch (SAXException e) {\n      LOG.fatal(\"error parsing conf file: \" + e);\n      throw new RuntimeException(e);\n    } catch (ParserConfigurationException e) {\n      LOG.fatal(\"error parsing conf file: \" + e);\n      throw new RuntimeException(e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.get": "  public String get(String name, String defaultValue) {\n    name = handleDeprecation(name);\n    return substituteVars(getProps().getProperty(name, defaultValue));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.toString": "  private <T> void toString(List<T> resources, StringBuilder sb) {\n    ListIterator<T> i = resources.listIterator();\n    while (i.hasNext()) {\n      if (i.nextIndex() != 0) {\n        sb.append(\", \");\n      }\n      sb.append(i.next());\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.loadProperty": "  private void loadProperty(Properties properties, Object name, String attr,\n      String value, boolean finalParameter) {\n    if (value != null) {\n      if (!finalParameters.contains(attr)) {\n        properties.setProperty(attr, value);\n        updatingResource.put(attr, name.toString());\n      } else if (!value.equals(properties.getProperty(attr))) {\n        LOG.warn(name+\":an attempt to override final parameter: \"+attr\n            +\";  Ignoring.\");\n      }\n    }\n    if (finalParameter) {\n      finalParameters.add(attr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getResource": "  public URL getResource(String name) {\n    return classLoader.getResource(name);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.loadResources": "  private void loadResources(Properties properties,\n                             ArrayList resources,\n                             boolean quiet) {\n    if(loadDefaults) {\n      for (String resource : defaultResources) {\n        loadResource(properties, resource, quiet);\n      }\n    \n      //support the hadoop-site.xml as a deprecated case\n      if(getResource(\"hadoop-site.xml\")!=null) {\n        loadResource(properties, \"hadoop-site.xml\", quiet);\n      }\n    }\n    \n    for (Object resource : resources) {\n      loadResource(properties, resource, quiet);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getProps": "  protected synchronized Properties getProps() {\n    if (properties == null) {\n      properties = new Properties();\n      loadResources(properties, resources, quietmode);\n      if (overlay!= null) {\n        properties.putAll(overlay);\n        for (Map.Entry<Object,Object> item: overlay.entrySet()) {\n          updatingResource.put((String) item.getKey(), UNKNOWN_RESOURCE);\n        }\n      }\n    }\n    return properties;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.set": "  public void set(String name, String value) {\n    if (deprecatedKeyMap.isEmpty()) {\n      getProps();\n    }\n    if (!isDeprecated(name)) {\n      getOverlay().setProperty(name, value);\n      getProps().setProperty(name, value);\n      updatingResource.put(name, UNKNOWN_RESOURCE);\n    }\n    else {\n      DeprecatedKeyInfo keyInfo = deprecatedKeyMap.get(name);\n      LOG.warn(keyInfo.getWarningMessage(name));\n      for (String newKey : keyInfo.newKeys) {\n        getOverlay().setProperty(newKey, value);\n        getProps().setProperty(newKey, value);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getOverlay": "  private synchronized Properties getOverlay() {\n    if (overlay==null){\n      overlay=new Properties();\n    }\n    return overlay;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.isDeprecated": "  private static boolean isDeprecated(String key) {\n    return deprecatedKeyMap.containsKey(key);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getWarningMessage": "    private final String getWarningMessage(String key) {\n      String warningMessage;\n      if(customMessage == null) {\n        StringBuilder message = new StringBuilder(key);\n        String deprecatedKeySuffix = \" is deprecated. Instead, use \";\n        message.append(deprecatedKeySuffix);\n        for (int i = 0; i < newKeys.length; i++) {\n          message.append(newKeys[i]);\n          if(i != newKeys.length-1) {\n            message.append(\", \");\n          }\n        }\n        warningMessage = message.toString();\n      }\n      else {\n        warningMessage = customMessage;\n      }\n      accessed = true;\n      return warningMessage;\n    }"
        },
        "bug_report": {
            "Title": "Configuration class fails to find embedded .jar resources; should use URL.openStream()",
            "Description": "While running a hadoop client within RHQ (monitoring software) using its classloader, I see this:\n\n2012-02-07 09:15:25,313 INFO  [ResourceContainer.invoker.daemon-2] (org.apache.hadoop.conf.Configuration)- parsing jar:file:/usr/local/rhq-agent/data/tmp/rhq-hadoop-plugin-4.3.0-SNAPSHOT.jar6856622641102893436.classloader/hadoop-core-0.20.2+737+1.jar7204287718482036191.tmp!/core-default.xml\n2012-02-07 09:15:25,318 ERROR [InventoryManager.discovery-1] (rhq.core.pc.inventory.InventoryManager)- Failed to start component for Resource[id=16290, type=NameNode, key=NameNode:/usr/lib/hadoop-0.20, name=NameNode, parent=vg61l01ad-hadoop002.apple.com] from synchronized merge.\norg.rhq.core.clientapi.agent.PluginContainerException: Failed to start component for resource Resource[id=16290, type=NameNode, key=NameNode:/usr/lib/hadoop-0.20, name=NameNode, parent=vg61l01ad-hadoop002.apple.com].\nCaused by: java.lang.RuntimeException: core-site.xml not found\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1308)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1228)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1169)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:438)\n\nThis is because the URL\n\njar:file:/usr/local/rhq-agent/data/tmp/rhq-hadoop-plugin-4.3.0-SNAPSHOT.jar6856622641102893436.classloader/hadoop-core-0.20.2+737+1.jar7204287718482036191.tmp!/core-default.xml\n\ncannot be found by DocumentBuilder (doesn't understand it). (Note: the logs are for an old version of Configuration class, but the new version has the same code.)\n\nThe solution is to obtain the resource stream directly from the URL object itself.\n\nThat is to say:\n\n{code}\n         URL url = getResource((String)name);\n-        if (url != null) {\n-          if (!quiet) {\n-            LOG.info(\"parsing \" + url);\n-          }\n-          doc = builder.parse(url.toString());\n-        }\n+        doc = builder.parse(url.openStream());\n{code}\n\nNote: I have a full patch pending approval at Apple for this change, including some cleanup."
        }
    },
    {
        "filename": "HADOOP-15411.json",
        "creation_time": "2018-04-24T23:18:39.000+0000",
        "stack_trace": "```\norg.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.\n at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:117)\n at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\n at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\n at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:919)\n at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:979)\nCaused by: org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server\n at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:377)\n at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:424)\n at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:420)\n at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:112)\n ... 5 more\nCaused by: java.io.IOException: java.util.ConcurrentModificationException\n at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:532)\n at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:117)\n at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:421)\n at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:333)\n ... 8 more\nCaused by: java.util.ConcurrentModificationException\n at java.util.Hashtable$Enumerator.next(Hashtable.java:1383)\n at org.apache.hadoop.conf.Configuration.iterator(Configuration.java:2853)\n at org.apache.hadoop.security.AuthenticationFilterInitializer.getFilterConfigMap(AuthenticationFilterInitializer.java:73)\n at org.apache.hadoop.http.HttpServer2.getFilterProperties(HttpServer2.java:647)\n at org.apache.hadoop.http.HttpServer2.constructSecretProvider(HttpServer2.java:637)\n at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:525)```",
        "source_code": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart": "  protected void serviceStart() throws Exception {\n    Configuration conf = getConfig();\n    String bindAddress = WebAppUtils.getWebAppBindURL(conf,\n                          YarnConfiguration.NM_BIND_HOST,\n                          WebAppUtils.getNMWebAppURLWithoutScheme(conf));\n    boolean enableCors = conf\n        .getBoolean(YarnConfiguration.NM_WEBAPP_ENABLE_CORS_FILTER,\n            YarnConfiguration.DEFAULT_NM_WEBAPP_ENABLE_CORS_FILTER);\n    if (enableCors) {\n      getConfig().setBoolean(HttpCrossOriginFilterInitializer.PREFIX\n          + HttpCrossOriginFilterInitializer.ENABLED_SUFFIX, true);\n    }\n\n    // Always load pseudo authentication filter to parse \"user.name\" in an URL\n    // to identify a HTTP request's user.\n    boolean hasHadoopAuthFilterInitializer = false;\n    String filterInitializerConfKey = \"hadoop.http.filter.initializers\";\n    Class<?>[] initializersClasses =\n            conf.getClasses(filterInitializerConfKey);\n    List<String> targets = new ArrayList<String>();\n    if (initializersClasses != null) {\n      for (Class<?> initializer : initializersClasses) {\n        if (initializer.getName().equals(\n            AuthenticationFilterInitializer.class.getName())) {\n          hasHadoopAuthFilterInitializer = true;\n          break;\n        }\n        targets.add(initializer.getName());\n      }\n    }\n    if (!hasHadoopAuthFilterInitializer) {\n      targets.add(AuthenticationFilterInitializer.class.getName());\n      conf.set(filterInitializerConfKey, StringUtils.join(\",\", targets));\n    }\n    LOG.info(\"Instantiating NMWebApp at \" + bindAddress);\n    try {\n      this.webApp =\n          WebApps\n            .$for(\"node\", Context.class, this.nmContext, \"ws\")\n            .at(bindAddress)\n            .with(conf)\n            .withHttpSpnegoPrincipalKey(\n              YarnConfiguration.NM_WEBAPP_SPNEGO_USER_NAME_KEY)\n            .withHttpSpnegoKeytabKey(\n                YarnConfiguration.NM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY)\n              .withCSRFProtection(YarnConfiguration.NM_CSRF_PREFIX)\n              .withXFSProtection(YarnConfiguration.NM_XFS_PREFIX)\n            .start(this.nmWebApp);\n      this.port = this.webApp.httpServer().getConnectorAddress(0).getPort();\n    } catch (Exception e) {\n      String msg = \"NMWebapps failed to start.\";\n      LOG.error(msg, e);\n      throw new YarnRuntimeException(msg, e);\n    }\n    super.serviceStart();\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.getPort": "  public int getPort() {\n    return this.port;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.start": "  public void start() {\n    if (isInState(STATE.STARTED)) {\n      return;\n    }\n    //enter the started state\n    synchronized (stateChangeLock) {\n      if (stateModel.enterState(STATE.STARTED) != STATE.STARTED) {\n        try {\n          startTime = System.currentTimeMillis();\n          serviceStart();\n          if (isInState(STATE.STARTED)) {\n            //if the service started (and isn't now in a later state), notify\n            LOG.debug(\"Service {} is started\", getName());\n            notifyListeners();\n          }\n        } catch (Exception e) {\n          noteFailure(e);\n          ServiceOperations.stopQuietly(LOG, this);\n          throw ServiceStateException.convert(e);\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.isInState": "  public final boolean isInState(Service.STATE expected) {\n    return stateModel.isInState(expected);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.getName": "  public String getName() {\n    return name;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.serviceStart": "  protected void serviceStart() throws Exception {\n\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.enterState": "  private STATE enterState(STATE newState) {\n    assert stateModel != null : \"null state in \" + name + \" \" + this.getClass();\n    STATE oldState = stateModel.enterState(newState);\n    if (oldState != newState) {\n      LOG.debug(\"Service: {} entered state {}\", getName(), getServiceState());\n\n      recordLifecycleEvent();\n    }\n    return oldState;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.noteFailure": "  protected final void noteFailure(Exception exception) {\n    LOG.debug(\"noteFailure {}\" + exception);\n    if (exception == null) {\n      //make sure failure logic doesn't itself cause problems\n      return;\n    }\n    //record the failure details, and log it\n    synchronized (this) {\n      if (failureCause == null) {\n        failureCause = exception;\n        failureState = getServiceState();\n        LOG.info(\"Service {} failed in state {}\",\n            getName(), failureState, exception);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.notifyListeners": "  private void notifyListeners() {\n    try {\n      listeners.notifyListeners(this);\n      globalListeners.notifyListeners(this);\n    } catch (Throwable e) {\n      LOG.warn(\"Exception while notifying listeners of {}\", this, e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.CompositeService.serviceStart": "  protected void serviceStart() throws Exception {\n    List<Service> services = getServices();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(getName() + \": starting services, size=\" + services.size());\n    }\n    for (Service service : services) {\n      // start the service. If this fails that service\n      // will be stopped and an exception raised\n      service.start();\n    }\n    super.serviceStart();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.CompositeService.getServices": "  public List<Service> getServices() {\n    synchronized (serviceList) {\n      return new ArrayList<Service>(serviceList);\n    }\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager": "  private void initAndStartNodeManager(Configuration conf, boolean hasToReboot) {\n    try {\n      // Failed to start if we're a Unix based system but we don't have bash.\n      // Bash is necessary to launch containers under Unix-based systems.\n      if (!Shell.WINDOWS) {\n        if (!Shell.checkIsBashSupported()) {\n          String message =\n              \"Failing NodeManager start since we're on a \"\n                  + \"Unix-based system but bash doesn't seem to be available.\";\n          LOG.error(message);\n          throw new YarnRuntimeException(message);\n        }\n      }\n\n      // Remove the old hook if we are rebooting.\n      if (hasToReboot && null != nodeManagerShutdownHook) {\n        ShutdownHookManager.get().removeShutdownHook(nodeManagerShutdownHook);\n      }\n\n      nodeManagerShutdownHook = new CompositeServiceShutdownHook(this);\n      ShutdownHookManager.get().addShutdownHook(nodeManagerShutdownHook,\n                                                SHUTDOWN_HOOK_PRIORITY);\n      // System exit should be called only when NodeManager is instantiated from\n      // main() funtion\n      this.shouldExitOnShutdownEvent = true;\n      this.init(conf);\n      this.start();\n    } catch (Throwable t) {\n      LOG.error(\"Error starting NodeManager\", t);\n      System.exit(-1);\n    }\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.init": "    public void init(Context context) {}",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.main": "  public static void main(String[] args) throws IOException {\n    Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\n    StringUtils.startupShutdownMessage(NodeManager.class, args, LOG);\n    @SuppressWarnings(\"resource\")\n    NodeManager nodeManager = new NodeManager();\n    Configuration conf = new YarnConfiguration();\n    new GenericOptionsParser(conf, args);\n    nodeManager.initAndStartNodeManager(conf, false);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.build": "    public HttpServer2 build() throws IOException {\n      Preconditions.checkNotNull(name, \"name is not set\");\n      Preconditions.checkState(!endpoints.isEmpty(), \"No endpoints specified\");\n\n      if (hostName == null) {\n        hostName = endpoints.get(0).getHost();\n      }\n\n      if (this.conf == null) {\n        conf = new Configuration();\n      }\n\n      HttpServer2 server = new HttpServer2(this);\n\n      if (this.securityEnabled) {\n        server.initSpnego(conf, hostName, usernameConfKey, keytabConfKey);\n      }\n\n      for (URI ep : endpoints) {\n        if (HTTPS_SCHEME.equals(ep.getScheme())) {\n          loadSSLConfiguration();\n          break;\n        }\n      }\n\n      int requestHeaderSize = conf.getInt(\n          HTTP_MAX_REQUEST_HEADER_SIZE_KEY,\n          HTTP_MAX_REQUEST_HEADER_SIZE_DEFAULT);\n      int responseHeaderSize = conf.getInt(\n          HTTP_MAX_RESPONSE_HEADER_SIZE_KEY,\n          HTTP_MAX_RESPONSE_HEADER_SIZE_DEFAULT);\n\n      HttpConfiguration httpConfig = new HttpConfiguration();\n      httpConfig.setRequestHeaderSize(requestHeaderSize);\n      httpConfig.setResponseHeaderSize(responseHeaderSize);\n      httpConfig.setSendServerVersion(false);\n\n      int backlogSize = conf.getInt(HTTP_SOCKET_BACKLOG_SIZE_KEY,\n          HTTP_SOCKET_BACKLOG_SIZE_DEFAULT);\n\n      for (URI ep : endpoints) {\n        final ServerConnector connector;\n        String scheme = ep.getScheme();\n        if (HTTP_SCHEME.equals(scheme)) {\n          connector = createHttpChannelConnector(server.webServer,\n              httpConfig);\n        } else if (HTTPS_SCHEME.equals(scheme)) {\n          connector = createHttpsChannelConnector(server.webServer,\n              httpConfig);\n        } else {\n          throw new HadoopIllegalArgumentException(\n              \"unknown scheme for endpoint:\" + ep);\n        }\n        connector.setHost(ep.getHost());\n        connector.setPort(ep.getPort() == -1 ? 0 : ep.getPort());\n        connector.setAcceptQueueSize(backlogSize);\n        server.addListener(connector);\n      }\n      server.loadListeners();\n      return server;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.createHttpsChannelConnector": "    private ServerConnector createHttpsChannelConnector(\n        Server server, HttpConfiguration httpConfig) {\n      httpConfig.setSecureScheme(HTTPS_SCHEME);\n      httpConfig.addCustomizer(new SecureRequestCustomizer());\n      ServerConnector conn = createHttpChannelConnector(server, httpConfig);\n\n      SslContextFactory sslContextFactory = new SslContextFactory();\n      sslContextFactory.setNeedClientAuth(needsClientAuth);\n      sslContextFactory.setKeyManagerPassword(keyPassword);\n      if (keyStore != null) {\n        sslContextFactory.setKeyStorePath(keyStore);\n        sslContextFactory.setKeyStoreType(keyStoreType);\n        sslContextFactory.setKeyStorePassword(keyStorePassword);\n      }\n      if (trustStore != null) {\n        sslContextFactory.setTrustStorePath(trustStore);\n        sslContextFactory.setTrustStoreType(trustStoreType);\n        sslContextFactory.setTrustStorePassword(trustStorePassword);\n      }\n      if(null != excludeCiphers && !excludeCiphers.isEmpty()) {\n        sslContextFactory.setExcludeCipherSuites(\n            StringUtils.getTrimmedStrings(excludeCiphers));\n        LOG.info(\"Excluded Cipher List:\" + excludeCiphers);\n      }\n\n      conn.addFirstConnectionFactory(new SslConnectionFactory(sslContextFactory,\n          HttpVersion.HTTP_1_1.asString()));\n\n      return conn;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.getPort": "  public int getPort() {\n    return ((ServerConnector)webServer.getConnectors()[0]).getLocalPort();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.initSpnego": "  private void initSpnego(Configuration conf, String hostName,\n      String usernameConfKey, String keytabConfKey) throws IOException {\n    Map<String, String> params = new HashMap<>();\n    String principalInConf = conf.get(usernameConfKey);\n    if (principalInConf != null && !principalInConf.isEmpty()) {\n      params.put(\"kerberos.principal\", SecurityUtil.getServerPrincipal(\n          principalInConf, hostName));\n    }\n    String httpKeytab = conf.get(keytabConfKey);\n    if (httpKeytab != null && !httpKeytab.isEmpty()) {\n      params.put(\"kerberos.keytab\", httpKeytab);\n    }\n    params.put(AuthenticationFilter.AUTH_TYPE, \"kerberos\");\n\n    defineFilter(webAppContext, SPNEGO_FILTER,\n                 AuthenticationFilter.class.getName(), params, null);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.addListener": "  private void addListener(ServerConnector connector) {\n    listeners.add(connector);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.loadListeners": "  private void loadListeners() {\n    for (Connector c : listeners) {\n      webServer.addConnector(c);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.createHttpChannelConnector": "    private ServerConnector createHttpChannelConnector(\n        Server server, HttpConfiguration httpConfig) {\n      ServerConnector conn = new ServerConnector(server,\n          conf.getInt(HTTP_ACCEPTOR_COUNT_KEY, HTTP_ACCEPTOR_COUNT_DEFAULT),\n          conf.getInt(HTTP_SELECTOR_COUNT_KEY, HTTP_SELECTOR_COUNT_DEFAULT));\n      ConnectionFactory connFactory = new HttpConnectionFactory(httpConfig);\n      conn.addConnectionFactory(connFactory);\n      configureChannelConnector(conn);\n      return conn;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.loadSSLConfiguration": "    private void loadSSLConfiguration() throws IOException {\n      if (sslConf == null) {\n        return;\n      }\n      needsClientAuth = sslConf.getBoolean(\n          SSLFactory.SSL_SERVER_NEED_CLIENT_AUTH,\n          SSLFactory.SSL_SERVER_NEED_CLIENT_AUTH_DEFAULT);\n      keyStore = sslConf.getTrimmed(SSLFactory.SSL_SERVER_KEYSTORE_LOCATION);\n      if (keyStore == null || keyStore.isEmpty()) {\n        throw new IOException(String.format(\"Property %s not specified\",\n            SSLFactory.SSL_SERVER_KEYSTORE_LOCATION));\n      }\n      keyStorePassword = getPasswordString(sslConf,\n          SSLFactory.SSL_SERVER_KEYSTORE_PASSWORD);\n      if (keyStorePassword == null) {\n        throw new IOException(String.format(\"Property %s not specified\",\n            SSLFactory.SSL_SERVER_KEYSTORE_PASSWORD));\n      }\n      keyStoreType = sslConf.get(SSLFactory.SSL_SERVER_KEYSTORE_TYPE,\n          SSLFactory.SSL_SERVER_KEYSTORE_TYPE_DEFAULT);\n      keyPassword = getPasswordString(sslConf,\n          SSLFactory.SSL_SERVER_KEYSTORE_KEYPASSWORD);\n      trustStore = sslConf.get(SSLFactory.SSL_SERVER_TRUSTSTORE_LOCATION);\n      trustStorePassword = getPasswordString(sslConf,\n          SSLFactory.SSL_SERVER_TRUSTSTORE_PASSWORD);\n      trustStoreType = sslConf.get(SSLFactory.SSL_SERVER_TRUSTSTORE_TYPE,\n          SSLFactory.SSL_SERVER_TRUSTSTORE_TYPE_DEFAULT);\n      excludeCiphers = sslConf.get(SSLFactory.SSL_SERVER_EXCLUDE_CIPHER_LIST);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.iterator": "  public Iterator<Map.Entry<String, String>> iterator() {\n    // Get a copy of just the string to string pairs. After the old object\n    // methods that allow non-strings to be put into configurations are removed,\n    // we could replace properties with a Map<String,String> and get rid of this\n    // code.\n    Map<String,String> result = new HashMap<String,String>();\n    for(Map.Entry<Object,Object> item: getProps().entrySet()) {\n      if (item.getKey() instanceof String &&\n          item.getValue() instanceof String) {\n          result.put((String) item.getKey(), (String) item.getValue());\n      }\n    }\n    return result.entrySet().iterator();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getProps": "  protected synchronized Properties getProps() {\n    if (properties == null) {\n      properties = new Properties();\n      Map<String, String[]> backup = updatingResource != null ?\n          new ConcurrentHashMap<String, String[]>(updatingResource) : null;\n      loadResources(properties, resources, quietmode);\n\n      if (overlay != null) {\n        properties.putAll(overlay);\n        if (backup != null) {\n          for (Map.Entry<Object, Object> item : overlay.entrySet()) {\n            String key = (String) item.getKey();\n            String[] source = backup.get(key);\n            if (source != null) {\n              updatingResource.put(key, source);\n            }\n          }\n        }\n      }\n    }\n    return properties;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getKey": "    public String getKey() {\n      return key;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.AuthenticationFilterInitializer.getFilterConfigMap": "  public static Map<String, String> getFilterConfigMap(Configuration conf,\n      String prefix) {\n    Map<String, String> filterConfig = new HashMap<String, String>();\n\n    //setting the cookie path to root '/' so it is used for all resources.\n    filterConfig.put(AuthenticationFilter.COOKIE_PATH, \"/\");\n\n    for (Map.Entry<String, String> entry : conf) {\n      String name = entry.getKey();\n      if (name.startsWith(prefix)) {\n        String value = conf.get(name);\n        name = name.substring(prefix.length());\n        filterConfig.put(name, value);\n      }\n    }\n\n    //Resolve _HOST into bind address\n    String bindAddress = conf.get(HttpServer2.BIND_ADDRESS);\n    String principal = filterConfig.get(KerberosAuthenticationHandler.PRINCIPAL);\n    if (principal != null) {\n      try {\n        principal = SecurityUtil.getServerPrincipal(principal, bindAddress);\n      }\n      catch (IOException ex) {\n        throw new RuntimeException(\"Could not resolve Kerberos principal name: \" + ex.toString(), ex);\n      }\n      filterConfig.put(KerberosAuthenticationHandler.PRINCIPAL, principal);\n    }\n    return filterConfig;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.getFilterProperties": "  private static Properties getFilterProperties(Configuration conf, String\n      prefix) {\n    Properties prop = new Properties();\n    Map<String, String> filterConfig = AuthenticationFilterInitializer\n        .getFilterConfigMap(conf, prefix);\n    prop.putAll(filterConfig);\n    return prop;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.constructSecretProvider": "  private static SignerSecretProvider constructSecretProvider(final Builder b,\n      ServletContext ctx)\n      throws Exception {\n    final Configuration conf = b.conf;\n    Properties config = getFilterProperties(conf,\n                                            b.authFilterConfigurationPrefix);\n    return AuthenticationFilter.constructSecretProvider(\n        ctx, config, b.disallowFallbackToRandomSignerSecretProvider);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceOperations.stopQuietly": "  public static Exception stopQuietly(Logger log, Service service) {\n    try {\n      stop(service);\n    } catch (Exception e) {\n      log.warn(\"When stopping the service {}\", service.getName(), e);\n      return e;\n    }\n    return null;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceOperations.stop": "  public static void stop(Service service) {\n    if (service != null) {\n      service.stop();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceStateException.convert": "  public static RuntimeException convert(String text, Throwable fault) {\n    if (fault instanceof RuntimeException) {\n      return (RuntimeException) fault;\n    } else {\n      return new ServiceStateException(text, fault);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.Service.start": "  void start();\n\n  /**\n   * Stop the service. This MUST be a no-op if the service is already\n   * in the {@link STATE#STOPPED} state. It SHOULD be a best-effort attempt",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SecurityUtil.getServerPrincipal": "  public static String getServerPrincipal(String principalConfig,\n      InetAddress addr) throws IOException {\n    String[] components = getComponents(principalConfig);\n    if (components == null || components.length != 3\n        || !components[1].equals(HOSTNAME_PATTERN)) {\n      return principalConfig;\n    } else {\n      if (addr == null) {\n        throw new IOException(\"Can't replace \" + HOSTNAME_PATTERN\n            + \" pattern since client address is null\");\n      }\n      return replacePattern(components, addr.getCanonicalHostName());\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SecurityUtil.getComponents": "  private static String[] getComponents(String principalConfig) {\n    if (principalConfig == null)\n      return null;\n    return principalConfig.split(\"[/@]\");\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SecurityUtil.replacePattern": "  private static String replacePattern(String[] components, String hostname)\n      throws IOException {\n    String fqdn = hostname;\n    if (fqdn == null || fqdn.isEmpty() || fqdn.equals(\"0.0.0.0\")) {\n      fqdn = getLocalHostName(null);\n    }\n    return components[0] + \"/\" +\n        StringUtils.toLowerCase(fqdn) + \"@\" + components[2];\n  }"
        },
        "bug_report": {
            "Title": "AuthenticationFilter should use Configuration.getPropsWithPrefix instead of iterator",
            "Description": "Node manager\u00a0start up fails\u00a0with the following stack trace\r\n\r\n{code}\r\n2018-04-19 13:08:30,638 ERROR nodemanager.NodeManager (NodeManager.java:initAndStartNodeManager(921)) - Error starting NodeManager\r\norg.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.\r\n at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:117)\r\n at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\r\n at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:919)\r\n at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:979)\r\nCaused by: org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server\r\n at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:377)\r\n at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:424)\r\n at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:420)\r\n at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:112)\r\n ... 5 more\r\nCaused by: java.io.IOException: java.util.ConcurrentModificationException\r\n at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:532)\r\n at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:117)\r\n at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:421)\r\n at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:333)\r\n ... 8 more\r\nCaused by: java.util.ConcurrentModificationException\r\n at java.util.Hashtable$Enumerator.next(Hashtable.java:1383)\r\n at org.apache.hadoop.conf.Configuration.iterator(Configuration.java:2853)\r\n at org.apache.hadoop.security.AuthenticationFilterInitializer.getFilterConfigMap(AuthenticationFilterInitializer.java:73)\r\n at org.apache.hadoop.http.HttpServer2.getFilterProperties(HttpServer2.java:647)\r\n at org.apache.hadoop.http.HttpServer2.constructSecretProvider(HttpServer2.java:637)\r\n at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:525)\r\n ... 11 more\r\n2018-04-19 13:08:30,639 INFO timeline.HadoopTimelineMetricsSink (AbstractTimelineMetricsSink.java:getCurrentCollectorHost(291)) - No live collector to send metrics to. Metrics to be sent will be discarded. This message will be skipped for the next 20 times.\r\n{code}"
        }
    },
    {
        "filename": "HADOOP-15850.json",
        "creation_time": "2018-10-13T14:24:29.000+0000",
        "stack_trace": "```\njava.io.IOException: Inconsistent sequence file: current chunk file org.apache.hadoop.tools.CopyListingFileStatus@bb8826ee{hdfs://localhost:42796/user/hbase/test-data/       160aeab5-6bca-9f87-465e-2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/a7599081e835440eb7bf0dd3ef4fd7a5_SeqId_205_ length = 5100 aclEntries  = null, xAttrs = null} doesnt match prior entry org.apache.hadoop.tools.CopyListingFileStatus@243d544d{hdfs://localhost:42796/user/hbase/test-data/160aeab5-6bca-9f87-465e-   2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/394e6d39a9b94b148b9089c4fb967aad_SeqId_205_ length = 5142 aclEntries = null, xAttrs = null}\n  at org.apache.hadoop.tools.mapred.CopyCommitter.concatFileChunks(CopyCommitter.java:276)\n  at org.apache.hadoop.tools.mapred.CopyCommitter.commitJob(CopyCommitter.java:100)\n  at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:567)\n```",
        "source_code": {
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.mapred.CopyCommitter.concatFileChunks": "  private void concatFileChunks(Configuration conf, Path targetFile,\n      LinkedList<Path> allChunkPaths) throws IOException {\n    if (allChunkPaths.size() == 1) {\n      return;\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat \" + targetFile + \" allChunkSize+ \"\n          + allChunkPaths.size());\n    }\n    FileSystem dstfs = targetFile.getFileSystem(conf);\n\n    Path firstChunkFile = allChunkPaths.removeFirst();\n    Path[] restChunkFiles = new Path[allChunkPaths.size()];\n    allChunkPaths.toArray(restChunkFiles);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat: firstchunk: \" + dstfs.getFileStatus(firstChunkFile));\n      int i = 0;\n      for (Path f : restChunkFiles) {\n        LOG.debug(\"concat: other chunk: \" + i + \": \" + dstfs.getFileStatus(f));\n        ++i;\n      }\n    }\n    dstfs.concat(firstChunkFile, restChunkFiles);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat: result: \" + dstfs.getFileStatus(firstChunkFile));\n    }\n    rename(dstfs, firstChunkFile, targetFile);\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.mapred.CopyCommitter.rename": "  private static void rename(FileSystem destFileSys, Path tmp, Path dst)\n      throws IOException {\n    try {\n      if (destFileSys.exists(dst)) {\n        destFileSys.delete(dst, true);\n      }\n      destFileSys.rename(tmp, dst);\n    } catch (IOException ioe) {\n      throw new IOException(\"Fail to rename tmp file (=\" + tmp\n          + \") to destination file (=\" + dst + \")\", ioe);\n    }\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.mapred.CopyCommitter.isFileNotFoundException": "  private boolean isFileNotFoundException(IOException e) {\n    if (e instanceof FileNotFoundException) {\n      return true;\n    }\n\n    if (e instanceof RemoteException) {\n      return ((RemoteException)e).unwrapRemoteException()\n          instanceof FileNotFoundException;\n    }\n\n    return false;\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.mapred.CopyCommitter.commitJob": "  public void commitJob(JobContext jobContext) throws IOException {\n    Configuration conf = jobContext.getConfiguration();\n    syncFolder = conf.getBoolean(DistCpConstants.CONF_LABEL_SYNC_FOLDERS, false);\n    overwrite = conf.getBoolean(DistCpConstants.CONF_LABEL_OVERWRITE, false);\n    targetPathExists = conf.getBoolean(\n        DistCpConstants.CONF_LABEL_TARGET_PATH_EXISTS, true);\n    ignoreFailures = conf.getBoolean(\n        DistCpOptionSwitch.IGNORE_FAILURES.getConfigLabel(), false);\n\n    concatFileChunks(conf);\n\n    super.commitJob(jobContext);\n\n    cleanupTempFiles(jobContext);\n\n    String attributes = conf.get(DistCpConstants.CONF_LABEL_PRESERVE_STATUS);\n    final boolean preserveRawXattrs =\n        conf.getBoolean(DistCpConstants.CONF_LABEL_PRESERVE_RAWXATTRS, false);\n    if ((attributes != null && !attributes.isEmpty()) || preserveRawXattrs) {\n      preserveFileAttributesForDirectories(conf);\n    }\n\n    try {\n      if (conf.getBoolean(DistCpConstants.CONF_LABEL_DELETE_MISSING, false)) {\n        deleteMissing(conf);\n      } else if (conf.getBoolean(DistCpConstants.CONF_LABEL_ATOMIC_COPY, false)) {\n        commitData(conf);\n      } else if (conf.get(CONF_LABEL_TRACK_MISSING) != null) {\n        // save missing information to a directory\n        trackMissing(conf);\n      }\n      taskAttemptContext.setStatus(\"Commit Successful\");\n    }\n    finally {\n      cleanup(conf);\n    }\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.mapred.CopyCommitter.preserveFileAttributesForDirectories": "  private void preserveFileAttributesForDirectories(Configuration conf)\n      throws IOException {\n    String attrSymbols = conf.get(DistCpConstants.CONF_LABEL_PRESERVE_STATUS);\n    final boolean syncOrOverwrite = syncFolder || overwrite;\n\n    LOG.info(\"About to preserve attributes: \" + attrSymbols);\n\n    EnumSet<FileAttribute> attributes = DistCpUtils.unpackAttributes(attrSymbols);\n    final boolean preserveRawXattrs =\n        conf.getBoolean(DistCpConstants.CONF_LABEL_PRESERVE_RAWXATTRS, false);\n\n    Path sourceListing = new Path(conf.get(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH));\n    FileSystem clusterFS = sourceListing.getFileSystem(conf);\n    SequenceFile.Reader sourceReader = new SequenceFile.Reader(conf,\n                                      SequenceFile.Reader.file(sourceListing));\n    long totalLen = clusterFS.getFileStatus(sourceListing).getLen();\n\n    Path targetRoot = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));\n\n    long preservedEntries = 0;\n    try {\n      CopyListingFileStatus srcFileStatus = new CopyListingFileStatus();\n      Text srcRelPath = new Text();\n\n      // Iterate over every source path that was copied.\n      while (sourceReader.next(srcRelPath, srcFileStatus)) {\n        // File-attributes for files are set at the time of copy,\n        // in the map-task.\n        if (! srcFileStatus.isDirectory()) continue;\n\n        Path targetFile = new Path(targetRoot.toString() + \"/\" + srcRelPath);\n        //\n        // Skip the root folder when syncOrOverwrite is true.\n        //\n        if (targetRoot.equals(targetFile) && syncOrOverwrite) continue;\n\n        FileSystem targetFS = targetFile.getFileSystem(conf);\n        DistCpUtils.preserve(targetFS, targetFile, srcFileStatus, attributes,\n            preserveRawXattrs);\n\n        taskAttemptContext.progress();\n        taskAttemptContext.setStatus(\"Preserving status on directory entries. [\" +\n            sourceReader.getPosition() * 100 / totalLen + \"%]\");\n      }\n    } finally {\n      IOUtils.closeStream(sourceReader);\n    }\n    LOG.info(\"Preserved status on \" + preservedEntries + \" dir entries on target\");\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.mapred.CopyCommitter.trackMissing": "  private void trackMissing(Configuration conf) throws IOException {\n    // destination directory for all output files\n    Path trackDir = new Path(\n        conf.get(DistCpConstants.CONF_LABEL_TRACK_MISSING));\n\n    // where is the existing source listing?\n    Path sourceListing = new Path(\n        conf.get(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH));\n    LOG.info(\"Tracking file changes to directory {}\", trackDir);\n\n    // the destination path is under the track directory\n    Path sourceSortedListing = new Path(trackDir,\n        DistCpConstants.SOURCE_SORTED_FILE);\n    LOG.info(\"Source listing {}\", sourceSortedListing);\n\n    DistCpUtils.sortListing(conf, sourceListing, sourceSortedListing);\n\n    // Similarly, create the listing of target-files. Sort alphabetically.\n    // target listing will be deleted after the sort\n    Path targetListing = new Path(trackDir, TARGET_LISTING_FILE);\n    Path sortedTargetListing = new Path(trackDir, TARGET_SORTED_FILE);\n    // list the target\n    listTargetFiles(conf, targetListing, sortedTargetListing);\n    LOG.info(\"Target listing {}\", sortedTargetListing);\n\n    targetListing.getFileSystem(conf).delete(targetListing, false);\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.mapred.CopyCommitter.cleanup": "  private void cleanup(Configuration conf) {\n    Path metaFolder = new Path(conf.get(DistCpConstants.CONF_LABEL_META_FOLDER));\n    try {\n      FileSystem fs = metaFolder.getFileSystem(conf);\n      LOG.info(\"Cleaning up temporary work folder: \" + metaFolder);\n      fs.delete(metaFolder, true);\n    } catch (IOException ignore) {\n      LOG.error(\"Exception encountered \", ignore);\n    }\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.mapred.CopyCommitter.cleanupTempFiles": "  private void cleanupTempFiles(JobContext context) {\n    try {\n      Configuration conf = context.getConfiguration();\n\n      Path targetWorkPath = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));\n      FileSystem targetFS = targetWorkPath.getFileSystem(conf);\n\n      String jobId = context.getJobID().toString();\n      deleteAttemptTempFiles(targetWorkPath, targetFS, jobId);\n      deleteAttemptTempFiles(targetWorkPath.getParent(), targetFS, jobId);\n    } catch (Throwable t) {\n      LOG.warn(\"Unable to cleanup temp files\", t);\n    }\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.mapred.CopyCommitter.deleteMissing": "  private void deleteMissing(Configuration conf) throws IOException {\n    LOG.info(\"-delete option is enabled. About to remove entries from \" +\n        \"target that are missing in source\");\n    long listingStart = System.currentTimeMillis();\n\n    // Sort the source-file listing alphabetically.\n    Path sourceListing = new Path(conf.get(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH));\n    FileSystem clusterFS = sourceListing.getFileSystem(conf);\n    Path sortedSourceListing = DistCpUtils.sortListing(conf, sourceListing);\n    long sourceListingCompleted = System.currentTimeMillis();\n    LOG.info(\"Source listing completed in {}\",\n        formatDuration(sourceListingCompleted - listingStart));\n\n    // Similarly, create the listing of target-files. Sort alphabetically.\n    Path targetListing = new Path(sourceListing.getParent(), \"targetListing.seq\");\n    Path sortedTargetListing = new Path(targetListing.toString() + \"_sorted\");\n\n    Path targetFinalPath = listTargetFiles(conf,\n        targetListing, sortedTargetListing);\n    long totalLen = clusterFS.getFileStatus(sortedTargetListing).getLen();\n\n    SequenceFile.Reader sourceReader = new SequenceFile.Reader(conf,\n                                 SequenceFile.Reader.file(sortedSourceListing));\n    SequenceFile.Reader targetReader = new SequenceFile.Reader(conf,\n                                 SequenceFile.Reader.file(sortedTargetListing));\n\n    // Walk both source and target file listings.\n    // Delete all from target that doesn't also exist on source.\n    long deletionStart = System.currentTimeMillis();\n    LOG.info(\"Destination listing completed in {}\",\n        formatDuration(deletionStart - sourceListingCompleted));\n\n    long deletedEntries = 0;\n    long filesDeleted = 0;\n    long missingDeletes = 0;\n    long failedDeletes = 0;\n    long skippedDeletes = 0;\n    long deletedDirectories = 0;\n    // this is an arbitrary constant.\n    final DeletedDirTracker tracker = new DeletedDirTracker(1000);\n    try {\n      CopyListingFileStatus srcFileStatus = new CopyListingFileStatus();\n      Text srcRelPath = new Text();\n      CopyListingFileStatus trgtFileStatus = new CopyListingFileStatus();\n      Text trgtRelPath = new Text();\n\n      final FileSystem targetFS = targetFinalPath.getFileSystem(conf);\n      boolean showProgress;\n      boolean srcAvailable = sourceReader.next(srcRelPath, srcFileStatus);\n      while (targetReader.next(trgtRelPath, trgtFileStatus)) {\n        // Skip sources that don't exist on target.\n        while (srcAvailable && trgtRelPath.compareTo(srcRelPath) > 0) {\n          srcAvailable = sourceReader.next(srcRelPath, srcFileStatus);\n        }\n        Path targetEntry = trgtFileStatus.getPath();\n        LOG.debug(\"Comparing {} and {}\",\n            srcFileStatus.getPath(), targetEntry);\n\n        if (srcAvailable && trgtRelPath.equals(srcRelPath)) continue;\n\n        // Target doesn't exist at source. Try to delete it.\n        if (tracker.shouldDelete(trgtFileStatus)) {\n          showProgress = true;\n          try {\n            if (targetFS.delete(targetEntry, true)) {\n              // the delete worked. Unless the file is actually missing, this is the\n              LOG.info(\"Deleted \" + targetEntry + \" - missing at source\");\n              deletedEntries++;\n              if (trgtFileStatus.isDirectory()) {\n                deletedDirectories++;\n              } else {\n                filesDeleted++;\n              }\n            } else {\n              // delete returned false.\n              // For all the filestores which implement the FS spec properly,\n              // this means \"the file wasn't there\".\n              // so track but don't worry about it.\n              LOG.info(\"delete({}) returned false ({})\",\n                  targetEntry, trgtFileStatus);\n              missingDeletes++;\n            }\n          } catch (IOException e) {\n            if (!ignoreFailures) {\n              throw e;\n            } else {\n              // failed to delete, but ignoring errors. So continue\n              LOG.info(\"Failed to delete {}, ignoring exception {}\",\n                  targetEntry, e.toString());\n              LOG.debug(\"Failed to delete {}\", targetEntry, e);\n              // count and break out the loop\n              failedDeletes++;\n            }\n          }\n        } else {\n          LOG.debug(\"Skipping deletion of {}\", targetEntry);\n          skippedDeletes++;\n          showProgress = false;\n        }\n        if (showProgress) {\n          // update progress if there's been any FS IO/files deleted.\n          taskAttemptContext.progress();\n          taskAttemptContext.setStatus(\"Deleting removed files from target. [\" +\n              targetReader.getPosition() * 100 / totalLen + \"%]\");\n        }\n      }\n      // if the FS toString() call prints statistics, they get logged here\n      LOG.info(\"Completed deletion of files from {}\", targetFS);\n    } finally {\n      IOUtils.closeStream(sourceReader);\n      IOUtils.closeStream(targetReader);\n    }\n    long deletionEnd = System.currentTimeMillis();\n    long deletedFileCount = deletedEntries - deletedDirectories;\n    LOG.info(\"Deleted from target: files: {} directories: {};\"\n            + \" skipped deletions {}; deletions already missing {};\"\n            + \" failed deletes {}\",\n        deletedFileCount, deletedDirectories, skippedDeletes,\n        missingDeletes, failedDeletes);\n    LOG.info(\"Number of tracked deleted directories {}\", tracker.size());\n    LOG.info(\"Duration of deletions: {}\",\n        formatDuration(deletionEnd - deletionStart));\n    LOG.info(\"Total duration of deletion operation: {}\",\n        formatDuration(deletionEnd - listingStart));\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.mapred.CopyCommitter.commitData": "  private void commitData(Configuration conf) throws IOException {\n\n    Path workDir = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));\n    Path finalDir = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH));\n    FileSystem targetFS = workDir.getFileSystem(conf);\n\n    LOG.info(\"Atomic commit enabled. Moving \" + workDir + \" to \" + finalDir);\n    if (targetFS.exists(finalDir) && targetFS.exists(workDir)) {\n      LOG.error(\"Pre-existing final-path found at: \" + finalDir);\n      throw new IOException(\"Target-path can't be committed to because it \" +\n          \"exists at \" + finalDir + \". Copied data is in temp-dir: \" + workDir + \". \");\n    }\n\n    boolean result = targetFS.rename(workDir, finalDir);\n    if (!result) {\n      LOG.warn(\"Rename failed. Perhaps data already moved. Verifying...\");\n      result = targetFS.exists(finalDir) && !targetFS.exists(workDir);\n    }\n    if (result) {\n      LOG.info(\"Data committed successfully to \" + finalDir);\n      taskAttemptContext.setStatus(\"Data committed successfully to \" + finalDir);\n    } else {\n      LOG.error(\"Unable to commit data to \" + finalDir);\n      throw new IOException(\"Atomic commit failed. Temporary data in \" + workDir +\n        \", Unable to move to \" + finalDir);\n    }\n  }"
        },
        "bug_report": {
            "Title": "CopyCommitter#concatFileChunks should check that the blocks per chunk is not 0",
            "Description": "I was investigating test failure of TestIncrementalBackupWithBulkLoad from hbase against hadoop 3.1.1\r\n\r\nhbase MapReduceBackupCopyJob$BackupDistCp would create listing file:\r\n{code}\r\n        LOG.debug(\"creating input listing \" + listing + \" , totalRecords=\" + totalRecords);\r\n        cfg.set(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH, listing);\r\n        cfg.setLong(DistCpConstants.CONF_LABEL_TOTAL_NUMBER_OF_RECORDS, totalRecords);\r\n{code}\r\nFor the test case, two bulk loaded hfiles are in the listing:\r\n{code}\r\n2018-10-13 14:09:24,123 DEBUG [Time-limited test] mapreduce.MapReduceBackupCopyJob$BackupDistCp(195): BackupDistCp : hdfs://localhost:42796/user/hbase/test-data/160aeab5-6bca-9f87-465e-2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/394e6d39a9b94b148b9089c4fb967aad_SeqId_205_\r\n2018-10-13 14:09:24,125 DEBUG [Time-limited test] mapreduce.MapReduceBackupCopyJob$BackupDistCp(195): BackupDistCp : hdfs://localhost:42796/user/hbase/test-data/160aeab5-6bca-9f87-465e-2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/a7599081e835440eb7bf0dd3ef4fd7a5_SeqId_205_\r\n2018-10-13 14:09:24,125 DEBUG [Time-limited test] mapreduce.MapReduceBackupCopyJob$BackupDistCp(197): BackupDistCp execute for 2 files of 10242\r\n{code}\r\nLater on, CopyCommitter#concatFileChunks would throw the following exception:\r\n{code}\r\n2018-10-13 14:09:25,351 WARN  [Thread-936] mapred.LocalJobRunner$Job(590): job_local1795473782_0004\r\njava.io.IOException: Inconsistent sequence file: current chunk file org.apache.hadoop.tools.CopyListingFileStatus@bb8826ee{hdfs://localhost:42796/user/hbase/test-data/       160aeab5-6bca-9f87-465e-2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/a7599081e835440eb7bf0dd3ef4fd7a5_SeqId_205_ length = 5100 aclEntries  = null, xAttrs = null} doesnt match prior entry org.apache.hadoop.tools.CopyListingFileStatus@243d544d{hdfs://localhost:42796/user/hbase/test-data/160aeab5-6bca-9f87-465e-   2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/394e6d39a9b94b148b9089c4fb967aad_SeqId_205_ length = 5142 aclEntries = null, xAttrs = null}\r\n  at org.apache.hadoop.tools.mapred.CopyCommitter.concatFileChunks(CopyCommitter.java:276)\r\n  at org.apache.hadoop.tools.mapred.CopyCommitter.commitJob(CopyCommitter.java:100)\r\n  at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:567)\r\n{code}\r\nThe above warning shouldn't happen - the two bulk loaded hfiles are independent.\r\n\r\nFrom the contents of the two CopyListingFileStatus instances, we can see that their isSplit() return false. Otherwise the following from toString should be logged:\r\n{code}\r\n    if (isSplit()) {\r\n      sb.append(\", chunkOffset = \").append(this.getChunkOffset());\r\n      sb.append(\", chunkLength = \").append(this.getChunkLength());\r\n    }\r\n{code}\r\nFrom hbase side, we can specify one bulk loaded hfile per job but that defeats the purpose of using DistCp.\r\n\r\n"
        }
    },
    {
        "filename": "HADOOP-11693.json",
        "creation_time": "2015-03-05T23:19:13.000+0000",
        "stack_trace": "```\norg.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2367)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1960)\n\tat org.apache.hadoop.hbase.util.FSUtils.renameAndSetModifyTime(FSUtils.java:1719)\n\tat org.apache.hadoop.hbase.regionserver.wal.FSHLog.archiveLogFile(FSHLog.java:798)\n\tat org.apache.hadoop.hbase.regionserver.wal.FSHLog.cleanOldLogs(FSHLog.java:656)\n\tat org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:593)\n\tat org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:97)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.\n\tat com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)\n\tat com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)\n\tat com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)\n\tat com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)\n\tat org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)\n\t... 8 more\n\njava.io.IOException: failed log splitting for workernode13.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845307901, will retry\n\tat org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:71)\n\tat org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem$FolderRenamePending.execute(NativeAzureFileSystem.java:393)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1973)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.getLogDirs(MasterFileSystem.java:319)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:406)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(MasterFileSystem.java:302)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(MasterFileSystem.java:293)\n\tat org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:64)\n\t... 4 more\nCaused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.\n\tat com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)\n\tat com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)\n\tat com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)\n\tat com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)\n\tat org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)\n\t... 11 more\n\norg.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region hbase:meta,,1 is not online on workernode13.hbaseproddb4001.f5.internal.cloudapp.net,60020,1425235081338\n\tat org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2676)\n\tat org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:4095)\n\tat org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3076)\n\tat org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:28861)\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2008)\n\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:92)\n\tat org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:160)\n\tat org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:38)\n\tat org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:110)\n\tat java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.rename": "  public void rename(String srcKey, String dstKey, boolean acquireLease,\n      SelfRenewingLease existingLease) throws IOException {\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Moving \" + srcKey + \" to \" + dstKey);\n    }\n\n    if (acquireLease && existingLease != null) {\n      throw new IOException(\"Cannot acquire new lease if one already exists.\");\n    }\n\n    try {\n      // Attempts rename may occur before opening any streams so first,\n      // check if a session exists, if not create a session with the Azure\n      // storage server.\n      if (null == storageInteractionLayer) {\n        final String errMsg = String.format(\n            \"Storage session expected for URI '%s' but does not exist.\",\n            sessionUri);\n        throw new AssertionError(errMsg);\n      }\n\n      checkContainer(ContainerAccessType.ReadThenWrite);\n      // Get the source blob and assert its existence. If the source key\n      // needs to be normalized then normalize it.\n      //\n      CloudBlobWrapper srcBlob = getBlobReference(srcKey);\n\n      if (!srcBlob.exists(getInstrumentedContext())) {\n        throw new AzureException (\"Source blob \" + srcKey +\n            \" does not exist.\");\n      }\n\n      /**\n       * Conditionally get a lease on the source blob to prevent other writers\n       * from changing it. This is used for correctness in HBase when log files\n       * are renamed. It generally should do no harm other than take a little\n       * more time for other rename scenarios. When the HBase master renames a\n       * log file folder, the lease locks out other writers.  This\n       * prevents a region server that the master thinks is dead, but is still\n       * alive, from committing additional updates.  This is different than\n       * when HBase runs on HDFS, where the region server recovers the lease\n       * on a log file, to gain exclusive access to it, before it splits it.\n       */\n      SelfRenewingLease lease = null;\n      if (acquireLease) {\n        lease = srcBlob.acquireLease();\n      } else if (existingLease != null) {\n        lease = existingLease;\n      }\n\n      // Get the destination blob. The destination key always needs to be\n      // normalized.\n      //\n      CloudBlobWrapper dstBlob = getBlobReference(dstKey);\n\n      // TODO: Remove at the time when we move to Azure Java SDK 1.2+.\n      // This is the workaround provided by Azure Java SDK team to\n      // mitigate the issue with un-encoded x-ms-copy-source HTTP\n      // request header. Azure sdk version before 1.2+ does not encode this\n      // header what causes all URIs that have special (category \"other\")\n      // characters in the URI not to work with startCopyFromBlob when\n      // specified as source (requests fail with HTTP 403).\n      URI srcUri = new URI(srcBlob.getUri().toASCIIString());\n\n      // Rename the source blob to the destination blob by copying it to\n      // the destination blob then deleting it.\n      //\n      dstBlob.startCopyFromBlob(srcUri, getInstrumentedContext());\n      waitForCopyToComplete(dstBlob, getInstrumentedContext());\n\n      safeDelete(srcBlob, lease);\n    } catch (Exception e) {\n      // Re-throw exception as an Azure storage exception.\n      throw new AzureException(e);\n    }\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.safeDelete": "  private void safeDelete(CloudBlobWrapper blob, SelfRenewingLease lease) throws StorageException {\n    OperationContext operationContext = getInstrumentedContext();\n    try {\n      blob.delete(operationContext, lease);\n    } catch (StorageException e) {\n      // On exception, check that if:\n      // 1. It's a BlobNotFound exception AND\n      // 2. It got there after one-or-more retries THEN\n      // we swallow the exception.\n      if (e.getErrorCode() != null &&\n          e.getErrorCode().equals(\"BlobNotFound\") &&\n          operationContext.getRequestResults().size() > 1 &&\n          operationContext.getRequestResults().get(0).getException() != null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Swallowing delete exception on retry: \" + e.getMessage());\n        }\n        return;\n      } else {\n        throw e;\n      }\n    } finally {\n      if (lease != null) {\n        lease.free();\n      }\n    }\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.getBlobReference": "  private CloudBlobWrapper getBlobReference(String aKey)\n      throws StorageException, URISyntaxException {\n\n    CloudBlobWrapper blob = null;\n    if (isPageBlobKey(aKey)) {\n      blob = this.container.getPageBlobReference(aKey);\n    } else {\n      blob = this.container.getBlockBlobReference(aKey);\n    blob.setStreamMinimumReadSizeInBytes(downloadBlockSizeBytes);\n    blob.setWriteBlockSizeInBytes(uploadBlockSizeBytes);\n    }\n\n    return blob;\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.getInstrumentedContext": "  private OperationContext getInstrumentedContext(boolean bindConcurrentOOBIo) {\n\n    OperationContext operationContext = new OperationContext();\n\n    if (selfThrottlingEnabled) {\n      SelfThrottlingIntercept.hook(operationContext, selfThrottlingReadFactor,\n          selfThrottlingWriteFactor);\n    }\n\n    if(bandwidthGaugeUpdater != null) {\n      //bandwidthGaugeUpdater is null when we config to skip azure metrics\n      ResponseReceivedMetricUpdater.hook(\n         operationContext,\n         instrumentation,\n         bandwidthGaugeUpdater);\n    }\n\n    // Bind operation context to receive send request callbacks on this operation.\n    // If reads concurrent to OOB writes are allowed, the interception will reset\n    // the conditional header on all Azure blob storage read requests.\n    if (bindConcurrentOOBIo) {\n      SendRequestIntercept.bind(storageInteractionLayer.getCredentials(),\n          operationContext, true);\n    }\n\n    if (testHookOperationContext != null) {\n      operationContext =\n          testHookOperationContext.modifyOperationContext(operationContext);\n    }\n\n    ErrorMetricUpdater.hook(operationContext, instrumentation);\n\n    // Return the operation context.\n    return operationContext;\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.acquireLease": "  public SelfRenewingLease acquireLease(String key) throws AzureException {\n    LOG.debug(\"acquiring lease on \" + key);\n    try {\n      checkContainer(ContainerAccessType.ReadThenWrite);\n      CloudBlobWrapper blob = getBlobReference(key);\n      return blob.acquireLease();\n    }\n    catch (Exception e) {\n\n      // Caught exception while attempting to get lease. Re-throw as an\n      // Azure storage exception.\n      throw new AzureException(e);\n    }\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.waitForCopyToComplete": "  private void waitForCopyToComplete(CloudBlobWrapper blob, OperationContext opContext){\n    boolean copyInProgress = true;\n    while (copyInProgress) {\n      try {\n        blob.downloadAttributes(opContext);\n        }\n      catch (StorageException se){\n      }\n\n      // test for null because mocked filesystem doesn't know about copystates yet.\n      copyInProgress = (blob.getCopyState() != null && blob.getCopyState().getStatus() == CopyStatus.PENDING);\n      if (copyInProgress) {\n        try {\n          Thread.sleep(1000);\n          }\n          catch (InterruptedException ie){\n            //ignore\n        }\n      }\n    }\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.checkContainer": "  private ContainerState checkContainer(ContainerAccessType accessType)\n      throws StorageException, AzureException {\n    synchronized (containerStateLock) {\n      if (isOkContainerState(accessType)) {\n        return currentKnownContainerState;\n      }\n      if (currentKnownContainerState == ContainerState.ExistsAtWrongVersion) {\n        String containerVersion = retrieveVersionAttribute(container);\n        throw wrongVersionException(containerVersion);\n      }\n      // This means I didn't check it before or it didn't exist or\n      // we need to stamp the version. Since things may have changed by\n      // other machines since then, do the check again and don't depend\n      // on past information.\n\n      // Sanity check: we don't expect this at this point.\n      if (currentKnownContainerState == ContainerState.ExistsAtRightVersion) {\n        throw new AssertionError(\"Unexpected state: \"\n            + currentKnownContainerState);\n      }\n\n      // Download the attributes - doubles as an existence check with just\n      // one service call\n      try {\n        container.downloadAttributes(getInstrumentedContext());\n        currentKnownContainerState = ContainerState.Unknown;\n      } catch (StorageException ex) {\n        if (ex.getErrorCode().equals(\n            StorageErrorCode.RESOURCE_NOT_FOUND.toString())) {\n          currentKnownContainerState = ContainerState.DoesntExist;\n        } else {\n          throw ex;\n        }\n      }\n\n      if (currentKnownContainerState == ContainerState.DoesntExist) {\n        // If the container doesn't exist and we intend to write to it,\n        // create it now.\n        if (needToCreateContainer(accessType)) {\n          storeVersionAttribute(container);\n          container.create(getInstrumentedContext());\n          currentKnownContainerState = ContainerState.ExistsAtRightVersion;\n        }\n      } else {\n        // The container exists, check the version.\n        String containerVersion = retrieveVersionAttribute(container);\n        if (containerVersion != null) {\n          if (containerVersion.equals(FIRST_WASB_VERSION)) {\n            // It's the version from when WASB was called ASV, just\n            // fix the version attribute if needed and proceed.\n            // We should be good otherwise.\n            if (needToStampVersion(accessType)) {\n              storeVersionAttribute(container);\n              container.uploadMetadata(getInstrumentedContext());\n            }\n          } else if (!containerVersion.equals(CURRENT_WASB_VERSION)) {\n            // Don't know this version - throw.\n            currentKnownContainerState = ContainerState.ExistsAtWrongVersion;\n            throw wrongVersionException(containerVersion);\n          } else {\n            // It's our correct version.\n            currentKnownContainerState = ContainerState.ExistsAtRightVersion;\n          }\n        } else {\n          // No version info exists.\n          currentKnownContainerState = ContainerState.ExistsNoVersion;\n          if (needToStampVersion(accessType)) {\n            // Need to stamp the version\n            storeVersionAttribute(container);\n            container.uploadMetadata(getInstrumentedContext());\n            currentKnownContainerState = ContainerState.ExistsAtRightVersion;\n          }\n        }\n      }\n      return currentKnownContainerState;\n    }\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.rename": "  public boolean rename(Path src, Path dst) throws IOException {\n\n    FolderRenamePending renamePending = null;\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Moving \" + src + \" to \" + dst);\n    }\n\n    if (containsColon(dst)) {\n      throw new IOException(\"Cannot rename to file \" + dst\n          + \" through WASB that has colons in the name\");\n    }\n\n    String srcKey = pathToKey(makeAbsolute(src));\n\n    if (srcKey.length() == 0) {\n      // Cannot rename root of file system\n      return false;\n    }\n\n    // Figure out the final destination\n    Path absoluteDst = makeAbsolute(dst);\n    String dstKey = pathToKey(absoluteDst);\n    FileMetadata dstMetadata = store.retrieveMetadata(dstKey);\n    if (dstMetadata != null && dstMetadata.isDir()) {\n      // It's an existing directory.\n      dstKey = pathToKey(makeAbsolute(new Path(dst, src.getName())));\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Destination \" + dst\n            + \" is a directory, adjusted the destination to be \" + dstKey);\n      }\n    } else if (dstMetadata != null) {\n      // Attempting to overwrite a file using rename()\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Destination \" + dst\n            + \" is an already existing file, failing the rename.\");\n      }\n      return false;\n    } else {\n      // Check that the parent directory exists.\n      FileMetadata parentOfDestMetadata =\n          store.retrieveMetadata(pathToKey(absoluteDst.getParent()));\n      if (parentOfDestMetadata == null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Parent of the destination \" + dst\n              + \" doesn't exist, failing the rename.\");\n        }\n        return false;\n      } else if (!parentOfDestMetadata.isDir()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Parent of the destination \" + dst\n              + \" is a file, failing the rename.\");\n        }\n        return false;\n      }\n    }\n    FileMetadata srcMetadata = store.retrieveMetadata(srcKey);\n    if (srcMetadata == null) {\n      // Source doesn't exist\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Source \" + src + \" doesn't exist, failing the rename.\");\n      }\n      return false;\n    } else if (!srcMetadata.isDir()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Source \" + src + \" found as a file, renaming.\");\n      }\n      store.rename(srcKey, dstKey);\n    } else {\n\n      // Prepare for, execute and clean up after of all files in folder, and\n      // the root file, and update the last modified time of the source and\n      // target parent folders. The operation can be redone if it fails part\n      // way through, by applying the \"Rename Pending\" file.\n\n      // The following code (internally) only does atomic rename preparation\n      // and lease management for page blob folders, limiting the scope of the\n      // operation to HBase log file folders, where atomic rename is required.\n      // In the future, we could generalize it easily to all folders.\n      renamePending = prepareAtomicFolderRename(srcKey, dstKey);\n      renamePending.execute();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Renamed \" + src + \" to \" + dst + \" successfully.\");\n      }\n      renamePending.cleanup();\n      return true;\n    }\n\n    // Update the last-modified time of the parent folders of both source\n    // and destination.\n    updateParentFolderLastModifiedTime(srcKey);\n    updateParentFolderLastModifiedTime(dstKey);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Renamed \" + src + \" to \" + dst + \" successfully.\");\n    }\n    return true;\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.prepareAtomicFolderRename": "  private FolderRenamePending prepareAtomicFolderRename(\n      String srcKey, String dstKey) throws IOException {\n\n    if (store.isAtomicRenameKey(srcKey)) {\n\n      // Block unwanted concurrent access to source folder.\n      SelfRenewingLease lease = leaseSourceFolder(srcKey);\n\n      // Prepare in-memory information needed to do or redo a folder rename.\n      FolderRenamePending renamePending =\n          new FolderRenamePending(srcKey, dstKey, lease, this);\n\n      // Save it to persistent storage to help recover if the operation fails.\n      renamePending.writeFile(this);\n      return renamePending;\n    } else {\n      FolderRenamePending renamePending =\n          new FolderRenamePending(srcKey, dstKey, null, this);\n      return renamePending;\n    }\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.makeAbsolute": "  public Path makeAbsolute(Path path) {\n    if (path.isAbsolute()) {\n      return path;\n    }\n    return new Path(workingDir, path);\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.execute": "    public void execute() throws IOException {\n\n      for (FileMetadata file : this.getFiles()) {\n\n        // Rename all materialized entries under the folder to point to the\n        // final destination.\n        if (file.getBlobMaterialization() == BlobMaterialization.Explicit) {\n          String srcName = file.getKey();\n          String suffix  = srcName.substring((this.getSrcKey()).length());\n          String dstName = this.getDstKey() + suffix;\n\n          // Rename gets exclusive access (via a lease) for files\n          // designated for atomic rename.\n          // The main use case is for HBase write-ahead log (WAL) and data\n          // folder processing correctness.  See the rename code for details.\n          boolean acquireLease = fs.getStoreInterface().isAtomicRenameKey(srcName);\n          fs.getStoreInterface().rename(srcName, dstName, acquireLease, null);\n        }\n      }\n\n      // Rename the source folder 0-byte root file itself.\n      FileMetadata srcMetadata2 = this.getSourceMetadata();\n      if (srcMetadata2.getBlobMaterialization() ==\n          BlobMaterialization.Explicit) {\n\n        // It already has a lease on it from the \"prepare\" phase so there's no\n        // need to get one now. Pass in existing lease to allow file delete.\n        fs.getStoreInterface().rename(this.getSrcKey(), this.getDstKey(),\n            false, folderLease);\n      }\n\n      // Update the last-modified time of the parent folders of both source and\n      // destination.\n      fs.updateParentFolderLastModifiedTime(srcKey);\n      fs.updateParentFolderLastModifiedTime(dstKey);\n    }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.pathToKey": "  public String pathToKey(Path path) {\n    // Convert the path to a URI to parse the scheme, the authority, and the\n    // path from the path object.\n    URI tmpUri = path.toUri();\n    String pathUri = tmpUri.getPath();\n\n    // The scheme and authority is valid. If the path does not exist add a \"/\"\n    // separator to list the root of the container.\n    Path newPath = path;\n    if (\"\".equals(pathUri)) {\n      newPath = new Path(tmpUri.toString() + Path.SEPARATOR);\n    }\n\n    // Verify path is absolute if the path refers to a windows drive scheme.\n    if (!newPath.isAbsolute()) {\n      throw new IllegalArgumentException(\"Path must be absolute: \" + path);\n    }\n\n    String key = null;\n    key = newPath.toUri().getPath();\n    key = removeTrailingSlash(key);\n    key = encodeTrailingPeriod(key);\n    if (key.length() == 1) {\n      return key;\n    } else {\n      return key.substring(1); // remove initial slash\n    }\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.updateParentFolderLastModifiedTime": "  private void updateParentFolderLastModifiedTime(String key)\n      throws IOException {\n    Path parent = makeAbsolute(keyToPath(key)).getParent();\n    if (parent != null && parent.getParent() != null) { // not root\n      String parentKey = pathToKey(parent);\n\n      // ensure the parent is a materialized folder\n      FileMetadata parentMetadata = store.retrieveMetadata(parentKey);\n      // The metadata could be null if the implicit folder only contains a\n      // single file. In this case, the parent folder no longer exists if the\n      // file is renamed; so we can safely ignore the null pointer case.\n      if (parentMetadata != null) {\n        if (parentMetadata.isDir()\n            && parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {\n          store.storeEmptyFolder(parentKey,\n              createPermissionStatus(FsPermission.getDefault()));\n        }\n\n        if (store.isAtomicRenameKey(parentKey)) {\n          SelfRenewingLease lease = null;\n          try {\n            lease = leaseSourceFolder(parentKey);\n            store.updateFolderLastModifiedTime(parentKey, lease);\n          } catch (AzureException e) {\n            String errorCode = \"\";\n            try {\n              StorageException e2 = (StorageException) e.getCause();\n              errorCode = e2.getErrorCode();\n            } catch (Exception e3) {\n              // do nothing if cast fails\n            }\n            if (errorCode.equals(\"BlobNotFound\")) {\n              throw new FileNotFoundException(\"Folder does not exist: \" + parentKey);\n            }\n            LOG.warn(\"Got unexpected exception trying to get lease on \"\n                + parentKey + \". \" + e.getMessage());\n            throw e;\n          } finally {\n            try {\n              if (lease != null) {\n                lease.free();\n              }\n            } catch (Exception e) {\n              LOG.error(\"Unable to free lease on \" + parentKey, e);\n            }\n          }\n        } else {\n          store.updateFolderLastModifiedTime(parentKey, null);\n        }\n      }\n    }\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.cleanup": "    public void cleanup() throws IOException {\n\n      if (fs.getStoreInterface().isAtomicRenameKey(srcKey)) {\n\n        // Remove RenamePending file\n        fs.delete(getRenamePendingFilePath(), false);\n\n        // Freeing source folder lease is not necessary since the source\n        // folder file was deleted.\n      }\n    }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.containsColon": "  private boolean containsColon(Path p) {\n    return p.toUri().getPath().toString().contains(\":\");\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.StorageInterfaceImpl.startCopyFromBlob": "    public void startCopyFromBlob(URI source,\n        OperationContext opContext)\n            throws StorageException, URISyntaxException {\n      getBlob().startCopyFromBlob(source,\n          null, null, null, opContext);\n    }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.StorageInterfaceImpl.getBlob": "    public CloudBlob getBlob() {\n      return blob;\n    }"
        },
        "bug_report": {
            "Title": "Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving.",
            "Description": "One of our customers' production HBase clusters was periodically throttled by Azure storage, when HBase was archiving old WALs. HMaster aborted the region server and tried to restart it.\n\nHowever, since the cluster was still being throttled by Azure storage, the upcoming distributed log splitting also failed. Sometimes hbase:meta table was on this region server and finally showed offline, which cause the whole cluster in bad state.\n\n{code}\n2015-03-01 18:36:45,623 ERROR org.apache.hadoop.hbase.master.HMaster: Region server workernode4.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845421044 reported a fatal error:\nABORTING region server workernode4.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845421044: IOE in log roller\nCause:\norg.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2367)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1960)\n\tat org.apache.hadoop.hbase.util.FSUtils.renameAndSetModifyTime(FSUtils.java:1719)\n\tat org.apache.hadoop.hbase.regionserver.wal.FSHLog.archiveLogFile(FSHLog.java:798)\n\tat org.apache.hadoop.hbase.regionserver.wal.FSHLog.cleanOldLogs(FSHLog.java:656)\n\tat org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:593)\n\tat org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:97)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.\n\tat com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)\n\tat com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)\n\tat com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)\n\tat com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)\n\tat org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)\n\t... 8 more\n\n2015-03-01 18:43:29,072 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_META_SERVER_SHUTDOWN\njava.io.IOException: failed log splitting for workernode13.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845307901, will retry\n\tat org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:71)\n\tat org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem$FolderRenamePending.execute(NativeAzureFileSystem.java:393)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1973)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.getLogDirs(MasterFileSystem.java:319)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:406)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(MasterFileSystem.java:302)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(MasterFileSystem.java:293)\n\tat org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:64)\n\t... 4 more\nCaused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.\n\tat com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)\n\tat com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)\n\tat com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)\n\tat com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)\n\tat org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)\n\t... 11 more\n\nSun Mar 01 18:59:51 GMT 2015, org.apache.hadoop.hbase.client.RpcRetryingCaller@aa93ac7, org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region hbase:meta,,1 is not online on workernode13.hbaseproddb4001.f5.internal.cloudapp.net,60020,1425235081338\n\tat org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2676)\n\tat org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:4095)\n\tat org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3076)\n\tat org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:28861)\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2008)\n\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:92)\n\tat org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:160)\n\tat org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:38)\n\tat org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:110)\n\tat java.lang.Thread.run(Thread.java:745)\n{code}\n\nWhen archiving old WALs, WASB will do rename operation by copying src blob to destination blob and deleting the src blob. Copy blob is very costly in Azure storage and during Azure storage gc, it will be highly likely throttled. The throttling by Azure storage usually ends within 15mins. Current WASB retry policy is exponential retry, but only last at most for 2min. Short term fix will be adding a more intensive exponential retry when copy blob is throttled."
        }
    },
    {
        "filename": "HADOOP-12441.json",
        "creation_time": "2015-09-25T23:26:43.000+0000",
        "stack_trace": "```\nExitCodeException exitCode=1: ERROR: garbage process ID \"--\".\nUsage:\n  kill pid ...              Send SIGTERM to every process listed.\n  kill signal pid ...       Send a signal to every process listed.\n  kill -s signal pid ...    Send a signal to every process listed.\n  kill -l                   List all signal names.\n  kill -L                   List all signal names in a nice table.\n  kill -l signal            Convert between signal numbers and names.\n\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:550)\n        at org.apache.hadoop.util.Shell.run(Shell.java:461)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:727)\n        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.containerIsAlive(DefaultContainerExecutor.java:432)\n        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.signalContainer(DefaultContainerExecutor.java:401)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:419)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:139)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:55)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108)\n        at java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.runCommand": "  private void runCommand() throws IOException { \n    ProcessBuilder builder = new ProcessBuilder(getExecString());\n    Timer timeOutTimer = null;\n    ShellTimeoutTimerTask timeoutTimerTask = null;\n    timedOut = new AtomicBoolean(false);\n    completed = new AtomicBoolean(false);\n    \n    if (environment != null) {\n      builder.environment().putAll(this.environment);\n    }\n    if (dir != null) {\n      builder.directory(this.dir);\n    }\n\n    builder.redirectErrorStream(redirectErrorStream);\n    \n    if (Shell.WINDOWS) {\n      synchronized (WindowsProcessLaunchLock) {\n        // To workaround the race condition issue with child processes\n        // inheriting unintended handles during process launch that can\n        // lead to hangs on reading output and error streams, we\n        // serialize process creation. More info available at:\n        // http://support.microsoft.com/kb/315939\n        process = builder.start();\n      }\n    } else {\n      process = builder.start();\n    }\n\n    if (timeOutInterval > 0) {\n      timeOutTimer = new Timer(\"Shell command timeout\");\n      timeoutTimerTask = new ShellTimeoutTimerTask(\n          this);\n      //One time scheduling.\n      timeOutTimer.schedule(timeoutTimerTask, timeOutInterval);\n    }\n    final BufferedReader errReader = \n            new BufferedReader(new InputStreamReader(\n                process.getErrorStream(), Charset.defaultCharset()));\n    BufferedReader inReader = \n            new BufferedReader(new InputStreamReader(\n                process.getInputStream(), Charset.defaultCharset()));\n    final StringBuffer errMsg = new StringBuffer();\n    \n    // read error and input streams as this would free up the buffers\n    // free the error stream buffer\n    Thread errThread = new Thread() {\n      @Override\n      public void run() {\n        try {\n          String line = errReader.readLine();\n          while((line != null) && !isInterrupted()) {\n            errMsg.append(line);\n            errMsg.append(System.getProperty(\"line.separator\"));\n            line = errReader.readLine();\n          }\n        } catch(IOException ioe) {\n          LOG.warn(\"Error reading the error stream\", ioe);\n        }\n      }\n    };\n    try {\n      errThread.start();\n    } catch (IllegalStateException ise) {\n    } catch (OutOfMemoryError oe) {\n      LOG.error(\"Caught \" + oe + \". One possible reason is that ulimit\"\n          + \" setting of 'max user processes' is too low. If so, do\"\n          + \" 'ulimit -u <largerNum>' and try again.\");\n      throw oe;\n    }\n    try {\n      parseExecResult(inReader); // parse the output\n      // clear the input stream buffer\n      String line = inReader.readLine();\n      while(line != null) { \n        line = inReader.readLine();\n      }\n      // wait for the process to finish and check the exit code\n      exitCode  = process.waitFor();\n      // make sure that the error thread exits\n      joinThread(errThread);\n      completed.set(true);\n      //the timeout thread handling\n      //taken care in finally block\n      if (exitCode != 0) {\n        throw new ExitCodeException(exitCode, errMsg.toString());\n      }\n    } catch (InterruptedException ie) {\n      InterruptedIOException iie = new InterruptedIOException(ie.toString());\n      iie.initCause(ie);\n      throw iie;\n    } finally {\n      if (timeOutTimer != null) {\n        timeOutTimer.cancel();\n      }\n      // close the input stream\n      try {\n        // JDK 7 tries to automatically drain the input streams for us\n        // when the process exits, but since close is not synchronized,\n        // it creates a race if we close the stream first and the same\n        // fd is recycled.  the stream draining thread will attempt to\n        // drain that fd!!  it may block, OOM, or cause bizarre behavior\n        // see: https://bugs.openjdk.java.net/browse/JDK-8024521\n        //      issue is fixed in build 7u60\n        InputStream stdout = process.getInputStream();\n        synchronized (stdout) {\n          inReader.close();\n        }\n      } catch (IOException ioe) {\n        LOG.warn(\"Error while closing the input stream\", ioe);\n      }\n      if (!completed.get()) {\n        errThread.interrupt();\n        joinThread(errThread);\n      }\n      try {\n        InputStream stderr = process.getErrorStream();\n        synchronized (stderr) {\n          errReader.close();\n        }\n      } catch (IOException ioe) {\n        LOG.warn(\"Error while closing the error stream\", ioe);\n      }\n      process.destroy();\n      lastTime = Time.monotonicNow();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.toString": "    public String toString() {\n      StringBuilder builder = new StringBuilder();\n      String[] args = getExecString();\n      for (String s : args) {\n        if (s.indexOf(' ') >= 0) {\n          builder.append('\"').append(s).append('\"');\n        } else {\n          builder.append(s);\n        }\n        builder.append(' ');\n      }\n      return builder.toString();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.parseExecResult": "    protected void parseExecResult(BufferedReader lines) throws IOException {\n      output = new StringBuffer();\n      char[] buf = new char[512];\n      int nRead;\n      while ( (nRead = lines.read(buf, 0, buf.length)) > 0 ) {\n        output.append(buf, 0, nRead);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.joinThread": "  private static void joinThread(Thread t) {\n    while (t.isAlive()) {\n      try {\n        t.join();\n      } catch (InterruptedException ie) {\n        if (LOG.isWarnEnabled()) {\n          LOG.warn(\"Interrupted while joining on: \" + t, ie);\n        }\n        t.interrupt(); // propagate interrupt\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.getExecString": "    public String[] getExecString() {\n      return command;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.close": "    public void close() {\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.run": "    public void run() {\n      Process p = shell.getProcess();\n      try {\n        p.exitValue();\n      } catch (Exception e) {\n        //Process has not terminated.\n        //So check if it has completed \n        //if not just destroy it.\n        if (p != null && !shell.completed.get()) {\n          shell.setTimedOut();\n          p.destroy();\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.setTimedOut": "  private void setTimedOut() {\n    this.timedOut.set(true);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.getProcess": "  public Process getProcess() {\n    return process;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.execute": "    public void execute() throws IOException {\n      this.run();    \n    }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.containerIsAlive": "  public static boolean containerIsAlive(String pid) throws IOException {\n    try {\n      new ShellCommandExecutor(Shell.getCheckProcessIsAliveCommand(pid))\n        .execute();\n      // successful execution means process is alive\n      return true;\n    }\n    catch (ExitCodeException e) {\n      // failure (non-zero exit code) means process is not alive\n      return false;\n    }\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.signalContainer": "  public boolean signalContainer(ContainerSignalContext ctx)\n      throws IOException {\n    String user = ctx.getUser();\n    String pid = ctx.getPid();\n    Signal signal = ctx.getSignal();\n\n    LOG.debug(\"Sending signal \" + signal.getValue() + \" to pid \" + pid\n        + \" as user \" + user);\n    if (!containerIsAlive(pid)) {\n      return false;\n    }\n    try {\n      killContainer(pid, signal);\n    } catch (IOException e) {\n      if (!containerIsAlive(pid)) {\n        return false;\n      }\n      throw e;\n    }\n    return true;\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.killContainer": "  protected void killContainer(String pid, Signal signal) throws IOException {\n    new ShellCommandExecutor(Shell.getSignalKillCommand(signal.getValue(), pid))\n      .execute();\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer": "  public void cleanupContainer() throws IOException {\n    ContainerId containerId = container.getContainerId();\n    String containerIdStr = ConverterUtils.toString(containerId);\n    LOG.info(\"Cleaning up container \" + containerIdStr);\n\n    try {\n      context.getNMStateStore().storeContainerKilled(containerId);\n    } catch (IOException e) {\n      LOG.error(\"Unable to mark container \" + containerId\n          + \" killed in store\", e);\n    }\n\n    // launch flag will be set to true if process already launched\n    boolean alreadyLaunched = !shouldLaunchContainer.compareAndSet(false, true);\n    if (!alreadyLaunched) {\n      LOG.info(\"Container \" + containerIdStr + \" not launched.\"\n          + \" No cleanup needed to be done\");\n      return;\n    }\n\n    LOG.debug(\"Marking container \" + containerIdStr + \" as inactive\");\n    // this should ensure that if the container process has not launched \n    // by this time, it will never be launched\n    exec.deactivateContainer(containerId);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Getting pid for container \" + containerIdStr + \" to kill\"\n          + \" from pid file \" \n          + (pidFilePath != null ? pidFilePath.toString() : \"null\"));\n    }\n    \n    // however the container process may have already started\n    try {\n\n      // get process id from pid file if available\n      // else if shell is still active, get it from the shell\n      String processId = null;\n      if (pidFilePath != null) {\n        processId = getContainerPid(pidFilePath);\n      }\n\n      // kill process\n      if (processId != null) {\n        String user = container.getUser();\n        LOG.debug(\"Sending signal to pid \" + processId\n            + \" as user \" + user\n            + \" for container \" + containerIdStr);\n\n        final Signal signal = sleepDelayBeforeSigKill > 0\n          ? Signal.TERM\n          : Signal.KILL;\n\n        boolean result = exec.signalContainer(\n            new ContainerSignalContext.Builder()\n                .setContainer(container)\n                .setUser(user)\n                .setPid(processId)\n                .setSignal(signal)\n                .build());\n\n        LOG.debug(\"Sent signal \" + signal + \" to pid \" + processId\n          + \" as user \" + user\n          + \" for container \" + containerIdStr\n          + \", result=\" + (result? \"success\" : \"failed\"));\n\n        if (sleepDelayBeforeSigKill > 0) {\n          new DelayedProcessKiller(container, user,\n              processId, sleepDelayBeforeSigKill, Signal.KILL, exec).start();\n        }\n      }\n    } catch (Exception e) {\n      String message =\n          \"Exception when trying to cleanup container \" + containerIdStr\n              + \": \" + StringUtils.stringifyException(e);\n      LOG.warn(message);\n      dispatcher.getEventHandler().handle(\n        new ContainerDiagnosticsUpdateEvent(containerId, message));\n    } finally {\n      // cleanup pid file if present\n      if (pidFilePath != null) {\n        FileContext lfs = FileContext.getLocalFSFileContext();\n        lfs.delete(pidFilePath, false);\n        lfs.delete(pidFilePath.suffix(EXIT_CODE_FILE_SUFFIX), false);\n      }\n    }\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.toString": "    public String toString() {\n      return sb.toString();\n    }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.getContainerPid": "  private String getContainerPid(Path pidFilePath) throws Exception {\n    String containerIdStr = \n        ConverterUtils.toString(container.getContainerId());\n    String processId = null;\n    LOG.debug(\"Accessing pid for container \" + containerIdStr\n        + \" from pid file \" + pidFilePath);\n    int sleepCounter = 0;\n    final int sleepInterval = 100;\n\n    // loop waiting for pid file to show up \n    // until our timer expires in which case we admit defeat\n    while (true) {\n      processId = ProcessIdFileReader.getProcessId(pidFilePath);\n      if (processId != null) {\n        LOG.debug(\"Got pid \" + processId + \" for container \"\n            + containerIdStr);\n        break;\n      }\n      else if ((sleepCounter*sleepInterval) > maxKillWaitTime) {\n        LOG.info(\"Could not get pid for \" + containerIdStr\n        \t\t+ \". Waited for \" + maxKillWaitTime + \" ms.\");\n        break;\n      }\n      else {\n        ++sleepCounter;\n        Thread.sleep(sleepInterval);\n      }\n    }\n    return processId;\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle": "  public void handle(ContainersLauncherEvent event) {\n    // TODO: ContainersLauncher launches containers one by one!!\n    Container container = event.getContainer();\n    ContainerId containerId = container.getContainerId();\n    switch (event.getType()) {\n      case LAUNCH_CONTAINER:\n        Application app =\n          context.getApplications().get(\n              containerId.getApplicationAttemptId().getApplicationId());\n\n        ContainerLaunch launch =\n            new ContainerLaunch(context, getConfig(), dispatcher, exec, app,\n              event.getContainer(), dirsHandler, containerManager);\n        containerLauncher.submit(launch);\n        running.put(containerId, launch);\n        break;\n      case RECOVER_CONTAINER:\n        app = context.getApplications().get(\n            containerId.getApplicationAttemptId().getApplicationId());\n        launch = new RecoveredContainerLaunch(context, getConfig(), dispatcher,\n            exec, app, event.getContainer(), dirsHandler, containerManager);\n        containerLauncher.submit(launch);\n        running.put(containerId, launch);\n        break;\n      case CLEANUP_CONTAINER:\n        ContainerLaunch launcher = running.remove(containerId);\n        if (launcher == null) {\n          // Container not launched. So nothing needs to be done.\n          return;\n        }\n\n        // Cleanup a container whether it is running/killed/completed, so that\n        // no sub-processes are alive.\n        try {\n          launcher.cleanupContainer();\n        } catch (IOException e) {\n          LOG.warn(\"Got exception while cleaning container \" + containerId\n              + \". Ignoring.\");\n        }\n        break;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Time.monotonicNow": "  public static long monotonicNow() {\n    return System.nanoTime() / NANOSECONDS_PER_MILLISECOND;\n  }"
        },
        "bug_report": {
            "Title": "Fix kill command behavior under some Linux distributions.",
            "Description": "After HADOOP-12317, kill command's execution will be failure under Ubuntu12. After NM restarts, it cannot get if a process is alive or not via pid of containers, and it cannot kill process correctly when RM/AM tells NM to kill a container.\n\nLogs from NM (customized logs):\n{code}\n2015-09-25 21:58:59,348 INFO  nodemanager.DefaultContainerExecutor (DefaultContainerExecutor.java:containerIsAlive(431)) -  ================== check alive cmd:[[Ljava.lang.String;@496e442d]\n2015-09-25 21:58:59,349 INFO  nodemanager.NMAuditLogger (NMAuditLogger.java:logSuccess(89)) - USER=hrt_qa       IP=10.0.1.14    OPERATION=Stop Container Request        TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1443218269460_0001    CONTAINERID=container_1443218269460_0001_01_000001\n2015-09-25 21:58:59,363 INFO  nodemanager.DefaultContainerExecutor (DefaultContainerExecutor.java:containerIsAlive(438)) -  ===========================\nExitCodeException exitCode=1: ERROR: garbage process ID \"--\".\nUsage:\n  kill pid ...              Send SIGTERM to every process listed.\n  kill signal pid ...       Send a signal to every process listed.\n  kill -s signal pid ...    Send a signal to every process listed.\n  kill -l                   List all signal names.\n  kill -L                   List all signal names in a nice table.\n  kill -l signal            Convert between signal numbers and names.\n\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:550)\n        at org.apache.hadoop.util.Shell.run(Shell.java:461)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:727)\n        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.containerIsAlive(DefaultContainerExecutor.java:432)\n        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.signalContainer(DefaultContainerExecutor.java:401)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:419)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:139)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:55)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108)\n        at java.lang.Thread.run(Thread.java:745)\n{code}"
        }
    },
    {
        "filename": "HADOOP-11685.json",
        "creation_time": "2015-03-06T22:00:14.000+0000",
        "stack_trace": "```\njava.io.IOException: org.apache.hadoop.fs.azure.AzureException: java.io.IOException\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.checkForErrors(HLogSplitter.java:633)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.access$000(HLogSplitter.java:121)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.finishWriting(HLogSplitter.java:964)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.finishWritingAndClose(HLogSplitter.java:1019)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:359)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:223)\n\tat org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:142)\n\tat org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.process(HLogSplitterHandler.java:79)\n\tat org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.fs.azure.AzureException: java.io.IOException\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.storeEmptyFolder(AzureNativeFileSystemStore.java:1477)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.mkdirs(NativeAzureFileSystem.java:1862)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.mkdirs(NativeAzureFileSystem.java:1812)\n\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1815)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getRegionSplitEditsPath(HLogSplitter.java:502)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.createWAP(HLogSplitter.java:1211)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.getWriterAndPath(HLogSplitter.java:1200)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.append(HLogSplitter.java:1243)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.writeBuffer(HLogSplitter.java:851)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.doRun(HLogSplitter.java:843)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.run(HLogSplitter.java:813)\nCaused by: java.io.IOException\n\tat com.microsoft.windowsazure.storage.core.Utility.initIOException(Utility.java:493)\n\tat com.microsoft.windowsazure.storage.blob.BlobOutputStream.close(BlobOutputStream.java:282)\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.storeEmptyFolder(AzureNativeFileSystemStore.java:1472)\n\t... 10 more\nCaused by: com.microsoft.windowsazure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)\n\tat com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)\n\tat com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)\n\tat com.microsoft.windowsazure.storage.blob.CloudBlockBlob.commitBlockList(CloudBlockBlob.java:248)\n\tat com.microsoft.windowsazure.storage.blob.BlobOutputStream.commit(BlobOutputStream.java:319)\n\tat com.microsoft.windowsazure.storage.blob.BlobOutputStream.close(BlobOutputStream.java:279)```",
        "source_code": {
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.storeEmptyFolder": "  public void storeEmptyFolder(String key, PermissionStatus permissionStatus)\n      throws AzureException {\n\n    if (null == storageInteractionLayer) {\n      final String errMsg = String.format(\n          \"Storage session expected for URI '%s' but does not exist.\",\n          sessionUri);\n      throw new AssertionError(errMsg);\n    }\n\n    // Check if there is an authenticated account associated with the file\n    // this instance of the WASB file system. If not the file system has not\n    // been authenticated and all access is anonymous.\n    if (!isAuthenticatedAccess()) {\n      // Preemptively raise an exception indicating no uploads are\n      // allowed to anonymous accounts.\n      throw new AzureException(\n          \"Uploads to to public accounts using anonymous access is prohibited.\");\n    }\n\n    try {\n      checkContainer(ContainerAccessType.PureWrite);\n\n      CloudBlobWrapper blob = getBlobReference(key);\n      storePermissionStatus(blob, permissionStatus);\n      storeFolderAttribute(blob);\n      openOutputStream(blob).close();\n    } catch (Exception e) {\n      // Caught exception while attempting upload. Re-throw as an Azure\n      // storage exception.\n      throw new AzureException(e);\n    }\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.isAuthenticatedAccess": "  private boolean isAuthenticatedAccess() throws AzureException {\n\n    if (isAnonymousCredentials) {\n      // Access to this storage account is unauthenticated.\n      return false;\n    }\n    // Access is authenticated.\n    return true;\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.getBlobReference": "  private CloudBlobWrapper getBlobReference(String aKey)\n      throws StorageException, URISyntaxException {\n\n    CloudBlobWrapper blob = null;\n    if (isPageBlobKey(aKey)) {\n      blob = this.container.getPageBlobReference(aKey);\n    } else {\n      blob = this.container.getBlockBlobReference(aKey);\n    blob.setStreamMinimumReadSizeInBytes(downloadBlockSizeBytes);\n    blob.setWriteBlockSizeInBytes(uploadBlockSizeBytes);\n    }\n\n    return blob;\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.storeFolderAttribute": "  private static void storeFolderAttribute(CloudBlobWrapper blob) {\n    storeMetadataAttribute(blob, IS_FOLDER_METADATA_KEY, \"true\");\n    // Remove the old metadata key if present\n    removeMetadataAttribute(blob, OLD_IS_FOLDER_METADATA_KEY);\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.storePermissionStatus": "  private static void storePermissionStatus(CloudBlobWrapper blob,\n      PermissionStatus permissionStatus) {\n    storeMetadataAttribute(blob, PERMISSION_METADATA_KEY,\n        PERMISSION_JSON_SERIALIZER.toJSON(permissionStatus));\n    // Remove the old metadata key if present\n    removeMetadataAttribute(blob, OLD_PERMISSION_METADATA_KEY);\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.close": "  public void close() {\n    if(bandwidthGaugeUpdater != null) {\n      bandwidthGaugeUpdater.close();\n      bandwidthGaugeUpdater = null;\n    }\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.openOutputStream": "  private OutputStream openOutputStream(final CloudBlobWrapper blob)\n      throws StorageException {\n    if (blob instanceof CloudPageBlobWrapperImpl){\n      return new PageBlobOutputStream(\n          (CloudPageBlobWrapper)blob, getInstrumentedContext(), sessionConfiguration);\n    } else {\n\n      // Handle both ClouldBlockBlobWrapperImpl and (only for the test code path)\n      // MockCloudBlockBlobWrapper.\n      return ((CloudBlockBlobWrapper) blob).openOutputStream(getUploadOptions(),\n                getInstrumentedContext());\n    }\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.checkContainer": "  private ContainerState checkContainer(ContainerAccessType accessType)\n      throws StorageException, AzureException {\n    synchronized (containerStateLock) {\n      if (isOkContainerState(accessType)) {\n        return currentKnownContainerState;\n      }\n      if (currentKnownContainerState == ContainerState.ExistsAtWrongVersion) {\n        String containerVersion = retrieveVersionAttribute(container);\n        throw wrongVersionException(containerVersion);\n      }\n      // This means I didn't check it before or it didn't exist or\n      // we need to stamp the version. Since things may have changed by\n      // other machines since then, do the check again and don't depend\n      // on past information.\n\n      // Sanity check: we don't expect this at this point.\n      if (currentKnownContainerState == ContainerState.ExistsAtRightVersion) {\n        throw new AssertionError(\"Unexpected state: \"\n            + currentKnownContainerState);\n      }\n\n      // Download the attributes - doubles as an existence check with just\n      // one service call\n      try {\n        container.downloadAttributes(getInstrumentedContext());\n        currentKnownContainerState = ContainerState.Unknown;\n      } catch (StorageException ex) {\n        if (ex.getErrorCode().equals(\n            StorageErrorCode.RESOURCE_NOT_FOUND.toString())) {\n          currentKnownContainerState = ContainerState.DoesntExist;\n        } else {\n          throw ex;\n        }\n      }\n\n      if (currentKnownContainerState == ContainerState.DoesntExist) {\n        // If the container doesn't exist and we intend to write to it,\n        // create it now.\n        if (needToCreateContainer(accessType)) {\n          storeVersionAttribute(container);\n          container.create(getInstrumentedContext());\n          currentKnownContainerState = ContainerState.ExistsAtRightVersion;\n        }\n      } else {\n        // The container exists, check the version.\n        String containerVersion = retrieveVersionAttribute(container);\n        if (containerVersion != null) {\n          if (containerVersion.equals(FIRST_WASB_VERSION)) {\n            // It's the version from when WASB was called ASV, just\n            // fix the version attribute if needed and proceed.\n            // We should be good otherwise.\n            if (needToStampVersion(accessType)) {\n              storeVersionAttribute(container);\n              container.uploadMetadata(getInstrumentedContext());\n            }\n          } else if (!containerVersion.equals(CURRENT_WASB_VERSION)) {\n            // Don't know this version - throw.\n            currentKnownContainerState = ContainerState.ExistsAtWrongVersion;\n            throw wrongVersionException(containerVersion);\n          } else {\n            // It's our correct version.\n            currentKnownContainerState = ContainerState.ExistsAtRightVersion;\n          }\n        } else {\n          // No version info exists.\n          currentKnownContainerState = ContainerState.ExistsNoVersion;\n          if (needToStampVersion(accessType)) {\n            // Need to stamp the version\n            storeVersionAttribute(container);\n            container.uploadMetadata(getInstrumentedContext());\n            currentKnownContainerState = ContainerState.ExistsAtRightVersion;\n          }\n        }\n      }\n      return currentKnownContainerState;\n    }\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.mkdirs": "  public boolean mkdirs(Path f, FsPermission permission, boolean noUmask) throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Creating directory: \" + f.toString());\n    }\n\n    if (containsColon(f)) {\n      throw new IOException(\"Cannot create directory \" + f\n          + \" through WASB that has colons in the name\");\n    }\n\n    Path absolutePath = makeAbsolute(f);\n    PermissionStatus permissionStatus = null;\n    if(noUmask) {\n      // ensure owner still has wx permissions at the minimum\n      permissionStatus = createPermissionStatus(\n          applyUMask(FsPermission.createImmutable((short) (permission.toShort() | USER_WX_PERMISION)),\n              UMaskApplyMode.NewDirectoryNoUmask));\n    } else {\n      permissionStatus = createPermissionStatus(\n          applyUMask(permission, UMaskApplyMode.NewDirectory));\n    }\n\n\n    ArrayList<String> keysToCreateAsFolder = new ArrayList<String>();\n    ArrayList<String> keysToUpdateAsFolder = new ArrayList<String>();\n    boolean childCreated = false;\n    // Check that there is no file in the parent chain of the given path.\n    for (Path current = absolutePath, parent = current.getParent();\n        parent != null; // Stop when you get to the root\n        current = parent, parent = current.getParent()) {\n      String currentKey = pathToKey(current);\n      FileMetadata currentMetadata = store.retrieveMetadata(currentKey);\n      if (currentMetadata != null && !currentMetadata.isDir()) {\n        throw new IOException(\"Cannot create directory \" + f + \" because \" +\n            current + \" is an existing file.\");\n      } else if (currentMetadata == null) {\n        keysToCreateAsFolder.add(currentKey);\n        childCreated = true;\n      } else {\n        // The directory already exists. Its last modified time need to be\n        // updated if there is a child directory created under it.\n        if (childCreated) {\n          keysToUpdateAsFolder.add(currentKey);\n        }\n        childCreated = false;\n      }\n    }\n\n    for (String currentKey : keysToCreateAsFolder) {\n      store.storeEmptyFolder(currentKey, permissionStatus);\n    }\n\n    instrumentation.directoryCreated();\n\n    // otherwise throws exception\n    return true;\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.makeAbsolute": "  public Path makeAbsolute(Path path) {\n    if (path.isAbsolute()) {\n      return path;\n    }\n    return new Path(workingDir, path);\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.createPermissionStatus": "  private PermissionStatus createPermissionStatus(FsPermission permission)\n      throws IOException {\n    // Create the permission status for this file based on current user\n    return new PermissionStatus(\n        UserGroupInformation.getCurrentUser().getShortUserName(),\n        getConf().get(AZURE_DEFAULT_GROUP_PROPERTY_NAME,\n            AZURE_DEFAULT_GROUP_DEFAULT),\n        permission);\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.pathToKey": "  public String pathToKey(Path path) {\n    // Convert the path to a URI to parse the scheme, the authority, and the\n    // path from the path object.\n    URI tmpUri = path.toUri();\n    String pathUri = tmpUri.getPath();\n\n    // The scheme and authority is valid. If the path does not exist add a \"/\"\n    // separator to list the root of the container.\n    Path newPath = path;\n    if (\"\".equals(pathUri)) {\n      newPath = new Path(tmpUri.toString() + Path.SEPARATOR);\n    }\n\n    // Verify path is absolute if the path refers to a windows drive scheme.\n    if (!newPath.isAbsolute()) {\n      throw new IllegalArgumentException(\"Path must be absolute: \" + path);\n    }\n\n    String key = null;\n    key = newPath.toUri().getPath();\n    key = removeTrailingSlash(key);\n    key = encodeTrailingPeriod(key);\n    if (key.length() == 1) {\n      return key;\n    } else {\n      return key.substring(1); // remove initial slash\n    }\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.applyUMask": "  private FsPermission applyUMask(final FsPermission permission,\n      final UMaskApplyMode applyMode) {\n    FsPermission newPermission = new FsPermission(permission);\n    // Apply the default umask - this applies for new files or directories.\n    if (applyMode == UMaskApplyMode.NewFile\n        || applyMode == UMaskApplyMode.NewDirectory) {\n      newPermission = newPermission\n          .applyUMask(FsPermission.getUMask(getConf()));\n    }\n    return newPermission;\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.containsColon": "  private boolean containsColon(Path p) {\n    return p.toUri().getPath().toString().contains(\":\");\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.mkdirs": "  public abstract boolean mkdirs(Path f, FsPermission permission\n      ) throws IOException;\n\n  /**\n   * The src file is on the local disk.  Add it to FS at\n   * the given dst name and the source is kept intact afterwards\n   * @param src path\n   * @param dst path\n   */\n  public void copyFromLocalFile(Path src, Path dst)\n    throws IOException {\n    copyFromLocalFile(false, src, dst);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.setPermission": "  public void setPermission(Path p, FsPermission permission\n      ) throws IOException {\n  }"
        },
        "bug_report": {
            "Title": "StorageException complaining \" no lease ID\" during HBase distributed log splitting",
            "Description": "This is similar to HADOOP-11523, but in a different place. During HBase distributed log splitting, multiple threads will access the same folder called \"recovered.edits\". However, lots of places in our WASB code did not acquire lease and simply passed null to Azure storage, which caused this issue.\n\n{code}\n2015-02-26 03:21:28,871 WARN org.apache.hadoop.hbase.regionserver.SplitLogWorker: log splitting of WALs/workernode4.xxx.g6.internal.cloudapp.net,60020,1422071058425-splitting/workernode4.xxx.g6.internal.cloudapp.net%2C60020%2C1422071058425.1424914216773 failed, returning error\njava.io.IOException: org.apache.hadoop.fs.azure.AzureException: java.io.IOException\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.checkForErrors(HLogSplitter.java:633)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.access$000(HLogSplitter.java:121)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.finishWriting(HLogSplitter.java:964)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.finishWritingAndClose(HLogSplitter.java:1019)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:359)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:223)\n\tat org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:142)\n\tat org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.process(HLogSplitterHandler.java:79)\n\tat org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.fs.azure.AzureException: java.io.IOException\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.storeEmptyFolder(AzureNativeFileSystemStore.java:1477)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.mkdirs(NativeAzureFileSystem.java:1862)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.mkdirs(NativeAzureFileSystem.java:1812)\n\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1815)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getRegionSplitEditsPath(HLogSplitter.java:502)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.createWAP(HLogSplitter.java:1211)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.getWriterAndPath(HLogSplitter.java:1200)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.append(HLogSplitter.java:1243)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.writeBuffer(HLogSplitter.java:851)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.doRun(HLogSplitter.java:843)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.run(HLogSplitter.java:813)\nCaused by: java.io.IOException\n\tat com.microsoft.windowsazure.storage.core.Utility.initIOException(Utility.java:493)\n\tat com.microsoft.windowsazure.storage.blob.BlobOutputStream.close(BlobOutputStream.java:282)\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.storeEmptyFolder(AzureNativeFileSystemStore.java:1472)\n\t... 10 more\nCaused by: com.microsoft.windowsazure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)\n\tat com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)\n\tat com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)\n\tat com.microsoft.windowsazure.storage.blob.CloudBlockBlob.commitBlockList(CloudBlockBlob.java:248)\n\tat com.microsoft.windowsazure.storage.blob.BlobOutputStream.commit(BlobOutputStream.java:319)\n\tat com.microsoft.windowsazure.storage.blob.BlobOutputStream.close(BlobOutputStream.java:279)\n\t... 11 more\n{code}"
        }
    },
    {
        "filename": "HADOOP-8589.json",
        "creation_time": "2012-07-11T23:27:00.000+0000",
        "stack_trace": "```\norg.apache.hadoop.fs.FileAlreadyExistsException: Path /var already exists as dir; cannot create link here\n\tat org.apache.hadoop.fs.viewfs.InodeTree.createLink(InodeTree.java:244)\n\tat org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:334)\n\tat org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:167)\n\tat org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:167)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2094)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:79)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem$Cache.java:2128)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2110)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290)\n\tat org.apache.hadoop.fs.viewfs.ViewFileSystemTestSetup.setupForViewFileSystem(ViewFileSystemTestSetup.java:76)\n\tat org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem.setUp(TestFSMainOperationsLocalFileSystem.java:40)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.viewfs.InodeTree.createLink": "  private void createLink(final String src, final String target,\n      final boolean isLinkMerge, final UserGroupInformation aUgi)\n      throws URISyntaxException, IOException,\n    FileAlreadyExistsException, UnsupportedFileSystemException {\n    // Validate that src is valid absolute path\n    final Path srcPath = new Path(src); \n    if (!srcPath.isAbsoluteAndSchemeAuthorityNull()) {\n      throw new IOException(\"ViewFs:Non absolute mount name in config:\" + src);\n    }\n \n    final String[] srcPaths = breakIntoPathComponents(src);\n    INodeDir<T> curInode = root;\n    int i;\n    // Ignore first initial slash, process all except last component\n    for (i = 1; i < srcPaths.length-1; i++) {\n      final String iPath = srcPaths[i];\n      INode<T> nextInode = curInode.resolveInternal(iPath);\n      if (nextInode == null) {\n        INodeDir<T> newDir = curInode.addDir(iPath, aUgi);\n        newDir.InodeDirFs = getTargetFileSystem(newDir);\n        nextInode = newDir;\n      }\n      if (nextInode instanceof INodeLink) {\n        // Error - expected a dir but got a link\n        throw new FileAlreadyExistsException(\"Path \" + nextInode.fullPath +\n            \" already exists as link\");\n      } else {\n        assert(nextInode instanceof INodeDir);\n        curInode = (INodeDir<T>) nextInode;\n      }\n    }\n    \n    // Now process the last component\n    // Add the link in 2 cases: does not exist or a link exists\n    String iPath = srcPaths[i];// last component\n    if (curInode.resolveInternal(iPath) != null) {\n      //  directory/link already exists\n      StringBuilder strB = new StringBuilder(srcPaths[0]);\n      for (int j = 1; j <= i; ++j) {\n        strB.append('/').append(srcPaths[j]);\n      }\n      throw new FileAlreadyExistsException(\"Path \" + strB +\n            \" already exists as dir; cannot create link here\");\n    }\n    \n    final INodeLink<T> newLink;\n    final String fullPath = curInode.fullPath + (curInode == root ? \"\" : \"/\")\n        + iPath;\n    if (isLinkMerge) { // Target is list of URIs\n      String[] targetsList = StringUtils.getStrings(target);\n      URI[] targetsListURI = new URI[targetsList.length];\n      int k = 0;\n      for (String itarget : targetsList) {\n        targetsListURI[k++] = new URI(itarget);\n      }\n      newLink = new INodeLink<T>(fullPath, aUgi,\n          getTargetFileSystem(targetsListURI), targetsListURI);\n    } else {\n      newLink = new INodeLink<T>(fullPath, aUgi,\n          getTargetFileSystem(new URI(target)), new URI(target));\n    }\n    curInode.addLink(iPath, newLink);\n    mountPoints.add(new MountPoint<T>(src, newLink));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.viewfs.InodeTree.getTargetFileSystem": "  protected abstract T getTargetFileSystem(final URI[] mergeFsURIList)\n  throws UnsupportedFileSystemException, URISyntaxException;\n  \n  /**\n   * Create Inode Tree from the specified mount-table specified in Config\n   * @param config - the mount table keys are prefixed with \n   *       FsConstants.CONFIG_VIEWFS_PREFIX\n   * @param viewName - the name of the mount table - if null use defaultMT name\n   * @throws UnsupportedFileSystemException\n   * @throws URISyntaxException\n   * @throws FileAlreadyExistsException\n   * @throws IOException\n   */\n  protected InodeTree(final Configuration config, final String viewName)\n      throws UnsupportedFileSystemException, URISyntaxException,\n    FileAlreadyExistsException, IOException { \n    String vName = viewName;\n    if (vName == null) {\n      vName = Constants.CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE;\n    }\n    homedirPrefix = ConfigUtil.getHomeDirValue(config, vName);\n    root = new INodeDir<T>(\"/\", UserGroupInformation.getCurrentUser());\n    root.InodeDirFs = getTargetFileSystem(root);\n    root.isRoot = true;\n    \n    final String mtPrefix = Constants.CONFIG_VIEWFS_PREFIX + \".\" + \n                            vName + \".\";\n    final String linkPrefix = Constants.CONFIG_VIEWFS_LINK + \".\";\n    final String linkMergePrefix = Constants.CONFIG_VIEWFS_LINK_MERGE + \".\";\n    boolean gotMountTableEntry = false;\n    final UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    for (Entry<String, String> si : config) {\n      final String key = si.getKey();\n      if (key.startsWith(mtPrefix)) {\n        gotMountTableEntry = true;\n        boolean isMergeLink = false;\n        String src = key.substring(mtPrefix.length());\n        if (src.startsWith(linkPrefix)) {\n          src = src.substring(linkPrefix.length());\n        } else if (src.startsWith(linkMergePrefix)) { // A merge link\n          isMergeLink = true;\n          src = src.substring(linkMergePrefix.length());\n        } else if (src.startsWith(Constants.CONFIG_VIEWFS_HOMEDIR)) {\n          // ignore - we set home dir from config\n          continue;\n        } else {\n          throw new IOException(\n          \"ViewFs: Cannot initialize: Invalid entry in Mount table in config: \"+ \n          src);\n        }\n        final String target = si.getValue(); // link or merge link\n        createLink(src, target, isMergeLink, ugi); \n      }\n    }\n    if (!gotMountTableEntry) {\n      throw new IOException(\n          \"ViewFs: Cannot initialize: Empty Mount table in config for \" + \n             vName == null ? \"viewfs:///\" : (\"viewfs://\" + vName + \"/\"));\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.viewfs.InodeTree.addDir": "    INodeDir<T> addDir(final String pathComponent,\n        final UserGroupInformation aUgi)\n      throws FileAlreadyExistsException {\n      if (children.containsKey(pathComponent)) {\n        throw new FileAlreadyExistsException();\n      }\n      final INodeDir<T> newDir = new INodeDir<T>(fullPath+ (isRoot ? \"\" : \"/\") + \n          pathComponent, aUgi);\n      children.put(pathComponent, newDir);\n      return newDir;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.viewfs.InodeTree.addLink": "    void addLink(final String pathComponent, final INodeLink<T> link)\n      throws FileAlreadyExistsException {\n      if (children.containsKey(pathComponent)) {\n        throw new FileAlreadyExistsException();\n      }\n      children.put(pathComponent, link);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.viewfs.InodeTree.resolveInternal": "    INode<T> resolveInternal(final String pathComponent)\n        throws FileNotFoundException {\n      return children.get(pathComponent);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.viewfs.InodeTree.breakIntoPathComponents": "  static String[] breakIntoPathComponents(final String path) {\n    return path == null ? null : path.split(Path.SEPARATOR);\n  } ",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize": "  public void initialize(final URI theUri, final Configuration conf)\n      throws IOException {\n    super.initialize(theUri, conf);\n    setConf(conf);\n    config = conf;\n    // Now build  client side view (i.e. client side mount table) from config.\n    final String authority = theUri.getAuthority();\n    try {\n      myUri = new URI(FsConstants.VIEWFS_SCHEME, authority, \"/\", null, null);\n      fsState = new InodeTree<FileSystem>(conf, authority) {\n\n        @Override\n        protected\n        FileSystem getTargetFileSystem(final URI uri)\n          throws URISyntaxException, IOException {\n            return new ChRootedFileSystem(uri, config);\n        }\n\n        @Override\n        protected\n        FileSystem getTargetFileSystem(final INodeDir<FileSystem> dir)\n          throws URISyntaxException {\n          return new InternalDirOfViewFs(dir, creationTime, ugi, myUri);\n        }\n\n        @Override\n        protected\n        FileSystem getTargetFileSystem(URI[] mergeFsURIList)\n            throws URISyntaxException, UnsupportedFileSystemException {\n          throw new UnsupportedFileSystemException(\"mergefs not implemented\");\n          // return MergeFs.createMergeFs(mergeFsURIList, config);\n        }\n      };\n      workingDir = this.getHomeDirectory();\n    } catch (URISyntaxException e) {\n      throw new IOException(\"URISyntax exception: \" + theUri);\n    }\n\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.viewfs.ViewFileSystem.getHomeDirectory": "  public Path getHomeDirectory() {\n    if (homeDir == null) {\n      String base = fsState.getHomeDirPrefixValue();\n      if (base == null) {\n        base = \"/user\";\n      }\n      homeDir = \n        this.makeQualified(new Path(base + \"/\" + ugi.getShortUserName()));\n    }\n    return homeDir;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.createFileSystem": "  private static FileSystem createFileSystem(URI uri, Configuration conf\n      ) throws IOException {\n    Class<?> clazz = getFileSystemClass(uri.getScheme(), conf);\n    if (clazz == null) {\n      throw new IOException(\"No FileSystem for scheme: \" + uri.getScheme());\n    }\n    FileSystem fs = (FileSystem)ReflectionUtils.newInstance(clazz, conf);\n    fs.initialize(uri, conf);\n    return fs;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.newInstance": "  public static FileSystem newInstance(Configuration conf) throws IOException {\n    return newInstance(getDefaultUri(conf), conf);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getScheme": "    public String getScheme() {\n      return scheme;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getFileSystemClass": "  public static Class<? extends FileSystem> getFileSystemClass(String scheme,\n      Configuration conf) throws IOException {\n    if (!FILE_SYSTEMS_LOADED) {\n      loadFileSystems();\n    }\n    Class<? extends FileSystem> clazz = null;\n    if (conf != null) {\n      clazz = (Class<? extends FileSystem>) conf.getClass(\"fs.\" + scheme + \".impl\", null);\n    }\n    if (clazz == null) {\n      clazz = SERVICE_FILE_SYSTEMS.get(scheme);\n    }\n    if (clazz == null) {\n      throw new IOException(\"No FileSystem for scheme: \" + scheme);\n    }\n    return clazz;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.initialize": "  public void initialize(URI name, Configuration conf) throws IOException {\n    statistics = getStatistics(name.getScheme(), getClass());    \n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.get": "    FileSystem get(URI uri, Configuration conf) throws IOException{\n      Key key = new Key(uri, conf);\n      return getInternal(uri, conf, key);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getInternal": "    private FileSystem getInternal(URI uri, Configuration conf, Key key) throws IOException{\n      FileSystem fs;\n      synchronized (this) {\n        fs = map.get(key);\n      }\n      if (fs != null) {\n        return fs;\n      }\n\n      fs = createFileSystem(uri, conf);\n      synchronized (this) { // refetch the lock again\n        FileSystem oldfs = map.get(key);\n        if (oldfs != null) { // a file system is created while lock is releasing\n          fs.close(); // close the new file system\n          return oldfs;  // return the old file system\n        }\n        \n        // now insert the new file system into the map\n        if (map.isEmpty() ) {\n          ShutdownHookManager.get().addShutdownHook(clientFinalizer, SHUTDOWN_HOOK_PRIORITY);\n        }\n        fs.key = key;\n        map.put(key, fs);\n        if (conf.getBoolean(\"fs.automatic.close\", true)) {\n          toAutoClose.add(key);\n        }\n        return fs;\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.equals": "      public boolean equals(Object obj) {\n        if (obj == this) {\n          return true;\n        }\n        if (obj != null && obj instanceof Key) {\n          Key that = (Key)obj;\n          return isEqual(this.scheme, that.scheme)\n                 && isEqual(this.authority, that.authority)\n                 && isEqual(this.ugi, that.ugi)\n                 && (this.unique == that.unique);\n        }\n        return false;        \n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getDefaultUri": "  public static URI getDefaultUri(Configuration conf) {\n    return URI.create(fixName(conf.get(FS_DEFAULT_NAME_KEY, DEFAULT_FS)));\n  }"
        },
        "bug_report": {
            "Title": "ViewFs tests fail when tests and home dirs are nested",
            "Description": "TestFSMainOperationsLocalFileSystem fails in case when the test root directory is under the user's home directory, and the user's home dir is deeper than 2 levels from /. This happens with the default 1-node installation of Jenkins. \n\nThis is the failure log:\n\n{code}\norg.apache.hadoop.fs.FileAlreadyExistsException: Path /var already exists as dir; cannot create link here\n\tat org.apache.hadoop.fs.viewfs.InodeTree.createLink(InodeTree.java:244)\n\tat org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:334)\n\tat org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:167)\n\tat org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:167)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2094)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:79)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2128)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2110)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290)\n\tat org.apache.hadoop.fs.viewfs.ViewFileSystemTestSetup.setupForViewFileSystem(ViewFileSystemTestSetup.java:76)\n\tat org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem.setUp(TestFSMainOperationsLocalFileSystem.java:40)\n\n...\n\nStandard Output\n2012-07-11 22:07:20,239 INFO  mortbay.log (Slf4jLog.java:info(67)) - Home dir base /var/lib\n{code}\n\nThe reason for the failure is that the code tries to mount links for both \"/var\" and \"/var/lib\", and it fails for the 2nd one as the \"/var\" is mounted already.\n\nThe fix was provided in HADOOP-8036 but later it was reverted in HADOOP-8129."
        }
    },
    {
        "filename": "HADOOP-11754.json",
        "creation_time": "2015-03-26T05:22:54.000+0000",
        "stack_trace": "```\njavax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:266)\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:225)\n\tat org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.init(DelegationTokenAuthenticationFilter.java:161)\n\tat org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.init(RMAuthenticationFilter.java:53)\n\tat org.mortbay.jetty.servlet.FilterHolder.doStart(FilterHolder.java:97)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:713)\n\tat org.mortbay.jetty.servlet.Context.startContext(Context.java:140)\n\tat org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)\n\tat org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)\n\tat org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)\n\tat org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)\n\tat org.mortbay.jetty.Server.doStart(Server.java:224)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:773)\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)\nCaused by: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret\n\tat org.apache.hadoop.security.authentication.util.FileSignerSecretProvider.init(FileSignerSecretProvider.java:59)\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:264)\n\t... 23 more\n\norg.apache.hadoop.yarn.webapp.WebAppException: Error starting http server\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:279)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)\nCaused by: java.io.IOException: Problem in starting http server. Server handlers failed\n\tat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:785)\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)```",
        "source_code": {
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider": "  protected void initializeSecretProvider(FilterConfig filterConfig)\n      throws ServletException {\n    secretProvider = (SignerSecretProvider) filterConfig.getServletContext().\n        getAttribute(SIGNER_SECRET_PROVIDER_ATTRIBUTE);\n    if (secretProvider == null) {\n      Class<? extends SignerSecretProvider> providerClass\n              = getProviderClass(config);\n      try {\n        secretProvider = providerClass.newInstance();\n      } catch (InstantiationException ex) {\n        throw new ServletException(ex);\n      } catch (IllegalAccessException ex) {\n        throw new ServletException(ex);\n      }\n      try {\n        secretProvider.init(config, filterConfig.getServletContext(), validity);\n      } catch (Exception ex) {\n        throw new ServletException(ex);\n      }\n    } else {\n      customSecretProvider = true;\n    }\n    signer = new Signer(secretProvider);\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.getProviderClass": "  private Class<? extends SignerSecretProvider> getProviderClass(Properties config)\n          throws ServletException {\n    String providerClassName;\n    String signerSecretProviderName\n            = config.getProperty(SIGNER_SECRET_PROVIDER, null);\n    // fallback to old behavior\n    if (signerSecretProviderName == null) {\n      String signatureSecret = config.getProperty(SIGNATURE_SECRET, null);\n      String signatureSecretFile = config.getProperty(\n          SIGNATURE_SECRET_FILE, null);\n      // The precedence from high to low : file, inline string, random\n      if (signatureSecretFile != null) {\n        providerClassName = FileSignerSecretProvider.class.getName();\n      } else if (signatureSecret != null) {\n        providerClassName = StringSignerSecretProvider.class.getName();\n      } else {\n        providerClassName = RandomSignerSecretProvider.class.getName();\n        randomSecret = true;\n      }\n    } else {\n      if (\"random\".equals(signerSecretProviderName)) {\n        providerClassName = RandomSignerSecretProvider.class.getName();\n        randomSecret = true;\n      } else if (\"string\".equals(signerSecretProviderName)) {\n        providerClassName = StringSignerSecretProvider.class.getName();\n      } else if (\"file\".equals(signerSecretProviderName)) {\n        providerClassName = FileSignerSecretProvider.class.getName();\n      } else if (\"zookeeper\".equals(signerSecretProviderName)) {\n        providerClassName = ZKSignerSecretProvider.class.getName();\n      } else {\n        providerClassName = signerSecretProviderName;\n        customSecretProvider = true;\n      }\n    }\n    try {\n      return (Class<? extends SignerSecretProvider>) Thread.currentThread().\n              getContextClassLoader().loadClass(providerClassName);\n    } catch (ClassNotFoundException ex) {\n      throw new ServletException(ex);\n    }\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.init": "  public void init(FilterConfig filterConfig) throws ServletException {\n    String configPrefix = filterConfig.getInitParameter(CONFIG_PREFIX);\n    configPrefix = (configPrefix != null) ? configPrefix + \".\" : \"\";\n    config = getConfiguration(configPrefix, filterConfig);\n    String authHandlerName = config.getProperty(AUTH_TYPE, null);\n    String authHandlerClassName;\n    if (authHandlerName == null) {\n      throw new ServletException(\"Authentication type must be specified: \" +\n          PseudoAuthenticationHandler.TYPE + \"|\" + \n          KerberosAuthenticationHandler.TYPE + \"|<class>\");\n    }\n    if (authHandlerName.toLowerCase(Locale.ENGLISH).equals(\n        PseudoAuthenticationHandler.TYPE)) {\n      authHandlerClassName = PseudoAuthenticationHandler.class.getName();\n    } else if (authHandlerName.toLowerCase(Locale.ENGLISH).equals(\n        KerberosAuthenticationHandler.TYPE)) {\n      authHandlerClassName = KerberosAuthenticationHandler.class.getName();\n    } else {\n      authHandlerClassName = authHandlerName;\n    }\n\n    validity = Long.parseLong(config.getProperty(AUTH_TOKEN_VALIDITY, \"36000\"))\n        * 1000; //10 hours\n    initializeSecretProvider(filterConfig);\n\n    initializeAuthHandler(authHandlerClassName, filterConfig);\n\n\n    cookieDomain = config.getProperty(COOKIE_DOMAIN, null);\n    cookiePath = config.getProperty(COOKIE_PATH, null);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.init": "  public void init(FilterConfig filterConfig) throws ServletException {\n    super.init(filterConfig);\n    AuthenticationHandler handler = getAuthenticationHandler();\n    AbstractDelegationTokenSecretManager dtSecretManager =\n        (AbstractDelegationTokenSecretManager) filterConfig.getServletContext().\n            getAttribute(DELEGATION_TOKEN_SECRET_MANAGER_ATTR);\n    if (dtSecretManager != null && handler\n        instanceof DelegationTokenAuthenticationHandler) {\n      DelegationTokenAuthenticationHandler dtHandler =\n          (DelegationTokenAuthenticationHandler) getAuthenticationHandler();\n      dtHandler.setExternalDelegationTokenSecretManager(dtSecretManager);\n    }\n    if (handler instanceof PseudoAuthenticationHandler ||\n        handler instanceof PseudoDelegationTokenAuthenticationHandler) {\n      setHandlerAuthMethod(SaslRpcServer.AuthMethod.SIMPLE);\n    }\n    if (handler instanceof KerberosAuthenticationHandler ||\n        handler instanceof KerberosDelegationTokenAuthenticationHandler) {\n      setHandlerAuthMethod(SaslRpcServer.AuthMethod.KERBEROS);\n    }\n\n    // proxyuser configuration\n    Configuration conf = getProxyuserConfiguration(filterConfig);\n    ProxyUsers.refreshSuperUserGroupsConfiguration(conf, PROXYUSER_PREFIX);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.getProxyuserConfiguration": "  protected Configuration getProxyuserConfiguration(FilterConfig filterConfig)\n      throws ServletException {\n    // this filter class gets the configuration from the filter configs, we are\n    // creating an empty configuration and injecting the proxyuser settings in\n    // it. In the initialization of the filter, the returned configuration is\n    // passed to the ProxyUsers which only looks for 'proxyusers.' properties.\n    Configuration conf = new Configuration(false);\n    Enumeration<?> names = filterConfig.getInitParameterNames();\n    while (names.hasMoreElements()) {\n      String name = (String) names.nextElement();\n      if (name.startsWith(PROXYUSER_PREFIX + \".\")) {\n        String value = filterConfig.getInitParameter(name);\n        conf.set(name, value);\n      }\n    }\n    return conf;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.setHandlerAuthMethod": "  protected void setHandlerAuthMethod(SaslRpcServer.AuthMethod authMethod) {\n    this.handlerAuthMethod = authMethod;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.start": "  public void start() throws IOException {\n    try {\n      try {\n        openListeners();\n        webServer.start();\n      } catch (IOException ex) {\n        LOG.info(\"HttpServer.start() threw a non Bind IOException\", ex);\n        throw ex;\n      } catch (MultiException ex) {\n        LOG.info(\"HttpServer.start() threw a MultiException\", ex);\n        throw ex;\n      }\n      // Make sure there is no handler failures.\n      Handler[] handlers = webServer.getHandlers();\n      for (Handler handler : handlers) {\n        if (handler.isFailed()) {\n          throw new IOException(\n              \"Problem in starting http server. Server handlers failed\");\n        }\n      }\n      // Make sure there are no errors initializing the context.\n      Throwable unavailableException = webAppContext.getUnavailableException();\n      if (unavailableException != null) {\n        // Have to stop the webserver, or else its non-daemon threads\n        // will hang forever.\n        webServer.stop();\n        throw new IOException(\"Unable to initialize WebAppContext\",\n            unavailableException);\n      }\n    } catch (IOException e) {\n      throw e;\n    } catch (InterruptedException e) {\n      throw (IOException) new InterruptedIOException(\n          \"Interrupted while starting HTTP server\").initCause(e);\n    } catch (Exception e) {\n      throw new IOException(\"Problem starting http server\", e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.openListeners": "  void openListeners() throws Exception {\n    for (Connector listener : listeners) {\n      if (listener.getLocalPort() != -1) {\n        // This listener is either started externally or has been bound\n        continue;\n      }\n      int port = listener.getPort();\n      while (true) {\n        // jetty has a bug where you can't reopen a listener that previously\n        // failed to open w/o issuing a close first, even if the port is changed\n        try {\n          listener.close();\n          listener.open();\n          LOG.info(\"Jetty bound to port \" + listener.getLocalPort());\n          break;\n        } catch (BindException ex) {\n          if (port == 0 || !findPort) {\n            BindException be = new BindException(\"Port in use: \"\n                + listener.getHost() + \":\" + listener.getPort());\n            be.initCause(ex);\n            throw be;\n          }\n        }\n        // try the next port number\n        listener.setPort(++port);\n        Thread.sleep(100);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.stop": "  public void stop() throws Exception {\n    MultiException exception = null;\n    for (Connector c : listeners) {\n      try {\n        c.close();\n      } catch (Exception e) {\n        LOG.error(\n            \"Error while stopping listener for webapp\"\n                + webAppContext.getDisplayName(), e);\n        exception = addMultiException(exception, e);\n      }\n    }\n\n    try {\n      // clear & stop webAppContext attributes to avoid memory leaks.\n      webAppContext.clearAttributes();\n      webAppContext.stop();\n    } catch (Exception e) {\n      LOG.error(\"Error while stopping web app context for webapp \"\n          + webAppContext.getDisplayName(), e);\n      exception = addMultiException(exception, e);\n    }\n\n    try {\n      webServer.stop();\n    } catch (Exception e) {\n      LOG.error(\"Error while stopping web server for webapp \"\n          + webAppContext.getDisplayName(), e);\n      exception = addMultiException(exception, e);\n    }\n\n    if (exception != null) {\n      exception.ifExceptionThrow();\n    }\n\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.start": "  public void start() {\n    if (isInState(STATE.STARTED)) {\n      return;\n    }\n    //enter the started state\n    synchronized (stateChangeLock) {\n      if (stateModel.enterState(STATE.STARTED) != STATE.STARTED) {\n        try {\n          startTime = System.currentTimeMillis();\n          serviceStart();\n          if (isInState(STATE.STARTED)) {\n            //if the service started (and isn't now in a later state), notify\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Service \" + getName() + \" is started\");\n            }\n            notifyListeners();\n          }\n        } catch (Exception e) {\n          noteFailure(e);\n          ServiceOperations.stopQuietly(LOG, this);\n          throw ServiceStateException.convert(e);\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.isInState": "  public final boolean isInState(Service.STATE expected) {\n    return stateModel.isInState(expected);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.getName": "  public String getName() {\n    return name;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.serviceStart": "  protected void serviceStart() throws Exception {\n\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.enterState": "  private STATE enterState(STATE newState) {\n    assert stateModel != null : \"null state in \" + name + \" \" + this.getClass();\n    STATE oldState = stateModel.enterState(newState);\n    if (oldState != newState) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\n          \"Service: \" + getName() + \" entered state \" + getServiceState());\n      }\n      recordLifecycleEvent();\n    }\n    return oldState;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.noteFailure": "  protected final void noteFailure(Exception exception) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"noteFailure \" + exception, null);\n    }\n    if (exception == null) {\n      //make sure failure logic doesn't itself cause problems\n      return;\n    }\n    //record the failure details, and log it\n    synchronized (this) {\n      if (failureCause == null) {\n        failureCause = exception;\n        failureState = getServiceState();\n        LOG.info(\"Service \" + getName()\n                 + \" failed in state \" + failureState\n                 + \"; cause: \" + exception,\n                 exception);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.notifyListeners": "  private void notifyListeners() {\n    try {\n      listeners.notifyListeners(this);\n      globalListeners.notifyListeners(this);\n    } catch (Throwable e) {\n      LOG.warn(\"Exception while notifying listeners of \" + this + \": \" + e,\n               e);\n    }\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.util.FileSignerSecretProvider.init": "  public void init(Properties config, ServletContext servletContext,\n                   long tokenValidity) throws Exception {\n\n    String signatureSecretFile = config.getProperty(\n        AuthenticationFilter.SIGNATURE_SECRET_FILE, null);\n\n    Reader reader = null;\n    if (signatureSecretFile != null) {\n      try {\n        StringBuilder sb = new StringBuilder();\n        reader = new InputStreamReader(\n            new FileInputStream(signatureSecretFile), Charsets.UTF_8);\n        int c = reader.read();\n        while (c > -1) {\n          sb.append((char) c);\n          c = reader.read();\n        }\n        secret = sb.toString().getBytes(Charset.forName(\"UTF-8\"));\n      } catch (IOException ex) {\n        throw new RuntimeException(\"Could not read signature secret file: \" +\n            signatureSecretFile);\n      } finally {\n        if (reader != null) {\n          try {\n            reader.close();\n          } catch (IOException e) {\n            // nothing to do\n          }\n        }\n      }\n    }\n\n    secrets = new byte[][]{secret};\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.setExternalDelegationTokenSecretManager": "  public void setExternalDelegationTokenSecretManager(\n      AbstractDelegationTokenSecretManager secretManager) {\n    tokenManager.setExternalDelegationTokenSecretManager(secretManager);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceOperations.stopQuietly": "  public static Exception stopQuietly(Log log, Service service) {\n    try {\n      stop(service);\n    } catch (Exception e) {\n      log.warn(\"When stopping the service \" + service.getName()\n               + \" : \" + e,\n               e);\n      return e;\n    }\n    return null;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceOperations.stop": "  public static void stop(Service service) {\n    if (service != null) {\n      service.stop();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceStateException.convert": "  public static RuntimeException convert(String text, Throwable fault) {\n    if (fault instanceof RuntimeException) {\n      return (RuntimeException) fault;\n    } else {\n      return new ServiceStateException(text, fault);\n    }\n  }"
        },
        "bug_report": {
            "Title": "RM fails to start in non-secure mode due to authentication filter failure",
            "Description": "RM fails to start in the non-secure mode with the following exception:\n\n{noformat}\n2015-03-25 22:02:42,526 WARN org.mortbay.log: failed RMAuthenticationFilter: javax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret\n2015-03-25 22:02:42,526 WARN org.mortbay.log: Failed startup of context org.mortbay.jetty.webapp.WebAppContext@6de50b08{/,jar:file:/Users/sjlee/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-common-3.0.0-SNAPSHOT.jar!/webapps/cluster}\njavax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:266)\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:225)\n\tat org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.init(DelegationTokenAuthenticationFilter.java:161)\n\tat org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.init(RMAuthenticationFilter.java:53)\n\tat org.mortbay.jetty.servlet.FilterHolder.doStart(FilterHolder.java:97)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:713)\n\tat org.mortbay.jetty.servlet.Context.startContext(Context.java:140)\n\tat org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)\n\tat org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)\n\tat org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)\n\tat org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)\n\tat org.mortbay.jetty.Server.doStart(Server.java:224)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:773)\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)\nCaused by: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret\n\tat org.apache.hadoop.security.authentication.util.FileSignerSecretProvider.init(FileSignerSecretProvider.java:59)\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:264)\n\t... 23 more\n...\n2015-03-25 22:02:42,538 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error starting ResourceManager\norg.apache.hadoop.yarn.webapp.WebAppException: Error starting http server\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:279)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)\nCaused by: java.io.IOException: Problem in starting http server. Server handlers failed\n\tat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:785)\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)\n\t... 4 more\n{noformat}\n\nThis is likely a regression introduced by HADOOP-10670."
        }
    },
    {
        "filename": "HADOOP-8225.json",
        "creation_time": "2012-03-13T20:33:53.000+0000",
        "stack_trace": "```\njava.lang.SecurityException: Intercepted System.exit(-999)\n    at org.apache.oozie.action.hadoop.LauncherSecurityManager.checkExit(LauncherMapper.java:651)\n    at java.lang.Runtime.exit(Runtime.java:88)\n    at java.lang.System.exit(System.java:904)\n    at org.apache.hadoop.tools.DistCp.main(DistCp.java:357)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:394)\n    at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)\n    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:396)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)\n    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)\n```",
        "source_code": {
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.DistCp.main": "  public static void main(String argv[]) {\n    try {\n      DistCp distCp = new DistCp();\n      Cleanup CLEANUP = new Cleanup(distCp);\n\n      Runtime.getRuntime().addShutdownHook(CLEANUP);\n      System.exit(ToolRunner.run(getDefaultConf(), distCp, argv));\n    }\n    catch (Exception e) {\n      LOG.error(\"Couldn't complete DistCp operation: \", e);\n      System.exit(DistCpConstants.UNKNOWN_ERROR);\n    }\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.DistCp.getDefaultConf": "  private static Configuration getDefaultConf() {\n    Configuration config = new Configuration();\n    config.addResource(DISTCP_DEFAULT_XML);\n    return config;\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.DistCp.run": "    public void run() {\n      if (distCp.isSubmitted()) return;\n\n      distCp.cleanup();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapRunner.run": "  public void run(RecordReader<K1, V1> input, OutputCollector<K2, V2> output,\n                  Reporter reporter)\n    throws IOException {\n    try {\n      // allocate key & value instances that are re-used for all entries\n      K1 key = input.createKey();\n      V1 value = input.createValue();\n      \n      while (input.next(key, value)) {\n        // map pair to output\n        mapper.map(key, value, output, reporter);\n        if(incrProcCount) {\n          reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n              SkipBadRecords.COUNTER_MAP_PROCESSED_RECORDS, 1);\n        }\n      }\n    } finally {\n      mapper.close();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.runOldMapper": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runOldMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, InterruptedException,\n                             ClassNotFoundException {\n    InputSplit inputSplit = getSplitDetails(new Path(splitIndex.getSplitLocation()),\n           splitIndex.getStartOffset());\n\n    updateJobWithSplit(job, inputSplit);\n    reporter.setInputSplit(inputSplit);\n\n    RecordReader<INKEY,INVALUE> in = isSkipping() ? \n        new SkippingRecordReader<INKEY,INVALUE>(umbilical, reporter, job) :\n          new TrackedRecordReader<INKEY,INVALUE>(reporter, job);\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n\n\n    int numReduceTasks = conf.getNumReduceTasks();\n    LOG.info(\"numReduceTasks: \" + numReduceTasks);\n    MapOutputCollector collector = null;\n    if (numReduceTasks > 0) {\n      collector = new MapOutputBuffer(umbilical, job, reporter);\n    } else { \n      collector = new DirectMapOutputCollector(umbilical, job, reporter);\n    }\n    MapRunnable<INKEY,INVALUE,OUTKEY,OUTVALUE> runner =\n      ReflectionUtils.newInstance(job.getMapRunnerClass(), job);\n\n    try {\n      runner.run(in, new OldOutputCollector(collector, conf), reporter);\n      mapPhase.complete();\n      // start the sort phase only if there are reducers\n      if (numReduceTasks > 0) {\n        setPhase(TaskStatus.Phase.SORT);\n      }\n      statusUpdate(umbilical);\n      collector.flush();\n    } finally {\n      //close\n      in.close();                               // close input\n      collector.close();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.updateJobWithSplit": "  private void updateJobWithSplit(final JobConf job, InputSplit inputSplit) {\n    if (inputSplit instanceof FileSplit) {\n      FileSplit fileSplit = (FileSplit) inputSplit;\n      job.set(JobContext.MAP_INPUT_FILE, fileSplit.getPath().toString());\n      job.setLong(JobContext.MAP_INPUT_START, fileSplit.getStart());\n      job.setLong(JobContext.MAP_INPUT_PATH, fileSplit.getLength());\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.run": "      public void run() {\n        spillLock.lock();\n        spillThreadRunning = true;\n        try {\n          while (true) {\n            spillDone.signal();\n            while (!spillInProgress) {\n              spillReady.await();\n            }\n            try {\n              spillLock.unlock();\n              sortAndSpill();\n            } catch (Throwable t) {\n              sortSpillException = t;\n            } finally {\n              spillLock.lock();\n              if (bufend < bufstart) {\n                bufvoid = kvbuffer.length;\n              }\n              kvstart = kvend;\n              bufstart = bufend;\n              spillInProgress = false;\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        } finally {\n          spillLock.unlock();\n          spillThreadRunning = false;\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.close": "      public void close() { }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getSplitDetails": " private <T> T getSplitDetails(Path file, long offset) \n  throws IOException {\n   FileSystem fs = file.getFileSystem(conf);\n   FSDataInputStream inFile = fs.open(file);\n   inFile.seek(offset);\n   String className = Text.readString(inFile);\n   Class<T> cls;\n   try {\n     cls = (Class<T>) conf.getClassByName(className);\n   } catch (ClassNotFoundException ce) {\n     IOException wrap = new IOException(\"Split class \" + className + \n                                         \" not found\");\n     wrap.initCause(ce);\n     throw wrap;\n   }\n   SerializationFactory factory = new SerializationFactory(conf);\n   Deserializer<T> deserializer = \n     (Deserializer<T>) factory.getDeserializer(cls);\n   deserializer.open(inFile);\n   T split = deserializer.deserialize(null);\n   long pos = inFile.getPos();\n   getCounters().findCounter(\n       TaskCounter.SPLIT_RAW_BYTES).increment(pos - offset);\n   inFile.close();\n   return split;\n }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.flush": "    public void flush() throws IOException, ClassNotFoundException,\n           InterruptedException {\n      LOG.info(\"Starting flush of map output\");\n      spillLock.lock();\n      try {\n        while (spillInProgress) {\n          reporter.progress();\n          spillDone.await();\n        }\n        checkSpillException();\n\n        final int kvbend = 4 * kvend;\n        if ((kvbend + METASIZE) % kvbuffer.length !=\n            equator - (equator % METASIZE)) {\n          // spill finished\n          resetSpill();\n        }\n        if (kvindex != kvend) {\n          kvend = (kvindex + NMETA) % kvmeta.capacity();\n          bufend = bufmark;\n          if (LOG.isInfoEnabled()) {\n            LOG.info(\"Spilling map output\");\n            LOG.info(\"bufstart = \" + bufstart + \"; bufend = \" + bufmark +\n                     \"; bufvoid = \" + bufvoid);\n            LOG.info(\"kvstart = \" + kvstart + \"(\" + (kvstart * 4) +\n                     \"); kvend = \" + kvend + \"(\" + (kvend * 4) +\n                     \"); length = \" + (distanceTo(kvend, kvstart,\n                           kvmeta.capacity()) + 1) + \"/\" + maxRec);\n          }\n          sortAndSpill();\n        }\n      } catch (InterruptedException e) {\n        throw new IOException(\"Interrupted while waiting for the writer\", e);\n      } finally {\n        spillLock.unlock();\n      }\n      assert !spillLock.isHeldByCurrentThread();\n      // shut down spill thread and wait for it to exit. Since the preceding\n      // ensures that it is finished with its work (and sortAndSpill did not\n      // throw), we elect to use an interrupt instead of setting a flag.\n      // Spilling simultaneously from this thread while the spill thread\n      // finishes its work might be both a useful way to extend this and also\n      // sufficient motivation for the latter approach.\n      try {\n        spillThread.interrupt();\n        spillThread.join();\n      } catch (InterruptedException e) {\n        throw new IOException(\"Spill failed\", e);\n      }\n      // release sort buffer before the merge\n      kvbuffer = null;\n      mergeParts();\n      Path outputPath = mapOutputFile.getOutputFile();\n      fileOutputByteCounter.increment(rfs.getFileStatus(outputPath).getLen());\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.run": "              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      LOG.error(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(pae,\"Unknown exception in doAs\");\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.main": "  public static void main(String[] args) throws Throwable {\n    LOG.debug(\"Child starting\");\n\n    final JobConf defaultConf = new JobConf();\n    defaultConf.addResource(MRJobConfig.JOB_CONF_FILE);\n    UserGroupInformation.setConfiguration(defaultConf);\n\n    String host = args[0];\n    int port = Integer.parseInt(args[1]);\n    final InetSocketAddress address = new InetSocketAddress(host, port);\n    final TaskAttemptID firstTaskid = TaskAttemptID.forName(args[2]);\n    int jvmIdInt = Integer.parseInt(args[3]);\n    JVMId jvmId = new JVMId(firstTaskid.getJobID(),\n        firstTaskid.getTaskType() == TaskType.MAP, jvmIdInt);\n\n    // initialize metrics\n    DefaultMetricsSystem.initialize(\n        StringUtils.camelize(firstTaskid.getTaskType().name()) +\"Task\");\n\n    Token<JobTokenIdentifier> jt = loadCredentials(defaultConf, address);\n\n    // Create TaskUmbilicalProtocol as actual task owner.\n    UserGroupInformation taskOwner =\n      UserGroupInformation.createRemoteUser(firstTaskid.getJobID().toString());\n    taskOwner.addToken(jt);\n    final TaskUmbilicalProtocol umbilical =\n      taskOwner.doAs(new PrivilegedExceptionAction<TaskUmbilicalProtocol>() {\n      @Override\n      public TaskUmbilicalProtocol run() throws Exception {\n        return (TaskUmbilicalProtocol)RPC.getProxy(TaskUmbilicalProtocol.class,\n            TaskUmbilicalProtocol.versionID, address, defaultConf);\n      }\n    });\n\n    // report non-pid to application master\n    JvmContext context = new JvmContext(jvmId, \"-1000\");\n    LOG.debug(\"PID: \" + System.getenv().get(\"JVM_PID\"));\n    Task task = null;\n    UserGroupInformation childUGI = null;\n\n    try {\n      int idleLoopCount = 0;\n      JvmTask myTask = null;;\n      // poll for new task\n      for (int idle = 0; null == myTask; ++idle) {\n        long sleepTimeMilliSecs = Math.min(idle * 500, 1500);\n        LOG.info(\"Sleeping for \" + sleepTimeMilliSecs\n            + \"ms before retrying again. Got null now.\");\n        MILLISECONDS.sleep(sleepTimeMilliSecs);\n        myTask = umbilical.getTask(context);\n      }\n      if (myTask.shouldDie()) {\n        return;\n      }\n\n      task = myTask.getTask();\n      YarnChild.taskid = task.getTaskID();\n\n      // Create the job-conf and set credentials\n      final JobConf job =\n        configureTask(task, defaultConf.getCredentials(), jt);\n\n      // Initiate Java VM metrics\n      JvmMetrics.initSingleton(jvmId.toString(), job.getSessionId());\n      childUGI = UserGroupInformation.createRemoteUser(System\n          .getenv(ApplicationConstants.Environment.USER.toString()));\n      // Add tokens to new user so that it may execute its task correctly.\n      for(Token<?> token : UserGroupInformation.getCurrentUser().getTokens()) {\n        childUGI.addToken(token);\n      }\n\n      // Create a final reference to the task for the doAs block\n      final Task taskFinal = task;\n      childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n        @Override\n        public Object run() throws Exception {\n          // use job-specified working directory\n          FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory());\n          taskFinal.run(job, umbilical); // run the task\n          return null;\n        }\n      });\n    } catch (FSError e) {\n      LOG.fatal(\"FSError from child\", e);\n      umbilical.fsError(taskid, e.getMessage());\n    } catch (Exception exception) {\n      LOG.warn(\"Exception running child : \"\n          + StringUtils.stringifyException(exception));\n      try {\n        if (task != null) {\n          // do cleanup for the task\n          if (childUGI == null) { // no need to job into doAs block\n            task.taskCleanup(umbilical);\n          } else {\n            final Task taskFinal = task;\n            childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n              @Override\n              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }\n            });\n          }\n        }\n      } catch (Exception e) {\n        LOG.info(\"Exception cleaning up: \" + StringUtils.stringifyException(e));\n      }\n      // Report back any failures, for diagnostic purposes\n      ByteArrayOutputStream baos = new ByteArrayOutputStream();\n      exception.printStackTrace(new PrintStream(baos));\n      if (taskid != null) {\n        umbilical.fatalError(taskid, baos.toString());\n      }\n    } catch (Throwable throwable) {\n      LOG.fatal(\"Error running child : \"\n    \t        + StringUtils.stringifyException(throwable));\n      if (taskid != null) {\n        Throwable tCause = throwable.getCause();\n        String cause = tCause == null\n                                 ? throwable.getMessage()\n                                 : StringUtils.stringifyException(tCause);\n        umbilical.fatalError(taskid, cause);\n      }\n    } finally {\n      RPC.stopProxy(umbilical);\n      DefaultMetricsSystem.shutdown();\n      // Shutting down log4j of the child-vm...\n      // This assumes that on return from Task.run()\n      // there is no more logging done.\n      LogManager.shutdown();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.loadCredentials": "  private static Token<JobTokenIdentifier> loadCredentials(JobConf conf,\n      InetSocketAddress address) throws IOException {\n    //load token cache storage\n    String tokenFileLocation =\n        System.getenv(ApplicationConstants.CONTAINER_TOKEN_FILE_ENV_NAME);\n    String jobTokenFile =\n        new Path(tokenFileLocation).makeQualified(FileSystem.getLocal(conf))\n            .toUri().getPath();\n    Credentials credentials =\n      TokenCache.loadTokens(jobTokenFile, conf);\n    LOG.debug(\"loading token. # keys =\" +credentials.numberOfSecretKeys() +\n        \"; from file=\" + jobTokenFile);\n    Token<JobTokenIdentifier> jt = TokenCache.getJobToken(credentials);\n    jt.setService(new Text(address.getAddress().getHostAddress() + \":\"\n        + address.getPort()));\n    UserGroupInformation current = UserGroupInformation.getCurrentUser();\n    current.addToken(jt);\n    for (Token<? extends TokenIdentifier> tok : credentials.getAllTokens()) {\n      current.addToken(tok);\n    }\n    // Set the credentials\n    conf.setCredentials(credentials);\n    return jt;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.configureTask": "  private static JobConf configureTask(Task task, Credentials credentials,\n      Token<JobTokenIdentifier> jt) throws IOException {\n    final JobConf job = new JobConf(MRJobConfig.JOB_CONF_FILE);\n    job.setCredentials(credentials);\n    \n    String appAttemptIdEnv = System\n        .getenv(MRJobConfig.APPLICATION_ATTEMPT_ID_ENV);\n    LOG.debug(\"APPLICATION_ATTEMPT_ID: \" + appAttemptIdEnv);\n    // Set it in conf, so as to be able to be used the the OutputCommitter.\n    job.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, Integer\n        .parseInt(appAttemptIdEnv));\n\n    // set tcp nodelay\n    job.setBoolean(\"ipc.client.tcpnodelay\", true);\n    job.setClass(MRConfig.TASK_LOCAL_OUTPUT_CLASS,\n        YarnOutputFiles.class, MapOutputFile.class);\n    // set the jobTokenFile into task\n    task.setJobTokenSecret(\n        JobTokenSecretManager.createSecretKey(jt.getPassword()));\n\n    // setup the child's MRConfig.LOCAL_DIR.\n    configureLocalDirs(task, job);\n\n    // setup the child's attempt directories\n    // Do the task-type specific localization\n    task.localizeConfiguration(job);\n\n    // Set up the DistributedCache related configs\n    setupDistributedCacheConfig(job);\n\n    // Overwrite the localized task jobconf which is linked to in the current\n    // work-dir.\n    Path localTaskFile = new Path(MRJobConfig.JOB_CONF_FILE);\n    writeLocalJobFile(localTaskFile, job);\n    task.setJobFile(localTaskFile.toString());\n    task.setConf(job);\n    return job;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.mapper.map": "  void map(K1 key, V1 value, OutputCollector<K2, V2> output, Reporter reporter)\n  throws IOException;\n}",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.reporter.incrCounter": "  public abstract void incrCounter(String group, String counter, long amount);\n  \n  /**\n   * Get the {@link InputSplit} object for a map.",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getWorkingDirectory": "  public Path getWorkingDirectory() {\n    String name = get(JobContext.WORKING_DIR);\n    if (name != null) {\n      return new Path(name);\n    } else {\n      try {\n        Path dir = FileSystem.get(this).getWorkingDirectory();\n        set(JobContext.WORKING_DIR, dir.toString());\n        return dir;\n      } catch (IOException e) {\n        throw new RuntimeException(e);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.taskCleanup": "  void taskCleanup(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    // set phase for this task\n    setPhase(TaskStatus.Phase.CLEANUP);\n    getProgress().setStatus(\"cleanup\");\n    statusUpdate(umbilical);\n    LOG.info(\"Runnning cleanup for the task\");\n    // do the cleanup\n    committer.abortTask(taskContext);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setPhase": "  protected synchronized void setPhase(TaskStatus.Phase phase){\n    this.taskStatus.setPhase(phase); \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getProgress": "    public float getProgress() {\n      return taskProgress.getProgress();\n    };",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setStatus": "    public void setStatus(String status) {\n      taskProgress.setStatus(status);\n      // indicate that progress update needs to be sent\n      setProgressFlag();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.statusUpdate": "  public void statusUpdate(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    int retries = MAX_RETRIES;\n    while (true) {\n      try {\n        if (!umbilical.statusUpdate(getTaskID(), taskStatus)) {\n          LOG.warn(\"Parent died.  Exiting \"+taskId);\n          System.exit(66);\n        }\n        taskStatus.clearStatus();\n        return;\n      } catch (InterruptedException ie) {\n        Thread.currentThread().interrupt(); // interrupt ourself\n      } catch (IOException ie) {\n        LOG.warn(\"Failure sending status update: \" + \n                  StringUtils.stringifyException(ie));\n        if (--retries == 0) {\n          throw ie;\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.forName": "  public static TaskAttemptID forName(String str\n                                      ) throws IllegalArgumentException {\n    return (TaskAttemptID) \n             org.apache.hadoop.mapreduce.TaskAttemptID.forName(str);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.getTask": "  JvmTask getTask(JvmContext context) throws IOException;\n  \n  /**\n   * Report child's progress to parent.\n   * \n   * @param taskId task-id of the child\n   * @param taskStatus status of the child\n   * @throws IOException\n   * @throws InterruptedException\n   * @return True if the task is known\n   */\n  boolean statusUpdate(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;\n  \n  /** Report error messages back to parent.  Calls should be sparing, since all\n   *  such messages are held in the job tracker.\n   *  @param taskid the id of the task involved\n   *  @param trace the text to report\n   */\n  void reportDiagnosticInfo(TaskAttemptID taskid, String trace) throws IOException;\n  \n  /**\n   * Report the record range which is going to process next by the Task.\n   * @param taskid the id of the task involved\n   * @param range the range of record sequence nos\n   * @throws IOException\n   */\n  void reportNextRecordRange(TaskAttemptID taskid, SortedRanges.Range range) \n    throws IOException;\n\n  /** Periodically called by child to check if parent is still alive. \n   * @return True if the task is known\n   */\n  boolean ping(TaskAttemptID taskid) throws IOException;\n\n  /** Report that the task is successfully completed.  Failure is assumed if\n   * the task process exits without calling this.\n   * @param taskid task's id\n   */\n  void done(TaskAttemptID taskid) throws IOException;\n  \n  /** \n   * Report that the task is complete, but its commit is pending.\n   * \n   * @param taskId task's id\n   * @param taskStatus status of the child\n   * @throws IOException\n   */\n  void commitPending(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;  \n\n  /**\n   * Polling to know whether the task can go-ahead with commit \n   * @param taskid\n   * @return true/false \n   * @throws IOException\n   */\n  boolean canCommit(TaskAttemptID taskid) throws IOException;\n\n  /** Report that a reduce-task couldn't shuffle map-outputs.*/\n  void shuffleError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Report that the task encounted a local filesystem error.*/\n  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getSessionId": "  public String getSessionId() {\n      return get(\"session.id\", \"\");\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fatalError": "  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.getJobID": "  public JobID getJobID() {\n    return (JobID) super.getJobID();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getCredentials": "  public Credentials getCredentials() {\n    return credentials;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.shouldDie": "  public boolean shouldDie() {\n    return shouldDie;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.getTask": "  public Task getTask() {\n    return t;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fsError": "  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.toString": "  public String toString() { \n    return appendTo(new StringBuilder(JVM)).toString();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.appendTo": "  protected StringBuilder appendTo(StringBuilder builder) {\n    return jobId.appendTo(builder).\n                 append(SEPARATOR).\n                 append(isMap ? 'm' : 'r').\n                 append(SEPARATOR).\n                 append(idFormat.format(id));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getTaskID": "  public TaskAttemptID getTaskID() { return taskId; }"
        },
        "bug_report": {
            "Title": "DistCp fails when invoked by Oozie",
            "Description": "When DistCp is invoked through a proxy-user (e.g. through Oozie), the delegation-token-store isn't picked up by DistCp correctly. One sees failures such as:\n\nERROR [main] org.apache.hadoop.tools.DistCp: Couldn't complete DistCp\noperation: \njava.lang.SecurityException: Intercepted System.exit(-999)\n    at\norg.apache.oozie.action.hadoop.LauncherSecurityManager.checkExit(LauncherMapper.java:651)\n    at java.lang.Runtime.exit(Runtime.java:88)\n    at java.lang.System.exit(System.java:904)\n    at org.apache.hadoop.tools.DistCp.main(DistCp.java:357)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at\norg.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:394)\n    at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)\n    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:396)\n    at\norg.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)\n    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)\n\nLooking over the DistCp code, one sees that HADOOP_TOKEN_FILE_LOCATION isn't being copied to mapreduce.job.credentials.binary, in the job-conf. I'll post a patch for this shortly."
        }
    },
    {
        "filename": "HADOOP-10866.json",
        "creation_time": "2014-07-18T22:00:15.000+0000",
        "stack_trace": "```\njava.io.IOException: Path file:/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/linkToFile is not a symbolic link\n\tat org.apache.hadoop.fs.FileStatus.getSymlink(FileStatus.java:266)\n\tat org.apache.hadoop.fs.TestSymlinkLocalFS.testDanglingLink(TestSymlinkLocalFS.java:163)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileStatus.getSymlink": "  public Path getSymlink() throws IOException {\n    if (!isSymlink()) {\n      throw new IOException(\"Path \" + path + \" is not a symbolic link\");\n    }\n    return symlink;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileStatus.isSymlink": "  public boolean isSymlink() {\n    return symlink != null;\n  }"
        },
        "bug_report": {
            "Title": "RawLocalFileSystem fails to read symlink targets via the stat command when the format of stat command uses non-curly quotes",
            "Description": "Symlink tests failure happened from time to time,\n\nhttps://builds.apache.org/job/PreCommit-HDFS-Build/7383//testReport/\nhttps://builds.apache.org/job/PreCommit-HDFS-Build/7376/testReport/\n\n{code}\nFailed\n\norg.apache.hadoop.fs.TestSymlinkLocalFSFileContext.testDanglingLink\n\nFailing for the past 1 build (Since Failed#7376 )\nTook 83 ms.\nError Message\n\nPath file:/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/linkToFile is not a symbolic link\nStacktrace\n\njava.io.IOException: Path file:/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/linkToFile is not a symbolic link\n\tat org.apache.hadoop.fs.FileStatus.getSymlink(FileStatus.java:266)\n\tat org.apache.hadoop.fs.TestSymlinkLocalFS.testDanglingLink(TestSymlinkLocalFS.java:163)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\nStandard Output\n\n2014-07-17 23:31:37,770 WARN  fs.FileUtil (FileUtil.java:symLink(829)) - Command 'ln -s /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/file /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test2/linkToFile' failed 1 with: ln: failed to create symbolic link '/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test2/linkToFile': No such file or directory\n\n2014-07-17 23:31:38,109 WARN  fs.FileUtil (FileUtil.java:symLink(829)) - Command 'ln -s /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/file /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/linkToFile' failed 1 with: ln: failed to create symbolic link '/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/linkToFile': File exists\n{code}\n"
        }
    },
    {
        "filename": "HADOOP-12089.json",
        "creation_time": "2015-06-15T17:34:46.000+0000",
        "stack_trace": "```\norg.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2602)\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2613)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete(NativeAzureFileSystem.java:1505)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete(NativeAzureFileSystem.java:1437)\n\tat org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteFiles(CleanerChore.java:256)\n\tat org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:157)\n\tat org.apache.hadoop.hbase.master.cleaner.CleanerChore.chore(CleanerChore.java:124)\n\tat org.apache.hadoop.hbase.Chore.run(Chore.java:80)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat com.microsoft.azure.storage.StorageException.translateException(StorageException.java:162)\n\tat com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:307)\n\tat com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:177)\n\tat com.microsoft.azure.storage.blob.CloudBlob.uploadProperties(CloudBlob.java:2991)\n\tat org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobWrapperImpl.uploadProperties(StorageInterfaceImpl.java:372)\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2597)```",
        "source_code": {
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime": "  public void updateFolderLastModifiedTime(String key,\n      SelfRenewingLease folderLease) throws AzureException {\n    final Calendar lastModifiedCalendar = Calendar\n        .getInstance(Utility.LOCALE_US);\n    lastModifiedCalendar.setTimeZone(Utility.UTC_ZONE);\n    Date lastModified = lastModifiedCalendar.getTime();\n    updateFolderLastModifiedTime(key, lastModified, folderLease);\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.getBlobReference": "  private CloudBlobWrapper getBlobReference(String aKey)\n      throws StorageException, URISyntaxException {\n\n    CloudBlobWrapper blob = null;\n    if (isPageBlobKey(aKey)) {\n      blob = this.container.getPageBlobReference(aKey);\n    } else {\n      blob = this.container.getBlockBlobReference(aKey);\n    blob.setStreamMinimumReadSizeInBytes(downloadBlockSizeBytes);\n    blob.setWriteBlockSizeInBytes(uploadBlockSizeBytes);\n    }\n\n    return blob;\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.getInstrumentedContext": "  private OperationContext getInstrumentedContext(boolean bindConcurrentOOBIo) {\n\n    OperationContext operationContext = new OperationContext();\n\n    if (selfThrottlingEnabled) {\n      SelfThrottlingIntercept.hook(operationContext, selfThrottlingReadFactor,\n          selfThrottlingWriteFactor);\n    }\n\n    if(bandwidthGaugeUpdater != null) {\n      //bandwidthGaugeUpdater is null when we config to skip azure metrics\n      ResponseReceivedMetricUpdater.hook(\n         operationContext,\n         instrumentation,\n         bandwidthGaugeUpdater);\n    }\n\n    // Bind operation context to receive send request callbacks on this operation.\n    // If reads concurrent to OOB writes are allowed, the interception will reset\n    // the conditional header on all Azure blob storage read requests.\n    if (bindConcurrentOOBIo) {\n      SendRequestIntercept.bind(storageInteractionLayer.getCredentials(),\n          operationContext, true);\n    }\n\n    if (testHookOperationContext != null) {\n      operationContext =\n          testHookOperationContext.modifyOperationContext(operationContext);\n    }\n\n    ErrorMetricUpdater.hook(operationContext, instrumentation);\n\n    // Return the operation context.\n    return operationContext;\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.checkContainer": "  private ContainerState checkContainer(ContainerAccessType accessType)\n      throws StorageException, AzureException {\n    synchronized (containerStateLock) {\n      if (isOkContainerState(accessType)) {\n        return currentKnownContainerState;\n      }\n      if (currentKnownContainerState == ContainerState.ExistsAtWrongVersion) {\n        String containerVersion = retrieveVersionAttribute(container);\n        throw wrongVersionException(containerVersion);\n      }\n      // This means I didn't check it before or it didn't exist or\n      // we need to stamp the version. Since things may have changed by\n      // other machines since then, do the check again and don't depend\n      // on past information.\n\n      // Sanity check: we don't expect this at this point.\n      if (currentKnownContainerState == ContainerState.ExistsAtRightVersion) {\n        throw new AssertionError(\"Unexpected state: \"\n            + currentKnownContainerState);\n      }\n\n      // Download the attributes - doubles as an existence check with just\n      // one service call\n      try {\n        container.downloadAttributes(getInstrumentedContext());\n        currentKnownContainerState = ContainerState.Unknown;\n      } catch (StorageException ex) {\n        if (ex.getErrorCode().equals(\n            StorageErrorCode.RESOURCE_NOT_FOUND.toString())) {\n          currentKnownContainerState = ContainerState.DoesntExist;\n        } else {\n          throw ex;\n        }\n      }\n\n      if (currentKnownContainerState == ContainerState.DoesntExist) {\n        // If the container doesn't exist and we intend to write to it,\n        // create it now.\n        if (needToCreateContainer(accessType)) {\n          storeVersionAttribute(container);\n          container.create(getInstrumentedContext());\n          currentKnownContainerState = ContainerState.ExistsAtRightVersion;\n        }\n      } else {\n        // The container exists, check the version.\n        String containerVersion = retrieveVersionAttribute(container);\n        if (containerVersion != null) {\n          if (containerVersion.equals(FIRST_WASB_VERSION)) {\n            // It's the version from when WASB was called ASV, just\n            // fix the version attribute if needed and proceed.\n            // We should be good otherwise.\n            if (needToStampVersion(accessType)) {\n              storeVersionAttribute(container);\n              container.uploadMetadata(getInstrumentedContext());\n            }\n          } else if (!containerVersion.equals(CURRENT_WASB_VERSION)) {\n            // Don't know this version - throw.\n            currentKnownContainerState = ContainerState.ExistsAtWrongVersion;\n            throw wrongVersionException(containerVersion);\n          } else {\n            // It's our correct version.\n            currentKnownContainerState = ContainerState.ExistsAtRightVersion;\n          }\n        } else {\n          // No version info exists.\n          currentKnownContainerState = ContainerState.ExistsNoVersion;\n          if (needToStampVersion(accessType)) {\n            // Need to stamp the version\n            storeVersionAttribute(container);\n            container.uploadMetadata(getInstrumentedContext());\n            currentKnownContainerState = ContainerState.ExistsAtRightVersion;\n          }\n        }\n      }\n      return currentKnownContainerState;\n    }\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete": "  public boolean delete(Path f, boolean recursive,\n      boolean skipParentFolderLastModifidedTimeUpdate) throws IOException {\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Deleting file: \" + f.toString());\n    }\n\n    Path absolutePath = makeAbsolute(f);\n    String key = pathToKey(absolutePath);\n\n    // Capture the metadata for the path.\n    //\n    FileMetadata metaFile = store.retrieveMetadata(key);\n\n    if (null == metaFile) {\n      // The path to be deleted does not exist.\n      return false;\n    }\n\n    // The path exists, determine if it is a folder containing objects,\n    // an empty folder, or a simple file and take the appropriate actions.\n    if (!metaFile.isDir()) {\n      // The path specifies a file. We need to check the parent path\n      // to make sure it's a proper materialized directory before we\n      // delete the file. Otherwise we may get into a situation where\n      // the file we were deleting was the last one in an implicit directory\n      // (e.g. the blob store only contains the blob a/b and there's no\n      // corresponding directory blob a) and that would implicitly delete\n      // the directory as well, which is not correct.\n      Path parentPath = absolutePath.getParent();\n      if (parentPath.getParent() != null) {// Not root\n        String parentKey = pathToKey(parentPath);\n        FileMetadata parentMetadata = store.retrieveMetadata(parentKey);\n        if (!parentMetadata.isDir()) {\n          // Invalid state: the parent path is actually a file. Throw.\n          throw new AzureException(\"File \" + f + \" has a parent directory \"\n              + parentPath + \" which is also a file. Can't resolve.\");\n        }\n        if (parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Found an implicit parent directory while trying to\"\n                + \" delete the file \" + f + \". Creating the directory blob for\"\n                + \" it in \" + parentKey + \".\");\n          }\n          store.storeEmptyFolder(parentKey,\n              createPermissionStatus(FsPermission.getDefault()));\n        } else {\n          if (!skipParentFolderLastModifidedTimeUpdate) {\n            store.updateFolderLastModifiedTime(parentKey, null);\n          }\n        }\n      }\n      store.delete(key);\n      instrumentation.fileDeleted();\n    } else {\n      // The path specifies a folder. Recursively delete all entries under the\n      // folder.\n      Path parentPath = absolutePath.getParent();\n      if (parentPath.getParent() != null) {\n        String parentKey = pathToKey(parentPath);\n        FileMetadata parentMetadata = store.retrieveMetadata(parentKey);\n\n        if (parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Found an implicit parent directory while trying to\"\n                + \" delete the directory \" + f\n                + \". Creating the directory blob for\" + \" it in \" + parentKey\n                + \".\");\n          }\n          store.storeEmptyFolder(parentKey,\n              createPermissionStatus(FsPermission.getDefault()));\n        }\n      }\n\n      // List all the blobs in the current folder.\n      String priorLastKey = null;\n      PartialListing listing = store.listAll(key, AZURE_LIST_ALL, 1,\n          priorLastKey);\n      FileMetadata[] contents = listing.getFiles();\n      if (!recursive && contents.length > 0) {\n        // The folder is non-empty and recursive delete was not specified.\n        // Throw an exception indicating that a non-recursive delete was\n        // specified for a non-empty folder.\n        throw new IOException(\"Non-recursive delete of non-empty directory \"\n            + f.toString());\n      }\n\n      // Delete all the files in the folder.\n      for (FileMetadata p : contents) {\n        // Tag on the directory name found as the suffix of the suffix of the\n        // parent directory to get the new absolute path.\n        String suffix = p.getKey().substring(\n            p.getKey().lastIndexOf(PATH_DELIMITER));\n        if (!p.isDir()) {\n          store.delete(key + suffix);\n          instrumentation.fileDeleted();\n        } else {\n          // Recursively delete contents of the sub-folders. Notice this also\n          // deletes the blob for the directory.\n          if (!delete(new Path(f.toString() + suffix), true)) {\n            return false;\n          }\n        }\n      }\n      store.delete(key);\n\n      // Update parent directory last modified time\n      Path parent = absolutePath.getParent();\n      if (parent != null && parent.getParent() != null) { // not root\n        String parentKey = pathToKey(parent);\n        if (!skipParentFolderLastModifidedTimeUpdate) {\n          store.updateFolderLastModifiedTime(parentKey, null);\n        }\n      }\n      instrumentation.directoryDeleted();\n    }\n\n    // File or directory was successfully deleted.\n    return true;\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.pathToKey": "  public String pathToKey(Path path) {\n    // Convert the path to a URI to parse the scheme, the authority, and the\n    // path from the path object.\n    URI tmpUri = path.toUri();\n    String pathUri = tmpUri.getPath();\n\n    // The scheme and authority is valid. If the path does not exist add a \"/\"\n    // separator to list the root of the container.\n    Path newPath = path;\n    if (\"\".equals(pathUri)) {\n      newPath = new Path(tmpUri.toString() + Path.SEPARATOR);\n    }\n\n    // Verify path is absolute if the path refers to a windows drive scheme.\n    if (!newPath.isAbsolute()) {\n      throw new IllegalArgumentException(\"Path must be absolute: \" + path);\n    }\n\n    String key = null;\n    key = newPath.toUri().getPath();\n    key = removeTrailingSlash(key);\n    key = encodeTrailingPeriod(key);\n    if (key.length() == 1) {\n      return key;\n    } else {\n      return key.substring(1); // remove initial slash\n    }\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.getKey": "    public String getKey() {\n      return key;\n    }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.makeAbsolute": "  public Path makeAbsolute(Path path) {\n    if (path.isAbsolute()) {\n      return path;\n    }\n    return new Path(workingDir, path);\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.getFiles": "    public FileMetadata[] getFiles() {\n      return fileMetadata;\n    }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.createPermissionStatus": "  private PermissionStatus createPermissionStatus(FsPermission permission)\n      throws IOException {\n    // Create the permission status for this file based on current user\n    return new PermissionStatus(\n        UserGroupInformation.getCurrentUser().getShortUserName(),\n        getConf().get(AZURE_DEFAULT_GROUP_PROPERTY_NAME,\n            AZURE_DEFAULT_GROUP_DEFAULT),\n        permission);\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.StorageInterfaceImpl.uploadProperties": "    public void uploadProperties(OperationContext opContext)\n        throws StorageException {\n      getBlob().uploadProperties(null, null, opContext);\n    }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.StorageInterfaceImpl.getBlob": "    public CloudBlob getBlob() {\n      return blob;\n    }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.StorageInterfaceImpl.getLeaseCondition": "    private AccessCondition getLeaseCondition(SelfRenewingLease lease) {\n      AccessCondition leaseCondition = null;\n      if (lease != null) {\n        leaseCondition = AccessCondition.generateLeaseCondition(lease.getLeaseID());\n      }\n      return leaseCondition;\n    }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.FileMetadata.isDir": "  public boolean isDir() {\n    return isDir;\n  }",
            "hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.FileMetadata.getBlobMaterialization": "  public BlobMaterialization getBlobMaterialization() {\n    return blobMaterialization;\n  }"
        },
        "bug_report": {
            "Title": "StorageException complaining \" no lease ID\" when updating FolderLastModifiedTime in WASB",
            "Description": "This is a similar issue to HADOOP-11523. HADOOP-11523 happens when HBase is doing distributed log splitting. This JIRA happens when HBase is deleting old WALs and trying to update /hbase/oldWALs folder.\n\nThe fix is the same as HADOOP-11523.\n\n{code}\n2015-06-10 08:11:40,636 WARN org.apache.hadoop.hbase.master.cleaner.CleanerChore: Error while deleting: wasb://basecus1-1@basestoragecus1.blob.core.windows.net/hbase/oldWALs/workernode10.dthbasecus1.g1.internal.cloudapp.net%2C60020%2C1433908062461.1433921692855\norg.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2602)\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2613)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete(NativeAzureFileSystem.java:1505)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete(NativeAzureFileSystem.java:1437)\n\tat org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteFiles(CleanerChore.java:256)\n\tat org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:157)\n\tat org.apache.hadoop.hbase.master.cleaner.CleanerChore.chore(CleanerChore.java:124)\n\tat org.apache.hadoop.hbase.Chore.run(Chore.java:80)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat com.microsoft.azure.storage.StorageException.translateException(StorageException.java:162)\n\tat com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:307)\n\tat com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:177)\n\tat com.microsoft.azure.storage.blob.CloudBlob.uploadProperties(CloudBlob.java:2991)\n\tat org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobWrapperImpl.uploadProperties(StorageInterfaceImpl.java:372)\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2597)\n\t... 8 more\n{code}"
        }
    },
    {
        "filename": "HADOOP-11934.json",
        "creation_time": "2015-05-07T00:38:53.000+0000",
        "stack_trace": "```\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:88)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:65)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:291)\n\tat org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:58)\n\tat org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:1863)\n\tat org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:1843)\n\tat org.apache.hadoop.security.LdapGroupsMapping.getPassword(LdapGroupsMapping.java:386)\n\tat org.apache.hadoop.security.LdapGroupsMapping.setConf(LdapGroupsMapping.java:349)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n\tat org.apache.hadoop.security.Groups.<init>(Groups.java:70)\n\tat org.apache.hadoop.security.Groups.<init>(Groups.java:66)\n\tat org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)\n\tat org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)\n\tat org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)\n\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:804)\n\tat org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)\n\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2753)\n\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2745)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:88)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:65)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:291)\n\tat org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:58)\n\tat org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:1863)\n\tat org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:1843)\n\tat org.apache.hadoop.security.LdapGroupsMapping.getPassword(LdapGroupsMapping.java:386)\n\tat org.apache.hadoop.security.LdapGroupsMapping.setConf(LdapGroupsMapping.java:349)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n\tat org.apache.hadoop.security.Groups.<init>(Groups.java:70)\n\tat org.apache.hadoop.security.Groups.<init>(Groups.java:66)\n\tat org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)\n\tat org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)\n\tat org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)\n\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:804)\n\tat org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)\n\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2753)\n\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2745)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.get": "    FileSystem get(URI uri, Configuration conf) throws IOException{\n      Key key = new Key(uri, conf);\n      return getInternal(uri, conf, key);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getScheme": "    public String getScheme() {\n      return scheme;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getInternal": "    private FileSystem getInternal(URI uri, Configuration conf, Key key) throws IOException{\n      FileSystem fs;\n      synchronized (this) {\n        fs = map.get(key);\n      }\n      if (fs != null) {\n        return fs;\n      }\n\n      fs = createFileSystem(uri, conf);\n      synchronized (this) { // refetch the lock again\n        FileSystem oldfs = map.get(key);\n        if (oldfs != null) { // a file system is created while lock is releasing\n          fs.close(); // close the new file system\n          return oldfs;  // return the old file system\n        }\n        \n        // now insert the new file system into the map\n        if (map.isEmpty()\n                && !ShutdownHookManager.get().isShutdownInProgress()) {\n          ShutdownHookManager.get().addShutdownHook(clientFinalizer, SHUTDOWN_HOOK_PRIORITY);\n        }\n        fs.key = key;\n        map.put(key, fs);\n        if (conf.getBoolean(\"fs.automatic.close\", true)) {\n          toAutoClose.add(key);\n        }\n        return fs;\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.createFileSystem": "  private static FileSystem createFileSystem(URI uri, Configuration conf\n      ) throws IOException {\n    Class<?> clazz = getFileSystemClass(uri.getScheme(), conf);\n    FileSystem fs = (FileSystem)ReflectionUtils.newInstance(clazz, conf);\n    fs.initialize(uri, conf);\n    return fs;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.equals": "      public boolean equals(Object obj) {\n        if (obj == this) {\n          return true;\n        }\n        if (obj != null && obj instanceof Key) {\n          Key that = (Key)obj;\n          return isEqual(this.scheme, that.scheme)\n                 && isEqual(this.authority, that.authority)\n                 && isEqual(this.ugi, that.ugi)\n                 && (this.unique == that.unique);\n        }\n        return false;        \n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getDefaultUri": "  public static URI getDefaultUri(Configuration conf) {\n    return URI.create(fixName(conf.get(FS_DEFAULT_NAME_KEY, DEFAULT_FS)));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Path.getFileSystem": "  public FileSystem getFileSystem(Configuration conf) throws IOException {\n    return FileSystem.get(this.toUri(), conf);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Path.toUri": "  public URI toUri() { return uri; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.JavaKeyStoreProvider.createProvider": "    public KeyProvider createProvider(URI providerName,\n                                      Configuration conf) throws IOException {\n      if (SCHEME_NAME.equals(providerName.getScheme())) {\n        return new JavaKeyStoreProvider(providerName, conf);\n      }\n      return null;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders": "  public static List<CredentialProvider> getProviders(Configuration conf\n                                               ) throws IOException {\n    List<CredentialProvider> result = new ArrayList<CredentialProvider>();\n    for(String path: conf.getStringCollection(CREDENTIAL_PROVIDER_PATH)) {\n      try {\n        URI uri = new URI(path);\n        boolean found = false;\n        for(CredentialProviderFactory factory: serviceLoader) {\n          CredentialProvider kp = factory.createProvider(uri, conf);\n          if (kp != null) {\n            result.add(kp);\n            found = true;\n            break;\n          }\n        }\n        if (!found) {\n          throw new IOException(\"No CredentialProviderFactory for \" + uri + \" in \" +\n              CREDENTIAL_PROVIDER_PATH);\n        }\n      } catch (URISyntaxException error) {\n        throw new IOException(\"Bad configuration of \" + CREDENTIAL_PROVIDER_PATH +\n            \" at \" + path, error);\n      }\n    }\n    return result;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.alias.CredentialProviderFactory.createProvider": "  public abstract CredentialProvider createProvider(URI providerName,\n                                             Configuration conf\n                                             ) throws IOException;\n\n  private static final ServiceLoader<CredentialProviderFactory> serviceLoader =\n      ServiceLoader.load(CredentialProviderFactory.class);\n\n  public static List<CredentialProvider> getProviders(Configuration conf\n                                               ) throws IOException {\n    List<CredentialProvider> result = new ArrayList<CredentialProvider>();\n    for(String path: conf.getStringCollection(CREDENTIAL_PROVIDER_PATH)) {\n      try {\n        URI uri = new URI(path);\n        boolean found = false;\n        for(CredentialProviderFactory factory: serviceLoader) {\n          CredentialProvider kp = factory.createProvider(uri, conf);\n          if (kp != null) {\n            result.add(kp);\n            found = true;\n            break;\n          }\n        }\n        if (!found) {\n          throw new IOException(\"No CredentialProviderFactory for \" + uri + \" in \" +\n              CREDENTIAL_PROVIDER_PATH);\n        }\n      } catch (URISyntaxException error) {\n        throw new IOException(\"Bad configuration of \" + CREDENTIAL_PROVIDER_PATH +\n            \" at \" + path, error);\n      }\n    }\n    return result;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders": "  protected char[] getPasswordFromCredentialProviders(String name)\n      throws IOException {\n    char[] pass = null;\n    try {\n      List<CredentialProvider> providers =\n          CredentialProviderFactory.getProviders(this);\n\n      if (providers != null) {\n        for (CredentialProvider provider : providers) {\n          try {\n            CredentialEntry entry = provider.getCredentialEntry(name);\n            if (entry != null) {\n              pass = entry.getCredential();\n              break;\n            }\n          }\n          catch (IOException ioe) {\n            throw new IOException(\"Can't get key \" + name + \" from key provider\" +\n            \t\t\"of type: \" + provider.getClass().getName() + \".\", ioe);\n          }\n        }\n      }\n    }\n    catch (IOException ioe) {\n      throw new IOException(\"Configuration problem with provider path.\", ioe);\n    }\n\n    return pass;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getClass": "  public <U> Class<? extends U> getClass(String name, \n                                         Class<? extends U> defaultValue, \n                                         Class<U> xface) {\n    try {\n      Class<?> theClass = getClass(name, defaultValue);\n      if (theClass != null && !xface.isAssignableFrom(theClass))\n        throw new RuntimeException(theClass+\" not \"+xface.getName());\n      else if (theClass != null)\n        return theClass.asSubclass(xface);\n      else\n        return null;\n    } catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getName": "    public String getName(){\n      return name;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getPassword": "  public char[] getPassword(String name) throws IOException {\n    char[] pass = null;\n\n    pass = getPasswordFromCredentialProviders(name);\n\n    if (pass == null) {\n      pass = getPasswordFromConfig(name);\n    }\n\n    return pass;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getPasswordFromConfig": "  protected char[] getPasswordFromConfig(String name) {\n    char[] pass = null;\n    if (getBoolean(CredentialProvider.CLEAR_TEXT_FALLBACK, true)) {\n      String passStr = get(name);\n      if (passStr != null) {\n        pass = passStr.toCharArray();\n      }\n    }\n    return pass;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.LdapGroupsMapping.getPassword": "  String getPassword(Configuration conf, String alias, String defaultPass) {\n    String password = null;\n    try {\n      char[] passchars = conf.getPassword(alias);\n      if (passchars != null) {\n        password = new String(passchars);\n      }\n      else {\n        password = defaultPass;\n      }\n    }\n    catch (IOException ioe) {\n      LOG.warn(\"Exception while trying to password for alias \" + alias + \": \"\n          + ioe.getMessage());\n    }\n    return password;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.LdapGroupsMapping.setConf": "  public synchronized void setConf(Configuration conf) {\n    ldapUrl = conf.get(LDAP_URL_KEY, LDAP_URL_DEFAULT);\n    if (ldapUrl == null || ldapUrl.isEmpty()) {\n      throw new RuntimeException(\"LDAP URL is not configured\");\n    }\n    \n    useSsl = conf.getBoolean(LDAP_USE_SSL_KEY, LDAP_USE_SSL_DEFAULT);\n    keystore = conf.get(LDAP_KEYSTORE_KEY, LDAP_KEYSTORE_DEFAULT);\n    \n    keystorePass = getPassword(conf, LDAP_KEYSTORE_PASSWORD_KEY,\n        LDAP_KEYSTORE_PASSWORD_DEFAULT);\n    if (keystorePass.isEmpty()) {\n      keystorePass = extractPassword(conf.get(LDAP_KEYSTORE_PASSWORD_FILE_KEY,\n          LDAP_KEYSTORE_PASSWORD_FILE_DEFAULT));\n    }\n    \n    bindUser = conf.get(BIND_USER_KEY, BIND_USER_DEFAULT);\n    bindPassword = getPassword(conf, BIND_PASSWORD_KEY, BIND_PASSWORD_DEFAULT);\n    if (bindPassword.isEmpty()) {\n      bindPassword = extractPassword(\n          conf.get(BIND_PASSWORD_FILE_KEY, BIND_PASSWORD_FILE_DEFAULT));\n    }\n    \n    baseDN = conf.get(BASE_DN_KEY, BASE_DN_DEFAULT);\n    groupSearchFilter =\n        conf.get(GROUP_SEARCH_FILTER_KEY, GROUP_SEARCH_FILTER_DEFAULT);\n    userSearchFilter =\n        conf.get(USER_SEARCH_FILTER_KEY, USER_SEARCH_FILTER_DEFAULT);\n    isPosix = groupSearchFilter.contains(POSIX_GROUP) && userSearchFilter\n        .contains(POSIX_ACCOUNT);\n    groupMemberAttr =\n        conf.get(GROUP_MEMBERSHIP_ATTR_KEY, GROUP_MEMBERSHIP_ATTR_DEFAULT);\n    groupNameAttr =\n        conf.get(GROUP_NAME_ATTR_KEY, GROUP_NAME_ATTR_DEFAULT);\n\n    int dirSearchTimeout = conf.getInt(DIRECTORY_SEARCH_TIMEOUT, DIRECTORY_SEARCH_TIMEOUT_DEFAULT);\n    SEARCH_CONTROLS.setTimeLimit(dirSearchTimeout);\n    // Limit the attributes returned to only those required to speed up the search. See HADOOP-10626 for more details.\n    SEARCH_CONTROLS.setReturningAttributes(new String[] {groupNameAttr});\n\n    this.conf = conf;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.LdapGroupsMapping.extractPassword": "  String extractPassword(String pwFile) {\n    if (pwFile.isEmpty()) {\n      // If there is no password file defined, we'll assume that we should do\n      // an anonymous bind\n      return \"\";\n    }\n\n    StringBuilder password = new StringBuilder();\n    try (Reader reader = new InputStreamReader(\n        new FileInputStream(pwFile), Charsets.UTF_8)) {\n      int c = reader.read();\n      while (c > -1) {\n        password.append((char)c);\n        c = reader.read();\n      }\n      return password.toString().trim();\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Could not read password file: \" + pwFile, ioe);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ReflectionUtils.setConf": "  public static void setConf(Object theObject, Configuration conf) {\n    if (conf != null) {\n      if (theObject instanceof Configurable) {\n        ((Configurable) theObject).setConf(conf);\n      }\n      setJobConf(theObject, conf);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ReflectionUtils.setJobConf": "  private static void setJobConf(Object theObject, Configuration conf) {\n    //If JobConf and JobConfigurable are in classpath, AND\n    //theObject is of type JobConfigurable AND\n    //conf is of type JobConf then\n    //invoke configure on theObject\n    try {\n      Class<?> jobConfClass = \n        conf.getClassByNameOrNull(\"org.apache.hadoop.mapred.JobConf\");\n      if (jobConfClass == null) {\n        return;\n      }\n      \n      Class<?> jobConfigurableClass = \n        conf.getClassByNameOrNull(\"org.apache.hadoop.mapred.JobConfigurable\");\n      if (jobConfigurableClass == null) {\n        return;\n      }\n      if (jobConfClass.isAssignableFrom(conf.getClass()) &&\n            jobConfigurableClass.isAssignableFrom(theObject.getClass())) {\n        Method configureMethod = \n          jobConfigurableClass.getMethod(\"configure\", jobConfClass);\n        configureMethod.invoke(theObject, conf);\n      }\n    } catch (Exception e) {\n      throw new RuntimeException(\"Error in configuring object\", e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ReflectionUtils.newInstance": "  public static <T> T newInstance(Class<T> theClass, Configuration conf) {\n    T result;\n    try {\n      Constructor<T> meth = (Constructor<T>) CONSTRUCTOR_CACHE.get(theClass);\n      if (meth == null) {\n        meth = theClass.getDeclaredConstructor(EMPTY_ARRAY);\n        meth.setAccessible(true);\n        CONSTRUCTOR_CACHE.put(theClass, meth);\n      }\n      result = meth.newInstance();\n    } catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n    setConf(result, conf);\n    return result;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.Groups.getUserToGroupsMappingService": "  public static synchronized Groups getUserToGroupsMappingService(\n    Configuration conf) {\n\n    if(GROUPS == null) {\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\" Creating new Groups object\");\n      }\n      GROUPS = new Groups(conf);\n    }\n    return GROUPS;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.initialize": "  private static synchronized void initialize(Configuration conf,\n                                              boolean overrideNameRules) {\n    authenticationMethod = SecurityUtil.getAuthenticationMethod(conf);\n    if (overrideNameRules || !HadoopKerberosName.hasRulesBeenSet()) {\n      try {\n        HadoopKerberosName.setConfiguration(conf);\n      } catch (IOException ioe) {\n        throw new RuntimeException(\n            \"Problem with Kerberos auth_to_local name configuration\", ioe);\n      }\n    }\n    try {\n        kerberosMinSecondsBeforeRelogin = 1000L * conf.getLong(\n                HADOOP_KERBEROS_MIN_SECONDS_BEFORE_RELOGIN,\n                HADOOP_KERBEROS_MIN_SECONDS_BEFORE_RELOGIN_DEFAULT);\n    }\n    catch(NumberFormatException nfe) {\n        throw new IllegalArgumentException(\"Invalid attribute value for \" +\n                HADOOP_KERBEROS_MIN_SECONDS_BEFORE_RELOGIN + \" of \" +\n                conf.get(HADOOP_KERBEROS_MIN_SECONDS_BEFORE_RELOGIN));\n    }\n    // If we haven't set up testing groups, use the configuration to find it\n    if (!(groups instanceof TestingGroups)) {\n      groups = Groups.getUserToGroupsMappingService(conf);\n    }\n    UserGroupInformation.conf = conf;\n\n    if (metrics.getGroupsQuantiles == null) {\n      int[] intervals = conf.getInts(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS);\n      if (intervals != null && intervals.length > 0) {\n        final int length = intervals.length;\n        MutableQuantiles[] getGroupsQuantiles = new MutableQuantiles[length];\n        for (int i = 0; i < length; i++) {\n          getGroupsQuantiles[i] = metrics.registry.newQuantiles(\n            \"getGroups\" + intervals[i] + \"s\",\n            \"Get groups\", \"ops\", \"latency\", intervals[i]);\n        }\n        metrics.getGroupsQuantiles = getGroupsQuantiles;\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getAuthenticationMethod": "  public synchronized AuthenticationMethod getAuthenticationMethod() {\n    return user.getAuthenticationMethod();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.setConfiguration": "  public static void setConfiguration(Configuration conf) {\n    initialize(conf, true);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.ensureInitialized": "  private static void ensureInitialized() {\n    if (conf == null) {\n      synchronized(UserGroupInformation.class) {\n        if (conf == null) { // someone might have beat us\n          initialize(new Configuration(), false);\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject": "  static void loginUserFromSubject(Subject subject) throws IOException {\n    ensureInitialized();\n    try {\n      if (subject == null) {\n        subject = new Subject();\n      }\n      LoginContext login =\n          newLoginContext(authenticationMethod.getLoginAppName(), \n                          subject, new HadoopConfiguration());\n      login.login();\n      UserGroupInformation realUser = new UserGroupInformation(subject);\n      realUser.setLogin(login);\n      realUser.setAuthenticationMethod(authenticationMethod);\n      realUser = new UserGroupInformation(login.getSubject());\n      // If the HADOOP_PROXY_USER environment variable or property\n      // is specified, create a proxy user as the logged in user.\n      String proxyUser = System.getenv(HADOOP_PROXY_USER);\n      if (proxyUser == null) {\n        proxyUser = System.getProperty(HADOOP_PROXY_USER);\n      }\n      loginUser = proxyUser == null ? realUser : createProxyUser(proxyUser, realUser);\n\n      String fileLocation = System.getenv(HADOOP_TOKEN_FILE_LOCATION);\n      if (fileLocation != null) {\n        // Load the token storage file and put all of the tokens into the\n        // user. Don't use the FileSystem API for reading since it has a lock\n        // cycle (HADOOP-9212).\n        Credentials cred = Credentials.readTokenStorageFile(\n            new File(fileLocation), conf);\n        loginUser.addCredentials(cred);\n      }\n      loginUser.spawnAutoRenewalThreadForUserCreds();\n    } catch (LoginException le) {\n      LOG.debug(\"failure to login\", le);\n      throw new IOException(\"failure to login\", le);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"UGI loginUser:\"+loginUser);\n    } \n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.login": "    public boolean login() throws LoginException {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"hadoop login\");\n      }\n      return true;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.newLoginContext": "  private static LoginContext\n  newLoginContext(String appName, Subject subject,\n    javax.security.auth.login.Configuration loginConf)\n      throws LoginException {\n    // Temporarily switch the thread's ContextClassLoader to match this\n    // class's classloader, so that we can properly load HadoopLoginModule\n    // from the JAAS libraries.\n    Thread t = Thread.currentThread();\n    ClassLoader oldCCL = t.getContextClassLoader();\n    t.setContextClassLoader(HadoopLoginModule.class.getClassLoader());\n    try {\n      return new LoginContext(appName, subject, null, loginConf);\n    } finally {\n      t.setContextClassLoader(oldCCL);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.addCredentials": "  public void addCredentials(Credentials credentials) {\n    synchronized (subject) {\n      getCredentialsInternal().addAll(credentials);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.setAuthenticationMethod": "  public void setAuthenticationMethod(AuthMethod authMethod) {\n    user.setAuthenticationMethod(AuthenticationMethod.valueOf(authMethod));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.createProxyUser": "  public static UserGroupInformation createProxyUser(String user,\n      UserGroupInformation realUser) {\n    if (user == null || user.isEmpty()) {\n      throw new IllegalArgumentException(\"Null user\");\n    }\n    if (realUser == null) {\n      throw new IllegalArgumentException(\"Null real user\");\n    }\n    Subject subject = new Subject();\n    Set<Principal> principals = subject.getPrincipals();\n    principals.add(new User(user));\n    principals.add(new RealUser(realUser));\n    UserGroupInformation result =new UserGroupInformation(subject);\n    result.setAuthenticationMethod(AuthenticationMethod.PROXY);\n    return result;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.spawnAutoRenewalThreadForUserCreds": "  private void spawnAutoRenewalThreadForUserCreds() {\n    if (isSecurityEnabled()) {\n      //spawn thread only if we have kerb credentials\n      if (user.getAuthenticationMethod() == AuthenticationMethod.KERBEROS &&\n          !isKeytab) {\n        Thread t = new Thread(new Runnable() {\n          \n          @Override\n          public void run() {\n            String cmd = conf.get(\"hadoop.kerberos.kinit.command\",\n                                  \"kinit\");\n            KerberosTicket tgt = getTGT();\n            if (tgt == null) {\n              return;\n            }\n            long nextRefresh = getRefreshTime(tgt);\n            while (true) {\n              try {\n                long now = Time.now();\n                if(LOG.isDebugEnabled()) {\n                  LOG.debug(\"Current time is \" + now);\n                  LOG.debug(\"Next refresh is \" + nextRefresh);\n                }\n                if (now < nextRefresh) {\n                  Thread.sleep(nextRefresh - now);\n                }\n                Shell.execCommand(cmd, \"-R\");\n                if(LOG.isDebugEnabled()) {\n                  LOG.debug(\"renewed ticket\");\n                }\n                reloginFromTicketCache();\n                tgt = getTGT();\n                if (tgt == null) {\n                  LOG.warn(\"No TGT after renewal. Aborting renew thread for \" +\n                           getUserName());\n                  return;\n                }\n                nextRefresh = Math.max(getRefreshTime(tgt),\n                                       now + kerberosMinSecondsBeforeRelogin);\n              } catch (InterruptedException ie) {\n                LOG.warn(\"Terminating renewal thread\");\n                return;\n              } catch (IOException ie) {\n                LOG.warn(\"Exception encountered while running the\" +\n                    \" renewal command. Aborting renew thread. \" + ie);\n                return;\n              }\n            }\n          }\n        });\n        t.setDaemon(true);\n        t.setName(\"TGT Renewer for \" + getUserName());\n        t.start();\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.setLogin": "  private void setLogin(LoginContext login) {\n    user.setLogin(login);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getSubject": "  protected Subject getSubject() {\n    return subject;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getLoginAppName": "    String getLoginAppName() {\n      if (loginAppName == null) {\n        throw new UnsupportedOperationException(\n            this + \" login authentication is not supported\");\n      }\n      return loginAppName;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getLoginUser": "  static UserGroupInformation getLoginUser() throws IOException {\n    if (loginUser == null) {\n      loginUserFromSubject(null);\n    }\n    return loginUser;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getCurrentUser": "  static UserGroupInformation getCurrentUser() throws IOException {\n    AccessControlContext context = AccessController.getContext();\n    Subject subject = Subject.getSubject(context);\n    if (subject == null || subject.getPrincipals(User.class).isEmpty()) {\n      return getLoginUser();\n    } else {\n      return new UserGroupInformation(subject);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.Credentials.readTokenStorageFile": "  public static Credentials readTokenStorageFile(File filename, Configuration conf)\n      throws IOException {\n    DataInputStream in = null;\n    Credentials credentials = new Credentials();\n    try {\n      in = new DataInputStream(new BufferedInputStream(\n          new FileInputStream(filename)));\n      credentials.readTokenStorageStream(in);\n      return credentials;\n    } catch(IOException ioe) {\n      throw new IOException(\"Exception reading \" + filename, ioe);\n    } finally {\n      IOUtils.cleanup(LOG, in);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.Credentials.readTokenStorageStream": "  public void readTokenStorageStream(DataInputStream in) throws IOException {\n    byte[] magic = new byte[TOKEN_STORAGE_MAGIC.length];\n    in.readFully(magic);\n    if (!Arrays.equals(magic, TOKEN_STORAGE_MAGIC)) {\n      throw new IOException(\"Bad header found in token storage.\");\n    }\n    byte version = in.readByte();\n    if (version != TOKEN_STORAGE_VERSION) {\n      throw new IOException(\"Unknown version \" + version + \n                            \" in token storage.\");\n    }\n    readFields(in);\n  }"
        },
        "bug_report": {
            "Title": "Use of JavaKeyStoreProvider in LdapGroupsMapping causes infinite loop",
            "Description": "I was attempting to use the LdapGroupsMapping code and the JavaKeyStoreProvider at the same time, and hit a really interesting, yet fatal, issue.  The code goes into what ought to have been an infinite loop, were it not for it overflowing the stack and Java ending the loop.  Here is a snippet of the stack; my annotations are at the bottom.\n\n{noformat}\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:88)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:65)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:291)\n\tat org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:58)\n\tat org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:1863)\n\tat org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:1843)\n\tat org.apache.hadoop.security.LdapGroupsMapping.getPassword(LdapGroupsMapping.java:386)\n\tat org.apache.hadoop.security.LdapGroupsMapping.setConf(LdapGroupsMapping.java:349)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n\tat org.apache.hadoop.security.Groups.<init>(Groups.java:70)\n\tat org.apache.hadoop.security.Groups.<init>(Groups.java:66)\n\tat org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)\n\tat org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)\n\tat org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)\n\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:804)\n\tat org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)\n\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2753)\n\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2745)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:88)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:65)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:291)\n\tat org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:58)\n\tat org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:1863)\n\tat org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:1843)\n\tat org.apache.hadoop.security.LdapGroupsMapping.getPassword(LdapGroupsMapping.java:386)\n\tat org.apache.hadoop.security.LdapGroupsMapping.setConf(LdapGroupsMapping.java:349)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n\tat org.apache.hadoop.security.Groups.<init>(Groups.java:70)\n\tat org.apache.hadoop.security.Groups.<init>(Groups.java:66)\n\tat org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)\n\tat org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)\n\tat org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)\n\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:804)\n\tat org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)\n\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2753)\n\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2745)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296){noformat}\n\nHere's my annotation, going from bottom to top.\n* Somehow we enter Path.getFileSystem()\n* This goes to FileSystem cache stuff, and then it wants the current user\n* So we get to UserGroupInformation.getCurrentUser(), which as you can imagine gets to\n* getUserToGroupsMappingService and thence to LdapGroupsMapping.setConf().\n* That code gets the needed passwords, and we're using the CredentialProvider, so unsurprisingly we get to\n* getPasswordFromCredentialProviders() - which chooses the JavaKeyStoreProvider like I told it to.\n* The JavaKeyStoreProvider, in its constructor, does \"fs = path.getFileSystem(conf);\"\n* And guess what, we're back in Path.getFileSystem, where we started at the beginning.\n\nPlease let me know if I've somehow configured something incorrectly, but if I have I can't figure out what it is..."
        }
    },
    {
        "filename": "HADOOP-11722.json",
        "creation_time": "2015-03-16T21:39:07.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: Could not remove Stored Token ZKDTSMDelegationToken_28\n\tat org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:770)\n\tat org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeExpiredToken(AbstractDelegationTokenSecretManager.java:605)\n\tat org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.access$400(AbstractDelegationTokenSecretManager.java:54)\n\tat org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:656)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /zkdtsm/ZKDTSMRoot/ZKDTSMTokensRoot/DT_28\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:111)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:238)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:233)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:214)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:41)\n\tat org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:764)\n\t... 4 more\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken": "  protected void removeStoredToken(TokenIdent ident)\n      throws IOException {\n    String nodeRemovePath =\n        getNodePath(ZK_DTSM_TOKENS_ROOT, DELEGATION_TOKEN_PREFIX\n            + ident.getSequenceNumber());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Removing ZKDTSMDelegationToken_\"\n          + ident.getSequenceNumber());\n    }\n    try {\n      if (zkClient.checkExists().forPath(nodeRemovePath) != null) {\n        while(zkClient.checkExists().forPath(nodeRemovePath) != null){\n          zkClient.delete().guaranteed().forPath(nodeRemovePath);\n        }\n      } else {\n        LOG.debug(\"Attempted to remove a non-existing znode \" + nodeRemovePath);\n      }\n    } catch (Exception e) {\n      throw new RuntimeException(\n          \"Could not remove Stored Token ZKDTSMDelegationToken_\"\n          + ident.getSequenceNumber(), e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.getNodePath": "  static String getNodePath(String root, String nodeName) {\n    return (root + \"/\" + nodeName);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeExpiredToken": "  private void removeExpiredToken() throws IOException {\n    long now = Time.now();\n    Set<TokenIdent> expiredTokens = new HashSet<TokenIdent>();\n    synchronized (this) {\n      Iterator<Map.Entry<TokenIdent, DelegationTokenInformation>> i =\n          currentTokens.entrySet().iterator();\n      while (i.hasNext()) {\n        Map.Entry<TokenIdent, DelegationTokenInformation> entry = i.next();\n        long renewDate = entry.getValue().getRenewDate();\n        if (renewDate < now) {\n          expiredTokens.add(entry.getKey());\n          i.remove();\n        }\n      }\n    }\n    // don't hold lock on 'this' to avoid edit log updates blocking token ops\n    for (TokenIdent ident : expiredTokens) {\n      logExpireToken(ident);\n      removeStoredToken(ident);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.logExpireToken": "  protected void logExpireToken(TokenIdent ident) throws IOException {\n    return;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.getRenewDate": "    public long getRenewDate() {\n      return renewDate;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeStoredToken": "  protected void removeStoredToken(TokenIdent ident) throws IOException {\n\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.run": "    public void run() {\n      LOG.info(\"Starting expired delegation token remover thread, \"\n          + \"tokenRemoverScanInterval=\" + tokenRemoverScanInterval\n          / (60 * 1000) + \" min(s)\");\n      try {\n        while (running) {\n          long now = Time.now();\n          if (lastMasterKeyUpdate + keyUpdateInterval < now) {\n            try {\n              rollMasterKey();\n              lastMasterKeyUpdate = now;\n            } catch (IOException e) {\n              LOG.error(\"Master key updating failed: \", e);\n            }\n          }\n          if (lastTokenCacheCleanup + tokenRemoverScanInterval < now) {\n            removeExpiredToken();\n            lastTokenCacheCleanup = now;\n          }\n          try {\n            Thread.sleep(Math.min(5000, keyUpdateInterval)); // 5 seconds\n          } catch (InterruptedException ie) {\n            LOG.error(\"ExpiredTokenRemover received \" + ie);\n          }\n        }\n      } catch (Throwable t) {\n        LOG.error(\"ExpiredTokenRemover thread received unexpected exception\", t);\n        Runtime.getRuntime().exit(-1);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.rollMasterKey": "  void rollMasterKey() throws IOException {\n    synchronized (this) {\n      removeExpiredKeys();\n      /* set final expiry date for retiring currentKey */\n      currentKey.setExpiryDate(Time.now() + tokenMaxLifetime);\n      /*\n       * currentKey might have been removed by removeExpiredKeys(), if\n       * updateMasterKey() isn't called at expected interval. Add it back to\n       * allKeys just in case.\n       */\n      updateDelegationKey(currentKey);\n    }\n    updateCurrentKey();\n  }"
        },
        "bug_report": {
            "Title": "Some Instances of Services using ZKDelegationTokenSecretManager go down when old token cannot be deleted",
            "Description": "The delete node code in {{ZKDelegationTokenSecretManager}} is as follows :\n{noformat}\n       while(zkClient.checkExists().forPath(nodeRemovePath) != null){\n          zkClient.delete().guaranteed().forPath(nodeRemovePath);\n       }\n{noformat}\n\nWhen instances of a Service using {{ZKDelegationTokenSecretManager}} try deleting a node simutaneously, It is possible that all of them enter into the while loop in which case, all peers will try to delete the node.. Only 1 will succeed and the rest will throw an exception.. which will bring down the node.\n\nThe Exception is as follows :\n{noformat}\n2015-03-15 10:24:54,000 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover thread received unexpected exception\njava.lang.RuntimeException: Could not remove Stored Token ZKDTSMDelegationToken_28\n\tat org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:770)\n\tat org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeExpiredToken(AbstractDelegationTokenSecretManager.java:605)\n\tat org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.access$400(AbstractDelegationTokenSecretManager.java:54)\n\tat org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:656)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /zkdtsm/ZKDTSMRoot/ZKDTSMTokensRoot/DT_28\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:111)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:238)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:233)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:214)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:41)\n\tat org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:764)\n\t... 4 more\n{noformat}  "
        }
    },
    {
        "filename": "HADOOP-15331.json",
        "creation_time": "2018-03-21T01:15:56.000+0000",
        "stack_trace": "```\n2018-02-28 08:23:19,589 ERROR org.apache.hadoop.conf.Configuration: error parsing conf java.io.BufferedInputStream@7741d346\ncom.ctc.wstx.exc.WstxIOException: Stream closed\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)\n\tat com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)\n\tat org.apache.hadoop.conf.Configuration.get(Configuration.java:1420)\n\tat org.apache.hadoop.security.authorize.ServiceAuthorizationManager.refreshWithLoadedConfiguration(ServiceAuthorizationManager.java:161)\n\tat org.apache.hadoop.ipc.Server.refreshServiceAclWithLoadedConfiguration(Server.java:607)\n\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:586)\n\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.startServer(AdminService.java:188)\n\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.serviceStart(AdminService.java:165)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(AbstractService.java:121)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1231)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1421)\n\n2018-02-28 08:23:20,702 ERROR org.apache.hadoop.conf.Configuration: error parsing conf java.io.BufferedInputStream@7741d346\ncom.ctc.wstx.exc.WstxIOException: Stream closed\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)\n\tat com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1326)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1298)\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.buildRedirectPath(RMWebApp.java:103)\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.getRedirectPath(RMWebApp.java:91)\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:125)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\n\tat com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n\tat org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n\tat org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n\tat org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1560)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:534)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint$2.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:100)\n\tat org.eclipse.jetty.io.ManagedSelector.run(ManagedSelector.java:147)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Stream closed\n\tat java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:336)\n\tat com.ctc.wstx.io.StreamBootstrapper.ensureLoaded(StreamBootstrapper.java:482)\n\tat com.ctc.wstx.io.StreamBootstrapper.resolveStreamEncoding(StreamBootstrapper.java:306)\n\tat com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:167)\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)\n\t... 50 more\n\n2018-02-28 08:23:20,705 WARN org.eclipse.jetty.servlet.ServletHandler: /jmx\njava.lang.RuntimeException: com.ctc.wstx.exc.WstxIOException: Stream closed\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3048)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1326)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1298)\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.buildRedirectPath(RMWebApp.java:103)\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.getRedirectPath(RMWebApp.java:91)\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:125)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\n\tat com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n\tat org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java.ServletHandler.java:1759)\n\tat org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n\tat org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1560)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:534)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint$2.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:100)\n\tat org.eclipse.jetty.io.ManagedSelector.run(ManagedSelector.java:147)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.ctc.wstx.exc.WstxIOException: Stream closed\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)\n\tat com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)\n\t... 46 more\nCaused by: java.io.IOException: Stream closed\n\tat java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:336)\n\tat com.ctc.wstx.io.StreamBootstrapper.ensureLoaded(StreamBootstrapper.java:482)\n\tat com.ctc.wstx.io.StreamBootstrapper.resolveStreamEncoding(StreamBootstrapper.java:306)\n\tat com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:167)\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.parse": "  private XMLStreamReader parse(InputStream is, String systemIdStr,\n      boolean restricted) throws IOException, XMLStreamException {\n    if (!quietmode) {\n      LOG.debug(\"parsing input stream \" + is);\n    }\n    if (is == null) {\n      return null;\n    }\n    SystemId systemId = SystemId.construct(systemIdStr);\n    ReaderConfig readerConfig = XML_INPUT_FACTORY.createPrivateConfig();\n    if (restricted) {\n      readerConfig.setProperty(XMLInputFactory.SUPPORT_DTD, false);\n    }\n    return XML_INPUT_FACTORY.createSR(readerConfig, systemId,\n        StreamBootstrapper.getInstance(null, systemId, is), false, true);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.toString": "  private <T> void toString(List<T> resources, StringBuilder sb) {\n    ListIterator<T> i = resources.listIterator();\n    while (i.hasNext()) {\n      if (i.nextIndex() != 0) {\n        sb.append(\", \");\n      }\n      sb.append(i.next());\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.loadResource": "  private Resource loadResource(Properties properties,\n                                Resource wrapper, boolean quiet) {\n    String name = UNKNOWN_RESOURCE;\n    try {\n      Object resource = wrapper.getResource();\n      name = wrapper.getName();\n      XMLStreamReader2 reader = null;\n      boolean returnCachedProperties = false;\n      boolean isRestricted = wrapper.isParserRestricted();\n\n      if (resource instanceof URL) {                  // an URL resource\n        reader = (XMLStreamReader2)parse((URL)resource, isRestricted);\n      } else if (resource instanceof String) {        // a CLASSPATH resource\n        URL url = getResource((String)resource);\n        reader = (XMLStreamReader2)parse(url, isRestricted);\n      } else if (resource instanceof Path) {          // a file resource\n        // Can't use FileSystem API or we get an infinite loop\n        // since FileSystem uses Configuration API.  Use java.io.File instead.\n        File file = new File(((Path)resource).toUri().getPath())\n          .getAbsoluteFile();\n        if (file.exists()) {\n          if (!quiet) {\n            LOG.debug(\"parsing File \" + file);\n          }\n          reader = (XMLStreamReader2)parse(new BufferedInputStream(\n              new FileInputStream(file)), ((Path)resource).toString(),\n              isRestricted);\n        }\n      } else if (resource instanceof InputStream) {\n        reader = (XMLStreamReader2)parse((InputStream)resource, null,\n            isRestricted);\n        returnCachedProperties = true;\n      } else if (resource instanceof Properties) {\n        overlay(properties, (Properties)resource);\n      }\n\n      if (reader == null) {\n        if (quiet) {\n          return null;\n        }\n        throw new RuntimeException(resource + \" not found\");\n      }\n      Properties toAddTo = properties;\n      if(returnCachedProperties) {\n        toAddTo = new Properties();\n      }\n      DeprecationContext deprecations = deprecationContext.get();\n\n      StringBuilder token = new StringBuilder();\n      String confName = null;\n      String confValue = null;\n      String confInclude = null;\n      String confTag = null;\n      boolean confFinal = false;\n      boolean fallbackAllowed = false;\n      boolean fallbackEntered = false;\n      boolean parseToken = false;\n      LinkedList<String> confSource = new LinkedList<String>();\n\n      while (reader.hasNext()) {\n        switch (reader.next()) {\n        case XMLStreamConstants.START_ELEMENT:\n          switch (reader.getLocalName()) {\n          case \"property\":\n            confName = null;\n            confValue = null;\n            confFinal = false;\n            confTag = null;\n            confSource.clear();\n\n            // First test for short format configuration\n            int attrCount = reader.getAttributeCount();\n            for (int i = 0; i < attrCount; i++) {\n              String propertyAttr = reader.getAttributeLocalName(i);\n              if (\"name\".equals(propertyAttr)) {\n                confName = StringInterner.weakIntern(\n                    reader.getAttributeValue(i));\n              } else if (\"value\".equals(propertyAttr)) {\n                confValue = StringInterner.weakIntern(\n                    reader.getAttributeValue(i));\n              } else if (\"final\".equals(propertyAttr)) {\n                confFinal = \"true\".equals(reader.getAttributeValue(i));\n              } else if (\"source\".equals(propertyAttr)) {\n                confSource.add(StringInterner.weakIntern(\n                    reader.getAttributeValue(i)));\n              } else if (\"tag\".equals(propertyAttr)) {\n                confTag = StringInterner\n                    .weakIntern(reader.getAttributeValue(i));\n              }\n            }\n            break;\n          case \"name\":\n          case \"value\":\n          case \"final\":\n          case \"source\":\n          case \"tag\":\n            parseToken = true;\n            token.setLength(0);\n            break;\n          case \"include\":\n            // Determine href for xi:include\n            confInclude = null;\n            attrCount = reader.getAttributeCount();\n            for (int i = 0; i < attrCount; i++) {\n              String attrName = reader.getAttributeLocalName(i);\n              if (\"href\".equals(attrName)) {\n                confInclude = reader.getAttributeValue(i);\n              }\n            }\n            if (confInclude == null) {\n              break;\n            }\n            if (isRestricted) {\n              throw new RuntimeException(\"Error parsing resource \" + wrapper\n                  + \": XInclude is not supported for restricted resources\");\n            }\n            // Determine if the included resource is a classpath resource\n            // otherwise fallback to a file resource\n            // xi:include are treated as inline and retain current source\n            URL include = getResource(confInclude);\n            if (include != null) {\n              Resource classpathResource = new Resource(include, name,\n                  wrapper.isParserRestricted());\n              loadResource(properties, classpathResource, quiet);\n            } else {\n              URL url;\n              try {\n                url = new URL(confInclude);\n                url.openConnection().connect();\n              } catch (IOException ioe) {\n                File href = new File(confInclude);\n                if (!href.isAbsolute()) {\n                  // Included resources are relative to the current resource\n                  File baseFile = new File(name).getParentFile();\n                  href = new File(baseFile, href.getPath());\n                }\n                if (!href.exists()) {\n                  // Resource errors are non-fatal iff there is 1 xi:fallback\n                  fallbackAllowed = true;\n                  break;\n                }\n                url = href.toURI().toURL();\n              }\n              Resource uriResource = new Resource(url, name,\n                  wrapper.isParserRestricted());\n              loadResource(properties, uriResource, quiet);\n            }\n            break;\n          case \"fallback\":\n            fallbackEntered = true;\n            break;\n          case \"configuration\":\n            break;\n          default:\n            break;\n          }\n          break;\n\n        case XMLStreamConstants.CHARACTERS:\n          if (parseToken) {\n            char[] text = reader.getTextCharacters();\n            token.append(text, reader.getTextStart(), reader.getTextLength());\n          }\n          break;\n\n        case XMLStreamConstants.END_ELEMENT:\n          switch (reader.getLocalName()) {\n          case \"name\":\n            if (token.length() > 0) {\n              confName = StringInterner.weakIntern(token.toString().trim());\n            }\n            break;\n          case \"value\":\n            if (token.length() > 0) {\n              confValue = StringInterner.weakIntern(token.toString());\n            }\n            break;\n          case \"final\":\n            confFinal = \"true\".equals(token.toString());\n            break;\n          case \"source\":\n            confSource.add(StringInterner.weakIntern(token.toString()));\n            break;\n          case \"tag\":\n            if (token.length() > 0) {\n              confTag = StringInterner.weakIntern(token.toString());\n            }\n            break;\n          case \"include\":\n            if (fallbackAllowed && !fallbackEntered) {\n              throw new IOException(\"Fetch fail on include for '\"\n                  + confInclude + \"' with no fallback while loading '\"\n                  + name + \"'\");\n            }\n            fallbackAllowed = false;\n            fallbackEntered = false;\n            break;\n          case \"property\":\n            if (confName == null || (!fallbackAllowed && fallbackEntered)) {\n              break;\n            }\n            confSource.add(name);\n            // Read tags and put them in propertyTagsMap\n            if (confTag != null) {\n              readTagFromConfig(confTag, confName, confValue, confSource);\n            }\n\n            DeprecatedKeyInfo keyInfo =\n                deprecations.getDeprecatedKeyMap().get(confName);\n            if (keyInfo != null) {\n              keyInfo.clearAccessed();\n              for (String key : keyInfo.newKeys) {\n                // update new keys with deprecated key's value\n                loadProperty(toAddTo, name, key, confValue, confFinal,\n                    confSource.toArray(new String[confSource.size()]));\n              }\n            } else {\n              loadProperty(toAddTo, name, confName, confValue, confFinal,\n                  confSource.toArray(new String[confSource.size()]));\n            }\n            break;\n          default:\n            break;\n          }\n        default:\n          break;\n        }\n      }\n      reader.close();\n\n      if (returnCachedProperties) {\n        overlay(properties, toAddTo);\n        return new Resource(toAddTo, name, wrapper.isParserRestricted());\n      }\n      return null;\n    } catch (IOException e) {\n      LOG.error(\"error parsing conf \" + name, e);\n      throw new RuntimeException(e);\n    } catch (XMLStreamException e) {\n      LOG.error(\"error parsing conf \" + name, e);\n      throw new RuntimeException(e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.hasNext": "      public boolean hasNext() {\n        if (at <= end) {\n          return true;\n        } else if (internal != null){\n          return internal.hasNext();\n        }\n        return false;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.get": "  public String get(String name, String defaultValue) {\n    String[] names = handleDeprecation(deprecationContext.get(), name);\n    String result = null;\n    for(String n : names) {\n      result = substituteVars(getProps().getProperty(n, defaultValue));\n    }\n    return result;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.readTagFromConfig": "  private void readTagFromConfig(String attributeValue, String confName, String\n      confValue, List<String> confSource) {\n    for (String tagStr : attributeValue.split(\",\")) {\n      tagStr = tagStr.trim();\n      try {\n        // Handle property with no/null value\n        if (confValue == null) {\n          confValue = \"\";\n        }\n        if (propertyTagsMap.containsKey(tagStr)) {\n          propertyTagsMap.get(tagStr).setProperty(confName, confValue);\n        } else {\n          Properties props = new Properties();\n          props.setProperty(confName, confValue);\n          propertyTagsMap.put(tagStr, props);\n        }\n      } catch (Exception ex) {\n        // Log the exception at trace level.\n        LOG.trace(\"Tag '{}' for property:{} Source:{}\", tagStr, confName,\n            Arrays.toString(confSource.toArray()), ex);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.size": "  public int size() {\n    return getProps().size();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.next": "      public Integer next() {\n        if (at <= end) {\n          at++;\n          return at - 1;\n        } else if (internal != null){\n          Range found = internal.next();\n          if (found != null) {\n            at = found.start;\n            end = found.end;\n            at++;\n            return at - 1;\n          }\n        }\n        return null;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.overlay": "  private void overlay(Properties to, Properties from) {\n    for (Entry<Object, Object> entry: from.entrySet()) {\n      to.put(entry.getKey(), entry.getValue());\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.clear": "  public void clear() {\n    getProps().clear();\n    getOverlay().clear();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getResource": "  public URL getResource(String name) {\n    return classLoader.getResource(name);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.clearAccessed": "    public void clearAccessed() {\n      accessed.set(false);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getDeprecatedKeyMap": "    Map<String, DeprecatedKeyInfo> getDeprecatedKeyMap() {\n      return deprecatedKeyMap;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.loadProperty": "  private void loadProperty(Properties properties, String name, String attr,\n      String value, boolean finalParameter, String[] source) {\n    if (value != null || allowNullValueProperties) {\n      if (value == null) {\n        value = DEFAULT_STRING_CHECK;\n      }\n      if (!finalParameters.contains(attr)) {\n        properties.setProperty(attr, value);\n        if (source != null) {\n          putIntoUpdatingResource(attr, source);\n        }\n      } else {\n        // This is a final parameter so check for overrides.\n        checkForOverride(this.properties, name, attr, value);\n        if (this.properties != properties) {\n          checkForOverride(properties, name, attr, value);\n        }\n      }\n    }\n    if (finalParameter && attr != null) {\n      finalParameters.add(attr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getName": "    public String getName(){\n      return name;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.isParserRestricted": "    public boolean isParserRestricted() {\n      return restrictParser;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.loadResources": "  private void loadResources(Properties properties,\n                             ArrayList<Resource> resources,\n                             boolean quiet) {\n    if(loadDefaults) {\n      for (String resource : defaultResources) {\n        loadResource(properties, new Resource(resource, false), quiet);\n      }\n    }\n    \n    for (int i = 0; i < resources.size(); i++) {\n      Resource ret = loadResource(properties, resources.get(i), quiet);\n      if (ret != null) {\n        resources.set(i, ret);\n      }\n    }\n    this.removeUndeclaredTags(properties);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.set": "  public void set(String name, String value, String source) {\n    Preconditions.checkArgument(\n        name != null,\n        \"Property name must not be null\");\n    Preconditions.checkArgument(\n        value != null,\n        \"The value of property %s must not be null\", name);\n    name = name.trim();\n    DeprecationContext deprecations = deprecationContext.get();\n    if (deprecations.getDeprecatedKeyMap().isEmpty()) {\n      getProps();\n    }\n    getOverlay().setProperty(name, value);\n    getProps().setProperty(name, value);\n    String newSource = (source == null ? \"programmatically\" : source);\n\n    if (!isDeprecated(name)) {\n      putIntoUpdatingResource(name, new String[] {newSource});\n      String[] altNames = getAlternativeNames(name);\n      if(altNames != null) {\n        for(String n: altNames) {\n          if(!n.equals(name)) {\n            getOverlay().setProperty(n, value);\n            getProps().setProperty(n, value);\n            putIntoUpdatingResource(n, new String[] {newSource});\n          }\n        }\n      }\n    }\n    else {\n      String[] names = handleDeprecation(deprecationContext.get(), name);\n      String altSource = \"because \" + name + \" is deprecated\";\n      for(String n : names) {\n        getOverlay().setProperty(n, value);\n        getProps().setProperty(n, value);\n        putIntoUpdatingResource(n, new String[] {altSource});\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.removeUndeclaredTags": "  private void removeUndeclaredTags(Properties prop) {\n    // Get all system tags\n    if (prop.containsKey(CommonConfigurationKeys.HADOOP_SYSTEM_TAGS)){\n      String systemTags = prop.getProperty(CommonConfigurationKeys\n              .HADOOP_SYSTEM_TAGS);\n      Arrays.stream(systemTags.split(\",\")).forEach(tag -> TAGS.add(tag));\n    }\n    // Get all custom tags\n    if (prop.containsKey(CommonConfigurationKeys.HADOOP_CUSTOM_TAGS)) {\n      String customTags = prop.getProperty(CommonConfigurationKeys\n          .HADOOP_CUSTOM_TAGS);\n      Arrays.stream(customTags.split(\",\")).forEach(tag -> TAGS.add(tag));\n    }\n\n    Set undeclaredTags = propertyTagsMap.keySet();\n    if (undeclaredTags.retainAll(TAGS)) {\n      LOG.info(\"Removed undeclared tags:\");\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getProps": "  protected synchronized Properties getProps() {\n    if (properties == null) {\n      properties = new Properties();\n      Map<String, String[]> backup = updatingResource != null ?\n          new ConcurrentHashMap<String, String[]>(updatingResource) : null;\n      loadResources(properties, resources, quietmode);\n\n      if (overlay != null) {\n        properties.putAll(overlay);\n        if (backup != null) {\n          for (Map.Entry<Object, Object> item : overlay.entrySet()) {\n            String key = (String) item.getKey();\n            String[] source = backup.get(key);\n            if (source != null) {\n              updatingResource.put(key, source);\n            }\n          }\n        }\n      }\n    }\n    return properties;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getKey": "    public String getKey() {\n      return key;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.authorize.ServiceAuthorizationManager.refreshWithLoadedConfiguration": "  public void refreshWithLoadedConfiguration(Configuration conf,\n      PolicyProvider provider) {\n    final Map<Class<?>, AccessControlList[]> newAcls =\n      new IdentityHashMap<Class<?>, AccessControlList[]>();\n    final Map<Class<?>, MachineList[]> newMachineLists =\n      new IdentityHashMap<Class<?>, MachineList[]>();\n    \n    String defaultAcl = conf.get(\n        CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_AUTHORIZATION_DEFAULT_ACL,\n        AccessControlList.WILDCARD_ACL_VALUE);\n\n    String defaultBlockedAcl = conf.get(\n      CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_AUTHORIZATION_DEFAULT_BLOCKED_ACL, \"\");\n\n    String defaultServiceHostsKey = getHostKey(\n      CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_AUTHORIZATION_DEFAULT_ACL);\n    String defaultMachineList = conf.get(defaultServiceHostsKey,\n      MachineList.WILDCARD_VALUE);\n    String defaultBlockedMachineList= conf.get(\n     defaultServiceHostsKey+ BLOCKED, \"\");\n\n    // Parse the config file\n    Service[] services = provider.getServices();\n    if (services != null) {\n      for (Service service : services) {\n        AccessControlList acl =\n            new AccessControlList(\n                conf.get(service.getServiceKey(),\n                    defaultAcl)\n            );\n        AccessControlList blockedAcl =\n           new AccessControlList(\n           conf.get(service.getServiceKey() + BLOCKED,\n           defaultBlockedAcl));\n        newAcls.put(service.getProtocol(), new AccessControlList[] {acl, blockedAcl});\n        String serviceHostsKey = getHostKey(service.getServiceKey());\n        MachineList machineList = new MachineList (conf.get(serviceHostsKey, defaultMachineList));\n        MachineList blockedMachineList = new MachineList(\n          conf.get(serviceHostsKey + BLOCKED, defaultBlockedMachineList));\n        newMachineLists.put(service.getProtocol(),\n            new MachineList[] {machineList, blockedMachineList});\n      }\n    }\n\n    // Flip to the newly parsed permissions\n    protocolToAcls = newAcls;\n    protocolToMachineLists = newMachineLists;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.authorize.ServiceAuthorizationManager.getHostKey": "  private String getHostKey(String serviceKey) {\n    int endIndex = serviceKey.lastIndexOf(\".\");\n    if (endIndex != -1) {\n      return serviceKey.substring(0, endIndex)+ HOSTS;\n    }\n    return serviceKey;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.refreshServiceAclWithLoadedConfiguration": "  public void refreshServiceAclWithLoadedConfiguration(Configuration conf,\n      PolicyProvider provider) {\n    serviceAuthorizationManager.refreshWithLoadedConfiguration(conf, provider);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.start": "  public void start() {\n    if (isInState(STATE.STARTED)) {\n      return;\n    }\n    //enter the started state\n    synchronized (stateChangeLock) {\n      if (stateModel.enterState(STATE.STARTED) != STATE.STARTED) {\n        try {\n          startTime = System.currentTimeMillis();\n          serviceStart();\n          if (isInState(STATE.STARTED)) {\n            //if the service started (and isn't now in a later state), notify\n            LOG.debug(\"Service {} is started\", getName());\n            notifyListeners();\n          }\n        } catch (Exception e) {\n          noteFailure(e);\n          ServiceOperations.stopQuietly(LOG, this);\n          throw ServiceStateException.convert(e);\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.isInState": "  public final boolean isInState(Service.STATE expected) {\n    return stateModel.isInState(expected);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.getName": "  public String getName() {\n    return name;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.serviceStart": "  protected void serviceStart() throws Exception {\n\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.enterState": "  private STATE enterState(STATE newState) {\n    assert stateModel != null : \"null state in \" + name + \" \" + this.getClass();\n    STATE oldState = stateModel.enterState(newState);\n    if (oldState != newState) {\n      LOG.debug(\"Service: {} entered state {}\", getName(), getServiceState());\n\n      recordLifecycleEvent();\n    }\n    return oldState;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.noteFailure": "  protected final void noteFailure(Exception exception) {\n    LOG.debug(\"noteFailure {}\" + exception);\n    if (exception == null) {\n      //make sure failure logic doesn't itself cause problems\n      return;\n    }\n    //record the failure details, and log it\n    synchronized (this) {\n      if (failureCause == null) {\n        failureCause = exception;\n        failureState = getServiceState();\n        LOG.info(\"Service {} failed in state {}\",\n            getName(), failureState, exception);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.AbstractService.notifyListeners": "  private void notifyListeners() {\n    try {\n      listeners.notifyListeners(this);\n      globalListeners.notifyListeners(this);\n    } catch (Throwable e) {\n      LOG.warn(\"Exception while notifying listeners of {}\", this, e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter": "  public void doFilter(ServletRequest req, ServletResponse res,\n      FilterChain chain) throws IOException, ServletException {\n    ((HttpServletResponse) res).setHeader(X_FRAME_OPTIONS, option);\n    chain.doFilter(req,\n        new XFrameOptionsResponseWrapper((HttpServletResponse) res));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.lib.StaticUserWebFilter.doFilter": "    public void doFilter(ServletRequest request, ServletResponse response,\n                         FilterChain chain\n                         ) throws IOException, ServletException {\n      HttpServletRequest httpRequest = (HttpServletRequest) request;\n      // if the user is already authenticated, don't override it\n      if (httpRequest.getRemoteUser() != null) {\n        chain.doFilter(request, response);\n      } else {\n        HttpServletRequestWrapper wrapper = \n            new HttpServletRequestWrapper(httpRequest) {\n          @Override\n          public Principal getUserPrincipal() {\n            return user;\n          }\n          @Override\n          public String getRemoteUser() {\n            return username;\n          }\n        };\n        chain.doFilter(wrapper, response);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.lib.StaticUserWebFilter.getRemoteUser": "          public String getRemoteUser() {\n            return username;\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.doFilter": "    public void doFilter(ServletRequest request,\n                         ServletResponse response,\n                         FilterChain chain\n                         ) throws IOException, ServletException {\n      HttpServletRequestWrapper quoted =\n        new RequestQuoter((HttpServletRequest) request);\n      HttpServletResponse httpResponse = (HttpServletResponse) response;\n\n      String mime = inferMimeType(request);\n      if (mime == null) {\n        httpResponse.setContentType(\"text/plain; charset=utf-8\");\n      } else if (mime.startsWith(\"text/html\")) {\n        // HTML with unspecified encoding, we want to\n        // force HTML with utf-8 encoding\n        // This is to avoid the following security issue:\n        // http://openmya.hacker.jp/hasegawa/security/utf7cs.html\n        httpResponse.setContentType(\"text/html; charset=utf-8\");\n      } else if (mime.startsWith(\"application/xml\")) {\n        httpResponse.setContentType(\"text/xml; charset=utf-8\");\n      }\n\n      if(Boolean.valueOf(this.config.getInitParameter(X_FRAME_ENABLED))) {\n        httpResponse.addHeader(\"X-FRAME-OPTIONS\",\n            this.config.getInitParameter(X_FRAME_VALUE));\n      }\n      chain.doFilter(quoted, httpResponse);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.inferMimeType": "    private String inferMimeType(ServletRequest request) {\n      String path = ((HttpServletRequest)request).getRequestURI();\n      ServletContextHandler.Context sContext =\n          (ServletContextHandler.Context)config.getServletContext();\n      String mime = sContext.getMimeType(path);\n      return (mime == null) ? null : mime;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.NoCacheFilter.doFilter": "  public void doFilter(ServletRequest req, ServletResponse res,\n                       FilterChain chain)\n    throws IOException, ServletException {\n    HttpServletResponse httpRes = (HttpServletResponse) res;\n    httpRes.setHeader(\"Cache-Control\", \"no-cache\");\n    long now = System.currentTimeMillis();\n    httpRes.addDateHeader(\"Expires\", now);\n    httpRes.addDateHeader(\"Date\", now);\n    httpRes.addHeader(\"Pragma\", \"no-cache\");\n    chain.doFilter(req, res);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.authorize.service.getServiceKey": "  public String getServiceKey() {\n    return key;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.authorize.service.getProtocol": "  public Class<?> getProtocol() {\n    return protocol;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceOperations.stopQuietly": "  public static Exception stopQuietly(Logger log, Service service) {\n    try {\n      stop(service);\n    } catch (Exception e) {\n      log.warn(\"When stopping the service {}\", service.getName(), e);\n      return e;\n    }\n    return null;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceOperations.stop": "  public static void stop(Service service) {\n    if (service != null) {\n      service.stop();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.service.ServiceStateException.convert": "  public static RuntimeException convert(String text, Throwable fault) {\n    if (fault instanceof RuntimeException) {\n      return (RuntimeException) fault;\n    } else {\n      return new ServiceStateException(text, fault);\n    }\n  }"
        },
        "bug_report": {
            "Title": "Fix a race condition causing parsing error of java.io.BufferedInputStream in class org.apache.hadoop.conf.Configuration",
            "Description": "There is a race condition in the way Hadoop handles the Configuration class. The scenario is the following. Let's assume that there are two threads sharing the same Configuration class. One adds some resources to the configuration, while the other one clones it. Resources are loaded lazily in a deferred call to {{loadResources()}}. If the cloning happens after adding the resources but before parsing them, some temporary resources like input stream pointers are cloned. Eventually both copies will load the input stream resources pointing to the same input streams. One parses the input stream XML and closes it updating it's own copy of the resource. The other one has another pointer to the same input stream. When it tries to load it, it will crash with a stream closed exception.\r\n\r\nHere is an example unit test:\r\n{code:java}\r\n@Test\r\npublic void testResourceRace() {\r\n  InputStream is =\r\n      new BufferedInputStream(new ByteArrayInputStream(\r\n          \"<configuration></configuration>\".getBytes()));\r\n  Configuration conf = new Configuration();\r\n  // Thread 1\r\n  conf.addResource(is);\r\n  // Thread 2\r\n  Configuration confClone = new Configuration(conf);\r\n  // Thread 2\r\n  confClone.get(\"firstParse\");\r\n  // Thread 1\r\n  conf.get(\"secondParse\");\r\n}{code}\r\nExample real world stack traces:\r\n{code:java}\r\n2018-02-28 08:23:19,589 ERROR org.apache.hadoop.conf.Configuration: error parsing conf java.io.BufferedInputStream@7741d346\r\ncom.ctc.wstx.exc.WstxIOException: Stream closed\r\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)\r\n\tat com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)\r\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)\r\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)\r\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)\r\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)\r\n\tat org.apache.hadoop.conf.Configuration.get(Configuration.java:1420)\r\n\tat org.apache.hadoop.security.authorize.ServiceAuthorizationManager.refreshWithLoadedConfiguration(ServiceAuthorizationManager.java:161)\r\n\tat org.apache.hadoop.ipc.Server.refreshServiceAclWithLoadedConfiguration(Server.java:607)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:586)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.startServer(AdminService.java:188)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.serviceStart(AdminService.java:165)\r\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1231)\r\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1421)\r\n{code}\r\nAnother example:\r\n{code:java}\r\n2018-02-28 08:23:20,702 ERROR org.apache.hadoop.conf.Configuration: error parsing conf java.io.BufferedInputStream@7741d346\r\ncom.ctc.wstx.exc.WstxIOException: Stream closed\r\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)\r\n\tat com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)\r\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)\r\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)\r\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)\r\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)\r\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1326)\r\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1298)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.buildRedirectPath(RMWebApp.java:103)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.getRedirectPath(RMWebApp.java:91)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:125)\r\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)\r\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)\r\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\r\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\r\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\r\n\tat com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)\r\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1560)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\r\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\r\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:534)\r\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\r\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\r\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\r\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.execute(ExecuteProduceConsume.java:100)\r\n\tat org.eclipse.jetty.io.ManagedSelector.run(ManagedSelector.java:147)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: Stream closed\r\n\tat java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:336)\r\n\tat com.ctc.wstx.io.StreamBootstrapper.ensureLoaded(StreamBootstrapper.java:482)\r\n\tat com.ctc.wstx.io.StreamBootstrapper.resolveStreamEncoding(StreamBootstrapper.java:306)\r\n\tat com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:167)\r\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)\r\n\t... 50 more\r\n2018-02-28 08:23:20,705 WARN org.eclipse.jetty.servlet.ServletHandler: /jmx\r\njava.lang.RuntimeException: com.ctc.wstx.exc.WstxIOException: Stream closed\r\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3048)\r\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)\r\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)\r\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1326)\r\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1298)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.buildRedirectPath(RMWebApp.java:103)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.getRedirectPath(RMWebApp.java:91)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:125)\r\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)\r\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\r\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\r\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\r\n\tat com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)\r\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1560)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\r\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\r\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:534)\r\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\r\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\r\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\r\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.execute(ExecuteProduceConsume.java:100)\r\n\tat org.eclipse.jetty.io.ManagedSelector.run(ManagedSelector.java:147)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: com.ctc.wstx.exc.WstxIOException: Stream closed\r\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)\r\n\tat com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)\r\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)\r\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)\r\n\t... 46 more\r\nCaused by: java.io.IOException: Stream closed\r\n\tat java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:336)\r\n\tat com.ctc.wstx.io.StreamBootstrapper.ensureLoaded(StreamBootstrapper.java:482)\r\n\tat com.ctc.wstx.io.StreamBootstrapper.resolveStreamEncoding(StreamBootstrapper.java:306)\r\n\tat com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:167)\r\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)\r\n\t... 49 more\r\n2018-02-28 08:23:20,715 INFO org.{code}"
        }
    },
    {
        "filename": "HADOOP-14062.json",
        "creation_time": "2016-12-20T00:35:48.000+0000",
        "stack_trace": "```\njava.io.EOFException: End of File Exception between local host is: \"<application_master_host>/<ip_addr>\"; destination host is: \"<rm_host>\":8030; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1486)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1428)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1338)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy80.allocate(Unknown Source)\n\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)\n\tat com.sun.proxy.$Proxy81.allocate(Unknown Source)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.makeRemoteRequest(RMContainerRequestor.java:204)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getResources(RMContainerAllocator.java:735)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:269)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$AllocatorRunnable.run(RMCommunicator.java:281)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1785)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1156)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.wrapWithMessage": "  private static <T extends IOException> T wrapWithMessage(\n      T exception, String msg) {\n    Class<? extends Throwable> clazz = exception.getClass();\n    try {\n      Constructor<? extends Throwable> ctor = clazz.getConstructor(String.class);\n      Throwable t = ctor.newInstance(msg);\n      return (T)(t.initCause(exception));\n    } catch (Throwable e) {\n      LOG.warn(\"Unable to wrap exception of type \" +\n          clazz + \": it has no (String) constructor\", e);\n      return exception;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.wrapException": "  public static IOException wrapException(final String destHost,\n                                          final int destPort,\n                                          final String localHost,\n                                          final int localPort,\n                                          final IOException exception) {\n    if (exception instanceof BindException) {\n      return wrapWithMessage(exception,\n          \"Problem binding to [\"\n              + localHost\n              + \":\"\n              + localPort\n              + \"] \"\n              + exception\n              + \";\"\n              + see(\"BindException\"));\n    } else if (exception instanceof ConnectException) {\n      // connection refused; include the host:port in the error\n      return wrapWithMessage(exception, \n          \"Call From \"\n              + localHost\n              + \" to \"\n              + destHost\n              + \":\"\n              + destPort\n              + \" failed on connection exception: \"\n              + exception\n              + \";\"\n              + see(\"ConnectionRefused\"));\n    } else if (exception instanceof UnknownHostException) {\n      return wrapWithMessage(exception,\n          \"Invalid host name: \"\n              + getHostDetailsAsString(destHost, destPort, localHost)\n              + exception\n              + \";\"\n              + see(\"UnknownHost\"));\n    } else if (exception instanceof SocketTimeoutException) {\n      return wrapWithMessage(exception,\n          \"Call From \"\n              + localHost + \" to \" + destHost + \":\" + destPort\n              + \" failed on socket timeout exception: \" + exception\n              + \";\"\n              + see(\"SocketTimeout\"));\n    } else if (exception instanceof NoRouteToHostException) {\n      return wrapWithMessage(exception,\n          \"No Route to Host from  \"\n              + localHost + \" to \" + destHost + \":\" + destPort\n              + \" failed on socket timeout exception: \" + exception\n              + \";\"\n              + see(\"NoRouteToHost\"));\n    } else if (exception instanceof EOFException) {\n      return wrapWithMessage(exception,\n          \"End of File Exception between \"\n              + getHostDetailsAsString(destHost,  destPort, localHost)\n              + \": \" + exception\n              + \";\"\n              + see(\"EOFException\"));\n    } else if (exception instanceof SocketException) {\n      // Many of the predecessor exceptions are subclasses of SocketException,\n      // so must be handled before this\n      return wrapWithMessage(exception,\n          \"Call From \"\n              + localHost + \" to \" + destHost + \":\" + destPort\n              + \" failed on socket exception: \" + exception\n              + \";\"\n              + see(\"SocketException\"));\n    }\n    else {\n      return (IOException) new IOException(\"Failed on local exception: \"\n             + exception\n             + \"; Host Details : \"\n             + getHostDetailsAsString(destHost, destPort, localHost))\n          .initCause(exception);\n\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.getHostDetailsAsString": "  private static String getHostDetailsAsString(final String destHost,\n                                               final int destPort,\n                                               final String localHost) {\n    StringBuilder hostDetails = new StringBuilder(27);\n    hostDetails.append(\"local host is: \")\n        .append(quoteHost(localHost))\n        .append(\"; \");\n    hostDetails.append(\"destination host is: \").append(quoteHost(destHost))\n        .append(\":\")\n        .append(destPort).append(\"; \");\n    return hostDetails.toString();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.see": "  private static String see(final String entry) {\n    return FOR_MORE_DETAILS_SEE + HADOOP_WIKI + entry;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRpcResponse": "  private Writable getRpcResponse(final Call call, final Connection connection,\n      final long timeout, final TimeUnit unit) throws IOException {\n    synchronized (call) {\n      while (!call.done) {\n        try {\n          AsyncGet.Util.wait(call, timeout, unit);\n          if (timeout >= 0 && !call.done) {\n            return null;\n          }\n        } catch (InterruptedException ie) {\n          Thread.currentThread().interrupt();\n          throw new InterruptedIOException(\"Call interrupted\");\n        }\n      }\n\n      if (call.error != null) {\n        if (call.error instanceof RemoteException) {\n          call.error.fillInStackTrace();\n          throw call.error;\n        } else { // local exception\n          InetSocketAddress address = connection.getRemoteAddress();\n          throw NetUtils.wrapException(address.getHostName(),\n                  address.getPort(),\n                  NetUtils.getHostname(),\n                  0,\n                  call.error);\n        }\n      } else {\n        return call.getRpcResponse();\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRemoteAddress": "    public InetSocketAddress getRemoteAddress() {\n      return server;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.call": "  Writable call(RPC.RpcKind rpcKind, Writable rpcRequest,\n      ConnectionId remoteId, int serviceClass,\n      AtomicBoolean fallbackToSimpleAuth) throws IOException {\n    final Call call = createCall(rpcKind, rpcRequest);\n    final Connection connection = getConnection(remoteId, call, serviceClass,\n        fallbackToSimpleAuth);\n\n    try {\n      checkAsyncCall();\n      try {\n        connection.sendRpcRequest(call);                 // send the rpc request\n      } catch (RejectedExecutionException e) {\n        throw new IOException(\"connection has been closed\", e);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n        LOG.warn(\"interrupted waiting to send rpc request to server\", e);\n        throw new IOException(e);\n      }\n    } catch(Exception e) {\n      if (isAsynchronousMode()) {\n        releaseAsyncCall();\n      }\n      throw e;\n    }\n\n    if (isAsynchronousMode()) {\n      final AsyncGet<Writable, IOException> asyncGet\n          = new AsyncGet<Writable, IOException>() {\n        @Override\n        public Writable get(long timeout, TimeUnit unit)\n            throws IOException, TimeoutException{\n          boolean done = true;\n          try {\n            final Writable w = getRpcResponse(call, connection, timeout, unit);\n            if (w == null) {\n              done = false;\n              throw new TimeoutException(call + \" timed out \"\n                  + timeout + \" \" + unit);\n            }\n            return w;\n          } finally {\n            if (done) {\n              releaseAsyncCall();\n            }\n          }\n        }\n\n        @Override\n        public boolean isDone() {\n          synchronized (call) {\n            return call.done;\n          }\n        }\n      };\n\n      ASYNC_RPC_RESPONSE.set(asyncGet);\n      return null;\n    } else {\n      return getRpcResponse(call, connection, -1, null);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.isAsynchronousMode": "  public static boolean isAsynchronousMode() {\n    return asynchronousMode.get();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.sendRpcRequest": "    public void sendRpcRequest(final Call call)\n        throws InterruptedException, IOException {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n\n      // Serialize the call to be sent. This is done from the actual\n      // caller thread, rather than the sendParamsExecutor thread,\n      \n      // so that if the serialization throws an error, it is reported\n      // properly. This also parallelizes the serialization.\n      //\n      // Format of a call on the wire:\n      // 0) Length of rest below (1 + 2)\n      // 1) RpcRequestHeader  - is serialized Delimited hence contains length\n      // 2) RpcRequest\n      //\n      // Items '1' and '2' are prepared here. \n      RpcRequestHeaderProto header = ProtoUtil.makeRpcRequestHeader(\n          call.rpcKind, OperationProto.RPC_FINAL_PACKET, call.id, call.retry,\n          clientId);\n\n      final ResponseBuffer buf = new ResponseBuffer();\n      header.writeDelimitedTo(buf);\n      RpcWritable.wrap(call.rpcRequest).writeTo(buf);\n\n      synchronized (sendRpcRequestLock) {\n        Future<?> senderFuture = sendParamsExecutor.submit(new Runnable() {\n          @Override\n          public void run() {\n            try {\n              synchronized (ipcStreams.out) {\n                if (shouldCloseConnection.get()) {\n                  return;\n                }\n                if (LOG.isDebugEnabled()) {\n                  LOG.debug(getName() + \" sending #\" + call.id);\n                }\n                // RpcRequestHeader + RpcRequest\n                ipcStreams.sendRequest(buf.toByteArray());\n                ipcStreams.flush();\n              }\n            } catch (IOException e) {\n              // exception at this point would leave the connection in an\n              // unrecoverable state (eg half a call left on the wire).\n              // So, close the connection, killing any outstanding calls\n              markClosed(e);\n            } finally {\n              //the buffer is just an in-memory buffer, but it is still polite to\n              // close early\n              IOUtils.closeStream(buf);\n            }\n          }\n        });\n      \n        try {\n          senderFuture.get();\n        } catch (ExecutionException e) {\n          Throwable cause = e.getCause();\n          \n          // cause should only be a RuntimeException as the Runnable above\n          // catches IOException\n          if (cause instanceof RuntimeException) {\n            throw (RuntimeException) cause;\n          } else {\n            throw new RuntimeException(\"unexpected checked exception\", cause);\n          }\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.createCall": "  Call createCall(RPC.RpcKind rpcKind, Writable rpcRequest) {\n    return new Call(rpcKind, rpcRequest);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnection": "  private Connection getConnection(ConnectionId remoteId,\n      Call call, int serviceClass, AtomicBoolean fallbackToSimpleAuth)\n      throws IOException {\n    if (!running.get()) {\n      // the client is stopped\n      throw new IOException(\"The client is stopped\");\n    }\n    Connection connection;\n    /* we could avoid this allocation for each RPC by having a  \n     * connectionsId object and with set() method. We need to manage the\n     * refs for keys in HashMap properly. For now its ok.\n     */\n    while (true) {\n      // These lines below can be shorten with computeIfAbsent in Java8\n      connection = connections.get(remoteId);\n      if (connection == null) {\n        connection = new Connection(remoteId, serviceClass);\n        Connection existing = connections.putIfAbsent(remoteId, connection);\n        if (existing != null) {\n          connection = existing;\n        }\n      }\n\n      if (connection.addCall(call)) {\n        break;\n      } else {\n        // This connection is closed, should be removed. But other thread could\n        // have already known this closedConnection, and replace it with a new\n        // connection. So we should call conditional remove to make sure we only\n        // remove this closedConnection.\n        connections.remove(remoteId, connection);\n      }\n    }\n\n    // If the server happens to be slow, the method below will take longer to\n    // establish a connection.\n    connection.setupIOstreams(fallbackToSimpleAuth);\n    return connection;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.releaseAsyncCall": "  private void releaseAsyncCall() {\n    asyncCallCounter.decrementAndGet();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.checkAsyncCall": "  private void checkAsyncCall() throws IOException {\n    if (isAsynchronousMode()) {\n      if (asyncCallCounter.incrementAndGet() > maxAsyncCalls) {\n        asyncCallCounter.decrementAndGet();\n        String errMsg = String.format(\n            \"Exceeded limit of max asynchronous calls: %d, \" +\n            \"please configure %s to adjust it.\",\n            maxAsyncCalls,\n            CommonConfigurationKeys.IPC_CLIENT_ASYNC_CALLS_MAX_KEY);\n        throw new AsyncCallLimitExceededException(errMsg);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.invoke": "    public Message invoke(Object proxy, final Method method, Object[] args)\n        throws ServiceException {\n      long startTime = 0;\n      if (LOG.isDebugEnabled()) {\n        startTime = Time.now();\n      }\n      \n      if (args.length != 2) { // RpcController + Message\n        throw new ServiceException(\n            \"Too many or few parameters for request. Method: [\"\n            + method.getName() + \"]\" + \", Expected: 2, Actual: \"\n            + args.length);\n      }\n      if (args[1] == null) {\n        throw new ServiceException(\"null param while calling Method: [\"\n            + method.getName() + \"]\");\n      }\n\n      // if Tracing is on then start a new span for this rpc.\n      // guard it in the if statement to make sure there isn't\n      // any extra string manipulation.\n      Tracer tracer = Tracer.curThreadTracer();\n      TraceScope traceScope = null;\n      if (tracer != null) {\n        traceScope = tracer.newScope(RpcClientUtil.methodToTraceString(method));\n      }\n\n      RequestHeaderProto rpcRequestHeader = constructRpcRequestHeader(method);\n      \n      if (LOG.isTraceEnabled()) {\n        LOG.trace(Thread.currentThread().getId() + \": Call -> \" +\n            remoteId + \": \" + method.getName() +\n            \" {\" + TextFormat.shortDebugString((Message) args[1]) + \"}\");\n      }\n\n\n      final Message theRequest = (Message) args[1];\n      final RpcWritable.Buffer val;\n      try {\n        val = (RpcWritable.Buffer) client.call(RPC.RpcKind.RPC_PROTOCOL_BUFFER,\n            new RpcProtobufRequest(rpcRequestHeader, theRequest), remoteId,\n            fallbackToSimpleAuth);\n\n      } catch (Throwable e) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Exception <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + e + \"}\");\n        }\n        if (traceScope != null) {\n          traceScope.addTimelineAnnotation(\"Call got exception: \" +\n              e.toString());\n        }\n        throw new ServiceException(e);\n      } finally {\n        if (traceScope != null) traceScope.close();\n      }\n\n      if (LOG.isDebugEnabled()) {\n        long callTime = Time.now() - startTime;\n        LOG.debug(\"Call: \" + method.getName() + \" took \" + callTime + \"ms\");\n      }\n      \n      if (Client.isAsynchronousMode()) {\n        final AsyncGet<RpcWritable.Buffer, IOException> arr\n            = Client.getAsyncRpcResponse();\n        final AsyncGet<Message, Exception> asyncGet\n            = new AsyncGet<Message, Exception>() {\n          @Override\n          public Message get(long timeout, TimeUnit unit) throws Exception {\n            return getReturnMessage(method, arr.get(timeout, unit));\n          }\n\n          @Override\n          public boolean isDone() {\n            return arr.isDone();\n          }\n        };\n        ASYNC_RETURN_MESSAGE.set(asyncGet);\n        return null;\n      } else {\n        return getReturnMessage(method, val);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.constructRpcRequestHeader": "    private RequestHeaderProto constructRpcRequestHeader(Method method) {\n      RequestHeaderProto.Builder builder = RequestHeaderProto\n          .newBuilder();\n      builder.setMethodName(method.getName());\n     \n\n      // For protobuf, {@code protocol} used when creating client side proxy is\n      // the interface extending BlockingInterface, which has the annotations \n      // such as ProtocolName etc.\n      //\n      // Using Method.getDeclaringClass(), as in WritableEngine to get at\n      // the protocol interface will return BlockingInterface, from where \n      // the annotation ProtocolName and Version cannot be\n      // obtained.\n      //\n      // Hence we simply use the protocol class used to create the proxy.\n      // For PB this may limit the use of mixins on client side.\n      builder.setDeclaringClassProtocolName(protocolName);\n      builder.setClientProtocolVersion(clientProtocolVersion);\n      return builder.build();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.toString": "    public String toString() {\n      try {\n        RequestHeaderProto header = getRequestHeader();\n        return header.getDeclaringClassProtocolName() + \".\" +\n               header.getMethodName();\n      } catch (IOException e) {\n        throw new IllegalArgumentException(e);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getReturnMessage": "    private Message getReturnMessage(final Method method,\n        final RpcWritable.Buffer buf) throws ServiceException {\n      Message prototype = null;\n      try {\n        prototype = getReturnProtoType(method);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      Message returnMessage;\n      try {\n        returnMessage = buf.getValue(prototype.getDefaultInstanceForType());\n\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Response <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + TextFormat.shortDebugString(returnMessage) + \"}\");\n        }\n\n      } catch (Throwable e) {\n        throw new ServiceException(e);\n      }\n      return returnMessage;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.get": "          public Message get(long timeout, TimeUnit unit) throws Exception {\n            return getReturnMessage(method, arr.get(timeout, unit));\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.close": "    public void close() throws IOException {\n      if (!isClosed) {\n        isClosed = true;\n        CLIENTS.stopClient(client);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcProtobufRequest request = (RpcProtobufRequest) writableRequest;\n        RequestHeaderProto rpcRequest = request.getRequestHeader();\n        String methodName = rpcRequest.getMethodName();\n\n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = request.getValue(prototype);\n\n        Message result;\n        long startTime = Time.now();\n        int qTime = (int) (startTime - receiveTime);\n        Exception exception = null;\n        boolean isDeferred = false;\n        try {\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          currentCallInfo.set(new CallInfo(server, methodName));\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n          // Check if this needs to be a deferred response,\n          // by checking the ThreadLocal callback being set\n          if (currentCallback.get() != null) {\n            Server.getCurCall().get().deferResponse();\n            isDeferred = true;\n            currentCallback.set(null);\n            return null;\n          }\n        } catch (ServiceException e) {\n          exception = (Exception) e.getCause();\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          exception = e;\n          throw e;\n        } finally {\n          currentCallInfo.set(null);\n          int processingTime = (int) (Time.now() - startTime);\n          if (LOG.isDebugEnabled()) {\n            String msg =\n                \"Served: \" + methodName + (isDeferred ? \", deferred\" : \"\") +\n                    \", queueTime= \" + qTime +\n                    \" procesingTime= \" + processingTime;\n            if (exception != null) {\n              msg += \" exception= \" + exception.getClass().getSimpleName();\n            }\n            LOG.debug(msg);\n          }\n          String detailedMetricsName = (exception == null) ?\n              methodName :\n              exception.getClass().getSimpleName();\n          server.updateMetrics(detailedMetricsName, qTime, processingTime,\n              isDeferred);\n        }\n        return RpcWritable.wrap(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.isDone": "          public boolean isDone() {\n            return arr.isDone();\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod": "  protected Object invokeMethod(Method method, Object[] args) throws Throwable {\n    try {\n      if (!method.isAccessible()) {\n        method.setAccessible(true);\n      }\n      final Object r = method.invoke(proxyDescriptor.getProxy(), args);\n      hasSuccessfulCall = true;\n      return r;\n    } catch (InvocationTargetException e) {\n      throw e.getCause();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invoke": "  public Object invoke(Object proxy, Method method, Object[] args)\n      throws Throwable {\n    final boolean isRpc = isRpcInvocation(proxyDescriptor.getProxy());\n    final int callId = isRpc? Client.nextCallId(): RpcConstants.INVALID_CALL_ID;\n\n    final Call call = newCall(method, args, isRpc, callId);\n    while (true) {\n      final CallReturn c = call.invokeOnce();\n      final CallReturn.State state = c.getState();\n      if (state == CallReturn.State.ASYNC_INVOKED) {\n        return null; // return null for async calls\n      } else if (c.getState() != CallReturn.State.RETRY) {\n        return c.getReturnValue();\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.getProxy": "    synchronized T getProxy() {\n      return proxyInfo.proxy;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invokeOnce": "    synchronized CallReturn invokeOnce() {\n      try {\n        if (retryInfo != null) {\n          return processWaitTimeAndRetryInfo();\n        }\n\n        // The number of times this invocation handler has ever been failed over\n        // before this method invocation attempt. Used to prevent concurrent\n        // failed method invocations from triggering multiple failover attempts.\n        final long failoverCount = retryInvocationHandler.getFailoverCount();\n        try {\n          return invoke();\n        } catch (Exception e) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(toString(), e);\n          }\n          if (Thread.currentThread().isInterrupted()) {\n            // If interrupted, do not retry.\n            throw e;\n          }\n\n          retryInfo = retryInvocationHandler.handleException(\n              method, callId, retryPolicy, counters, failoverCount, e);\n          return processWaitTimeAndRetryInfo();\n        }\n      } catch(Throwable t) {\n        return new CallReturn(t);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.processWaitTimeAndRetryInfo": "    CallReturn processWaitTimeAndRetryInfo() throws InterruptedIOException {\n      final Long waitTime = getWaitTime(Time.monotonicNow());\n      LOG.trace(\"#{} processRetryInfo: retryInfo={}, waitTime={}\",\n          callId, retryInfo, waitTime);\n      if (waitTime != null && waitTime > 0) {\n        try {\n          Thread.sleep(retryInfo.delay);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Interrupted while waiting to retry\", e);\n          InterruptedIOException intIOE = new InterruptedIOException(\n              \"Retry interrupted\");\n          intIOE.initCause(e);\n          throw intIOE;\n        }\n      }\n      processRetryInfo();\n      return CallReturn.RETRY;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.toString": "    public String toString() {\n      return getClass().getSimpleName() + \"#\" + callId + \": \"\n          + method.getDeclaringClass().getSimpleName() + \".\" + method.getName()\n          + \"(\" + (args == null || args.length == 0? \"\": Arrays.toString(args))\n          +  \")\";\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.handleException": "  private RetryInfo handleException(final Method method, final int callId,\n      final RetryPolicy policy, final Counters counters,\n      final long expectFailoverCount, final Exception e) throws Exception {\n    final RetryInfo retryInfo = RetryInfo.newRetryInfo(policy, e,\n        counters, proxyDescriptor.idempotentOrAtMostOnce(method),\n        expectFailoverCount);\n    if (retryInfo.isFail()) {\n      // fail.\n      if (retryInfo.action.reason != null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Exception while invoking call #\" + callId + \" \"\n              + proxyDescriptor.getProxyInfo().getString(method.getName())\n              + \". Not retrying because \" + retryInfo.action.reason, e);\n        }\n      }\n      throw e;\n    }\n\n    log(method, retryInfo.isFailover(), counters.failovers, retryInfo.delay, e);\n    return retryInfo;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.getFailoverCount": "  private long getFailoverCount() {\n    return proxyDescriptor.getFailoverCount();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.makeRemoteRequest": "  protected AllocateResponse makeRemoteRequest() throws YarnException,\n      IOException {\n    applyRequestLimits();\n    ResourceBlacklistRequest blacklistRequest =\n        ResourceBlacklistRequest.newInstance(new ArrayList<String>(blacklistAdditions),\n            new ArrayList<String>(blacklistRemovals));\n    AllocateRequest allocateRequest =\n        AllocateRequest.newInstance(lastResponseID,\n          super.getApplicationProgress(), new ArrayList<ResourceRequest>(ask),\n          new ArrayList<ContainerId>(release), blacklistRequest);\n    AllocateResponse allocateResponse = scheduler.allocate(allocateRequest);\n    lastResponseID = allocateResponse.getResponseId();\n    availableResources = allocateResponse.getAvailableResources();\n    lastClusterNmCount = clusterNmCount;\n    clusterNmCount = allocateResponse.getNumClusterNodes();\n    int numCompletedContainers =\n        allocateResponse.getCompletedContainersStatuses().size();\n\n    if (ask.size() > 0 || release.size() > 0) {\n      LOG.info(\"getResources() for \" + applicationId + \":\" + \" ask=\"\n          + ask.size() + \" release= \" + release.size() + \" newContainers=\"\n          + allocateResponse.getAllocatedContainers().size()\n          + \" finishedContainers=\" + numCompletedContainers\n          + \" resourcelimit=\" + availableResources + \" knownNMs=\"\n          + clusterNmCount);\n    }\n\n    ask.clear();\n    release.clear();\n\n    if (numCompletedContainers > 0) {\n      // re-send limited requests when a container completes to trigger asking\n      // for more containers\n      requestLimitsToUpdate.addAll(requestLimits.keySet());\n    }\n\n    if (blacklistAdditions.size() > 0 || blacklistRemovals.size() > 0) {\n      LOG.info(\"Update the blacklist for \" + applicationId +\n          \": blacklistAdditions=\" + blacklistAdditions.size() +\n          \" blacklistRemovals=\" +  blacklistRemovals.size());\n    }\n    blacklistAdditions.clear();\n    blacklistRemovals.clear();\n    return allocateResponse;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.applyRequestLimits": "  private void applyRequestLimits() {\n    Iterator<ResourceRequest> iter = requestLimits.values().iterator();\n    while (iter.hasNext()) {\n      ResourceRequest reqLimit = iter.next();\n      int limit = reqLimit.getNumContainers();\n      Map<String, Map<Resource, ResourceRequest>> remoteRequests =\n          remoteRequestsTable.get(reqLimit.getPriority());\n      Map<Resource, ResourceRequest> reqMap = (remoteRequests != null)\n          ? remoteRequests.get(ResourceRequest.ANY) : null;\n      ResourceRequest req = (reqMap != null)\n          ? reqMap.get(reqLimit.getCapability()) : null;\n      if (req == null) {\n        continue;\n      }\n      // update an existing ask or send a new one if updating\n      if (ask.remove(req) || requestLimitsToUpdate.contains(req)) {\n        ResourceRequest newReq = req.getNumContainers() > limit\n            ? reqLimit : req;\n        ask.add(newReq);\n        LOG.info(\"Applying ask limit of \" + newReq.getNumContainers()\n            + \" for priority:\" + reqLimit.getPriority()\n            + \" and capability:\" + reqLimit.getCapability());\n      }\n      if (limit == Integer.MAX_VALUE) {\n        iter.remove();\n      }\n    }\n    requestLimitsToUpdate.clear();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.getAvailableResources": "  protected Resource getAvailableResources() {\n    return availableResources == null ? Resources.none() : availableResources;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getResources": "  private List<Container> getResources() throws Exception {\n    applyConcurrentTaskLimits();\n\n    // will be null the first time\n    Resource headRoom = Resources.clone(getAvailableResources());\n    AllocateResponse response;\n    /*\n     * If contact with RM is lost, the AM will wait MR_AM_TO_RM_WAIT_INTERVAL_MS\n     * milliseconds before aborting. During this interval, AM will still try\n     * to contact the RM.\n     */\n    try {\n      response = makeRemoteRequest();\n      // Reset retry count if no exception occurred.\n      retrystartTime = System.currentTimeMillis();\n    } catch (ApplicationAttemptNotFoundException e ) {\n      // This can happen if the RM has been restarted. If it is in that state,\n      // this application must clean itself up.\n      eventHandler.handle(new JobEvent(this.getJob().getID(),\n        JobEventType.JOB_AM_REBOOT));\n      throw new RMContainerAllocationException(\n        \"Resource Manager doesn't recognize AttemptId: \"\n            + this.getContext().getApplicationAttemptId(), e);\n    } catch (ApplicationMasterNotRegisteredException e) {\n      LOG.info(\"ApplicationMaster is out of sync with ResourceManager,\"\n          + \" hence resync and send outstanding requests.\");\n      // RM may have restarted, re-register with RM.\n      lastResponseID = 0;\n      register();\n      addOutstandingRequestOnResync();\n      return null;\n    } catch (InvalidLabelResourceRequestException e) {\n      // If Invalid label exception is received means the requested label doesnt\n      // have access so killing job in this case.\n      String diagMsg = \"Requested node-label-expression is invalid: \"\n          + StringUtils.stringifyException(e);\n      LOG.info(diagMsg);\n      JobId jobId = this.getJob().getID();\n      eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n      eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n      throw e;\n    } catch (Exception e) {\n      // This can happen when the connection to the RM has gone down. Keep\n      // re-trying until the retryInterval has expired.\n      if (System.currentTimeMillis() - retrystartTime >= retryInterval) {\n        LOG.error(\"Could not contact RM after \" + retryInterval + \" milliseconds.\");\n        eventHandler.handle(new JobEvent(this.getJob().getID(),\n                                         JobEventType.JOB_AM_REBOOT));\n        throw new RMContainerAllocationException(\"Could not contact RM after \" +\n                                retryInterval + \" milliseconds.\");\n      }\n      // Throw this up to the caller, which may decide to ignore it and\n      // continue to attempt to contact the RM.\n      throw e;\n    }\n    Resource newHeadRoom = getAvailableResources();\n    List<Container> newContainers = response.getAllocatedContainers();\n    // Setting NMTokens\n    if (response.getNMTokens() != null) {\n      for (NMToken nmToken : response.getNMTokens()) {\n        NMTokenCache.setNMToken(nmToken.getNodeId().toString(),\n            nmToken.getToken());\n      }\n    }\n\n    // Setting AMRMToken\n    if (response.getAMRMToken() != null) {\n      updateAMRMToken(response.getAMRMToken());\n    }\n\n    List<ContainerStatus> finishedContainers = response.getCompletedContainersStatuses();\n\n    // propagate preemption requests\n    final PreemptionMessage preemptReq = response.getPreemptionMessage();\n    if (preemptReq != null) {\n      preemptionPolicy.preempt(\n          new PreemptionContext(assignedRequests), preemptReq);\n    }\n\n    if (newContainers.size() + finishedContainers.size() > 0\n        || !headRoom.equals(newHeadRoom)) {\n      //something changed\n      recalculateReduceSchedule = true;\n      if (LOG.isDebugEnabled() && !headRoom.equals(newHeadRoom)) {\n        LOG.debug(\"headroom=\" + newHeadRoom);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      for (Container cont : newContainers) {\n        LOG.debug(\"Received new Container :\" + cont);\n      }\n    }\n\n    //Called on each allocation. Will know about newly blacklisted/added hosts.\n    computeIgnoreBlacklisting();\n\n    handleUpdatedNodes(response);\n    handleJobPriorityChange(response);\n    // handle receiving the timeline collector address for this app\n    String collectorAddr = response.getCollectorAddr();\n    MRAppMaster.RunningAppContext appContext =\n        (MRAppMaster.RunningAppContext)this.getContext();\n    if (collectorAddr != null && !collectorAddr.isEmpty()\n        && appContext.getTimelineClient() != null) {\n      appContext.getTimelineClient().setTimelineServiceAddress(\n          response.getCollectorAddr());\n    }\n\n    for (ContainerStatus cont : finishedContainers) {\n      processFinishedContainer(cont);\n    }\n    return newContainers;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.handle": "  public void handle(ContainerAllocatorEvent event) {\n    int qSize = eventQueue.size();\n    if (qSize != 0 && qSize % 1000 == 0) {\n      LOG.info(\"Size of event-queue in RMContainerAllocator is \" + qSize);\n    }\n    int remCapacity = eventQueue.remainingCapacity();\n    if (remCapacity < 1000) {\n      LOG.warn(\"Very low remaining capacity in the event-queue \"\n          + \"of RMContainerAllocator: \" + remCapacity);\n    }\n    try {\n      eventQueue.put(event);\n    } catch (InterruptedException e) {\n      throw new YarnRuntimeException(e);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.processFinishedContainer": "  void processFinishedContainer(ContainerStatus container) {\n    LOG.info(\"Received completed container \" + container.getContainerId());\n    TaskAttemptId attemptID = assignedRequests.get(container.getContainerId());\n    if (attemptID == null) {\n      LOG.error(\"Container complete event for unknown container \"\n          + container.getContainerId());\n    } else {\n      pendingRelease.remove(container.getContainerId());\n      assignedRequests.remove(attemptID);\n\n      // Send the diagnostics\n      String diagnostic = StringInterner.weakIntern(container.getDiagnostics());\n      eventHandler.handle(new TaskAttemptDiagnosticsUpdateEvent(attemptID,\n          diagnostic));\n\n      // send the container completed event to Task attempt\n      eventHandler.handle(createContainerFinishedEvent(container, attemptID));\n\n      preemptionPolicy.handleCompletedContainer(attemptID);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.handleUpdatedNodes": "  private void handleUpdatedNodes(AllocateResponse response) {\n    // send event to the job about on updated nodes\n    List<NodeReport> updatedNodes = response.getUpdatedNodes();\n    if (!updatedNodes.isEmpty()) {\n\n      // send event to the job to act upon completed tasks\n      eventHandler.handle(new JobUpdatedNodesEvent(getJob().getID(),\n          updatedNodes));\n\n      // act upon running tasks\n      HashSet<NodeId> unusableNodes = new HashSet<NodeId>();\n      for (NodeReport nr : updatedNodes) {\n        NodeState nodeState = nr.getNodeState();\n        if (nodeState.isUnusable()) {\n          unusableNodes.add(nr.getNodeId());\n        }\n      }\n      for (int i = 0; i < 2; ++i) {\n        HashMap<TaskAttemptId, Container> taskSet = i == 0 ? assignedRequests.maps\n            : assignedRequests.reduces;\n        // kill running containers\n        for (Map.Entry<TaskAttemptId, Container> entry : taskSet.entrySet()) {\n          TaskAttemptId tid = entry.getKey();\n          NodeId taskAttemptNodeId = entry.getValue().getNodeId();\n          if (unusableNodes.contains(taskAttemptNodeId)) {\n            LOG.info(\"Killing taskAttempt:\" + tid\n                + \" because it is running on unusable node:\"\n                + taskAttemptNodeId);\n            // If map, reschedule next task attempt.\n            boolean rescheduleNextAttempt = (i == 0) ? true : false;\n            eventHandler.handle(new TaskAttemptKillEvent(tid,\n                \"TaskAttempt killed because it ran on unusable node\"\n                    + taskAttemptNodeId, rescheduleNextAttempt));\n          }\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.updateAMRMToken": "  private void updateAMRMToken(Token token) throws IOException {\n    org.apache.hadoop.security.token.Token<AMRMTokenIdentifier> amrmToken =\n        new org.apache.hadoop.security.token.Token<AMRMTokenIdentifier>(token\n          .getIdentifier().array(), token.getPassword().array(), new Text(\n          token.getKind()), new Text(token.getService()));\n    UserGroupInformation currentUGI = UserGroupInformation.getCurrentUser();\n    currentUGI.addToken(amrmToken);\n    amrmToken.setService(ClientRMProxy.getAMRMTokenService(getConfig()));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.applyConcurrentTaskLimits": "  private void applyConcurrentTaskLimits() {\n    int numScheduledMaps = scheduledRequests.maps.size();\n    if (maxRunningMaps > 0 && numScheduledMaps > 0) {\n      int maxRequestedMaps = Math.max(0,\n          maxRunningMaps - assignedRequests.maps.size());\n      int numScheduledFailMaps = scheduledRequests.earlierFailedMaps.size();\n      int failedMapRequestLimit = Math.min(maxRequestedMaps,\n          numScheduledFailMaps);\n      int normalMapRequestLimit = Math.min(\n          maxRequestedMaps - failedMapRequestLimit,\n          numScheduledMaps - numScheduledFailMaps);\n      setRequestLimit(PRIORITY_FAST_FAIL_MAP, mapResourceRequest,\n          failedMapRequestLimit);\n      setRequestLimit(PRIORITY_MAP, mapResourceRequest, normalMapRequestLimit);\n      setRequestLimit(PRIORITY_OPPORTUNISTIC_MAP, mapResourceRequest,\n          normalMapRequestLimit);\n    }\n\n    int numScheduledReduces = scheduledRequests.reduces.size();\n    if (maxRunningReduces > 0 && numScheduledReduces > 0) {\n      int maxRequestedReduces = Math.max(0,\n          maxRunningReduces - assignedRequests.reduces.size());\n      int reduceRequestLimit = Math.min(maxRequestedReduces,\n          numScheduledReduces);\n      setRequestLimit(PRIORITY_REDUCE, reduceResourceRequest,\n          reduceRequestLimit);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.handleJobPriorityChange": "  private void handleJobPriorityChange(AllocateResponse response) {\n    Priority priorityFromResponse = Priority.newInstance(response\n        .getApplicationPriority().getPriority());\n\n    // Update the job priority to Job directly.\n    getJob().setJobPriority(priorityFromResponse);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat": "  protected synchronized void heartbeat() throws Exception {\n    scheduleStats.updateAndLogIfChanged(\"Before Scheduling: \");\n    List<Container> allocatedContainers = getResources();\n    if (allocatedContainers != null && allocatedContainers.size() > 0) {\n      scheduledRequests.assign(allocatedContainers);\n    }\n\n    int completedMaps = getJob().getCompletedMaps();\n    int completedTasks = completedMaps + getJob().getCompletedReduces();\n    if ((lastCompletedTasks != completedTasks) ||\n          (scheduledRequests.maps.size() > 0)) {\n      lastCompletedTasks = completedTasks;\n      recalculateReduceSchedule = true;\n    }\n\n    if (recalculateReduceSchedule) {\n      boolean reducerPreempted = preemptReducesIfNeeded();\n\n      if (!reducerPreempted) {\n        // Only schedule new reducers if no reducer preemption happens for\n        // this heartbeat\n        scheduleReduces(getJob().getTotalMaps(), completedMaps,\n            scheduledRequests.maps.size(), scheduledRequests.reduces.size(),\n            assignedRequests.maps.size(), assignedRequests.reduces.size(),\n            mapResourceRequest, reduceResourceRequest, pendingReduces.size(),\n            maxReduceRampupLimit, reduceSlowStart);\n      }\n\n      recalculateReduceSchedule = false;\n    }\n\n    scheduleStats.updateAndLogIfChanged(\"After Scheduling: \");\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.assign": "    private void assign(List<Container> allocatedContainers) {\n      Iterator<Container> it = allocatedContainers.iterator();\n      LOG.info(\"Got allocated containers \" + allocatedContainers.size());\n      containersAllocated += allocatedContainers.size();\n      int reducePending = reduces.size();\n      while (it.hasNext()) {\n        Container allocated = it.next();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Assigning container \" + allocated.getId()\n              + \" with priority \" + allocated.getPriority() + \" to NM \"\n              + allocated.getNodeId());\n        }\n        \n        // check if allocated container meets memory requirements \n        // and whether we have any scheduled tasks that need \n        // a container to be assigned\n        boolean isAssignable = true;\n        Priority priority = allocated.getPriority();\n        Resource allocatedResource = allocated.getResource();\n        if (PRIORITY_FAST_FAIL_MAP.equals(priority) \n            || PRIORITY_MAP.equals(priority)\n            || PRIORITY_OPPORTUNISTIC_MAP.equals(priority)) {\n          if (ResourceCalculatorUtils.computeAvailableContainers(allocatedResource,\n              mapResourceRequest, getSchedulerResourceTypes()) <= 0\n              || maps.isEmpty()) {\n            LOG.info(\"Cannot assign container \" + allocated \n                + \" for a map as either \"\n                + \" container memory less than required \" + mapResourceRequest\n                + \" or no pending map tasks - maps.isEmpty=\" \n                + maps.isEmpty()); \n            isAssignable = false; \n          }\n        } \n        else if (PRIORITY_REDUCE.equals(priority)) {\n          if (ResourceCalculatorUtils.computeAvailableContainers(allocatedResource,\n              reduceResourceRequest, getSchedulerResourceTypes()) <= 0\n              || (reducePending <= 0)) {\n            LOG.info(\"Cannot assign container \" + allocated\n                + \" for a reduce as either \"\n                + \" container memory less than required \" + reduceResourceRequest\n                + \" or no pending reduce tasks.\");\n            isAssignable = false;\n          } else {\n            reducePending--;\n          }\n        } else {\n          LOG.warn(\"Container allocated at unwanted priority: \" + priority + \n              \". Returning to RM...\");\n          isAssignable = false;\n        }\n        \n        if(!isAssignable) {\n          // release container if we could not assign it \n          containerNotAssigned(allocated);\n          it.remove();\n          continue;\n        }\n        \n        // do not assign if allocated container is on a  \n        // blacklisted host\n        String allocatedHost = allocated.getNodeId().getHost();\n        if (isNodeBlacklisted(allocatedHost)) {\n          // we need to request for a new container \n          // and release the current one\n          LOG.info(\"Got allocated container on a blacklisted \"\n              + \" host \"+allocatedHost\n              +\". Releasing container \" + allocated);\n\n          // find the request matching this allocated container \n          // and replace it with a new one \n          ContainerRequest toBeReplacedReq = \n              getContainerReqToReplace(allocated);\n          if (toBeReplacedReq != null) {\n            LOG.info(\"Placing a new container request for task attempt \" \n                + toBeReplacedReq.attemptID);\n            ContainerRequest newReq = \n                getFilteredContainerRequest(toBeReplacedReq);\n            decContainerReq(toBeReplacedReq);\n            if (toBeReplacedReq.attemptID.getTaskId().getTaskType() ==\n                TaskType.MAP) {\n              maps.put(newReq.attemptID, newReq);\n            }\n            else {\n              reduces.put(newReq.attemptID, newReq);\n            }\n            addContainerReq(newReq);\n          }\n          else {\n            LOG.info(\"Could not map allocated container to a valid request.\"\n                + \" Releasing allocated container \" + allocated);\n          }\n          \n          // release container if we could not assign it \n          containerNotAssigned(allocated);\n          it.remove();\n          continue;\n        }\n      }\n\n      assignContainers(allocatedContainers);\n       \n      // release container if we could not assign it \n      it = allocatedContainers.iterator();\n      while (it.hasNext()) {\n        Container allocated = it.next();\n        LOG.info(\"Releasing unassigned container \" + allocated);\n        containerNotAssigned(allocated);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.preemptReducesIfNeeded": "  boolean preemptReducesIfNeeded() {\n    if (reduceResourceRequest.equals(Resources.none())) {\n      return false; // no reduces\n    }\n\n    if (assignedRequests.maps.size() > 0) {\n      // there are assigned mappers\n      return false;\n    }\n\n    if (scheduledRequests.maps.size() <= 0) {\n      // there are no pending requests for mappers\n      return false;\n    }\n\n    // At this point:\n    // we have pending mappers and all assigned resources are taken by reducers\n    if (reducerUnconditionalPreemptionDelayMs >= 0) {\n      // Unconditional preemption is enabled.\n      // If mappers are pending for longer than the configured threshold,\n      // preempt reducers irrespective of what the headroom is.\n      if (preemptReducersForHangingMapRequests(\n          reducerUnconditionalPreemptionDelayMs)) {\n        return true;\n      }\n    }\n\n    // The pending mappers haven't been waiting for too long. Let us see if\n    // there are enough resources for a mapper to run. This is calculated by\n    // excluding scheduled reducers from headroom and comparing it against\n    // resources required to run one mapper.\n    Resource scheduledReducesResource = Resources.multiply(\n         reduceResourceRequest, scheduledRequests.reduces.size());\n    Resource availableResourceForMap =\n         Resources.subtract(getAvailableResources(), scheduledReducesResource);\n    if (ResourceCalculatorUtils.computeAvailableContainers(availableResourceForMap,\n        mapResourceRequest, getSchedulerResourceTypes()) > 0) {\n       // Enough room to run a mapper\n      return false;\n    }\n\n    // Available resources are not enough to run mapper. See if we should hold\n    // off before preempting reducers and preempt if okay.\n    return preemptReducersForHangingMapRequests(reducerNoHeadroomPreemptionDelayMs);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.scheduleReduces": "  public void scheduleReduces(\n      int totalMaps, int completedMaps,\n      int scheduledMaps, int scheduledReduces,\n      int assignedMaps, int assignedReduces,\n      Resource mapResourceReqt, Resource reduceResourceReqt,\n      int numPendingReduces,\n      float maxReduceRampupLimit, float reduceSlowStart) {\n    \n    if (numPendingReduces == 0) {\n      return;\n    }\n    \n    // get available resources for this job\n    Resource headRoom = getAvailableResources();\n\n    LOG.info(\"Recalculating schedule, headroom=\" + headRoom);\n    \n    //check for slow start\n    if (!getIsReduceStarted()) {//not set yet\n      int completedMapsForReduceSlowstart = (int)Math.ceil(reduceSlowStart * \n                      totalMaps);\n      if(completedMaps < completedMapsForReduceSlowstart) {\n        LOG.info(\"Reduce slow start threshold not met. \" +\n              \"completedMapsForReduceSlowstart \" + \n            completedMapsForReduceSlowstart);\n        return;\n      } else {\n        LOG.info(\"Reduce slow start threshold reached. Scheduling reduces.\");\n        setIsReduceStarted(true);\n      }\n    }\n    \n    //if all maps are assigned, then ramp up all reduces irrespective of the\n    //headroom\n    if (scheduledMaps == 0 && numPendingReduces > 0) {\n      LOG.info(\"All maps assigned. \" +\n          \"Ramping up all remaining reduces:\" + numPendingReduces);\n      scheduleAllReduces();\n      return;\n    }\n\n    float completedMapPercent = 0f;\n    if (totalMaps != 0) {//support for 0 maps\n      completedMapPercent = (float)completedMaps/totalMaps;\n    } else {\n      completedMapPercent = 1;\n    }\n    \n    Resource netScheduledMapResource =\n        Resources.multiply(mapResourceReqt, (scheduledMaps + assignedMaps));\n\n    Resource netScheduledReduceResource =\n        Resources.multiply(reduceResourceReqt,\n          (scheduledReduces + assignedReduces));\n\n    Resource finalMapResourceLimit;\n    Resource finalReduceResourceLimit;\n\n    // ramp up the reduces based on completed map percentage\n    Resource totalResourceLimit = getResourceLimit();\n\n    Resource idealReduceResourceLimit =\n        Resources.multiply(totalResourceLimit,\n          Math.min(completedMapPercent, maxReduceRampupLimit));\n    Resource ideaMapResourceLimit =\n        Resources.subtract(totalResourceLimit, idealReduceResourceLimit);\n\n    // check if there aren't enough maps scheduled, give the free map capacity\n    // to reduce.\n    // Even when container number equals, there may be unused resources in one\n    // dimension\n    if (ResourceCalculatorUtils.computeAvailableContainers(ideaMapResourceLimit,\n      mapResourceReqt, getSchedulerResourceTypes()) >= (scheduledMaps + assignedMaps)) {\n      // enough resource given to maps, given the remaining to reduces\n      Resource unusedMapResourceLimit =\n          Resources.subtract(ideaMapResourceLimit, netScheduledMapResource);\n      finalReduceResourceLimit =\n          Resources.add(idealReduceResourceLimit, unusedMapResourceLimit);\n      finalMapResourceLimit =\n          Resources.subtract(totalResourceLimit, finalReduceResourceLimit);\n    } else {\n      finalMapResourceLimit = ideaMapResourceLimit;\n      finalReduceResourceLimit = idealReduceResourceLimit;\n    }\n\n    LOG.info(\"completedMapPercent \" + completedMapPercent\n        + \" totalResourceLimit:\" + totalResourceLimit\n        + \" finalMapResourceLimit:\" + finalMapResourceLimit\n        + \" finalReduceResourceLimit:\" + finalReduceResourceLimit\n        + \" netScheduledMapResource:\" + netScheduledMapResource\n        + \" netScheduledReduceResource:\" + netScheduledReduceResource);\n\n    int rampUp =\n        ResourceCalculatorUtils.computeAvailableContainers(Resources.subtract(\n                finalReduceResourceLimit, netScheduledReduceResource),\n            reduceResourceReqt, getSchedulerResourceTypes());\n\n    if (rampUp > 0) {\n      rampUp = Math.min(rampUp, numPendingReduces);\n      LOG.info(\"Ramping up \" + rampUp);\n      rampUpReduces(rampUp);\n    } else if (rampUp < 0) {\n      int rampDown = -1 * rampUp;\n      rampDown = Math.min(rampDown, scheduledReduces);\n      LOG.info(\"Ramping down \" + rampDown);\n      rampDownReduces(rampDown);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.updateAndLogIfChanged": "    public void updateAndLogIfChanged(String msgPrefix) {\n      boolean changed = false;\n\n      // synchronized to fix findbug warnings\n      synchronized (RMContainerAllocator.this) {\n        changed |= (numPendingReduces != pendingReduces.size());\n        numPendingReduces = pendingReduces.size();\n        changed |= (numScheduledMaps != scheduledRequests.maps.size());\n        numScheduledMaps = scheduledRequests.maps.size();\n        changed |= (numScheduledReduces != scheduledRequests.reduces.size());\n        numScheduledReduces = scheduledRequests.reduces.size();\n        changed |= (numAssignedMaps != assignedRequests.maps.size());\n        numAssignedMaps = assignedRequests.maps.size();\n        changed |= (numAssignedReduces != assignedRequests.reduces.size());\n        numAssignedReduces = assignedRequests.reduces.size();\n        changed |= (numCompletedMaps != getJob().getCompletedMaps());\n        numCompletedMaps = getJob().getCompletedMaps();\n        changed |= (numCompletedReduces != getJob().getCompletedReduces());\n        numCompletedReduces = getJob().getCompletedReduces();\n        changed |= (numContainersAllocated != containersAllocated);\n        numContainersAllocated = containersAllocated;\n        changed |= (numContainersReleased != containersReleased);\n        numContainersReleased = containersReleased;\n      }\n\n      if (changed) {\n        log(msgPrefix);\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator.run": "    public void run() {\n      while (!stopped.get() && !Thread.currentThread().isInterrupted()) {\n        try {\n          Thread.sleep(rmPollInterval);\n          try {\n            heartbeat();\n          } catch (RMContainerAllocationException e) {\n            LOG.error(\"Error communicating with RM: \" + e.getMessage() , e);\n            return;\n          } catch (Exception e) {\n            LOG.error(\"ERROR IN CONTACTING RM. \", e);\n            continue;\n            // TODO: for other exceptions\n          }\n\n          lastHeartbeatTime = context.getClock().getTime();\n          executeHeartbeatCallbacks();\n        } catch (InterruptedException e) {\n          if (!stopped.get()) {\n            LOG.warn(\"Allocated thread interrupted. Returning.\");\n          }\n          return;\n        }\n      }\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator.executeHeartbeatCallbacks": "  private void executeHeartbeatCallbacks() {\n    Runnable callback = null;\n    while ((callback = heartbeatCallbacks.poll()) != null) {\n      callback.run();\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator.heartbeat": "  protected abstract void heartbeat() throws Exception;\n\n  private void executeHeartbeatCallbacks() {\n    Runnable callback = null;\n    while ((callback = heartbeatCallbacks.poll()) != null) {\n      callback.run();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.readResponse": "    public ByteBuffer readResponse() throws IOException {\n      int length = in.readInt();\n      if (firstResponse) {\n        firstResponse = false;\n        // pre-rpcv9 exception, almost certainly a version mismatch.\n        if (length == -1) {\n          in.readInt(); // ignore fatal/error status, it's fatal for us.\n          throw new RemoteException(WritableUtils.readString(in),\n                                    WritableUtils.readString(in));\n        }\n      }\n      if (length <= 0) {\n        throw new RpcException(\"RPC response has invalid length\");\n      }\n      if (maxResponseLength > 0 && length > maxResponseLength) {\n        throw new RpcException(\"RPC response exceeds maximum data length\");\n      }\n      ByteBuffer bb = ByteBuffer.allocate(length);\n      in.readFully(bb.array());\n      return bb;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.receiveRpcResponse": "    private void receiveRpcResponse() {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n      touch();\n      \n      try {\n        ByteBuffer bb = ipcStreams.readResponse();\n        RpcWritable.Buffer packet = RpcWritable.Buffer.wrap(bb);\n        RpcResponseHeaderProto header =\n            packet.getValue(RpcResponseHeaderProto.getDefaultInstance());\n        checkResponse(header);\n\n        int callId = header.getCallId();\n        if (LOG.isDebugEnabled())\n          LOG.debug(getName() + \" got value #\" + callId);\n\n        RpcStatusProto status = header.getStatus();\n        if (status == RpcStatusProto.SUCCESS) {\n          Writable value = packet.newInstance(valueClass, conf);\n          final Call call = calls.remove(callId);\n          call.setRpcResponse(value);\n        }\n        // verify that packet length was correct\n        if (packet.remaining() > 0) {\n          throw new RpcClientException(\"RPC response length mismatch\");\n        }\n        if (status != RpcStatusProto.SUCCESS) { // Rpc Request failed\n          final String exceptionClassName = header.hasExceptionClassName() ?\n                header.getExceptionClassName() : \n                  \"ServerDidNotSetExceptionClassName\";\n          final String errorMsg = header.hasErrorMsg() ? \n                header.getErrorMsg() : \"ServerDidNotSetErrorMsg\" ;\n          final RpcErrorCodeProto erCode = \n                    (header.hasErrorDetail() ? header.getErrorDetail() : null);\n          if (erCode == null) {\n             LOG.warn(\"Detailed error code not set by server on rpc error\");\n          }\n          RemoteException re = new RemoteException(exceptionClassName, errorMsg, erCode);\n          if (status == RpcStatusProto.ERROR) {\n            final Call call = calls.remove(callId);\n            call.setException(re);\n          } else if (status == RpcStatusProto.FATAL) {\n            // Close the connection\n            markClosed(re);\n          }\n        }\n      } catch (IOException e) {\n        markClosed(e);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setException": "    public synchronized void setException(IOException error) {\n      this.error = error;\n      callComplete();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.get": "        public Writable get(long timeout, TimeUnit unit)\n            throws IOException, TimeoutException{\n          boolean done = true;\n          try {\n            final Writable w = getRpcResponse(call, connection, timeout, unit);\n            if (w == null) {\n              done = false;\n              throw new TimeoutException(call + \" timed out \"\n                  + timeout + \" \" + unit);\n            }\n            return w;\n          } finally {\n            if (done) {\n              releaseAsyncCall();\n            }\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.checkResponse": "  void checkResponse(RpcResponseHeaderProto header) throws IOException {\n    if (header == null) {\n      throw new EOFException(\"Response is null.\");\n    }\n    if (header.hasClientId()) {\n      // check client IDs\n      final byte[] id = header.getClientId().toByteArray();\n      if (!Arrays.equals(id, RpcConstants.DUMMY_CLIENT_ID)) {\n        if (!Arrays.equals(id, clientId)) {\n          throw new IOException(\"Client IDs not matched: local ID=\"\n              + StringUtils.byteToHexString(clientId) + \", ID in response=\"\n              + StringUtils.byteToHexString(header.getClientId().toByteArray()));\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setRpcResponse": "    public synchronized void setRpcResponse(Writable rpcResponse) {\n      this.rpcResponse = rpcResponse;\n      callComplete();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.markClosed": "    private synchronized void markClosed(IOException e) {\n      if (shouldCloseConnection.compareAndSet(false, true)) {\n        closeException = e;\n        notifyAll();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.touch": "    private void touch() {\n      lastActivity.set(Time.now());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.run": "          public void run() {\n            try {\n              synchronized (ipcStreams.out) {\n                if (shouldCloseConnection.get()) {\n                  return;\n                }\n                if (LOG.isDebugEnabled()) {\n                  LOG.debug(getName() + \" sending #\" + call.id);\n                }\n                // RpcRequestHeader + RpcRequest\n                ipcStreams.sendRequest(buf.toByteArray());\n                ipcStreams.flush();\n              }\n            } catch (IOException e) {\n              // exception at this point would leave the connection in an\n              // unrecoverable state (eg half a call left on the wire).\n              // So, close the connection, killing any outstanding calls\n              markClosed(e);\n            } finally {\n              //the buffer is just an in-memory buffer, but it is still polite to\n              // close early\n              IOUtils.closeStream(buf);\n            }\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupSaslConnection": "    private synchronized AuthMethod setupSaslConnection(IpcStreams streams)\n        throws IOException {\n      // Do not use Client.conf here! We must use ConnectionId.conf, since the\n      // Client object is cached and shared between all RPC clients, even those\n      // for separate services.\n      saslRpcClient = new SaslRpcClient(remoteId.getTicket(),\n          remoteId.getProtocol(), remoteId.getAddress(), remoteId.conf);\n      return saslRpcClient.saslConnect(streams);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.disposeSasl": "    private synchronized void disposeSasl() {\n      if (saslRpcClient != null) {\n        try {\n          saslRpcClient.dispose();\n          saslRpcClient = null;\n        } catch (IOException ignored) {\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.waitForWork": "    private synchronized boolean waitForWork() {\n      if (calls.isEmpty() && !shouldCloseConnection.get()  && running.get())  {\n        long timeout = maxIdleTime-\n              (Time.now()-lastActivity.get());\n        if (timeout>0) {\n          try {\n            wait(timeout);\n          } catch (InterruptedException e) {}\n        }\n      }\n      \n      if (!calls.isEmpty() && !shouldCloseConnection.get() && running.get()) {\n        return true;\n      } else if (shouldCloseConnection.get()) {\n        return false;\n      } else if (calls.isEmpty()) { // idle connection closed or stopped\n        markClosed(null);\n        return false;\n      } else { // get stopped but there are still pending requests \n        markClosed((IOException)new IOException().initCause(\n            new InterruptedException()));\n        return false;\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.sendRequest": "    public void sendRequest(byte[] buf) throws IOException {\n      out.write(buf);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.shouldAuthenticateOverKrb": "    private synchronized boolean shouldAuthenticateOverKrb() throws IOException {\n      UserGroupInformation loginUser = UserGroupInformation.getLoginUser();\n      UserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\n      UserGroupInformation realUser = currentUser.getRealUser();\n      if (authMethod == AuthMethod.KERBEROS && loginUser != null &&\n      // Make sure user logged in using Kerberos either keytab or TGT\n          loginUser.hasKerberosCredentials() &&\n          // relogin only in case it is the login user (e.g. JT)\n          // or superuser (like oozie).\n          (loginUser.equals(currentUser) || loginUser.equals(realUser))) {\n        return true;\n      }\n      return false;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.closeConnection": "    private void closeConnection() {\n      if (socket == null) {\n        return;\n      }\n      // close the current connection\n      try {\n        socket.close();\n      } catch (IOException e) {\n        LOG.warn(\"Not able to close a socket\", e);\n      }\n      // set socket to null so that the next call to setupIOstreams\n      // can start the process of connect all over again.\n      socket = null;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.flush": "    public void flush() throws IOException {\n      out.flush();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.close": "    public void close() {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(in);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcClientUtil.methodToTraceString": "  public static String methodToTraceString(Method method) {\n    Class<?> clazz = method.getDeclaringClass();\n    while (true) {\n      Class<?> next = clazz.getEnclosingClass();\n      if (next == null || next.getEnclosingClass() == null) break;\n      clazz = next;\n    }\n    return clazz.getSimpleName() + \"#\" + method.getName();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getAsyncRpcResponse": "  public static <T extends Writable> AsyncGet<T, IOException>\n      getAsyncRpcResponse() {\n    return (AsyncGet<T, IOException>) ASYNC_RPC_RESPONSE.get();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcWritable.newInstance": "    public <T> T newInstance(Class<T> valueClass,\n        Configuration conf) throws IOException {\n      T instance;\n      try {\n        // this is much faster than ReflectionUtils!\n        instance = valueClass.newInstance();\n        if (instance instanceof Configurable) {\n          ((Configurable)instance).setConf(conf);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      }\n      return getValue(instance);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcWritable.getValue": "    public <T> T getValue(T value) throws IOException {\n      return RpcWritable.wrap(value).readFrom(bb);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcWritable.remaining": "    public int remaining() {\n      return bb.remaining();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ResponseBuffer.toByteArray": "  byte[] toByteArray() {\n    return getFramedBuffer().toByteArray();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ResponseBuffer.getFramedBuffer": "  private FramedBuffer getFramedBuffer() {\n    FramedBuffer buf = (FramedBuffer)out;\n    buf.setSize(written);\n    return buf;\n  }"
        },
        "bug_report": {
            "Title": "ApplicationMasterProtocolPBClientImpl.allocate fails with EOFException when RPC privacy is enabled",
            "Description": "When privacy is enabled for RPC (hadoop.rpc.protection = privacy), {{ApplicationMasterProtocolPBClientImpl.allocate}} sometimes (but not always) fails with an EOFException. I've reproduced this with Spark 2.0.2 built against latest branch-2.8 and with a simple distcp job on latest branch-2.8.\n\nSteps to reproduce using distcp:\n\n1. Set hadoop.rpc.protection equal to privacy\n2. Write data to HDFS. I did this with Spark as follows: \n\n{code}\nsc.parallelize(1 to (5*1024*1024)).map(k => Seq(k, org.apache.commons.lang.RandomStringUtils.random(1024, \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWxyZ0123456789\")).mkString(\"|\")).toDF().repartition(100).write.parquet(\"hdfs:///tmp/testData\")\n{code}\n\n3. Attempt to distcp that data to another location in HDFS. For example:\n\n{code}\nhadoop distcp -Dmapreduce.framework.name=yarn hdfs:///tmp/testData hdfs:///tmp/testDataCopy\n{code}\n\nI observed this error in the ApplicationMaster's syslog:\n\n{code}\n2016-12-19 19:13:50,097 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Event Writer setup for JobId: job_1482189777425_0004, File: hdfs://<namenode_host>:8020/tmp/hadoop-yarn/staging/<hdfs_user>/.staging/job_1482189777425_0004/job_1482189777425_0004_1.jhist\n2016-12-19 19:13:51,004 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:4 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0\n2016-12-19 19:13:51,031 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1482189777425_0004: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:22528, vCores:23> knownNMs=3\n2016-12-19 19:13:52,043 INFO [RMCommunicator Allocator] org.apache.hadoop.io.retry.RetryInvocationHandler: Exception while invoking ApplicationMasterProtocolPBClientImpl.allocate over null. Retrying after sleeping for 30000ms.\njava.io.EOFException: End of File Exception between local host is: \"<application_master_host>/<ip_addr>\"; destination host is: \"<rm_host>\":8030; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1486)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1428)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1338)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy80.allocate(Unknown Source)\n\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)\n\tat com.sun.proxy.$Proxy81.allocate(Unknown Source)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.makeRemoteRequest(RMContainerRequestor.java:204)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getResources(RMContainerAllocator.java:735)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:269)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$AllocatorRunnable.run(RMCommunicator.java:281)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1785)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1156)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)\n{code}\n\nMarking as \"critical\" since this blocks YARN users from encrypting RPC in their Hadoop clusters."
        }
    },
    {
        "filename": "HADOOP-11149.json",
        "creation_time": "2014-09-27T16:37:06.000+0000",
        "stack_trace": "```\njava.lang.Exception: test timed out after 25000 milliseconds\n\tat java.lang.Object.wait(Native Method)\n\tat org.apache.hadoop.ha.ZKFailoverController.waitForActiveAttempt(ZKFailoverController.java:467)\n\tat org.apache.hadoop.ha.ZKFailoverController.doGracefulFailover(ZKFailoverController.java:657)\n\tat org.apache.hadoop.ha.ZKFailoverController.access$400(ZKFailoverController.java:61)\n\tat org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:602)\n\tat org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:599)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1621)\n\tat org.apache.hadoop.ha.ZKFailoverController.gracefulFailoverToYou(ZKFailoverController.java:599)\n\tat org.apache.hadoop.ha.ZKFCRpcServer.gracefulFailover(ZKFCRpcServer.java:94)\n\tat org.apache.hadoop.ha.TestZKFailoverController.testGracefulFailover(TestZKFailoverController.java:448)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.ZKFailoverController.waitForActiveAttempt": "  private ActiveAttemptRecord waitForActiveAttempt(int timeoutMillis)\n      throws InterruptedException {\n    long st = System.nanoTime();\n    long waitUntil = st + TimeUnit.NANOSECONDS.convert(\n        timeoutMillis, TimeUnit.MILLISECONDS);\n    \n    do {\n      // periodically check health state, because entering an\n      // unhealthy state could prevent us from ever attempting to\n      // become active. We can detect this and respond to the user\n      // immediately.\n      synchronized (this) {\n        if (lastHealthState != State.SERVICE_HEALTHY) {\n          // early out if service became unhealthy\n          return null;\n        }\n      }\n\n      synchronized (activeAttemptRecordLock) {\n        if ((lastActiveAttemptRecord != null &&\n            lastActiveAttemptRecord.nanoTime >= st)) {\n          return lastActiveAttemptRecord;\n        }\n        // Only wait 1sec so that we periodically recheck the health state\n        // above.\n        activeAttemptRecordLock.wait(1000);\n      }\n    } while (System.nanoTime() < waitUntil);\n    \n    // Timeout elapsed.\n    LOG.warn(timeoutMillis + \"ms timeout elapsed waiting for an attempt \" +\n        \"to become active\");\n    return null;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.ZKFailoverController.doGracefulFailover": "  private void doGracefulFailover()\n      throws ServiceFailedException, IOException, InterruptedException {\n    int timeout = FailoverController.getGracefulFenceTimeout(conf) * 2;\n    \n    // Phase 1: pre-flight checks\n    checkEligibleForFailover();\n    \n    // Phase 2: determine old/current active node. Check that we're not\n    // ourselves active, etc.\n    HAServiceTarget oldActive = getCurrentActive();\n    if (oldActive == null) {\n      // No node is currently active. So, if we aren't already\n      // active ourselves by means of a normal election, then there's\n      // probably something preventing us from becoming active.\n      throw new ServiceFailedException(\n          \"No other node is currently active.\");\n    }\n    \n    if (oldActive.getAddress().equals(localTarget.getAddress())) {\n      LOG.info(\"Local node \" + localTarget + \" is already active. \" +\n          \"No need to failover. Returning success.\");\n      return;\n    }\n    \n    // Phase 3: ask the old active to yield from the election.\n    LOG.info(\"Asking \" + oldActive + \" to cede its active state for \" +\n        timeout + \"ms\");\n    ZKFCProtocol oldZkfc = oldActive.getZKFCProxy(conf, timeout);\n    oldZkfc.cedeActive(timeout);\n\n    // Phase 4: wait for the normal election to make the local node\n    // active.\n    ActiveAttemptRecord attempt = waitForActiveAttempt(timeout + 60000);\n    \n    if (attempt == null) {\n      // We didn't even make an attempt to become active.\n      synchronized(this) {\n        if (lastHealthState != State.SERVICE_HEALTHY) {\n          throw new ServiceFailedException(\"Unable to become active. \" +\n            \"Service became unhealthy while trying to failover.\");          \n        }\n      }\n      \n      throw new ServiceFailedException(\"Unable to become active. \" +\n          \"Local node did not get an opportunity to do so from ZooKeeper, \" +\n          \"or the local node took too long to transition to active.\");\n    }\n\n    // Phase 5. At this point, we made some attempt to become active. So we\n    // can tell the old active to rejoin if it wants. This allows a quick\n    // fail-back if we immediately crash.\n    oldZkfc.cedeActive(-1);\n    \n    if (attempt.succeeded) {\n      LOG.info(\"Successfully became active. \" + attempt.status);\n    } else {\n      // Propagate failure\n      String msg = \"Failed to become active. \" + attempt.status;\n      throw new ServiceFailedException(msg);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.ZKFailoverController.checkEligibleForFailover": "  private synchronized void checkEligibleForFailover()\n      throws ServiceFailedException {\n    // Check health\n    if (this.getLastHealthState() != State.SERVICE_HEALTHY) {\n      throw new ServiceFailedException(\n          localTarget + \" is not currently healthy. \" +\n          \"Cannot be failover target\");\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.ZKFailoverController.cedeActive": "  void cedeActive(final int millisToCede)\n      throws AccessControlException, ServiceFailedException, IOException {\n    try {\n      UserGroupInformation.getLoginUser().doAs(new PrivilegedExceptionAction<Void>() {\n        @Override\n        public Void run() throws Exception {\n          doCedeActive(millisToCede);\n          return null;\n        }\n      });\n    } catch (InterruptedException e) {\n      throw new IOException(e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.ZKFailoverController.getCurrentActive": "  private HAServiceTarget getCurrentActive()\n      throws IOException, InterruptedException {\n    synchronized (elector) {\n      synchronized (this) {\n        byte[] activeData;\n        try {\n          activeData = elector.getActiveData();\n        } catch (ActiveNotFoundException e) {\n          return null;\n        } catch (KeeperException ke) {\n          throw new IOException(\n              \"Unexpected ZooKeeper issue fetching active node info\", ke);\n        }\n        \n        HAServiceTarget oldActive = dataToTarget(activeData);\n        return oldActive;\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.ZKFailoverController.run": "          public void run() {\n            try {\n              recheckElectability();\n            } catch (Throwable t) {\n              fatalError(\"Failed to recheck electability: \" +\n                  StringUtils.stringifyException(t));\n            }\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.ZKFailoverController.loginAsFCUser": "  protected abstract void loginAsFCUser() throws IOException;\n  protected abstract void checkRpcAdminAccess()\n      throws AccessControlException, IOException;\n  protected abstract InetSocketAddress getRpcAddressToBindTo();\n  protected abstract PolicyProvider getPolicyProvider();\n\n  /**\n   * Return the name of a znode inside the configured parent znode in which\n   * the ZKFC will do all of its work. This is so that multiple federated\n   * nameservices can run on the same ZK quorum without having to manually\n   * configure them to separate subdirectories.\n   */\n  protected abstract String getScopeInsideParentNode();\n\n  public HAServiceTarget getLocalTarget() {\n    return localTarget;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.ZKFailoverController.recheckElectability": "  private void recheckElectability() {\n    // Maintain lock ordering of elector -> ZKFC\n    synchronized (elector) {\n      synchronized (this) {\n        boolean healthy = lastHealthState == State.SERVICE_HEALTHY;\n    \n        long remainingDelay = delayJoiningUntilNanotime - System.nanoTime(); \n        if (remainingDelay > 0) {\n          if (healthy) {\n            LOG.info(\"Would have joined master election, but this node is \" +\n                \"prohibited from doing so for \" +\n                TimeUnit.NANOSECONDS.toMillis(remainingDelay) + \" more ms\");\n          }\n          scheduleRecheck(remainingDelay);\n          return;\n        }\n    \n        switch (lastHealthState) {\n        case SERVICE_HEALTHY:\n          elector.joinElection(targetToData(localTarget));\n          if (quitElectionOnBadState) {\n            quitElectionOnBadState = false;\n          }\n          break;\n          \n        case INITIALIZING:\n          LOG.info(\"Ensuring that \" + localTarget + \" does not \" +\n              \"participate in active master election\");\n          elector.quitElection(false);\n          serviceState = HAServiceState.INITIALIZING;\n          break;\n    \n        case SERVICE_UNHEALTHY:\n        case SERVICE_NOT_RESPONDING:\n          LOG.info(\"Quitting master election for \" + localTarget +\n              \" and marking that fencing is necessary\");\n          elector.quitElection(true);\n          serviceState = HAServiceState.INITIALIZING;\n          break;\n          \n        case HEALTH_MONITOR_FAILED:\n          fatalError(\"Health monitor failed!\");\n          break;\n          \n        default:\n          throw new IllegalArgumentException(\"Unhandled state:\" + lastHealthState);\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.ZKFailoverController.doCedeActive": "  private void doCedeActive(int millisToCede) \n      throws AccessControlException, ServiceFailedException, IOException {\n    int timeout = FailoverController.getGracefulFenceTimeout(conf);\n\n    // Lock elector to maintain lock ordering of elector -> ZKFC\n    synchronized (elector) {\n      synchronized (this) {\n        if (millisToCede <= 0) {\n          delayJoiningUntilNanotime = 0;\n          recheckElectability();\n          return;\n        }\n  \n        LOG.info(\"Requested by \" + UserGroupInformation.getCurrentUser() +\n            \" at \" + Server.getRemoteAddress() + \" to cede active role.\");\n        boolean needFence = false;\n        try {\n          localTarget.getProxy(conf, timeout).transitionToStandby(createReqInfo());\n          LOG.info(\"Successfully ensured local node is in standby mode\");\n        } catch (IOException ioe) {\n          LOG.warn(\"Unable to transition local node to standby: \" +\n              ioe.getLocalizedMessage());\n          LOG.warn(\"Quitting election but indicating that fencing is \" +\n              \"necessary\");\n          needFence = true;\n        }\n        delayJoiningUntilNanotime = System.nanoTime() +\n            TimeUnit.MILLISECONDS.toNanos(millisToCede);\n        elector.quitElection(needFence);\n        serviceState = HAServiceState.INITIALIZING;\n      }\n    }\n    recheckElectability();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.ZKFailoverController.fatalError": "  private synchronized void fatalError(String err) {\n    LOG.fatal(\"Fatal error occurred:\" + err);\n    fatalError = err;\n    notifyAll();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.ZKFailoverController.doRun": "  private int doRun(String[] args)\n      throws HadoopIllegalArgumentException, IOException, InterruptedException {\n    try {\n      initZK();\n    } catch (KeeperException ke) {\n      LOG.fatal(\"Unable to start failover controller. Unable to connect \"\n          + \"to ZooKeeper quorum at \" + zkQuorum + \". Please check the \"\n          + \"configured value for \" + ZK_QUORUM_KEY + \" and ensure that \"\n          + \"ZooKeeper is running.\");\n      return ERR_CODE_NO_ZK;\n    }\n    if (args.length > 0) {\n      if (\"-formatZK\".equals(args[0])) {\n        boolean force = false;\n        boolean interactive = true;\n        for (int i = 1; i < args.length; i++) {\n          if (\"-force\".equals(args[i])) {\n            force = true;\n          } else if (\"-nonInteractive\".equals(args[i])) {\n            interactive = false;\n          } else {\n            badArg(args[i]);\n          }\n        }\n        return formatZK(force, interactive);\n      } else {\n        badArg(args[0]);\n      }\n    }\n\n    if (!elector.parentZNodeExists()) {\n      LOG.fatal(\"Unable to start failover controller. \"\n          + \"Parent znode does not exist.\\n\"\n          + \"Run with -formatZK flag to initialize ZooKeeper.\");\n      return ERR_CODE_NO_PARENT_ZNODE;\n    }\n\n    try {\n      localTarget.checkFencingConfigured();\n    } catch (BadFencingConfigurationException e) {\n      LOG.fatal(\"Fencing is not configured for \" + localTarget + \".\\n\" +\n          \"You must configure a fencing method before using automatic \" +\n          \"failover.\", e);\n      return ERR_CODE_NO_FENCER;\n    }\n\n    initRPC();\n    initHM();\n    startRPC();\n    try {\n      mainLoop();\n    } finally {\n      rpcServer.stopAndJoin();\n      \n      elector.quitElection(true);\n      healthMonitor.shutdown();\n      healthMonitor.join();\n    }\n    return 0;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.ZKFailoverController.gracefulFailoverToYou": "  void gracefulFailoverToYou() throws ServiceFailedException, IOException {\n    try {\n      UserGroupInformation.getLoginUser().doAs(new PrivilegedExceptionAction<Void>() {\n        @Override\n        public Void run() throws Exception {\n          doGracefulFailover();\n          return null;\n        }\n        \n      });\n    } catch (InterruptedException e) {\n      throw new IOException(e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.ZKFCRpcServer.gracefulFailover": "  public void gracefulFailover() throws IOException, AccessControlException {\n    zkfc.checkRpcAdminAccess();\n    zkfc.gracefulFailoverToYou();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.HAServiceTarget.getZKFCProxy": "  public ZKFCProtocol getZKFCProxy(Configuration conf, int timeoutMs)\n      throws IOException {\n    Configuration confCopy = new Configuration(conf);\n    // Lower the timeout so we quickly fail to connect\n    confCopy.setInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY, 1);\n    SocketFactory factory = NetUtils.getDefaultSocketFactory(confCopy);\n    return new ZKFCProtocolClientSideTranslatorPB(\n        getZKFCAddress(),\n        confCopy, factory, timeoutMs);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.HAServiceTarget.getZKFCAddress": "  public abstract InetSocketAddress getZKFCAddress();\n\n  /**\n   * @return a Fencer implementation configured for this target node\n   */\n  public abstract NodeFencer getFencer();\n  \n  /**\n   * @throws BadFencingConfigurationException if the fencing configuration\n   * appears to be invalid. This is divorced from the above\n   * {@link #getFencer()} method so that the configuration can be checked",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.HAServiceTarget.getAddress": "  public abstract InetSocketAddress getAddress();\n\n  /**\n   * @return the IPC address of the ZKFC on the target node\n   */\n  public abstract InetSocketAddress getZKFCAddress();\n\n  /**\n   * @return a Fencer implementation configured for this target node\n   */\n  public abstract NodeFencer getFencer();\n  \n  /**\n   * @throws BadFencingConfigurationException if the fencing configuration\n   * appears to be invalid. This is divorced from the above\n   * {@link #getFencer()} method so that the configuration can be checked",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.FailoverController.getGracefulFenceTimeout": "  static int getGracefulFenceTimeout(Configuration conf) {\n    return conf.getInt(\n        CommonConfigurationKeys.HA_FC_GRACEFUL_FENCE_TIMEOUT_KEY,\n        CommonConfigurationKeys.HA_FC_GRACEFUL_FENCE_TIMEOUT_DEFAULT);\n  }"
        },
        "bug_report": {
            "Title": "Increase the timeout of TestZKFailoverController",
            "Description": "{code}\nRunning org.apache.hadoop.ha.TestZKFailoverController\nTests run: 19, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 56.875 sec <<< FAILURE! - in org.apache.hadoop.ha.TestZKFailoverController\ntestGracefulFailover(org.apache.hadoop.ha.TestZKFailoverController)  Time elapsed: 25.045 sec  <<< ERROR!\njava.lang.Exception: test timed out after 25000 milliseconds\n\tat java.lang.Object.wait(Native Method)\n\tat org.apache.hadoop.ha.ZKFailoverController.waitForActiveAttempt(ZKFailoverController.java:467)\n\tat org.apache.hadoop.ha.ZKFailoverController.doGracefulFailover(ZKFailoverController.java:657)\n\tat org.apache.hadoop.ha.ZKFailoverController.access$400(ZKFailoverController.java:61)\n\tat org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:602)\n\tat org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:599)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1621)\n\tat org.apache.hadoop.ha.ZKFailoverController.gracefulFailoverToYou(ZKFailoverController.java:599)\n\tat org.apache.hadoop.ha.ZKFCRpcServer.gracefulFailover(ZKFCRpcServer.java:94)\n\tat org.apache.hadoop.ha.TestZKFailoverController.testGracefulFailover(TestZKFailoverController.java:448)\n\n\nResults :\n\nTests in error:\n  TestZKFailoverController.testGracefulFailover:448->Object.wait:-2 \u00bb  test time...\n{code}\n\nRunning on centos6.5"
        }
    },
    {
        "filename": "HADOOP-15059.json",
        "creation_time": "2017-11-21T20:54:52.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: Unable to determine current user\n\tat org.apache.hadoop.conf.Configuration$Resource.getRestrictParserDefault(Configuration.java:254)\n\tat org.apache.hadoop.conf.Configuration$Resource.<init>(Configuration.java:220)\n\tat org.apache.hadoop.conf.Configuration$Resource.<init>(Configuration.java:212)\n\tat org.apache.hadoop.conf.Configuration.addResource(Configuration.java:888)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1638)\nCaused by: java.io.IOException: Exception reading /tmp/nm-local-dir/usercache/jdu/appcache/application_1511295641738_0003/container_e03_1511295641738_0003_01_000001/container_tokens\n\tat org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:208)\n\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:907)\n\tat org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:820)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:689)\n\tat org.apache.hadoop.conf.Configuration$Resource.getRestrictParserDefault(Configuration.java:252)\n\t... 4 more\nCaused by: java.io.IOException: Unknown version 1 in token storage.\n\tat org.apache.hadoop.security.Credentials.readTokenStorageStream(Credentials.java:226)\n\tat org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:205)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getRestrictParserDefault": "    private static boolean getRestrictParserDefault(Object resource) {\n      if (resource instanceof String) {\n        return false;\n      }\n      UserGroupInformation user;\n      try {\n        user = UserGroupInformation.getCurrentUser();\n      } catch (IOException e) {\n        throw new RuntimeException(\"Unable to determine current user\", e);\n      }\n      return user.getRealUser() != null;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.addResource": "  public void addResource(Configuration conf) {\n    addResourceObject(new Resource(conf.getProps(), conf.restrictSystemProps));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getProps": "  protected synchronized Properties getProps() {\n    if (properties == null) {\n      properties = new Properties();\n      Map<String, String[]> backup = updatingResource != null ?\n          new ConcurrentHashMap<String, String[]>(updatingResource) : null;\n      loadResources(properties, resources, quietmode);\n\n      if (overlay != null) {\n        properties.putAll(overlay);\n        if (backup != null) {\n          for (Map.Entry<Object, Object> item : overlay.entrySet()) {\n            String key = (String) item.getKey();\n            String[] source = backup.get(key);\n            if (source != null) {\n              updatingResource.put(key, source);\n            }\n          }\n        }\n      }\n    }\n    return properties;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.addResourceObject": "  private synchronized void addResourceObject(Resource resource) {\n    resources.add(resource);                      // add to resources\n    restrictSystemProps |= resource.isParserRestricted();\n    reloadConfiguration();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main": "  public static void main(String[] args) {\n    try {\n      mainStarted = true;\n      Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\n      String containerIdStr =\n          System.getenv(Environment.CONTAINER_ID.name());\n      String nodeHostString = System.getenv(Environment.NM_HOST.name());\n      String nodePortString = System.getenv(Environment.NM_PORT.name());\n      String nodeHttpPortString =\n          System.getenv(Environment.NM_HTTP_PORT.name());\n      String appSubmitTimeStr =\n          System.getenv(ApplicationConstants.APP_SUBMIT_TIME_ENV);\n      \n      validateInputParam(containerIdStr,\n          Environment.CONTAINER_ID.name());\n      validateInputParam(nodeHostString, Environment.NM_HOST.name());\n      validateInputParam(nodePortString, Environment.NM_PORT.name());\n      validateInputParam(nodeHttpPortString,\n          Environment.NM_HTTP_PORT.name());\n      validateInputParam(appSubmitTimeStr,\n          ApplicationConstants.APP_SUBMIT_TIME_ENV);\n\n      ContainerId containerId = ContainerId.fromString(containerIdStr);\n      ApplicationAttemptId applicationAttemptId =\n          containerId.getApplicationAttemptId();\n      if (applicationAttemptId != null) {\n        CallerContext.setCurrent(new CallerContext.Builder(\n            \"mr_appmaster_\" + applicationAttemptId.toString()).build());\n      }\n      long appSubmitTime = Long.parseLong(appSubmitTimeStr);\n      \n      \n      MRAppMaster appMaster =\n          new MRAppMaster(applicationAttemptId, containerId, nodeHostString,\n              Integer.parseInt(nodePortString),\n              Integer.parseInt(nodeHttpPortString), appSubmitTime);\n      ShutdownHookManager.get().addShutdownHook(\n        new MRAppMasterShutdownHook(appMaster), SHUTDOWN_HOOK_PRIORITY);\n      JobConf conf = new JobConf(new YarnConfiguration());\n      conf.addResource(new Path(MRJobConfig.JOB_CONF_FILE));\n      \n      MRWebAppUtil.initialize(conf);\n      // log the system properties\n      String systemPropsToLog = MRApps.getSystemPropertiesToLog(conf);\n      if (systemPropsToLog != null) {\n        LOG.info(systemPropsToLog);\n      }\n\n      String jobUserName = System\n          .getenv(ApplicationConstants.Environment.USER.name());\n      conf.set(MRJobConfig.USER_NAME, jobUserName);\n      initAndStartAppMaster(appMaster, conf, jobUserName);\n    } catch (Throwable t) {\n      LOG.error(\"Error starting MRAppMaster\", t);\n      ExitUtil.terminate(1, t);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.validateInputParam": "  private static void validateInputParam(String value, String param)\n      throws IOException {\n    if (value == null) {\n      String msg = param + \" is null\";\n      LOG.error(msg);\n      throw new IOException(msg);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster": "  protected static void initAndStartAppMaster(final MRAppMaster appMaster,\n      final JobConf conf, String jobUserName) throws IOException,\n      InterruptedException {\n    UserGroupInformation.setConfiguration(conf);\n    // MAPREDUCE-6565: need to set configuration for SecurityUtil.\n    SecurityUtil.setConfiguration(conf);\n    // Security framework already loaded the tokens into current UGI, just use\n    // them\n    Credentials credentials =\n        UserGroupInformation.getCurrentUser().getCredentials();\n    LOG.info(\"Executing with tokens: {}\", credentials.getAllTokens());\n    \n    UserGroupInformation appMasterUgi = UserGroupInformation\n        .createRemoteUser(jobUserName);\n    appMasterUgi.addCredentials(credentials);\n\n    // Now remove the AM->RM token so tasks don't have it\n    Iterator<Token<?>> iter = credentials.getAllTokens().iterator();\n    while (iter.hasNext()) {\n      Token<?> token = iter.next();\n      if (token.getKind().equals(AMRMTokenIdentifier.KIND_NAME)) {\n        iter.remove();\n      }\n    }\n    conf.getCredentials().addAll(credentials);\n    appMasterUgi.doAs(new PrivilegedExceptionAction<Object>() {\n      @Override\n      public Object run() throws Exception {\n        appMaster.init(conf);\n        appMaster.start();\n        if(appMaster.errorHappenedShutDown) {\n          throw new IOException(\"Was asked to shut down.\");\n        }\n        return null;\n      }\n    });\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.getApplicationAttemptId": "    public ApplicationAttemptId getApplicationAttemptId() {\n      return appAttemptID;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.Credentials.readTokenStorageFile": "  public static Credentials readTokenStorageFile(File filename,\n                                                 Configuration conf)\n      throws IOException {\n    DataInputStream in = null;\n    Credentials credentials = new Credentials();\n    try {\n      in = new DataInputStream(new BufferedInputStream(\n          new FileInputStream(filename)));\n      credentials.readTokenStorageStream(in);\n      return credentials;\n    } catch(IOException ioe) {\n      throw new IOException(\"Exception reading \" + filename, ioe);\n    } finally {\n      IOUtils.cleanupWithLogger(LOG, in);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.Credentials.readTokenStorageStream": "  public void readTokenStorageStream(DataInputStream in) throws IOException {\n    byte[] magic = new byte[TOKEN_STORAGE_MAGIC.length];\n    in.readFully(magic);\n    if (!Arrays.equals(magic, TOKEN_STORAGE_MAGIC)) {\n      throw new IOException(\"Bad header found in token storage.\");\n    }\n    byte version = in.readByte();\n    if (version != TOKEN_STORAGE_VERSION &&\n        version != OLD_TOKEN_STORAGE_VERSION) {\n      throw new IOException(\"Unknown version \" + version +\n                            \" in token storage.\");\n    }\n    if (version == OLD_TOKEN_STORAGE_VERSION) {\n      readFields(in);\n    } else if (version == TOKEN_STORAGE_VERSION) {\n      readProto(in);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject": "  static void loginUserFromSubject(Subject subject) throws IOException {\n    ensureInitialized();\n    boolean externalSubject = false;\n    try {\n      if (subject == null) {\n        subject = new Subject();\n      } else {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Treat subject external: \" + treatSubjectExternal\n              + \". When true, assuming keytab is managed extenally since \"\n              + \" logged in from subject\");\n        }\n        externalSubject = treatSubjectExternal;\n      }\n      LoginContext login =\n          newLoginContext(authenticationMethod.getLoginAppName(), \n                          subject, new HadoopConfiguration());\n      login.login();\n\n      UserGroupInformation realUser =\n          new UserGroupInformation(subject, externalSubject);\n      realUser.setLogin(login);\n      realUser.setAuthenticationMethod(authenticationMethod);\n      // If the HADOOP_PROXY_USER environment variable or property\n      // is specified, create a proxy user as the logged in user.\n      String proxyUser = System.getenv(HADOOP_PROXY_USER);\n      if (proxyUser == null) {\n        proxyUser = System.getProperty(HADOOP_PROXY_USER);\n      }\n      loginUser = proxyUser == null ? realUser : createProxyUser(proxyUser, realUser);\n\n      String tokenFileLocation = System.getProperty(HADOOP_TOKEN_FILES);\n      if (tokenFileLocation == null) {\n        tokenFileLocation = conf.get(HADOOP_TOKEN_FILES);\n      }\n      if (tokenFileLocation != null) {\n        for (String tokenFileName:\n             StringUtils.getTrimmedStrings(tokenFileLocation)) {\n          if (tokenFileName.length() > 0) {\n            File tokenFile = new File(tokenFileName);\n            if (tokenFile.exists() && tokenFile.isFile()) {\n              Credentials cred = Credentials.readTokenStorageFile(\n                  tokenFile, conf);\n              loginUser.addCredentials(cred);\n            } else {\n              LOG.info(\"tokenFile(\"+tokenFileName+\") does not exist\");\n            }\n          }\n        }\n      }\n\n      String fileLocation = System.getenv(HADOOP_TOKEN_FILE_LOCATION);\n      if (fileLocation != null) {\n        // Load the token storage file and put all of the tokens into the\n        // user. Don't use the FileSystem API for reading since it has a lock\n        // cycle (HADOOP-9212).\n        File source = new File(fileLocation);\n        LOG.debug(\"Reading credentials from location set in {}: {}\",\n            HADOOP_TOKEN_FILE_LOCATION,\n            source.getCanonicalPath());\n        if (!source.isFile()) {\n          throw new FileNotFoundException(\"Source file \"\n              + source.getCanonicalPath() + \" from \"\n              + HADOOP_TOKEN_FILE_LOCATION\n              + \" not found\");\n        }\n        Credentials cred = Credentials.readTokenStorageFile(\n            source, conf);\n        LOG.debug(\"Loaded {} tokens\", cred.numberOfTokens());\n        loginUser.addCredentials(cred);\n      }\n      loginUser.spawnAutoRenewalThreadForUserCreds();\n    } catch (LoginException le) {\n      LOG.debug(\"failure to login\", le);\n      throw new KerberosAuthException(FAILURE_TO_LOGIN, le);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"UGI loginUser:\"+loginUser);\n    } \n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.login": "    public boolean login() throws LoginException {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"hadoop login\");\n      }\n      return true;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.newLoginContext": "  private static LoginContext\n  newLoginContext(String appName, Subject subject,\n    javax.security.auth.login.Configuration loginConf)\n      throws LoginException {\n    // Temporarily switch the thread's ContextClassLoader to match this\n    // class's classloader, so that we can properly load HadoopLoginModule\n    // from the JAAS libraries.\n    Thread t = Thread.currentThread();\n    ClassLoader oldCCL = t.getContextClassLoader();\n    t.setContextClassLoader(HadoopLoginModule.class.getClassLoader());\n    try {\n      return new LoginContext(appName, subject, null, loginConf);\n    } finally {\n      t.setContextClassLoader(oldCCL);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.setLogin": "  private void setLogin(LoginContext login) {\n    user.setLogin(login);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getLoginAppName": "    String getLoginAppName() {\n      if (loginAppName == null) {\n        throw new UnsupportedOperationException(\n            this + \" login authentication is not supported\");\n      }\n      return loginAppName;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.createProxyUser": "  public static UserGroupInformation createProxyUser(String user,\n      UserGroupInformation realUser) {\n    if (user == null || user.isEmpty()) {\n      throw new IllegalArgumentException(\"Null user\");\n    }\n    if (realUser == null) {\n      throw new IllegalArgumentException(\"Null real user\");\n    }\n    Subject subject = new Subject();\n    Set<Principal> principals = subject.getPrincipals();\n    principals.add(new User(user));\n    principals.add(new RealUser(realUser));\n    UserGroupInformation result =new UserGroupInformation(subject, false);\n    result.setAuthenticationMethod(AuthenticationMethod.PROXY);\n    return result;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.spawnAutoRenewalThreadForUserCreds": "  private void spawnAutoRenewalThreadForUserCreds() {\n    if (getEnableRenewThreadCreationForTest()) {\n      LOG.warn(\"Spawning thread to auto renew user credential since \" +\n          \" enableRenewThreadCreationForTest was set to true.\");\n    } else if (!shouldRelogin() || isKeytab) {\n      return;\n    }\n\n    //spawn thread only if we have kerb credentials\n    Thread t = new Thread(new Runnable() {\n\n      @Override\n      public void run() {\n        String cmd = conf.get(\"hadoop.kerberos.kinit.command\", \"kinit\");\n        KerberosTicket tgt = getTGT();\n        if (tgt == null) {\n          return;\n        }\n        long nextRefresh = getRefreshTime(tgt);\n        RetryPolicy rp = null;\n        while (true) {\n          try {\n            long now = Time.now();\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Current time is \" + now);\n              LOG.debug(\"Next refresh is \" + nextRefresh);\n            }\n            if (now < nextRefresh) {\n              Thread.sleep(nextRefresh - now);\n            }\n            Shell.execCommand(cmd, \"-R\");\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"renewed ticket\");\n            }\n            reloginFromTicketCache();\n            tgt = getTGT();\n            if (tgt == null) {\n              LOG.warn(\"No TGT after renewal. Aborting renew thread for \" +\n                  getUserName());\n              return;\n            }\n            nextRefresh = Math.max(getRefreshTime(tgt),\n              now + kerberosMinSecondsBeforeRelogin);\n            metrics.renewalFailures.set(0);\n            rp = null;\n          } catch (InterruptedException ie) {\n            LOG.warn(\"Terminating renewal thread\");\n            return;\n          } catch (IOException ie) {\n            metrics.renewalFailuresTotal.incr();\n            final long tgtEndTime = tgt.getEndTime().getTime();\n            LOG.warn(\"Exception encountered while running the renewal \"\n                    + \"command for {}. (TGT end time:{}, renewalFailures: {},\"\n                    + \"renewalFailuresTotal: {})\", getUserName(), tgtEndTime,\n                metrics.renewalFailures, metrics.renewalFailuresTotal, ie);\n            final long now = Time.now();\n            if (rp == null) {\n              // Use a dummy maxRetries to create the policy. The policy will\n              // only be used to get next retry time with exponential back-off.\n              // The final retry time will be later limited within the\n              // tgt endTime in getNextTgtRenewalTime.\n              rp = RetryPolicies.exponentialBackoffRetry(Long.SIZE - 2,\n                  kerberosMinSecondsBeforeRelogin, TimeUnit.MILLISECONDS);\n            }\n            try {\n              nextRefresh = getNextTgtRenewalTime(tgtEndTime, now, rp);\n            } catch (Exception e) {\n              LOG.error(\"Exception when calculating next tgt renewal time\", e);\n              return;\n            }\n            metrics.renewalFailures.incr();\n            // retry until close enough to tgt endTime.\n            if (now > nextRefresh) {\n              LOG.error(\"TGT is expired. Aborting renew thread for {}.\",\n                  getUserName());\n              return;\n            }\n          }\n        }\n      }\n    });\n    t.setDaemon(true);\n    t.setName(\"TGT Renewer for \" + getUserName());\n    t.start();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.setAuthenticationMethod": "  public void setAuthenticationMethod(AuthMethod authMethod) {\n    user.setAuthenticationMethod(AuthenticationMethod.valueOf(authMethod));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.addCredentials": "  public void addCredentials(Credentials credentials) {\n    synchronized (subject) {\n      getCredentialsInternal().addAll(credentials);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.ensureInitialized": "  private static void ensureInitialized() {\n    if (conf == null) {\n      synchronized(UserGroupInformation.class) {\n        if (conf == null) { // someone might have beat us\n          initialize(new Configuration(), false);\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getLoginUser": "  static UserGroupInformation getLoginUser() throws IOException {\n    if (loginUser == null) {\n      loginUserFromSubject(null);\n    }\n    return loginUser;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getCurrentUser": "  static UserGroupInformation getCurrentUser() throws IOException {\n    AccessControlContext context = AccessController.getContext();\n    Subject subject = Subject.getSubject(context);\n    if (subject == null || subject.getPrincipals(User.class).isEmpty()) {\n      return getLoginUser();\n    } else {\n      return new UserGroupInformation(subject);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getSubject": "  protected Subject getSubject() {\n    return subject;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.Credentials.numberOfTokens": "  public int numberOfTokens() {\n    return tokenMap.size();\n  }"
        },
        "bug_report": {
            "Title": "3.0 deployment cannot work with old version MR tar ball which breaks rolling upgrade",
            "Description": "I tried to deploy 3.0 cluster with 2.9 MR tar ball. The MR job is failed because following error:\r\n{noformat}\r\n2017-11-21 12:42:50,911 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1511295641738_0003_000001\r\n2017-11-21 12:42:51,070 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n2017-11-21 12:42:51,118 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster\r\njava.lang.RuntimeException: Unable to determine current user\r\n\tat org.apache.hadoop.conf.Configuration$Resource.getRestrictParserDefault(Configuration.java:254)\r\n\tat org.apache.hadoop.conf.Configuration$Resource.<init>(Configuration.java:220)\r\n\tat org.apache.hadoop.conf.Configuration$Resource.<init>(Configuration.java:212)\r\n\tat org.apache.hadoop.conf.Configuration.addResource(Configuration.java:888)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1638)\r\nCaused by: java.io.IOException: Exception reading /tmp/nm-local-dir/usercache/jdu/appcache/application_1511295641738_0003/container_e03_1511295641738_0003_01_000001/container_tokens\r\n\tat org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:208)\r\n\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:907)\r\n\tat org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:820)\r\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:689)\r\n\tat org.apache.hadoop.conf.Configuration$Resource.getRestrictParserDefault(Configuration.java:252)\r\n\t... 4 more\r\nCaused by: java.io.IOException: Unknown version 1 in token storage.\r\n\tat org.apache.hadoop.security.Credentials.readTokenStorageStream(Credentials.java:226)\r\n\tat org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:205)\r\n\t... 8 more\r\n2017-11-21 12:42:51,122 INFO [main] org.apache.hadoop.util.ExitUtil: Exiting with status 1: java.lang.RuntimeException: Unable to determine current user\r\n{noformat}\r\nI think it is due to token incompatiblity change between 2.9 and 3.0. As we claim \"rolling upgrade\" is supported in Hadoop 3, we should fix this before we ship 3.0 otherwise all MR running applications will get stuck during/after upgrade."
        }
    },
    {
        "filename": "HADOOP-15307.json",
        "creation_time": "2018-03-12T14:56:59.000+0000",
        "stack_trace": "```\njava.lang.UnsupportedOperationException: Unsupported verifier flavorAUTH_SYS\nat org.apache.hadoop.oncrpc.security.Verifier.readFlavorAndVerifier(Verifier.java:45)\nat org.apache.hadoop.oncrpc.RpcDeniedReply.read(RpcDeniedReply.java:50)\nat org.apache.hadoop.oncrpc.RpcReply.read(RpcReply.java:67)\nat org.apache.hadoop.oncrpc.SimpleUdpClient.run(SimpleUdpClient.java:71)\nat org.apache.hadoop.oncrpc.RpcProgram.register(RpcProgram.java:130)\nat org.apache.hadoop.oncrpc.RpcProgram.register(RpcProgram.java:101)\nat org.apache.hadoop.mount.MountdBase.start(MountdBase.java:83)\nat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)\nat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)\nat org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:60)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:498)\nat org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)```",
        "source_code": {
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.security.Verifier.readFlavorAndVerifier": "  public static Verifier readFlavorAndVerifier(XDR xdr) {\n    AuthFlavor flavor = AuthFlavor.fromValue(xdr.readInt());\n    final Verifier verifer;\n    if(flavor == AuthFlavor.AUTH_NONE) {\n      verifer = new VerifierNone();\n    } else if(flavor == AuthFlavor.RPCSEC_GSS) {\n      verifer = new VerifierGSS();\n    } else {\n      throw new UnsupportedOperationException(\"Unsupported verifier flavor\"\n          + flavor);\n    }\n    verifer.read(xdr);\n    return verifer;\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcDeniedReply.read": "  public static RpcDeniedReply read(int xid, ReplyState replyState, XDR xdr) {\n    Verifier verifier = Verifier.readFlavorAndVerifier(xdr);\n    RejectState rejectState = RejectState.fromValue(xdr.readInt());\n    return new RpcDeniedReply(xid, replyState, rejectState, verifier);\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcDeniedReply.fromValue": "    static RejectState fromValue(int value) {\n      return values()[value];\n    }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcReply.read": "  public static RpcReply read(XDR xdr) {\n    int xid = xdr.readInt();\n    final Type messageType = Type.fromValue(xdr.readInt());\n    Preconditions.checkState(messageType == RpcMessage.Type.RPC_REPLY);\n    \n    ReplyState stat = ReplyState.fromValue(xdr.readInt());\n    switch (stat) {\n    case MSG_ACCEPTED:\n      return RpcAcceptedReply.read(xid, stat, xdr);\n    case MSG_DENIED:\n      return RpcDeniedReply.read(xid, stat, xdr);\n    }\n    return null;\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcReply.fromValue": "    public static ReplyState fromValue(int value) {\n      return values()[value];\n    }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.SimpleUdpClient.run": "  public void run() throws IOException {\n    InetAddress IPAddress = InetAddress.getByName(host);\n    byte[] sendData = request.getBytes();\n    byte[] receiveData = new byte[65535];\n    // Use the provided socket if there is one, else just make a new one.\n    DatagramSocket socket = this.clientSocket == null ?\n        new DatagramSocket() : this.clientSocket;\n\n    try {\n      DatagramPacket sendPacket = new DatagramPacket(sendData, sendData.length,\n          IPAddress, port);\n      socket.send(sendPacket);\n      socket.setSoTimeout(udpTimeoutMillis);\n      DatagramPacket receivePacket = new DatagramPacket(receiveData,\n          receiveData.length);\n      socket.receive(receivePacket);\n  \n      // Check reply status\n      XDR xdr = new XDR(Arrays.copyOfRange(receiveData, 0,\n          receivePacket.getLength()));\n      RpcReply reply = RpcReply.read(xdr);\n      if (reply.getState() != RpcReply.ReplyState.MSG_ACCEPTED) {\n        throw new IOException(\"Request failed: \" + reply.getState());\n      }\n    } finally {\n      // If the client socket was passed in to this UDP client, it's on the\n      // caller of this UDP client to close that socket.\n      if (this.clientSocket == null) {\n        socket.close();\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcProgram.register": "  protected void register(PortmapMapping mapEntry, boolean set) {\n    XDR mappingRequest = PortmapRequest.create(mapEntry, set);\n    SimpleUdpClient registrationClient = new SimpleUdpClient(host, RPCB_PORT,\n        mappingRequest, true, registrationSocket, portmapUdpTimeoutMillis);\n    try {\n      registrationClient.run();\n    } catch (IOException e) {\n      String request = set ? \"Registration\" : \"Unregistration\";\n      LOG.error(request + \" failure with \" + host + \":\" + port\n          + \", portmap entry: \" + mapEntry);\n      throw new RuntimeException(request + \" failure\", e);\n    }\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.mount.MountdBase.start": "  public void start(boolean register) {\n    startUDPServer();\n    startTCPServer();\n    if (register) {\n      ShutdownHookManager.get().addShutdownHook(new Unregister(),\n          SHUTDOWN_HOOK_PRIORITY);\n      try {\n        rpcProgram.register(PortmapMapping.TRANSPORT_UDP, udpBoundPort);\n        rpcProgram.register(PortmapMapping.TRANSPORT_TCP, tcpBoundPort);\n      } catch (Throwable e) {\n        LOG.error(\"Failed to register the MOUNT service.\", e);\n        terminate(1, e);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.mount.MountdBase.startTCPServer": "  private void startTCPServer() {\n    SimpleTcpServer tcpServer = new SimpleTcpServer(rpcProgram.getPort(),\n        rpcProgram, 1);\n    rpcProgram.startDaemons();\n    try {\n      tcpServer.run();\n    } catch (Throwable e) {\n      LOG.error(\"Failed to start the TCP server.\", e);\n      if (tcpServer.getBoundPort() > 0) {\n        rpcProgram.unregister(PortmapMapping.TRANSPORT_TCP,\n            tcpServer.getBoundPort());\n      }\n      tcpServer.shutdown();\n      terminate(1, e);\n    }\n    tcpBoundPort = tcpServer.getBoundPort();\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.mount.MountdBase.startUDPServer": "  private void startUDPServer() {\n    SimpleUdpServer udpServer = new SimpleUdpServer(rpcProgram.getPort(),\n        rpcProgram, 1);\n    rpcProgram.startDaemons();\n    try {\n      udpServer.run();\n    } catch (Throwable e) {\n      LOG.error(\"Failed to start the UDP server.\", e);\n      if (udpServer.getBoundPort() > 0) {\n        rpcProgram.unregister(PortmapMapping.TRANSPORT_UDP,\n            udpServer.getBoundPort());\n      }\n      udpServer.shutdown();\n      terminate(1, e);\n    }\n    udpBoundPort = udpServer.getBoundPort();\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.xdr.readInt": "  public int readInt() {\n    Preconditions.checkState(state == State.READING);\n    return buf.getInt();\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcReply.getState": "  public ReplyState getState() {\n    return replyState;\n  }"
        },
        "bug_report": {
            "Title": "NFS: flavor AUTH_SYS should use VerifierNone",
            "Description": "When NFS gateway starts and if the portmapper request is denied by rpcbind for any reason (in our case, /etc/hosts.allow did not have the localhost), NFS gateway fails with the following obscure exception:\r\n{noformat}\r\n\r\n2018-03-05 12:49:31,976 INFO org.apache.hadoop.oncrpc.SimpleUdpServer: Started listening to UDP requests at port 4242 for Rpc program: mountd at localhost:4242 with workerCount 1\r\n2018-03-05 12:49:31,988 INFO org.apache.hadoop.oncrpc.SimpleTcpServer: Started listening to TCP requests at port 4242 for Rpc program: mountd at localhost:4242 with workerCount 1\r\n2018-03-05 12:49:31,993 TRACE org.apache.hadoop.oncrpc.RpcCall: Xid:692394656, messageType:RPC_CALL, rpcVersion:2, program:100000, version:2, procedure:1, credential:(AuthFlavor:AUTH_NONE), verifier:(AuthFlavor:AUTH_NONE)\r\n2018-03-05 12:49:31,998 FATAL org.apache.hadoop.mount.MountdBase: Failed to start the server. Cause:\r\njava.lang.UnsupportedOperationException: Unsupported verifier flavorAUTH_SYS\r\nat org.apache.hadoop.oncrpc.security.Verifier.readFlavorAndVerifier(Verifier.java:45)\r\nat org.apache.hadoop.oncrpc.RpcDeniedReply.read(RpcDeniedReply.java:50)\r\nat org.apache.hadoop.oncrpc.RpcReply.read(RpcReply.java:67)\r\nat org.apache.hadoop.oncrpc.SimpleUdpClient.run(SimpleUdpClient.java:71)\r\nat org.apache.hadoop.oncrpc.RpcProgram.register(RpcProgram.java:130)\r\nat org.apache.hadoop.oncrpc.RpcProgram.register(RpcProgram.java:101)\r\nat org.apache.hadoop.mount.MountdBase.start(MountdBase.java:83)\r\nat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)\r\nat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)\r\nat org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:60)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)\r\n2018-03-05 12:49:32,007 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1{noformat}\r\n\u00a0Reading the code comment for class Verifier, I think this bug existed since its inception\r\n{code:java}\r\n/**\r\n * Base class for verifier. Currently our authentication only supports 3 types\r\n * of auth flavors: {@link RpcAuthInfo.AuthFlavor#AUTH_NONE}, {@link RpcAuthInfo.AuthFlavor#AUTH_SYS},\r\n * and {@link RpcAuthInfo.AuthFlavor#RPCSEC_GSS}. Thus for verifier we only need to handle\r\n * AUTH_NONE and RPCSEC_GSS\r\n */\r\npublic abstract class Verifier extends RpcAuthInfo {{code}\r\nThe verifier should also handle AUTH_SYS too."
        }
    },
    {
        "filename": "HADOOP-11446.json",
        "creation_time": "2014-12-23T22:15:23.000+0000",
        "stack_trace": "```\nException in thread \"main\" java.lang.OutOfMemoryError: unable to create new native thread\n        at java.lang.Thread.start0(Native Method)\n        at java.lang.Thread.start(Thread.java:713)\n        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)\n        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1360)\n        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:132)\n        at com.amazonaws.services.s3.transfer.internal.UploadMonitor.<init>(UploadMonitor.java:129)\n        at com.amazonaws.services.s3.transfer.TransferManager.upload(TransferManager.java:449)\n        at com.amazonaws.services.s3.transfer.TransferManager.upload(TransferManager.java:382)\n        at org.apache.hadoop.fs.s3a.S3AOutputStream.close(S3AOutputStream.java:127)\n        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:54)\n        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:112)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:366)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:356)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:356)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)\n        at org.apache.hadoop.hbase.snapshot.ExportSnapshot.run(ExportSnapshot.java:791)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.hadoop.hbase.snapshot.ExportSnapshot.innerMain(ExportSnapshot.java:882)\n        at org.apache.hadoop.hbase.snapshot.ExportSnapshot.main(ExportSnapshot.java:886)\n```",
        "source_code": {
            "hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3a.S3AOutputStream.close": "  public synchronized void close() throws IOException {\n    if (closed) {\n      return;\n    }\n\n    backupStream.close();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"OutputStream for key '\" + key + \"' closed. Now beginning upload\");\n      LOG.debug(\"Minimum upload part size: \" + partSize + \" threshold \" + partSizeThreshold);\n    }\n\n\n    try {\n      TransferManagerConfiguration transferConfiguration = new TransferManagerConfiguration();\n      transferConfiguration.setMinimumUploadPartSize(partSize);\n      transferConfiguration.setMultipartUploadThreshold(partSizeThreshold);\n\n      TransferManager transfers = new TransferManager(client);\n      transfers.setConfiguration(transferConfiguration);\n\n      final ObjectMetadata om = new ObjectMetadata();\n      if (StringUtils.isNotBlank(serverSideEncryptionAlgorithm)) {\n        om.setServerSideEncryption(serverSideEncryptionAlgorithm);\n      }\n      PutObjectRequest putObjectRequest = new PutObjectRequest(bucket, key, backupFile);\n      putObjectRequest.setCannedAcl(cannedACL);\n      putObjectRequest.setMetadata(om);\n\n      Upload upload = transfers.upload(putObjectRequest);\n\n      ProgressableProgressListener listener = \n        new ProgressableProgressListener(upload, progress, statistics);\n      upload.addProgressListener(listener);\n\n      upload.waitForUploadResult();\n\n      long delta = upload.getProgress().getBytesTransferred() - listener.getLastBytesTransferred();\n      if (statistics != null && delta != 0) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"S3A write delta changed after finished: \" + delta + \" bytes\");\n        }\n        statistics.incrementBytesWritten(delta);\n      }\n\n      // This will delete unnecessary fake parent directories\n      fs.finishedWrite(key);\n    } catch (InterruptedException e) {\n      throw new IOException(e);\n    } finally {\n      if (!backupFile.delete()) {\n        LOG.warn(\"Could not delete temporary s3a file: {}\", backupFile);\n      }\n      super.close();\n      closed = true;\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"OutputStream for key '\" + key + \"' upload complete\");\n    }\n  }",
            "hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3a.S3AOutputStream.getLastBytesTransferred": "    public long getLastBytesTransferred() {\n      return lastBytesTransferred;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FSDataOutputStream.close": "  public void close() throws IOException {\n    out.close(); // This invokes PositionCache.close()\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.IOUtils.copyBytes": "  public static void copyBytes(InputStream in, OutputStream out, long count,\n      boolean close) throws IOException {\n    byte buf[] = new byte[4096];\n    long bytesRemaining = count;\n    int bytesRead;\n\n    try {\n      while (bytesRemaining > 0) {\n        int bytesToRead = (int)\n          (bytesRemaining < buf.length ? bytesRemaining : buf.length);\n\n        bytesRead = in.read(buf, 0, bytesToRead);\n        if (bytesRead == -1)\n          break;\n\n        out.write(buf, 0, bytesRead);\n        bytesRemaining -= bytesRead;\n      }\n      if (close) {\n        out.close();\n        out = null;\n        in.close();\n        in = null;\n      }\n    } finally {\n      if (close) {\n        closeStream(out);\n        closeStream(in);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.IOUtils.closeStream": "  public static void closeStream(java.io.Closeable stream) {\n    cleanup(null, stream);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.IOUtils.write": "    public void write(int b) throws IOException {\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileUtil.copy": "  private static boolean copy(FileSystem srcFS, FileStatus srcStatus,\n                              File dst, boolean deleteSource,\n                              Configuration conf) throws IOException {\n    Path src = srcStatus.getPath();\n    if (srcStatus.isDirectory()) {\n      if (!dst.mkdirs()) {\n        return false;\n      }\n      FileStatus contents[] = srcFS.listStatus(src);\n      for (int i = 0; i < contents.length; i++) {\n        copy(srcFS, contents[i],\n             new File(dst, contents[i].getPath().getName()),\n             deleteSource, conf);\n      }\n    } else {\n      InputStream in = srcFS.open(src);\n      IOUtils.copyBytes(in, new FileOutputStream(dst), conf);\n    }\n    if (deleteSource) {\n      return srcFS.delete(src, true);\n    } else {\n      return true;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileUtil.fullyDelete": "  public static void fullyDelete(FileSystem fs, Path dir) \n  throws IOException {\n    fs.delete(dir, true);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileUtil.checkDest": "  private static Path checkDest(String srcName, FileSystem dstFS, Path dst,\n      boolean overwrite) throws IOException {\n    if (dstFS.exists(dst)) {\n      FileStatus sdst = dstFS.getFileStatus(dst);\n      if (sdst.isDirectory()) {\n        if (null == srcName) {\n          throw new IOException(\"Target \" + dst + \" is a directory\");\n        }\n        return checkDest(null, dstFS, new Path(dst, srcName), overwrite);\n      } else if (!overwrite) {\n        throw new IOException(\"Target \" + dst + \" already exists\");\n      }\n    }\n    return dst;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileUtil.checkDependencies": "  private static void checkDependencies(FileSystem srcFS, \n                                        Path src, \n                                        FileSystem dstFS, \n                                        Path dst)\n                                        throws IOException {\n    if (srcFS == dstFS) {\n      String srcq = src.makeQualified(srcFS).toString() + Path.SEPARATOR;\n      String dstq = dst.makeQualified(dstFS).toString() + Path.SEPARATOR;\n      if (dstq.startsWith(srcq)) {\n        if (srcq.length() == dstq.length()) {\n          throw new IOException(\"Cannot copy \" + src + \" to itself.\");\n        } else {\n          throw new IOException(\"Cannot copy \" + src + \" to its subdirectory \" +\n                                dst);\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileUtil.listFiles": "  public static File[] listFiles(File dir) throws IOException {\n    File[] files = dir.listFiles();\n    if(files == null) {\n      throw new IOException(\"Invalid directory or I/O error occurred for dir: \"\n                + dir.toString());\n    }\n    return files;\n  }  ",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ToolRunner.run": "  public static int run(Tool tool, String[] args) \n    throws Exception{\n    return run(tool.getConf(), tool, args);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileStatus.isDirectory": "  public boolean isDirectory() {\n    return isdir;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Path.getName": "  public String getName() {\n    String path = uri.getPath();\n    int slash = path.lastIndexOf(SEPARATOR);\n    return path.substring(slash+1);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Path.toString": "  public String toString() {\n    // we can't use uri.toString(), which escapes everything, because we want\n    // illegal characters unescaped in the string, for glob processing, etc.\n    StringBuilder buffer = new StringBuilder();\n    if (uri.getScheme() != null) {\n      buffer.append(uri.getScheme());\n      buffer.append(\":\");\n    }\n    if (uri.getAuthority() != null) {\n      buffer.append(\"//\");\n      buffer.append(uri.getAuthority());\n    }\n    if (uri.getPath() != null) {\n      String path = uri.getPath();\n      if (path.indexOf('/')==0 &&\n          hasWindowsDrive(path) &&                // has windows drive\n          uri.getScheme() == null &&              // but no scheme\n          uri.getAuthority() == null)             // or authority\n        path = path.substring(1);                 // remove slash before drive\n      buffer.append(path);\n    }\n    if (uri.getFragment() != null) {\n      buffer.append(\"#\");\n      buffer.append(uri.getFragment());\n    }\n    return buffer.toString();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Path.hasWindowsDrive": "  private static boolean hasWindowsDrive(String path) {\n    return (WINDOWS && hasDriveLetterSpecifier.matcher(path).find());\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs": "  public String[] getRemainingArgs() {\n    return (commandLine == null) ? new String[]{} : commandLine.getArgs();\n  }"
        },
        "bug_report": {
            "Title": "S3AOutputStream should use shared thread pool to avoid OutOfMemoryError",
            "Description": "When working with Terry Padgett who used s3a for hbase snapshot, the following issue was uncovered.\nHere is part of the output including the OOME when hbase snapshot is exported to s3a (nofile ulimit was increased to 102400):\n{code}\n2014-12-19 13:15:03,895 INFO  [main] s3a.S3AFileSystem: OutputStream for key 'FastQueryPOC/2014-12-11/EVENT1-IDX-snapshot/.hbase-snapshot/.tmp/EVENT1_IDX_snapshot_2012_12_11/    650a5678810fbdaa91809668d11ccf09/.regioninfo' closed. Now beginning upload\n2014-12-19 13:15:03,895 INFO  [main] s3a.S3AFileSystem: Minimum upload part size: 16777216 threshold2147483647\nException in thread \"main\" java.lang.OutOfMemoryError: unable to create new native thread\n        at java.lang.Thread.start0(Native Method)\n        at java.lang.Thread.start(Thread.java:713)\n        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)\n        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1360)\n        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:132)\n        at com.amazonaws.services.s3.transfer.internal.UploadMonitor.<init>(UploadMonitor.java:129)\n        at com.amazonaws.services.s3.transfer.TransferManager.upload(TransferManager.java:449)\n        at com.amazonaws.services.s3.transfer.TransferManager.upload(TransferManager.java:382)\n        at org.apache.hadoop.fs.s3a.S3AOutputStream.close(S3AOutputStream.java:127)\n        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:54)\n        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:112)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:366)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:356)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:356)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)\n        at org.apache.hadoop.hbase.snapshot.ExportSnapshot.run(ExportSnapshot.java:791)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.hadoop.hbase.snapshot.ExportSnapshot.innerMain(ExportSnapshot.java:882)\n        at org.apache.hadoop.hbase.snapshot.ExportSnapshot.main(ExportSnapshot.java:886)\n{code}\nIn S3AOutputStream#close():\n{code}\n      TransferManager transfers = new TransferManager(client);\n{code}\nThis results in each TransferManager creating its own thread pool, leading to the OOME.\nOne solution is to pass shared thread pool to TransferManager."
        }
    },
    {
        "filename": "HADOOP-12689.json",
        "creation_time": "2016-01-04T23:30:49.000+0000",
        "stack_trace": "```\njava.io.IOException: /test doesn't exist\nat org.apache.hadoop.fs.s3.Jets3tFileSystemStore.get(Jets3tFileSystemStore.java:170)\nat org.apache.hadoop.fs.s3.Jets3tFileSystemStore.retrieveINode(Jets3tFileSystemStore.java:221)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\nat com.sun.proxy.$Proxy17.retrieveINode(Unknown Source)\nat org.apache.hadoop.fs.s3.S3FileSystem.getFileStatus(S3FileSystem.java:340)\nat org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:230)\nat org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:50)\nat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)\nat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)```",
        "source_code": {
            "hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3.Jets3tFileSystemStore.get": "  private InputStream get(String key, long byteRangeStart) throws IOException {\n    try {\n      S3Object object = s3Service.getObject(bucket, key, null, null, null,\n                                            null, byteRangeStart, null);\n      return object.getDataInputStream();\n    } catch (S3ServiceException e) {\n      if (\"NoSuchKey\".equals(e.getS3ErrorCode())) {\n        return null;\n      }\n      if (e.getCause() instanceof IOException) {\n        throw (IOException) e.getCause();\n      }\n      throw new S3Exception(e);\n    } catch (ServiceException e) {\n      handleServiceException(e);\n      return null;\n    }\n  }",
            "hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3.Jets3tFileSystemStore.handleServiceException": "  private void handleServiceException(ServiceException e) throws IOException {\n      if (e.getCause() instanceof IOException) {\n        throw (IOException) e.getCause();\n      }\n      else {\n        if(LOG.isDebugEnabled()) {\n          LOG.debug(\"Got ServiceException with Error code: \" + e.getErrorCode() + \";and Error message: \" + e.getErrorMessage());\n        }\n      }\n    }",
            "hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3.Jets3tFileSystemStore.checkMetadata": "  private void checkMetadata(S3Object object) throws S3FileSystemException,\n      S3ServiceException {\n    \n    String name = (String) object.getMetadata(FILE_SYSTEM_NAME);\n    if (!FILE_SYSTEM_VALUE.equals(name)) {\n      throw new S3FileSystemException(\"Not a Hadoop S3 file.\");\n    }\n    String type = (String) object.getMetadata(FILE_SYSTEM_TYPE_NAME);\n    if (!FILE_SYSTEM_TYPE_VALUE.equals(type)) {\n      throw new S3FileSystemException(\"Not a block file.\");\n    }\n    String dataVersion = (String) object.getMetadata(FILE_SYSTEM_VERSION_NAME);\n    if (!FILE_SYSTEM_VERSION_VALUE.equals(dataVersion)) {\n      throw new VersionMismatchException(FILE_SYSTEM_VERSION_VALUE,\n          dataVersion);\n    }\n  }",
            "hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3.Jets3tFileSystemStore.retrieveINode": "  public INode retrieveINode(Path path) throws IOException {\n    String key = pathToKey(path);\n    InputStream in = get(key, true);\n    if (in == null && isRoot(key)) {\n      storeINode(path, INode.DIRECTORY_INODE);\n      return INode.DIRECTORY_INODE;\n    }\n    return INode.deserialize(in);\n  }",
            "hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3.Jets3tFileSystemStore.pathToKey": "  private String pathToKey(Path path) {\n    if (!path.isAbsolute()) {\n      throw new IllegalArgumentException(\"Path must be absolute: \" + path);\n    }\n    return path.toUri().getPath();\n  }",
            "hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3.Jets3tFileSystemStore.storeINode": "  public void storeINode(Path path, INode inode) throws IOException {\n    put(pathToKey(path), inode.serialize(), inode.getSerializedLength(), true);\n  }",
            "hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3.Jets3tFileSystemStore.isRoot": "  private boolean isRoot(String key) {\n    return key.isEmpty() || key.equals(\"/\");\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod": "  protected Object invokeMethod(Method method, Object[] args) throws Throwable {\n    try {\n      if (!method.isAccessible()) {\n        method.setAccessible(true);\n      }\n      return method.invoke(currentProxy.proxy, args);\n    } catch (InvocationTargetException e) {\n      throw e.getCause();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invoke": "  public Object invoke(Object proxy, Method method, Object[] args)\n    throws Throwable {\n    RetryPolicy policy = methodNameToPolicyMap.get(method.getName());\n    if (policy == null) {\n      policy = defaultPolicy;\n    }\n    \n    // The number of times this method invocation has been failed over.\n    int invocationFailoverCount = 0;\n    final boolean isRpc = isRpcInvocation(currentProxy.proxy);\n    final int callId = isRpc? Client.nextCallId(): RpcConstants.INVALID_CALL_ID;\n    int retries = 0;\n    while (true) {\n      // The number of times this invocation handler has ever been failed over,\n      // before this method invocation attempt. Used to prevent concurrent\n      // failed method invocations from triggering multiple failover attempts.\n      long invocationAttemptFailoverCount;\n      synchronized (proxyProvider) {\n        invocationAttemptFailoverCount = proxyProviderFailoverCount;\n      }\n\n      if (isRpc) {\n        Client.setCallIdAndRetryCount(callId, retries);\n      }\n      try {\n        Object ret = invokeMethod(method, args);\n        hasMadeASuccessfulCall = true;\n        return ret;\n      } catch (Exception ex) {\n        if (Thread.currentThread().isInterrupted()) {\n          // If interrupted, do not retry.\n          throw ex;\n        }\n        boolean isIdempotentOrAtMostOnce = proxyProvider.getInterface()\n            .getMethod(method.getName(), method.getParameterTypes())\n            .isAnnotationPresent(Idempotent.class);\n        if (!isIdempotentOrAtMostOnce) {\n          isIdempotentOrAtMostOnce = proxyProvider.getInterface()\n              .getMethod(method.getName(), method.getParameterTypes())\n              .isAnnotationPresent(AtMostOnce.class);\n        }\n        List<RetryAction> actions = extractActions(policy, ex, retries++,\n                invocationFailoverCount, isIdempotentOrAtMostOnce);\n        RetryAction failAction = getFailAction(actions);\n        if (failAction != null) {\n          if (failAction.reason != null) {\n            LOG.warn(\"Exception while invoking \" + currentProxy.proxy.getClass()\n                + \".\" + method.getName() + \" over \" + currentProxy.proxyInfo\n                + \". Not retrying because \" + failAction.reason, ex);\n          }\n          throw ex;\n        } else { // retry or failover\n          // avoid logging the failover if this is the first call on this\n          // proxy object, and we successfully achieve the failover without\n          // any flip-flopping\n          boolean worthLogging = \n            !(invocationFailoverCount == 0 && !hasMadeASuccessfulCall);\n          worthLogging |= LOG.isDebugEnabled();\n          RetryAction failOverAction = getFailOverAction(actions);\n          long delay = getDelayMillis(actions);\n          if (failOverAction != null && worthLogging) {\n            String msg = \"Exception while invoking \" + method.getName()\n                + \" of class \" + currentProxy.proxy.getClass().getSimpleName()\n                + \" over \" + currentProxy.proxyInfo;\n\n            if (invocationFailoverCount > 0) {\n              msg += \" after \" + invocationFailoverCount + \" fail over attempts\"; \n            }\n            msg += \". Trying to fail over \" + formatSleepMessage(delay);\n            LOG.info(msg, ex);\n          } else {\n            if(LOG.isDebugEnabled()) {\n              LOG.debug(\"Exception while invoking \" + method.getName()\n                  + \" of class \" + currentProxy.proxy.getClass().getSimpleName()\n                  + \" over \" + currentProxy.proxyInfo + \". Retrying \"\n                  + formatSleepMessage(delay), ex);\n            }\n          }\n\n          if (delay > 0) {\n            Thread.sleep(delay);\n          }\n          \n          if (failOverAction != null) {\n            // Make sure that concurrent failed method invocations only cause a\n            // single actual fail over.\n            synchronized (proxyProvider) {\n              if (invocationAttemptFailoverCount == proxyProviderFailoverCount) {\n                proxyProvider.performFailover(currentProxy.proxy);\n                proxyProviderFailoverCount++;\n              } else {\n                LOG.warn(\"A failover has occurred since the start of this method\"\n                    + \" invocation attempt.\");\n              }\n              currentProxy = proxyProvider.getProxy();\n            }\n            invocationFailoverCount++;\n          }\n        }\n      }\n    }\n  }",
            "hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3.S3FileSystem.getFileStatus": "  public FileStatus getFileStatus(Path f)  throws IOException {\n    INode inode = store.retrieveINode(makeAbsolute(f));\n    if (inode == null) {\n      throw new FileNotFoundException(f + \": No such file or directory.\");\n    }\n    return new S3FileStatus(f.makeQualified(this), inode);\n  }",
            "hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3.S3FileSystem.makeAbsolute": "  private Path makeAbsolute(Path path) {\n    if (path.isAbsolute()) {\n      return path;\n    }\n    return new Path(workingDir, path);\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.mapred.CopyMapper.map": "  public void map(Text relPath, CopyListingFileStatus sourceFileStatus,\n          Context context) throws IOException, InterruptedException {\n    Path sourcePath = sourceFileStatus.getPath();\n\n    if (LOG.isDebugEnabled())\n      LOG.debug(\"DistCpMapper::map(): Received \" + sourcePath + \", \" + relPath);\n\n    Path target = new Path(targetWorkPath.makeQualified(targetFS.getUri(),\n                          targetFS.getWorkingDirectory()) + relPath.toString());\n\n    EnumSet<DistCpOptions.FileAttribute> fileAttributes\n            = getFileAttributeSettings(context);\n    final boolean preserveRawXattrs = context.getConfiguration().getBoolean(\n        DistCpConstants.CONF_LABEL_PRESERVE_RAWXATTRS, false);\n\n    final String description = \"Copying \" + sourcePath + \" to \" + target;\n    context.setStatus(description);\n\n    LOG.info(description);\n\n    try {\n      CopyListingFileStatus sourceCurrStatus;\n      FileSystem sourceFS;\n      try {\n        sourceFS = sourcePath.getFileSystem(conf);\n        final boolean preserveXAttrs =\n            fileAttributes.contains(FileAttribute.XATTR);\n        sourceCurrStatus = DistCpUtils.toCopyListingFileStatus(sourceFS,\n          sourceFS.getFileStatus(sourcePath),\n          fileAttributes.contains(FileAttribute.ACL), \n          preserveXAttrs, preserveRawXattrs);\n      } catch (FileNotFoundException e) {\n        throw new IOException(new RetriableFileCopyCommand.CopyReadException(e));\n      }\n\n      FileStatus targetStatus = null;\n\n      try {\n        targetStatus = targetFS.getFileStatus(target);\n      } catch (FileNotFoundException ignore) {\n        if (LOG.isDebugEnabled())\n          LOG.debug(\"Path could not be found: \" + target, ignore);\n      }\n\n      if (targetStatus != null && (targetStatus.isDirectory() != sourceCurrStatus.isDirectory())) {\n        throw new IOException(\"Can't replace \" + target + \". Target is \" +\n            getFileType(targetStatus) + \", Source is \" + getFileType(sourceCurrStatus));\n      }\n\n      if (sourceCurrStatus.isDirectory()) {\n        createTargetDirsWithRetry(description, target, context);\n        return;\n      }\n\n      FileAction action = checkUpdate(sourceFS, sourceCurrStatus, target);\n      if (action == FileAction.SKIP) {\n        LOG.info(\"Skipping copy of \" + sourceCurrStatus.getPath()\n                 + \" to \" + target);\n        updateSkipCounters(context, sourceCurrStatus);\n        context.write(null, new Text(\"SKIP: \" + sourceCurrStatus.getPath()));\n      } else {\n        copyFileWithRetry(description, sourceCurrStatus, target, context,\n            action, fileAttributes);\n      }\n\n      DistCpUtils.preserve(target.getFileSystem(conf), target, sourceCurrStatus,\n          fileAttributes, preserveRawXattrs);\n    } catch (IOException exception) {\n      handleFailures(exception, sourceFileStatus, target, context);\n    }\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.mapred.CopyMapper.handleFailures": "  private void handleFailures(IOException exception,\n                                     FileStatus sourceFileStatus, Path target,\n                                     Context context) throws IOException, InterruptedException {\n    LOG.error(\"Failure in copying \" + sourceFileStatus.getPath() + \" to \" +\n                target, exception);\n\n    if (ignoreFailures && exception.getCause() instanceof\n            RetriableFileCopyCommand.CopyReadException) {\n      incrementCounter(context, Counter.FAIL, 1);\n      incrementCounter(context, Counter.BYTESFAILED, sourceFileStatus.getLen());\n      context.write(null, new Text(\"FAIL: \" + sourceFileStatus.getPath() + \" - \" +\n          StringUtils.stringifyException(exception)));\n    }\n    else\n      throw exception;\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.mapred.CopyMapper.updateSkipCounters": "  private static void updateSkipCounters(Context context,\n                                         FileStatus sourceFile) {\n    incrementCounter(context, Counter.SKIP, 1);\n    incrementCounter(context, Counter.BYTESSKIPPED, sourceFile.getLen());\n\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.mapred.CopyMapper.getFileAttributeSettings": "  private static EnumSet<DistCpOptions.FileAttribute>\n          getFileAttributeSettings(Mapper.Context context) {\n    String attributeString = context.getConfiguration().get(\n            DistCpOptionSwitch.PRESERVE_STATUS.getConfigLabel());\n    return DistCpUtils.unpackAttributes(attributeString);\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.mapred.CopyMapper.checkUpdate": "  private FileAction checkUpdate(FileSystem sourceFS, FileStatus source,\n      Path target) throws IOException {\n    final FileStatus targetFileStatus;\n    try {\n      targetFileStatus = targetFS.getFileStatus(target);\n    } catch (FileNotFoundException e) {\n      return FileAction.OVERWRITE;\n    }\n    if (targetFileStatus != null && !overWrite) {\n      if (canSkip(sourceFS, source, targetFileStatus)) {\n        return FileAction.SKIP;\n      } else if (append) {\n        long targetLen = targetFileStatus.getLen();\n        if (targetLen < source.getLen()) {\n          FileChecksum sourceChecksum = sourceFS.getFileChecksum(\n              source.getPath(), targetLen);\n          if (sourceChecksum != null\n              && sourceChecksum.equals(targetFS.getFileChecksum(target))) {\n            // We require that the checksum is not null. Thus currently only\n            // DistributedFileSystem is supported\n            return FileAction.APPEND;\n          }\n        }\n      }\n    }\n    return FileAction.OVERWRITE;\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.mapred.CopyMapper.getFileType": "  private String getFileType(FileStatus fileStatus) {\n    return fileStatus == null ? \"N/A\" : (fileStatus.isDirectory() ? \"dir\" : \"file\");\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.mapred.CopyMapper.createTargetDirsWithRetry": "  private void createTargetDirsWithRetry(String description,\n                   Path target, Context context) throws IOException {\n    try {\n      new RetriableDirectoryCreateCommand(description).execute(target, context);\n    } catch (Exception e) {\n      throw new IOException(\"mkdir failed for \" + target, e);\n    }\n    incrementCounter(context, Counter.COPY, 1);\n  }",
            "hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry": "  private void copyFileWithRetry(String description,\n      FileStatus sourceFileStatus, Path target, Context context,\n      FileAction action, EnumSet<DistCpOptions.FileAttribute> fileAttributes)\n      throws IOException {\n    long bytesCopied;\n    try {\n      bytesCopied = (Long) new RetriableFileCopyCommand(skipCrc, description,\n          action).execute(sourceFileStatus, target, context, fileAttributes);\n    } catch (Exception e) {\n      context.setStatus(\"Copy Failure: \" + sourceFileStatus.getPath());\n      throw new IOException(\"File copy failed: \" + sourceFileStatus.getPath() +\n          \" --> \" + target, e);\n    }\n    incrementCounter(context, Counter.BYTESEXPECTED, sourceFileStatus.getLen());\n    incrementCounter(context, Counter.BYTESCOPIED, bytesCopied);\n    incrementCounter(context, Counter.COPY, 1);\n    totalBytesCopied += bytesCopied;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.runNewMapper": "  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runNewMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, ClassNotFoundException,\n                             InterruptedException {\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, \n                                                                  getTaskID(),\n                                                                  reporter);\n    // make a mapper\n    org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE> mapper =\n      (org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>)\n        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);\n    // make the input format\n    org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE> inputFormat =\n      (org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE>)\n        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);\n    // rebuild the input split\n    org.apache.hadoop.mapreduce.InputSplit split = null;\n    split = getSplitDetails(new Path(splitIndex.getSplitLocation()),\n        splitIndex.getStartOffset());\n    LOG.info(\"Processing split: \" + split);\n\n    org.apache.hadoop.mapreduce.RecordReader<INKEY,INVALUE> input =\n      new NewTrackingRecordReader<INKEY,INVALUE>\n        (split, inputFormat, reporter, taskContext);\n    \n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.RecordWriter output = null;\n    \n    // get an output object\n    if (job.getNumReduceTasks() == 0) {\n      output = \n        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);\n    } else {\n      output = new NewOutputCollector(taskContext, job, umbilical, reporter);\n    }\n\n    org.apache.hadoop.mapreduce.MapContext<INKEY, INVALUE, OUTKEY, OUTVALUE> \n    mapContext = \n      new MapContextImpl<INKEY, INVALUE, OUTKEY, OUTVALUE>(job, getTaskID(), \n          input, output, \n          committer, \n          reporter, split);\n\n    org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context \n        mapperContext = \n          new WrappedMapper<INKEY, INVALUE, OUTKEY, OUTVALUE>().getMapContext(\n              mapContext);\n\n    try {\n      input.initialize(split, mapperContext);\n      mapper.run(mapperContext);\n      mapPhase.complete();\n      setPhase(TaskStatus.Phase.SORT);\n      statusUpdate(umbilical);\n      input.close();\n      input = null;\n      output.close(mapperContext);\n      output = null;\n    } finally {\n      closeQuietly(input);\n      closeQuietly(output, mapperContext);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.close": "      public void close() { }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getSplitDetails": " private <T> T getSplitDetails(Path file, long offset) \n  throws IOException {\n   FileSystem fs = file.getFileSystem(conf);\n   FSDataInputStream inFile = fs.open(file);\n   inFile.seek(offset);\n   String className = StringInterner.weakIntern(Text.readString(inFile));\n   Class<T> cls;\n   try {\n     cls = (Class<T>) conf.getClassByName(className);\n   } catch (ClassNotFoundException ce) {\n     IOException wrap = new IOException(\"Split class \" + className + \n                                         \" not found\");\n     wrap.initCause(ce);\n     throw wrap;\n   }\n   SerializationFactory factory = new SerializationFactory(conf);\n   Deserializer<T> deserializer = \n     (Deserializer<T>) factory.getDeserializer(cls);\n   deserializer.open(inFile);\n   T split = deserializer.deserialize(null);\n   long pos = inFile.getPos();\n   getCounters().findCounter(\n       TaskCounter.SPLIT_RAW_BYTES).increment(pos - offset);\n   inFile.close();\n   return split;\n }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.run": "      public void run() {\n        spillLock.lock();\n        spillThreadRunning = true;\n        try {\n          while (true) {\n            spillDone.signal();\n            while (!spillInProgress) {\n              spillReady.await();\n            }\n            try {\n              spillLock.unlock();\n              sortAndSpill();\n            } catch (Throwable t) {\n              sortSpillException = t;\n            } finally {\n              spillLock.lock();\n              if (bufend < bufstart) {\n                bufvoid = kvbuffer.length;\n              }\n              kvstart = kvend;\n              bufstart = bufend;\n              spillInProgress = false;\n            }\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        } finally {\n          spillLock.unlock();\n          spillThreadRunning = false;\n        }\n      }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.closeQuietly": "  private <INKEY, INVALUE, OUTKEY, OUTVALUE>\n  void closeQuietly(\n      org.apache.hadoop.mapreduce.RecordWriter<OUTKEY, OUTVALUE> c,\n      org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context\n          mapperContext) {\n    if (c != null) {\n      try {\n        c.close(mapperContext);\n      } catch (Exception ie) {\n        // Ignore\n        LOG.info(\"Ignoring exception during close for \" + c, ie);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.initialize": "    public void initialize(org.apache.hadoop.mapreduce.InputSplit split,\n                           org.apache.hadoop.mapreduce.TaskAttemptContext context\n                           ) throws IOException, InterruptedException {\n      long bytesInPrev = getInputBytes(fsStats);\n      real.initialize(split, context);\n      long bytesInCurr = getInputBytes(fsStats);\n      fileInputByteCounter.increment(bytesInCurr - bytesInPrev);\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.MapTask.getTaskID": "    private TaskAttemptID getTaskID() {\n      return mapTask.getTaskID();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.run": "              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.setEncryptedSpillKeyIfRequired": "  public static void setEncryptedSpillKeyIfRequired(Task task) throws\n          Exception {\n    if ((task != null) && (task.getEncryptedSpillKey() != null) && (task\n            .getEncryptedSpillKey().length > 1)) {\n      Credentials creds =\n              UserGroupInformation.getCurrentUser().getCredentials();\n      TokenCache.setEncryptedSpillKey(task.getEncryptedSpillKey(), creds);\n      UserGroupInformation.getCurrentUser().addCredentials(creds);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause == null) {\n        throw new RuntimeException(\"PrivilegedActionException with no \" +\n                \"underlying cause. UGI [\" + this + \"]\", pae);\n      } else if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.main": "  public static void main(String[] args) throws Throwable {\n    Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\n    LOG.debug(\"Child starting\");\n\n    final JobConf job = new JobConf(MRJobConfig.JOB_CONF_FILE);\n    // Initing with our JobConf allows us to avoid loading confs twice\n    Limits.init(job);\n    UserGroupInformation.setConfiguration(job);\n\n    String host = args[0];\n    int port = Integer.parseInt(args[1]);\n    final InetSocketAddress address =\n        NetUtils.createSocketAddrForHost(host, port);\n    final TaskAttemptID firstTaskid = TaskAttemptID.forName(args[2]);\n    long jvmIdLong = Long.parseLong(args[3]);\n    JVMId jvmId = new JVMId(firstTaskid.getJobID(),\n        firstTaskid.getTaskType() == TaskType.MAP, jvmIdLong);\n    \n    CallerContext.setCurrent(\n        new CallerContext.Builder(\"mr_\" + firstTaskid.toString()).build());\n\n    // initialize metrics\n    DefaultMetricsSystem.initialize(\n        StringUtils.camelize(firstTaskid.getTaskType().name()) +\"Task\");\n\n    // Security framework already loaded the tokens into current ugi\n    Credentials credentials =\n        UserGroupInformation.getCurrentUser().getCredentials();\n    LOG.info(\"Executing with tokens:\");\n    for (Token<?> token: credentials.getAllTokens()) {\n      LOG.info(token);\n    }\n\n    // Create TaskUmbilicalProtocol as actual task owner.\n    UserGroupInformation taskOwner =\n      UserGroupInformation.createRemoteUser(firstTaskid.getJobID().toString());\n    Token<JobTokenIdentifier> jt = TokenCache.getJobToken(credentials);\n    SecurityUtil.setTokenService(jt, address);\n    taskOwner.addToken(jt);\n    final TaskUmbilicalProtocol umbilical =\n      taskOwner.doAs(new PrivilegedExceptionAction<TaskUmbilicalProtocol>() {\n      @Override\n      public TaskUmbilicalProtocol run() throws Exception {\n        return (TaskUmbilicalProtocol)RPC.getProxy(TaskUmbilicalProtocol.class,\n            TaskUmbilicalProtocol.versionID, address, job);\n      }\n    });\n\n    // report non-pid to application master\n    JvmContext context = new JvmContext(jvmId, \"-1000\");\n    LOG.debug(\"PID: \" + System.getenv().get(\"JVM_PID\"));\n    Task task = null;\n    UserGroupInformation childUGI = null;\n    ScheduledExecutorService logSyncer = null;\n\n    try {\n      int idleLoopCount = 0;\n      JvmTask myTask = null;;\n      // poll for new task\n      for (int idle = 0; null == myTask; ++idle) {\n        long sleepTimeMilliSecs = Math.min(idle * 500, 1500);\n        LOG.info(\"Sleeping for \" + sleepTimeMilliSecs\n            + \"ms before retrying again. Got null now.\");\n        MILLISECONDS.sleep(sleepTimeMilliSecs);\n        myTask = umbilical.getTask(context);\n      }\n      if (myTask.shouldDie()) {\n        return;\n      }\n\n      task = myTask.getTask();\n      YarnChild.taskid = task.getTaskID();\n\n      // Create the job-conf and set credentials\n      configureTask(job, task, credentials, jt);\n\n      // log the system properties\n      String systemPropsToLog = MRApps.getSystemPropertiesToLog(job);\n      if (systemPropsToLog != null) {\n        LOG.info(systemPropsToLog);\n      }\n\n      // Initiate Java VM metrics\n      JvmMetrics.initSingleton(jvmId.toString(), job.getSessionId());\n      childUGI = UserGroupInformation.createRemoteUser(System\n          .getenv(ApplicationConstants.Environment.USER.toString()));\n      // Add tokens to new user so that it may execute its task correctly.\n      childUGI.addCredentials(credentials);\n\n      // set job classloader if configured before invoking the task\n      MRApps.setJobClassLoader(job);\n\n      logSyncer = TaskLog.createLogSyncer();\n\n      // Create a final reference to the task for the doAs block\n      final Task taskFinal = task;\n      childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n        @Override\n        public Object run() throws Exception {\n          // use job-specified working directory\n          setEncryptedSpillKeyIfRequired(taskFinal);\n          FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory());\n          taskFinal.run(job, umbilical); // run the task\n          return null;\n        }\n      });\n    } catch (FSError e) {\n      LOG.fatal(\"FSError from child\", e);\n      if (!ShutdownHookManager.get().isShutdownInProgress()) {\n        umbilical.fsError(taskid, e.getMessage());\n      }\n    } catch (Exception exception) {\n      LOG.warn(\"Exception running child : \"\n          + StringUtils.stringifyException(exception));\n      try {\n        if (task != null) {\n          // do cleanup for the task\n          if (childUGI == null) { // no need to job into doAs block\n            task.taskCleanup(umbilical);\n          } else {\n            final Task taskFinal = task;\n            childUGI.doAs(new PrivilegedExceptionAction<Object>() {\n              @Override\n              public Object run() throws Exception {\n                taskFinal.taskCleanup(umbilical);\n                return null;\n              }\n            });\n          }\n        }\n      } catch (Exception e) {\n        LOG.info(\"Exception cleaning up: \" + StringUtils.stringifyException(e));\n      }\n      // Report back any failures, for diagnostic purposes\n      if (taskid != null) {\n        if (!ShutdownHookManager.get().isShutdownInProgress()) {\n          umbilical.fatalError(taskid,\n              StringUtils.stringifyException(exception));\n        }\n      }\n    } catch (Throwable throwable) {\n      LOG.fatal(\"Error running child : \"\n    \t        + StringUtils.stringifyException(throwable));\n      if (taskid != null) {\n        if (!ShutdownHookManager.get().isShutdownInProgress()) {\n          Throwable tCause = throwable.getCause();\n          String cause =\n              tCause == null ? throwable.getMessage() : StringUtils\n                  .stringifyException(tCause);\n          umbilical.fatalError(taskid, cause);\n        }\n      }\n    } finally {\n      RPC.stopProxy(umbilical);\n      DefaultMetricsSystem.shutdown();\n      TaskLog.syncLogsShutdown(logSyncer);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapred.YarnChild.configureTask": "  private static void configureTask(JobConf job, Task task,\n      Credentials credentials, Token<JobTokenIdentifier> jt) throws IOException {\n    job.setCredentials(credentials);\n    \n    ApplicationAttemptId appAttemptId =\n        ConverterUtils.toContainerId(\n            System.getenv(Environment.CONTAINER_ID.name()))\n            .getApplicationAttemptId();\n    LOG.debug(\"APPLICATION_ATTEMPT_ID: \" + appAttemptId);\n    // Set it in conf, so as to be able to be used the the OutputCommitter.\n    job.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID,\n        appAttemptId.getAttemptId());\n\n    // set tcp nodelay\n    job.setBoolean(\"ipc.client.tcpnodelay\", true);\n    job.setClass(MRConfig.TASK_LOCAL_OUTPUT_CLASS,\n        YarnOutputFiles.class, MapOutputFile.class);\n    // set the jobToken and shuffle secrets into task\n    task.setJobTokenSecret(\n        JobTokenSecretManager.createSecretKey(jt.getPassword()));\n    byte[] shuffleSecret = TokenCache.getShuffleSecretKey(credentials);\n    if (shuffleSecret == null) {\n      LOG.warn(\"Shuffle secret missing from task credentials.\"\n          + \" Using job token secret as shuffle secret.\");\n      shuffleSecret = jt.getPassword();\n    }\n    task.setShuffleSecret(\n        JobTokenSecretManager.createSecretKey(shuffleSecret));\n\n    // setup the child's MRConfig.LOCAL_DIR.\n    configureLocalDirs(task, job);\n\n    // setup the child's attempt directories\n    // Do the task-type specific localization\n    task.localizeConfiguration(job);\n\n    // Set up the DistributedCache related configs\n    MRApps.setupDistributedCacheLocal(job);\n\n    // Overwrite the localized task jobconf which is linked to in the current\n    // work-dir.\n    Path localTaskFile = new Path(MRJobConfig.JOB_CONF_FILE);\n    writeLocalJobFile(localTaskFile, job);\n    task.setJobFile(localTaskFile.toString());\n    task.setConf(job);\n  }",
            "hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3.INode.deserialize": "  public static INode deserialize(InputStream in) throws IOException {\n    if (in == null) {\n      return null;\n    }\n    DataInputStream dataIn = new DataInputStream(in);\n    FileType fileType = INode.FILE_TYPES[dataIn.readByte()];\n    switch (fileType) {\n    case DIRECTORY:\n      in.close();\n      return INode.DIRECTORY_INODE;\n    case FILE:\n      int numBlocks = dataIn.readInt();\n      Block[] blocks = new Block[numBlocks];\n      for (int i = 0; i < numBlocks; i++) {\n        long id = dataIn.readLong();\n        long length = dataIn.readLong();\n        blocks[i] = new Block(id, length);\n      }\n      in.close();\n      return new INode(fileType, blocks);\n    default:\n      throw new IllegalArgumentException(\"Cannot deserialize inode.\");\n    }    \n  }  ",
            "hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3.FileSystemStore.retrieveINode": "  INode retrieveINode(Path path) throws IOException;\n  File retrieveBlock(Block block, long byteRangeStart) throws IOException;\n\n  void deleteINode(Path path) throws IOException;\n  void deleteBlock(Block block) throws IOException;\n\n  Set<Path> listSubPaths(Path path) throws IOException;\n  Set<Path> listDeepSubPaths(Path path) throws IOException;\n\n  /**\n   * Delete everything. Used for testing.\n   * @throws IOException\n   */\n  void purge() throws IOException;\n  \n  /**\n   * Diagnostic method to dump all INodes to the console.\n   * @throws IOException\n   */\n  void dump() throws IOException;\n}",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getNumReduceTasks": "  public int getNumReduceTasks() { return getInt(JobContext.NUM_REDUCES, 1); }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getWorkingDirectory": "  public Path getWorkingDirectory() {\n    String name = get(JobContext.WORKING_DIR);\n    if (name != null) {\n      return new Path(name);\n    } else {\n      try {\n        Path dir = FileSystem.get(this).getWorkingDirectory();\n        set(JobContext.WORKING_DIR, dir.toString());\n        return dir;\n      } catch (IOException e) {\n        throw new RuntimeException(e);\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.taskCleanup": "  void taskCleanup(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    // set phase for this task\n    setPhase(TaskStatus.Phase.CLEANUP);\n    getProgress().setStatus(\"cleanup\");\n    statusUpdate(umbilical);\n    LOG.info(\"Runnning cleanup for the task\");\n    // do the cleanup\n    committer.abortTask(taskContext);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setPhase": "  protected synchronized void setPhase(TaskStatus.Phase phase){\n    this.taskStatus.setPhase(phase); \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getProgress": "    public float getProgress() {\n      return taskProgress.getProgress();\n    };",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.setStatus": "    public void setStatus(String status) {\n      taskProgress.setStatus(normalizeStatus(status, conf));\n      // indicate that progress update needs to be sent\n      setProgressFlag();\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.statusUpdate": "  public void statusUpdate(TaskUmbilicalProtocol umbilical) \n  throws IOException {\n    int retries = MAX_RETRIES;\n    while (true) {\n      try {\n        if (!umbilical.statusUpdate(getTaskID(), taskStatus).getTaskFound()) {\n          LOG.warn(\"Parent died.  Exiting \"+taskId);\n          System.exit(66);\n        }\n        taskStatus.clearStatus();\n        return;\n      } catch (InterruptedException ie) {\n        Thread.currentThread().interrupt(); // interrupt ourself\n      } catch (IOException ie) {\n        LOG.warn(\"Failure sending status update: \" + \n                  StringUtils.stringifyException(ie));\n        if (--retries == 0) {\n          throw ie;\n        }\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.forName": "  public static TaskAttemptID forName(String str\n                                      ) throws IllegalArgumentException {\n    return (TaskAttemptID) \n             org.apache.hadoop.mapreduce.TaskAttemptID.forName(str);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.getTask": "  JvmTask getTask(JvmContext context) throws IOException;\n  \n  /**\n   * Report child's progress to parent. Also invoked to report still alive (used\n   * to be in ping). It reports an AMFeedback used to propagate preemption requests.\n   * \n   * @param taskId task-id of the child\n   * @param taskStatus status of the child\n   * @throws IOException\n   * @throws InterruptedException\n   * @return True if the task is known\n   */\n  AMFeedback statusUpdate(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;\n  \n  /** Report error messages back to parent.  Calls should be sparing, since all\n   *  such messages are held in the job tracker.\n   *  @param taskid the id of the task involved\n   *  @param trace the text to report\n   */\n  void reportDiagnosticInfo(TaskAttemptID taskid, String trace) throws IOException;\n  \n  /**\n   * Report the record range which is going to process next by the Task.\n   * @param taskid the id of the task involved\n   * @param range the range of record sequence nos\n   * @throws IOException\n   */\n  void reportNextRecordRange(TaskAttemptID taskid, SortedRanges.Range range) \n    throws IOException;\n\n  /** Report that the task is successfully completed.  Failure is assumed if\n   * the task process exits without calling this.\n   * @param taskid task's id\n   */\n  void done(TaskAttemptID taskid) throws IOException;\n  \n  /** \n   * Report that the task is complete, but its commit is pending.\n   * \n   * @param taskId task's id\n   * @param taskStatus status of the child\n   * @throws IOException\n   */\n  void commitPending(TaskAttemptID taskId, TaskStatus taskStatus) \n  throws IOException, InterruptedException;  \n\n  /**\n   * Polling to know whether the task can go-ahead with commit \n   * @param taskid\n   * @return true/false \n   * @throws IOException\n   */\n  boolean canCommit(TaskAttemptID taskid) throws IOException;\n\n  /** Report that a reduce-task couldn't shuffle map-outputs.*/\n  void shuffleError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Report that the task encounted a local filesystem error.*/\n  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JobConf.getSessionId": "  public String getSessionId() {\n      return get(\"session.id\", \"\");\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fatalError": "  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskAttemptID.getJobID": "  public JobID getJobID() {\n    return (JobID) super.getJobID();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.createLogSyncer": "  public static ScheduledExecutorService createLogSyncer() {\n    final ScheduledExecutorService scheduler =\n      Executors.newSingleThreadScheduledExecutor(\n        new ThreadFactory() {\n          @Override\n          public Thread newThread(Runnable r) {\n            final Thread t = Executors.defaultThreadFactory().newThread(r);\n            t.setDaemon(true);\n            t.setName(\"Thread for syncLogs\");\n            return t;\n          }\n        });\n    ShutdownHookManager.get().addShutdownHook(new Runnable() {\n        @Override\n        public void run() {\n          TaskLog.syncLogsShutdown(scheduler);\n        }\n      }, 50);\n    scheduler.scheduleWithFixedDelay(\n        new Runnable() {\n          @Override\n          public void run() {\n            TaskLog.syncLogs();\n          }\n        }, 0L, 5L, TimeUnit.SECONDS);\n    return scheduler;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.syncLogsShutdown": "  public static synchronized void syncLogsShutdown(\n    ScheduledExecutorService scheduler) \n  {\n    // flush standard streams\n    //\n    System.out.flush();\n    System.err.flush();\n\n    if (scheduler != null) {\n      scheduler.shutdownNow();\n    }\n\n    // flush & close all appenders\n    LogManager.shutdown(); \n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.syncLogs": "  public static synchronized void syncLogs() {\n    // flush standard streams\n    //\n    System.out.flush();\n    System.err.flush();\n\n    // flush flushable appenders\n    //\n    final Logger rootLogger = Logger.getRootLogger();\n    flushAppenders(rootLogger);\n    final Enumeration<Logger> allLoggers = rootLogger.getLoggerRepository().\n      getCurrentLoggers();\n    while (allLoggers.hasMoreElements()) {\n      final Logger l = allLoggers.nextElement();\n      flushAppenders(l);\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskLog.newThread": "          public Thread newThread(Runnable r) {\n            final Thread t = Executors.defaultThreadFactory().newThread(r);\n            t.setDaemon(true);\n            t.setName(\"Thread for syncLogs\");\n            return t;\n          }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.shouldDie": "  public boolean shouldDie() {\n    return shouldDie;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JvmTask.getTask": "  public Task getTask() {\n    return t;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.TaskUmbilicalProtocol.fsError": "  void fsError(TaskAttemptID taskId, String message) throws IOException;\n\n  /** Report that the task encounted a fatal error.*/\n  void fatalError(TaskAttemptID taskId, String message) throws IOException;\n  \n  /** Called by a reduce task to get the map output locations for finished maps.\n   * Returns an update centered around the map-task-completion-events. \n   * The update also piggybacks the information whether the events copy at the \n   * task-tracker has changed or not. This will trigger some action at the \n   * child-process.\n   *\n   * @param fromIndex the index starting from which the locations should be \n   * fetched\n   * @param maxLocs the max number of locations to fetch\n   * @param id The attempt id of the task that is trying to communicate\n   * @return A {@link MapTaskCompletionEventsUpdate} ",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.toString": "  public String toString() { \n    return appendTo(new StringBuilder(JVM)).toString();\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.JVMId.appendTo": "  protected StringBuilder appendTo(StringBuilder builder) {\n    return jobId.appendTo(builder).\n                 append(SEPARATOR).\n                 append(isMap ? 'm' : 'r').\n                 append(SEPARATOR).\n                 append(idFormat.format(jvmId));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.getTaskID": "  public TaskAttemptID getTaskID() { return taskId; }"
        },
        "bug_report": {
            "Title": "S3 filesystem operations stopped working correctly",
            "Description": "HADOOP-10542 was resolved by replacing \"return null;\" with throwing  IOException.   This causes several S3 filesystem operations to fail (possibly more code is expecting that null return value; these are just the calls I noticed):\n\nS3FileSystem.getFileStatus() (which no longer raises FileNotFoundException but instead IOException)\nFileSystem.exists() (which no longer returns false but instead raises IOException)\nS3FileSystem.create() (which no longer succeeds but instead raises IOException)\n\nRun command:\n\nhadoop distcp hdfs://localhost:9000/test s3://xxx:yyy@com.bar.foo/\n\nResulting stack trace:\n\n2015-12-11 10:04:34,030 FATAL [IPC Server handler 6 on 44861] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1449826461866_0005_m_000006_0 - exited : java.io.IOException: /test doesn't exist\nat org.apache.hadoop.fs.s3.Jets3tFileSystemStore.get(Jets3tFileSystemStore.java:170)\nat org.apache.hadoop.fs.s3.Jets3tFileSystemStore.retrieveINode(Jets3tFileSystemStore.java:221)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\nat com.sun.proxy.$Proxy17.retrieveINode(Unknown Source)\nat org.apache.hadoop.fs.s3.S3FileSystem.getFileStatus(S3FileSystem.java:340)\nat org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:230)\nat org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:50)\nat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)\nat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n\nchanging the \"raise IOE...\" to \"return null\" fixes all of the above code sites and allows distcp to succeed.\n"
        }
    },
    {
        "filename": "HADOOP-13132.json",
        "creation_time": "2016-05-11T11:42:30.000+0000",
        "stack_trace": "```\njava.lang.ClassCastException: org.apache.hadoop.security.authentication.client.AuthenticationException cannot be cast to java.security.GeneralSecurityException\nat org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.decryptEncryptedKey(LoadBalancingKMSClientProvider.java:189)\nat org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:388)\nat org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1419)\nat org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1521)\nat org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:108)\nat org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:59)\nat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:577)\nat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:683)\nat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:679)\nat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\nat org.apache.hadoop.fs.FileContext.create(FileContext.java:679)\nat org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter$1.run(AggregatedLogFormat.java:382)\nat org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter$1.run(AggregatedLogFormat.java:377)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:422)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\nat org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter.<init>(AggregatedLogFormat.java:376)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainers(AppLogAggregatorImpl.java:246)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregation(AppLogAggregatorImpl.java:456)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run(AppLogAggregatorImpl.java:421)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$2.run(LogAggregationService.java:384)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.decryptEncryptedKey": "  public KeyVersion\n      decryptEncryptedKey(final EncryptedKeyVersion encryptedKeyVersion)\n          throws IOException, GeneralSecurityException {\n    try {\n      return doOp(new ProviderCallable<KeyVersion>() {\n        @Override\n        public KeyVersion call(KMSClientProvider provider)\n            throws IOException, GeneralSecurityException {\n          return provider.decryptEncryptedKey(encryptedKeyVersion);\n        }\n      }, nextIdx());\n    } catch (WrapperException we) {\n      throw (GeneralSecurityException)we.getCause();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.doOp": "  private <T> T doOp(ProviderCallable<T> op, int currPos)\n      throws IOException {\n    IOException ex = null;\n    for (int i = 0; i < providers.length; i++) {\n      KMSClientProvider provider = providers[(currPos + i) % providers.length];\n      try {\n        return op.call(provider);\n      } catch (IOException ioe) {\n        LOG.warn(\"KMS provider at [{}] threw an IOException [{}]!!\",\n            provider.getKMSUrl(), ioe.getMessage());\n        ex = ioe;\n      } catch (Exception e) {\n        if (e instanceof RuntimeException) {\n          throw (RuntimeException)e;\n        } else {\n          throw new WrapperException(e);\n        }\n      }\n    }\n    if (ex != null) {\n      LOG.warn(\"Aborting since the Request has failed with all KMS\"\n          + \" providers in the group. !!\");\n      throw ex;\n    }\n    throw new IOException(\"No providers configured !!\");\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.nextIdx": "  private int nextIdx() {\n    while (true) {\n      int current = currentIdx.get();\n      int next = (current + 1) % providers.length;\n      if (currentIdx.compareAndSet(current, next)) {\n        return current;\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey": "  public KeyVersion decryptEncryptedKey(EncryptedKeyVersion encryptedKey) \n      throws IOException, GeneralSecurityException {\n    return getExtension().decryptEncryptedKey(encryptedKey);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.getEncryptionKeyVersionName": "    public String getEncryptionKeyVersionName() {\n      return encryptionKeyVersionName;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.deriveIV": "    protected static byte[] deriveIV(byte[] encryptedKeyIV) {\n      byte[] rIv = new byte[encryptedKeyIV.length];\n      // Do a simple XOR transformation to flip all the bits\n      for (int i = 0; i < encryptedKeyIV.length; i++) {\n        rIv[i] = (byte) (encryptedKeyIV[i] ^ 0xff);\n      }\n      return rIv;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.getEncryptedKeyIv": "    public byte[] getEncryptedKeyIv() {\n      return encryptedKeyIv;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.getEncryptedKeyVersion": "    public KeyVersion getEncryptedKeyVersion() {\n      return encryptedKeyVersion;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.AbstractFileSystem.create": "  public final FSDataOutputStream create(final Path f,\n      final EnumSet<CreateFlag> createFlag, Options.CreateOpts... opts)\n      throws AccessControlException, FileAlreadyExistsException,\n      FileNotFoundException, ParentNotDirectoryException,\n      UnsupportedFileSystemException, UnresolvedLinkException, IOException {\n    checkPath(f);\n    int bufferSize = -1;\n    short replication = -1;\n    long blockSize = -1;\n    int bytesPerChecksum = -1;\n    ChecksumOpt checksumOpt = null;\n    FsPermission permission = null;\n    Progressable progress = null;\n    Boolean createParent = null;\n \n    for (CreateOpts iOpt : opts) {\n      if (CreateOpts.BlockSize.class.isInstance(iOpt)) {\n        if (blockSize != -1) {\n          throw new HadoopIllegalArgumentException(\n              \"BlockSize option is set multiple times\");\n        }\n        blockSize = ((CreateOpts.BlockSize) iOpt).getValue();\n      } else if (CreateOpts.BufferSize.class.isInstance(iOpt)) {\n        if (bufferSize != -1) {\n          throw new HadoopIllegalArgumentException(\n              \"BufferSize option is set multiple times\");\n        }\n        bufferSize = ((CreateOpts.BufferSize) iOpt).getValue();\n      } else if (CreateOpts.ReplicationFactor.class.isInstance(iOpt)) {\n        if (replication != -1) {\n          throw new HadoopIllegalArgumentException(\n              \"ReplicationFactor option is set multiple times\");\n        }\n        replication = ((CreateOpts.ReplicationFactor) iOpt).getValue();\n      } else if (CreateOpts.BytesPerChecksum.class.isInstance(iOpt)) {\n        if (bytesPerChecksum != -1) {\n          throw new HadoopIllegalArgumentException(\n              \"BytesPerChecksum option is set multiple times\");\n        }\n        bytesPerChecksum = ((CreateOpts.BytesPerChecksum) iOpt).getValue();\n      } else if (CreateOpts.ChecksumParam.class.isInstance(iOpt)) {\n        if (checksumOpt != null) {\n          throw new  HadoopIllegalArgumentException(\n              \"CreateChecksumType option is set multiple times\");\n        }\n        checksumOpt = ((CreateOpts.ChecksumParam) iOpt).getValue();\n      } else if (CreateOpts.Perms.class.isInstance(iOpt)) {\n        if (permission != null) {\n          throw new HadoopIllegalArgumentException(\n              \"Perms option is set multiple times\");\n        }\n        permission = ((CreateOpts.Perms) iOpt).getValue();\n      } else if (CreateOpts.Progress.class.isInstance(iOpt)) {\n        if (progress != null) {\n          throw new HadoopIllegalArgumentException(\n              \"Progress option is set multiple times\");\n        }\n        progress = ((CreateOpts.Progress) iOpt).getValue();\n      } else if (CreateOpts.CreateParent.class.isInstance(iOpt)) {\n        if (createParent != null) {\n          throw new HadoopIllegalArgumentException(\n              \"CreateParent option is set multiple times\");\n        }\n        createParent = ((CreateOpts.CreateParent) iOpt).getValue();\n      } else {\n        throw new HadoopIllegalArgumentException(\"Unkown CreateOpts of type \" +\n            iOpt.getClass().getName());\n      }\n    }\n    if (permission == null) {\n      throw new HadoopIllegalArgumentException(\"no permission supplied\");\n    }\n\n\n    FsServerDefaults ssDef = getServerDefaults();\n    if (ssDef.getBlockSize() % ssDef.getBytesPerChecksum() != 0) {\n      throw new IOException(\"Internal error: default blockSize is\" + \n          \" not a multiple of default bytesPerChecksum \");\n    }\n    \n    if (blockSize == -1) {\n      blockSize = ssDef.getBlockSize();\n    }\n\n    // Create a checksum option honoring user input as much as possible.\n    // If bytesPerChecksum is specified, it will override the one set in\n    // checksumOpt. Any missing value will be filled in using the default.\n    ChecksumOpt defaultOpt = new ChecksumOpt(\n        ssDef.getChecksumType(),\n        ssDef.getBytesPerChecksum());\n    checksumOpt = ChecksumOpt.processChecksumOpt(defaultOpt,\n        checksumOpt, bytesPerChecksum);\n\n    if (bufferSize == -1) {\n      bufferSize = ssDef.getFileBufferSize();\n    }\n    if (replication == -1) {\n      replication = ssDef.getReplication();\n    }\n    if (createParent == null) {\n      createParent = false;\n    }\n\n    if (blockSize % bytesPerChecksum != 0) {\n      throw new HadoopIllegalArgumentException(\n             \"blockSize should be a multiple of checksumsize\");\n    }\n\n    return this.createInternal(f, createFlag, permission, bufferSize,\n      replication, blockSize, progress, checksumOpt, createParent);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.AbstractFileSystem.getServerDefaults": "  public abstract FsServerDefaults getServerDefaults() throws IOException; \n\n  /**\n   * Return the fully-qualified path of path f resolving the path\n   * through any internal symlinks or mount point\n   * @param p path to be resolved\n   * @return fully qualified path \n   * @throws FileNotFoundException, AccessControlException, IOException\n   *         UnresolvedLinkException if symbolic link on path cannot be resolved\n   *          internally\n   */\n   public Path resolvePath(final Path p) throws FileNotFoundException,\n           UnresolvedLinkException, AccessControlException, IOException {\n     checkPath(p);\n     return getFileStatus(p).getPath(); // default impl is to return the path\n   }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.AbstractFileSystem.createInternal": "  public abstract FSDataOutputStream createInternal(Path f,\n      EnumSet<CreateFlag> flag, FsPermission absolutePermission,\n      int bufferSize, short replication, long blockSize, Progressable progress,\n      ChecksumOpt checksumOpt, boolean createParent)\n      throws AccessControlException, FileAlreadyExistsException,\n      FileNotFoundException, ParentNotDirectoryException,\n      UnsupportedFileSystemException, UnresolvedLinkException, IOException;\n\n  /**\n   * The specification of this method matches that of\n   * {@link FileContext#mkdir(Path, FsPermission, boolean)} except that the Path",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.AbstractFileSystem.checkPath": "  public void checkPath(Path path) {\n    URI uri = path.toUri();\n    String thatScheme = uri.getScheme();\n    String thatAuthority = uri.getAuthority();\n    if (thatScheme == null) {\n      if (thatAuthority == null) {\n        if (path.isUriPathAbsolute()) {\n          return;\n        }\n        throw new InvalidPathException(\"relative paths not allowed:\" + \n            path);\n      } else {\n        throw new InvalidPathException(\n            \"Path without scheme with non-null authority:\" + path);\n      }\n    }\n    String thisScheme = this.getUri().getScheme();\n    String thisHost = this.getUri().getHost();\n    String thatHost = uri.getHost();\n    \n    // Schemes and hosts must match.\n    // Allow for null Authority for file:///\n    if (!thisScheme.equalsIgnoreCase(thatScheme) ||\n       (thisHost != null && \n            !thisHost.equalsIgnoreCase(thatHost)) ||\n       (thisHost == null && thatHost != null)) {\n      throw new InvalidPathException(\"Wrong FS: \" + path + \", expected: \"\n          + this.getUri());\n    }\n    \n    // Ports must match, unless this FS instance is using the default port, in\n    // which case the port may be omitted from the given URI\n    int thisPort = this.getUri().getPort();\n    int thatPort = uri.getPort();\n    if (thatPort == -1) { // -1 => defaultPort of Uri scheme\n      thatPort = this.getUriDefaultPort();\n    }\n    if (thisPort != thatPort) {\n      throw new InvalidPathException(\"Wrong FS: \" + path + \", expected: \"\n          + this.getUri());\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.next": "      public BlockStoragePolicySpi next(final AbstractFileSystem fs,\n          final Path p)\n          throws IOException {\n        return fs.getStoragePolicy(p);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.setStoragePolicy": "  public void setStoragePolicy(final Path path, final String policyName)\n      throws IOException {\n    final Path absF = fixRelativePart(path);\n    new FSLinkResolver<Void>() {\n      @Override\n      public Void next(final AbstractFileSystem fs, final Path p)\n          throws IOException {\n        fs.setStoragePolicy(path, policyName);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.create": "  public FSDataOutputStream create(final Path f,\n      final EnumSet<CreateFlag> createFlag, Options.CreateOpts... opts)\n      throws AccessControlException, FileAlreadyExistsException,\n      FileNotFoundException, ParentNotDirectoryException,\n      UnsupportedFileSystemException, IOException {\n    Path absF = fixRelativePart(f);\n\n    // If one of the options is a permission, extract it & apply umask\n    // If not, add a default Perms and apply umask;\n    // AbstractFileSystem#create\n\n    CreateOpts.Perms permOpt = CreateOpts.getOpt(CreateOpts.Perms.class, opts);\n    FsPermission permission = (permOpt != null) ? permOpt.getValue() :\n                                      FILE_DEFAULT_PERM;\n    permission = permission.applyUMask(umask);\n\n    final CreateOpts[] updatedOpts = \n                      CreateOpts.setOpt(CreateOpts.perms(permission), opts);\n    return new FSLinkResolver<FSDataOutputStream>() {\n      @Override\n      public FSDataOutputStream next(final AbstractFileSystem fs, final Path p) \n        throws IOException {\n        return fs.create(p, createFlag, updatedOpts);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.hasNext": "        public boolean hasNext() throws IOException {\n          while (curFile == null) {\n            if (curItor.hasNext()) {\n              handleFileStat(curItor.next());\n            } else if (!itors.empty()) {\n              curItor = itors.pop();\n            } else {\n              return false;\n            }\n          }\n          return true;\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.listStatus": "    public FileStatus[] listStatus(final Path f) throws AccessControlException,\n        FileNotFoundException, UnsupportedFileSystemException,\n        IOException {\n      final Path absF = fixRelativePart(f);\n      return new FSLinkResolver<FileStatus[]>() {\n        @Override\n        public FileStatus[] next(final AbstractFileSystem fs, final Path p) \n          throws IOException, UnresolvedLinkException {\n          return fs.listStatus(p);\n        }\n      }.resolve(FileContext.this, absF);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.removeAclEntries": "  public void removeAclEntries(final Path path, final List<AclEntry> aclSpec)\n      throws IOException {\n    Path absF = fixRelativePart(path);\n    new FSLinkResolver<Void>() {\n      @Override\n      public Void next(final AbstractFileSystem fs, final Path p)\n          throws IOException {\n        fs.removeAclEntries(p, aclSpec);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.createSymlink": "  public void createSymlink(final Path target, final Path link,\n      final boolean createParent) throws AccessControlException,\n      FileAlreadyExistsException, FileNotFoundException,\n      ParentNotDirectoryException, UnsupportedFileSystemException, \n      IOException { \n    if (!FileSystem.areSymlinksEnabled()) {\n      throw new UnsupportedOperationException(\"Symlinks not supported\");\n    }\n    final Path nonRelLink = fixRelativePart(link);\n    new FSLinkResolver<Void>() {\n      @Override\n      public Void next(final AbstractFileSystem fs, final Path p) \n        throws IOException, UnresolvedLinkException {\n        fs.createSymlink(target, p, createParent);\n        return null;\n      }\n    }.resolve(this, nonRelLink);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.setTimes": "  public void setTimes(final Path f, final long mtime, final long atime)\n      throws AccessControlException, FileNotFoundException,\n      UnsupportedFileSystemException, IOException {\n    final Path absF = fixRelativePart(f);\n    new FSLinkResolver<Void>() {\n      @Override\n      public Void next(final AbstractFileSystem fs, final Path p) \n        throws IOException, UnresolvedLinkException {\n        fs.setTimes(p, mtime, atime);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.getXAttr": "  public byte[] getXAttr(Path path, final String name) throws IOException {\n    final Path absF = fixRelativePart(path);\n    return new FSLinkResolver<byte[]>() {\n      @Override\n      public byte[] next(final AbstractFileSystem fs, final Path p)\n          throws IOException {\n        return fs.getXAttr(p, name);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.unsetStoragePolicy": "  public void unsetStoragePolicy(final Path src) throws IOException {\n    final Path absF = fixRelativePart(src);\n    new FSLinkResolver<Void>() {\n      @Override\n      public Void next(final AbstractFileSystem fs, final Path p)\n          throws IOException {\n        fs.unsetStoragePolicy(src);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.delete": "  public boolean delete(final Path f, final boolean recursive)\n      throws AccessControlException, FileNotFoundException,\n      UnsupportedFileSystemException, IOException {\n    Path absF = fixRelativePart(f);\n    return new FSLinkResolver<Boolean>() {\n      @Override\n      public Boolean next(final AbstractFileSystem fs, final Path p) \n        throws IOException, UnresolvedLinkException {\n        return fs.delete(p, recursive);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.open": "  public FSDataInputStream open(final Path f, final int bufferSize)\n      throws AccessControlException, FileNotFoundException,\n      UnsupportedFileSystemException, IOException {\n    final Path absF = fixRelativePart(f);\n    return new FSLinkResolver<FSDataInputStream>() {\n      @Override\n      public FSDataInputStream next(final AbstractFileSystem fs, final Path p) \n        throws IOException, UnresolvedLinkException {\n        return fs.open(p, bufferSize);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.resolvePath": "  public Path resolvePath(final Path f) throws FileNotFoundException,\n      UnresolvedLinkException, AccessControlException, IOException {\n    return resolve(f);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.setXAttr": "  public void setXAttr(Path path, final String name, final byte[] value,\n      final EnumSet<XAttrSetFlag> flag) throws IOException {\n    final Path absF = fixRelativePart(path);\n    new FSLinkResolver<Void>() {\n      @Override\n      public Void next(final AbstractFileSystem fs, final Path p)\n          throws IOException {\n        fs.setXAttr(p, name, value, flag);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.getFileStatus": "  public FileStatus getFileStatus(final Path f) throws AccessControlException,\n      FileNotFoundException, UnsupportedFileSystemException, IOException {\n    final Path absF = fixRelativePart(f);\n    return new FSLinkResolver<FileStatus>() {\n      @Override\n      public FileStatus next(final AbstractFileSystem fs, final Path p) \n        throws IOException, UnresolvedLinkException {\n        return fs.getFileStatus(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.removeXAttr": "  public void removeXAttr(Path path, final String name) throws IOException {\n    final Path absF = fixRelativePart(path);\n    new FSLinkResolver<Void>() {\n      @Override\n      public Void next(final AbstractFileSystem fs, final Path p)\n          throws IOException {\n        fs.removeXAttr(p, name);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.getStoragePolicy": "  public BlockStoragePolicySpi getStoragePolicy(Path path) throws IOException {\n    final Path absF = fixRelativePart(path);\n    return new FSLinkResolver<BlockStoragePolicySpi>() {\n      @Override\n      public BlockStoragePolicySpi next(final AbstractFileSystem fs,\n          final Path p)\n          throws IOException {\n        return fs.getStoragePolicy(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.listLocatedStatus": "  public RemoteIterator<LocatedFileStatus> listLocatedStatus(\n      final Path f) throws\n      AccessControlException, FileNotFoundException,\n      UnsupportedFileSystemException, IOException {\n    final Path absF = fixRelativePart(f);\n    return new FSLinkResolver<RemoteIterator<LocatedFileStatus>>() {\n      @Override\n      public RemoteIterator<LocatedFileStatus> next(\n          final AbstractFileSystem fs, final Path p) \n        throws IOException, UnresolvedLinkException {\n        return fs.listLocatedStatus(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.truncate": "  public boolean truncate(final Path f, final long newLength)\n      throws AccessControlException, FileNotFoundException,\n      UnsupportedFileSystemException, IOException {\n    final Path absF = fixRelativePart(f);\n    return new FSLinkResolver<Boolean>() {\n      @Override\n      public Boolean next(final AbstractFileSystem fs, final Path p)\n          throws IOException, UnresolvedLinkException {\n        return fs.truncate(p, newLength);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.removeAcl": "  public void removeAcl(Path path) throws IOException {\n    Path absF = fixRelativePart(path);\n    new FSLinkResolver<Void>() {\n      @Override\n      public Void next(final AbstractFileSystem fs, final Path p)\n          throws IOException {\n        fs.removeAcl(p);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.getFsStatus": "  public FsStatus getFsStatus(final Path f) throws AccessControlException,\n      FileNotFoundException, UnsupportedFileSystemException, IOException {\n    if (f == null) {\n      return defaultFS.getFsStatus();\n    }\n    final Path absF = fixRelativePart(f);\n    return new FSLinkResolver<FsStatus>() {\n      @Override\n      public FsStatus next(final AbstractFileSystem fs, final Path p) \n        throws IOException, UnresolvedLinkException {\n        return fs.getFsStatus(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.getXAttrs": "  public Map<String, byte[]> getXAttrs(Path path, final List<String> names)\n      throws IOException {\n    final Path absF = fixRelativePart(path);\n    return new FSLinkResolver<Map<String, byte[]>>() {\n      @Override\n      public Map<String, byte[]> next(final AbstractFileSystem fs, final Path p)\n          throws IOException {\n        return fs.getXAttrs(p, names);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.setAcl": "  public void setAcl(Path path, final List<AclEntry> aclSpec)\n      throws IOException {\n    Path absF = fixRelativePart(path);\n    new FSLinkResolver<Void>() {\n      @Override\n      public Void next(final AbstractFileSystem fs, final Path p)\n          throws IOException {\n        fs.setAcl(p, aclSpec);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.access": "  public void access(final Path path, final FsAction mode)\n      throws AccessControlException, FileNotFoundException,\n      UnsupportedFileSystemException, IOException {\n    final Path absPath = fixRelativePart(path);\n    new FSLinkResolver<Void>() {\n      @Override\n      public Void next(AbstractFileSystem fs, Path p) throws IOException,\n          UnresolvedLinkException {\n        fs.access(p, mode);\n        return null;\n      }\n    }.resolve(this, absPath);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.createSnapshot": "  public Path createSnapshot(final Path path, final String snapshotName)\n      throws IOException {\n    final Path absF = fixRelativePart(path);\n    return new FSLinkResolver<Path>() {\n\n      @Override\n      public Path next(final AbstractFileSystem fs, final Path p)\n          throws IOException {\n        return fs.createSnapshot(p, snapshotName);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.removeDefaultAcl": "  public void removeDefaultAcl(Path path)\n      throws IOException {\n    Path absF = fixRelativePart(path);\n    new FSLinkResolver<Void>() {\n      @Override\n      public Void next(final AbstractFileSystem fs, final Path p)\n          throws IOException {\n        fs.removeDefaultAcl(p);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.renameSnapshot": "  public void renameSnapshot(final Path path, final String snapshotOldName,\n      final String snapshotNewName) throws IOException {\n    final Path absF = fixRelativePart(path);\n    new FSLinkResolver<Void>() {\n      @Override\n      public Void next(final AbstractFileSystem fs, final Path p)\n          throws IOException {\n        fs.renameSnapshot(p, snapshotOldName, snapshotNewName);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.mkdir": "  public void mkdir(final Path dir, final FsPermission permission,\n      final boolean createParent) throws AccessControlException,\n      FileAlreadyExistsException, FileNotFoundException,\n      ParentNotDirectoryException, UnsupportedFileSystemException, \n      IOException {\n    final Path absDir = fixRelativePart(dir);\n    final FsPermission absFerms = (permission == null ? \n          FsPermission.getDirDefault() : permission).applyUMask(umask);\n    new FSLinkResolver<Void>() {\n      @Override\n      public Void next(final AbstractFileSystem fs, final Path p) \n        throws IOException, UnresolvedLinkException {\n        fs.mkdir(p, absFerms, createParent);\n        return null;\n      }\n    }.resolve(this, absDir);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.getFileBlockLocations": "  public BlockLocation[] getFileBlockLocations(final Path f, final long start,\n      final long len) throws AccessControlException, FileNotFoundException,\n      UnsupportedFileSystemException, IOException {\n    final Path absF = fixRelativePart(f);\n    return new FSLinkResolver<BlockLocation[]>() {\n      @Override\n      public BlockLocation[] next(final AbstractFileSystem fs, final Path p) \n        throws IOException, UnresolvedLinkException {\n        return fs.getFileBlockLocations(p, start, len);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.modifyAclEntries": "  public void modifyAclEntries(final Path path, final List<AclEntry> aclSpec)\n      throws IOException {\n    Path absF = fixRelativePart(path);\n    new FSLinkResolver<Void>() {\n      @Override\n      public Void next(final AbstractFileSystem fs, final Path p)\n          throws IOException {\n        fs.modifyAclEntries(p, aclSpec);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.rename": "  public void rename(final Path src, final Path dst,\n      final Options.Rename... options) throws AccessControlException,\n      FileAlreadyExistsException, FileNotFoundException,\n      ParentNotDirectoryException, UnsupportedFileSystemException,\n      IOException {\n    final Path absSrc = fixRelativePart(src);\n    final Path absDst = fixRelativePart(dst);\n    AbstractFileSystem srcFS = getFSofPath(absSrc);\n    AbstractFileSystem dstFS = getFSofPath(absDst);\n    if(!srcFS.getUri().equals(dstFS.getUri())) {\n      throw new IOException(\"Renames across AbstractFileSystems not supported\");\n    }\n    try {\n      srcFS.rename(absSrc, absDst, options);\n    } catch (UnresolvedLinkException e) {\n      /* We do not know whether the source or the destination path\n       * was unresolved. Resolve the source path up until the final\n       * path component, then fully resolve the destination. \n       */\n      final Path source = resolveIntermediate(absSrc);    \n      new FSLinkResolver<Void>() {\n        @Override\n        public Void next(final AbstractFileSystem fs, final Path p) \n          throws IOException, UnresolvedLinkException {\n          fs.rename(source, p, options);\n          return null;\n        }\n      }.resolve(this, absDst);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.getFileLinkStatus": "  public FileStatus getFileLinkStatus(final Path f)\n      throws AccessControlException, FileNotFoundException,\n      UnsupportedFileSystemException, IOException {\n    final Path absF = fixRelativePart(f);\n    return new FSLinkResolver<FileStatus>() {\n      @Override\n      public FileStatus next(final AbstractFileSystem fs, final Path p) \n        throws IOException, UnresolvedLinkException {\n        FileStatus fi = fs.getFileLinkStatus(p);\n        if (fi.isSymlink()) {\n          fi.setSymlink(FSLinkResolver.qualifySymlinkTarget(fs.getUri(), p,\n              fi.getSymlink()));\n        }\n        return fi;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.setPermission": "  public void setPermission(final Path f, final FsPermission permission)\n      throws AccessControlException, FileNotFoundException,\n      UnsupportedFileSystemException, IOException {\n    final Path absF = fixRelativePart(f);\n    new FSLinkResolver<Void>() {\n      @Override\n      public Void next(final AbstractFileSystem fs, final Path p) \n        throws IOException, UnresolvedLinkException {\n        fs.setPermission(p, permission);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.getAclStatus": "  public AclStatus getAclStatus(Path path) throws IOException {\n    Path absF = fixRelativePart(path);\n    return new FSLinkResolver<AclStatus>() {\n      @Override\n      public AclStatus next(final AbstractFileSystem fs, final Path p)\n          throws IOException {\n        return fs.getAclStatus(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.setReplication": "  public boolean setReplication(final Path f, final short replication)\n      throws AccessControlException, FileNotFoundException,\n      IOException {\n    final Path absF = fixRelativePart(f);\n    return new FSLinkResolver<Boolean>() {\n      @Override\n      public Boolean next(final AbstractFileSystem fs, final Path p) \n        throws IOException, UnresolvedLinkException {\n        return fs.setReplication(p, replication);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.setOwner": "  public void setOwner(final Path f, final String username,\n      final String groupname) throws AccessControlException,\n      UnsupportedFileSystemException, FileNotFoundException,\n      IOException {\n    if ((username == null) && (groupname == null)) {\n      throw new HadoopIllegalArgumentException(\n          \"username and groupname cannot both be null\");\n    }\n    final Path absF = fixRelativePart(f);\n    new FSLinkResolver<Void>() {\n      @Override\n      public Void next(final AbstractFileSystem fs, final Path p) \n        throws IOException, UnresolvedLinkException {\n        fs.setOwner(p, username, groupname);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.getFileChecksum": "  public FileChecksum getFileChecksum(final Path f)\n      throws AccessControlException, FileNotFoundException,\n      IOException {\n    final Path absF = fixRelativePart(f);\n    return new FSLinkResolver<FileChecksum>() {\n      @Override\n      public FileChecksum next(final AbstractFileSystem fs, final Path p) \n        throws IOException, UnresolvedLinkException {\n        return fs.getFileChecksum(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.listCorruptFileBlocks": "  public RemoteIterator<Path> listCorruptFileBlocks(Path path)\n    throws IOException {\n    final Path absF = fixRelativePart(path);\n    return new FSLinkResolver<RemoteIterator<Path>>() {\n      @Override\n      public RemoteIterator<Path> next(final AbstractFileSystem fs,\n                                       final Path p) \n        throws IOException, UnresolvedLinkException {\n        return fs.listCorruptFileBlocks(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.listXAttrs": "  public List<String> listXAttrs(Path path) throws IOException {\n    final Path absF = fixRelativePart(path);\n    return new FSLinkResolver<List<String>>() {\n      @Override\n      public List<String> next(final AbstractFileSystem fs, final Path p)\n          throws IOException {\n        return fs.listXAttrs(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.deleteSnapshot": "  public void deleteSnapshot(final Path path, final String snapshotName)\n      throws IOException {\n    final Path absF = fixRelativePart(path);\n    new FSLinkResolver<Void>() {\n      @Override\n      public Void next(final AbstractFileSystem fs, final Path p)\n          throws IOException {\n        fs.deleteSnapshot(p, snapshotName);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FSLinkResolver.resolve": "  public T resolve(final FileContext fc, final Path path) throws IOException {\n    int count = 0;\n    T in = null;\n    Path p = path;\n    // NB: More than one AbstractFileSystem can match a scheme, eg \n    // \"file\" resolves to LocalFs but could have come by RawLocalFs.\n    AbstractFileSystem fs = fc.getFSofPath(p);\n\n    // Loop until all symlinks are resolved or the limit is reached\n    for (boolean isLink = true; isLink;) {\n      try {\n        in = next(fs, p);\n        isLink = false;\n      } catch (UnresolvedLinkException e) {\n        if (!fc.resolveSymlinks) {\n          throw new IOException(\"Path \" + path + \" contains a symlink\"\n              + \" and symlink resolution is disabled (\"\n              + CommonConfigurationKeys.FS_CLIENT_RESOLVE_REMOTE_SYMLINKS_KEY + \").\", e);\n        }\n        if (!FileSystem.areSymlinksEnabled()) {\n          throw new IOException(\"Symlink resolution is disabled in\"\n              + \" this version of Hadoop.\");\n        }\n        if (count++ > FsConstants.MAX_PATH_LINKS) {\n          throw new IOException(\"Possible cyclic loop while \" +\n                                \"following symbolic link \" + path);\n        }\n        // Resolve the first unresolved path component\n        p = qualifySymlinkTarget(fs.getUri(), p, fs.getLinkTarget(p));\n        fs = fc.getFSofPath(p);\n      }\n    }\n    return in;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FSLinkResolver.next": "  abstract public T next(final AbstractFileSystem fs, final Path p)\n      throws IOException, UnresolvedLinkException;\n\n  /**\n   * Performs the operation specified by the next function, calling it\n   * repeatedly until all symlinks in the given path are resolved.\n   * @param fc FileContext used to access file systems.\n   * @param path The path to resolve symlinks on.\n   * @return Generic type determined by the implementation of next.\n   * @throws IOException\n   */\n  public T resolve(final FileContext fc, final Path path) throws IOException {\n    int count = 0;\n    T in = null;\n    Path p = path;\n    // NB: More than one AbstractFileSystem can match a scheme, eg \n    // \"file\" resolves to LocalFs but could have come by RawLocalFs.\n    AbstractFileSystem fs = fc.getFSofPath(p);\n\n    // Loop until all symlinks are resolved or the limit is reached\n    for (boolean isLink = true; isLink;) {\n      try {\n        in = next(fs, p);\n        isLink = false;\n      } catch (UnresolvedLinkException e) {\n        if (!fc.resolveSymlinks) {\n          throw new IOException(\"Path \" + path + \" contains a symlink\"\n              + \" and symlink resolution is disabled (\"\n              + CommonConfigurationKeys.FS_CLIENT_RESOLVE_REMOTE_SYMLINKS_KEY + \").\", e);\n        }\n        if (!FileSystem.areSymlinksEnabled()) {\n          throw new IOException(\"Symlink resolution is disabled in\"\n              + \" this version of Hadoop.\");\n        }\n        if (count++ > FsConstants.MAX_PATH_LINKS) {\n          throw new IOException(\"Possible cyclic loop while \" +\n                                \"following symbolic link \" + path);\n        }\n        // Resolve the first unresolved path component\n        p = qualifySymlinkTarget(fs.getUri(), p, fs.getLinkTarget(p));\n        fs = fc.getFSofPath(p);\n      }\n    }\n    return in;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FSLinkResolver.qualifySymlinkTarget": "  public static Path qualifySymlinkTarget(final URI pathURI,\n      Path pathWithLink, Path target) {\n    // NB: makeQualified uses the target's scheme and authority, if\n    // specified, and the scheme and authority of pathURI, if not.\n    final URI targetUri = target.toUri();\n    final String scheme = targetUri.getScheme();\n    final String auth = targetUri.getAuthority();\n    return (scheme == null && auth == null) ? target.makeQualified(pathURI,\n        pathWithLink.getParent()) : target;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause == null) {\n        throw new RuntimeException(\"PrivilegedActionException with no \" +\n                \"underlying cause. UGI [\" + this + \"]\" +\": \" + pae, pae);\n      } else if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainers": "  private void uploadLogsForContainers(boolean appFinished) {\n    if (this.logAggregationDisabled) {\n      return;\n    }\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      Credentials systemCredentials =\n          context.getSystemCredentialsForApps().get(appId);\n      if (systemCredentials != null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Adding new framework-token for \" + appId\n              + \" for log-aggregation: \" + systemCredentials.getAllTokens()\n              + \"; userUgi=\" + userUgi);\n        }\n        // this will replace old token\n        userUgi.addCredentials(systemCredentials);\n      }\n    }\n\n    // Create a set of Containers whose logs will be uploaded in this cycle.\n    // It includes:\n    // a) all containers in pendingContainers: those containers are finished\n    //    and satisfy the ContainerLogAggregationPolicy.\n    // b) some set of running containers: For all the Running containers,\n    //    we use exitCode of 0 to find those which satisfy the\n    //    ContainerLogAggregationPolicy.\n    Set<ContainerId> pendingContainerInThisCycle = new HashSet<ContainerId>();\n    this.pendingContainers.drainTo(pendingContainerInThisCycle);\n    Set<ContainerId> finishedContainers =\n        new HashSet<ContainerId>(pendingContainerInThisCycle);\n    if (this.context.getApplications().get(this.appId) != null) {\n      for (Container container : this.context.getApplications()\n        .get(this.appId).getContainers().values()) {\n        ContainerType containerType =\n            container.getContainerTokenIdentifier().getContainerType();\n        if (shouldUploadLogs(new ContainerLogContext(\n            container.getContainerId(), containerType, 0))) {\n          pendingContainerInThisCycle.add(container.getContainerId());\n        }\n      }\n    }\n\n    LogWriter writer = null;\n    String diagnosticMessage = \"\";\n    boolean logAggregationSucceedInThisCycle = true;\n    try {\n      if (pendingContainerInThisCycle.isEmpty()) {\n        return;\n      }\n\n      logAggregationTimes++;\n\n      try {\n        writer =\n            new LogWriter(this.conf, this.remoteNodeTmpLogFileForApp,\n              this.userUgi);\n        // Write ACLs once when the writer is created.\n        writer.writeApplicationACLs(appAcls);\n        writer.writeApplicationOwner(this.userUgi.getShortUserName());\n\n      } catch (IOException e1) {\n        logAggregationSucceedInThisCycle = false;\n        LOG.error(\"Cannot create writer for app \" + this.applicationId\n            + \". Skip log upload this time. \", e1);\n        return;\n      }\n\n      boolean uploadedLogsInThisCycle = false;\n      for (ContainerId container : pendingContainerInThisCycle) {\n        ContainerLogAggregator aggregator = null;\n        if (containerLogAggregators.containsKey(container)) {\n          aggregator = containerLogAggregators.get(container);\n        } else {\n          aggregator = new ContainerLogAggregator(container);\n          containerLogAggregators.put(container, aggregator);\n        }\n        Set<Path> uploadedFilePathsInThisCycle =\n            aggregator.doContainerLogAggregation(writer, appFinished);\n        if (uploadedFilePathsInThisCycle.size() > 0) {\n          uploadedLogsInThisCycle = true;\n          this.delService.delete(this.userUgi.getShortUserName(), null,\n              uploadedFilePathsInThisCycle\n                  .toArray(new Path[uploadedFilePathsInThisCycle.size()]));\n        }\n\n        // This container is finished, and all its logs have been uploaded,\n        // remove it from containerLogAggregators.\n        if (finishedContainers.contains(container)) {\n          containerLogAggregators.remove(container);\n        }\n      }\n\n      // Before upload logs, make sure the number of existing logs\n      // is smaller than the configured NM log aggregation retention size.\n      if (uploadedLogsInThisCycle && logAggregationInRolling) {\n        cleanOldLogs();\n        cleanupOldLogTimes++;\n      }\n\n      if (writer != null) {\n        writer.close();\n        writer = null;\n      }\n\n      long currentTime = System.currentTimeMillis();\n      final Path renamedPath = this.rollingMonitorInterval <= 0\n              ? remoteNodeLogFileForApp : new Path(\n                remoteNodeLogFileForApp.getParent(),\n                remoteNodeLogFileForApp.getName() + \"_\"\n                    + currentTime);\n\n      final boolean rename = uploadedLogsInThisCycle;\n      try {\n        userUgi.doAs(new PrivilegedExceptionAction<Object>() {\n          @Override\n          public Object run() throws Exception {\n            FileSystem remoteFS = remoteNodeLogFileForApp.getFileSystem(conf);\n            if (rename) {\n              remoteFS.rename(remoteNodeTmpLogFileForApp, renamedPath);\n            } else {\n              remoteFS.delete(remoteNodeTmpLogFileForApp, false);\n            }\n            return null;\n          }\n        });\n        diagnosticMessage =\n            \"Log uploaded successfully for Application: \" + appId\n                + \" in NodeManager: \"\n                + LogAggregationUtils.getNodeString(nodeId) + \" at \"\n                + Times.format(currentTime) + \"\\n\";\n      } catch (Exception e) {\n        LOG.error(\n          \"Failed to move temporary log file to final location: [\"\n              + remoteNodeTmpLogFileForApp + \"] to [\"\n              + renamedPath + \"]\", e);\n        diagnosticMessage =\n            \"Log uploaded failed for Application: \" + appId\n                + \" in NodeManager: \"\n                + LogAggregationUtils.getNodeString(nodeId) + \" at \"\n                + Times.format(currentTime) + \"\\n\";\n        renameTemporaryLogFileFailed = true;\n        logAggregationSucceedInThisCycle = false;\n      }\n    } finally {\n      LogAggregationStatus logAggregationStatus =\n          logAggregationSucceedInThisCycle\n              ? LogAggregationStatus.RUNNING\n              : LogAggregationStatus.RUNNING_WITH_FAILURE;\n      sendLogAggregationReport(logAggregationStatus, diagnosticMessage);\n      if (appFinished) {\n        // If the app is finished, one extra final report with log aggregation\n        // status SUCCEEDED/FAILED will be sent to RM to inform the RM\n        // that the log aggregation in this NM is completed.\n        LogAggregationStatus finalLogAggregationStatus =\n            renameTemporaryLogFileFailed || !logAggregationSucceedInThisCycle\n                ? LogAggregationStatus.FAILED\n                : LogAggregationStatus.SUCCEEDED;\n        sendLogAggregationReport(finalLogAggregationStatus, \"\");\n      }\n\n      if (writer != null) {\n        writer.close();\n      }\n    }\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.sendLogAggregationReport": "  private void sendLogAggregationReport(\n      LogAggregationStatus logAggregationStatus, String diagnosticMessage) {\n    LogAggregationReport report =\n        Records.newRecord(LogAggregationReport.class);\n    report.setApplicationId(appId);\n    report.setDiagnosticMessage(diagnosticMessage);\n    report.setLogAggregationStatus(logAggregationStatus);\n    this.context.getLogAggregationStatusForApps().add(report);\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.cleanOldLogs": "  private void cleanOldLogs() {\n    try {\n      final FileSystem remoteFS =\n          this.remoteNodeLogFileForApp.getFileSystem(conf);\n      Path appDir =\n          this.remoteNodeLogFileForApp.getParent().makeQualified(\n            remoteFS.getUri(), remoteFS.getWorkingDirectory());\n      Set<FileStatus> status =\n          new HashSet<FileStatus>(Arrays.asList(remoteFS.listStatus(appDir)));\n\n      Iterable<FileStatus> mask =\n          Iterables.filter(status, new Predicate<FileStatus>() {\n            @Override\n            public boolean apply(FileStatus next) {\n              return next.getPath().getName()\n                .contains(LogAggregationUtils.getNodeString(nodeId))\n                && !next.getPath().getName().endsWith(\n                    LogAggregationUtils.TMP_FILE_SUFFIX);\n            }\n          });\n      status = Sets.newHashSet(mask);\n      // Normally, we just need to delete one oldest log\n      // before we upload a new log.\n      // If we can not delete the older logs in this cycle,\n      // we will delete them in next cycle.\n      if (status.size() >= this.retentionSize) {\n        // sort by the lastModificationTime ascending\n        List<FileStatus> statusList = new ArrayList<FileStatus>(status);\n        Collections.sort(statusList, new Comparator<FileStatus>() {\n          public int compare(FileStatus s1, FileStatus s2) {\n            return s1.getModificationTime() < s2.getModificationTime() ? -1\n                : s1.getModificationTime() > s2.getModificationTime() ? 1 : 0;\n          }\n        });\n        for (int i = 0 ; i <= statusList.size() - this.retentionSize; i++) {\n          final FileStatus remove = statusList.get(i);\n          try {\n            userUgi.doAs(new PrivilegedExceptionAction<Object>() {\n              @Override\n              public Object run() throws Exception {\n                remoteFS.delete(remove.getPath(), false);\n                return null;\n              }\n            });\n          } catch (Exception e) {\n            LOG.error(\"Failed to delete \" + remove.getPath(), e);\n          }\n        }\n      }\n    } catch (Exception e) {\n      LOG.error(\"Failed to clean old logs\", e);\n    }\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doContainerLogAggregation": "    public Set<Path> doContainerLogAggregation(LogWriter writer,\n        boolean appFinished) {\n      LOG.info(\"Uploading logs for container \" + containerId\n          + \". Current good log dirs are \"\n          + StringUtils.join(\",\", dirsHandler.getLogDirsForRead()));\n      final LogKey logKey = new LogKey(containerId);\n      final LogValue logValue =\n          new LogValue(dirsHandler.getLogDirsForRead(), containerId,\n            userUgi.getShortUserName(), logAggregationContext,\n            this.uploadedFileMeta, appFinished);\n      try {\n        writer.append(logKey, logValue);\n      } catch (Exception e) {\n        LOG.error(\"Couldn't upload logs for \" + containerId\n            + \". Skipping this container.\", e);\n        return new HashSet<Path>();\n      }\n      this.uploadedFileMeta.addAll(logValue\n        .getCurrentUpLoadedFileMeta());\n      // if any of the previous uploaded logs have been deleted,\n      // we need to remove them from alreadyUploadedLogs\n      Iterable<String> mask =\n          Iterables.filter(uploadedFileMeta, new Predicate<String>() {\n            @Override\n            public boolean apply(String next) {\n              return logValue.getAllExistingFilesMeta().contains(next);\n            }\n          });\n\n      this.uploadedFileMeta = Sets.newHashSet(mask);\n      return logValue.getCurrentUpLoadedFilesPath();\n    }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.shouldUploadLogs": "  private boolean shouldUploadLogs(ContainerLogContext logContext) {\n    return logAggPolicy.shouldDoLogAggregation(logContext);\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregation": "  private void doAppLogAggregation() {\n    while (!this.appFinishing.get() && !this.aborted.get()) {\n      synchronized(this) {\n        try {\n          waiting.set(true);\n          if (logAggregationInRolling) {\n            wait(this.rollingMonitorInterval * 1000);\n            if (this.appFinishing.get() || this.aborted.get()) {\n              break;\n            }\n            uploadLogsForContainers(false);\n          } else {\n            wait(THREAD_SLEEP_TIME);\n          }\n        } catch (InterruptedException e) {\n          LOG.warn(\"PendingContainers queue is interrupted\");\n          this.appFinishing.set(true);\n        }\n      }\n    }\n\n    if (this.aborted.get()) {\n      return;\n    }\n\n    // App is finished, upload the container logs.\n    uploadLogsForContainers(true);\n\n    doAppLogAggregationPostCleanUp();\n\n    this.dispatcher.getEventHandler().handle(\n        new ApplicationEvent(this.appId,\n            ApplicationEventType.APPLICATION_LOG_HANDLING_FINISHED));\n    this.appAggregationFinished.set(true);\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregationPostCleanUp": "  private void doAppLogAggregationPostCleanUp() {\n    // Remove the local app-log-dirs\n    List<Path> localAppLogDirs = new ArrayList<Path>();\n    for (String rootLogDir : dirsHandler.getLogDirsForCleanup()) {\n      Path logPath = new Path(rootLogDir, applicationId);\n      try {\n        // check if log dir exists\n        lfs.getFileStatus(logPath);\n        localAppLogDirs.add(logPath);\n      } catch (UnsupportedFileSystemException ue) {\n        LOG.warn(\"Log dir \" + rootLogDir + \"is an unsupported file system\", ue);\n        continue;\n      } catch (IOException fe) {\n        continue;\n      }\n    }\n\n    if (localAppLogDirs.size() > 0) {\n      this.delService.delete(this.userUgi.getShortUserName(), null,\n        localAppLogDirs.toArray(new Path[localAppLogDirs.size()]));\n    }\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run": "  public void run() {\n    try {\n      doAppLogAggregation();\n    } catch (Exception e) {\n      // do post clean up of log directories on any exception\n      LOG.error(\"Error occured while aggregating the log for the application \"\n          + appId, e);\n      doAppLogAggregationPostCleanUp();\n    } finally {\n      if (!this.appAggregationFinished.get()) {\n        LOG.warn(\"Aggregation did not complete for application \" + appId);\n      }\n      this.appAggregationFinished.set(true);\n    }\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.run": "      public void run() {\n        try {\n          appLogAggregator.run();\n        } finally {\n          appLogAggregators.remove(appId);\n          closeFileSystems(userUgi);\n        }\n      }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.closeFileSystems": "  protected void closeFileSystems(final UserGroupInformation userUgi) {\n    try {\n      FileSystem.closeAllForUGI(userUgi);\n    } catch (IOException e) {\n      LOG.warn(\"Failed to close filesystems: \", e);\n    }\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.createDir": "  private void createDir(FileSystem fs, Path path, FsPermission fsPerm)\n      throws IOException {\n    FsPermission dirPerm = new FsPermission(fsPerm);\n    fs.mkdirs(path, dirPerm);\n    FsPermission umask = FsPermission.getUMask(fs.getConf());\n    if (!dirPerm.equals(dirPerm.applyUMask(umask))) {\n      fs.setPermission(path, new FsPermission(fsPerm));\n    }\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.checkExists": "  private boolean checkExists(FileSystem fs, Path path, FsPermission fsPerm)\n      throws IOException {\n    boolean exists = true;\n    try {\n      FileStatus appDirStatus = fs.getFileStatus(path);\n      if (!APP_DIR_PERMISSIONS.equals(appDirStatus.getPermission())) {\n        fs.setPermission(path, APP_DIR_PERMISSIONS);\n      }\n    } catch (FileNotFoundException fnfe) {\n      exists = false;\n    }\n    return exists;\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.getFileSystem": "  protected FileSystem getFileSystem(Configuration conf) throws IOException {\n    return this.remoteRootLogDir.getFileSystem(conf);\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.getRemoteAppLogDir": "  Path getRemoteAppLogDir(ApplicationId appId, String user) {\n    return LogAggregationUtils.getRemoteAppLogDir(this.remoteRootLogDir, appId,\n        user, this.remoteRootLogDirSuffix);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.keyProvider.getConf": "  public Configuration getConf() {\n    return conf;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.keyProvider.getKeyVersion": "  public abstract KeyVersion getKeyVersion(String versionName\n                                            ) throws IOException;\n\n  /**\n   * Get the key names for all keys.\n   * @return the list of key names\n   * @throws IOException\n   */\n  public abstract List<String> getKeys() throws IOException;\n\n  /**\n   * Get key metadata in bulk.\n   * @param names the names of the keys to get\n   * @throws IOException\n   */\n  public Metadata[] getKeysMetadata(String... names) throws IOException {\n    Metadata[] result = new Metadata[names.length];\n    for (int i=0; i < names.length; ++i) {\n      result[i] = getMetadata(names[i]);\n    }\n    return result;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FsServerDefaults.getReplication": "  public short getReplication() {\n    return replication;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FsServerDefaults.getBytesPerChecksum": "  public int getBytesPerChecksum() {\n    return bytesPerChecksum;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FsServerDefaults.getFileBufferSize": "  public int getFileBufferSize() {\n    return fileBufferSize;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FsServerDefaults.getChecksumType": "  public DataChecksum.Type getChecksumType() {\n    return checksumType;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FsServerDefaults.getBlockSize": "  public long getBlockSize() {\n    return blockSize;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileStatus.setSymlink": "  public void setSymlink(final Path p) {\n    symlink = p;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileStatus.isSymlink": "  public boolean isSymlink() {\n    return symlink != null;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileStatus.getSymlink": "  public Path getSymlink() throws IOException {\n    if (!isSymlink()) {\n      throw new IOException(\"Path \" + path + \" is not a symbolic link\");\n    }\n    return symlink;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.AbstractFileSystem.getUri": "  public URI getUri() {\n    return myUri;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.AbstractFileSystem.checkScheme": "  public void checkScheme(URI uri, String supportedScheme) {\n    String scheme = uri.getScheme();\n    if (scheme == null) {\n      throw new HadoopIllegalArgumentException(\"Uri without scheme: \" + uri);\n    }\n    if (!scheme.equals(supportedScheme)) {\n      throw new HadoopIllegalArgumentException(\"Uri scheme \" + uri\n          + \" does not match the scheme \" + supportedScheme);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.AbstractFileSystem.getLinkTarget": "  public Path getLinkTarget(final Path f) throws IOException {\n    throw new AssertionError(\"Implementation Error: \" + getClass()\n        + \" that threw an UnresolvedLinkException, causing this method to be\"\n        + \" called, needs to override this method.\");\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.areSymlinksEnabled": "  public static boolean areSymlinksEnabled() {\n    return symlinksEnabled;\n  }"
        },
        "bug_report": {
            "Title": "Handle ClassCastException on AuthenticationException in LoadBalancingKMSClientProvider",
            "Description": "An Oozie job with a single shell action fails (may not be important, but if you needs the exact details I can provide them) with an error message coming from NodeManager:\n{code}\n2016-05-10 11:10:14,290 ERROR org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[LogAggregationService #652,5,main] threw an Exception.\njava.lang.ClassCastException: org.apache.hadoop.security.authentication.client.AuthenticationException cannot be cast to java.security.GeneralSecurityException\nat org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.decryptEncryptedKey(LoadBalancingKMSClientProvider.java:189)\nat org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:388)\nat org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1419)\nat org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1521)\nat org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:108)\nat org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:59)\nat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:577)\nat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:683)\nat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:679)\nat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\nat org.apache.hadoop.fs.FileContext.create(FileContext.java:679)\nat org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter$1.run(AggregatedLogFormat.java:382)\nat org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter$1.run(AggregatedLogFormat.java:377)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:422)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\nat org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter.<init>(AggregatedLogFormat.java:376)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainers(AppLogAggregatorImpl.java:246)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregation(AppLogAggregatorImpl.java:456)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run(AppLogAggregatorImpl.java:421)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$2.run(LogAggregationService.java:384)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\n{code}\n\nThe unsafe cast is here:\nhttps://github.com/apache/hadoop/blob/2e1d0ff4e901b8313c8d71869735b94ed8bc40a0/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java#L189\n\nBecause of this ClassCastException:\n- an uncaught exception is raised\n- we do not see the exact \"caused by\" exception/message\n- the oozie job fails\n- YARN logs are not reported/saved"
        }
    },
    {
        "filename": "HADOOP-15121.json",
        "creation_time": "2017-12-15T07:41:39.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getMetrics(DecayRpcScheduler.java:781)\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:199)\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:182)\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:155)\n        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)\n        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)\n        at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)\n        at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:66)\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:100)\n        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)\n        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)\n        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.registerMetrics2Source(DecayRpcScheduler.java:709)\n        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.<init>(DecayRpcScheduler.java:685)\n        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getInstance(DecayRpcScheduler.java:693)\n        at org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:236)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n        at org.apache.hadoop.ipc.CallQueueManager.createScheduler(CallQueueManager.java:102)\n        at org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:76)\n        at org.apache.hadoop.ipc.Server.<init>(Server.java:2612)\n        at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:958)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:374)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:349)\n        at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:800)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:415)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:755)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:697)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:905)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:884)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1610)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1678)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.DecayRpcScheduler.getMetrics": "  public void getMetrics(MetricsCollector collector, boolean all) {\n    // Metrics2 interface to act as a Metric source\n    try {\n      MetricsRecordBuilder rb = collector.addRecord(getClass().getName())\n          .setContext(namespace);\n      addDecayedCallVolume(rb);\n      addUniqueIdentityCount(rb);\n      addTopNCallerSummary(rb);\n      addAvgResponseTimePerPriority(rb);\n      addCallVolumePerPriority(rb);\n      addRawCallVolume(rb);\n    } catch (Exception e) {\n      LOG.warn(\"Exception thrown while metric collection. Exception : \"\n          + e.getMessage());\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.DecayRpcScheduler.addTopNCallerSummary": "  private void addTopNCallerSummary(MetricsRecordBuilder rb) {\n    TopN topNCallers = getTopCallers(topUsersCount);\n    Map<Object, Integer> decisions = scheduleCacheRef.get();\n    final int actualCallerCount = topNCallers.size();\n    for (int i = 0; i < actualCallerCount; i++) {\n      NameValuePair entry =  topNCallers.poll();\n      String topCaller = \"Caller(\" + entry.getName() + \")\";\n      String topCallerVolume = topCaller + \".Volume\";\n      String topCallerPriority = topCaller + \".Priority\";\n      rb.addCounter(Interns.info(topCallerVolume, topCallerVolume),\n          entry.getValue());\n      Integer priority = decisions.get(entry.getName());\n      if (priority != null) {\n        rb.addCounter(Interns.info(topCallerPriority, topCallerPriority),\n            priority);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.DecayRpcScheduler.addRawCallVolume": "  private void addRawCallVolume(MetricsRecordBuilder rb) {\n    rb.addCounter(Interns.info(\"CallVolume\", \"Raw Total \" +\n        \"incoming Call Volume\"), getTotalRawCallVolume());\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.DecayRpcScheduler.addDecayedCallVolume": "  private void addDecayedCallVolume(MetricsRecordBuilder rb) {\n    rb.addCounter(Interns.info(\"DecayedCallVolume\", \"Decayed Total \" +\n        \"incoming Call Volume\"), getTotalCallVolume());\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.DecayRpcScheduler.addCallVolumePerPriority": "  private void addCallVolumePerPriority(MetricsRecordBuilder rb) {\n    for (int i = 0; i < responseTimeCountInLastWindow.length(); i++) {\n      rb.addGauge(Interns.info(\"Priority.\" + i + \".CompletedCallVolume\",\n          \"Completed Call volume \" +\n          \"of priority \"+ i), responseTimeCountInLastWindow.get(i));\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.DecayRpcScheduler.addAvgResponseTimePerPriority": "  private void addAvgResponseTimePerPriority(MetricsRecordBuilder rb) {\n    for (int i = 0; i < responseTimeAvgInLastWindow.length(); i++) {\n      rb.addGauge(Interns.info(\"Priority.\" + i + \".AvgResponseTime\", \"Average\" +\n          \" response time of priority \" + i),\n          responseTimeAvgInLastWindow.get(i));\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.DecayRpcScheduler.addUniqueIdentityCount": "  private void addUniqueIdentityCount(MetricsRecordBuilder rb) {\n    rb.addCounter(Interns.info(\"UniqueCallers\", \"Total unique callers\"),\n        getUniqueIdentityCount());\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics": "  Iterable<MetricsRecordImpl> getMetrics(MetricsCollectorImpl builder,\n                                         boolean all) {\n    builder.setRecordFilter(recordFilter).setMetricFilter(metricFilter);\n    try {\n      source.getMetrics(builder, all);\n    } catch (Exception e) {\n      LOG.error(\"Error getting metrics from source \"+ name, e);\n    }\n    for (MetricsRecordBuilderImpl rb : builder) {\n      for (MetricsTag t : injectedTags) {\n        rb.add(t);\n      }\n    }\n    return builder.getRecords();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache": "  private void updateJmxCache() {\n    boolean getAllMetrics = false;\n    synchronized(this) {\n      if (Time.now() - jmxCacheTS >= jmxCacheTTL) {\n        // temporarilly advance the expiry while updating the cache\n        jmxCacheTS = Time.now() + jmxCacheTTL;\n        // lastRecs might have been set to an object already by another thread.\n        // Track the fact that lastRecs has been reset once to make sure refresh\n        // is correctly triggered.\n        if (lastRecsCleared) {\n          getAllMetrics = true;\n          lastRecsCleared = false;\n        }\n      }\n      else {\n        return;\n      }\n    }\n\n    // HADOOP-11361: Release lock here for avoid deadlock between\n    // MetricsSystemImpl's lock and MetricsSourceAdapter's lock.\n    Iterable<MetricsRecordImpl> lastRecs = null;\n    if (getAllMetrics) {\n      lastRecs = getMetrics(new MetricsCollectorImpl(), true);\n    }\n\n    synchronized (this) {\n      if (lastRecs != null) {\n        updateAttrCache(lastRecs);\n        updateInfoCache(lastRecs);\n      }\n      jmxCacheTS = Time.now();\n      lastRecsCleared = true;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateInfoCache": "  private void updateInfoCache(Iterable<MetricsRecordImpl> lastRecs) {\n    Preconditions.checkNotNull(lastRecs, \"LastRecs should not be null\");\n    LOG.debug(\"Updating info cache...\");\n    infoCache = infoBuilder.reset(lastRecs).get();\n    LOG.debug(\"Done\");\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateAttrCache": "  private int updateAttrCache(Iterable<MetricsRecordImpl> lastRecs) {\n    Preconditions.checkNotNull(lastRecs, \"LastRecs should not be null\");\n    LOG.debug(\"Updating attr cache...\");\n    int recNo = 0;\n    int numMetrics = 0;\n    for (MetricsRecordImpl record : lastRecs) {\n      for (MetricsTag t : record.tags()) {\n        setAttrCacheTag(t, recNo);\n        ++numMetrics;\n      }\n      for (AbstractMetric m : record.metrics()) {\n        setAttrCacheMetric(m, recNo);\n        ++numMetrics;\n      }\n      ++recNo;\n    }\n    LOG.debug(\"Done. # tags & metrics=\"+ numMetrics);\n    return numMetrics;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo": "  public MBeanInfo getMBeanInfo() {\n    updateJmxCache();\n    return infoCache;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.util.MBeans.register": "  static public ObjectName register(String serviceName, String nameName,\n                                    Object theMbean) {\n    final MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();\n    ObjectName name = getMBeanName(serviceName, nameName);\n    if (name != null) {\n      try {\n        mbs.registerMBean(theMbean, name);\n        LOG.debug(\"Registered \" + name);\n        return name;\n      } catch (InstanceAlreadyExistsException iaee) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Failed to register MBean \\\"\" + name + \"\\\"\", iaee);\n        } else {\n          LOG.warn(\"Failed to register MBean \\\"\" + name\n              + \"\\\": Instance already exists.\");\n        }\n      } catch (Exception e) {\n        LOG.warn(\"Failed to register MBean \\\"\" + name + \"\\\"\", e);\n      }\n    }\n    return null;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.util.MBeans.getMBeanName": "  static private ObjectName getMBeanName(String serviceName, String nameName) {\n    String nameStr = DOMAIN_PREFIX + SERVICE_PREFIX + serviceName + \",\" +\n        NAME_PREFIX + nameName;\n    try {\n      return DefaultMetricsSystem.newMBeanName(nameStr);\n    } catch (Exception e) {\n      LOG.warn(\"Error creating MBean object name: \"+ nameStr, e);\n      return null;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans": "  synchronized void startMBeans() {\n    if (mbeanName != null) {\n      LOG.warn(\"MBean \"+ name +\" already initialized!\");\n      LOG.debug(\"Stacktrace: \", new Throwable());\n      return;\n    }\n    mbeanName = MBeans.register(prefix, name, this);\n    LOG.debug(\"MBean for source \"+ name +\" registered.\");\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start": "  void start() {\n    if (startMBeans) startMBeans();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource": "  void registerSource(String name, String desc, MetricsSource source) {\n    checkNotNull(config, \"config\");\n    MetricsConfig conf = sourceConfigs.get(name);\n    MetricsSourceAdapter sa = new MetricsSourceAdapter(prefix, name, desc,\n        source, injectedTags, period, conf != null ? conf\n            : config.subset(SOURCE_KEY));\n    sources.put(name, sa);\n    sa.start();\n    LOG.debug(\"Registered source \"+ name);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start": "  public synchronized void start() {\n    checkNotNull(prefix, \"prefix\");\n    if (monitoring) {\n      LOG.warn(prefix +\" metrics system already started!\",\n               new MetricsException(\"Illegal start\"));\n      return;\n    }\n    for (Callback cb : callbacks) cb.preStart();\n    for (Callback cb : namedCallbacks.values()) cb.preStart();\n    configure(prefix);\n    startTimer();\n    monitoring = true;\n    LOG.info(prefix +\" metrics system started\");\n    for (Callback cb : callbacks) cb.postStart();\n    for (Callback cb : namedCallbacks.values()) cb.postStart();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register": "  private synchronized void register(String name, final Callback callback) {\n    namedCallbacks.put(name, (Callback) getProxyForCallback(callback));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSink": "  synchronized void registerSink(String name, String desc, MetricsSink sink) {\n    checkNotNull(config, \"config\");\n    MetricsConfig conf = sinkConfigs.get(name);\n    MetricsSinkAdapter sa = conf != null\n        ? newSink(name, desc, sink, conf)\n        : newSink(name, desc, sink, config.subset(SINK_KEY));\n    sinks.put(name, sa);\n    sa.start();\n    LOG.info(\"Registered sink \"+ name);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSystemImpl.getProxyForCallback": "  private Object getProxyForCallback(final Callback callback) {\n    return Proxy.newProxyInstance(callback.getClass().getClassLoader(),\n        new Class<?>[] { Callback.class }, new InvocationHandler() {\n          @Override\n          public Object invoke(Object proxy, Method method, Object[] args)\n              throws Throwable {\n            try {\n              return method.invoke(callback, args);\n            } catch (Exception e) {\n              // These are not considered fatal.\n              LOG.warn(\"Caught exception in callback \" + method.getName(), e);\n            }\n            return null;\n          }\n        });\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.DecayRpcScheduler.registerMetrics2Source": "    void registerMetrics2Source(String namespace) {\n      final String name = \"DecayRpcSchedulerMetrics2.\" + namespace;\n      DefaultMetricsSystem.instance().register(name, name, this);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.DecayRpcScheduler.getInstance": "    public static synchronized MetricsProxy getInstance(String namespace,\n        int numLevels) {\n      MetricsProxy mp = INSTANCES.get(namespace);\n      if (mp == null) {\n        // We must create one\n        mp = new MetricsProxy(namespace, numLevels);\n        INSTANCES.put(namespace, mp);\n      }\n      return mp;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.CallQueueManager.createScheduler": "  private static <T extends RpcScheduler> T createScheduler(\n      Class<T> theClass, int priorityLevels, String ns, Configuration conf) {\n    // Used for custom, configurable scheduler\n    try {\n      Constructor<T> ctor = theClass.getDeclaredConstructor(int.class,\n          String.class, Configuration.class);\n      return ctor.newInstance(priorityLevels, ns, conf);\n    } catch (RuntimeException e) {\n      throw e;\n    } catch (InvocationTargetException e) {\n      throw new RuntimeException(theClass.getName()\n          + \" could not be constructed.\", e.getCause());\n    } catch (Exception e) {\n    }\n\n    try {\n      Constructor<T> ctor = theClass.getDeclaredConstructor(int.class);\n      return ctor.newInstance(priorityLevels);\n    } catch (RuntimeException e) {\n      throw e;\n    } catch (InvocationTargetException e) {\n      throw new RuntimeException(theClass.getName()\n          + \" could not be constructed.\", e.getCause());\n    } catch (Exception e) {\n    }\n\n    // Last attempt\n    try {\n      Constructor<T> ctor = theClass.getDeclaredConstructor();\n      return ctor.newInstance();\n    } catch (RuntimeException e) {\n      throw e;\n    } catch (InvocationTargetException e) {\n      throw new RuntimeException(theClass.getName()\n          + \" could not be constructed.\", e.getCause());\n    } catch (Exception e) {\n    }\n\n    // Nothing worked\n    throw new RuntimeException(theClass.getName() +\n        \" could not be constructed.\");\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.CallQueueManager.getCause": "    public IOException getCause() {\n      return (IOException)super.getCause();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getServer": "  public RPC.Server getServer(Class<?> protocol, Object protocolImpl,\n      String bindAddress, int port, int numHandlers, int numReaders,\n      int queueSizePerHandler, boolean verbose, Configuration conf,\n      SecretManager<? extends TokenIdentifier> secretManager,\n      String portRangeConfig)\n      throws IOException {\n    return new Server(protocol, protocolImpl, conf, bindAddress, port,\n        numHandlers, numReaders, queueSizePerHandler, verbose, secretManager,\n        portRangeConfig);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.build": "    public Server build() throws IOException, HadoopIllegalArgumentException {\n      if (this.conf == null) {\n        throw new HadoopIllegalArgumentException(\"conf is not set\");\n      }\n      if (this.protocol == null) {\n        throw new HadoopIllegalArgumentException(\"protocol is not set\");\n      }\n      if (this.instance == null) {\n        throw new HadoopIllegalArgumentException(\"instance is not set\");\n      }\n      \n      return getProtocolEngine(this.protocol, this.conf).getServer(\n          this.protocol, this.instance, this.bindAddress, this.port,\n          this.numHandlers, this.numReaders, this.queueSizePerHandler,\n          this.verbose, this.conf, this.secretManager, this.portRangeConfig);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.getProtocolEngine": "  static synchronized RpcEngine getProtocolEngine(Class<?> protocol,\n      Configuration conf) {\n    RpcEngine engine = PROTOCOL_ENGINES.get(protocol);\n    if (engine == null) {\n      Class<?> impl = conf.getClass(ENGINE_PROP+\".\"+protocol.getName(),\n                                    WritableRpcEngine.class);\n      engine = (RpcEngine)ReflectionUtils.newInstance(impl, conf);\n      PROTOCOL_ENGINES.put(protocol, engine);\n    }\n    return engine;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer": "  protected NameNodeRpcServer createRpcServer(Configuration conf)\n      throws IOException {\n    return new NameNodeRpcServer(conf, this);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initialize": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) == null) {\n      String intervals = conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals != null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    pauseMonitor = new JvmPauseMonitor();\n    pauseMonitor.init(conf);\n    pauseMonitor.start();\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n\n    if (NamenodeRole.NAMENODE == role) {\n      startHttpServer(conf);\n    }\n\n    loadNamesystem(conf);\n\n    rpcServer = createRpcServer(conf);\n\n    initReconfigurableBackoffKey();\n\n    if (clientNamenodeAddress == null) {\n      // This is expected for MiniDFSCluster. Set it now using \n      // the RPC server's bind address.\n      clientNamenodeAddress = \n          NetUtils.getHostPortString(getNameNodeAddress());\n      LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n          + \" this namenode/service.\");\n    }\n    if (NamenodeRole.NAMENODE == role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n\n    startCommonServices(conf);\n    startMetricsLogger(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initReconfigurableBackoffKey": "  private void initReconfigurableBackoffKey() {\n    ipcClientRPCBackoffEnable = buildBackoffEnableKey(rpcServer\n        .getClientRpcServer().getPort());\n    reconfigurableProperties.add(ipcClientRPCBackoffEnable);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getRole": "  public NamenodeRole getRole() {\n    return role;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.startMetricsLogger": "  protected void startMetricsLogger(Configuration conf) {\n    long metricsLoggerPeriodSec =\n        conf.getInt(DFS_NAMENODE_METRICS_LOGGER_PERIOD_SECONDS_KEY,\n            DFS_NAMENODE_METRICS_LOGGER_PERIOD_SECONDS_DEFAULT);\n\n    if (metricsLoggerPeriodSec <= 0) {\n      return;\n    }\n\n    MetricsLoggerTask.makeMetricsLoggerAsync(MetricsLog);\n\n    // Schedule the periodic logging.\n    metricsLoggerTimer = new ScheduledThreadPoolExecutor(1);\n    metricsLoggerTimer.setExecuteExistingDelayedTasksAfterShutdownPolicy(\n        false);\n    metricsLoggerTimer.scheduleWithFixedDelay(new MetricsLoggerTask(MetricsLog,\n        \"NameNode\", (short) 128),\n        metricsLoggerPeriodSec,\n        metricsLoggerPeriodSec,\n        TimeUnit.SECONDS);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initMetrics": "  public static void initMetrics(Configuration conf, NamenodeRole role) {\n    metrics = NameNodeMetrics.create(conf, role);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.loginAsNameNodeUser": "  void loginAsNameNodeUser(Configuration conf) throws IOException {\n    InetSocketAddress socAddr = getRpcServerAddress(conf);\n    SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n        DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, socAddr.getHostName());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem": "  protected void loadNamesystem(Configuration conf) throws IOException {\n    this.namesystem = FSNamesystem.loadFromDisk(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getFSImage": "  public FSImage getFSImage() {\n    return namesystem.getFSImage();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getNameNodeAddress": "  public InetSocketAddress getNameNodeAddress() {\n    return rpcServer.getRpcAddress();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer": "  private void startHttpServer(final Configuration conf) throws IOException {\n    httpServer = new NameNodeHttpServer(conf, this, getHttpServerBindAddress(conf));\n    httpServer.start();\n    httpServer.setStartupProgress(startupProgress);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.startCommonServices": "  private void startCommonServices(Configuration conf) throws IOException {\n    namesystem.startCommonServices(conf, haContext);\n    registerNNSMXBean();\n    if (NamenodeRole.NAMENODE != role) {\n      startHttpServer(conf);\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n    rpcServer.start();\n    try {\n      plugins = conf.getInstances(DFS_NAMENODE_PLUGINS_KEY,\n          ServicePlugin.class);\n    } catch (RuntimeException e) {\n      String pluginsValue = conf.get(DFS_NAMENODE_PLUGINS_KEY);\n      LOG.error(\"Unable to load NameNode plugins. Specified list of plugins: \" +\n          pluginsValue, e);\n      throw e;\n    }\n    for (ServicePlugin p: plugins) {\n      try {\n        p.start(this);\n      } catch (Throwable t) {\n        LOG.warn(\"ServicePlugin \" + p + \" could not be started\", t);\n      }\n    }\n    LOG.info(getRole() + \" RPC up at: \" + getNameNodeAddress());\n    if (rpcServer.getServiceRpcAddress() != null) {\n      LOG.info(getRole() + \" service RPC up at: \"\n          + rpcServer.getServiceRpcAddress());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    LOG.info(\"createNameNode \" + Arrays.asList(argv));\n    if (conf == null)\n      conf = new HdfsConfiguration();\n    // Parse out some generic args into Configuration.\n    GenericOptionsParser hParser = new GenericOptionsParser(conf, argv);\n    argv = hParser.getRemainingArgs();\n    // Parse the rest, NN specific args.\n    StartupOption startOpt = parseArguments(argv);\n    if (startOpt == null) {\n      printUsage(System.err);\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n\n    boolean aborted = false;\n    switch (startOpt) {\n    case FORMAT:\n      aborted = format(conf, startOpt.getForceFormat(),\n          startOpt.getInteractiveFormat());\n      terminate(aborted ? 1 : 0);\n      return null; // avoid javac warning\n    case GENCLUSTERID:\n      System.err.println(\"Generating new cluster id:\");\n      System.out.println(NNStorage.newClusterID());\n      terminate(0);\n      return null;\n    case ROLLBACK:\n      aborted = doRollback(conf, true);\n      terminate(aborted ? 1 : 0);\n      return null; // avoid warning\n    case BOOTSTRAPSTANDBY:\n      String[] toolArgs = Arrays.copyOfRange(argv, 1, argv.length);\n      int rc = BootstrapStandby.run(toolArgs, conf);\n      terminate(rc);\n      return null; // avoid warning\n    case INITIALIZESHAREDEDITS:\n      aborted = initializeSharedEdits(conf,\n          startOpt.getForceFormat(),\n          startOpt.getInteractiveFormat());\n      terminate(aborted ? 1 : 0);\n      return null; // avoid warning\n    case BACKUP:\n    case CHECKPOINT:\n      NamenodeRole role = startOpt.toNodeRole();\n      DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n      return new BackupNode(conf, role);\n    case RECOVER:\n      NameNode.doRecovery(startOpt, conf);\n      return null;\n    case METADATAVERSION:\n      printMetadataVersion(conf);\n      terminate(0);\n      return null; // avoid javac warning\n    case UPGRADEONLY:\n      DefaultMetricsSystem.initialize(\"NameNode\");\n      new NameNode(conf);\n      terminate(0);\n      return null;\n    default:\n      DefaultMetricsSystem.initialize(\"NameNode\");\n      return new NameNode(conf);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.setStartupOption": "  private static void setStartupOption(Configuration conf, StartupOption opt) {\n    conf.set(DFS_NAMENODE_STARTUP_KEY, opt.name());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.doRecovery": "  private static void doRecovery(StartupOption startOpt, Configuration conf)\n      throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    if (startOpt.getForce() < MetaRecoveryContext.FORCE_ALL) {\n      if (!confirmPrompt(\"You have selected Metadata Recovery mode.  \" +\n          \"This mode is intended to recover lost metadata on a corrupt \" +\n          \"filesystem.  Metadata recovery mode often permanently deletes \" +\n          \"data from your HDFS filesystem.  Please back up your edit log \" +\n          \"and fsimage before trying this!\\n\\n\" +\n          \"Are you ready to proceed? (Y/N)\\n\")) {\n        System.err.println(\"Recovery aborted at user request.\\n\");\n        return;\n      }\n    }\n    MetaRecoveryContext.LOG.info(\"starting recovery...\");\n    UserGroupInformation.setConfiguration(conf);\n    NameNode.initMetrics(conf, startOpt.toNodeRole());\n    FSNamesystem fsn = null;\n    try {\n      fsn = FSNamesystem.loadFromDisk(conf);\n      fsn.getFSImage().saveNamespace(fsn);\n      MetaRecoveryContext.LOG.info(\"RECOVERY COMPLETE\");\n    } catch (IOException e) {\n      MetaRecoveryContext.LOG.info(\"RECOVERY FAILED: caught exception\", e);\n      throw e;\n    } catch (RuntimeException e) {\n      MetaRecoveryContext.LOG.info(\"RECOVERY FAILED: caught exception\", e);\n      throw e;\n    } finally {\n      if (fsn != null)\n        fsn.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.run": "          public FileSystem run() throws IOException {\n            return FileSystem.get(conf);\n          }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.printUsage": "  private static void printUsage(PrintStream out) {\n    out.println(USAGE + \"\\n\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits": "  private static boolean initializeSharedEdits(Configuration conf,\n      boolean force, boolean interactive) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    \n    if (conf.get(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY) == null) {\n      LOG.error(\"No shared edits directory configured for namespace \" +\n          nsId + \" namenode \" + namenodeId);\n      return false;\n    }\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      InetSocketAddress socAddr = DFSUtilClient.getNNAddress(conf);\n      SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n          DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, socAddr.getHostName());\n    }\n\n    NNStorage existingStorage = null;\n    FSImage sharedEditsImage = null;\n    try {\n      FSNamesystem fsns =\n          FSNamesystem.loadFromDisk(getConfigurationWithoutSharedEdits(conf));\n      \n      existingStorage = fsns.getFSImage().getStorage();\n      NamespaceInfo nsInfo = existingStorage.getNamespaceInfo();\n      \n      List<URI> sharedEditsDirs = FSNamesystem.getSharedEditsDirs(conf);\n      \n      sharedEditsImage = new FSImage(conf,\n          Lists.<URI>newArrayList(),\n          sharedEditsDirs);\n      sharedEditsImage.getEditLog().initJournalsForWrite();\n      \n      if (!sharedEditsImage.confirmFormat(force, interactive)) {\n        return true; // abort\n      }\n      \n      NNStorage newSharedStorage = sharedEditsImage.getStorage();\n      // Call Storage.format instead of FSImage.format here, since we don't\n      // actually want to save a checkpoint - just prime the dirs with\n      // the existing namespace info\n      newSharedStorage.format(nsInfo);\n      sharedEditsImage.getEditLog().formatNonFileJournals(nsInfo);\n\n      // Need to make sure the edit log segments are in good shape to initialize\n      // the shared edits dir.\n      fsns.getFSImage().getEditLog().close();\n      fsns.getFSImage().getEditLog().initJournalsForWrite();\n      fsns.getFSImage().getEditLog().recoverUnclosedStreams();\n\n      copyEditLogSegmentsToSharedDir(fsns, sharedEditsDirs, newSharedStorage,\n          conf);\n    } catch (IOException ioe) {\n      LOG.error(\"Could not initialize shared edits dir\", ioe);\n      return true; // aborted\n    } finally {\n      if (sharedEditsImage != null) {\n        try {\n          sharedEditsImage.close();\n        }  catch (IOException ioe) {\n          LOG.warn(\"Could not close sharedEditsImage\", ioe);\n        }\n      }\n      // Have to unlock storage explicitly for the case when we're running in a\n      // unit test, which runs in the same JVM as NNs.\n      if (existingStorage != null) {\n        try {\n          existingStorage.unlockAll();\n        } catch (IOException ioe) {\n          LOG.warn(\"Could not unlock storage directories\", ioe);\n          return true; // aborted\n        }\n      }\n    }\n    return false; // did not abort\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.format": "  private static boolean format(Configuration conf, boolean force,\n      boolean isInteractive) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    checkAllowFormat(conf);\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      InetSocketAddress socAddr = DFSUtilClient.getNNAddress(conf);\n      SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n          DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, socAddr.getHostName());\n    }\n    \n    Collection<URI> nameDirsToFormat = FSNamesystem.getNamespaceDirs(conf);\n    List<URI> sharedDirs = FSNamesystem.getSharedEditsDirs(conf);\n    List<URI> dirsToPrompt = new ArrayList<URI>();\n    dirsToPrompt.addAll(nameDirsToFormat);\n    dirsToPrompt.addAll(sharedDirs);\n    List<URI> editDirsToFormat = \n                 FSNamesystem.getNamespaceEditsDirs(conf);\n\n    // if clusterID is not provided - see if you can find the current one\n    String clusterId = StartupOption.FORMAT.getClusterId();\n    if(clusterId == null || clusterId.equals(\"\")) {\n      //Generate a new cluster id\n      clusterId = NNStorage.newClusterID();\n    }\n    System.out.println(\"Formatting using clusterid: \" + clusterId);\n    \n    FSImage fsImage = new FSImage(conf, nameDirsToFormat, editDirsToFormat);\n    try {\n      FSNamesystem fsn = new FSNamesystem(conf, fsImage);\n      fsImage.getEditLog().initJournalsForWrite();\n\n      // Abort NameNode format if reformat is disabled and if\n      // meta-dir already exists\n      if (conf.getBoolean(DFSConfigKeys.DFS_REFORMAT_DISABLED,\n          DFSConfigKeys.DFS_REFORMAT_DISABLED_DEFAULT)) {\n        force = false;\n        isInteractive = false;\n        for (StorageDirectory sd : fsImage.storage.dirIterable(null)) {\n          if (sd.hasSomeData()) {\n            throw new NameNodeFormatException(\n                \"NameNode format aborted as reformat is disabled for \"\n                    + \"this cluster.\");\n          }\n        }\n      }\n\n      if (!fsImage.confirmFormat(force, isInteractive)) {\n        return true; // aborted\n      }\n\n      fsImage.format(fsn, clusterId);\n    } catch (IOException ioe) {\n      LOG.warn(\"Encountered exception during format: \", ioe);\n      fsImage.close();\n      throw ioe;\n    }\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.parseArguments": "  static StartupOption parseArguments(String args[]) {\n    int argsLen = (args == null) ? 0 : args.length;\n    StartupOption startOpt = StartupOption.REGULAR;\n    for(int i=0; i < argsLen; i++) {\n      String cmd = args[i];\n      if (StartupOption.FORMAT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.FORMAT;\n        for (i = i + 1; i < argsLen; i++) {\n          if (args[i].equalsIgnoreCase(StartupOption.CLUSTERID.getName())) {\n            i++;\n            if (i >= argsLen) {\n              // if no cluster id specified, return null\n              LOG.error(\"Must specify a valid cluster ID after the \"\n                  + StartupOption.CLUSTERID.getName() + \" flag\");\n              return null;\n            }\n            String clusterId = args[i];\n            // Make sure an id is specified and not another flag\n            if (clusterId.isEmpty() ||\n                clusterId.equalsIgnoreCase(StartupOption.FORCE.getName()) ||\n                clusterId.equalsIgnoreCase(\n                    StartupOption.NONINTERACTIVE.getName())) {\n              LOG.error(\"Must specify a valid cluster ID after the \"\n                  + StartupOption.CLUSTERID.getName() + \" flag\");\n              return null;\n            }\n            startOpt.setClusterId(clusterId);\n          }\n\n          if (args[i].equalsIgnoreCase(StartupOption.FORCE.getName())) {\n            startOpt.setForceFormat(true);\n          }\n\n          if (args[i].equalsIgnoreCase(StartupOption.NONINTERACTIVE.getName())) {\n            startOpt.setInteractiveFormat(false);\n          }\n        }\n      } else if (StartupOption.GENCLUSTERID.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.GENCLUSTERID;\n      } else if (StartupOption.REGULAR.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.REGULAR;\n      } else if (StartupOption.BACKUP.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.BACKUP;\n      } else if (StartupOption.CHECKPOINT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.CHECKPOINT;\n      } else if (StartupOption.UPGRADE.getName().equalsIgnoreCase(cmd)\n          || StartupOption.UPGRADEONLY.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.UPGRADE.getName().equalsIgnoreCase(cmd) ? \n            StartupOption.UPGRADE : StartupOption.UPGRADEONLY;\n        /* Can be followed by CLUSTERID with a required parameter or\n         * RENAMERESERVED with an optional parameter\n         */\n        while (i + 1 < argsLen) {\n          String flag = args[i + 1];\n          if (flag.equalsIgnoreCase(StartupOption.CLUSTERID.getName())) {\n            if (i + 2 < argsLen) {\n              i += 2;\n              startOpt.setClusterId(args[i]);\n            } else {\n              LOG.error(\"Must specify a valid cluster ID after the \"\n                  + StartupOption.CLUSTERID.getName() + \" flag\");\n              return null;\n            }\n          } else if (flag.equalsIgnoreCase(StartupOption.RENAMERESERVED\n              .getName())) {\n            if (i + 2 < argsLen) {\n              FSImageFormat.setRenameReservedPairs(args[i + 2]);\n              i += 2;\n            } else {\n              FSImageFormat.useDefaultRenameReservedPairs();\n              i += 1;\n            }\n          } else {\n            LOG.error(\"Unknown upgrade flag \" + flag);\n            return null;\n          }\n        }\n      } else if (StartupOption.ROLLINGUPGRADE.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.ROLLINGUPGRADE;\n        ++i;\n        if (i >= argsLen) {\n          LOG.error(\"Must specify a rolling upgrade startup option \"\n              + RollingUpgradeStartupOption.getAllOptionString());\n          return null;\n        }\n        startOpt.setRollingUpgradeStartupOption(args[i]);\n      } else if (StartupOption.ROLLBACK.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.ROLLBACK;\n      } else if (StartupOption.IMPORT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.IMPORT;\n      } else if (StartupOption.BOOTSTRAPSTANDBY.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.BOOTSTRAPSTANDBY;\n        return startOpt;\n      } else if (StartupOption.INITIALIZESHAREDEDITS.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.INITIALIZESHAREDEDITS;\n        for (i = i + 1 ; i < argsLen; i++) {\n          if (StartupOption.NONINTERACTIVE.getName().equals(args[i])) {\n            startOpt.setInteractiveFormat(false);\n          } else if (StartupOption.FORCE.getName().equals(args[i])) {\n            startOpt.setForceFormat(true);\n          } else {\n            LOG.error(\"Invalid argument: \" + args[i]);\n            return null;\n          }\n        }\n        return startOpt;\n      } else if (StartupOption.RECOVER.getName().equalsIgnoreCase(cmd)) {\n        if (startOpt != StartupOption.REGULAR) {\n          throw new RuntimeException(\"Can't combine -recover with \" +\n              \"other startup options.\");\n        }\n        startOpt = StartupOption.RECOVER;\n        while (++i < argsLen) {\n          if (args[i].equalsIgnoreCase(\n                StartupOption.FORCE.getName())) {\n            startOpt.setForce(MetaRecoveryContext.FORCE_FIRST_CHOICE);\n          } else {\n            throw new RuntimeException(\"Error parsing recovery options: \" + \n              \"can't understand option \\\"\" + args[i] + \"\\\"\");\n          }\n        }\n      } else if (StartupOption.METADATAVERSION.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.METADATAVERSION;\n      } else {\n        return null;\n      }\n    }\n    return startOpt;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.printMetadataVersion": "  private static boolean printMetadataVersion(Configuration conf)\n    throws IOException {\n    final String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    final String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    NameNode.initializeGenericKeys(conf, nsId, namenodeId);\n    final FSImage fsImage = new FSImage(conf);\n    final FSNamesystem fs = new FSNamesystem(conf, fsImage, false);\n    return fsImage.recoverTransitionRead(\n      StartupOption.METADATAVERSION, fs, null);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.doRollback": "  public static boolean doRollback(Configuration conf,\n      boolean isConfirmationNeeded) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n\n    FSNamesystem nsys = new FSNamesystem(conf, new FSImage(conf));\n    System.err.print(\n        \"\\\"rollBack\\\" will remove the current state of the file system,\\n\"\n        + \"returning you to the state prior to initiating your recent.\\n\"\n        + \"upgrade. This action is permanent and cannot be undone. If you\\n\"\n        + \"are performing a rollback in an HA environment, you should be\\n\"\n        + \"certain that no NameNode process is running on any host.\");\n    if (isConfirmationNeeded) {\n      if (!confirmPrompt(\"Roll back file system state?\")) {\n        System.err.println(\"Rollback aborted.\");\n        return true;\n      }\n    }\n    nsys.getFSImage().doRollback(nsys);\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.main": "  public static void main(String argv[]) throws Exception {\n    if (DFSUtil.parseHelpArgument(argv, NameNode.USAGE, System.out, true)) {\n      System.exit(0);\n    }\n\n    try {\n      StringUtils.startupShutdownMessage(NameNode.class, argv, LOG);\n      NameNode namenode = createNameNode(argv, null);\n      if (namenode != null) {\n        namenode.join();\n      }\n    } catch (Throwable e) {\n      LOG.error(\"Failed to start namenode.\", e);\n      terminate(1, e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.join": "  public void join() {\n    try {\n      rpcServer.join();\n    } catch (InterruptedException ie) {\n      LOG.info(\"Caught interrupted exception \", ie);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.newClusterID": "  public static String newClusterID() {\n    return \"CID-\" + UUID.randomUUID().toString();\n  }"
        },
        "bug_report": {
            "Title": "Encounter NullPointerException when using DecayRpcScheduler",
            "Description": "I set ipc.8020.scheduler.impl to org.apache.hadoop.ipc.DecayRpcScheduler, but got excetion in namenode:\r\n{code}\r\n2017-12-15 15:26:34,662 ERROR impl.MetricsSourceAdapter (MetricsSourceAdapter.java:getMetrics(202)) - Error getting metrics from source DecayRpcSchedulerMetrics2.ipc.8020\r\njava.lang.NullPointerException\r\n        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getMetrics(DecayRpcScheduler.java:781)\r\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:199)\r\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:182)\r\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:155)\r\n        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)\r\n        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)\r\n        at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)\r\n        at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:66)\r\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)\r\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:100)\r\n        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)\r\n        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)\r\n        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.registerMetrics2Source(DecayRpcScheduler.java:709)\r\n        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.<init>(DecayRpcScheduler.java:685)\r\n        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getInstance(DecayRpcScheduler.java:693)\r\n        at org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:236)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n        at org.apache.hadoop.ipc.CallQueueManager.createScheduler(CallQueueManager.java:102)\r\n        at org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:76)\r\n        at org.apache.hadoop.ipc.Server.<init>(Server.java:2612)\r\n        at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:958)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:374)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:349)\r\n        at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:800)\r\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:415)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:755)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:697)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:905)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:884)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1610)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1678)\r\n{code}\r\nIt seems that {{metricsProxy}} in DecayRpcScheduler should initiate its {{delegate}} field in its Initialization method\r\n"
        }
    },
    {
        "filename": "HADOOP-8110.json",
        "creation_time": "2012-02-24T18:49:27.000+0000",
        "stack_trace": "```\njunit.framework.AssertionFailedError: -expunge failed expected:<0> but was:<1>\n\tat junit.framework.Assert.fail(Assert.java:47)\n\tat junit.framework.Assert.failNotEquals(Assert.java:283)\n\tat junit.framework.Assert.assertEquals(Assert.java:64)\n\tat junit.framework.Assert.assertEquals(Assert.java:195)\n\tat org.apache.hadoop.fs.TestTrash.trashShell(TestTrash.java:322)\n\tat org.apache.hadoop.fs.viewfs.TestViewFsTrash.testTrash(TestViewFsTrash.java:73)\n```",
        "source_code": {},
        "bug_report": {
            "Title": "TestViewFsTrash occasionally fails",
            "Description": "{noformat}\njunit.framework.AssertionFailedError: -expunge failed expected:<0> but was:<1>\n\tat junit.framework.Assert.fail(Assert.java:47)\n\tat junit.framework.Assert.failNotEquals(Assert.java:283)\n\tat junit.framework.Assert.assertEquals(Assert.java:64)\n\tat junit.framework.Assert.assertEquals(Assert.java:195)\n\tat org.apache.hadoop.fs.TestTrash.trashShell(TestTrash.java:322)\n\tat org.apache.hadoop.fs.viewfs.TestViewFsTrash.testTrash(TestViewFsTrash.java:73)\n\t...\n{noformat}\nThere are quite a few TestViewFsTrash failures recently.  E.g. [build #624 for trunk|https://builds.apache.org/job/PreCommit-HADOOP-Build/624//testReport/org.apache.hadoop.fs.viewfs/TestViewFsTrash/testTrash/] and [build #2 for 0.23-PB|https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-Common-0.23-PB-Build/2/testReport/junit/org.apache.hadoop.fs.viewfs/TestViewFsTrash/testTrash/].\n"
        }
    },
    {
        "filename": "HADOOP-11400.json",
        "creation_time": "2014-12-12T10:05:52.000+0000",
        "stack_trace": "```\norg.apache.hadoop.metrics2.MetricsException: Error flushing metrics\n        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:120)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:184)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)\n        at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:129)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:88)\nCaused by: java.net.SocketException: Broken pipe\n        at java.net.SocketOutputStream.socketWrite0(Native Method)\n        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)\n        at java.net.SocketOutputStream.write(SocketOutputStream.java:159)\n        at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221)\n        at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291)\n        at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:295)\n        at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:141)\n        at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:229)\n        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:118)\n        ... 5 more\n\norg.apache.hadoop.metrics2.MetricsException: Error flushing metrics\n        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:120)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:184)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)\n        at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:129)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:88)\nCaused by: java.net.SocketException: Broken pipe\n        at java.net.SocketOutputStream.socketWrite0(Native Method)\n        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)\n        at java.net.SocketOutputStream.write(SocketOutputStream.java:159)\n        at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221)\n        at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291)\n        at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:295)\n        at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:141)\n        at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:229)\n        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:118)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume": "  public void consume(MetricsBuffer buffer) {\n    long ts = 0;\n    for (MetricsBuffer.Entry entry : buffer) {\n      if (sourceFilter == null || sourceFilter.accepts(entry.name())) {\n        for (MetricsRecordImpl record : entry.records()) {\n          if ((context == null || context.equals(record.context())) &&\n              (recordFilter == null || recordFilter.accepts(record))) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Pushing record \"+ entry.name() +\".\"+ record.context() +\n                        \".\"+ record.name() +\" to \"+ name);\n            }\n            sink.putMetrics(metricFilter == null\n                ? record\n                : new MetricsRecordFiltered(record, metricFilter));\n            if (ts == 0) ts = record.timestamp();\n          }\n        }\n      }\n    }\n    if (ts > 0) {\n      sink.flush();\n      latency.add(Time.now() - ts);\n    }\n    if (buffer instanceof WaitableMetricsBuffer) {\n      ((WaitableMetricsBuffer)buffer).notifyAnyWaiters();\n    }\n    LOG.debug(\"Done\");\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.name": "  String name() {\n    return name;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.putMetrics": "  boolean putMetrics(MetricsBuffer buffer, long logicalTime) {\n    if (logicalTime % period == 0) {\n      LOG.debug(\"enqueue, logicalTime=\"+ logicalTime);\n      if (queue.enqueue(buffer)) return true;\n      dropped.incr();\n      return false;\n    }\n    return true; // OK\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll": "  void consumeAll(Consumer<T> consumer) throws InterruptedException {\n    waitForData();\n\n    try {\n      for (int i = size(); i-- > 0; ) {\n        consumer.consume(front()); // can take forever\n        _dequeue();\n      }\n    }\n    finally {\n      clearConsumerLock();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.SinkQueue.front": "  synchronized T front() {\n    return data[(head + 1) % data.length];\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.SinkQueue._dequeue": "  private synchronized T _dequeue() {\n    if (0 == size) {\n      throw new IllegalStateException(\"Size must > 0 here.\");\n    }\n    --size;\n    head = (head + 1) % data.length;\n    T ret = data[head];\n    data[head] = null;  // hint to gc\n    return ret;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.SinkQueue.clearConsumerLock": "  private synchronized void clearConsumerLock() {\n    currentConsumer = null;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.SinkQueue.size": "  synchronized int size() {\n    return size;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.SinkQueue.consume": "  void consume(Consumer<T> consumer) throws InterruptedException {\n    T e = waitForData();\n\n    try {\n      consumer.consume(e);  // can take forever\n      _dequeue();\n    }\n    finally {\n      clearConsumerLock();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.SinkQueue.waitForData": "  private synchronized T waitForData() throws InterruptedException {\n    checkConsumer();\n\n    while (0 == size) {\n      wait();\n    }\n    setConsumerLock();\n    return front();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue": "  void publishMetricsFromQueue() {\n    int retryDelay = firstRetryDelay;\n    int n = retryCount;\n    int minDelay = Math.min(500, retryDelay * 1000); // millis\n    Random rng = new Random(System.nanoTime());\n    while (!stopping) {\n      try {\n        queue.consumeAll(this);\n        retryDelay = firstRetryDelay;\n        n = retryCount;\n        inError = false;\n      } catch (InterruptedException e) {\n        LOG.info(name +\" thread interrupted.\");\n      } catch (Exception e) {\n        if (n > 0) {\n          int retryWindow = Math.max(0, 1000 / 2 * retryDelay - minDelay);\n          int awhile = rng.nextInt(retryWindow) + minDelay;\n          if (!inError) {\n            LOG.error(\"Got sink exception, retry in \"+ awhile +\"ms\", e);\n          }\n          retryDelay *= retryBackoff;\n          try { Thread.sleep(awhile); }\n          catch (InterruptedException e2) {\n            LOG.info(name +\" thread interrupted while waiting for retry\", e2);\n          }\n          --n;\n        } else {\n          if (!inError) {\n            LOG.error(\"Got sink exception and over retry limit, \"+\n                      \"suppressing further error messages\", e);\n          }\n          queue.clear();\n          inError = true; // Don't keep complaining ad infinitum\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.run": "      @Override public void run() {\n        publishMetricsFromQueue();\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketOutputStream.write": "  public int write(ByteBuffer src) throws IOException {\n    return writer.doIO(src, SelectionKey.OP_WRITE);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketOutputStream.close": "  public synchronized void close() throws IOException {\n    /* close the channel since Socket.getOuputStream().close() \n     * closes the socket.\n     */\n    writer.channel.close();\n    writer.close();\n  }"
        },
        "bug_report": {
            "Title": "GraphiteSink does not reconnect to Graphite after 'broken pipe'",
            "Description": "I see that after network error GraphiteSink does not reconnects to Graphite server and in effect metrics are not sent. \n\nHere is stacktrace I see (this is from nodemanager):\n\n2014-12-11 16:39:21,655 ERROR org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: Got sink exception, retry in 4806ms\norg.apache.hadoop.metrics2.MetricsException: Error flushing metrics\n        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:120)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:184)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)\n        at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:129)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:88)\nCaused by: java.net.SocketException: Broken pipe\n        at java.net.SocketOutputStream.socketWrite0(Native Method)\n        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)\n        at java.net.SocketOutputStream.write(SocketOutputStream.java:159)\n        at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221)\n        at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291)\n        at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:295)\n        at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:141)\n        at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:229)\n        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:118)\n        ... 5 more\n2014-12-11 16:39:26,463 ERROR org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: Got sink exception and over retry limit, suppressing further error messages\norg.apache.hadoop.metrics2.MetricsException: Error flushing metrics\n        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:120)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:184)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)\n        at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:129)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:88)\nCaused by: java.net.SocketException: Broken pipe\n        at java.net.SocketOutputStream.socketWrite0(Native Method)\n        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)\n        at java.net.SocketOutputStream.write(SocketOutputStream.java:159)\n        at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221)\n        at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291)\n        at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:295)\n        at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:141)\n        at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:229)\n        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:118)\n        ... 5 more\n\n\nGraphiteSinkFixed.java is simply GraphiteSink.java from Hadoop 2.6.0 (with fixed https://issues.apache.org/jira/browse/HADOOP-11182) because I cannot simply upgrade Hadoop (I am using CDH5).\n\nI see that GraphiteSink is using OutputStreamWriter which is created only in init method (which is probably called only once per application runtime) and there is no reconnection logic."
        }
    },
    {
        "filename": "HADOOP-9865.json",
        "creation_time": "2013-08-12T23:22:58.000+0000",
        "stack_trace": "```\norg.apache.hadoop.HadoopIllegalArgumentException: Path is relative\n\tat org.apache.hadoop.fs.Path.checkNotRelative(Path.java:74)\n\tat org.apache.hadoop.fs.FileContext.getFSofPath(FileContext.java:304)\n\tat org.apache.hadoop.fs.Globber.schemeFromPath(Globber.java:107)\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:128)\n\tat org.apache.hadoop.fs.FileContext$Util.globStatus(FileContext.java:1908)\n\tat org.apache.hadoop.fs.FileUtil.createJarWithClassPath(FileUtil.java:1247)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.sanitizeEnv(ContainerLaunch.java:679)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:232)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:1)\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:138)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:662)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Path.checkNotRelative": "  void checkNotRelative() {\n    if (!isAbsolute() && toUri().getScheme() == null) {\n      throw new HadoopIllegalArgumentException(\"Path is relative\");\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Path.isAbsolute": "  public boolean isAbsolute() {\n     return isUriPathAbsolute();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Path.toUri": "  public URI toUri() { return uri; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Globber.schemeFromPath": "  private String schemeFromPath(Path path) throws IOException {\n    String scheme = pathPattern.toUri().getScheme();\n    if (scheme == null) {\n      if (fs != null) {\n        scheme = fs.getUri().getScheme();\n      } else {\n        scheme = fc.getFSofPath(path).getUri().getScheme();\n      }\n    }\n    return scheme;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Globber.glob": "  public FileStatus[] glob() throws IOException {\n    // First we get the scheme and authority of the pattern that was passed\n    // in.\n    String scheme = schemeFromPath(pathPattern);\n    String authority = authorityFromPath(pathPattern);\n\n    // Next we strip off everything except the pathname itself, and expand all\n    // globs.  Expansion is a process which turns \"grouping\" clauses,\n    // expressed as brackets, into separate path patterns.\n    String pathPatternString = pathPattern.toUri().getPath();\n    List<String> flattenedPatterns = GlobExpander.expand(pathPatternString);\n\n    // Now loop over all flattened patterns.  In every case, we'll be trying to\n    // match them to entries in the filesystem.\n    ArrayList<FileStatus> results = \n        new ArrayList<FileStatus>(flattenedPatterns.size());\n    boolean sawWildcard = false;\n    for (String flatPattern : flattenedPatterns) {\n      // Get the absolute path for this flattened pattern.  We couldn't do \n      // this prior to flattening because of patterns like {/,a}, where which\n      // path you go down influences how the path must be made absolute.\n      Path absPattern =\n          fixRelativePart(new Path(flatPattern .isEmpty() ? \".\" : flatPattern ));\n      // Now we break the flattened, absolute pattern into path components.\n      // For example, /a/*/c would be broken into the list [a, *, c]\n      List<String> components =\n          getPathComponents(absPattern.toUri().getPath());\n      // Starting out at the root of the filesystem, we try to match\n      // filesystem entries against pattern components.\n      ArrayList<FileStatus> candidates = new ArrayList<FileStatus>(1);\n      candidates.add(new FileStatus(0, true, 0, 0, 0,\n          new Path(scheme, authority, \"/\")));\n\n      for (String component : components) {\n        ArrayList<FileStatus> newCandidates =\n            new ArrayList<FileStatus>(candidates.size());\n        GlobFilter globFilter = new GlobFilter(component);\n        if (globFilter.hasPattern()) {\n          sawWildcard = true;\n        }\n        if (candidates.isEmpty() && sawWildcard) {\n          break;\n        }\n        for (FileStatus candidate : candidates) {\n          FileStatus resolvedCandidate = candidate;\n          if (candidate.isSymlink()) {\n            // We have to resolve symlinks, because otherwise we don't know\n            // whether they are directories.\n            resolvedCandidate = getFileStatus(candidate.getPath());\n          }\n          if (resolvedCandidate == null ||\n              resolvedCandidate.isDirectory() == false) {\n            continue;\n          }\n          FileStatus[] children = listStatus(candidate.getPath());\n          for (FileStatus child : children) {\n            // Set the child path based on the parent path.\n            // This keeps the symlinks in our path.\n            child.setPath(new Path(candidate.getPath(),\n                    child.getPath().getName()));\n            if (globFilter.accept(child.getPath())) {\n              newCandidates.add(child);\n            }\n          }\n        }\n        candidates = newCandidates;\n      }\n      for (FileStatus status : candidates) {\n        // HADOOP-3497 semantics: the user-defined filter is applied at the\n        // end, once the full path is built up.\n        if (filter.accept(status.getPath())) {\n          results.add(status);\n        }\n      }\n    }\n    /*\n     * When the input pattern \"looks\" like just a simple filename, and we\n     * can't find it, we return null rather than an empty array.\n     * This is a special case which the shell relies on.\n     *\n     * To be more precise: if there were no results, AND there were no\n     * groupings (aka brackets), and no wildcards in the input (aka stars),\n     * we return null.\n     */\n    if ((!sawWildcard) && results.isEmpty() &&\n        (flattenedPatterns.size() <= 1)) {\n      return null;\n    }\n    return results.toArray(new FileStatus[0]);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Globber.getFileStatus": "  private FileStatus getFileStatus(Path path) {\n    try {\n      if (fs != null) {\n        return fs.getFileStatus(path);\n      } else {\n        return fc.getFileStatus(path);\n      }\n    } catch (IOException e) {\n      return null;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Globber.authorityFromPath": "  private String authorityFromPath(Path path) throws IOException {\n    String authority = pathPattern.toUri().getAuthority();\n    if (authority == null) {\n      if (fs != null) {\n        authority = fs.getUri().getAuthority();\n      } else {\n        authority = fc.getFSofPath(path).getUri().getAuthority();\n      }\n    }\n    return authority ;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Globber.listStatus": "  private FileStatus[] listStatus(Path path) {\n    try {\n      if (fs != null) {\n        return fs.listStatus(path);\n      } else {\n        return fc.util().listStatus(path);\n      }\n    } catch (IOException e) {\n      return new FileStatus[0];\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Globber.getPathComponents": "  private static List<String> getPathComponents(String path)\n      throws IOException {\n    ArrayList<String> ret = new ArrayList<String>();\n    for (String component : path.split(Path.SEPARATOR)) {\n      if (!component.isEmpty()) {\n        ret.add(component);\n      }\n    }\n    return ret;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Globber.fixRelativePart": "  private Path fixRelativePart(Path path) {\n    if (fs != null) {\n      return fs.fixRelativePart(path);\n    } else {\n      return fc.fixRelativePart(path);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileUtil.createJarWithClassPath": "  public static String createJarWithClassPath(String inputClassPath, Path pwd,\n      Map<String, String> callerEnv) throws IOException {\n    // Replace environment variables, case-insensitive on Windows\n    @SuppressWarnings(\"unchecked\")\n    Map<String, String> env = Shell.WINDOWS ? new CaseInsensitiveMap(callerEnv) :\n      callerEnv;\n    String[] classPathEntries = inputClassPath.split(File.pathSeparator);\n    for (int i = 0; i < classPathEntries.length; ++i) {\n      classPathEntries[i] = StringUtils.replaceTokens(classPathEntries[i],\n        StringUtils.ENV_VAR_PATTERN, env);\n    }\n    File workingDir = new File(pwd.toString());\n    if (!workingDir.mkdirs()) {\n      // If mkdirs returns false because the working directory already exists,\n      // then this is acceptable.  If it returns false due to some other I/O\n      // error, then this method will fail later with an IOException while saving\n      // the jar.\n      LOG.debug(\"mkdirs false for \" + workingDir + \", execution will continue\");\n    }\n\n    // Append all entries\n    List<String> classPathEntryList = new ArrayList<String>(\n      classPathEntries.length);\n    for (String classPathEntry: classPathEntries) {\n      if (classPathEntry.endsWith(\"*\")) {\n        // Append all jars that match the wildcard\n        Path globPath = new Path(classPathEntry).suffix(\"{.jar,.JAR}\");\n        FileStatus[] wildcardJars = FileContext.getLocalFSFileContext().util()\n          .globStatus(globPath);\n        if (wildcardJars != null) {\n          for (FileStatus wildcardJar: wildcardJars) {\n            classPathEntryList.add(wildcardJar.getPath().toUri().toURL()\n              .toExternalForm());\n          }\n        }\n      } else {\n        // Append just this entry\n        String classPathEntryUrl = new File(classPathEntry).toURI().toURL()\n          .toExternalForm();\n\n        // File.toURI only appends trailing '/' if it can determine that it is a\n        // directory that already exists.  (See JavaDocs.)  If this entry had a\n        // trailing '/' specified by the caller, then guarantee that the\n        // classpath entry in the manifest has a trailing '/', and thus refers to\n        // a directory instead of a file.  This can happen if the caller is\n        // creating a classpath jar referencing a directory that hasn't been\n        // created yet, but will definitely be created before running.\n        if (classPathEntry.endsWith(Path.SEPARATOR) &&\n            !classPathEntryUrl.endsWith(Path.SEPARATOR)) {\n          classPathEntryUrl = classPathEntryUrl + Path.SEPARATOR;\n        }\n        classPathEntryList.add(classPathEntryUrl);\n      }\n    }\n    String jarClassPath = StringUtils.join(\" \", classPathEntryList);\n\n    // Create the manifest\n    Manifest jarManifest = new Manifest();\n    jarManifest.getMainAttributes().putValue(\n        Attributes.Name.MANIFEST_VERSION.toString(), \"1.0\");\n    jarManifest.getMainAttributes().putValue(\n        Attributes.Name.CLASS_PATH.toString(), jarClassPath);\n\n    // Write the manifest to output JAR file\n    File classPathJar = File.createTempFile(\"classpath-\", \".jar\", workingDir);\n    FileOutputStream fos = null;\n    BufferedOutputStream bos = null;\n    JarOutputStream jos = null;\n    try {\n      fos = new FileOutputStream(classPathJar);\n      bos = new BufferedOutputStream(fos);\n      jos = new JarOutputStream(bos, jarManifest);\n    } finally {\n      IOUtils.cleanup(LOG, jos, bos, fos);\n    }\n\n    return classPathJar.getCanonicalPath();\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.sanitizeEnv": "  public void sanitizeEnv(Map<String, String> environment, Path pwd,\n      List<Path> appDirs, List<String> containerLogDirs,\n      Map<Path, List<String>> resources) throws IOException {\n    /**\n     * Non-modifiable environment variables\n     */\n\n    environment.put(Environment.CONTAINER_ID.name(), container\n        .getContainerId().toString());\n\n    environment.put(Environment.NM_PORT.name(),\n      String.valueOf(this.context.getNodeId().getPort()));\n\n    environment.put(Environment.NM_HOST.name(), this.context.getNodeId()\n      .getHost());\n\n    environment.put(Environment.NM_HTTP_PORT.name(),\n      String.valueOf(this.context.getHttpPort()));\n\n    environment.put(Environment.LOCAL_DIRS.name(),\n        StringUtils.join(\",\", appDirs));\n\n    environment.put(Environment.LOG_DIRS.name(),\n      StringUtils.join(\",\", containerLogDirs));\n\n    putEnvIfNotNull(environment, Environment.USER.name(), container.getUser());\n    \n    putEnvIfNotNull(environment, \n        Environment.LOGNAME.name(),container.getUser());\n    \n    putEnvIfNotNull(environment, \n        Environment.HOME.name(),\n        conf.get(\n            YarnConfiguration.NM_USER_HOME_DIR, \n            YarnConfiguration.DEFAULT_NM_USER_HOME_DIR\n            )\n        );\n    \n    putEnvIfNotNull(environment, Environment.PWD.name(), pwd.toString());\n    \n    putEnvIfNotNull(environment, \n        Environment.HADOOP_CONF_DIR.name(), \n        System.getenv(Environment.HADOOP_CONF_DIR.name())\n        );\n\n    if (!Shell.WINDOWS) {\n      environment.put(\"JVM_PID\", \"$$\");\n    }\n\n    /**\n     * Modifiable environment variables\n     */\n    \n    // allow containers to override these variables\n    String[] whitelist = conf.get(YarnConfiguration.NM_ENV_WHITELIST, YarnConfiguration.DEFAULT_NM_ENV_WHITELIST).split(\",\");\n    \n    for(String whitelistEnvVariable : whitelist) {\n      putEnvIfAbsent(environment, whitelistEnvVariable.trim());\n    }\n\n    // variables here will be forced in, even if the container has specified them.\n    Apps.setEnvFromInputString(\n      environment,\n      conf.get(\n        YarnConfiguration.NM_ADMIN_USER_ENV,\n        YarnConfiguration.DEFAULT_NM_ADMIN_USER_ENV)\n    );\n\n    // TODO: Remove Windows check and use this approach on all platforms after\n    // additional testing.  See YARN-358.\n    if (Shell.WINDOWS) {\n      String inputClassPath = environment.get(Environment.CLASSPATH.name());\n      if (inputClassPath != null && !inputClassPath.isEmpty()) {\n        StringBuilder newClassPath = new StringBuilder(inputClassPath);\n\n        // Localized resources do not exist at the desired paths yet, because the\n        // container launch script has not run to create symlinks yet.  This\n        // means that FileUtil.createJarWithClassPath can't automatically expand\n        // wildcards to separate classpath entries for each file in the manifest.\n        // To resolve this, append classpath entries explicitly for each\n        // resource.\n        for (Map.Entry<Path,List<String>> entry : resources.entrySet()) {\n          boolean targetIsDirectory = new File(entry.getKey().toUri().getPath())\n            .isDirectory();\n\n          for (String linkName : entry.getValue()) {\n            // Append resource.\n            newClassPath.append(File.pathSeparator).append(pwd.toString())\n              .append(Path.SEPARATOR).append(linkName);\n\n            // FileUtil.createJarWithClassPath must use File.toURI to convert\n            // each file to a URI to write into the manifest's classpath.  For\n            // directories, the classpath must have a trailing '/', but\n            // File.toURI only appends the trailing '/' if it is a directory that\n            // already exists.  To resolve this, add the classpath entries with\n            // explicit trailing '/' here for any localized resource that targets\n            // a directory.  Then, FileUtil.createJarWithClassPath will guarantee\n            // that the resulting entry in the manifest's classpath will have a\n            // trailing '/', and thus refer to a directory instead of a file.\n            if (targetIsDirectory) {\n              newClassPath.append(Path.SEPARATOR);\n            }\n          }\n        }\n\n        // When the container launches, it takes the parent process's environment\n        // and then adds/overwrites with the entries from the container launch\n        // context.  Do the same thing here for correct substitution of\n        // environment variables in the classpath jar manifest.\n        Map<String, String> mergedEnv = new HashMap<String, String>(\n          System.getenv());\n        mergedEnv.putAll(environment);\n\n        String classPathJar = FileUtil.createJarWithClassPath(\n          newClassPath.toString(), pwd, mergedEnv);\n        environment.put(Environment.CLASSPATH.name(), classPathJar);\n      }\n    }\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.putEnvIfNotNull": "  private static void putEnvIfNotNull(\n      Map<String, String> environment, String variable, String value) {\n    if (value != null) {\n      environment.put(variable, value);\n    }\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.putEnvIfAbsent": "  private static void putEnvIfAbsent(\n      Map<String, String> environment, String variable) {\n    if (environment.get(variable) == null) {\n      putEnvIfNotNull(environment, variable, System.getenv(variable));\n    }\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.toString": "    public String toString() {\n      return sb.toString();\n    }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call": "  public Integer call() {\n    final ContainerLaunchContext launchContext = container.getLaunchContext();\n    Map<Path,List<String>> localResources = null;\n    ContainerId containerID = container.getContainerId();\n    String containerIdStr = ConverterUtils.toString(containerID);\n    final List<String> command = launchContext.getCommands();\n    int ret = -1;\n\n    try {\n      localResources = container.getLocalizedResources();\n      if (localResources == null) {\n        RPCUtil.getRemoteException(\n            \"Unable to get local resources when Container \" + containerID +\n            \" is at \" + container.getContainerState());\n      }\n\n      final String user = container.getUser();\n      // /////////////////////////// Variable expansion\n      // Before the container script gets written out.\n      List<String> newCmds = new ArrayList<String>(command.size());\n      String appIdStr = app.getAppId().toString();\n      String relativeContainerLogDir = ContainerLaunch\n          .getRelativeContainerLogDir(appIdStr, containerIdStr);\n      Path containerLogDir =\n          dirsHandler.getLogPathForWrite(relativeContainerLogDir, false);\n      for (String str : command) {\n        // TODO: Should we instead work via symlinks without this grammar?\n        newCmds.add(str.replace(ApplicationConstants.LOG_DIR_EXPANSION_VAR,\n            containerLogDir.toString()));\n      }\n      launchContext.setCommands(newCmds);\n\n      Map<String, String> environment = launchContext.getEnvironment();\n      // Make a copy of env to iterate & do variable expansion\n      for (Entry<String, String> entry : environment.entrySet()) {\n        String value = entry.getValue();\n        entry.setValue(\n            value.replace(\n                ApplicationConstants.LOG_DIR_EXPANSION_VAR,\n                containerLogDir.toString())\n            );\n      }\n      // /////////////////////////// End of variable expansion\n\n      FileContext lfs = FileContext.getLocalFSFileContext();\n\n      Path nmPrivateContainerScriptPath =\n          dirsHandler.getLocalPathForWrite(\n              getContainerPrivateDir(appIdStr, containerIdStr) + Path.SEPARATOR\n                  + CONTAINER_SCRIPT);\n      Path nmPrivateTokensPath =\n          dirsHandler.getLocalPathForWrite(\n              getContainerPrivateDir(appIdStr, containerIdStr)\n                  + Path.SEPARATOR\n                  + String.format(ContainerLocalizer.TOKEN_FILE_NAME_FMT,\n                      containerIdStr));\n\n      DataOutputStream containerScriptOutStream = null;\n      DataOutputStream tokensOutStream = null;\n\n      // Select the working directory for the container\n      Path containerWorkDir =\n          dirsHandler.getLocalPathForWrite(ContainerLocalizer.USERCACHE\n              + Path.SEPARATOR + user + Path.SEPARATOR\n              + ContainerLocalizer.APPCACHE + Path.SEPARATOR + appIdStr\n              + Path.SEPARATOR + containerIdStr,\n              LocalDirAllocator.SIZE_UNKNOWN, false);\n\n      String pidFileSuffix = String.format(ContainerLaunch.PID_FILE_NAME_FMT,\n          containerIdStr);\n\n      // pid file should be in nm private dir so that it is not \n      // accessible by users\n      pidFilePath = dirsHandler.getLocalPathForWrite(\n          ResourceLocalizationService.NM_PRIVATE_DIR + Path.SEPARATOR \n          + pidFileSuffix);\n      List<String> localDirs = dirsHandler.getLocalDirs();\n      List<String> logDirs = dirsHandler.getLogDirs();\n\n      List<String> containerLogDirs = new ArrayList<String>();\n      for( String logDir : logDirs) {\n        containerLogDirs.add(logDir + Path.SEPARATOR + relativeContainerLogDir);\n      }\n\n      if (!dirsHandler.areDisksHealthy()) {\n        ret = ContainerExitStatus.DISKS_FAILED;\n        throw new IOException(\"Most of the disks failed. \"\n            + dirsHandler.getDisksHealthReport());\n      }\n\n      try {\n        // /////////// Write out the container-script in the nmPrivate space.\n        List<Path> appDirs = new ArrayList<Path>(localDirs.size());\n        for (String localDir : localDirs) {\n          Path usersdir = new Path(localDir, ContainerLocalizer.USERCACHE);\n          Path userdir = new Path(usersdir, user);\n          Path appsdir = new Path(userdir, ContainerLocalizer.APPCACHE);\n          appDirs.add(new Path(appsdir, appIdStr));\n        }\n        containerScriptOutStream =\n          lfs.create(nmPrivateContainerScriptPath,\n              EnumSet.of(CREATE, OVERWRITE));\n\n        // Set the token location too.\n        environment.put(\n            ApplicationConstants.CONTAINER_TOKEN_FILE_ENV_NAME, \n            new Path(containerWorkDir, \n                FINAL_CONTAINER_TOKENS_FILE).toUri().getPath());\n\n        // Sanitize the container's environment\n        sanitizeEnv(environment, containerWorkDir, appDirs, containerLogDirs,\n          localResources);\n        \n        // Write out the environment\n        writeLaunchEnv(containerScriptOutStream, environment, localResources,\n            launchContext.getCommands());\n        \n        // /////////// End of writing out container-script\n\n        // /////////// Write out the container-tokens in the nmPrivate space.\n        tokensOutStream =\n            lfs.create(nmPrivateTokensPath, EnumSet.of(CREATE, OVERWRITE));\n        Credentials creds = container.getCredentials();\n        creds.writeTokenStorageToStream(tokensOutStream);\n        // /////////// End of writing out container-tokens\n      } finally {\n        IOUtils.cleanup(LOG, containerScriptOutStream, tokensOutStream);\n      }\n\n      // LaunchContainer is a blocking call. We are here almost means the\n      // container is launched, so send out the event.\n      dispatcher.getEventHandler().handle(new ContainerEvent(\n            containerID,\n            ContainerEventType.CONTAINER_LAUNCHED));\n\n      // Check if the container is signalled to be killed.\n      if (!shouldLaunchContainer.compareAndSet(false, true)) {\n        LOG.info(\"Container \" + containerIdStr + \" not launched as \"\n            + \"cleanup already called\");\n        ret = ExitCode.TERMINATED.getExitCode();\n      }\n      else {\n        exec.activateContainer(containerID, pidFilePath);\n        ret = exec.launchContainer(container, nmPrivateContainerScriptPath,\n                nmPrivateTokensPath, user, appIdStr, containerWorkDir,\n                localDirs, logDirs);\n      }\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to launch container.\", e);\n      dispatcher.getEventHandler().handle(new ContainerExitEvent(\n          containerID, ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, ret,\n          e.getMessage()));\n      return ret;\n    } finally {\n      completed.set(true);\n      exec.deactivateContainer(containerID);\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Container \" + containerIdStr + \" completed with exit code \"\n                + ret);\n    }\n    if (ret == ExitCode.FORCE_KILLED.getExitCode()\n        || ret == ExitCode.TERMINATED.getExitCode()) {\n      // If the process was killed, Send container_cleanedup_after_kill and\n      // just break out of this method.\n      dispatcher.getEventHandler().handle(\n            new ContainerExitEvent(containerID,\n                ContainerEventType.CONTAINER_KILLED_ON_REQUEST, ret,\n                \"Container exited with a non-zero exit code \" + ret));\n      return ret;\n    }\n\n    if (ret != 0) {\n      LOG.warn(\"Container exited with a non-zero exit code \" + ret);\n      this.dispatcher.getEventHandler().handle(new ContainerExitEvent(\n          containerID,\n          ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, ret,\n          \"Container exited with a non-zero exit code \" + ret));\n      return ret;\n    }\n\n    LOG.info(\"Container \" + containerIdStr + \" succeeded \");\n    dispatcher.getEventHandler().handle(\n        new ContainerEvent(containerID,\n            ContainerEventType.CONTAINER_EXITED_WITH_SUCCESS));\n    return 0;\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.writeLaunchEnv": "  static void writeLaunchEnv(OutputStream out,\n      Map<String,String> environment, Map<Path,List<String>> resources,\n      List<String> command)\n      throws IOException {\n    ShellScriptBuilder sb = Shell.WINDOWS ? new WindowsShellScriptBuilder() :\n      new UnixShellScriptBuilder();\n    if (environment != null) {\n      for (Map.Entry<String,String> env : environment.entrySet()) {\n        sb.env(env.getKey().toString(), env.getValue().toString());\n      }\n    }\n    if (resources != null) {\n      for (Map.Entry<Path,List<String>> entry : resources.entrySet()) {\n        for (String linkName : entry.getValue()) {\n          sb.symlink(entry.getKey(), new Path(linkName));\n        }\n      }\n    }\n\n    sb.command(command);\n\n    PrintStream pout = null;\n    try {\n      pout = new PrintStream(out);\n      sb.write(pout);\n    } finally {\n      if (out != null) {\n        out.close();\n      }\n    }\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.getRelativeContainerLogDir": "  public static String getRelativeContainerLogDir(String appIdStr,\n      String containerIdStr) {\n    return appIdStr + Path.SEPARATOR + containerIdStr;\n  }",
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.getContainerPrivateDir": "  private String getContainerPrivateDir(String appIdStr, String containerIdStr) {\n    return getAppPrivateDir(appIdStr) + Path.SEPARATOR + containerIdStr\n        + Path.SEPARATOR;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileStatus.isDirectory": "  public boolean isDirectory() {\n    return isdir;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.GlobFilter.accept": "  public boolean accept(Path path) {\n    return pattern.matches(path.getName()) && userFilter.accept(path);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.GlobExpander.expand": "  public static List<String> expand(String filePattern) throws IOException {\n    List<String> fullyExpanded = new ArrayList<String>();\n    List<StringWithOffset> toExpand = new ArrayList<StringWithOffset>();\n    toExpand.add(new StringWithOffset(filePattern, 0));\n    while (!toExpand.isEmpty()) {\n      StringWithOffset path = toExpand.remove(0);\n      List<StringWithOffset> expanded = expandLeftmost(path);\n      if (expanded == null) {\n        fullyExpanded.add(path.string);\n      } else {\n        toExpand.addAll(0, expanded);\n      }\n    }\n    return fullyExpanded;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.GlobExpander.expandLeftmost": "  private static List<StringWithOffset> expandLeftmost(StringWithOffset\n      filePatternWithOffset) throws IOException {\n    \n    String filePattern = filePatternWithOffset.string;\n    int leftmost = leftmostOuterCurlyContainingSlash(filePattern,\n        filePatternWithOffset.offset);\n    if (leftmost == -1) {\n      return null;\n    }\n    int curlyOpen = 0;\n    StringBuilder prefix = new StringBuilder(filePattern.substring(0, leftmost));\n    StringBuilder suffix = new StringBuilder();\n    List<String> alts = new ArrayList<String>();\n    StringBuilder alt = new StringBuilder();\n    StringBuilder cur = prefix;\n    for (int i = leftmost; i < filePattern.length(); i++) {\n      char c = filePattern.charAt(i);\n      if (cur == suffix) {\n        cur.append(c);\n      } else if (c == '\\\\') {\n        i++;\n        if (i >= filePattern.length()) {\n          throw new IOException(\"Illegal file pattern: \"\n              + \"An escaped character does not present for glob \"\n              + filePattern + \" at \" + i);\n        }\n        c = filePattern.charAt(i);\n        cur.append(c);\n      } else if (c == '{') {\n        if (curlyOpen++ == 0) {\n          alt.setLength(0);\n          cur = alt;\n        } else {\n          cur.append(c);\n        }\n\n      } else if (c == '}' && curlyOpen > 0) {\n        if (--curlyOpen == 0) {\n          alts.add(alt.toString());\n          alt.setLength(0);\n          cur = suffix;\n        } else {\n          cur.append(c);\n        }\n      } else if (c == ',') {\n        if (curlyOpen == 1) {\n          alts.add(alt.toString());\n          alt.setLength(0);\n        } else {\n          cur.append(c);\n        }\n      } else {\n        cur.append(c);\n      }\n    }\n    List<StringWithOffset> exp = new ArrayList<StringWithOffset>();\n    for (String string : alts) {\n      exp.add(new StringWithOffset(prefix + string + suffix, prefix.length()));\n    }\n    return exp;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.GlobFilter.hasPattern": "  boolean hasPattern() {\n    return pattern.hasWildcard();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.getLocalFSFileContext": "  public static FileContext getLocalFSFileContext(final Configuration aConf)\n      throws UnsupportedFileSystemException {\n    return getFileContext(FsConstants.LOCAL_FS_URI, aConf);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.getFileContext": "  public static FileContext getFileContext(final Configuration aConf)\n      throws UnsupportedFileSystemException {\n    return getFileContext(\n      URI.create(aConf.get(FS_DEFAULT_NAME_KEY, FS_DEFAULT_NAME_DEFAULT)), \n      aConf);\n  }"
        },
        "bug_report": {
            "Title": "FileContext.globStatus() has a regression with respect to relative path",
            "Description": "I discovered the problem when running unit test TestMRJobClient on Windows. The cause is indirect in this case. In the unit test, we try to launch a job and list its status. The job failed, and caused the list command get a result of 0, which triggered the unit test assert. From the log and debug, the job failed because we failed to create the Jar with classpath (see code around {{FileUtil.createJarWithClassPath}}) in {{ContainerLaunch}}. This is a Windows specific step right now; so the test still passes on Linux. This step failed because we passed in a relative path to {{FileContext.globStatus()}} in {{FileUtil.createJarWithClassPath}}. The relevant log looks like the following.\n\n{noformat}\n2013-08-12 16:12:05,937 WARN  [ContainersLauncher #0] launcher.ContainerLaunch (ContainerLaunch.java:call(270)) - Failed to launch container.\norg.apache.hadoop.HadoopIllegalArgumentException: Path is relative\n\tat org.apache.hadoop.fs.Path.checkNotRelative(Path.java:74)\n\tat org.apache.hadoop.fs.FileContext.getFSofPath(FileContext.java:304)\n\tat org.apache.hadoop.fs.Globber.schemeFromPath(Globber.java:107)\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:128)\n\tat org.apache.hadoop.fs.FileContext$Util.globStatus(FileContext.java:1908)\n\tat org.apache.hadoop.fs.FileUtil.createJarWithClassPath(FileUtil.java:1247)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.sanitizeEnv(ContainerLaunch.java:679)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:232)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:1)\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:138)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:662)\n{noformat}\n\nI think this is a regression from HADOOP-9817. I modified some code and the unit test passed. (See the attached patch.) However, I think the impact is larger. I will add some unit tests to verify the behavior, and work on a more complete fix."
        }
    },
    {
        "filename": "HADOOP-9977.json",
        "creation_time": "2013-09-17T22:11:54.000+0000",
        "stack_trace": "```\njava.io.IOException: java.security.UnrecoverableKeyException: Cannot recover key\n        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:222)\n        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:174)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer$1.<init>(NameNodeHttpServer.java:76)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:74)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:626)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:488)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:684)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:669)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1254)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1320)\nCaused by: java.security.UnrecoverableKeyException: Cannot recover key\n        at sun.security.provider.KeyProtector.recover(KeyProtector.java:328)\n        at sun.security.provider.JavaKeyStore.engineGetKey(JavaKeyStore.java:138)\n        at sun.security.provider.JavaKeyStore$JKS.engineGetKey(JavaKeyStore.java:55)\n        at java.security.KeyStore.getKey(KeyStore.java:792)\n        at sun.security.ssl.SunX509KeyManagerImpl.<init>(SunX509KeyManagerImpl.java:131)\n        at sun.security.ssl.KeyManagerFactoryImpl$SunX509.engineInit(KeyManagerFactoryImpl.java:68)\n        at javax.net.ssl.KeyManagerFactory.init(KeyManagerFactory.java:259)\n        at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:170)\n        at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:121)\n        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:220)```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start": "  public void start() throws IOException {\n    final String infoHost = bindAddress.getHostName();\n    int infoPort = bindAddress.getPort();\n    httpServer = new HttpServer.Builder().setName(\"hdfs\")\n        .setBindAddress(infoHost).setPort(infoPort)\n        .setFindPort(infoPort == 0).setConf(conf).setACL(\n            new AccessControlList(conf.get(DFS_ADMIN, \" \")))\n        .setSecurityEnabled(UserGroupInformation.isSecurityEnabled())\n        .setUsernameConfKey(\n            DFSConfigKeys.DFS_NAMENODE_INTERNAL_SPNEGO_USER_NAME_KEY)\n        .setKeytabConfKey(DFSUtil.getSpnegoKeytabKey(conf,\n            DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY)).build();\n    if (WebHdfsFileSystem.isEnabled(conf, HttpServer.LOG)) {\n      //add SPNEGO authentication filter for webhdfs\n      final String name = \"SPNEGO\";\n      final String classname = AuthFilter.class.getName();\n      final String pathSpec = WebHdfsFileSystem.PATH_PREFIX + \"/*\";\n      Map<String, String> params = getAuthFilterParams(conf);\n      httpServer.defineFilter(httpServer.getWebAppContext(), name, classname, params,\n          new String[]{pathSpec});\n      HttpServer.LOG.info(\"Added filter '\" + name + \"' (class=\" + classname + \")\");\n\n      // add webhdfs packages\n      httpServer.addJerseyResourcePackage(\n          NamenodeWebHdfsMethods.class.getPackage().getName()\n          + \";\" + Param.class.getPackage().getName(), pathSpec);\n      }\n\n    boolean certSSL = conf.getBoolean(DFSConfigKeys.DFS_HTTPS_ENABLE_KEY, false);\n    if (certSSL) {\n      boolean needClientAuth = conf.getBoolean(\"dfs.https.need.client.auth\", false);\n      InetSocketAddress secInfoSocAddr = NetUtils.createSocketAddr(infoHost + \":\" + conf.get(\n        DFSConfigKeys.DFS_NAMENODE_HTTPS_PORT_KEY, infoHost + \":\" + 0));\n      Configuration sslConf = new Configuration(false);\n      if (certSSL) {\n        sslConf.addResource(conf.get(DFSConfigKeys.DFS_SERVER_HTTPS_KEYSTORE_RESOURCE_KEY,\n                                     \"ssl-server.xml\"));\n      }\n      httpServer.addSslListener(secInfoSocAddr, sslConf, needClientAuth);\n      // assume same ssl port for all datanodes\n      InetSocketAddress datanodeSslPort = NetUtils.createSocketAddr(conf.get(\n        DFSConfigKeys.DFS_DATANODE_HTTPS_ADDRESS_KEY, infoHost + \":\" + 50475));\n      httpServer.setAttribute(DFSConfigKeys.DFS_DATANODE_HTTPS_PORT_KEY, datanodeSslPort\n        .getPort());\n    }\n    httpServer.setAttribute(NAMENODE_ATTRIBUTE_KEY, nn);\n    httpServer.setAttribute(JspHelper.CURRENT_CONF, conf);\n    setupServlets(httpServer, conf);\n    httpServer.start();\n    httpAddress = new InetSocketAddress(bindAddress.getAddress(), httpServer.getPort());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.getAuthFilterParams": "  private Map<String, String> getAuthFilterParams(Configuration conf)\n      throws IOException {\n    Map<String, String> params = new HashMap<String, String>();\n    String principalInConf = conf\n        .get(DFSConfigKeys.DFS_WEB_AUTHENTICATION_KERBEROS_PRINCIPAL_KEY);\n    if (principalInConf != null && !principalInConf.isEmpty()) {\n      params\n          .put(\n              DFSConfigKeys.DFS_WEB_AUTHENTICATION_KERBEROS_PRINCIPAL_KEY,\n              SecurityUtil.getServerPrincipal(principalInConf,\n                                              bindAddress.getHostName()));\n    } else if (UserGroupInformation.isSecurityEnabled()) {\n      HttpServer.LOG.error(\n          \"WebHDFS and security are enabled, but configuration property '\" +\n          DFSConfigKeys.DFS_WEB_AUTHENTICATION_KERBEROS_PRINCIPAL_KEY +\n          \"' is not set.\");\n    }\n    String httpKeytab = conf.get(DFSUtil.getSpnegoKeytabKey(conf,\n        DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY));\n    if (httpKeytab != null && !httpKeytab.isEmpty()) {\n      params.put(\n          DFSConfigKeys.DFS_WEB_AUTHENTICATION_KERBEROS_KEYTAB_KEY,\n          httpKeytab);\n    } else if (UserGroupInformation.isSecurityEnabled()) {\n      HttpServer.LOG.error(\n          \"WebHDFS and security are enabled, but configuration property '\" +\n          DFSConfigKeys.DFS_WEB_AUTHENTICATION_KERBEROS_KEYTAB_KEY +\n          \"' is not set.\");\n    }\n    return params;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.setupServlets": "  private static void setupServlets(HttpServer httpServer, Configuration conf) {\n    httpServer.addInternalServlet(\"startupProgress\",\n        StartupProgressServlet.PATH_SPEC, StartupProgressServlet.class);\n    httpServer.addInternalServlet(\"getDelegationToken\",\n        GetDelegationTokenServlet.PATH_SPEC, \n        GetDelegationTokenServlet.class, true);\n    httpServer.addInternalServlet(\"renewDelegationToken\", \n        RenewDelegationTokenServlet.PATH_SPEC, \n        RenewDelegationTokenServlet.class, true);\n    httpServer.addInternalServlet(\"cancelDelegationToken\", \n        CancelDelegationTokenServlet.PATH_SPEC, \n        CancelDelegationTokenServlet.class, true);\n    httpServer.addInternalServlet(\"fsck\", \"/fsck\", FsckServlet.class,\n        true);\n    httpServer.addInternalServlet(\"getimage\", \"/getimage\",\n        GetImageServlet.class, true);\n    httpServer.addInternalServlet(\"listPaths\", \"/listPaths/*\",\n        ListPathsServlet.class, false);\n    httpServer.addInternalServlet(\"data\", \"/data/*\",\n        FileDataServlet.class, false);\n    httpServer.addInternalServlet(\"checksum\", \"/fileChecksum/*\",\n        FileChecksumServlets.RedirectServlet.class, false);\n    httpServer.addInternalServlet(\"contentSummary\", \"/contentSummary/*\",\n        ContentSummaryServlet.class, false);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer": "  private void startHttpServer(final Configuration conf) throws IOException {\n    httpServer = new NameNodeHttpServer(conf, this, getHttpServerAddress(conf));\n    httpServer.start();\n    httpServer.setStartupProgress(startupProgress);\n    setHttpServerAddress(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.setHttpServerAddress": "  protected void setHttpServerAddress(Configuration conf) {\n    String hostPort = NetUtils.getHostPortString(getHttpAddress());\n    conf.set(DFS_NAMENODE_HTTP_ADDRESS_KEY, hostPort);\n    LOG.info(\"Web-server up at: \" + hostPort);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getHttpServerAddress": "  protected InetSocketAddress getHttpServerAddress(Configuration conf) {\n    return getHttpAddress(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initialize": "  protected void initialize(Configuration conf) throws IOException {\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n\n    if (NamenodeRole.NAMENODE == role) {\n      startHttpServer(conf);\n      validateConfigurationSettingsOrAbort(conf);\n    }\n    loadNamesystem(conf);\n\n    rpcServer = createRpcServer(conf);\n    if (NamenodeRole.NAMENODE == role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    } else {\n      validateConfigurationSettingsOrAbort(conf);\n    }\n    \n    pauseMonitor = new JvmPauseMonitor(conf);\n    pauseMonitor.start();\n\n    startCommonServices(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.validateConfigurationSettingsOrAbort": "  private void validateConfigurationSettingsOrAbort(Configuration conf)\n      throws IOException {\n    try {\n      validateConfigurationSettings(conf);\n    } catch (IOException e) {\n      LOG.fatal(e.toString());\n      throw e;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initMetrics": "  static void initMetrics(Configuration conf, NamenodeRole role) {\n    metrics = NameNodeMetrics.create(conf, role);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getNameNodeAddress": "  public InetSocketAddress getNameNodeAddress() {\n    return rpcServer.getRpcAddress();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer": "  protected NameNodeRpcServer createRpcServer(Configuration conf)\n      throws IOException {\n    return new NameNodeRpcServer(conf, this);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.startCommonServices": "  private void startCommonServices(Configuration conf) throws IOException {\n    namesystem.startCommonServices(conf, haContext);\n    registerNNSMXBean();\n    if (NamenodeRole.NAMENODE != role) {\n      startHttpServer(conf);\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n    rpcServer.start();\n    plugins = conf.getInstances(DFS_NAMENODE_PLUGINS_KEY,\n        ServicePlugin.class);\n    for (ServicePlugin p: plugins) {\n      try {\n        p.start(this);\n      } catch (Throwable t) {\n        LOG.warn(\"ServicePlugin \" + p + \" could not be started\", t);\n      }\n    }\n    LOG.info(getRole() + \" RPC up at: \" + rpcServer.getRpcAddress());\n    if (rpcServer.getServiceRpcAddress() != null) {\n      LOG.info(getRole() + \" service RPC up at: \"\n          + rpcServer.getServiceRpcAddress());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.loginAsNameNodeUser": "  void loginAsNameNodeUser(Configuration conf) throws IOException {\n    InetSocketAddress socAddr = getRpcServerAddress(conf);\n    SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n        DFS_NAMENODE_USER_NAME_KEY, socAddr.getHostName());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getRole": "  public NamenodeRole getRole() {\n    return role;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getFSImage": "  public FSImage getFSImage() {\n    return namesystem.dir.fsImage;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem": "  protected void loadNamesystem(Configuration conf) throws IOException {\n    this.namesystem = FSNamesystem.loadFromDisk(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf == null)\n      conf = new HdfsConfiguration();\n    // Parse out some generic args into Configuration.\n    GenericOptionsParser hParser = new GenericOptionsParser(conf, argv);\n    argv = hParser.getRemainingArgs();\n    // Parse the rest, NN specific args.\n    StartupOption startOpt = parseArguments(argv);\n    if (startOpt == null) {\n      printUsage(System.err);\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n    \n    if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) &&\n        (startOpt == StartupOption.UPGRADE ||\n         startOpt == StartupOption.ROLLBACK ||\n         startOpt == StartupOption.FINALIZE)) {\n      throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n          \"Cannot perform DFS upgrade with HA enabled.\");\n    }\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted = format(conf, startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        terminate(0);\n        return null;\n      }\n      case FINALIZE: {\n        boolean aborted = finalize(conf, true);\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] = Arrays.copyOfRange(argv, 1, argv.length);\n        int rc = BootstrapStandby.run(toolArgs, conf);\n        terminate(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted = initializeSharedEdits(conf,\n            startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role = startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      case RECOVER: {\n        NameNode.doRecovery(startOpt, conf);\n        return null;\n      }\n      default: {\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.setStartupOption": "  private static void setStartupOption(Configuration conf, StartupOption opt) {\n    conf.set(DFS_NAMENODE_STARTUP_KEY, opt.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.doRecovery": "  private static void doRecovery(StartupOption startOpt, Configuration conf)\n      throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    if (startOpt.getForce() < MetaRecoveryContext.FORCE_ALL) {\n      if (!confirmPrompt(\"You have selected Metadata Recovery mode.  \" +\n          \"This mode is intended to recover lost metadata on a corrupt \" +\n          \"filesystem.  Metadata recovery mode often permanently deletes \" +\n          \"data from your HDFS filesystem.  Please back up your edit log \" +\n          \"and fsimage before trying this!\\n\\n\" +\n          \"Are you ready to proceed? (Y/N)\\n\")) {\n        System.err.println(\"Recovery aborted at user request.\\n\");\n        return;\n      }\n    }\n    MetaRecoveryContext.LOG.info(\"starting recovery...\");\n    UserGroupInformation.setConfiguration(conf);\n    NameNode.initMetrics(conf, startOpt.toNodeRole());\n    FSNamesystem fsn = null;\n    try {\n      fsn = FSNamesystem.loadFromDisk(conf);\n      fsn.saveNamespace();\n      MetaRecoveryContext.LOG.info(\"RECOVERY COMPLETE\");\n    } catch (IOException e) {\n      MetaRecoveryContext.LOG.info(\"RECOVERY FAILED: caught exception\", e);\n      throw e;\n    } catch (RuntimeException e) {\n      MetaRecoveryContext.LOG.info(\"RECOVERY FAILED: caught exception\", e);\n      throw e;\n    } finally {\n      if (fsn != null)\n        fsn.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.run": "          public FileSystem run() throws IOException {\n            return FileSystem.get(conf);\n          }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.printUsage": "  private static void printUsage(PrintStream out) {\n    out.println(USAGE + \"\\n\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits": "  private static boolean initializeSharedEdits(Configuration conf,\n      boolean force, boolean interactive) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    \n    if (conf.get(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY) == null) {\n      LOG.fatal(\"No shared edits directory configured for namespace \" +\n          nsId + \" namenode \" + namenodeId);\n      return false;\n    }\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      InetSocketAddress socAddr = getAddress(conf);\n      SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n          DFS_NAMENODE_USER_NAME_KEY, socAddr.getHostName());\n    }\n\n    NNStorage existingStorage = null;\n    try {\n      FSNamesystem fsns =\n          FSNamesystem.loadFromDisk(getConfigurationWithoutSharedEdits(conf));\n      \n      existingStorage = fsns.getFSImage().getStorage();\n      NamespaceInfo nsInfo = existingStorage.getNamespaceInfo();\n      \n      List<URI> sharedEditsDirs = FSNamesystem.getSharedEditsDirs(conf);\n      \n      FSImage sharedEditsImage = new FSImage(conf,\n          Lists.<URI>newArrayList(),\n          sharedEditsDirs);\n      sharedEditsImage.getEditLog().initJournalsForWrite();\n      \n      if (!sharedEditsImage.confirmFormat(force, interactive)) {\n        return true; // abort\n      }\n      \n      NNStorage newSharedStorage = sharedEditsImage.getStorage();\n      // Call Storage.format instead of FSImage.format here, since we don't\n      // actually want to save a checkpoint - just prime the dirs with\n      // the existing namespace info\n      newSharedStorage.format(nsInfo);\n      sharedEditsImage.getEditLog().formatNonFileJournals(nsInfo);\n\n      // Need to make sure the edit log segments are in good shape to initialize\n      // the shared edits dir.\n      fsns.getFSImage().getEditLog().close();\n      fsns.getFSImage().getEditLog().initJournalsForWrite();\n      fsns.getFSImage().getEditLog().recoverUnclosedStreams();\n\n      copyEditLogSegmentsToSharedDir(fsns, sharedEditsDirs, newSharedStorage,\n          conf);\n    } catch (IOException ioe) {\n      LOG.error(\"Could not initialize shared edits dir\", ioe);\n      return true; // aborted\n    } finally {\n      // Have to unlock storage explicitly for the case when we're running in a\n      // unit test, which runs in the same JVM as NNs.\n      if (existingStorage != null) {\n        try {\n          existingStorage.unlockAll();\n        } catch (IOException ioe) {\n          LOG.warn(\"Could not unlock storage directories\", ioe);\n          return true; // aborted\n        }\n      }\n    }\n    return false; // did not abort\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.finalize": "  private static boolean finalize(Configuration conf,\n                               boolean isConfirmationNeeded\n                               ) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n\n    FSNamesystem nsys = new FSNamesystem(conf, new FSImage(conf));\n    System.err.print(\n        \"\\\"finalize\\\" will remove the previous state of the files system.\\n\"\n        + \"Recent upgrade will become permanent.\\n\"\n        + \"Rollback option will not be available anymore.\\n\");\n    if (isConfirmationNeeded) {\n      if (!confirmPrompt(\"Finalize filesystem state?\")) {\n        System.err.println(\"Finalize aborted.\");\n        return true;\n      }\n    }\n    nsys.dir.fsImage.finalizeUpgrade();\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.format": "  private static boolean format(Configuration conf, boolean force,\n      boolean isInteractive) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    checkAllowFormat(conf);\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      InetSocketAddress socAddr = getAddress(conf);\n      SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n          DFS_NAMENODE_USER_NAME_KEY, socAddr.getHostName());\n    }\n    \n    Collection<URI> nameDirsToFormat = FSNamesystem.getNamespaceDirs(conf);\n    List<URI> sharedDirs = FSNamesystem.getSharedEditsDirs(conf);\n    List<URI> dirsToPrompt = new ArrayList<URI>();\n    dirsToPrompt.addAll(nameDirsToFormat);\n    dirsToPrompt.addAll(sharedDirs);\n    List<URI> editDirsToFormat = \n                 FSNamesystem.getNamespaceEditsDirs(conf);\n\n    // if clusterID is not provided - see if you can find the current one\n    String clusterId = StartupOption.FORMAT.getClusterId();\n    if(clusterId == null || clusterId.equals(\"\")) {\n      //Generate a new cluster id\n      clusterId = NNStorage.newClusterID();\n    }\n    System.out.println(\"Formatting using clusterid: \" + clusterId);\n    \n    FSImage fsImage = new FSImage(conf, nameDirsToFormat, editDirsToFormat);\n    FSNamesystem fsn = new FSNamesystem(conf, fsImage);\n    fsImage.getEditLog().initJournalsForWrite();\n    \n    if (!fsImage.confirmFormat(force, isInteractive)) {\n      return true; // aborted\n    }\n    \n    fsImage.format(fsn, clusterId);\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.parseArguments": "  private static StartupOption parseArguments(String args[]) {\n    int argsLen = (args == null) ? 0 : args.length;\n    StartupOption startOpt = StartupOption.REGULAR;\n    for(int i=0; i < argsLen; i++) {\n      String cmd = args[i];\n      if (StartupOption.FORMAT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.FORMAT;\n        for (i = i + 1; i < argsLen; i++) {\n          if (args[i].equalsIgnoreCase(StartupOption.CLUSTERID.getName())) {\n            i++;\n            if (i >= argsLen) {\n              // if no cluster id specified, return null\n              LOG.fatal(\"Must specify a valid cluster ID after the \"\n                  + StartupOption.CLUSTERID.getName() + \" flag\");\n              return null;\n            }\n            String clusterId = args[i];\n            // Make sure an id is specified and not another flag\n            if (clusterId.isEmpty() ||\n                clusterId.equalsIgnoreCase(StartupOption.FORCE.getName()) ||\n                clusterId.equalsIgnoreCase(\n                    StartupOption.NONINTERACTIVE.getName())) {\n              LOG.fatal(\"Must specify a valid cluster ID after the \"\n                  + StartupOption.CLUSTERID.getName() + \" flag\");\n              return null;\n            }\n            startOpt.setClusterId(clusterId);\n          }\n\n          if (args[i].equalsIgnoreCase(StartupOption.FORCE.getName())) {\n            startOpt.setForceFormat(true);\n          }\n\n          if (args[i].equalsIgnoreCase(StartupOption.NONINTERACTIVE.getName())) {\n            startOpt.setInteractiveFormat(false);\n          }\n        }\n      } else if (StartupOption.GENCLUSTERID.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.GENCLUSTERID;\n      } else if (StartupOption.REGULAR.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.REGULAR;\n      } else if (StartupOption.BACKUP.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.BACKUP;\n      } else if (StartupOption.CHECKPOINT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.CHECKPOINT;\n      } else if (StartupOption.UPGRADE.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.UPGRADE;\n        // might be followed by two args\n        if (i + 2 < argsLen\n            && args[i + 1].equalsIgnoreCase(StartupOption.CLUSTERID.getName())) {\n          i += 2;\n          startOpt.setClusterId(args[i]);\n        }\n      } else if (StartupOption.ROLLBACK.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.ROLLBACK;\n      } else if (StartupOption.FINALIZE.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.FINALIZE;\n      } else if (StartupOption.IMPORT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.IMPORT;\n      } else if (StartupOption.BOOTSTRAPSTANDBY.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.BOOTSTRAPSTANDBY;\n        return startOpt;\n      } else if (StartupOption.INITIALIZESHAREDEDITS.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.INITIALIZESHAREDEDITS;\n        for (i = i + 1 ; i < argsLen; i++) {\n          if (StartupOption.NONINTERACTIVE.getName().equals(args[i])) {\n            startOpt.setInteractiveFormat(false);\n          } else if (StartupOption.FORCE.getName().equals(args[i])) {\n            startOpt.setForceFormat(true);\n          } else {\n            LOG.fatal(\"Invalid argument: \" + args[i]);\n            return null;\n          }\n        }\n        return startOpt;\n      } else if (StartupOption.RECOVER.getName().equalsIgnoreCase(cmd)) {\n        if (startOpt != StartupOption.REGULAR) {\n          throw new RuntimeException(\"Can't combine -recover with \" +\n              \"other startup options.\");\n        }\n        startOpt = StartupOption.RECOVER;\n        while (++i < argsLen) {\n          if (args[i].equalsIgnoreCase(\n                StartupOption.FORCE.getName())) {\n            startOpt.setForce(MetaRecoveryContext.FORCE_FIRST_CHOICE);\n          } else {\n            throw new RuntimeException(\"Error parsing recovery options: \" + \n              \"can't understand option \\\"\" + args[i] + \"\\\"\");\n          }\n        }\n      } else {\n        return null;\n      }\n    }\n    return startOpt;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.main": "  public static void main(String argv[]) throws Exception {\n    if (DFSUtil.parseHelpArgument(argv, NameNode.USAGE, System.out, true)) {\n      System.exit(0);\n    }\n\n    try {\n      StringUtils.startupShutdownMessage(NameNode.class, argv, LOG);\n      NameNode namenode = createNameNode(argv, null);\n      if (namenode != null) {\n        namenode.join();\n      }\n    } catch (Throwable e) {\n      LOG.fatal(\"Exception in namenode join\", e);\n      terminate(1, e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.join": "  public void join() {\n    try {\n      rpcServer.join();\n    } catch (InterruptedException ie) {\n      LOG.info(\"Caught interrupted exception \", ie);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init": "  public void init(SSLFactory.Mode mode)\n    throws IOException, GeneralSecurityException {\n\n    boolean requireClientCert =\n      conf.getBoolean(SSLFactory.SSL_REQUIRE_CLIENT_CERT_KEY, true);\n\n    // certificate store\n    String keystoreType =\n      conf.get(resolvePropertyName(mode, SSL_KEYSTORE_TYPE_TPL_KEY),\n               DEFAULT_KEYSTORE_TYPE);\n    KeyStore keystore = KeyStore.getInstance(keystoreType);\n    String keystorePassword = null;\n    if (requireClientCert || mode == SSLFactory.Mode.SERVER) {\n      String locationProperty =\n        resolvePropertyName(mode, SSL_KEYSTORE_LOCATION_TPL_KEY);\n      String keystoreLocation = conf.get(locationProperty, \"\");\n      if (keystoreLocation.isEmpty()) {\n        throw new GeneralSecurityException(\"The property '\" + locationProperty +\n          \"' has not been set in the ssl configuration file.\");\n      }\n      String passwordProperty =\n        resolvePropertyName(mode, SSL_KEYSTORE_PASSWORD_TPL_KEY);\n      keystorePassword = conf.get(passwordProperty, \"\");\n      if (keystorePassword.isEmpty()) {\n        throw new GeneralSecurityException(\"The property '\" + passwordProperty +\n          \"' has not been set in the ssl configuration file.\");\n      }\n      LOG.debug(mode.toString() + \" KeyStore: \" + keystoreLocation);\n\n      InputStream is = new FileInputStream(keystoreLocation);\n      try {\n        keystore.load(is, keystorePassword.toCharArray());\n      } finally {\n        is.close();\n      }\n      LOG.debug(mode.toString() + \" Loaded KeyStore: \" + keystoreLocation);\n    } else {\n      keystore.load(null, null);\n    }\n    KeyManagerFactory keyMgrFactory = KeyManagerFactory\n        .getInstance(SSLFactory.SSLCERTIFICATE);\n      \n    keyMgrFactory.init(keystore, (keystorePassword != null) ?\n                                 keystorePassword.toCharArray() : null);\n    keyManagers = keyMgrFactory.getKeyManagers();\n\n    //trust store\n    String truststoreType =\n      conf.get(resolvePropertyName(mode, SSL_TRUSTSTORE_TYPE_TPL_KEY),\n               DEFAULT_KEYSTORE_TYPE);\n\n    String locationProperty =\n      resolvePropertyName(mode, SSL_TRUSTSTORE_LOCATION_TPL_KEY);\n    String truststoreLocation = conf.get(locationProperty, \"\");\n    if (truststoreLocation.isEmpty()) {\n      throw new GeneralSecurityException(\"The property '\" + locationProperty +\n        \"' has not been set in the ssl configuration file.\");\n    }\n\n    String passwordProperty = resolvePropertyName(mode,\n                                                  SSL_TRUSTSTORE_PASSWORD_TPL_KEY);\n    String truststorePassword = conf.get(passwordProperty, \"\");\n    if (truststorePassword.isEmpty()) {\n      throw new GeneralSecurityException(\"The property '\" + passwordProperty +\n        \"' has not been set in the ssl configuration file.\");\n    }\n    long truststoreReloadInterval =\n      conf.getLong(\n        resolvePropertyName(mode, SSL_TRUSTSTORE_RELOAD_INTERVAL_TPL_KEY),\n        DEFAULT_SSL_TRUSTSTORE_RELOAD_INTERVAL);\n\n    LOG.debug(mode.toString() + \" TrustStore: \" + truststoreLocation);\n\n    trustManager = new ReloadingX509TrustManager(truststoreType,\n                                                 truststoreLocation,\n                                                 truststorePassword,\n                                                 truststoreReloadInterval);\n    trustManager.init();\n    LOG.debug(mode.toString() + \" Loaded TrustStore: \" + truststoreLocation);\n\n    trustManagers = new TrustManager[]{trustManager};\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.resolvePropertyName": "  public static String resolvePropertyName(SSLFactory.Mode mode,\n                                           String template) {\n    return MessageFormat.format(template, mode.toString().toLowerCase());\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.getKeyManagers": "  public KeyManager[] getKeyManagers() {\n    return keyManagers;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ssl.SSLFactory.init": "  public void init() throws GeneralSecurityException, IOException {\n    keystoresFactory.init(mode);\n    context = SSLContext.getInstance(\"TLS\");\n    context.init(keystoresFactory.getKeyManagers(),\n                 keystoresFactory.getTrustManagers(), null);\n\n    hostnameVerifier = getHostnameVerifier(conf);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ssl.SSLFactory.getHostnameVerifier": "  public HostnameVerifier getHostnameVerifier() {\n    if (mode != Mode.CLIENT) {\n      throw new IllegalStateException(\"Factory is in CLIENT mode\");\n    }\n    return hostnameVerifier;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.newClusterID": "  public static String newClusterID() {\n    return \"CID-\" + UUID.randomUUID().toString();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ssl.keystoresFactory.getTrustManagers": "  public TrustManager[] getTrustManagers();\n\n}",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ssl.keystoresFactory.getKeyManagers": "  public KeyManager[] getKeyManagers();\n\n  /**\n   * Returns the trustmanagers for trusted certificates.\n   *\n   * @return the trustmanagers for trusted certificates.\n   */\n  public TrustManager[] getTrustManagers();\n\n}"
        },
        "bug_report": {
            "Title": "Hadoop services won't start with different keypass and keystorepass when https is enabled",
            "Description": "Enable ssl in the configuration. While creating keystore, give different keypass and keystore password. (here, keypass = hadoop and storepass=hadoopKey)\n\nkeytool -genkey -alias host1 -keyalg RSA -keysize 1024 -dname \"CN=host1,OU=cm,O=cm,L=san jose,ST=ca,C=us\" -keypass hadoop -keystore keystore.jks -storepass hadoopKey\n\nIn , ssl-server.xml set below two properties.\n<property><name>ssl.server.keystore.keypassword</name><value>hadoop</value></property>\n<property><name>ssl.server.keystore.password</name><value>hadoopKey</value></property>\n\nNamenode, ResourceManager, Datanode, Nodemanager, SecondaryNamenode fails to start with below error.\n\n2013-09-17 21:39:00,794 FATAL namenode.NameNode (NameNode.java:main(1325)) - Exception in namenode join\njava.io.IOException: java.security.UnrecoverableKeyException: Cannot recover key\n        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:222)\n        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:174)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer$1.<init>(NameNodeHttpServer.java:76)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:74)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:626)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:488)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:684)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:669)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1254)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1320)\nCaused by: java.security.UnrecoverableKeyException: Cannot recover key\n        at sun.security.provider.KeyProtector.recover(KeyProtector.java:328)\n        at sun.security.provider.JavaKeyStore.engineGetKey(JavaKeyStore.java:138)\n        at sun.security.provider.JavaKeyStore$JKS.engineGetKey(JavaKeyStore.java:55)\n        at java.security.KeyStore.getKey(KeyStore.java:792)\n        at sun.security.ssl.SunX509KeyManagerImpl.<init>(SunX509KeyManagerImpl.java:131)\n        at sun.security.ssl.KeyManagerFactoryImpl$SunX509.engineInit(KeyManagerFactoryImpl.java:68)\n        at javax.net.ssl.KeyManagerFactory.init(KeyManagerFactory.java:259)\n        at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:170)\n        at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:121)\n        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:220)\n        ... 9 more"
        }
    },
    {
        "filename": "HADOOP-12611.json",
        "creation_time": "2015-12-01T17:14:38.000+0000",
        "stack_trace": "```\njava.lang.AssertionError: expected null, but was:<[B@142bad79>\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.failNotNull(Assert.java:664)\n\tat org.junit.Assert.assertNull(Assert.java:646)\n\tat org.junit.Assert.assertNull(Assert.java:656)\n\tat org.apache.hadoop.security.authentication.util.TestZKSignerSecretProvider.testMultipleInit(TestZKSignerSecretProvider.java:149)\n\njava.lang.IllegalStateException: instance must be started before calling this method\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:145)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.getData(CuratorFrameworkImpl.java:363)\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.pullFromZK(ZKSignerSecretProvider.java:341)\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.rollSecret(ZKSignerSecretProvider.java:264)\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider$$EnhancerByMockitoWithCGLIB$$575f06d8.CGLIB$rollSecret$2(<generated>)\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider$$EnhancerByMockitoWithCGLIB$$575f06d8$$FastClassByMockitoWithCGLIB$$6f94a716.invoke(<generated>)\n\tat org.mockito.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:216)\n\tat org.mockito.internal.creation.AbstractMockitoMethodProxy.invokeSuper(AbstractMockitoMethodProxy.java:10)\n\tat org.mockito.internal.invocation.realmethod.CGLIBProxyRealMethod.invoke(CGLIBProxyRealMethod.java:22)\n\tat org.mockito.internal.invocation.realmethod.FilteredCGLIBProxyRealMethod.invoke(FilteredCGLIBProxyRealMethod.java:27)\n\tat org.mockito.internal.invocation.Invocation.callRealMethod(Invocation.java:211)\n\tat org.mockito.internal.stubbing.answers.CallsRealMethods.answer(CallsRealMethods.java:36)\n\tat org.mockito.internal.MockHandler.handle(MockHandler.java:99)\n\tat org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:47)\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider$$EnhancerByMockitoWithCGLIB$$575f06d8.rollSecret(<generated>)\n\tat org.apache.hadoop.security.authentication.util.RolloverSignerSecretProvider$1.run(RolloverSignerSecretProvider.java:97)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.pullFromZK": "  private synchronized void pullFromZK(boolean isInit) {\n    try {\n      Stat stat = new Stat();\n      byte[] bytes = client.getData().storingStatIn(stat).forPath(path);\n      ByteBuffer bb = ByteBuffer.wrap(bytes);\n      int dataVersion = bb.getInt();\n      if (dataVersion > DATA_VERSION) {\n        throw new IllegalStateException(\"Cannot load data from ZooKeeper; it\"\n                + \"was written with a newer version\");\n      }\n      int nextSecretLength = bb.getInt();\n      byte[] nextSecret = new byte[nextSecretLength];\n      bb.get(nextSecret);\n      this.nextSecret = nextSecret;\n      zkVersion = stat.getVersion();\n      if (isInit) {\n        int currentSecretLength = bb.getInt();\n        byte[] currentSecret = new byte[currentSecretLength];\n        bb.get(currentSecret);\n        int previousSecretLength = bb.getInt();\n        byte[] previousSecret = null;\n        if (previousSecretLength > 0) {\n          previousSecret = new byte[previousSecretLength];\n          bb.get(previousSecret);\n        }\n        super.initSecrets(currentSecret, previousSecret);\n        nextRolloverDate = bb.getLong();\n      }\n    } catch (Exception ex) {\n      LOG.error(\"An unexpected exception occurred while pulling data from\"\n              + \"ZooKeeper\", ex);\n    }\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.rollSecret": "  protected synchronized void rollSecret() {\n    super.rollSecret();\n    // Try to push the information to ZooKeeper with a potential next secret.\n    nextRolloverDate += tokenValidity;\n    byte[][] secrets = super.getAllSecrets();\n    pushToZK(generateRandomSecret(), secrets[0], secrets[1]);\n    // Pull info from ZooKeeper to get the decided next secret\n    // passing false tells it that we don't care about most of the data\n    pullFromZK(false);\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.generateRandomSecret": "  private byte[] generateRandomSecret() {\n    return Long.toString(rand.nextLong()).getBytes(Charset.forName(\"UTF-8\"));\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.pushToZK": "  private synchronized void pushToZK(byte[] newSecret, byte[] currentSecret,\n          byte[] previousSecret) {\n    byte[] bytes = generateZKData(newSecret, currentSecret, previousSecret);\n    try {\n      client.setData().withVersion(zkVersion).forPath(path, bytes);\n    } catch (KeeperException.BadVersionException bve) {\n      LOG.debug(\"Unable to push to znode; another server already did it\");\n    } catch (Exception ex) {\n      LOG.error(\"An unexpected exception occured pushing data to ZooKeeper\",\n              ex);\n    }\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.util.RolloverSignerSecretProvider.run": "        public void run() {\n          rollSecret();\n        }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.util.RolloverSignerSecretProvider.rollSecret": "  protected synchronized void rollSecret() {\n    if (!isDestroyed) {\n      LOG.debug(\"rolling secret\");\n      byte[] newSecret = generateNewSecret();\n      secrets = new byte[][]{newSecret, secrets[0]};\n    }\n  }"
        },
        "bug_report": {
            "Title": "TestZKSignerSecretProvider#testMultipleInit occasionally fail",
            "Description": "https://builds.apache.org/job/Hadoop-Common-trunk/2053/testReport/junit/org.apache.hadoop.security.authentication.util/TestZKSignerSecretProvider/testMultipleInit/\r\n\r\nError Message\r\n\r\nexpected null, but was:<[B@142bad79>\r\n\r\nStacktrace\r\n\r\njava.lang.AssertionError: expected null, but was:<[B@142bad79>\r\n\tat org.junit.Assert.fail(Assert.java:88)\r\n\tat org.junit.Assert.failNotNull(Assert.java:664)\r\n\tat org.junit.Assert.assertNull(Assert.java:646)\r\n\tat org.junit.Assert.assertNull(Assert.java:656)\r\n\tat org.apache.hadoop.security.authentication.util.TestZKSignerSecretProvider.testMultipleInit(TestZKSignerSecretProvider.java:149)\r\n\r\n\r\nI think the failure was introduced after HADOOP-12181\r\n\r\nThis is likely where the root cause is:\r\n\r\n2015-11-29 00:24:33,325 ERROR ZKSignerSecretProvider - An unexpected exception occurred while pulling data fromZooKeeper\r\njava.lang.IllegalStateException: instance must be started before calling this method\r\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:145)\r\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.getData(CuratorFrameworkImpl.java:363)\r\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.pullFromZK(ZKSignerSecretProvider.java:341)\r\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.rollSecret(ZKSignerSecretProvider.java:264)\r\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider$$EnhancerByMockitoWithCGLIB$$575f06d8.CGLIB$rollSecret$2(<generated>)\r\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider$$EnhancerByMockitoWithCGLIB$$575f06d8$$FastClassByMockitoWithCGLIB$$6f94a716.invoke(<generated>)\r\n\tat org.mockito.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:216)\r\n\tat org.mockito.internal.creation.AbstractMockitoMethodProxy.invokeSuper(AbstractMockitoMethodProxy.java:10)\r\n\tat org.mockito.internal.invocation.realmethod.CGLIBProxyRealMethod.invoke(CGLIBProxyRealMethod.java:22)\r\n\tat org.mockito.internal.invocation.realmethod.FilteredCGLIBProxyRealMethod.invoke(FilteredCGLIBProxyRealMethod.java:27)\r\n\tat org.mockito.internal.invocation.Invocation.callRealMethod(Invocation.java:211)\r\n\tat org.mockito.internal.stubbing.answers.CallsRealMethods.answer(CallsRealMethods.java:36)\r\n\tat org.mockito.internal.MockHandler.handle(MockHandler.java:99)\r\n\tat org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:47)\r\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider$$EnhancerByMockitoWithCGLIB$$575f06d8.rollSecret(<generated>)\r\n\tat org.apache.hadoop.security.authentication.util.RolloverSignerSecretProvider$1.run(RolloverSignerSecretProvider.java:97)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\r\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "HADOOP-10142.json",
        "creation_time": "2013-12-03T06:33:32.000+0000",
        "stack_trace": "```\norg.apache.hadoop.util.Shell$ExitCodeException: id: dr.who: No such user\n\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:504)\n        at org.apache.hadoop.util.Shell.run(Shell.java:417)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:636)\n        at org.apache.hadoop.util.Shell.execCommand(Shell.java:725)\n        at org.apache.hadoop.util.Shell.execCommand(Shell.java:708)\n        at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)\n        at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52)\n        at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)\n        at org.apache.hadoop.security.Groups.getGroups(Groups.java:95)\n        at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1376)\n        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:63)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker(FSNamesystem.java:3228)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListingInt(FSNamesystem.java:4063)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListing(FSNamesystem.java:4052)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getListing(NameNodeRpcServer.java:748)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getDirectoryListing(NamenodeWebHdfsMethods.java:715)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getListingStream(NamenodeWebHdfsMethods.java:727)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get(NamenodeWebHdfsMethods.java:675)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.access$400(NamenodeWebHdfsMethods.java:114)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3.run(NamenodeWebHdfsMethods.java:623)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3.run(NamenodeWebHdfsMethods.java:618)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1515)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get(NamenodeWebHdfsMethods.java:618)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getRoot(NamenodeWebHdfsMethods.java:586)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:384)\n        at org.apache.hadoop.hdfs.web.AuthFilter.doFilter(AuthFilter.java:85)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1310)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.runCommand": "  private void runCommand() throws IOException { \n    ProcessBuilder builder = new ProcessBuilder(getExecString());\n    Timer timeOutTimer = null;\n    ShellTimeoutTimerTask timeoutTimerTask = null;\n    timedOut = new AtomicBoolean(false);\n    completed = new AtomicBoolean(false);\n    \n    if (environment != null) {\n      builder.environment().putAll(this.environment);\n    }\n    if (dir != null) {\n      builder.directory(this.dir);\n    }\n\n    builder.redirectErrorStream(redirectErrorStream);\n    \n    if (Shell.WINDOWS) {\n      synchronized (WindowsProcessLaunchLock) {\n        // To workaround the race condition issue with child processes\n        // inheriting unintended handles during process launch that can\n        // lead to hangs on reading output and error streams, we\n        // serialize process creation. More info available at:\n        // http://support.microsoft.com/kb/315939\n        process = builder.start();\n      }\n    } else {\n      process = builder.start();\n    }\n\n    if (timeOutInterval > 0) {\n      timeOutTimer = new Timer(\"Shell command timeout\");\n      timeoutTimerTask = new ShellTimeoutTimerTask(\n          this);\n      //One time scheduling.\n      timeOutTimer.schedule(timeoutTimerTask, timeOutInterval);\n    }\n    final BufferedReader errReader = \n            new BufferedReader(new InputStreamReader(process\n                                                     .getErrorStream()));\n    BufferedReader inReader = \n            new BufferedReader(new InputStreamReader(process\n                                                     .getInputStream()));\n    final StringBuffer errMsg = new StringBuffer();\n    \n    // read error and input streams as this would free up the buffers\n    // free the error stream buffer\n    Thread errThread = new Thread() {\n      @Override\n      public void run() {\n        try {\n          String line = errReader.readLine();\n          while((line != null) && !isInterrupted()) {\n            errMsg.append(line);\n            errMsg.append(System.getProperty(\"line.separator\"));\n            line = errReader.readLine();\n          }\n        } catch(IOException ioe) {\n          LOG.warn(\"Error reading the error stream\", ioe);\n        }\n      }\n    };\n    try {\n      errThread.start();\n    } catch (IllegalStateException ise) { }\n    try {\n      parseExecResult(inReader); // parse the output\n      // clear the input stream buffer\n      String line = inReader.readLine();\n      while(line != null) { \n        line = inReader.readLine();\n      }\n      // wait for the process to finish and check the exit code\n      exitCode  = process.waitFor();\n      try {\n        // make sure that the error thread exits\n        errThread.join();\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Interrupted while reading the error stream\", ie);\n      }\n      completed.set(true);\n      //the timeout thread handling\n      //taken care in finally block\n      if (exitCode != 0) {\n        throw new ExitCodeException(exitCode, errMsg.toString());\n      }\n    } catch (InterruptedException ie) {\n      throw new IOException(ie.toString());\n    } finally {\n      if (timeOutTimer != null) {\n        timeOutTimer.cancel();\n      }\n      // close the input stream\n      try {\n        inReader.close();\n      } catch (IOException ioe) {\n        LOG.warn(\"Error while closing the input stream\", ioe);\n      }\n      try {\n        if (!completed.get()) {\n          errThread.interrupt();\n          errThread.join();\n        }\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Interrupted while joining errThread\");\n      }\n      try {\n        errReader.close();\n      } catch (IOException ioe) {\n        LOG.warn(\"Error while closing the error stream\", ioe);\n      }\n      process.destroy();\n      lastTime = Time.now();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.toString": "    public String toString() {\n      StringBuilder builder = new StringBuilder();\n      String[] args = getExecString();\n      for (String s : args) {\n        if (s.indexOf(' ') >= 0) {\n          builder.append('\"').append(s).append('\"');\n        } else {\n          builder.append(s);\n        }\n        builder.append(' ');\n      }\n      return builder.toString();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.parseExecResult": "    protected void parseExecResult(BufferedReader lines) throws IOException {\n      output = new StringBuffer();\n      char[] buf = new char[512];\n      int nRead;\n      while ( (nRead = lines.read(buf, 0, buf.length)) > 0 ) {\n        output.append(buf, 0, nRead);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.getExecString": "    public String[] getExecString() {\n      return command;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.run": "    public void run() {\n      Process p = shell.getProcess();\n      try {\n        p.exitValue();\n      } catch (Exception e) {\n        //Process has not terminated.\n        //So check if it has completed \n        //if not just destroy it.\n        if (p != null && !shell.completed.get()) {\n          shell.setTimedOut();\n          p.destroy();\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.setTimedOut": "  private void setTimedOut() {\n    this.timedOut.set(true);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.getProcess": "  public Process getProcess() {\n    return process;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.execute": "    public void execute() throws IOException {\n      this.run();    \n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.execCommand": "  public static String execCommand(Map<String,String> env, String ... cmd) \n  throws IOException {\n    return execCommand(env, cmd, 0L);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.getOutput": "    public String getOutput() {\n      return (output == null) ? \"\" : output.toString();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups": "  private static List<String> getUnixGroups(final String user) throws IOException {\n    String result = \"\";\n    try {\n      result = Shell.execCommand(Shell.getGroupsForUserCommand(user));\n    } catch (ExitCodeException e) {\n      // if we didn't get the group - just return empty list;\n      LOG.warn(\"got exception trying to get groups for user \" + user, e);\n    }\n    \n    StringTokenizer tokenizer =\n        new StringTokenizer(result, Shell.TOKEN_SEPARATOR_REGEX);\n    List<String> groups = new LinkedList<String>();\n    while (tokenizer.hasMoreTokens()) {\n      groups.add(tokenizer.nextToken());\n    }\n    return groups;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups": "  public List<String> getGroups(String user) throws IOException {\n    return getUnixGroups(user);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups": "  public List<String> getGroups(String user) throws IOException {\n    return impl.getGroups(user);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.Groups.getGroups": "    public List<String> getGroups() {\n      return groups;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.Groups.getTimestamp": "    public long getTimestamp() {\n      return timestamp;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getGroupNames": "  public synchronized String[] getGroupNames() {\n    ensureInitialized();\n    try {\n      List<String> result = groups.getGroups(getShortUserName());\n      return result.toArray(new String[result.size()]);\n    } catch (IOException ie) {\n      LOG.warn(\"No groups available for user \" + getShortUserName());\n      return new String[0];\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getShortUserName": "  public String getShortUserName() {\n    for (User p: subject.getPrincipals(User.class)) {\n      return p.getShortName();\n    }\n    return null;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.ensureInitialized": "  private static void ensureInitialized() {\n    if (conf == null) {\n      synchronized(UserGroupInformation.class) {\n        if (conf == null) { // someone might have beat us\n          initialize(new Configuration(), false);\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getGroups": "    public List<String> getGroups(String user) throws IOException {\n      List<String> result = userToGroupsMapping.get(user);\n      \n      if (result == null) {\n        result = underlyingImplementation.getGroups(user);\n      }\n\n      return result;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker": "  private FSPermissionChecker getPermissionChecker()\n      throws AccessControlException {\n    try {\n      return new FSPermissionChecker(fsOwnerShortUserName, supergroup, getRemoteUser());\n    } catch (IOException ioe) {\n      throw new AccessControlException(ioe);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getRemoteUser": "  private static UserGroupInformation getRemoteUser() throws IOException {\n    return NameNode.getRemoteUser();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListingInt": "  private DirectoryListing getListingInt(String src, byte[] startAfter,\n      boolean needLocation) \n    throws AccessControlException, UnresolvedLinkException, IOException {\n    DirectoryListing dl;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.READ);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    String startAfterString = new String(startAfter);\n    readLock();\n    try {\n      checkOperation(OperationCategory.READ);\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n\n      // Get file name when startAfter is an INodePath\n      if (FSDirectory.isReservedName(startAfterString)) {\n        byte[][] startAfterComponents = FSDirectory\n            .getPathComponentsForReservedPath(startAfterString);\n        try {\n          String tmp = FSDirectory.resolvePath(src, startAfterComponents, dir);\n          byte[][] regularPath = INode.getPathComponents(tmp);\n          startAfter = regularPath[regularPath.length - 1];\n        } catch (IOException e) {\n          // Possibly the inode is deleted\n          throw new DirectoryListingStartAfterNotFoundException(\n              \"Can't find startAfter \" + startAfterString);\n        }\n      }\n      \n      if (isPermissionEnabled) {\n        if (dir.isDir(src)) {\n          checkPathAccess(pc, src, FsAction.READ_EXECUTE);\n        } else {\n          checkTraverse(pc, src);\n        }\n      }\n      logAuditEvent(true, \"listStatus\", src);\n      dl = dir.getListing(src, startAfter, needLocation);\n    } finally {\n      readUnlock();\n    }\n    return dl;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.readUnlock": "  public void readUnlock() {\n    this.fsLock.readLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.readLock": "  public void readLock() {\n    this.fsLock.readLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPathAccess": "  private void checkPathAccess(FSPermissionChecker pc,\n      String path, FsAction access) throws AccessControlException,\n      UnresolvedLinkException {\n    checkPermission(pc, path, false, null, null, access, null);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkTraverse": "  private void checkTraverse(FSPermissionChecker pc, String path)\n      throws AccessControlException, UnresolvedLinkException {\n    checkPermission(pc, path, false, null, null, null, null);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListing": "  DirectoryListing getListing(String src, byte[] startAfter,\n      boolean needLocation) \n      throws AccessControlException, UnresolvedLinkException, IOException {\n    try {\n      return getListingInt(src, startAfter, needLocation);\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"listStatus\", src);\n      throw e;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation": "  public void checkOperation(OperationCategory op) throws StandbyException {\n    if (haContext != null) {\n      // null in some unit tests\n      haContext.checkOperation(op);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logAuditEvent": "    public void logAuditEvent(boolean succeeded, String userName,\n        InetAddress addr, String cmd, String src, String dst,\n        FileStatus status, UserGroupInformation ugi,\n        DelegationTokenSecretManager dtSecretManager) {\n      if (auditLog.isInfoEnabled()) {\n        final StringBuilder sb = auditBuffer.get();\n        sb.setLength(0);\n        sb.append(\"allowed=\").append(succeeded).append(\"\\t\");\n        sb.append(\"ugi=\").append(userName).append(\"\\t\");\n        sb.append(\"ip=\").append(addr).append(\"\\t\");\n        sb.append(\"cmd=\").append(cmd).append(\"\\t\");\n        sb.append(\"src=\").append(src).append(\"\\t\");\n        sb.append(\"dst=\").append(dst).append(\"\\t\");\n        if (null == status) {\n          sb.append(\"perm=null\");\n        } else {\n          sb.append(\"perm=\");\n          sb.append(status.getOwner()).append(\":\");\n          sb.append(status.getGroup()).append(\":\");\n          sb.append(status.getPermission());\n        }\n        if (logTokenTrackingId) {\n          sb.append(\"\\t\").append(\"trackingId=\");\n          String trackingId = null;\n          if (ugi != null && dtSecretManager != null\n              && ugi.getAuthenticationMethod() == AuthenticationMethod.TOKEN) {\n            for (TokenIdentifier tid: ugi.getTokenIdentifiers()) {\n              if (tid instanceof DelegationTokenIdentifier) {\n                DelegationTokenIdentifier dtid =\n                    (DelegationTokenIdentifier)tid;\n                trackingId = dtSecretManager.getTokenTrackingId(dtid);\n                break;\n              }\n            }\n          }\n          sb.append(trackingId);\n        }\n        logAuditMessage(sb.toString());\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getListing": "  public DirectoryListing getListing(String src, byte[] startAfter,\n      boolean needLocation) throws IOException {\n    DirectoryListing files = namesystem.getListing(\n        src, startAfter, needLocation);\n    if (files != null) {\n      metrics.incrGetListingOps();\n      metrics.incrFilesInGetListingOps(files.getPartialListing().length);\n    }\n    return files;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getDirectoryListing": "  private static DirectoryListing getDirectoryListing(final NamenodeProtocols np,\n      final String p, byte[] startAfter) throws IOException {\n    final DirectoryListing listing = np.getListing(p, startAfter, false);\n    if (listing == null) { // the directory does not exist\n      throw new FileNotFoundException(\"File \" + p + \" does not exist.\");\n    }\n    return listing;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getListingStream": "  private static StreamingOutput getListingStream(final NamenodeProtocols np, \n      final String p) throws IOException {\n    // allows exceptions like FNF or ACE to prevent http response of 200 for\n    // a failure since we can't (currently) return error responses in the\n    // middle of a streaming operation\n    final DirectoryListing firstDirList = getDirectoryListing(np, p,\n        HdfsFileStatus.EMPTY_NAME);\n\n    // must save ugi because the streaming object will be executed outside\n    // the remote user's ugi\n    final UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    return new StreamingOutput() {\n      @Override\n      public void write(final OutputStream outstream) throws IOException {\n        final PrintWriter out = new PrintWriter(new OutputStreamWriter(\n            outstream, Charsets.UTF_8));\n        out.println(\"{\\\"\" + FileStatus.class.getSimpleName() + \"es\\\":{\\\"\"\n            + FileStatus.class.getSimpleName() + \"\\\":[\");\n\n        try {\n          // restore remote user's ugi\n          ugi.doAs(new PrivilegedExceptionAction<Void>() {\n            @Override\n            public Void run() throws IOException {\n              long n = 0;\n              for (DirectoryListing dirList = firstDirList; ;\n                   dirList = getDirectoryListing(np, p, dirList.getLastName())\n              ) {\n                // send each segment of the directory listing\n                for (HdfsFileStatus s : dirList.getPartialListing()) {\n                  if (n++ > 0) {\n                    out.println(',');\n                  }\n                  out.print(JsonUtil.toJsonString(s, false));\n                }\n                // stop if last segment\n                if (!dirList.hasMore()) {\n                  break;\n                }\n              }\n              return null;\n            }\n          });\n        } catch (InterruptedException e) {\n          throw new IOException(e);\n        }\n        \n        out.println();\n        out.println(\"]}}\");\n        out.flush();\n      }\n    };\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get": "  private Response get(\n      final UserGroupInformation ugi,\n      final DelegationParam delegation,\n      final UserParam username,\n      final DoAsParam doAsUser,\n      final String fullpath,\n      final GetOpParam op,\n      final OffsetParam offset,\n      final LengthParam length,\n      final RenewerParam renewer,\n      final BufferSizeParam bufferSize\n      ) throws IOException, URISyntaxException {\n    final NameNode namenode = (NameNode)context.getAttribute(\"name.node\");\n    final NamenodeProtocols np = namenode.getRpcServer();\n\n    switch(op.getValue()) {\n    case OPEN:\n    {\n      final URI uri = redirectURI(namenode, ugi, delegation, username, doAsUser,\n          fullpath, op.getValue(), offset.getValue(), -1L, offset, length, bufferSize);\n      return Response.temporaryRedirect(uri).type(MediaType.APPLICATION_OCTET_STREAM).build();\n    }\n    case GET_BLOCK_LOCATIONS:\n    {\n      final long offsetValue = offset.getValue();\n      final Long lengthValue = length.getValue();\n      final LocatedBlocks locatedblocks = np.getBlockLocations(fullpath,\n          offsetValue, lengthValue != null? lengthValue: Long.MAX_VALUE);\n      final String js = JsonUtil.toJsonString(locatedblocks);\n      return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n    }\n    case GETFILESTATUS:\n    {\n      final HdfsFileStatus status = np.getFileInfo(fullpath);\n      if (status == null) {\n        throw new FileNotFoundException(\"File does not exist: \" + fullpath);\n      }\n\n      final String js = JsonUtil.toJsonString(status, true);\n      return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n    }\n    case LISTSTATUS:\n    {\n      final StreamingOutput streaming = getListingStream(np, fullpath);\n      return Response.ok(streaming).type(MediaType.APPLICATION_JSON).build();\n    }\n    case GETCONTENTSUMMARY:\n    {\n      final ContentSummary contentsummary = np.getContentSummary(fullpath);\n      final String js = JsonUtil.toJsonString(contentsummary);\n      return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n    }\n    case GETFILECHECKSUM:\n    {\n      final URI uri = redirectURI(namenode, ugi, delegation, username, doAsUser,\n          fullpath, op.getValue(), -1L, -1L);\n      return Response.temporaryRedirect(uri).type(MediaType.APPLICATION_OCTET_STREAM).build();\n    }\n    case GETDELEGATIONTOKEN:\n    {\n      if (delegation.getValue() != null) {\n        throw new IllegalArgumentException(delegation.getName()\n            + \" parameter is not null.\");\n      }\n      final Token<? extends TokenIdentifier> token = generateDelegationToken(\n          namenode, ugi, renewer.getValue());\n      final String js = JsonUtil.toJsonString(token);\n      return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n    }\n    case GETHOMEDIRECTORY:\n    {\n      final String js = JsonUtil.toJsonString(\n          org.apache.hadoop.fs.Path.class.getSimpleName(),\n          WebHdfsFileSystem.getHomeDirectoryString(ugi));\n      return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n    }\n    default:\n      throw new UnsupportedOperationException(op + \" is not supported\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.init": "  private void init(final UserGroupInformation ugi,\n      final DelegationParam delegation,\n      final UserParam username, final DoAsParam doAsUser,\n      final UriFsPathParam path, final HttpOpParam<?> op,\n      final Param<?, ?>... parameters) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"HTTP \" + op.getValue().getType() + \": \" + op + \", \" + path\n          + \", ugi=\" + ugi + \", \" + username + \", \" + doAsUser\n          + Param.toSortedString(\", \", parameters));\n    }\n\n    //clear content type\n    response.setContentType(null);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.generateDelegationToken": "  private Token<? extends TokenIdentifier> generateDelegationToken(\n      final NameNode namenode, final UserGroupInformation ugi,\n      final String renewer) throws IOException {\n    final Credentials c = DelegationTokenSecretManager.createCredentials(\n        namenode, ugi, renewer != null? renewer: ugi.getShortUserName());\n    final Token<? extends TokenIdentifier> t = c.getAllTokens().iterator().next();\n    Text kind = request.getScheme().equals(\"http\") ? WebHdfsFileSystem.TOKEN_KIND : SWebHdfsFileSystem.TOKEN_KIND;\n    t.setKind(kind);\n    return t;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.redirectURI": "  private URI redirectURI(final NameNode namenode,\n      final UserGroupInformation ugi, final DelegationParam delegation,\n      final UserParam username, final DoAsParam doAsUser,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize,\n      final Param<?, ?>... parameters) throws URISyntaxException, IOException {\n    final Configuration conf = (Configuration)context.getAttribute(JspHelper.CURRENT_CONF);\n    final DatanodeInfo dn = chooseDatanode(namenode, path, op, openOffset,\n        blocksize, conf);\n\n    final String delegationQuery;\n    if (!UserGroupInformation.isSecurityEnabled()) {\n      //security disabled\n      delegationQuery = Param.toSortedString(\"&\", doAsUser, username);\n    } else if (delegation.getValue() != null) {\n      //client has provided a token\n      delegationQuery = \"&\" + delegation;\n    } else {\n      //generate a token\n      final Token<? extends TokenIdentifier> t = generateDelegationToken(\n          namenode, ugi, request.getUserPrincipal().getName());\n      delegationQuery = \"&\" + new DelegationParam(t.encodeToUrlString());\n    }\n    final String query = op.toQueryString() + delegationQuery\n        + \"&\" + new NamenodeRpcAddressParam(namenode)\n        + Param.toSortedString(\"&\", parameters);\n    final String uripath = WebHdfsFileSystem.PATH_PREFIX + path;\n\n    final URI uri = new URI(\"http\", null, dn.getHostName(), dn.getInfoPort(),\n        uripath, query, null);\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"redirectURI=\" + uri);\n    }\n    return uri;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.run": "      public Response run() throws IOException {\n        REMOTE_ADDRESS.set(request.getRemoteAddr());\n        try {\n          return delete(ugi, delegation, username, doAsUser,\n              path.getAbsolutePath(), op, recursive);\n        } finally {\n          REMOTE_ADDRESS.set(null);\n        }\n      }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.post": "  private Response post(\n      final UserGroupInformation ugi,\n      final DelegationParam delegation,\n      final UserParam username,\n      final DoAsParam doAsUser,\n      final String fullpath,\n      final PostOpParam op,\n      final ConcatSourcesParam concatSrcs,\n      final BufferSizeParam bufferSize\n      ) throws IOException, URISyntaxException {\n    final NameNode namenode = (NameNode)context.getAttribute(\"name.node\");\n\n    switch(op.getValue()) {\n    case APPEND:\n    {\n      final URI uri = redirectURI(namenode, ugi, delegation, username, doAsUser,\n          fullpath, op.getValue(), -1L, -1L, bufferSize);\n      return Response.temporaryRedirect(uri).type(MediaType.APPLICATION_OCTET_STREAM).build();\n    }\n    case CONCAT:\n    {\n      namenode.getRpcServer().concat(fullpath, concatSrcs.getAbsolutePaths());\n      return Response.ok().build();\n    }\n    default:\n      throw new UnsupportedOperationException(op + \" is not supported\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.delete": "  private Response delete(\n      final UserGroupInformation ugi,\n      final DelegationParam delegation,\n      final UserParam username,\n      final DoAsParam doAsUser,\n      final String fullpath,\n      final DeleteOpParam op,\n      final RecursiveParam recursive\n      ) throws IOException {\n    final NameNode namenode = (NameNode)context.getAttribute(\"name.node\");\n\n    switch(op.getValue()) {\n    case DELETE:\n    {\n      final boolean b = namenode.getRpcServer().delete(fullpath, recursive.getValue());\n      final String js = JsonUtil.toJsonString(\"boolean\", b);\n      return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n    }\n    default:\n      throw new UnsupportedOperationException(op + \" is not supported\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.put": "  private Response put(\n      final UserGroupInformation ugi,\n      final DelegationParam delegation,\n      final UserParam username,\n      final DoAsParam doAsUser,\n      final String fullpath,\n      final PutOpParam op,\n      final DestinationParam destination,\n      final OwnerParam owner,\n      final GroupParam group,\n      final PermissionParam permission,\n      final OverwriteParam overwrite,\n      final BufferSizeParam bufferSize,\n      final ReplicationParam replication,\n      final BlockSizeParam blockSize,\n      final ModificationTimeParam modificationTime,\n      final AccessTimeParam accessTime,\n      final RenameOptionSetParam renameOptions,\n      final CreateParentParam createParent,\n      final TokenArgumentParam delegationTokenArgument\n      ) throws IOException, URISyntaxException {\n\n    final Configuration conf = (Configuration)context.getAttribute(JspHelper.CURRENT_CONF);\n    final NameNode namenode = (NameNode)context.getAttribute(\"name.node\");\n    final NamenodeProtocols np = namenode.getRpcServer();\n\n    switch(op.getValue()) {\n    case CREATE:\n    {\n      final URI uri = redirectURI(namenode, ugi, delegation, username, doAsUser,\n          fullpath, op.getValue(), -1L, blockSize.getValue(conf),\n          permission, overwrite, bufferSize, replication, blockSize);\n      return Response.temporaryRedirect(uri).type(MediaType.APPLICATION_OCTET_STREAM).build();\n    } \n    case MKDIRS:\n    {\n      final boolean b = np.mkdirs(fullpath, permission.getFsPermission(), true);\n      final String js = JsonUtil.toJsonString(\"boolean\", b);\n      return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n    }\n    case CREATESYMLINK:\n    {\n      np.createSymlink(destination.getValue(), fullpath,\n          PermissionParam.getDefaultFsPermission(), createParent.getValue());\n      return Response.ok().type(MediaType.APPLICATION_OCTET_STREAM).build();\n    }\n    case RENAME:\n    {\n      final EnumSet<Options.Rename> s = renameOptions.getValue();\n      if (s.isEmpty()) {\n        final boolean b = np.rename(fullpath, destination.getValue());\n        final String js = JsonUtil.toJsonString(\"boolean\", b);\n        return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n      } else {\n        np.rename2(fullpath, destination.getValue(),\n            s.toArray(new Options.Rename[s.size()]));\n        return Response.ok().type(MediaType.APPLICATION_OCTET_STREAM).build();\n      }\n    }\n    case SETREPLICATION:\n    {\n      final boolean b = np.setReplication(fullpath, replication.getValue(conf));\n      final String js = JsonUtil.toJsonString(\"boolean\", b);\n      return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n    }\n    case SETOWNER:\n    {\n      if (owner.getValue() == null && group.getValue() == null) {\n        throw new IllegalArgumentException(\"Both owner and group are empty.\");\n      }\n\n      np.setOwner(fullpath, owner.getValue(), group.getValue());\n      return Response.ok().type(MediaType.APPLICATION_OCTET_STREAM).build();\n    }\n    case SETPERMISSION:\n    {\n      np.setPermission(fullpath, permission.getFsPermission());\n      return Response.ok().type(MediaType.APPLICATION_OCTET_STREAM).build();\n    }\n    case SETTIMES:\n    {\n      np.setTimes(fullpath, modificationTime.getValue(), accessTime.getValue());\n      return Response.ok().type(MediaType.APPLICATION_OCTET_STREAM).build();\n    }\n    case RENEWDELEGATIONTOKEN:\n    {\n      final Token<DelegationTokenIdentifier> token = new Token<DelegationTokenIdentifier>();\n      token.decodeFromUrlString(delegationTokenArgument.getValue());\n      final long expiryTime = np.renewDelegationToken(token);\n      final String js = JsonUtil.toJsonString(\"long\", expiryTime);\n      return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n    }\n    case CANCELDELEGATIONTOKEN:\n    {\n      final Token<DelegationTokenIdentifier> token = new Token<DelegationTokenIdentifier>();\n      token.decodeFromUrlString(delegationTokenArgument.getValue());\n      np.cancelDelegationToken(token);\n      return Response.ok().type(MediaType.APPLICATION_OCTET_STREAM).build();\n    }\n    default:\n      throw new UnsupportedOperationException(op + \" is not supported\");\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      LOG.error(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getRoot": "  public Response getRoot(\n      @Context final UserGroupInformation ugi,\n      @QueryParam(DelegationParam.NAME) @DefaultValue(DelegationParam.DEFAULT)\n          final DelegationParam delegation,\n      @QueryParam(UserParam.NAME) @DefaultValue(UserParam.DEFAULT)\n          final UserParam username,\n      @QueryParam(DoAsParam.NAME) @DefaultValue(DoAsParam.DEFAULT)\n          final DoAsParam doAsUser,\n      @QueryParam(GetOpParam.NAME) @DefaultValue(GetOpParam.DEFAULT)\n          final GetOpParam op,\n      @QueryParam(OffsetParam.NAME) @DefaultValue(OffsetParam.DEFAULT)\n          final OffsetParam offset,\n      @QueryParam(LengthParam.NAME) @DefaultValue(LengthParam.DEFAULT)\n          final LengthParam length,\n      @QueryParam(RenewerParam.NAME) @DefaultValue(RenewerParam.DEFAULT)\n          final RenewerParam renewer,\n      @QueryParam(BufferSizeParam.NAME) @DefaultValue(BufferSizeParam.DEFAULT)\n          final BufferSizeParam bufferSize\n      ) throws IOException, InterruptedException {\n    return get(ugi, delegation, username, doAsUser, ROOT, op,\n        offset, length, renewer, bufferSize);\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter": "  public void doFilter(ServletRequest request, ServletResponse response, FilterChain filterChain)\n      throws IOException, ServletException {\n    boolean unauthorizedResponse = true;\n    String unauthorizedMsg = \"\";\n    HttpServletRequest httpRequest = (HttpServletRequest) request;\n    HttpServletResponse httpResponse = (HttpServletResponse) response;\n    try {\n      boolean newToken = false;\n      AuthenticationToken token;\n      try {\n        token = getToken(httpRequest);\n      }\n      catch (AuthenticationException ex) {\n        LOG.warn(\"AuthenticationToken ignored: \" + ex.getMessage());\n        token = null;\n      }\n      if (authHandler.managementOperation(token, httpRequest, httpResponse)) {\n        if (token == null) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Request [{}] triggering authentication\", getRequestURL(httpRequest));\n          }\n          token = authHandler.authenticate(httpRequest, httpResponse);\n          if (token != null && token.getExpires() != 0 &&\n              token != AuthenticationToken.ANONYMOUS) {\n            token.setExpires(System.currentTimeMillis() + getValidity() * 1000);\n          }\n          newToken = true;\n        }\n        if (token != null) {\n          unauthorizedResponse = false;\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Request [{}] user [{}] authenticated\", getRequestURL(httpRequest), token.getUserName());\n          }\n          final AuthenticationToken authToken = token;\n          httpRequest = new HttpServletRequestWrapper(httpRequest) {\n\n            @Override\n            public String getAuthType() {\n              return authToken.getType();\n            }\n\n            @Override\n            public String getRemoteUser() {\n              return authToken.getUserName();\n            }\n\n            @Override\n            public Principal getUserPrincipal() {\n              return (authToken != AuthenticationToken.ANONYMOUS) ? authToken : null;\n            }\n          };\n          if (newToken && !token.isExpired() && token != AuthenticationToken.ANONYMOUS) {\n            String signedToken = signer.sign(token.toString());\n            Cookie cookie = createCookie(signedToken);\n            httpResponse.addCookie(cookie);\n          }\n          filterChain.doFilter(httpRequest, httpResponse);\n        }\n      } else {\n        unauthorizedResponse = false;\n      }\n    } catch (AuthenticationException ex) {\n      unauthorizedMsg = ex.toString();\n      LOG.warn(\"Authentication exception: \" + ex.getMessage(), ex);\n    }\n    if (unauthorizedResponse) {\n      if (!httpResponse.isCommitted()) {\n        Cookie cookie = createCookie(\"\");\n        cookie.setMaxAge(0);\n        httpResponse.addCookie(cookie);\n        httpResponse.sendError(HttpServletResponse.SC_UNAUTHORIZED, unauthorizedMsg);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.createCookie": "  protected Cookie createCookie(String token) {\n    Cookie cookie = new Cookie(AuthenticatedURL.AUTH_COOKIE, token);\n    if (getCookieDomain() != null) {\n      cookie.setDomain(getCookieDomain());\n    }\n    if (getCookiePath() != null) {\n      cookie.setPath(getCookiePath());\n    }\n    return cookie;\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.getRequestURL": "  protected String getRequestURL(HttpServletRequest request) {\n    StringBuffer sb = request.getRequestURL();\n    if (request.getQueryString() != null) {\n      sb.append(\"?\").append(request.getQueryString());\n    }\n    return sb.toString();\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.getToken": "  protected AuthenticationToken getToken(HttpServletRequest request) throws IOException, AuthenticationException {\n    AuthenticationToken token = null;\n    String tokenStr = null;\n    Cookie[] cookies = request.getCookies();\n    if (cookies != null) {\n      for (Cookie cookie : cookies) {\n        if (cookie.getName().equals(AuthenticatedURL.AUTH_COOKIE)) {\n          tokenStr = cookie.getValue();\n          try {\n            tokenStr = signer.verifyAndExtract(tokenStr);\n          } catch (SignerException ex) {\n            throw new AuthenticationException(ex);\n          }\n          break;\n        }\n      }\n    }\n    if (tokenStr != null) {\n      token = AuthenticationToken.parse(tokenStr);\n      if (!token.getType().equals(authHandler.getType())) {\n        throw new AuthenticationException(\"Invalid AuthenticationToken type\");\n      }\n      if (token.isExpired()) {\n        throw new AuthenticationException(\"AuthenticationToken expired\");\n      }\n    }\n    return token;\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.getValidity": "  protected long getValidity() {\n    return validity / 1000;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.AuthFilter.doFilter": "  public void doFilter(ServletRequest request, ServletResponse response,\n      FilterChain filterChain) throws IOException, ServletException {\n    final HttpServletRequest httpRequest = toLowerCase((HttpServletRequest)request);\n    final String tokenString = httpRequest.getParameter(DelegationParam.NAME);\n    if (tokenString != null) {\n      //Token is present in the url, therefore token will be used for\n      //authentication, bypass kerberos authentication.\n      filterChain.doFilter(httpRequest, response);\n      return;\n    }\n    super.doFilter(httpRequest, response, filterChain);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.AuthFilter.toLowerCase": "  private static HttpServletRequest toLowerCase(final HttpServletRequest request) {\n    @SuppressWarnings(\"unchecked\")\n    final Map<String, String[]> original = (Map<String, String[]>)request.getParameterMap();\n    if (!ParamFilter.containsUpperCase(original.keySet())) {\n      return request;\n    }\n\n    final Map<String, List<String>> m = new HashMap<String, List<String>>();\n    for(Map.Entry<String, String[]> entry : original.entrySet()) {\n      final String key = entry.getKey().toLowerCase();\n      List<String> strings = m.get(key);\n      if (strings == null) {\n        strings = new ArrayList<String>();\n        m.put(key, strings);\n      }\n      for(String v : entry.getValue()) {\n        strings.add(v);\n      }\n    }\n\n    return new HttpServletRequestWrapper(request) {\n      private Map<String, String[]> parameters = null;\n\n      @Override\n      public Map<String, String[]> getParameterMap() {\n        if (parameters == null) {\n          parameters = new HashMap<String, String[]>();\n          for(Map.Entry<String, List<String>> entry : m.entrySet()) {\n            final List<String> a = entry.getValue();\n            parameters.put(entry.getKey(), a.toArray(new String[a.size()]));\n          }\n        }\n       return parameters;\n      }\n\n      @Override\n      public String getParameter(String name) {\n        final List<String> a = m.get(name);\n        return a == null? null: a.get(0);\n      }\n      \n      @Override\n      public String[] getParameterValues(String name) {\n        return getParameterMap().get(name);\n      }\n\n      @Override\n      public Enumeration<String> getParameterNames() {\n        final Iterator<String> i = m.keySet().iterator();\n        return new Enumeration<String>() {\n          @Override\n          public boolean hasMoreElements() {\n            return i.hasNext();\n          }\n          @Override\n          public String nextElement() {\n            return i.next();\n          }\n        };\n      }\n    };\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.AuthFilter.getParameter": "      public String getParameter(String name) {\n        final List<String> a = m.get(name);\n        return a == null? null: a.get(0);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer.doFilter": "    public void doFilter(ServletRequest request, \n                         ServletResponse response,\n                         FilterChain chain\n                         ) throws IOException, ServletException {\n      HttpServletRequestWrapper quoted = \n        new RequestQuoter((HttpServletRequest) request);\n      HttpServletResponse httpResponse = (HttpServletResponse) response;\n\n      String mime = inferMimeType(request);\n      if (mime == null) {\n        httpResponse.setContentType(\"text/plain; charset=utf-8\");\n      } else if (mime.startsWith(\"text/html\")) {\n        // HTML with unspecified encoding, we want to\n        // force HTML with utf-8 encoding\n        // This is to avoid the following security issue:\n        // http://openmya.hacker.jp/hasegawa/security/utf7cs.html\n        httpResponse.setContentType(\"text/html; charset=utf-8\");\n      } else if (mime.startsWith(\"application/xml\")) {\n        httpResponse.setContentType(\"text/xml; charset=utf-8\");\n      }\n      chain.doFilter(quoted, httpResponse);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer.inferMimeType": "    private String inferMimeType(ServletRequest request) {\n      String path = ((HttpServletRequest)request).getRequestURI();\n      ContextHandler.SContext sContext = (ContextHandler.SContext)config.getServletContext();\n      MimeTypes mimes = sContext.getContextHandler().getMimeTypes();\n      Buffer mimeBuffer = mimes.getMimeByExtension(path);\n      return (mimeBuffer == null) ? null : mimeBuffer.toString();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.NoCacheFilter.doFilter": "  public void doFilter(ServletRequest req, ServletResponse res,\n                       FilterChain chain)\n    throws IOException, ServletException {\n    HttpServletResponse httpRes = (HttpServletResponse) res;\n    httpRes.setHeader(\"Cache-Control\", \"no-cache\");\n    long now = System.currentTimeMillis();\n    httpRes.addDateHeader(\"Expires\", now);\n    httpRes.addDateHeader(\"Date\", now);\n    httpRes.addHeader(\"Pragma\", \"no-cache\");\n    chain.doFilter(req, res);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Time.now": "  public static long now() {\n    return System.currentTimeMillis();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.isReservedName": "  public static boolean isReservedName(String src) {\n    return src.startsWith(DOT_RESERVED_PATH_PREFIX);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getPathComponentsForReservedPath": "  static byte[][] getPathComponentsForReservedPath(String src) {\n    return !isReservedName(src) ? null : INode.getPathComponents(src);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getPathComponents": "  static byte[][] getPathComponents(INode inode) {\n    List<byte[]> components = new ArrayList<byte[]>();\n    components.add(0, inode.getLocalNameBytes());\n    while(inode.getParent() != null) {\n      components.add(0, inode.getParent().getLocalNameBytes());\n      inode = inode.getParent();\n    }\n    return components.toArray(new byte[components.size()][]);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath": "  static String resolvePath(String src, byte[][] pathComponents, FSDirectory fsd)\n      throws FileNotFoundException {\n    if (pathComponents == null || pathComponents.length <= 3) {\n      return src;\n    }\n    // Not /.reserved/.inodes\n    if (!Arrays.equals(DOT_RESERVED, pathComponents[1])\n        || !Arrays.equals(DOT_INODES, pathComponents[2])) { // Not .inodes path\n      return src;\n    }\n    final String inodeId = DFSUtil.bytes2String(pathComponents[3]);\n    long id = 0;\n    try {\n      id = Long.valueOf(inodeId);\n    } catch (NumberFormatException e) {\n      throw new FileNotFoundException(\"Invalid inode path: \" + src);\n    }\n    if (id == INodeId.ROOT_INODE_ID && pathComponents.length == 4) {\n      return Path.SEPARATOR;\n    }\n    INode inode = fsd.getInode(id);\n    if (inode == null) {\n      throw new FileNotFoundException(\n          \"File for given inode path does not exist: \" + src);\n    }\n    \n    // Handle single \"..\" for NFS lookup support.\n    if ((pathComponents.length > 4)\n        && DFSUtil.bytes2String(pathComponents[4]).equals(\"..\")) {\n      INode parent = inode.getParent();\n      if (parent == null || parent.getId() == INodeId.ROOT_INODE_ID) {\n        // inode is root, or its parent is root.\n        return Path.SEPARATOR;\n      } else {\n        return parent.getFullPathName();\n      }\n    }\n\n    StringBuilder path = id == INodeId.ROOT_INODE_ID ? new StringBuilder()\n        : new StringBuilder(inode.getFullPathName());\n    for (int i = 4; i < pathComponents.length; i++) {\n      path.append(Path.SEPARATOR).append(DFSUtil.bytes2String(pathComponents[i]));\n    }\n    if (NameNode.LOG.isDebugEnabled()) {\n      NameNode.LOG.debug(\"Resolved path is \" + path);\n    }\n    return path.toString();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFullPathName": "  static String getFullPathName(INode inode) {\n    INode[] inodes = getFullPathINodes(inode);\n    return getFullPathName(inodes, inodes.length - 1);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getInode": "  public INode getInode(long id) {\n    readLock();\n    try {\n      return inodeMap.get(id);\n    } finally {\n      readUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.getPathComponents": "  static byte[][] getPathComponents(String[] strings) {\n    if (strings.length == 0) {\n      return new byte[][]{null};\n    }\n    byte[][] bytes = new byte[strings.length][];\n    for (int i = 0; i < strings.length; i++)\n      bytes[i] = DFSUtil.string2Bytes(strings[i]);\n    return bytes;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.getPathNames": "  static String[] getPathNames(String path) {\n    if (path == null || !path.startsWith(Path.SEPARATOR)) {\n      throw new AssertionError(\"Absolute path required\");\n    }\n    return StringUtils.split(path, Path.SEPARATOR_CHAR);\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationToken.getExpires": "  public long getExpires() {\n    return expires;\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationToken.isExpired": "  public boolean isExpired() {\n    return getExpires() != -1 && System.currentTimeMillis() > getExpires();\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationToken.getUserName": "  public String getUserName() {\n    return userName;\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationToken.toString": "  public String toString() {\n    return token;\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationToken.setExpires": "  public void setExpires(long expires) {\n    if (this != AuthenticationToken.ANONYMOUS) {\n      this.expires = expires;\n      generateToken();\n    }\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationToken.generateToken": "  private void generateToken() {\n    StringBuffer sb = new StringBuffer();\n    sb.append(USER_NAME).append(\"=\").append(getUserName()).append(ATTR_SEPARATOR);\n    sb.append(PRINCIPAL).append(\"=\").append(getName()).append(ATTR_SEPARATOR);\n    sb.append(TYPE).append(\"=\").append(getType()).append(ATTR_SEPARATOR);\n    sb.append(EXPIRES).append(\"=\").append(getExpires());\n    token = sb.toString();\n  }",
            "hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationToken.getType": "  public String getType() {\n    return type;\n  }"
        },
        "bug_report": {
            "Title": "Avoid groups lookup for unprivileged users such as \"dr.who\"",
            "Description": "Reduce the logs generated by ShellBasedUnixGroupsMapping.\nFor ex: Using WebHdfs from windows generates following log for each request\n\n{noformat}2013-12-03 11:34:56,589 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user dr.who\norg.apache.hadoop.util.Shell$ExitCodeException: id: dr.who: No such user\n\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:504)\n        at org.apache.hadoop.util.Shell.run(Shell.java:417)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:636)\n        at org.apache.hadoop.util.Shell.execCommand(Shell.java:725)\n        at org.apache.hadoop.util.Shell.execCommand(Shell.java:708)\n        at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)\n        at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52)\n        at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)\n        at org.apache.hadoop.security.Groups.getGroups(Groups.java:95)\n        at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1376)\n        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:63)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker(FSNamesystem.java:3228)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListingInt(FSNamesystem.java:4063)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListing(FSNamesystem.java:4052)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getListing(NameNodeRpcServer.java:748)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getDirectoryListing(NamenodeWebHdfsMethods.java:715)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getListingStream(NamenodeWebHdfsMethods.java:727)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get(NamenodeWebHdfsMethods.java:675)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.access$400(NamenodeWebHdfsMethods.java:114)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3.run(NamenodeWebHdfsMethods.java:623)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3.run(NamenodeWebHdfsMethods.java:618)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1515)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get(NamenodeWebHdfsMethods.java:618)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getRoot(NamenodeWebHdfsMethods.java:586)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:384)\n        at org.apache.hadoop.hdfs.web.AuthFilter.doFilter(AuthFilter.java:85)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1310)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n2013-12-03 11:34:56,590 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user dr.who{noformat}"
        }
    },
    {
        "filename": "HADOOP-14727.json",
        "creation_time": "2017-08-02T21:56:38.000+0000",
        "stack_trace": "```\njava.lang.Exception: test\n        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:745)\n        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n        at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:636)\n        at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)\n        at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:749)\n        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:807)\n        at java.io.DataInputStream.read(DataInputStream.java:149)\n        at com.ctc.wstx.io.StreamBootstrapper.ensureLoaded(StreamBootstrapper.java:482)\n        at com.ctc.wstx.io.StreamBootstrapper.resolveStreamEncoding(StreamBootstrapper.java:306)\n        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:167)\n        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)\n        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)\n        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:647)\n        at com.ctc.wstx.stax.WstxInputFactory.createXMLStreamReader(WstxInputFactory.java:366)\n        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2649)\n        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2697)\n        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2662)\n        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2545)\n        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1076)\n        at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1126)\n        at org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1344)\n        at org.apache.hadoop.mapreduce.counters.Limits.init(Limits.java:45)\n        at org.apache.hadoop.mapreduce.counters.Limits.reset(Limits.java:130)\n        at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.loadFullHistoryData(CompletedJob.java:363)\n        at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.<init>(CompletedJob.java:105)\n        at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.loadJob(HistoryFileManager.java:473)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.loadJob(CachedHistoryStorage.java:180)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.access$000(CachedHistoryStorage.java:52)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage$1.load(CachedHistoryStorage.java:103)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage$1.load(CachedHistoryStorage.java:100)\n        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568)\n        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350)\n        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313)\n        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228)\n        at com.google.common.cache.LocalCache.get(LocalCache.java:3965)\n        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3969)\n        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4829)\n        at com.google.common.cache.LocalCache$LocalManualCache.getUnchecked(LocalCache.java:4834)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getFullJob(CachedHistoryStorage.java:193)\n        at org.apache.hadoop.mapreduce.v2.hs.JobHistory.getJob(JobHistory.java:220)\n        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.requireJob(AppController.java:416)\n        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts(AppController.java:277)\n        at org.apache.hadoop.mapreduce.v2.hs.webapp.HsController.attempts(HsController.java:152)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:162)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n        at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287)\n        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)\n        at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)\n        at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:941)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:875)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)\n        at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\n        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\n        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\n        at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter$1.call(GuiceFilter.java:203)\n        at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1552)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(ServletHandler.java:185)\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ServletHandler.java:1112)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n        at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n        at org.eclipse.jetty.server.Server.handle(Server.java:534)\n        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\n        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n        at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n        at java.lang.Thread.run(Thread.java:748)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.parse": "  private XMLStreamReader parse(InputStream is,\n      String systemId) throws IOException, XMLStreamException {\n    if (!quietmode) {\n      LOG.debug(\"parsing input stream \" + is);\n    }\n    if (is == null) {\n      return null;\n    }\n    return XML_INPUT_FACTORY.createXMLStreamReader(systemId, is);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.toString": "  private <T> void toString(List<T> resources, StringBuilder sb) {\n    ListIterator<T> i = resources.listIterator();\n    while (i.hasNext()) {\n      if (i.nextIndex() != 0) {\n        sb.append(\", \");\n      }\n      sb.append(i.next());\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.loadResource": "  private Resource loadResource(Properties properties,\n                                Resource wrapper, boolean quiet) {\n    String name = UNKNOWN_RESOURCE;\n    try {\n      Object resource = wrapper.getResource();\n      name = wrapper.getName();\n      XMLStreamReader2 reader = null;\n      boolean returnCachedProperties = false;\n\n      if (resource instanceof URL) {                  // an URL resource\n        reader = (XMLStreamReader2)parse((URL)resource);\n      } else if (resource instanceof String) {        // a CLASSPATH resource\n        URL url = getResource((String)resource);\n        reader = (XMLStreamReader2)parse(url);\n      } else if (resource instanceof Path) {          // a file resource\n        // Can't use FileSystem API or we get an infinite loop\n        // since FileSystem uses Configuration API.  Use java.io.File instead.\n        File file = new File(((Path)resource).toUri().getPath())\n          .getAbsoluteFile();\n        if (file.exists()) {\n          if (!quiet) {\n            LOG.debug(\"parsing File \" + file);\n          }\n          reader = (XMLStreamReader2)parse(new BufferedInputStream(\n              new FileInputStream(file)), ((Path)resource).toString());\n        }\n      } else if (resource instanceof InputStream) {\n        reader = (XMLStreamReader2)parse((InputStream)resource, null);\n        returnCachedProperties = true;\n      } else if (resource instanceof Properties) {\n        overlay(properties, (Properties)resource);\n      }\n\n      if (reader == null) {\n        if (quiet) {\n          return null;\n        }\n        throw new RuntimeException(resource + \" not found\");\n      }\n      Properties toAddTo = properties;\n      if(returnCachedProperties) {\n        toAddTo = new Properties();\n      }\n      DeprecationContext deprecations = deprecationContext.get();\n\n      StringBuilder token = new StringBuilder();\n      String confName = null;\n      String confValue = null;\n      String confInclude = null;\n      boolean confFinal = false;\n      boolean fallbackAllowed = false;\n      boolean fallbackEntered = false;\n      boolean parseToken = false;\n      LinkedList<String> confSource = new LinkedList<String>();\n\n      while (reader.hasNext()) {\n        switch (reader.next()) {\n        case XMLStreamConstants.START_ELEMENT:\n          switch (reader.getLocalName()) {\n          case \"property\":\n            confName = null;\n            confValue = null;\n            confFinal = false;\n            confSource.clear();\n\n            // First test for short format configuration\n            int attrCount = reader.getAttributeCount();\n            for (int i = 0; i < attrCount; i++) {\n              String propertyAttr = reader.getAttributeLocalName(i);\n              if (\"name\".equals(propertyAttr)) {\n                confName = StringInterner.weakIntern(\n                    reader.getAttributeValue(i));\n              } else if (\"value\".equals(propertyAttr)) {\n                confValue = StringInterner.weakIntern(\n                    reader.getAttributeValue(i));\n              } else if (\"final\".equals(propertyAttr)) {\n                confFinal = \"true\".equals(reader.getAttributeValue(i));\n              } else if (\"source\".equals(propertyAttr)) {\n                confSource.add(StringInterner.weakIntern(\n                    reader.getAttributeValue(i)));\n              }\n            }\n            break;\n          case \"name\":\n          case \"value\":\n          case \"final\":\n          case \"source\":\n            parseToken = true;\n            token.setLength(0);\n            break;\n          case \"include\":\n            // Determine href for xi:include\n            confInclude = null;\n            attrCount = reader.getAttributeCount();\n            for (int i = 0; i < attrCount; i++) {\n              String attrName = reader.getAttributeLocalName(i);\n              if (\"href\".equals(attrName)) {\n                confInclude = reader.getAttributeValue(i);\n              }\n            }\n            if (confInclude == null) {\n              break;\n            }\n            // Determine if the included resource is a classpath resource\n            // otherwise fallback to a file resource\n            // xi:include are treated as inline and retain current source\n            URL include = getResource(confInclude);\n            if (include != null) {\n              Resource classpathResource = new Resource(include, name);\n              loadResource(properties, classpathResource, quiet);\n            } else {\n              URL url;\n              try {\n                url = new URL(confInclude);\n                url.openConnection().connect();\n              } catch (IOException ioe) {\n                File href = new File(confInclude);\n                if (!href.isAbsolute()) {\n                  // Included resources are relative to the current resource\n                  File baseFile = new File(name).getParentFile();\n                  href = new File(baseFile, href.getPath());\n                }\n                if (!href.exists()) {\n                  // Resource errors are non-fatal iff there is 1 xi:fallback\n                  fallbackAllowed = true;\n                  break;\n                }\n                url = href.toURI().toURL();\n              }\n              Resource uriResource = new Resource(url, name);\n              loadResource(properties, uriResource, quiet);\n            }\n            break;\n          case \"fallback\":\n            fallbackEntered = true;\n            break;\n          case \"configuration\":\n            break;\n          default:\n            break;\n          }\n          break;\n\n        case XMLStreamConstants.CHARACTERS:\n          if (parseToken) {\n            char[] text = reader.getTextCharacters();\n            token.append(text, reader.getTextStart(), reader.getTextLength());\n          }\n          break;\n\n        case XMLStreamConstants.END_ELEMENT:\n          switch (reader.getLocalName()) {\n          case \"name\":\n            if (token.length() > 0) {\n              confName = StringInterner.weakIntern(token.toString().trim());\n            }\n            break;\n          case \"value\":\n            if (token.length() > 0) {\n              confValue = StringInterner.weakIntern(token.toString());\n            }\n            break;\n          case \"final\":\n            confFinal = \"true\".equals(token.toString());\n            break;\n          case \"source\":\n            confSource.add(StringInterner.weakIntern(token.toString()));\n            break;\n          case \"include\":\n            if (fallbackAllowed && !fallbackEntered) {\n              throw new IOException(\"Fetch fail on include for '\"\n                  + confInclude + \"' with no fallback while loading '\"\n                  + name + \"'\");\n            }\n            fallbackAllowed = false;\n            fallbackEntered = false;\n            break;\n          case \"property\":\n            if (confName == null || (!fallbackAllowed && fallbackEntered)) {\n              break;\n            }\n            confSource.add(name);\n            DeprecatedKeyInfo keyInfo =\n                deprecations.getDeprecatedKeyMap().get(confName);\n            if (keyInfo != null) {\n              keyInfo.clearAccessed();\n              for (String key : keyInfo.newKeys) {\n                // update new keys with deprecated key's value\n                loadProperty(toAddTo, name, key, confValue, confFinal,\n                    confSource.toArray(new String[confSource.size()]));\n              }\n            } else {\n              loadProperty(toAddTo, name, confName, confValue, confFinal,\n                  confSource.toArray(new String[confSource.size()]));\n            }\n            break;\n          default:\n            break;\n          }\n        default:\n          break;\n        }\n      }\n      reader.close();\n\n      if (returnCachedProperties) {\n        overlay(properties, toAddTo);\n        return new Resource(toAddTo, name);\n      }\n      return null;\n    } catch (IOException e) {\n      LOG.error(\"error parsing conf \" + name, e);\n      throw new RuntimeException(e);\n    } catch (XMLStreamException e) {\n      LOG.error(\"error parsing conf \" + name, e);\n      throw new RuntimeException(e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.hasNext": "      public boolean hasNext() {\n        if (at <= end) {\n          return true;\n        } else if (internal != null){\n          return internal.hasNext();\n        }\n        return false;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.get": "  public String get(String name, String defaultValue) {\n    String[] names = handleDeprecation(deprecationContext.get(), name);\n    String result = null;\n    for(String n : names) {\n      result = substituteVars(getProps().getProperty(n, defaultValue));\n    }\n    return result;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.size": "  public int size() {\n    return getProps().size();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.next": "      public Integer next() {\n        if (at <= end) {\n          at++;\n          return at - 1;\n        } else if (internal != null){\n          Range found = internal.next();\n          if (found != null) {\n            at = found.start;\n            end = found.end;\n            at++;\n            return at - 1;\n          }\n        }\n        return null;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.overlay": "  private void overlay(Properties to, Properties from) {\n    for (Entry<Object, Object> entry: from.entrySet()) {\n      to.put(entry.getKey(), entry.getValue());\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.clear": "  public void clear() {\n    getProps().clear();\n    getOverlay().clear();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getResource": "  public URL getResource(String name) {\n    return classLoader.getResource(name);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.clearAccessed": "    public void clearAccessed() {\n      accessed.set(false);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getDeprecatedKeyMap": "    Map<String, DeprecatedKeyInfo> getDeprecatedKeyMap() {\n      return deprecatedKeyMap;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.loadProperty": "  private void loadProperty(Properties properties, String name, String attr,\n      String value, boolean finalParameter, String[] source) {\n    if (value != null || allowNullValueProperties) {\n      if (value == null) {\n        value = DEFAULT_STRING_CHECK;\n      }\n      if (!finalParameters.contains(attr)) {\n        properties.setProperty(attr, value);\n        if(source != null) {\n          updatingResource.put(attr, source);\n        }\n      } else {\n        // This is a final parameter so check for overrides.\n        checkForOverride(this.properties, name, attr, value);\n        if (this.properties != properties) {\n          checkForOverride(properties, name, attr, value);\n        }\n      }\n    }\n    if (finalParameter && attr != null) {\n      finalParameters.add(attr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getName": "    public String getName(){\n      return name;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.loadResources": "  private void loadResources(Properties properties,\n                             ArrayList<Resource> resources,\n                             boolean quiet) {\n    if(loadDefaults) {\n      for (String resource : defaultResources) {\n        loadResource(properties, new Resource(resource), quiet);\n      }\n    }\n    \n    for (int i = 0; i < resources.size(); i++) {\n      Resource ret = loadResource(properties, resources.get(i), quiet);\n      if (ret != null) {\n        resources.set(i, ret);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.set": "  public void set(String name, String value, String source) {\n    Preconditions.checkArgument(\n        name != null,\n        \"Property name must not be null\");\n    Preconditions.checkArgument(\n        value != null,\n        \"The value of property %s must not be null\", name);\n    name = name.trim();\n    DeprecationContext deprecations = deprecationContext.get();\n    if (deprecations.getDeprecatedKeyMap().isEmpty()) {\n      getProps();\n    }\n    getOverlay().setProperty(name, value);\n    getProps().setProperty(name, value);\n    String newSource = (source == null ? \"programmatically\" : source);\n\n    if (!isDeprecated(name)) {\n      updatingResource.put(name, new String[] {newSource});\n      String[] altNames = getAlternativeNames(name);\n      if(altNames != null) {\n        for(String n: altNames) {\n          if(!n.equals(name)) {\n            getOverlay().setProperty(n, value);\n            getProps().setProperty(n, value);\n            updatingResource.put(n, new String[] {newSource});\n          }\n        }\n      }\n    }\n    else {\n      String[] names = handleDeprecation(deprecationContext.get(), name);\n      String altSource = \"because \" + name + \" is deprecated\";\n      for(String n : names) {\n        getOverlay().setProperty(n, value);\n        getProps().setProperty(n, value);\n        updatingResource.put(n, new String[] {altSource});\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getProps": "  protected synchronized Properties getProps() {\n    if (properties == null) {\n      properties = new Properties();\n      Map<String, String[]> backup =\n          new ConcurrentHashMap<String, String[]>(updatingResource);\n      loadResources(properties, resources, quietmode);\n\n      if (overlay != null) {\n        properties.putAll(overlay);\n        for (Map.Entry<Object,Object> item: overlay.entrySet()) {\n          String key = (String)item.getKey();\n          String[] source = backup.get(key);\n          if(source != null) {\n            updatingResource.put(key, source);\n          }\n        }\n      }\n    }\n    return properties;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getKey": "    public String getKey() {\n      return key;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getTrimmed": "  public String getTrimmed(String name, String defaultValue) {\n    String ret = getTrimmed(name);\n    return ret == null ? defaultValue : ret;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getInt": "  public int getInt(String name, int defaultValue) {\n    String valueString = getTrimmed(name);\n    if (valueString == null)\n      return defaultValue;\n    String hexString = getHexDigits(valueString);\n    if (hexString != null) {\n      return Integer.parseInt(hexString, 16);\n    }\n    return Integer.parseInt(valueString);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getHexDigits": "  private String getHexDigits(String value) {\n    boolean negative = false;\n    String str = value;\n    String hexString = null;\n    if (value.startsWith(\"-\")) {\n      negative = true;\n      str = value.substring(1);\n    }\n    if (str.startsWith(\"0x\") || str.startsWith(\"0X\")) {\n      hexString = str.substring(2);\n      if (negative) {\n        hexString = \"-\" + hexString;\n      }\n      return hexString;\n    }\n    return null;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.counters.Limits.init": "  public synchronized static void init(Configuration conf) {\n    if (!isInited) {\n      if (conf == null) {\n        conf = new JobConf();\n      }\n      GROUP_NAME_MAX = conf.getInt(COUNTER_GROUP_NAME_MAX_KEY,\n          COUNTER_GROUP_NAME_MAX_DEFAULT);\n      COUNTER_NAME_MAX = conf.getInt(COUNTER_NAME_MAX_KEY,\n          COUNTER_NAME_MAX_DEFAULT);\n      GROUPS_MAX = conf.getInt(COUNTER_GROUPS_MAX_KEY, COUNTER_GROUPS_MAX_DEFAULT);\n      COUNTERS_MAX = conf.getInt(COUNTERS_MAX_KEY, COUNTERS_MAX_DEFAULT);\n    }\n    isInited = true;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.webapp.AppController.requireJob": "  public void requireJob() {\n    if ($(JOB_ID).isEmpty()) {\n      badRequest(\"missing job ID\");\n      throw new RuntimeException(\"Bad Request: Missing job ID\");\n    }\n\n    JobId jobID = MRApps.toJobID($(JOB_ID));\n    app.setJob(app.context.getJob(jobID));\n    if (app.getJob() == null) {\n      notFound($(JOB_ID));\n      throw new RuntimeException(\"Not Found: \" + $(JOB_ID));\n    }\n\n    /* check for acl access */\n    Job job = app.context.getJob(jobID);\n    if (!checkAccess(job)) {\n      accessDenied(\"User \" + request().getRemoteUser() + \" does not have \" +\n          \" permission to view job \" + $(JOB_ID));\n      throw new RuntimeException(\"Access denied: User \" +\n          request().getRemoteUser() + \" does not have permission to view job \" +\n          $(JOB_ID));\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.webapp.AppController.notFound": "  void notFound(String s) {\n    setStatus(HttpServletResponse.SC_NOT_FOUND);\n    setTitle(join(\"Not found: \", s));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.webapp.AppController.checkAccess": "  boolean checkAccess(Job job) {\n    String remoteUser = request().getRemoteUser();\n    if (remoteUser == null) {\n      return false;\n    }\n    UserGroupInformation callerUGI =\n        UserGroupInformation.createRemoteUser(remoteUser);\n    if (callerUGI != null && !job.checkAccess(callerUGI, JobACL.VIEW_JOB)) {\n      return false;\n    }\n    return true;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.webapp.AppController.accessDenied": "  void accessDenied(String s) {\n    setStatus(HttpServletResponse.SC_FORBIDDEN);\n    setTitle(join(\"Access denied: \", s));\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.webapp.AppController.badRequest": "  void badRequest(String s) {\n    setStatus(HttpServletResponse.SC_BAD_REQUEST);\n    String title = \"Bad request: \";\n    setTitle((s != null) ? join(title, s) : title);\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts": "  public void attempts() {\n    try {\n      requireJob();\n    }\n    catch (Exception e) {\n      renderText(e.getMessage());\n      return;\n    }\n    if (app.getJob() != null) {\n      try {\n        String taskType = $(TASK_TYPE);\n        if (taskType.isEmpty()) {\n          throw new RuntimeException(\"missing task-type.\");\n        }\n        String attemptState = $(ATTEMPT_STATE);\n        if (attemptState.isEmpty()) {\n          throw new RuntimeException(\"missing attempt-state.\");\n        }\n        setTitle(join(attemptState, \" \",\n            MRApps.taskType(taskType).toString(), \" attempts in \", $(JOB_ID)));\n\n        render(attemptsPage());\n      } catch (Exception e) {\n        LOG.error(\"Failed to render attempts page with task type : \"\n            + $(TASK_TYPE) + \" for job id : \" + $(JOB_ID), e);\n        badRequest(e.getMessage());\n      }\n    }\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attemptsPage": "  protected Class<? extends View> attemptsPage() {\n    return AttemptsPage.class;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter": "  public void doFilter(ServletRequest req, ServletResponse res,\n      FilterChain chain) throws IOException, ServletException {\n    ((HttpServletResponse) res).setHeader(X_FRAME_OPTIONS, option);\n    chain.doFilter(req,\n        new XFrameOptionsResponseWrapper((HttpServletResponse) res));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.lib.StaticUserWebFilter.doFilter": "    public void doFilter(ServletRequest request, ServletResponse response,\n                         FilterChain chain\n                         ) throws IOException, ServletException {\n      HttpServletRequest httpRequest = (HttpServletRequest) request;\n      // if the user is already authenticated, don't override it\n      if (httpRequest.getRemoteUser() != null) {\n        chain.doFilter(request, response);\n      } else {\n        HttpServletRequestWrapper wrapper = \n            new HttpServletRequestWrapper(httpRequest) {\n          @Override\n          public Principal getUserPrincipal() {\n            return user;\n          }\n          @Override\n          public String getRemoteUser() {\n            return username;\n          }\n        };\n        chain.doFilter(wrapper, response);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.lib.StaticUserWebFilter.getRemoteUser": "          public String getRemoteUser() {\n            return username;\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.doFilter": "    public void doFilter(ServletRequest request,\n                         ServletResponse response,\n                         FilterChain chain\n                         ) throws IOException, ServletException {\n      HttpServletRequestWrapper quoted =\n        new RequestQuoter((HttpServletRequest) request);\n      HttpServletResponse httpResponse = (HttpServletResponse) response;\n\n      String mime = inferMimeType(request);\n      if (mime == null) {\n        httpResponse.setContentType(\"text/plain; charset=utf-8\");\n      } else if (mime.startsWith(\"text/html\")) {\n        // HTML with unspecified encoding, we want to\n        // force HTML with utf-8 encoding\n        // This is to avoid the following security issue:\n        // http://openmya.hacker.jp/hasegawa/security/utf7cs.html\n        httpResponse.setContentType(\"text/html; charset=utf-8\");\n      } else if (mime.startsWith(\"application/xml\")) {\n        httpResponse.setContentType(\"text/xml; charset=utf-8\");\n      }\n\n      if(Boolean.valueOf(this.config.getInitParameter(X_FRAME_ENABLED))) {\n        httpResponse.addHeader(\"X-FRAME-OPTIONS\",\n            this.config.getInitParameter(X_FRAME_VALUE));\n      }\n      chain.doFilter(quoted, httpResponse);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.inferMimeType": "    private String inferMimeType(ServletRequest request) {\n      String path = ((HttpServletRequest)request).getRequestURI();\n      ServletContextHandler.Context sContext =\n          (ServletContextHandler.Context)config.getServletContext();\n      String mime = sContext.getMimeType(path);\n      return (mime == null) ? null : mime;\n    }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.webapp.app.setJob": "  void setJob(Job job) {\n    this.job = job;\n  }",
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.webapp.app.getJob": "  public Job getJob() {\n    return job;\n  }"
        },
        "bug_report": {
            "Title": "Socket not closed properly when reading Configurations with BlockReaderRemote",
            "Description": "This is caught by Cloudera's internal testing over the alpha4 release.\n\nWe got reports that some hosts ran out of FDs. Triaging that, found out both oozie server and Yarn JobHistoryServer have tons of sockets on {{CLOSE_WAIT}} state.\n\n[~haibochen] helped narrow down to a consistent reproduction by simply visiting the JHS web UI, and clicking through a job and its logs.\n\nI then look at the {{BlockReaderRemote}} and related code, and didn't spot any leaks in the implementation. After adding a debug log whenever a {{Peer}} is created/closed/in/out {{PeerCache}}, it looks like all the {{CLOSE_WAIT}} sockets are created from this call stack:\n{noformat}\n2017-08-02 13:58:59,901 INFO org.apache.hadoop.hdfs.client.impl.BlockReaderFactory: ____ associated peer NioInetPeer(Socket[addr=/10.17.196.28,port=20002,localport=42512]) with blockreader org.apache.hadoop.hdfs.client.impl.BlockReaderRemote@717ce109\njava.lang.Exception: test\n        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:745)\n        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n        at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:636)\n        at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)\n        at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:749)\n        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:807)\n        at java.io.DataInputStream.read(DataInputStream.java:149)\n        at com.ctc.wstx.io.StreamBootstrapper.ensureLoaded(StreamBootstrapper.java:482)\n        at com.ctc.wstx.io.StreamBootstrapper.resolveStreamEncoding(StreamBootstrapper.java:306)\n        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:167)\n        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)\n        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)\n        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:647)\n        at com.ctc.wstx.stax.WstxInputFactory.createXMLStreamReader(WstxInputFactory.java:366)\n        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2649)\n        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2697)\n        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2662)\n        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2545)\n        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1076)\n        at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1126)\n        at org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1344)\n        at org.apache.hadoop.mapreduce.counters.Limits.init(Limits.java:45)\n        at org.apache.hadoop.mapreduce.counters.Limits.reset(Limits.java:130)\n        at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.loadFullHistoryData(CompletedJob.java:363)\n        at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.<init>(CompletedJob.java:105)\n        at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.loadJob(HistoryFileManager.java:473)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.loadJob(CachedHistoryStorage.java:180)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.access$000(CachedHistoryStorage.java:52)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage$1.load(CachedHistoryStorage.java:103)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage$1.load(CachedHistoryStorage.java:100)\n        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568)\n        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350)\n        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313)\n        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228)\n        at com.google.common.cache.LocalCache.get(LocalCache.java:3965)\n        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3969)\n        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4829)\n        at com.google.common.cache.LocalCache$LocalManualCache.getUnchecked(LocalCache.java:4834)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getFullJob(CachedHistoryStorage.java:193)\n        at org.apache.hadoop.mapreduce.v2.hs.JobHistory.getJob(JobHistory.java:220)\n        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.requireJob(AppController.java:416)\n        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts(AppController.java:277)\n        at org.apache.hadoop.mapreduce.v2.hs.webapp.HsController.attempts(HsController.java:152)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:162)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n        at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287)\n        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)\n        at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)\n        at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:941)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:875)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)\n        at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\n        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\n        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\n        at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)\n        at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1552)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n        at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n        at org.eclipse.jetty.server.Server.handle(Server.java:534)\n        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\n        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n        at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n        at java.lang.Thread.run(Thread.java:748)\n{noformat}\n\nI was able to further confirm this theory by backing out the 4 recent commits to {{Configuration}} on alpha3 and no longer seeing {{CLOSE_WAIT}} sockets.\n- HADOOP-14501. \n- HADOOP-14399. (only reverted to make other reverts easier)\n- HADOOP-14216. Addendum \n- HADOOP-14216. \n\nIt's not clear to me who's responsible to close the InputStream though."
        }
    }
]