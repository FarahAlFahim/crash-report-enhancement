[
    {
        "filename": "HDFS-4558.json",
        "creation_time": "2013-03-06T11:10:08.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n at org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicy.getInstance(BlockPlacementPolicy.java:165)\n at org.apache.hadoop.hdfs.server.balancer.Balancer.checkReplicationPolicyCompatibility(Balancer.java:799)\n at org.apache.hadoop.hdfs.server.balancer.Balancer.<init>(Balancer.java:808)\n at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:831)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.getInstance": "  public static BlockPlacementPolicy getInstance(Configuration conf, \n                                                 FSClusterStats stats,\n                                                 NetworkTopology clusterMap) {\n    Class<? extends BlockPlacementPolicy> replicatorClass =\n                      conf.getClass(\"dfs.block.replicator.classname\",\n                                    BlockPlacementPolicyDefault.class,\n                                    BlockPlacementPolicy.class);\n    BlockPlacementPolicy replicator = (BlockPlacementPolicy) ReflectionUtils.newInstance(\n                                                             replicatorClass, conf);\n    replicator.initialize(conf, stats, clusterMap);\n    return replicator;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.initialize": "  abstract protected void initialize(Configuration conf,  FSClusterStats stats, \n                                     NetworkTopology clusterMap);\n    \n  /**\n   * Get an instance of the configured Block Placement Policy based on the\n   * value of the configuration paramater dfs.block.replicator.classname.\n   * \n   * @param conf the configuration to be used\n   * @param stats an object that is used to retrieve the load on the cluster\n   * @param clusterMap the network topology of the cluster\n   * @return an instance of BlockPlacementPolicy\n   */\n  public static BlockPlacementPolicy getInstance(Configuration conf, \n                                                 FSClusterStats stats,\n                                                 NetworkTopology clusterMap) {\n    Class<? extends BlockPlacementPolicy> replicatorClass =\n                      conf.getClass(\"dfs.block.replicator.classname\",\n                                    BlockPlacementPolicyDefault.class,\n                                    BlockPlacementPolicy.class);\n    BlockPlacementPolicy replicator = (BlockPlacementPolicy) ReflectionUtils.newInstance(\n                                                             replicatorClass, conf);\n    replicator.initialize(conf, stats, clusterMap);\n    return replicator;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Balancer.checkReplicationPolicyCompatibility": "  private static void checkReplicationPolicyCompatibility(Configuration conf\n      ) throws UnsupportedActionException {\n    if (!(BlockPlacementPolicy.getInstance(conf, null, null) instanceof \n        BlockPlacementPolicyDefault)) {\n      throw new UnsupportedActionException(\n          \"Balancer without BlockPlacementPolicyDefault\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Balancer.main": "  public static void main(String[] args) {\n    if (DFSUtil.parseHelpArgument(args, USAGE, System.out, true)) {\n      System.exit(0);\n    }\n\n    try {\n      System.exit(ToolRunner.run(new HdfsConfiguration(), new Cli(), args));\n    } catch (Throwable e) {\n      LOG.error(\"Exiting balancer due an exception\", e);\n      System.exit(-1);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Balancer.run": "    public int run(String[] args) {\n      final long startTime = Time.now();\n      final Configuration conf = getConf();\n      WIN_WIDTH = conf.getLong(\n          DFSConfigKeys.DFS_BALANCER_MOVEDWINWIDTH_KEY, \n          DFSConfigKeys.DFS_BALANCER_MOVEDWINWIDTH_DEFAULT);\n\n      try {\n        checkReplicationPolicyCompatibility(conf);\n\n        final Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(conf);\n        return Balancer.run(namenodes, parse(args), conf);\n      } catch (IOException e) {\n        System.out.println(e + \".  Exiting ...\");\n        return ReturnStatus.IO_EXCEPTION.code;\n      } catch (InterruptedException e) {\n        System.out.println(e + \".  Exiting ...\");\n        return ReturnStatus.INTERRUPTED.code;\n      } finally {\n        System.out.println(\"Balancing took \" + time2Str(Time.now()-startTime));\n      }\n    }"
        },
        "bug_report": {
            "Title": "start balancer failed with NPE",
            "Description": "start balancer failed with NPE\n File this issue to track for QE and dev take a look\n\nbalancer.log:\n 2013-03-06 00:19:55,174 ERROR org.apache.hadoop.hdfs.server.balancer.Balancer: java.lang.NullPointerException\n at org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicy.getInstance(BlockPlacementPolicy.java:165)\n at org.apache.hadoop.hdfs.server.balancer.Balancer.checkReplicationPolicyCompatibility(Balancer.java:799)\n at org.apache.hadoop.hdfs.server.balancer.Balancer.<init>(Balancer.java:808)\n at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:831)\n\nBalancer.java\n private void checkReplicationPolicyCompatibility(Configuration conf)\n throws UnsupportedActionException {\n if (!(BlockPlacementPolicy.getInstance(conf, null, null) <== here\n instanceof BlockPlacementPolicyDefault)) \n{ throw new UnsupportedActionException( \"Balancer without BlockPlacementPolicyDefault\"); }\n }\n"
        }
    },
    {
        "filename": "HDFS-13039.json",
        "creation_time": "2018-01-19T18:52:12.000+0000",
        "stack_trace": "```\njava.io.IOException: Too many open files\n        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)\n        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422)\n        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250)\n        at io.netty.channel.socket.nio.NioServerSocketChannel.doReadMessages(NioServerSocketChannel.java:135)\n        at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:75)\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:563)\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:504)\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:418)\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:390)\n        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:742)\n        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:145)\n        at java.lang.Thread.run(Thread.java:748)\n```",
        "source_code": {},
        "bug_report": {
            "Title": "StripedBlockReader#createBlockReader leaks socket on IOException",
            "Description": "When running EC on one cluster, DataNode has millions of {{CLOSE_WAIT}} connections\r\n{code:java}\r\n$ grep CLOSE_WAIT lsof.out | wc -l\r\n10358700\r\n\r\n// All CLOSW_WAITs belong to the same DataNode process (pid=88527)\r\n$ grep CLOSE_WAIT lsof.out | awk '{print $2}' | sort | uniq\r\n88527\r\n{code}\r\n\r\nAnd DN can not open any file / socket, as shown in the log:\r\n\r\n{noformat}\r\n2018-01-19 06:47:09,424 WARN io.netty.channel.DefaultChannelPipeline: An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.\r\njava.io.IOException: Too many open files\r\n        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)\r\n        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422)\r\n        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250)\r\n        at io.netty.channel.socket.nio.NioServerSocketChannel.doReadMessages(NioServerSocketChannel.java:135)\r\n        at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:75)\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:563)\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:504)\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:418)\r\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:390)\r\n        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:742)\r\n        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:145)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n{noformat}\r\n"
        }
    },
    {
        "filename": "HDFS-13023.json",
        "creation_time": "2018-01-16T20:27:39.000+0000",
        "stack_trace": "```\ncom.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User nn/xxx (auth:PROXY) via jn/xxx (auth:KERBEROS) is not authorized for protocol interface org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol: this service is only accessible by nn/xxx@EXAMPLE.COM\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:242)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n at com.sun.proxy.$Proxy16.getEditLogManifest(Unknown Source)\n at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncWithJournalAtIndex(JournalNodeSyncer.java:254)\n at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncJournals(JournalNodeSyncer.java:230)\n at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.lambda$startSyncJournalsDaemon$0(JournalNodeSyncer.java:190)\n at java.lang.Thread.run(Thread.java:748)\n Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User nn/xxx (auth:PROXY) via jn/xxx (auth:KERBEROS) is not authorized for protocol interface org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol: this service is only accessible by nn/xxx\n at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491)\n at org.apache.hadoop.ipc.Client.call(Client.java:1437)\n at org.apache.hadoop.ipc.Client.call(Client.java:1347)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.invoke": "    public Message invoke(Object proxy, final Method method, Object[] args)\n        throws ServiceException {\n      long startTime = 0;\n      if (LOG.isDebugEnabled()) {\n        startTime = Time.now();\n      }\n      \n      if (args.length != 2) { // RpcController + Message\n        throw new ServiceException(\n            \"Too many or few parameters for request. Method: [\"\n            + method.getName() + \"]\" + \", Expected: 2, Actual: \"\n            + args.length);\n      }\n      if (args[1] == null) {\n        throw new ServiceException(\"null param while calling Method: [\"\n            + method.getName() + \"]\");\n      }\n\n      // if Tracing is on then start a new span for this rpc.\n      // guard it in the if statement to make sure there isn't\n      // any extra string manipulation.\n      Tracer tracer = Tracer.curThreadTracer();\n      TraceScope traceScope = null;\n      if (tracer != null) {\n        traceScope = tracer.newScope(RpcClientUtil.methodToTraceString(method));\n      }\n\n      RequestHeaderProto rpcRequestHeader = constructRpcRequestHeader(method);\n      \n      if (LOG.isTraceEnabled()) {\n        LOG.trace(Thread.currentThread().getId() + \": Call -> \" +\n            remoteId + \": \" + method.getName() +\n            \" {\" + TextFormat.shortDebugString((Message) args[1]) + \"}\");\n      }\n\n\n      final Message theRequest = (Message) args[1];\n      final RpcWritable.Buffer val;\n      try {\n        val = (RpcWritable.Buffer) client.call(RPC.RpcKind.RPC_PROTOCOL_BUFFER,\n            new RpcProtobufRequest(rpcRequestHeader, theRequest), remoteId,\n            fallbackToSimpleAuth);\n\n      } catch (Throwable e) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Exception <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + e + \"}\");\n        }\n        if (traceScope != null) {\n          traceScope.addTimelineAnnotation(\"Call got exception: \" +\n              e.toString());\n        }\n        throw new ServiceException(e);\n      } finally {\n        if (traceScope != null) traceScope.close();\n      }\n\n      if (LOG.isDebugEnabled()) {\n        long callTime = Time.now() - startTime;\n        LOG.debug(\"Call: \" + method.getName() + \" took \" + callTime + \"ms\");\n      }\n      \n      if (Client.isAsynchronousMode()) {\n        final AsyncGet<RpcWritable.Buffer, IOException> arr\n            = Client.getAsyncRpcResponse();\n        final AsyncGet<Message, Exception> asyncGet\n            = new AsyncGet<Message, Exception>() {\n          @Override\n          public Message get(long timeout, TimeUnit unit) throws Exception {\n            return getReturnMessage(method, arr.get(timeout, unit));\n          }\n\n          @Override\n          public boolean isDone() {\n            return arr.isDone();\n          }\n        };\n        ASYNC_RETURN_MESSAGE.set(asyncGet);\n        return null;\n      } else {\n        return getReturnMessage(method, val);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.get": "          public Message get(long timeout, TimeUnit unit) throws Exception {\n            return getReturnMessage(method, arr.get(timeout, unit));\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.toString": "    public String toString() {\n      try {\n        RequestHeaderProto header = getRequestHeader();\n        return header.getDeclaringClassProtocolName() + \".\" +\n               header.getMethodName();\n      } catch (IOException e) {\n        throw new IllegalArgumentException(e);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.constructRpcRequestHeader": "    private RequestHeaderProto constructRpcRequestHeader(Method method) {\n      RequestHeaderProto.Builder builder = RequestHeaderProto\n          .newBuilder();\n      builder.setMethodName(method.getName());\n     \n\n      // For protobuf, {@code protocol} used when creating client side proxy is\n      // the interface extending BlockingInterface, which has the annotations \n      // such as ProtocolName etc.\n      //\n      // Using Method.getDeclaringClass(), as in WritableEngine to get at\n      // the protocol interface will return BlockingInterface, from where \n      // the annotation ProtocolName and Version cannot be\n      // obtained.\n      //\n      // Hence we simply use the protocol class used to create the proxy.\n      // For PB this may limit the use of mixins on client side.\n      builder.setDeclaringClassProtocolName(protocolName);\n      builder.setClientProtocolVersion(clientProtocolVersion);\n      return builder.build();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcProtobufRequest request = (RpcProtobufRequest) writableRequest;\n        RequestHeaderProto rpcRequest = request.getRequestHeader();\n        String methodName = rpcRequest.getMethodName();\n\n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = request.getValue(prototype);\n\n        Message result;\n        long startTime = Time.now();\n        int qTime = (int) (startTime - receiveTime);\n        Exception exception = null;\n        boolean isDeferred = false;\n        try {\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          currentCallInfo.set(new CallInfo(server, methodName));\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n          // Check if this needs to be a deferred response,\n          // by checking the ThreadLocal callback being set\n          if (currentCallback.get() != null) {\n            Server.getCurCall().get().deferResponse();\n            isDeferred = true;\n            currentCallback.set(null);\n            return null;\n          }\n        } catch (ServiceException e) {\n          exception = (Exception) e.getCause();\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          exception = e;\n          throw e;\n        } finally {\n          currentCallInfo.set(null);\n          int processingTime = (int) (Time.now() - startTime);\n          if (LOG.isDebugEnabled()) {\n            String msg =\n                \"Served: \" + methodName + (isDeferred ? \", deferred\" : \"\") +\n                    \", queueTime= \" + qTime +\n                    \" procesingTime= \" + processingTime;\n            if (exception != null) {\n              msg += \" exception= \" + exception.getClass().getSimpleName();\n            }\n            LOG.debug(msg);\n          }\n          String detailedMetricsName = (exception == null) ?\n              methodName :\n              exception.getClass().getSimpleName();\n          server.updateMetrics(detailedMetricsName, qTime, processingTime,\n              isDeferred);\n        }\n        return RpcWritable.wrap(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.close": "    public void close() throws IOException {\n      if (!isClosed) {\n        isClosed = true;\n        CLIENTS.stopClient(client);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.isDone": "          public boolean isDone() {\n            return arr.isDone();\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getReturnMessage": "    private Message getReturnMessage(final Method method,\n        final RpcWritable.Buffer buf) throws ServiceException {\n      Message prototype = null;\n      try {\n        prototype = getReturnProtoType(method);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      Message returnMessage;\n      try {\n        returnMessage = buf.getValue(prototype.getDefaultInstanceForType());\n\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Response <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + TextFormat.shortDebugString(returnMessage) + \"}\");\n        }\n\n      } catch (Throwable e) {\n        throw new ServiceException(e);\n      }\n      return returnMessage;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncWithJournalAtIndex": "  private void syncWithJournalAtIndex(int index) {\n    LOG.info(\"Syncing Journal \" + jn.getBoundIpcAddress().getAddress() + \":\"\n        + jn.getBoundIpcAddress().getPort() + \" with \"\n        + otherJNProxies.get(index) + \", journal id: \" + jid);\n    final QJournalProtocolPB jnProxy = otherJNProxies.get(index).jnProxy;\n    if (jnProxy == null) {\n      LOG.error(\"JournalNode Proxy not found.\");\n      return;\n    }\n\n    List<RemoteEditLog> thisJournalEditLogs;\n    try {\n      thisJournalEditLogs = journal.getEditLogManifest(0, false).getLogs();\n    } catch (IOException e) {\n      LOG.error(\"Exception in getting local edit log manifest\", e);\n      return;\n    }\n\n    GetEditLogManifestResponseProto editLogManifest;\n    try {\n      editLogManifest = jnProxy.getEditLogManifest(null,\n          GetEditLogManifestRequestProto.newBuilder().setJid(jidProto)\n              .setSinceTxId(0)\n              .setInProgressOk(false).build());\n    } catch (ServiceException e) {\n      LOG.error(\"Could not sync with Journal at \" +\n          otherJNProxies.get(journalNodeIndexForSync), e);\n      return;\n    }\n\n    getMissingLogSegments(thisJournalEditLogs, editLogManifest,\n        otherJNProxies.get(index));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.getMissingLogSegments": "  private void getMissingLogSegments(List<RemoteEditLog> thisJournalEditLogs,\n      GetEditLogManifestResponseProto response,\n      JournalNodeProxy remoteJNproxy) {\n\n    List<RemoteEditLog> otherJournalEditLogs = PBHelper.convert(\n        response.getManifest()).getLogs();\n    if (otherJournalEditLogs == null || otherJournalEditLogs.isEmpty()) {\n      LOG.warn(\"Journal at \" + remoteJNproxy.jnAddr + \" has no edit logs\");\n      return;\n    }\n    List<RemoteEditLog> missingLogs = getMissingLogList(thisJournalEditLogs,\n        otherJournalEditLogs);\n\n    if (!missingLogs.isEmpty()) {\n      NamespaceInfo nsInfo = jnStorage.getNamespaceInfo();\n\n      for (RemoteEditLog missingLog : missingLogs) {\n        URL url = null;\n        boolean success = false;\n        try {\n          if (remoteJNproxy.httpServerUrl == null) {\n            if (response.hasFromURL()) {\n              remoteJNproxy.httpServerUrl = getHttpServerURI(\n                  response.getFromURL(), remoteJNproxy.jnAddr.getHostName());\n            } else {\n              LOG.error(\"EditLogManifest response does not have fromUrl \" +\n                  \"field set. Aborting current sync attempt\");\n              break;\n            }\n          }\n\n          String urlPath = GetJournalEditServlet.buildPath(jid, missingLog\n              .getStartTxId(), nsInfo, false);\n          url = new URL(remoteJNproxy.httpServerUrl, urlPath);\n          success = downloadMissingLogSegment(url, missingLog);\n        } catch (URISyntaxException e) {\n          LOG.error(\"EditLogManifest's fromUrl field syntax incorrect\", e);\n        } catch (MalformedURLException e) {\n          LOG.error(\"MalformedURL when download missing log segment\", e);\n        } catch (Exception e) {\n          LOG.error(\"Exception in downloading missing log segment from url \" +\n              url, e);\n        }\n        if (!success) {\n          LOG.error(\"Aborting current sync attempt.\");\n          break;\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncJournals": "  private void syncJournals() {\n    syncWithJournalAtIndex(journalNodeIndexForSync);\n    journalNodeIndexForSync = (journalNodeIndexForSync + 1) % numOtherJNs;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRpcResponse": "  private Writable getRpcResponse(final Call call, final Connection connection,\n      final long timeout, final TimeUnit unit) throws IOException {\n    synchronized (call) {\n      while (!call.done) {\n        try {\n          AsyncGet.Util.wait(call, timeout, unit);\n          if (timeout >= 0 && !call.done) {\n            return null;\n          }\n        } catch (InterruptedException ie) {\n          Thread.currentThread().interrupt();\n          throw new InterruptedIOException(\"Call interrupted\");\n        }\n      }\n\n      if (call.error != null) {\n        if (call.error instanceof RemoteException) {\n          call.error.fillInStackTrace();\n          throw call.error;\n        } else { // local exception\n          InetSocketAddress address = connection.getRemoteAddress();\n          throw NetUtils.wrapException(address.getHostName(),\n                  address.getPort(),\n                  NetUtils.getHostname(),\n                  0,\n                  call.error);\n        }\n      } else {\n        return call.getRpcResponse();\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRemoteAddress": "    public InetSocketAddress getRemoteAddress() {\n      return server;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.call": "  Writable call(RPC.RpcKind rpcKind, Writable rpcRequest,\n      ConnectionId remoteId, int serviceClass,\n      AtomicBoolean fallbackToSimpleAuth) throws IOException {\n    final Call call = createCall(rpcKind, rpcRequest);\n    final Connection connection = getConnection(remoteId, call, serviceClass,\n        fallbackToSimpleAuth);\n\n    try {\n      checkAsyncCall();\n      try {\n        connection.sendRpcRequest(call);                 // send the rpc request\n      } catch (RejectedExecutionException e) {\n        throw new IOException(\"connection has been closed\", e);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n        LOG.warn(\"interrupted waiting to send rpc request to server\", e);\n        throw new IOException(e);\n      }\n    } catch(Exception e) {\n      if (isAsynchronousMode()) {\n        releaseAsyncCall();\n      }\n      throw e;\n    }\n\n    if (isAsynchronousMode()) {\n      final AsyncGet<Writable, IOException> asyncGet\n          = new AsyncGet<Writable, IOException>() {\n        @Override\n        public Writable get(long timeout, TimeUnit unit)\n            throws IOException, TimeoutException{\n          boolean done = true;\n          try {\n            final Writable w = getRpcResponse(call, connection, timeout, unit);\n            if (w == null) {\n              done = false;\n              throw new TimeoutException(call + \" timed out \"\n                  + timeout + \" \" + unit);\n            }\n            return w;\n          } finally {\n            if (done) {\n              releaseAsyncCall();\n            }\n          }\n        }\n\n        @Override\n        public boolean isDone() {\n          synchronized (call) {\n            return call.done;\n          }\n        }\n      };\n\n      ASYNC_RPC_RESPONSE.set(asyncGet);\n      return null;\n    } else {\n      return getRpcResponse(call, connection, -1, null);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.sendRpcRequest": "    public void sendRpcRequest(final Call call)\n        throws InterruptedException, IOException {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n\n      // Serialize the call to be sent. This is done from the actual\n      // caller thread, rather than the sendParamsExecutor thread,\n      \n      // so that if the serialization throws an error, it is reported\n      // properly. This also parallelizes the serialization.\n      //\n      // Format of a call on the wire:\n      // 0) Length of rest below (1 + 2)\n      // 1) RpcRequestHeader  - is serialized Delimited hence contains length\n      // 2) RpcRequest\n      //\n      // Items '1' and '2' are prepared here. \n      RpcRequestHeaderProto header = ProtoUtil.makeRpcRequestHeader(\n          call.rpcKind, OperationProto.RPC_FINAL_PACKET, call.id, call.retry,\n          clientId);\n\n      final ResponseBuffer buf = new ResponseBuffer();\n      header.writeDelimitedTo(buf);\n      RpcWritable.wrap(call.rpcRequest).writeTo(buf);\n\n      synchronized (sendRpcRequestLock) {\n        Future<?> senderFuture = sendParamsExecutor.submit(new Runnable() {\n          @Override\n          public void run() {\n            try {\n              synchronized (ipcStreams.out) {\n                if (shouldCloseConnection.get()) {\n                  return;\n                }\n                if (LOG.isDebugEnabled()) {\n                  LOG.debug(getName() + \" sending #\" + call.id\n                      + \" \" + call.rpcRequest);\n                }\n                // RpcRequestHeader + RpcRequest\n                ipcStreams.sendRequest(buf.toByteArray());\n                ipcStreams.flush();\n              }\n            } catch (IOException e) {\n              // exception at this point would leave the connection in an\n              // unrecoverable state (eg half a call left on the wire).\n              // So, close the connection, killing any outstanding calls\n              markClosed(e);\n            } finally {\n              //the buffer is just an in-memory buffer, but it is still polite to\n              // close early\n              IOUtils.closeStream(buf);\n            }\n          }\n        });\n      \n        try {\n          senderFuture.get();\n        } catch (ExecutionException e) {\n          Throwable cause = e.getCause();\n          \n          // cause should only be a RuntimeException as the Runnable above\n          // catches IOException\n          if (cause instanceof RuntimeException) {\n            throw (RuntimeException) cause;\n          } else {\n            throw new RuntimeException(\"unexpected checked exception\", cause);\n          }\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.createCall": "  Call createCall(RPC.RpcKind rpcKind, Writable rpcRequest) {\n    return new Call(rpcKind, rpcRequest);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.checkAsyncCall": "  private void checkAsyncCall() throws IOException {\n    if (isAsynchronousMode()) {\n      if (asyncCallCounter.incrementAndGet() > maxAsyncCalls) {\n        asyncCallCounter.decrementAndGet();\n        String errMsg = String.format(\n            \"Exceeded limit of max asynchronous calls: %d, \" +\n            \"please configure %s to adjust it.\",\n            maxAsyncCalls,\n            CommonConfigurationKeys.IPC_CLIENT_ASYNC_CALLS_MAX_KEY);\n        throw new AsyncCallLimitExceededException(errMsg);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.isAsynchronousMode": "  public static boolean isAsynchronousMode() {\n    return asynchronousMode.get();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnection": "  private Connection getConnection(ConnectionId remoteId,\n      Call call, int serviceClass, AtomicBoolean fallbackToSimpleAuth)\n      throws IOException {\n    if (!running.get()) {\n      // the client is stopped\n      throw new IOException(\"The client is stopped\");\n    }\n    Connection connection;\n    /* we could avoid this allocation for each RPC by having a  \n     * connectionsId object and with set() method. We need to manage the\n     * refs for keys in HashMap properly. For now its ok.\n     */\n    while (true) {\n      // These lines below can be shorten with computeIfAbsent in Java8\n      connection = connections.get(remoteId);\n      if (connection == null) {\n        connection = new Connection(remoteId, serviceClass);\n        Connection existing = connections.putIfAbsent(remoteId, connection);\n        if (existing != null) {\n          connection = existing;\n        }\n      }\n\n      if (connection.addCall(call)) {\n        break;\n      } else {\n        // This connection is closed, should be removed. But other thread could\n        // have already known this closedConnection, and replace it with a new\n        // connection. So we should call conditional remove to make sure we only\n        // remove this closedConnection.\n        connections.remove(remoteId, connection);\n      }\n    }\n\n    // If the server happens to be slow, the method below will take longer to\n    // establish a connection.\n    connection.setupIOstreams(fallbackToSimpleAuth);\n    return connection;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.releaseAsyncCall": "  private void releaseAsyncCall() {\n    asyncCallCounter.decrementAndGet();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getAsyncRpcResponse": "  public static <T extends Writable> AsyncGet<T, IOException>\n      getAsyncRpcResponse() {\n    return (AsyncGet<T, IOException>) ASYNC_RPC_RESPONSE.get();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.get": "        public Writable get(long timeout, TimeUnit unit)\n            throws IOException, TimeoutException{\n          boolean done = true;\n          try {\n            final Writable w = getRpcResponse(call, connection, timeout, unit);\n            if (w == null) {\n              done = false;\n              throw new TimeoutException(call + \" timed out \"\n                  + timeout + \" \" + unit);\n            }\n            return w;\n          } finally {\n            if (done) {\n              releaseAsyncCall();\n            }\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcClientUtil.methodToTraceString": "  public static String methodToTraceString(Method method) {\n    Class<?> clazz = method.getDeclaringClass();\n    while (true) {\n      Class<?> next = clazz.getEnclosingClass();\n      if (next == null || next.getEnclosingClass() == null) break;\n      clazz = next;\n    }\n    return clazz.getSimpleName() + \"#\" + method.getName();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.journal.getEditLogManifest": "  public RemoteEditLogManifest getEditLogManifest(long sinceTxId,\n      boolean inProgressOk) throws IOException {\n    // No need to checkRequest() here - anyone may ask for the list\n    // of segments.\n    checkFormatted();\n    \n    List<RemoteEditLog> logs = fjm.getRemoteEditLogs(sinceTxId, inProgressOk);\n    \n    if (inProgressOk) {\n      RemoteEditLog log = null;\n      for (Iterator<RemoteEditLog> iter = logs.iterator(); iter.hasNext();) {\n        log = iter.next();\n        if (log.isInProgress()) {\n          iter.remove();\n          break;\n        }\n      }\n      if (log != null && log.isInProgress()) {\n        logs.add(new RemoteEditLog(log.getStartTxId(),\n            getHighestWrittenTxId(), true));\n      }\n    }\n\n    return new RemoteEditLogManifest(logs, getCommittedTxnId());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.journal.getHighestWrittenTxId": "  synchronized long getHighestWrittenTxId() {\n    return highestWrittenTxId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.journal.checkFormatted": "  private void checkFormatted() throws JournalNotFormattedException {\n    if (!isFormatted()) {\n      throw new JournalNotFormattedException(\"Journal \" +\n          storage.getSingularStorageDir() + \" not formatted\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.journal.getCommittedTxnId": "  synchronized long getCommittedTxnId() throws IOException {\n    return committedTxnId.get();\n  }"
        },
        "bug_report": {
            "Title": "Journal Sync does not work on a secure cluster",
            "Description": "Fails with the following exception.\r\n\r\n{code}\r\n\r\n2018-01-10 01:15:40,517 INFO server.JournalNodeSyncer (JournalNodeSyncer.java:syncWithJournalAtIndex(235)) - Syncing Journal /0.0.0.0:8485 with xxx, journal id: mycluster\r\n 2018-01-10 01:15:40,583 ERROR server.JournalNodeSyncer (JournalNodeSyncer.java:syncWithJournalAtIndex(259)) - Could not sync with Journal at xxx/xxx:8485\r\n com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User nn/xxx (auth:PROXY) via jn/xxx (auth:KERBEROS) is not authorized for protocol interface org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol: this service is only accessible by nn/xxx@EXAMPLE.COM\r\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:242)\r\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\r\n at com.sun.proxy.$Proxy16.getEditLogManifest(Unknown Source)\r\n at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncWithJournalAtIndex(JournalNodeSyncer.java:254)\r\n at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncJournals(JournalNodeSyncer.java:230)\r\n at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.lambda$startSyncJournalsDaemon$0(JournalNodeSyncer.java:190)\r\n at java.lang.Thread.run(Thread.java:748)\r\n Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User nn/xxx (auth:PROXY) via jn/xxx (auth:KERBEROS) is not authorized for protocol interface org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol: this service is only accessible by nn/xxx\r\n at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491)\r\n at org.apache.hadoop.ipc.Client.call(Client.java:1437)\r\n at org.apache.hadoop.ipc.Client.call(Client.java:1347)\r\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\r\n ... 6 more\r\n\r\n{code}"
        }
    },
    {
        "filename": "HDFS-3157.json",
        "creation_time": "2012-03-28T14:35:53.000+0000",
        "stack_trace": "```\njava.io.IOException: Error in deleting blocks.\n\tat org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate(FSDataset.java:2061)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:581)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:545)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand(BPServiceActor.java:690)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:522)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:662)\n\tat java.lang.Thread.run(Thread.java:619)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error = false;\n    for (int i = 0; i < invalidBlks.length; i++) {\n      File f = null;\n      final FSVolume v;\n      synchronized (this) {\n        f = getFile(bpid, invalidBlks[i].getBlockId());\n        ReplicaInfo dinfo = volumeMap.get(bpid, invalidBlks[i]);\n        if (dinfo == null || \n            dinfo.getGenerationStamp() != invalidBlks[i].getGenerationStamp()) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                           + invalidBlks[i] + \n                           \". BlockInfo not found in volumeMap.\");\n          error = true;\n          continue;\n        }\n        v = (FSVolume)dinfo.getVolume();\n        if (f == null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Block not found in blockMap.\" +\n                            ((v == null) ? \" \" : \" Block found in volumeMap.\"));\n          error = true;\n          continue;\n        }\n        if (v == null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". No volume for this block.\" +\n                            \" Block found in blockMap. \" + f + \".\");\n          error = true;\n          continue;\n        }\n        File parent = f.getParentFile();\n        if (parent == null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Parent not found for file \" + f + \".\");\n          error = true;\n          continue;\n        }\n        ReplicaState replicaState = dinfo.getState();\n        if (replicaState == ReplicaState.FINALIZED || \n            (replicaState == ReplicaState.RUR && \n                ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() == \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      File metaFile = DatanodeUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp());\n\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f, metaFile,\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.FSDataset.getVolume": "  public synchronized FSVolume getVolume(final ExtendedBlock b) {\n    final ReplicaInfo r =  volumeMap.get(b.getBlockPoolId(), b.getLocalBlock());\n    return r != null? (FSVolume)r.getVolume(): null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.FSDataset.clearPath": "    void clearPath(String bpid, File f) throws IOException {\n      BlockPoolSlice bp = getBlockPoolSlice(bpid);\n      bp.clearPath(f);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.FSDataset.remove": "      public void remove() {\n        throw new UnsupportedOperationException();\n      }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.FSDataset.getMetaFile": "  protected File getMetaFile(ExtendedBlock b) throws IOException {\n    return DatanodeUtil.getMetaFile(getBlockFile(b), b.getGenerationStamp());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.FSDataset.getFile": "  File getFile(final String bpid, final long blockId) {\n    ReplicaInfo info = volumeMap.get(bpid, blockId);\n    if (info != null) {\n      return info.getBlockFile();\n    }\n    return null;    \n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive": "  private boolean processCommandFromActive(DatanodeCommand cmd,\n      BPServiceActor actor) throws IOException {\n    if (cmd == null)\n      return true;\n    final BlockCommand bcmd = \n      cmd instanceof BlockCommand? (BlockCommand)cmd: null;\n\n    switch(cmd.getAction()) {\n    case DatanodeProtocol.DNA_TRANSFER:\n      // Send a copy of a block to another datanode\n      dn.transferBlocks(bcmd.getBlockPoolId(), bcmd.getBlocks(), bcmd.getTargets());\n      dn.metrics.incrBlocksReplicated(bcmd.getBlocks().length);\n      break;\n    case DatanodeProtocol.DNA_INVALIDATE:\n      //\n      // Some local block(s) are obsolete and can be \n      // safely garbage-collected.\n      //\n      Block toDelete[] = bcmd.getBlocks();\n      try {\n        if (dn.blockScanner != null) {\n          dn.blockScanner.deleteBlocks(bcmd.getBlockPoolId(), toDelete);\n        }\n        // using global fsdataset\n        dn.getFSDataset().invalidate(bcmd.getBlockPoolId(), toDelete);\n      } catch(IOException e) {\n        dn.checkDiskError();\n        throw e;\n      }\n      dn.metrics.incrBlocksRemoved(toDelete.length);\n      break;\n    case DatanodeProtocol.DNA_SHUTDOWN:\n      // TODO: DNA_SHUTDOWN appears to be unused - the NN never sends this command\n      // See HDFS-2987.\n      throw new UnsupportedOperationException(\"Received unimplemented DNA_SHUTDOWN\");\n    case DatanodeProtocol.DNA_REGISTER:\n      // namenode requested a registration - at start or if NN lost contact\n      LOG.info(\"DatanodeCommand action: DNA_REGISTER\");\n      actor.reRegister();\n      break;\n    case DatanodeProtocol.DNA_FINALIZE:\n      String bp = ((FinalizeCommand) cmd).getBlockPoolId(); \n      assert getBlockPoolId().equals(bp) :\n        \"BP \" + getBlockPoolId() + \" received DNA_FINALIZE \" +\n        \"for other block pool \" + bp;\n\n      dn.finalizeUpgradeForPool(bp);\n      break;\n    case UpgradeCommand.UC_ACTION_START_UPGRADE:\n      // start distributed upgrade here\n      processDistributedUpgradeCommand((UpgradeCommand)cmd);\n      break;\n    case DatanodeProtocol.DNA_RECOVERBLOCK:\n      dn.recoverBlocks(((BlockRecoveryCommand)cmd).getRecoveringBlocks());\n      break;\n    case DatanodeProtocol.DNA_ACCESSKEYUPDATE:\n      LOG.info(\"DatanodeCommand action: DNA_ACCESSKEYUPDATE\");\n      if (dn.isBlockTokenEnabled) {\n        dn.blockPoolTokenSecretManager.setKeys(\n            getBlockPoolId(), \n            ((KeyUpdateCommand) cmd).getExportedKeys());\n      }\n      break;\n    case DatanodeProtocol.DNA_BALANCERBANDWIDTHUPDATE:\n      LOG.info(\"DatanodeCommand action: DNA_BALANCERBANDWIDTHUPDATE\");\n      long bandwidth =\n                 ((BalancerBandwidthCommand) cmd).getBalancerBandwidthValue();\n      if (bandwidth > 0) {\n        DataXceiverServer dxcs =\n                     (DataXceiverServer) dn.dataXceiverServer.getRunnable();\n        dxcs.balanceThrottler.setBandwidth(bandwidth);\n      }\n      break;\n    default:\n      LOG.warn(\"Unknown DatanodeCommand action: \" + cmd.getAction());\n    }\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId": "  String getBlockPoolId() {\n    if (bpNSInfo != null) {\n      return bpNSInfo.getBlockPoolID();\n    } else {\n      LOG.warn(\"Block pool ID needed, but service not yet registered with NN\",\n          new Exception(\"trace\"));\n      return null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPOfferService.processDistributedUpgradeCommand": "  void processDistributedUpgradeCommand(UpgradeCommand comm)\n  throws IOException {\n    UpgradeManagerDatanode upgradeManager = getUpgradeManager();\n    upgradeManager.processUpgradeCommand(comm);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor": "  synchronized boolean processCommandFromActor(DatanodeCommand cmd,\n      BPServiceActor actor) throws IOException {\n    assert bpServices.contains(actor);\n    if (actor == bpServiceToActive) {\n      return processCommandFromActive(cmd, actor);\n    } else {\n      return processCommandFromStandby(cmd, actor);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromStandby": "  private boolean processCommandFromStandby(DatanodeCommand cmd,\n      BPServiceActor actor) throws IOException {\n    if (cmd == null)\n      return true;\n    switch(cmd.getAction()) {\n    case DatanodeProtocol.DNA_REGISTER:\n      // namenode requested a registration - at start or if NN lost contact\n      LOG.info(\"DatanodeCommand action: DNA_REGISTER\");\n      actor.reRegister();\n      return true;\n    case DatanodeProtocol.DNA_TRANSFER:\n    case DatanodeProtocol.DNA_INVALIDATE:\n    case DatanodeProtocol.DNA_SHUTDOWN:\n    case DatanodeProtocol.DNA_RECOVERBLOCK:\n    case DatanodeProtocol.DNA_ACCESSKEYUPDATE:\n    case DatanodeProtocol.DNA_BALANCERBANDWIDTHUPDATE:\n      LOG.warn(\"Got a command from standby NN - ignoring command:\" + cmd.getAction());\n      return true;   \n    default:\n      LOG.warn(\"Unknown DatanodeCommand action: \" + cmd.getAction());\n    }\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand": "  boolean processCommand(DatanodeCommand[] cmds) {\n    if (cmds != null) {\n      for (DatanodeCommand cmd : cmds) {\n        try {\n          if (bpos.processCommandFromActor(cmd, this) == false) {\n            return false;\n          }\n        } catch (IOException ioe) {\n          LOG.warn(\"Error processing datanode Command\", ioe);\n        }\n      }\n    }\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService": "  private void offerService() throws Exception {\n    LOG.info(\"For namenode \" + nnAddr + \" using DELETEREPORT_INTERVAL of \"\n        + dnConf.deleteReportInterval + \" msec \" + \" BLOCKREPORT_INTERVAL of \"\n        + dnConf.blockReportInterval + \"msec\" + \" Initial delay: \"\n        + dnConf.initialBlockReportDelay + \"msec\" + \"; heartBeatInterval=\"\n        + dnConf.heartBeatInterval);\n\n    //\n    // Now loop for a long time....\n    //\n    while (shouldRun()) {\n      try {\n        long startTime = now();\n\n        //\n        // Every so often, send heartbeat or block-report\n        //\n        if (startTime - lastHeartbeat > dnConf.heartBeatInterval) {\n          //\n          // All heartbeat messages include following info:\n          // -- Datanode name\n          // -- data transfer port\n          // -- Total capacity\n          // -- Bytes remaining\n          //\n          lastHeartbeat = startTime;\n          if (!dn.areHeartbeatsDisabledForTests()) {\n            HeartbeatResponse resp = sendHeartBeat();\n            assert resp != null;\n            dn.getMetrics().addHeartbeat(now() - startTime);\n\n            // If the state of this NN has changed (eg STANDBY->ACTIVE)\n            // then let the BPOfferService update itself.\n            //\n            // Important that this happens before processCommand below,\n            // since the first heartbeat to a new active might have commands\n            // that we should actually process.\n            bpos.updateActorStatesFromHeartbeat(\n                this, resp.getNameNodeHaState());\n\n            long startProcessCommands = now();\n            if (!processCommand(resp.getCommands()))\n              continue;\n            long endProcessCommands = now();\n            if (endProcessCommands - startProcessCommands > 2000) {\n              LOG.info(\"Took \" + (endProcessCommands - startProcessCommands)\n                  + \"ms to process \" + resp.getCommands().length\n                  + \" commands from NN\");\n            }\n          }\n        }\n        if (pendingReceivedRequests > 0\n            || (startTime - lastDeletedReport > dnConf.deleteReportInterval)) {\n          reportReceivedDeletedBlocks();\n          lastDeletedReport = startTime;\n        }\n\n        DatanodeCommand cmd = blockReport();\n        processCommand(new DatanodeCommand[]{ cmd });\n\n        // Now safe to start scanning the block pool.\n        // If it has already been started, this is a no-op.\n        if (dn.blockScanner != null) {\n          dn.blockScanner.addBlockPool(bpos.getBlockPoolId());\n        }\n\n        //\n        // There is no work to do;  sleep until hearbeat timer elapses, \n        // or work arrives, and then iterate again.\n        //\n        long waitTime = dnConf.heartBeatInterval - \n        (System.currentTimeMillis() - lastHeartbeat);\n        synchronized(pendingIncrementalBR) {\n          if (waitTime > 0 && pendingReceivedRequests == 0) {\n            try {\n              pendingIncrementalBR.wait(waitTime);\n            } catch (InterruptedException ie) {\n              LOG.warn(\"BPOfferService for \" + this + \" interrupted\");\n            }\n          }\n        } // synchronized\n      } catch(RemoteException re) {\n        String reClass = re.getClassName();\n        if (UnregisteredNodeException.class.getName().equals(reClass) ||\n            DisallowedDatanodeException.class.getName().equals(reClass) ||\n            IncorrectVersionException.class.getName().equals(reClass)) {\n          LOG.warn(this + \" is shutting down\", re);\n          shouldServiceRun = false;\n          return;\n        }\n        LOG.warn(\"RemoteException in offerService\", re);\n        try {\n          long sleepTime = Math.min(1000, dnConf.heartBeatInterval);\n          Thread.sleep(sleepTime);\n        } catch (InterruptedException ie) {\n          Thread.currentThread().interrupt();\n        }\n      } catch (IOException e) {\n        LOG.warn(\"IOException in offerService\", e);\n      }\n    } // while (shouldRun())\n  } // offerService",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reportReceivedDeletedBlocks": "  private void reportReceivedDeletedBlocks() throws IOException {\n\n    // check if there are newly received blocks\n    ReceivedDeletedBlockInfo[] receivedAndDeletedBlockArray = null;\n    synchronized (pendingIncrementalBR) {\n      int numBlocks = pendingIncrementalBR.size();\n      if (numBlocks > 0) {\n        //\n        // Send newly-received and deleted blockids to namenode\n        //\n        receivedAndDeletedBlockArray = pendingIncrementalBR\n            .values().toArray(new ReceivedDeletedBlockInfo[numBlocks]);\n      }\n      pendingIncrementalBR.clear();\n    }\n    if (receivedAndDeletedBlockArray != null) {\n      StorageReceivedDeletedBlocks[] report = { new StorageReceivedDeletedBlocks(\n          bpRegistration.getStorageID(), receivedAndDeletedBlockArray) };\n      boolean success = false;\n      try {\n        bpNamenode.blockReceivedAndDeleted(bpRegistration, bpos.getBlockPoolId(),\n            report);\n        success = true;\n      } finally {\n        synchronized (pendingIncrementalBR) {\n          if (!success) {\n            // If we didn't succeed in sending the report, put all of the\n            // blocks back onto our queue, but only in the case where we didn't\n            // put something newer in the meantime.\n            for (ReceivedDeletedBlockInfo rdbi : receivedAndDeletedBlockArray) {\n              if (!pendingIncrementalBR.containsKey(rdbi.getBlock().getBlockId())) {\n                pendingIncrementalBR.put(rdbi.getBlock().getBlockId(), rdbi);\n              }\n            }\n          }\n          pendingReceivedRequests = pendingIncrementalBR.size();\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.shouldRun": "  private boolean shouldRun() {\n    return shouldServiceRun && dn.shouldRun();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.blockReport": "  DatanodeCommand blockReport() throws IOException {\n    // send block report if timer has expired.\n    DatanodeCommand cmd = null;\n    long startTime = now();\n    if (startTime - lastBlockReport > dnConf.blockReportInterval) {\n\n      // Flush any block information that precedes the block report. Otherwise\n      // we have a chance that we will miss the delHint information\n      // or we will report an RBW replica after the BlockReport already reports\n      // a FINALIZED one.\n      reportReceivedDeletedBlocks();\n\n      // Create block report\n      long brCreateStartTime = now();\n      BlockListAsLongs bReport = dn.getFSDataset().getBlockReport(\n          bpos.getBlockPoolId());\n\n      // Send block report\n      long brSendStartTime = now();\n      StorageBlockReport[] report = { new StorageBlockReport(\n          new DatanodeStorage(bpRegistration.getStorageID()),\n          bReport.getBlockListAsLongs()) };\n      cmd = bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), report);\n\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost = now() - brSendStartTime;\n      long brCreateCost = brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n          + \" blocks took \" + brCreateCost + \" msec to generate and \"\n          + brSendCost + \" msecs for RPC and NN processing\");\n\n      // If we have sent the first block report, then wait a random\n      // time before we start the periodic block reports.\n      if (resetBlockReportTime) {\n        lastBlockReport = startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n        resetBlockReportTime = false;\n      } else {\n        /* say the last block report was at 8:20:14. The current report\n         * should have started around 9:20:14 (default 1 hour interval).\n         * If current time is :\n         *   1) normal like 9:20:18, next report should be at 10:20:14\n         *   2) unexpected like 11:35:43, next report should be at 12:20:14\n         */\n        lastBlockReport += (now() - lastBlockReport) /\n        dnConf.blockReportInterval * dnConf.blockReportInterval;\n      }\n      LOG.info(\"sent block report, processed command:\" + cmd);\n    }\n    return cmd;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat": "  HeartbeatResponse sendHeartBeat() throws IOException {\n    LOG.info(\"heartbeat: \" + this);\n    // reports number of failed volumes\n    StorageReport[] report = { new StorageReport(bpRegistration.getStorageID(),\n        false,\n        dn.getFSDataset().getCapacity(),\n        dn.getFSDataset().getDfsUsed(),\n        dn.getFSDataset().getRemaining(),\n        dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId())) };\n    return bpNamenode.sendHeartbeat(bpRegistration, report,\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        dn.getFSDataset().getNumFailedVolumes());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run": "  public void run() {\n    LOG.info(this + \" starting to offer service\");\n\n    try {\n      // init stuff\n      try {\n        // setup storage\n        connectToNNAndHandshake();\n      } catch (IOException ioe) {\n        // Initial handshake, storage recovery or registration failed\n        // End BPOfferService thread\n        LOG.fatal(\"Initialization failed for block pool \" + this, ioe);\n        return;\n      }\n\n      initialized = true; // bp is initialized;\n      \n      while (shouldRun()) {\n        try {\n          bpos.startDistributedUpgradeIfNeeded();\n          offerService();\n        } catch (Exception ex) {\n          LOG.error(\"Exception in BPOfferService for \" + this, ex);\n          sleepAndLogInterrupts(5000, \"offering service\");\n        }\n      }\n    } catch (Throwable ex) {\n      LOG.warn(\"Unexpected exception in block pool \" + this, ex);\n    } finally {\n      LOG.warn(\"Ending block pool service for: \" + this);\n      cleanUp();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp": "  private synchronized void cleanUp() {\n    \n    shouldServiceRun = false;\n    IOUtils.cleanup(LOG, bpNamenode);\n    bpos.shutdownActor(this);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sleepAndLogInterrupts": "  private void sleepAndLogInterrupts(int millis,\n      String stateString) {\n    try {\n      Thread.sleep(millis);\n    } catch (InterruptedException ie) {\n      LOG.info(\"BPOfferService \" + this +\n          \" interrupted while \" + stateString);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake": "  private void connectToNNAndHandshake() throws IOException {\n    // get NN proxy\n    bpNamenode = dn.connectToNN(nnAddr);\n\n    // First phase of the handshake with NN - get the namespace\n    // info.\n    NamespaceInfo nsInfo = retrieveNamespaceInfo();\n    \n    // Verify that this matches the other NN in this HA pair.\n    // This also initializes our block pool in the DN if we are\n    // the first NN connection for this BP.\n    bpos.verifyAndSetNamespaceInfo(nsInfo);\n    \n    // Second phase of the handshake with the NN.\n    register();\n  }"
        },
        "bug_report": {
            "Title": "Error in deleting block is keep on coming from DN even after the block report and directory scanning has happened",
            "Description": "Cluster setup:\n\n1NN,Three DN(DN1,DN2,DN3),replication factor-2,\"dfs.blockreport.intervalMsec\" 300,\"dfs.datanode.directoryscan.interval\" 1\n\nstep 1: write one file \"a.txt\" with sync(not closed)\nstep 2: Delete the blocks in one of the datanode say DN1(from rbw) to which replication happened.\nstep 3: close the file.\n\nSince the replication factor is 2 the blocks are replicated to the other datanode.\n\nThen at the NN side the following cmd is issued to DN from which the block is deleted\n-------------------------------------------------------------------------------------\n{noformat}\n2012-03-19 13:41:36,905 INFO org.apache.hadoop.hdfs.StateChange: BLOCK NameSystem.addToCorruptReplicasMap: duplicate requested for blk_2903555284838653156 to add as corrupt on XX.XX.XX.XX by /XX.XX.XX.XX because reported RBW replica with genstamp 1002 does not match COMPLETE block's genstamp in block map 1003\n2012-03-19 13:41:39,588 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* Removing block blk_2903555284838653156_1003 from neededReplications as it has enough replicas.\n{noformat}\n\nFrom the datanode side in which the block is deleted the following exception occured\n\n\n{noformat}\n2012-02-29 13:54:13,126 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block blk_2903555284838653156_1003. BlockInfo not found in volumeMap.\n2012-02-29 13:54:13,126 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Error processing datanode Command\njava.io.IOException: Error in deleting blocks.\n\tat org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate(FSDataset.java:2061)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:581)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:545)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand(BPServiceActor.java:690)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:522)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:662)\n\tat java.lang.Thread.run(Thread.java:619)\n{noformat}"
        }
    },
    {
        "filename": "HDFS-4850.json",
        "creation_time": "2013-05-25T00:18:15.000+0000",
        "stack_trace": "```\nException in thread \"main\" java.lang.NegativeArraySizeException\n\tat org.apache.hadoop.io.Text.readString(Text.java:458)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processPermission(ImageLoaderCurrent.java:370)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINode(ImageLoaderCurrent.java:671)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processChildren(ImageLoaderCurrent.java:557)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:464)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:470)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:470)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processLocalNameINodesWithSnapshot(ImageLoaderCurrent.java:444)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINodes(ImageLoaderCurrent.java:398)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.loadImage(ImageLoaderCurrent.java:199)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.go(OfflineImageViewer.java:136)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.main(OfflineImageViewer.java:260)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.Text.readString": "  public static String readString(DataInput in, int maxLength)\n      throws IOException {\n    int length = WritableUtils.readVIntInRange(in, 0, maxLength);\n    byte [] bytes = new byte[length];\n    in.readFully(bytes, 0, length);\n    return decode(bytes);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.Text.decode": "  private static String decode(ByteBuffer utf8, boolean replace) \n    throws CharacterCodingException {\n    CharsetDecoder decoder = DECODER_FACTORY.get();\n    if (replace) {\n      decoder.onMalformedInput(\n          java.nio.charset.CodingErrorAction.REPLACE);\n      decoder.onUnmappableCharacter(CodingErrorAction.REPLACE);\n    }\n    String str = decoder.decode(utf8).toString();\n    // set decoder back to its default value: REPORT\n    if (replace) {\n      decoder.onMalformedInput(CodingErrorAction.REPORT);\n      decoder.onUnmappableCharacter(CodingErrorAction.REPORT);\n    }\n    return str;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processPermission": "  private void processPermission(DataInputStream in, ImageVisitor v)\n      throws IOException {\n    v.visitEnclosingElement(ImageElement.PERMISSIONS);\n    v.visit(ImageElement.USER_NAME, Text.readString(in));\n    v.visit(ImageElement.GROUP_NAME, Text.readString(in));\n    FsPermission fsp = new FsPermission(in.readShort());\n    v.visit(ImageElement.PERMISSION_STRING, fsp.toString());\n    v.leaveEnclosingElement(); // Permissions\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINode": "  private void processINode(DataInputStream in, ImageVisitor v,\n      boolean skipBlocks, String parentName, boolean isSnapshotCopy)\n      throws IOException {\n    boolean supportSnapshot = \n        LayoutVersion.supports(Feature.SNAPSHOT, imageVersion);\n    boolean supportInodeId = \n        LayoutVersion.supports(Feature.ADD_INODE_ID, imageVersion);\n    \n    v.visitEnclosingElement(ImageElement.INODE);\n    String pathName = FSImageSerialization.readString(in);\n    if (parentName != null) {  // local name\n      pathName = \"/\" + pathName;\n      if (!\"/\".equals(parentName)) { // children of non-root directory\n        pathName = parentName + pathName;\n      }\n    }\n\n    long inodeId = INodeId.GRANDFATHER_INODE_ID;\n    v.visit(ImageElement.INODE_PATH, pathName);\n    if (supportInodeId) {\n      inodeId = in.readLong();\n      v.visit(ImageElement.INODE_ID, inodeId);\n    }\n    v.visit(ImageElement.REPLICATION, in.readShort());\n    v.visit(ImageElement.MODIFICATION_TIME, formatDate(in.readLong()));\n    if(LayoutVersion.supports(Feature.FILE_ACCESS_TIME, imageVersion))\n      v.visit(ImageElement.ACCESS_TIME, formatDate(in.readLong()));\n    v.visit(ImageElement.BLOCK_SIZE, in.readLong());\n    int numBlocks = in.readInt();\n\n    processBlocks(in, v, numBlocks, skipBlocks);\n    \n    if (numBlocks > 0) { // File\n      if (supportSnapshot) {\n        // process file diffs\n        processFileDiffList(in, v, parentName);\n        if (isSnapshotCopy) {\n          boolean underConstruction = in.readBoolean();\n          if (underConstruction) {\n            v.visit(ImageElement.CLIENT_NAME,\n                FSImageSerialization.readString(in));\n            v.visit(ImageElement.CLIENT_MACHINE,\n                FSImageSerialization.readString(in));\n          }\n        }\n      }\n    } else if (numBlocks == -1) { // Directory\n      if (supportSnapshot && supportInodeId) {\n        dirNodeMap.put(inodeId, pathName);\n      }\n      v.visit(ImageElement.NS_QUOTA, numBlocks == -1 ? in.readLong() : -1);\n      if (LayoutVersion.supports(Feature.DISKSPACE_QUOTA, imageVersion))\n        v.visit(ImageElement.DS_QUOTA, numBlocks == -1 ? in.readLong() : -1);\n      if (supportSnapshot) {\n        boolean snapshottable = in.readBoolean();\n        if (!snapshottable) {\n          boolean withSnapshot = in.readBoolean();\n          v.visit(ImageElement.IS_WITHSNAPSHOT_DIR, Boolean.toString(withSnapshot));\n        } else {\n          v.visit(ImageElement.IS_SNAPSHOTTABLE_DIR, Boolean.toString(snapshottable));\n        }\n      }\n    } else if (numBlocks == -2) {\n      v.visit(ImageElement.SYMLINK, Text.readString(in));\n    } else if (numBlocks == -3) { // reference node\n      final boolean isWithName = in.readBoolean();\n      int snapshotId = in.readInt();\n      if (isWithName) {\n        v.visit(ImageElement.SNAPSHOT_LAST_SNAPSHOT_ID, snapshotId);\n      } else {\n        v.visit(ImageElement.SNAPSHOT_DST_SNAPSHOT_ID, snapshotId);\n      }\n      \n      final boolean firstReferred = in.readBoolean();\n      if (firstReferred) {\n        v.visitEnclosingElement(ImageElement.SNAPSHOT_REF_INODE);\n        processINode(in, v, skipBlocks, parentName, isSnapshotCopy);\n        v.leaveEnclosingElement();  // referred inode    \n      } else {\n        v.visit(ImageElement.SNAPSHOT_REF_INODE_ID, in.readLong());\n      }\n    }\n\n    processPermission(in, v);\n    v.leaveEnclosingElement(); // INode\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processFileDiffList": "  private void processFileDiffList(DataInputStream in, ImageVisitor v,\n      String currentINodeName) throws IOException {\n    final int size = in.readInt();\n    if (size >= 0) {\n      v.visitEnclosingElement(ImageElement.SNAPSHOT_FILE_DIFFS,\n          ImageElement.NUM_SNAPSHOT_FILE_DIFF, size);\n      String snapshot = FSImageSerialization.readString(in);\n      v.visit(ImageElement.SNAPSHOT_DIFF_SNAPSHOTROOT, snapshot);\n      v.visit(ImageElement.SNAPSHOT_FILE_SIZE, in.readLong());\n      if (in.readBoolean()) {\n        v.visitEnclosingElement(ImageElement.SNAPSHOT_DIFF_SNAPSHOTINODE);\n        processINode(in, v, true, currentINodeName, true);\n        v.leaveEnclosingElement();\n      }\n      v.leaveEnclosingElement();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processBlocks": "  private void processBlocks(DataInputStream in, ImageVisitor v,\n      int numBlocks, boolean skipBlocks) throws IOException {\n    v.visitEnclosingElement(ImageElement.BLOCKS,\n                            ImageElement.NUM_BLOCKS, numBlocks);\n    \n    // directory or symlink, no blocks to process    \n    if(numBlocks == -1 || numBlocks == -2) { \n      v.leaveEnclosingElement(); // Blocks\n      return;\n    }\n    \n    if(skipBlocks) {\n      int bytesToSkip = ((Long.SIZE * 3 /* fields */) / 8 /*bits*/) * numBlocks;\n      if(in.skipBytes(bytesToSkip) != bytesToSkip)\n        throw new IOException(\"Error skipping over blocks\");\n      \n    } else {\n      for(int j = 0; j < numBlocks; j++) {\n        v.visitEnclosingElement(ImageElement.BLOCK);\n        v.visit(ImageElement.BLOCK_ID, in.readLong());\n        v.visit(ImageElement.NUM_BYTES, in.readLong());\n        v.visit(ImageElement.GENERATION_STAMP, in.readLong());\n        v.leaveEnclosingElement(); // Block\n      }\n    }\n    v.leaveEnclosingElement(); // Blocks\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.formatDate": "  private String formatDate(long date) {\n    return dateFormat.format(new Date(date));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processChildren": "  private int processChildren(DataInputStream in, ImageVisitor v,\n      boolean skipBlocks, String parentName) throws IOException {\n    int numChildren = in.readInt();\n    for (int i = 0; i < numChildren; i++) {\n      processINode(in, v, skipBlocks, parentName, false);\n    }\n    return numChildren;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot": "  private void processDirectoryWithSnapshot(DataInputStream in, ImageVisitor v,\n      boolean skipBlocks) throws IOException {\n    // 1. load dir node id\n    long inodeId = in.readLong();\n    \n    String dirName = dirNodeMap.get(inodeId);\n    String oldValue = subtreeMap.put(inodeId, dirName);\n    if (oldValue != null) { // the subtree has been visited\n      return;\n    }\n    \n    // 2. load possible snapshots\n    processSnapshots(in, v, dirName);\n    // 3. load children nodes\n    processChildren(in, v, skipBlocks, dirName);\n    // 4. load possible directory diff list\n    processDirectoryDiffList(in, v, dirName);\n    // recursively process sub-directories\n    final int numSubTree = in.readInt();\n    for (int i = 0; i < numSubTree; i++) {\n      processDirectoryWithSnapshot(in, v, skipBlocks);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryDiffList": "  private void processDirectoryDiffList(DataInputStream in, ImageVisitor v,\n      String currentINodeName) throws IOException {\n    final int numDirDiff = in.readInt();\n    if (numDirDiff >= 0) {\n      v.visitEnclosingElement(ImageElement.SNAPSHOT_DIR_DIFFS,\n          ImageElement.NUM_SNAPSHOT_DIR_DIFF, numDirDiff);\n      for (int i = 0; i < numDirDiff; i++) {\n        // process directory diffs in reverse chronological oder\n        processDirectoryDiff(in, v, currentINodeName); \n      }\n      v.leaveEnclosingElement();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processSnapshots": "  private void processSnapshots(DataInputStream in, ImageVisitor v,\n      String rootName) throws IOException {\n    final int numSnapshots = in.readInt();\n    if (numSnapshots >= 0) {\n      v.visitEnclosingElement(ImageElement.SNAPSHOTS,\n          ImageElement.NUM_SNAPSHOTS, numSnapshots);\n      for (int i = 0; i < numSnapshots; i++) {\n        // process snapshot\n        v.visitEnclosingElement(ImageElement.SNAPSHOT);\n        v.visit(ImageElement.SNAPSHOT_ID, in.readInt());\n        // process root of snapshot\n        v.visitEnclosingElement(ImageElement.SNAPSHOT_ROOT);\n        processINode(in, v, true, rootName, false);\n        v.leaveEnclosingElement();\n        v.leaveEnclosingElement();\n      }\n      v.visit(ImageElement.SNAPSHOT_QUOTA, in.readInt());\n      v.leaveEnclosingElement();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processLocalNameINodesWithSnapshot": "  private void processLocalNameINodesWithSnapshot(DataInputStream in,\n      ImageVisitor v, boolean skipBlocks) throws IOException {\n    // process root\n    processINode(in, v, skipBlocks, \"\", false);\n    processDirectoryWithSnapshot(in, v, skipBlocks);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINodes": "  private void processINodes(DataInputStream in, ImageVisitor v,\n      long numInodes, boolean skipBlocks, boolean supportSnapshot)\n      throws IOException {\n    v.visitEnclosingElement(ImageElement.INODES,\n        ImageElement.NUM_INODES, numInodes);\n    \n    if (LayoutVersion.supports(Feature.FSIMAGE_NAME_OPTIMIZATION, imageVersion)) {\n      if (!supportSnapshot) {\n        processLocalNameINodes(in, v, numInodes, skipBlocks);\n      } else {\n        processLocalNameINodesWithSnapshot(in, v, skipBlocks);\n      }\n    } else { // full path name\n      processFullNameINodes(in, v, numInodes, skipBlocks);\n    }\n\n    \n    v.leaveEnclosingElement(); // INodes\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processFullNameINodes": "  private void processFullNameINodes(DataInputStream in, ImageVisitor v,\n      long numInodes, boolean skipBlocks) throws IOException {\n    for(long i = 0; i < numInodes; i++) {\n      processINode(in, v, skipBlocks, null, false);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processLocalNameINodes": "  private void processLocalNameINodes(DataInputStream in, ImageVisitor v,\n      long numInodes, boolean skipBlocks) throws IOException {\n    // process root\n    processINode(in, v, skipBlocks, \"\", false);\n    numInodes--;\n    while (numInodes > 0) {\n      numInodes -= processDirectory(in, v, skipBlocks);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.loadImage": "  public void loadImage(DataInputStream in, ImageVisitor v,\n      boolean skipBlocks) throws IOException {\n    boolean done = false;\n    try {\n      v.start();\n      v.visitEnclosingElement(ImageElement.FS_IMAGE);\n\n      imageVersion = in.readInt();\n      if( !canLoadVersion(imageVersion))\n        throw new IOException(\"Cannot process fslayout version \" + imageVersion);\n\n      v.visit(ImageElement.IMAGE_VERSION, imageVersion);\n      v.visit(ImageElement.NAMESPACE_ID, in.readInt());\n\n      long numInodes = in.readLong();\n\n      v.visit(ImageElement.GENERATION_STAMP, in.readLong());\n\n      if (LayoutVersion.supports(Feature.STORED_TXIDS, imageVersion)) {\n        v.visit(ImageElement.TRANSACTION_ID, in.readLong());\n      }\n      \n      if (LayoutVersion.supports(Feature.ADD_INODE_ID, imageVersion)) {\n        v.visit(ImageElement.LAST_INODE_ID, in.readLong());\n      }\n      \n      boolean supportSnapshot = LayoutVersion.supports(Feature.SNAPSHOT,\n          imageVersion);\n      if (supportSnapshot) {\n        v.visit(ImageElement.SNAPSHOT_COUNTER, in.readInt());\n        v.visit(ImageElement.NUM_SNAPSHOTS_TOTAL, in.readInt());\n      }\n      \n      if (LayoutVersion.supports(Feature.FSIMAGE_COMPRESSION, imageVersion)) {\n        boolean isCompressed = in.readBoolean();\n        v.visit(ImageElement.IS_COMPRESSED, String.valueOf(isCompressed));\n        if (isCompressed) {\n          String codecClassName = Text.readString(in);\n          v.visit(ImageElement.COMPRESS_CODEC, codecClassName);\n          CompressionCodecFactory codecFac = new CompressionCodecFactory(\n              new Configuration());\n          CompressionCodec codec = codecFac.getCodecByClassName(codecClassName);\n          if (codec == null) {\n            throw new IOException(\"Image compression codec not supported: \"\n                + codecClassName);\n          }\n          in = new DataInputStream(codec.createInputStream(in));\n        }\n      }\n      processINodes(in, v, numInodes, skipBlocks, supportSnapshot);\n      subtreeMap.clear();\n      dirNodeMap.clear();\n\n      processINodesUC(in, v, skipBlocks);\n\n      if (LayoutVersion.supports(Feature.DELEGATION_TOKEN, imageVersion)) {\n        processDelegationTokens(in, v);\n      }\n      \n      v.leaveEnclosingElement(); // FSImage\n      done = true;\n    } finally {\n      if (done) {\n        v.finish();\n      } else {\n        v.finishAbnormally();\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINodesUC": "  private void processINodesUC(DataInputStream in, ImageVisitor v,\n      boolean skipBlocks) throws IOException {\n    int numINUC = in.readInt();\n\n    v.visitEnclosingElement(ImageElement.INODES_UNDER_CONSTRUCTION,\n                           ImageElement.NUM_INODES_UNDER_CONSTRUCTION, numINUC);\n\n    for(int i = 0; i < numINUC; i++) {\n      v.visitEnclosingElement(ImageElement.INODE_UNDER_CONSTRUCTION);\n      byte [] name = FSImageSerialization.readBytes(in);\n      String n = new String(name, \"UTF8\");\n      v.visit(ImageElement.INODE_PATH, n);\n      \n      if (LayoutVersion.supports(Feature.ADD_INODE_ID, imageVersion)) {\n        long inodeId = in.readLong();\n        v.visit(ImageElement.INODE_ID, inodeId);\n      }\n      \n      v.visit(ImageElement.REPLICATION, in.readShort());\n      v.visit(ImageElement.MODIFICATION_TIME, formatDate(in.readLong()));\n\n      v.visit(ImageElement.PREFERRED_BLOCK_SIZE, in.readLong());\n      int numBlocks = in.readInt();\n      processBlocks(in, v, numBlocks, skipBlocks);\n\n      processPermission(in, v);\n      v.visit(ImageElement.CLIENT_NAME, FSImageSerialization.readString(in));\n      v.visit(ImageElement.CLIENT_MACHINE, FSImageSerialization.readString(in));\n\n      // Skip over the datanode descriptors, which are still stored in the\n      // file but are not used by the datanode or loaded into memory\n      int numLocs = in.readInt();\n      for(int j = 0; j < numLocs; j++) {\n        in.readShort();\n        in.readLong();\n        in.readLong();\n        in.readLong();\n        in.readInt();\n        FSImageSerialization.readString(in);\n        FSImageSerialization.readString(in);\n        WritableUtils.readEnum(in, AdminStates.class);\n      }\n\n      v.leaveEnclosingElement(); // INodeUnderConstruction\n    }\n\n    v.leaveEnclosingElement(); // INodesUnderConstruction\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.canLoadVersion": "  public boolean canLoadVersion(int version) {\n    for(int v : versions)\n      if(v == version) return true;\n\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDelegationTokens": "  private void processDelegationTokens(DataInputStream in, ImageVisitor v)\n      throws IOException {\n    v.visit(ImageElement.CURRENT_DELEGATION_KEY_ID, in.readInt());\n    int numDKeys = in.readInt();\n    v.visitEnclosingElement(ImageElement.DELEGATION_KEYS,\n        ImageElement.NUM_DELEGATION_KEYS, numDKeys);\n    for(int i =0; i < numDKeys; i++) {\n      DelegationKey key = new DelegationKey();\n      key.readFields(in);\n      v.visit(ImageElement.DELEGATION_KEY, key.toString());\n    }\n    v.leaveEnclosingElement();\n    v.visit(ImageElement.DELEGATION_TOKEN_SEQUENCE_NUMBER, in.readInt());\n    int numDTokens = in.readInt();\n    v.visitEnclosingElement(ImageElement.DELEGATION_TOKENS,\n        ImageElement.NUM_DELEGATION_TOKENS, numDTokens);\n    for(int i=0; i<numDTokens; i++){\n      DelegationTokenIdentifier id = new  DelegationTokenIdentifier();\n      id.readFields(in);\n      long expiryTime = in.readLong();\n      v.visitEnclosingElement(ImageElement.DELEGATION_TOKEN_IDENTIFIER);\n      v.visit(ImageElement.DELEGATION_TOKEN_IDENTIFIER_KIND,\n          id.getKind().toString());\n      v.visit(ImageElement.DELEGATION_TOKEN_IDENTIFIER_SEQNO,\n          id.getSequenceNumber());\n      v.visit(ImageElement.DELEGATION_TOKEN_IDENTIFIER_OWNER,\n          id.getOwner().toString());\n      v.visit(ImageElement.DELEGATION_TOKEN_IDENTIFIER_RENEWER,\n          id.getRenewer().toString());\n      v.visit(ImageElement.DELEGATION_TOKEN_IDENTIFIER_REALUSER,\n          id.getRealUser().toString());\n      v.visit(ImageElement.DELEGATION_TOKEN_IDENTIFIER_ISSUE_DATE,\n          id.getIssueDate());\n      v.visit(ImageElement.DELEGATION_TOKEN_IDENTIFIER_MAX_DATE,\n          id.getMaxDate());\n      v.visit(ImageElement.DELEGATION_TOKEN_IDENTIFIER_EXPIRY_TIME,\n          expiryTime);\n      v.visit(ImageElement.DELEGATION_TOKEN_IDENTIFIER_MASTER_KEY_ID,\n          id.getMasterKeyId());\n      v.leaveEnclosingElement(); // DELEGATION_TOKEN_IDENTIFIER\n    }\n    v.leaveEnclosingElement(); // DELEGATION_TOKENS\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.go": "  public void go() throws IOException  {\n    DataInputStream in = null;\n    PositionTrackingInputStream tracker = null;\n    ImageLoader fsip = null;\n    boolean done = false;\n    try {\n      tracker = new PositionTrackingInputStream(new BufferedInputStream(\n               new FileInputStream(new File(inputFile))));\n      in = new DataInputStream(tracker);\n\n      int imageVersionFile = findImageVersion(in);\n\n      fsip = ImageLoader.LoaderFactory.getLoader(imageVersionFile);\n\n      if(fsip == null) \n        throw new IOException(\"No image processor to read version \" +\n            imageVersionFile + \" is available.\");\n      fsip.loadImage(in, processor, skipBlocks);\n      done = true;\n    } finally {\n      if (!done) {\n        LOG.error(\"image loading failed at offset \" + tracker.getPos());\n      }\n      IOUtils.cleanup(LOG, in, tracker);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.findImageVersion": "  private int findImageVersion(DataInputStream in) throws IOException {\n    in.mark(42); // arbitrary amount, resetting immediately\n\n    int version = in.readInt();\n    in.reset();\n\n    return version;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.main": "  public static void main(String[] args) throws IOException {\n    Options options = buildOptions();\n    if(args.length == 0) {\n      printUsage();\n      return;\n    }\n    \n    CommandLineParser parser = new PosixParser();\n    CommandLine cmd;\n\n    try {\n      cmd = parser.parse(options, args);\n    } catch (ParseException e) {\n      System.out.println(\"Error parsing command-line options: \");\n      printUsage();\n      return;\n    }\n\n    if(cmd.hasOption(\"h\")) { // print help and exit\n      printUsage();\n      return;\n    }\n\n    boolean skipBlocks = cmd.hasOption(\"skipBlocks\");\n    boolean printToScreen = cmd.hasOption(\"printToScreen\");\n    String inputFile = cmd.getOptionValue(\"i\");\n    String processor = cmd.getOptionValue(\"p\", \"Ls\");\n    String outputFile = cmd.getOptionValue(\"o\");\n    String delimiter = cmd.getOptionValue(\"delimiter\");\n    \n    if( !(delimiter == null || processor.equals(\"Delimited\")) ) {\n      System.out.println(\"Can only specify -delimiter with Delimited processor\");\n      printUsage();\n      return;\n    }\n    \n    ImageVisitor v;\n    if(processor.equals(\"Indented\")) {\n      v = new IndentedImageVisitor(outputFile, printToScreen);\n    } else if (processor.equals(\"XML\")) {\n      v = new XmlImageVisitor(outputFile, printToScreen);\n    } else if (processor.equals(\"Delimited\")) {\n      v = delimiter == null ?  \n                 new DelimitedImageVisitor(outputFile, printToScreen) :\n                 new DelimitedImageVisitor(outputFile, printToScreen, delimiter);\n      skipBlocks = false;\n    } else if (processor.equals(\"FileDistribution\")) {\n      long maxSize = Long.parseLong(cmd.getOptionValue(\"maxSize\", \"0\"));\n      int step = Integer.parseInt(cmd.getOptionValue(\"step\", \"0\"));\n      v = new FileDistributionVisitor(outputFile, maxSize, step);\n    } else if (processor.equals(\"NameDistribution\")) {\n      v = new NameDistributionVisitor(outputFile, printToScreen);\n    } else {\n      v = new LsImageVisitor(outputFile, printToScreen);\n      skipBlocks = false;\n    }\n    \n    try {\n      OfflineImageViewer d = new OfflineImageViewer(inputFile, v, skipBlocks);\n      d.go();\n    } catch (EOFException e) {\n      System.err.println(\"Input file ended unexpectedly.  Exiting\");\n    } catch(IOException e) {\n      System.err.println(\"Encountered exception.  Exiting: \" + e.getMessage());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.buildOptions": "  public static Options buildOptions() {\n    Options options = new Options();\n\n    // Build in/output file arguments, which are required, but there is no \n    // addOption method that can specify this\n    OptionBuilder.isRequired();\n    OptionBuilder.hasArgs();\n    OptionBuilder.withLongOpt(\"outputFile\");\n    options.addOption(OptionBuilder.create(\"o\"));\n    \n    OptionBuilder.isRequired();\n    OptionBuilder.hasArgs();\n    OptionBuilder.withLongOpt(\"inputFile\");\n    options.addOption(OptionBuilder.create(\"i\"));\n    \n    options.addOption(\"p\", \"processor\", true, \"\");\n    options.addOption(\"h\", \"help\", false, \"\");\n    options.addOption(\"skipBlocks\", false, \"\");\n    options.addOption(\"printToScreen\", false, \"\");\n    options.addOption(\"delimiter\", true, \"\");\n\n    return options;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.printUsage": "  private static void printUsage() {\n    System.out.println(usage);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.WritableUtils.readVIntInRange": "  public static int readVIntInRange(DataInput stream, int lower, int upper)\n      throws IOException {\n    long n = readVLong(stream);\n    if (n < lower) {\n      if (lower == 0) {\n        throw new IOException(\"expected non-negative integer, got \" + n);\n      } else {\n        throw new IOException(\"expected integer greater than or equal to \" +\n            lower + \", got \" + n);\n      }\n    }\n    if (n > upper) {\n      throw new IOException(\"expected integer less or equal to \" + upper +\n          \", got \" + n);\n    }\n    return (int)n;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.WritableUtils.readVLong": "  public static long readVLong(DataInput stream) throws IOException {\n    byte firstByte = stream.readByte();\n    int len = decodeVIntSize(firstByte);\n    if (len == 1) {\n      return firstByte;\n    }\n    long i = 0;\n    for (int idx = 0; idx < len-1; idx++) {\n      byte b = stream.readByte();\n      i = i << 8;\n      i = i | (b & 0xFF);\n    }\n    return (isNegativeVInt(firstByte) ? (i ^ -1L) : i);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.WritableUtils.readVInt": "  public static int readVInt(DataInput stream) throws IOException {\n    long n = readVLong(stream);\n    if ((n > Integer.MAX_VALUE) || (n < Integer.MIN_VALUE)) {\n      throw new IOException(\"value too long to fit in integer\");\n    }\n    return (int)n;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoader.loadImage": "  public void loadImage(DataInputStream in, ImageVisitor v,\n      boolean enumerateBlocks) throws IOException;\n\n  /**\n   * Can this processor handle the specified version of FSImage file?\n   *\n   * @param version FSImage version file\n   * @return True if this instance can process the file\n   */\n  public boolean canLoadVersion(int version);\n\n  /**\n   * Factory for obtaining version of image loader that can read\n   * a particular image format.\n   */\n  @InterfaceAudience.Private\n  public class LoaderFactory {\n    // Java doesn't support static methods on interfaces, which necessitates\n    // this factory class\n\n    /**\n     * Find an image loader capable of interpreting the specified\n     * layout version number.  If none, return null;\n     *\n     * @param version fsimage layout version number to be processed\n     * @return ImageLoader that can interpret specified version, or null\n     */\n    static public ImageLoader getLoader(int version) {\n      // Easy to add more image processors as they are written\n      ImageLoader[] loaders = { new ImageLoaderCurrent() };\n\n      for (ImageLoader l : loaders) {\n        if (l.canLoadVersion(version))\n          return l;\n      }\n\n      return null;\n    }\n  }"
        },
        "bug_report": {
            "Title": "fix OfflineImageViewer to work on fsimages with empty files or snapshots",
            "Description": "I deployed hadoop-trunk HDFS and created _/user/schu/_. I then forced a checkpoint, fetched the fsimage, and ran the default OfflineImageViewer successfully on the fsimage.\n\n{code}\nschu-mbp:~ schu$ hdfs oiv -i fsimage_0000000000000000004 -o oiv_out_1\nschu-mbp:~ schu$ cat oiv_out_1\ndrwxr-xr-x  -     schu supergroup          0 2013-05-24 16:59 /\ndrwxr-xr-x  -     schu supergroup          0 2013-05-24 16:59 /user\ndrwxr-xr-x  -     schu supergroup          0 2013-05-24 16:59 /user/schu\nschu-mbp:~ schu$ \n{code}\n\nI then touched an empty file _/user/schu/testFile1_\n{code}\nschu-mbp:~ schu$ hadoop fs -lsr /\nlsr: DEPRECATED: Please use 'ls -R' instead.\ndrwxr-xr-x   - schu supergroup          0 2013-05-24 16:59 /user\ndrwxr-xr-x   - schu supergroup          0 2013-05-24 17:00 /user/schu\n-rw-r--r--   1 schu supergroup          0 2013-05-24 17:00 /user/schu/testFile1\n{code}\n\nand forced another checkpoint, fetched the fsimage, and reran the OfflineImageViewer. I encountered a NegativeArraySizeException:\n\n\n{code}\nschu-mbp:~ schu$ hdfs oiv -i fsimage_0000000000000000008 -o oiv_out_2\nInput ended unexpectedly.\n2013-05-24 17:01:13,622 ERROR [main] offlineImageViewer.OfflineImageViewer (OfflineImageViewer.java:go(140)) - image loading failed at offset 402\nException in thread \"main\" java.lang.NegativeArraySizeException\n\tat org.apache.hadoop.io.Text.readString(Text.java:458)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processPermission(ImageLoaderCurrent.java:370)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINode(ImageLoaderCurrent.java:671)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processChildren(ImageLoaderCurrent.java:557)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:464)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:470)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:470)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processLocalNameINodesWithSnapshot(ImageLoaderCurrent.java:444)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINodes(ImageLoaderCurrent.java:398)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.loadImage(ImageLoaderCurrent.java:199)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.go(OfflineImageViewer.java:136)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.main(OfflineImageViewer.java:260)\n{code}\n\nThis is reproducible. I've reproduced this scenario after formatting HDFS and restarting and touching an empty file _/testFile1_.\n\nAttached are the data dirs, the fsimage before creating the empty file (fsimage_0000000000000000004) and the fsimage afterwards (fsimage_0000000000000000004) and their outputs, oiv_out_1 and oiv_out_2 respectively.\n\n\nThe oiv_out_2 does not include the empty _/user/schu/testFile1_.\n\nI don't run into this problem using hadoop-2.0.4-alpha."
        }
    },
    {
        "filename": "HDFS-3415.json",
        "creation_time": "2012-05-13T13:44:43.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.namenode.NNStorage.getStorageFile(NNStorage.java:686)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditsInStorageDir(FSImagePreTransactionalStorageInspector.java:243)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getLatestEditsFiles(FSImagePreTransactionalStorageInspector.java:261)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditLogStreams(FSImagePreTransactionalStorageInspector.java:276)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:596)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:247)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:498)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:390)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:354)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:368)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:402)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:564)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:545)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1093)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1151)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.getStorageFile": "  static File getStorageFile(StorageDirectory sd, NameNodeFile type) {\n    return new File(sd.getCurrentDir(), type.getName());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.format": "  public void format() throws IOException {\n    this.layoutVersion = HdfsConstants.LAYOUT_VERSION;\n    for (Iterator<StorageDirectory> it =\n                           dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      format(sd);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.getName": "    String getName() { return fileName; }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditsInStorageDir": "  static List<File> getEditsInStorageDir(StorageDirectory sd) {\n    ArrayList<File> files = new ArrayList<File>();\n    File edits = NNStorage.getStorageFile(sd, NameNodeFile.EDITS);\n    assert edits.exists() : \"Expected edits file at \" + edits;\n    files.add(edits);\n    File editsNew = NNStorage.getStorageFile(sd, NameNodeFile.EDITS_NEW);\n    if (editsNew.exists()) {\n      files.add(editsNew);\n    }\n    return files;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getLatestEditsFiles": "  private List<File> getLatestEditsFiles() {\n    if (latestNameCheckpointTime > latestEditsCheckpointTime) {\n      // the image is already current, discard edits\n      LOG.debug(\n          \"Name checkpoint time is newer than edits, not loading edits.\");\n      return Collections.<File>emptyList();\n    }\n    \n    return getEditsInStorageDir(latestEditsSD);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditLogStreams": "  static Iterable<EditLogInputStream> getEditLogStreams(NNStorage storage)\n      throws IOException {\n    FSImagePreTransactionalStorageInspector inspector \n      = new FSImagePreTransactionalStorageInspector();\n    storage.inspectStorageDirs(inspector);\n\n    List<EditLogInputStream> editStreams = new ArrayList<EditLogInputStream>();\n    for (File f : inspector.getLatestEditsFiles()) {\n      editStreams.add(new EditLogFileInputStream(f));\n    }\n    return editStreams;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage": "  private void loadFSImage(File curFile, MD5Hash expectedMd5,\n      FSNamesystem target, MetaRecoveryContext recovery) throws IOException {\n    FSImageFormat.Loader loader = new FSImageFormat.Loader(\n        conf, target);\n    loader.load(curFile);\n    target.setBlockPoolId(this.getBlockPoolID());\n\n    // Check that the image digest we loaded matches up with what\n    // we expected\n    MD5Hash readImageMd5 = loader.getLoadedImageMd5();\n    if (expectedMd5 != null &&\n        !expectedMd5.equals(readImageMd5)) {\n      throw new IOException(\"Image file \" + curFile +\n          \" is corrupt with MD5 checksum of \" + readImageMd5 +\n          \" but expecting \" + expectedMd5);\n    }\n\n    long txId = loader.getLoadedImageTxId();\n    LOG.info(\"Loaded image for txid \" + txId + \" from \" + curFile);\n    lastAppliedTxId = txId;\n    storage.setMostRecentCheckpointInfo(txId, curFile.lastModified());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getBlockPoolID": "  public String getBlockPoolID() {\n    return storage.getBlockPoolID();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getLayoutVersion": "  public int getLayoutVersion() {\n    return storage.getLayoutVersion();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.needsResaveBasedOnStaleCheckpoint": "  private boolean needsResaveBasedOnStaleCheckpoint(\n      File imageFile, long numEditsLoaded) {\n    final long checkpointPeriod = conf.getLong(\n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_KEY, \n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_DEFAULT);\n    final long checkpointTxnCount = conf.getLong(\n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_KEY, \n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_DEFAULT);\n    long checkpointAge = System.currentTimeMillis() - imageFile.lastModified();\n\n    return (checkpointAge > checkpointPeriod * 1000) ||\n           (numEditsLoaded > checkpointTxnCount);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.isUpgradeFinalized": "  boolean isUpgradeFinalized() {\n    return isUpgradeFinalized;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits": "  public long loadEdits(Iterable<EditLogInputStream> editStreams,\n      FSNamesystem target, MetaRecoveryContext recovery) throws IOException {\n    LOG.debug(\"About to load edits:\\n  \" + Joiner.on(\"\\n  \").join(editStreams));\n    \n    long prevLastAppliedTxId = lastAppliedTxId;  \n    try {    \n      FSEditLogLoader loader = new FSEditLogLoader(target, lastAppliedTxId);\n      \n      // Load latest edits\n      for (EditLogInputStream editIn : editStreams) {\n        LOG.info(\"Reading \" + editIn + \" expecting start txid #\" +\n              (lastAppliedTxId + 1));\n        try {\n          loader.loadFSEdits(editIn, lastAppliedTxId + 1, recovery);\n        } finally {\n          // Update lastAppliedTxId even in case of error, since some ops may\n          // have been successfully applied before the error.\n          lastAppliedTxId = loader.getLastAppliedTxId();\n        }\n        // If we are in recovery mode, we may have skipped over some txids.\n        if (editIn.getLastTxId() != HdfsConstants.INVALID_TXID) {\n          lastAppliedTxId = editIn.getLastTxId();\n        }\n      }\n    } finally {\n      FSEditLog.closeAllStreams(editStreams);\n      // update the counts\n      target.dir.updateCountForINodeWithQuota();   \n    }\n    return lastAppliedTxId - prevLastAppliedTxId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead": "  boolean recoverTransitionRead(StartupOption startOpt, FSNamesystem target,\n      MetaRecoveryContext recovery) throws IOException {\n    assert startOpt != StartupOption.FORMAT : \n      \"NameNode formatting should be performed before reading the image\";\n    \n    Collection<URI> imageDirs = storage.getImageDirectories();\n    Collection<URI> editsDirs = editLog.getEditURIs();\n\n    // none of the data dirs exist\n    if((imageDirs.size() == 0 || editsDirs.size() == 0) \n                             && startOpt != StartupOption.IMPORT)  \n      throw new IOException(\n          \"All specified directories are not accessible or do not exist.\");\n    \n    storage.setUpgradeManager(target.upgradeManager);\n    \n    // 1. For each data directory calculate its state and \n    // check whether all is consistent before transitioning.\n    Map<StorageDirectory, StorageState> dataDirStates = \n             new HashMap<StorageDirectory, StorageState>();\n    boolean isFormatted = recoverStorageDirs(startOpt, dataDirStates);\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Data dir states:\\n  \" +\n        Joiner.on(\"\\n  \").withKeyValueSeparator(\": \")\n        .join(dataDirStates));\n    }\n    \n    if (!isFormatted && startOpt != StartupOption.ROLLBACK \n                     && startOpt != StartupOption.IMPORT) {\n      throw new IOException(\"NameNode is not formatted.\");      \n    }\n\n\n    int layoutVersion = storage.getLayoutVersion();\n    if (layoutVersion < Storage.LAST_PRE_UPGRADE_LAYOUT_VERSION) {\n      NNStorage.checkVersionUpgradable(storage.getLayoutVersion());\n    }\n    if (startOpt != StartupOption.UPGRADE\n        && layoutVersion < Storage.LAST_PRE_UPGRADE_LAYOUT_VERSION\n        && layoutVersion != HdfsConstants.LAYOUT_VERSION) {\n      throw new IOException(\n          \"\\nFile system image contains an old layout version \" \n          + storage.getLayoutVersion() + \".\\nAn upgrade to version \"\n          + HdfsConstants.LAYOUT_VERSION + \" is required.\\n\"\n          + \"Please restart NameNode with -upgrade option.\");\n    }\n    \n    storage.processStartupOptionsForUpgrade(startOpt, layoutVersion);\n\n    // check whether distributed upgrade is required and/or should be continued\n    storage.verifyDistributedUpgradeProgress(startOpt);\n\n    // 2. Format unformatted dirs.\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      StorageState curState = dataDirStates.get(sd);\n      switch(curState) {\n      case NON_EXISTENT:\n        throw new IOException(StorageState.NON_EXISTENT + \n                              \" state cannot be here\");\n      case NOT_FORMATTED:\n        LOG.info(\"Storage directory \" + sd.getRoot() + \" is not formatted.\");\n        LOG.info(\"Formatting ...\");\n        sd.clearDirectory(); // create empty currrent dir\n        break;\n      default:\n        break;\n      }\n    }\n\n    // 3. Do transitions\n    switch(startOpt) {\n    case UPGRADE:\n      doUpgrade(target);\n      return false; // upgrade saved image already\n    case IMPORT:\n      doImportCheckpoint(target);\n      return false; // import checkpoint saved image already\n    case ROLLBACK:\n      doRollback();\n      break;\n    case REGULAR:\n      // just load the image\n    }\n    \n    return loadFSImage(target, recovery);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.doRollback": "  private void doRollback() throws IOException {\n    // Rollback is allowed only if there is \n    // a previous fs states in at least one of the storage directories.\n    // Directories that don't have previous state do not rollback\n    boolean canRollback = false;\n    FSImage prevState = new FSImage(conf);\n    prevState.getStorage().layoutVersion = HdfsConstants.LAYOUT_VERSION;\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      File prevDir = sd.getPreviousDir();\n      if (!prevDir.exists()) {  // use current directory then\n        LOG.info(\"Storage directory \" + sd.getRoot()\n                 + \" does not contain previous fs state.\");\n        // read and verify consistency with other directories\n        storage.readProperties(sd);\n        continue;\n      }\n\n      // read and verify consistency of the prev dir\n      prevState.getStorage().readPreviousVersionProperties(sd);\n\n      if (prevState.getLayoutVersion() != HdfsConstants.LAYOUT_VERSION) {\n        throw new IOException(\n          \"Cannot rollback to storage version \" +\n          prevState.getLayoutVersion() +\n          \" using this version of the NameNode, which uses storage version \" +\n          HdfsConstants.LAYOUT_VERSION + \". \" +\n          \"Please use the previous version of HDFS to perform the rollback.\");\n      }\n      canRollback = true;\n    }\n    if (!canRollback)\n      throw new IOException(\"Cannot rollback. None of the storage \"\n                            + \"directories contain previous fs state.\");\n\n    // Now that we know all directories are going to be consistent\n    // Do rollback for each directory containing previous state\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      File prevDir = sd.getPreviousDir();\n      if (!prevDir.exists())\n        continue;\n\n      LOG.info(\"Rolling back storage directory \" + sd.getRoot()\n               + \".\\n   new LV = \" + prevState.getStorage().getLayoutVersion()\n               + \"; new CTime = \" + prevState.getStorage().getCTime());\n      File tmpDir = sd.getRemovedTmp();\n      assert !tmpDir.exists() : \"removed.tmp directory must not exist.\";\n      // rename current to tmp\n      File curDir = sd.getCurrentDir();\n      assert curDir.exists() : \"Current directory must exist.\";\n      NNStorage.rename(curDir, tmpDir);\n      // rename previous to current\n      NNStorage.rename(prevDir, curDir);\n\n      // delete tmp dir\n      NNStorage.deleteDir(tmpDir);\n      LOG.info(\"Rollback of \" + sd.getRoot()+ \" is complete.\");\n    }\n    isUpgradeFinalized = true;\n    // check whether name-node can start in regular mode\n    storage.verifyDistributedUpgradeProgress(StartupOption.REGULAR);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs": "  private boolean recoverStorageDirs(StartupOption startOpt,\n      Map<StorageDirectory, StorageState> dataDirStates) throws IOException {\n    boolean isFormatted = false;\n    for (Iterator<StorageDirectory> it = \n                      storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      StorageState curState;\n      try {\n        curState = sd.analyzeStorage(startOpt, storage);\n        String nameserviceId = DFSUtil.getNamenodeNameServiceId(conf);\n        if (curState != StorageState.NORMAL && HAUtil.isHAEnabled(conf, nameserviceId)) {\n          throw new IOException(\"Cannot start an HA namenode with name dirs \" +\n              \"that need recovery. Dir: \" + sd + \" state: \" + curState);\n        }\n        // sd is locked but not opened\n        switch(curState) {\n        case NON_EXISTENT:\n          // name-node fails if any of the configured storage dirs are missing\n          throw new InconsistentFSStateException(sd.getRoot(),\n                      \"storage directory does not exist or is not accessible.\");\n        case NOT_FORMATTED:\n          break;\n        case NORMAL:\n          break;\n        default:  // recovery is possible\n          sd.doRecover(curState);      \n        }\n        if (curState != StorageState.NOT_FORMATTED \n            && startOpt != StartupOption.ROLLBACK) {\n          // read and verify consistency with other directories\n          storage.readProperties(sd);\n          isFormatted = true;\n        }\n        if (startOpt == StartupOption.IMPORT && isFormatted)\n          // import of a checkpoint is allowed only into empty image directories\n          throw new IOException(\"Cannot import image from a checkpoint. \" \n              + \" NameNode already contains an image in \" + sd.getRoot());\n      } catch (IOException ioe) {\n        sd.unlock();\n        throw ioe;\n      }\n      dataDirStates.put(sd,curState);\n    }\n    return isFormatted;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade": "  private void doUpgrade(FSNamesystem target) throws IOException {\n    if(storage.getDistributedUpgradeState()) {\n      // only distributed upgrade need to continue\n      // don't do version upgrade\n      this.loadFSImage(target, null);\n      storage.initializeDistributedUpgrade();\n      return;\n    }\n    // Upgrade is allowed only if there are \n    // no previous fs states in any of the directories\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      if (sd.getPreviousDir().exists())\n        throw new InconsistentFSStateException(sd.getRoot(),\n            \"previous fs state should not exist during upgrade. \"\n            + \"Finalize or rollback first.\");\n    }\n\n    // load the latest image\n    this.loadFSImage(target, null);\n\n    // Do upgrade for each directory\n    long oldCTime = storage.getCTime();\n    storage.cTime = now();  // generate new cTime for the state\n    int oldLV = storage.getLayoutVersion();\n    storage.layoutVersion = HdfsConstants.LAYOUT_VERSION;\n    \n    List<StorageDirectory> errorSDs =\n      Collections.synchronizedList(new ArrayList<StorageDirectory>());\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      LOG.info(\"Starting upgrade of image directory \" + sd.getRoot()\n               + \".\\n   old LV = \" + oldLV\n               + \"; old CTime = \" + oldCTime\n               + \".\\n   new LV = \" + storage.getLayoutVersion()\n               + \"; new CTime = \" + storage.getCTime());\n      try {\n        File curDir = sd.getCurrentDir();\n        File prevDir = sd.getPreviousDir();\n        File tmpDir = sd.getPreviousTmp();\n        assert curDir.exists() : \"Current directory must exist.\";\n        assert !prevDir.exists() : \"previous directory must not exist.\";\n        assert !tmpDir.exists() : \"previous.tmp directory must not exist.\";\n        assert !editLog.isSegmentOpen() : \"Edits log must not be open.\";\n\n        // rename current to tmp\n        NNStorage.rename(curDir, tmpDir);\n        \n        if (!curDir.mkdir()) {\n          throw new IOException(\"Cannot create directory \" + curDir);\n        }\n      } catch (Exception e) {\n        LOG.error(\"Failed to move aside pre-upgrade storage \" +\n            \"in image directory \" + sd.getRoot(), e);\n        errorSDs.add(sd);\n        continue;\n      }\n    }\n    storage.reportErrorsOnDirectories(errorSDs);\n    errorSDs.clear();\n\n    saveFSImageInAllDirs(target, editLog.getLastWrittenTxId());\n\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      try {\n        // Write the version file, since saveFsImage above only makes the\n        // fsimage_<txid>, and the directory is otherwise empty.\n        storage.writeProperties(sd);\n        \n        File prevDir = sd.getPreviousDir();\n        File tmpDir = sd.getPreviousTmp();\n        // rename tmp to previous\n        NNStorage.rename(tmpDir, prevDir);\n      } catch (IOException ioe) {\n        LOG.error(\"Unable to rename temp to previous for \" + sd.getRoot(), ioe);\n        errorSDs.add(sd);\n        continue;\n      }\n      LOG.info(\"Upgrade of \" + sd.getRoot() + \" is complete.\");\n    }\n    storage.reportErrorsOnDirectories(errorSDs);\n\n    isUpgradeFinalized = false;\n    if (!storage.getRemovedStorageDirs().isEmpty()) {\n      //during upgrade, it's a fatal error to fail any storage directory\n      throw new IOException(\"Upgrade failed in \"\n          + storage.getRemovedStorageDirs().size()\n          + \" storage directory(ies), previously logged.\");\n    }\n    storage.initializeDistributedUpgrade();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.doImportCheckpoint": "  void doImportCheckpoint(FSNamesystem target) throws IOException {\n    Collection<URI> checkpointDirs =\n      FSImage.getCheckpointDirs(conf, null);\n    List<URI> checkpointEditsDirs =\n      FSImage.getCheckpointEditsDirs(conf, null);\n\n    if (checkpointDirs == null || checkpointDirs.isEmpty()) {\n      throw new IOException(\"Cannot import image from a checkpoint. \"\n                            + \"\\\"dfs.namenode.checkpoint.dir\\\" is not set.\" );\n    }\n    \n    if (checkpointEditsDirs == null || checkpointEditsDirs.isEmpty()) {\n      throw new IOException(\"Cannot import image from a checkpoint. \"\n                            + \"\\\"dfs.namenode.checkpoint.dir\\\" is not set.\" );\n    }\n\n    FSImage realImage = target.getFSImage();\n    FSImage ckptImage = new FSImage(conf, \n                                    checkpointDirs, checkpointEditsDirs);\n    target.dir.fsImage = ckptImage;\n    // load from the checkpoint dirs\n    try {\n      ckptImage.recoverTransitionRead(StartupOption.REGULAR, target, null);\n    } finally {\n      ckptImage.close();\n    }\n    // return back the real image\n    realImage.getStorage().setStorageInfo(ckptImage.getStorage());\n    realImage.getEditLog().setNextTxId(ckptImage.getEditLog().getLastWrittenTxId()+1);\n\n    target.dir.fsImage = realImage;\n    realImage.getStorage().setBlockPoolID(ckptImage.getBlockPoolID());\n\n    // and save it but keep the same checkpointTime\n    saveNamespace(target);\n    getStorage().writeAll();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage": "  void loadFSImage(StartupOption startOpt, FSImage fsImage, boolean haEnabled)\n      throws IOException {\n    // format before starting up if requested\n    if (startOpt == StartupOption.FORMAT) {\n      \n      fsImage.format(this, fsImage.getStorage().determineClusterId());// reuse current id\n\n      startOpt = StartupOption.REGULAR;\n    }\n    boolean success = false;\n    writeLock();\n    try {\n      // We shouldn't be calling saveNamespace if we've come up in standby state.\n      MetaRecoveryContext recovery = startOpt.createRecoveryContext();\n      if (fsImage.recoverTransitionRead(startOpt, this, recovery) && !haEnabled) {\n        fsImage.saveNamespace(this);\n      }\n      // This will start a new log segment and write to the seen_txid file, so\n      // we shouldn't do it when coming up in standby state\n      if (!haEnabled) {\n        fsImage.openEditLogForWrite();\n      }\n      \n      success = true;\n    } finally {\n      if (!success) {\n        fsImage.close();\n      }\n      writeUnlock();\n    }\n    dir.imageLoadComplete();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.close": "  void close() {\n    fsRunning = false;\n    try {\n      stopCommonServices();\n      if (smmthread != null) smmthread.interrupt();\n    } finally {\n      // using finally to ensure we also wait for lease daemon\n      try {\n        stopActiveServices();\n        stopStandbyServices();\n        if (dir != null) {\n          dir.close();\n        }\n      } catch (IOException ie) {\n        LOG.error(\"Error closing FSDirectory\", ie);\n        IOUtils.cleanup(LOG, dir);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock": "  public void writeUnlock() {\n    this.fsLock.writeLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock": "  public void writeLock() {\n    this.fsLock.writeLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveNamespace": "  void saveNamespace() throws AccessControlException, IOException {\n    readLock();\n    try {\n      checkSuperuserPrivilege();\n      if (!isInSafeMode()) {\n        throw new IOException(\"Safe mode should be turned ON \" +\n                              \"in order to create namespace image.\");\n      }\n      getFSImage().saveNamespace(this);\n      LOG.info(\"New namespace image has been created.\");\n    } finally {\n      readUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk": "  public static FSNamesystem loadFromDisk(Configuration conf,\n      Collection<URI> namespaceDirs, List<URI> namespaceEditsDirs)\n      throws IOException {\n\n    if (namespaceDirs.size() == 1) {\n      LOG.warn(\"Only one \" + DFS_NAMENODE_NAME_DIR_KEY\n          + \" directory configured , beware data loss!\");\n    }\n    if (namespaceEditsDirs.size() == 1) {\n      LOG.warn(\"Only one \" + DFS_NAMENODE_EDITS_DIR_KEY\n          + \" directory configured , beware data loss!\");\n    }\n\n    FSImage fsImage = new FSImage(conf, namespaceDirs, namespaceEditsDirs);\n    FSNamesystem namesystem = new FSNamesystem(conf, fsImage);\n    StartupOption startOpt = NameNode.getStartupOption(conf);\n    if (startOpt == StartupOption.RECOVER) {\n      namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n    }\n\n    long loadStart = now();\n    String nameserviceId = DFSUtil.getNamenodeNameServiceId(conf);\n    namesystem.loadFSImage(startOpt, fsImage,\n      HAUtil.isHAEnabled(conf, nameserviceId));\n    long timeTakenToLoadFSImage = now() - loadStart;\n    LOG.info(\"Finished loading FSImage in \" + timeTakenToLoadFSImage + \" msecs\");\n    NameNodeMetrics nnMetrics = NameNode.getNameNodeMetrics();\n    if (nnMetrics != null) {\n      nnMetrics.setFsImageLoadTime((int) timeTakenToLoadFSImage);\n    }\n    return namesystem;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNamespaceEditsDirs": "  public static List<URI> getNamespaceEditsDirs(Configuration conf,\n      boolean includeShared)\n      throws IOException {\n    // Use a LinkedHashSet so that order is maintained while we de-dup\n    // the entries.\n    LinkedHashSet<URI> editsDirs = new LinkedHashSet<URI>();\n    \n    if (includeShared) {\n      List<URI> sharedDirs = getSharedEditsDirs(conf);\n  \n      // Fail until multiple shared edits directories are supported (HDFS-2782)\n      if (sharedDirs.size() > 1) {\n        throw new IOException(\n            \"Multiple shared edits directories are not yet supported\");\n      }\n  \n      // First add the shared edits dirs. It's critical that the shared dirs\n      // are added first, since JournalSet syncs them in the order they are listed,\n      // and we need to make sure all edits are in place in the shared storage\n      // before they are replicated locally. See HDFS-2874.\n      for (URI dir : sharedDirs) {\n        if (!editsDirs.add(dir)) {\n          LOG.warn(\"Edits URI \" + dir + \" listed multiple times in \" + \n              DFS_NAMENODE_SHARED_EDITS_DIR_KEY + \". Ignoring duplicates.\");\n        }\n      }\n    }    \n    // Now add the non-shared dirs.\n    for (URI dir : getStorageDirs(conf, DFS_NAMENODE_EDITS_DIR_KEY)) {\n      if (!editsDirs.add(dir)) {\n        LOG.warn(\"Edits URI \" + dir + \" listed multiple times in \" + \n            DFS_NAMENODE_SHARED_EDITS_DIR_KEY + \" and \" +\n            DFS_NAMENODE_EDITS_DIR_KEY + \". Ignoring duplicates.\");\n      }\n    }\n\n    if (editsDirs.isEmpty()) {\n      // If this is the case, no edit dirs have been explicitly configured.\n      // Image dirs are to be used for edits too.\n      return Lists.newArrayList(getNamespaceDirs(conf));\n    } else {\n      return Lists.newArrayList(editsDirs);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setSafeMode": "  boolean setSafeMode(SafeModeAction action) throws IOException {\n    if (action != SafeModeAction.SAFEMODE_GET) {\n      checkSuperuserPrivilege();\n      switch(action) {\n      case SAFEMODE_LEAVE: // leave safe mode\n        leaveSafeMode(false);\n        break;\n      case SAFEMODE_ENTER: // enter safe mode\n        enterSafeMode(false);\n        break;\n      }\n    }\n    return isInSafeMode();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNamespaceDirs": "  public static Collection<URI> getNamespaceDirs(Configuration conf) {\n    return getStorageDirs(conf, DFS_NAMENODE_NAME_DIR_KEY);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem": "  protected void loadNamesystem(Configuration conf) throws IOException {\n    this.namesystem = FSNamesystem.loadFromDisk(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initialize": "  protected void initialize(Configuration conf) throws IOException {\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    loadNamesystem(conf);\n\n    rpcServer = createRpcServer(conf);\n    \n    try {\n      validateConfigurationSettings(conf);\n    } catch (IOException e) {\n      LOG.fatal(e.toString());\n      throw e;\n    }\n\n    startCommonServices(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getRole": "  public NamenodeRole getRole() {\n    return role;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.startCommonServices": "  private void startCommonServices(Configuration conf) throws IOException {\n    namesystem.startCommonServices(conf, haContext);\n    startHttpServer(conf);\n    rpcServer.start();\n    plugins = conf.getInstances(DFS_NAMENODE_PLUGINS_KEY,\n        ServicePlugin.class);\n    for (ServicePlugin p: plugins) {\n      try {\n        p.start(this);\n      } catch (Throwable t) {\n        LOG.warn(\"ServicePlugin \" + p + \" could not be started\", t);\n      }\n    }\n    LOG.info(getRole() + \" up at: \" + rpcServer.getRpcAddress());\n    if (rpcServer.getServiceRpcAddress() != null) {\n      LOG.info(getRole() + \" service server is up at: \"\n          + rpcServer.getServiceRpcAddress());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initMetrics": "  static void initMetrics(Configuration conf, NamenodeRole role) {\n    metrics = NameNodeMetrics.create(conf, role);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer": "  protected NameNodeRpcServer createRpcServer(Configuration conf)\n      throws IOException {\n    return new NameNodeRpcServer(conf, this);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.loginAsNameNodeUser": "  void loginAsNameNodeUser(Configuration conf) throws IOException {\n    InetSocketAddress socAddr = getRpcServerAddress(conf);\n    SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n        DFS_NAMENODE_USER_NAME_KEY, socAddr.getHostName());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.validateConfigurationSettings": "  protected void validateConfigurationSettings(final Configuration conf) \n      throws IOException {\n    // check to make sure the web port and rpc port do not match \n    if(getHttpServerAddress(conf).getPort() \n        == getRpcServerAddress(conf).getPort()) {\n      String errMsg = \"dfs.namenode.rpc-address \" +\n          \"(\"+ getRpcServerAddress(conf) + \") and \" +\n          \"dfs.namenode.http-address (\"+ getHttpServerAddress(conf) + \") \" +\n          \"configuration keys are bound to the same port, unable to start \" +\n          \"NameNode. Port: \" + getRpcServerAddress(conf).getPort();\n      throw new IOException(errMsg);\n    } \n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf == null)\n      conf = new HdfsConfiguration();\n    StartupOption startOpt = parseArguments(argv);\n    if (startOpt == null) {\n      printUsage();\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n    \n    if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) &&\n        (startOpt == StartupOption.UPGRADE ||\n         startOpt == StartupOption.ROLLBACK ||\n         startOpt == StartupOption.FINALIZE)) {\n      throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n          \"Cannot perform DFS upgrade with HA enabled.\");\n    }\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted = format(conf, startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        System.exit(0);\n        return null;\n      }\n      case FINALIZE: {\n        boolean aborted = finalize(conf, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] = Arrays.copyOfRange(argv, 1, argv.length);\n        int rc = BootstrapStandby.run(toolArgs, conf);\n        System.exit(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted = initializeSharedEdits(conf, false, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role = startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      case RECOVER: {\n        NameNode.doRecovery(startOpt, conf);\n        return null;\n      }\n      default:\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.printUsage": "  private static void printUsage() {\n    System.err.println(\n      \"Usage: java NameNode [\" +\n      StartupOption.BACKUP.getName() + \"] | [\" +\n      StartupOption.CHECKPOINT.getName() + \"] | [\" +\n      StartupOption.FORMAT.getName() + \" [\" + StartupOption.CLUSTERID.getName() +  \n      \" cid ] [\" + StartupOption.FORCE.getName() + \"] [\" +\n      StartupOption.NONINTERACTIVE.getName() + \"] ] | [\" +\n      StartupOption.UPGRADE.getName() + \"] | [\" +\n      StartupOption.ROLLBACK.getName() + \"] | [\" +\n      StartupOption.FINALIZE.getName() + \"] | [\" +\n      StartupOption.IMPORT.getName() + \"] | [\" +\n      StartupOption.INITIALIZESHAREDEDITS.getName() + \"] | [\" +\n      StartupOption.BOOTSTRAPSTANDBY.getName() + \"] | [\" + \n      StartupOption.RECOVER.getName() + \" [ \" +\n        StartupOption.FORCE.getName() + \" ] ]\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits": "  private static boolean initializeSharedEdits(Configuration conf,\n      boolean force, boolean interactive) {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    NNStorage existingStorage = null;\n    try {\n      FSNamesystem fsns = FSNamesystem.loadFromDisk(conf,\n          FSNamesystem.getNamespaceDirs(conf),\n          FSNamesystem.getNamespaceEditsDirs(conf, false));\n      \n      existingStorage = fsns.getFSImage().getStorage();\n      \n      Collection<URI> sharedEditsDirs = FSNamesystem.getSharedEditsDirs(conf);\n      if (!confirmFormat(sharedEditsDirs, force, interactive)) {\n        return true; // aborted\n      }\n      NNStorage newSharedStorage = new NNStorage(conf,\n          Lists.<URI>newArrayList(),\n          sharedEditsDirs);\n      \n      newSharedStorage.format(new NamespaceInfo(\n          existingStorage.getNamespaceID(),\n          existingStorage.getClusterID(),\n          existingStorage.getBlockPoolID(),\n          existingStorage.getCTime(),\n          existingStorage.getDistributedUpgradeVersion()));\n      \n      // Need to make sure the edit log segments are in good shape to initialize\n      // the shared edits dir.\n      fsns.getFSImage().getEditLog().close();\n      fsns.getFSImage().getEditLog().initJournalsForWrite();\n      fsns.getFSImage().getEditLog().recoverUnclosedStreams();\n      \n      if (copyEditLogSegmentsToSharedDir(fsns, sharedEditsDirs,\n          newSharedStorage, conf)) {\n        return true; // aborted\n      }\n    } catch (IOException ioe) {\n      LOG.error(\"Could not initialize shared edits dir\", ioe);\n      return true; // aborted\n    } finally {\n      // Have to unlock storage explicitly for the case when we're running in a\n      // unit test, which runs in the same JVM as NNs.\n      if (existingStorage != null) {\n        try {\n          existingStorage.unlockAll();\n        } catch (IOException ioe) {\n          LOG.warn(\"Could not unlock storage directories\", ioe);\n          return true; // aborted\n        }\n      }\n    }\n    return false; // did not abort\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.setStartupOption": "  private static void setStartupOption(Configuration conf, StartupOption opt) {\n    conf.set(DFS_NAMENODE_STARTUP_KEY, opt.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.parseArguments": "  private static StartupOption parseArguments(String args[]) {\n    int argsLen = (args == null) ? 0 : args.length;\n    StartupOption startOpt = StartupOption.REGULAR;\n    for(int i=0; i < argsLen; i++) {\n      String cmd = args[i];\n      if (StartupOption.FORMAT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.FORMAT;\n        for (i = i + 1; i < argsLen; i++) {\n          if (args[i].equalsIgnoreCase(StartupOption.CLUSTERID.getName())) {\n            i++;\n            if (i >= argsLen) {\n              // if no cluster id specified, return null\n              LOG.fatal(\"Must specify a valid cluster ID after the \"\n                  + StartupOption.CLUSTERID.getName() + \" flag\");\n              return null;\n            }\n            String clusterId = args[i];\n            // Make sure an id is specified and not another flag\n            if (clusterId.isEmpty() ||\n                clusterId.equalsIgnoreCase(StartupOption.FORCE.getName()) ||\n                clusterId.equalsIgnoreCase(\n                    StartupOption.NONINTERACTIVE.getName())) {\n              LOG.fatal(\"Must specify a valid cluster ID after the \"\n                  + StartupOption.CLUSTERID.getName() + \" flag\");\n              return null;\n            }\n            startOpt.setClusterId(clusterId);\n          }\n\n          if (args[i].equalsIgnoreCase(StartupOption.FORCE.getName())) {\n            startOpt.setForceFormat(true);\n          }\n\n          if (args[i].equalsIgnoreCase(StartupOption.NONINTERACTIVE.getName())) {\n            startOpt.setInteractiveFormat(false);\n          }\n        }\n      } else if (StartupOption.GENCLUSTERID.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.GENCLUSTERID;\n      } else if (StartupOption.REGULAR.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.REGULAR;\n      } else if (StartupOption.BACKUP.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.BACKUP;\n      } else if (StartupOption.CHECKPOINT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.CHECKPOINT;\n      } else if (StartupOption.UPGRADE.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.UPGRADE;\n        // might be followed by two args\n        if (i + 2 < argsLen\n            && args[i + 1].equalsIgnoreCase(StartupOption.CLUSTERID.getName())) {\n          i += 2;\n          startOpt.setClusterId(args[i]);\n        }\n      } else if (StartupOption.ROLLBACK.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.ROLLBACK;\n      } else if (StartupOption.FINALIZE.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.FINALIZE;\n      } else if (StartupOption.IMPORT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.IMPORT;\n      } else if (StartupOption.BOOTSTRAPSTANDBY.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.BOOTSTRAPSTANDBY;\n        return startOpt;\n      } else if (StartupOption.INITIALIZESHAREDEDITS.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.INITIALIZESHAREDEDITS;\n        return startOpt;\n      } else if (StartupOption.RECOVER.getName().equalsIgnoreCase(cmd)) {\n        if (startOpt != StartupOption.REGULAR) {\n          throw new RuntimeException(\"Can't combine -recover with \" +\n              \"other startup options.\");\n        }\n        startOpt = StartupOption.RECOVER;\n        while (++i < argsLen) {\n          if (args[i].equalsIgnoreCase(\n                StartupOption.FORCE.getName())) {\n            startOpt.setForce(MetaRecoveryContext.FORCE_FIRST_CHOICE);\n          } else {\n            throw new RuntimeException(\"Error parsing recovery options: \" + \n              \"can't understand option \\\"\" + args[i] + \"\\\"\");\n          }\n        }\n      } else {\n        return null;\n      }\n    }\n    return startOpt;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.finalize": "  private static boolean finalize(Configuration conf,\n                               boolean isConfirmationNeeded\n                               ) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n\n    FSNamesystem nsys = new FSNamesystem(conf, new FSImage(conf));\n    System.err.print(\n        \"\\\"finalize\\\" will remove the previous state of the files system.\\n\"\n        + \"Recent upgrade will become permanent.\\n\"\n        + \"Rollback option will not be available anymore.\\n\");\n    if (isConfirmationNeeded) {\n      if (!confirmPrompt(\"Finalize filesystem state?\")) {\n        System.err.println(\"Finalize aborted.\");\n        return true;\n      }\n    }\n    nsys.dir.fsImage.finalizeUpgrade();\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.doRecovery": "  private static void doRecovery(StartupOption startOpt, Configuration conf)\n      throws IOException {\n    if (startOpt.getForce() < MetaRecoveryContext.FORCE_ALL) {\n      if (!confirmPrompt(\"You have selected Metadata Recovery mode.  \" +\n          \"This mode is intended to recover lost metadata on a corrupt \" +\n          \"filesystem.  Metadata recovery mode often permanently deletes \" +\n          \"data from your HDFS filesystem.  Please back up your edit log \" +\n          \"and fsimage before trying this!\\n\\n\" +\n          \"Are you ready to proceed? (Y/N)\\n\")) {\n        System.err.println(\"Recovery aborted at user request.\\n\");\n        return;\n      }\n    }\n    MetaRecoveryContext.LOG.info(\"starting recovery...\");\n    UserGroupInformation.setConfiguration(conf);\n    NameNode.initMetrics(conf, startOpt.toNodeRole());\n    FSNamesystem fsn = null;\n    try {\n      fsn = FSNamesystem.loadFromDisk(conf);\n      fsn.saveNamespace();\n      MetaRecoveryContext.LOG.info(\"RECOVERY COMPLETE\");\n    } catch (IOException e) {\n      MetaRecoveryContext.LOG.info(\"RECOVERY FAILED: caught exception\", e);\n      throw e;\n    } catch (RuntimeException e) {\n      MetaRecoveryContext.LOG.info(\"RECOVERY FAILED: caught exception\", e);\n      throw e;\n    } finally {\n      if (fsn != null)\n        fsn.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.format": "  private static boolean format(Configuration conf, boolean force,\n      boolean isInteractive) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    checkAllowFormat(conf);\n    \n    Collection<URI> nameDirsToFormat = FSNamesystem.getNamespaceDirs(conf);\n    List<URI> sharedDirs = FSNamesystem.getSharedEditsDirs(conf);\n    List<URI> dirsToPrompt = new ArrayList<URI>();\n    dirsToPrompt.addAll(nameDirsToFormat);\n    dirsToPrompt.addAll(sharedDirs);\n    List<URI> editDirsToFormat = \n                 FSNamesystem.getNamespaceEditsDirs(conf);\n    if (!confirmFormat(dirsToPrompt, force, isInteractive)) {\n      return true; // aborted\n    }\n\n    // if clusterID is not provided - see if you can find the current one\n    String clusterId = StartupOption.FORMAT.getClusterId();\n    if(clusterId == null || clusterId.equals(\"\")) {\n      //Generate a new cluster id\n      clusterId = NNStorage.newClusterID();\n    }\n    System.out.println(\"Formatting using clusterid: \" + clusterId);\n    \n    FSImage fsImage = new FSImage(conf, nameDirsToFormat, editDirsToFormat);\n    FSNamesystem fsn = new FSNamesystem(conf, fsImage);\n    fsImage.format(fsn, clusterId);\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.main": "  public static void main(String argv[]) throws Exception {\n    try {\n      StringUtils.startupShutdownMessage(NameNode.class, argv, LOG);\n      NameNode namenode = createNameNode(argv, null);\n      if (namenode != null)\n        namenode.join();\n    } catch (Throwable e) {\n      LOG.error(\"Exception in namenode join\", e);\n      System.exit(-1);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.join": "  public void join() {\n    try {\n      this.rpcServer.join();\n    } catch (InterruptedException ie) {\n      LOG.info(\"Caught interrupted exception \", ie);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.closeAllStreams": "  static void closeAllStreams(Iterable<EditLogInputStream> streams) {\n    for (EditLogInputStream s : streams) {\n      IOUtils.closeStream(s);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getCheckpointTxId": "    public long getCheckpointTxId() {\n      return txId;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getLatestImage": "  abstract FSImageFile getLatestImage() throws IOException;\n\n  /** \n   * Get the minimum tx id which should be loaded with this set of images.\n   */\n  abstract long getMaxSeenTxId();\n\n  /**\n   * @return true if the directories are in such a state that the image should be re-saved\n   * following the load\n   */\n  abstract boolean needToSave();\n\n  /**\n   * Record of an image that has been located and had its filename parsed.\n   */\n  static class FSImageFile {\n    final StorageDirectory sd;    \n    final long txId;\n    private final File file;\n    \n    FSImageFile(StorageDirectory sd, File file, long txId) {\n      assert txId >= 0 || txId == HdfsConstants.INVALID_TXID \n        : \"Invalid txid on \" + file +\": \" + txId;\n      \n      this.sd = sd;\n      this.txId = txId;\n      this.file = file;\n    } \n    \n    File getFile() {\n      return file;\n    }\n\n    public long getCheckpointTxId() {\n      return txId;\n    }\n    \n    @Override\n    public String toString() {\n      return String.format(\"FSImageFile(file=%s, cpktTxId=%019d)\", \n                           file.toString(), txId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getFile": "    File getFile() {\n      return file;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.needToSave": "  abstract boolean needToSave();\n\n  /**\n   * Record of an image that has been located and had its filename parsed.\n   */\n  static class FSImageFile {\n    final StorageDirectory sd;    \n    final long txId;\n    private final File file;\n    \n    FSImageFile(StorageDirectory sd, File file, long txId) {\n      assert txId >= 0 || txId == HdfsConstants.INVALID_TXID \n        : \"Invalid txid on \" + file +\": \" + txId;\n      \n      this.sd = sd;\n      this.txId = txId;\n      this.file = file;\n    } \n    \n    File getFile() {\n      return file;\n    }\n\n    public long getCheckpointTxId() {\n      return txId;\n    }\n    \n    @Override\n    public String toString() {\n      return String.format(\"FSImageFile(file=%s, cpktTxId=%019d)\", \n                           file.toString(), txId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getMaxSeenTxId": "  abstract long getMaxSeenTxId();\n\n  /**\n   * @return true if the directories are in such a state that the image should be re-saved\n   * following the load\n   */\n  abstract boolean needToSave();\n\n  /**\n   * Record of an image that has been located and had its filename parsed.\n   */\n  static class FSImageFile {\n    final StorageDirectory sd;    \n    final long txId;\n    private final File file;\n    \n    FSImageFile(StorageDirectory sd, File file, long txId) {\n      assert txId >= 0 || txId == HdfsConstants.INVALID_TXID \n        : \"Invalid txid on \" + file +\": \" + txId;\n      \n      this.sd = sd;\n      this.txId = txId;\n      this.file = file;\n    } \n    \n    File getFile() {\n      return file;\n    }\n\n    public long getCheckpointTxId() {\n      return txId;\n    }\n    \n    @Override\n    public String toString() {\n      return String.format(\"FSImageFile(file=%s, cpktTxId=%019d)\", \n                           file.toString(), txId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.format": "  void format(FSNamesystem fsn, String clusterId) throws IOException {\n    long fileCount = fsn.getTotalFiles();\n    // Expect 1 file, which is the root inode\n    Preconditions.checkState(fileCount == 1,\n        \"FSImage.format should be called with an uninitialized namesystem, has \" +\n        fileCount + \" files\");\n    NamespaceInfo ns = NNStorage.newNamespaceInfo();\n    ns.clusterID = clusterId;\n    storage.format(ns);\n    saveFSImageInAllDirs(fsn, 0);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImageInAllDirs": "  protected synchronized void saveFSImageInAllDirs(FSNamesystem source, long txid)\n      throws IOException {    \n    if (storage.getNumStorageDirs(NameNodeDirType.IMAGE) == 0) {\n      throw new IOException(\"No image directories available!\");\n    }\n    \n    SaveNamespaceContext ctx = new SaveNamespaceContext(\n        source, txid);\n    curSaveNamespaceContext = ctx;\n    \n    try {\n      List<Thread> saveThreads = new ArrayList<Thread>();\n      // save images into current\n      for (Iterator<StorageDirectory> it\n             = storage.dirIterator(NameNodeDirType.IMAGE); it.hasNext();) {\n        StorageDirectory sd = it.next();\n        FSImageSaver saver = new FSImageSaver(ctx, sd);\n        Thread saveThread = new Thread(saver, saver.toString());\n        saveThreads.add(saveThread);\n        saveThread.start();\n      }\n      waitForThreads(saveThreads);\n      saveThreads.clear();\n      storage.reportErrorsOnDirectories(ctx.getErrorSDs());\n  \n      if (storage.getNumStorageDirs(NameNodeDirType.IMAGE) == 0) {\n        throw new IOException(\n          \"Failed to save in any storage directories while saving namespace.\");\n      }\n      if (ctx.isCancelled()) {\n        deleteCancelledCheckpoint(txid);\n        ctx.checkCancelled(); // throws\n        assert false : \"should have thrown above!\";\n      }\n  \n      renameCheckpoint(txid);\n  \n      // Since we now have a new checkpoint, we can clean up some\n      // old edit logs and checkpoints.\n      purgeOldStorage();\n    } finally {\n      // Notify any threads waiting on the checkpoint to be canceled\n      // that it is complete.\n      ctx.markComplete();\n      ctx = null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.openEditLogForWrite": "  void openEditLogForWrite() throws IOException {\n    assert editLog != null : \"editLog must be initialized\";\n    editLog.openForWrite();\n    storage.writeTransactionIdFileToStorage(editLog.getCurSegmentTxId());\n  };",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getStorage": "  public NNStorage getStorage() {\n    return storage;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getStartupOption": "  static StartupOption getStartupOption(Configuration conf) {\n    return StartupOption.valueOf(conf.get(DFS_NAMENODE_STARTUP_KEY,\n                                          StartupOption.REGULAR.toString()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getNameNodeMetrics": "  public static NameNodeMetrics getNameNodeMetrics() {\n    return metrics;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.newClusterID": "  public static String newClusterID() {\n    return \"CID-\" + UUID.randomUUID().toString();\n  }"
        },
        "bug_report": {
            "Title": "During NameNode starting up, it may pick wrong storage directory inspector when the layout versions of the storage directories are different",
            "Description": "Scenario:\n=========\nstart Namenode and datanode by configuring three storage dir's for namenode\nwrite 10 files\nedit version file of one of the storage dir and give layout version as 123 which different with default(-40).\nStop namenode\nstart Namenode.\n\n\nThen I am getting follwong exception...\n\n\n{noformat}\n2012-05-13 19:01:41,483 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.namenode.NNStorage.getStorageFile(NNStorage.java:686)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditsInStorageDir(FSImagePreTransactionalStorageInspector.java:243)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getLatestEditsFiles(FSImagePreTransactionalStorageInspector.java:261)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditLogStreams(FSImagePreTransactionalStorageInspector.java:276)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:596)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:247)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:498)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:390)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:354)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:368)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:402)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:564)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:545)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1093)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1151)\n2012-05-13 19:01:41,485 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: \n\n{noformat}\n"
        }
    },
    {
        "filename": "HDFS-2245.json",
        "creation_time": "2011-08-10T22:55:33.000+0000",
        "stack_trace": "```\njava.io.IOException: java.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1225)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1455)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:786)\n```",
        "source_code": {
            "hdfs.src.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget": "  public DatanodeDescriptor[] chooseTarget(final String src,\n      final int numOfReplicas, final DatanodeDescriptor client,\n      final HashMap<Node, Node> excludedNodes,\n      final long blocksize) throws IOException {\n    // choose targets for the new block to be allocated.\n    final DatanodeDescriptor targets[] = blockplacement.chooseTarget(\n        src, numOfReplicas, client, excludedNodes, blocksize);\n    if (targets.length < minReplication) {\n      throw new IOException(\"File \" + src + \" could only be replicated to \" +\n                            targets.length + \" nodes, instead of \" +\n                            minReplication + \". There are \"\n                            + getDatanodeManager().getNetworkTopology().getNumOfLeaves()\n                            + \" datanode(s) running but \"+excludedNodes.size() +\n                            \" node(s) are excluded in this operation.\");\n    }\n    return targets;\n  }",
            "hdfs.src.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.getDatanodeManager": "  public DatanodeManager getDatanodeManager() {\n    return datanodeManager;\n  }",
            "hdfs.src.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock": "  LocatedBlock getAdditionalBlock(String src,\n                                         String clientName,\n                                         ExtendedBlock previous,\n                                         HashMap<Node, Node> excludedNodes\n                                         ) \n      throws LeaseExpiredException, NotReplicatedYetException,\n      QuotaExceededException, SafeModeException, UnresolvedLinkException,\n      IOException {\n    checkBlock(previous);\n    long fileLength, blockSize;\n    int replication;\n    DatanodeDescriptor clientNode = null;\n    Block newBlock = null;\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\n          \"BLOCK* NameSystem.getAdditionalBlock: file \"\n          +src+\" for \"+clientName);\n    }\n\n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot add block to \" + src, safeMode);\n      }\n\n      // have we exceeded the configured limit of fs objects.\n      checkFsObjectLimit();\n\n      INodeFileUnderConstruction pendingFile  = checkLease(src, clientName);\n\n      // commit the last block and complete it if it has minimum replicas\n      blockManager.commitOrCompleteLastBlock(pendingFile, ExtendedBlock\n          .getLocalBlock(previous));\n\n      //\n      // If we fail this, bad things happen!\n      //\n      if (!checkFileProgress(pendingFile, false)) {\n        throw new NotReplicatedYetException(\"Not replicated yet:\" + src);\n      }\n      fileLength = pendingFile.computeContentSummary().getLength();\n      blockSize = pendingFile.getPreferredBlockSize();\n      clientNode = pendingFile.getClientNode();\n      replication = (int)pendingFile.getReplication();\n    } finally {\n      writeUnlock();\n    }\n\n    // choose targets for the new block to be allocated.\n    final DatanodeDescriptor targets[] = blockManager.chooseTarget(\n        src, replication, clientNode, excludedNodes, blockSize);\n\n    // Allocate a new block and record it in the INode. \n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot add block to \" + src, safeMode);\n      }\n      INode[] pathINodes = dir.getExistingPathINodes(src);\n      int inodesLen = pathINodes.length;\n      checkLease(src, clientName, pathINodes[inodesLen-1]);\n      INodeFileUnderConstruction pendingFile  = (INodeFileUnderConstruction) \n                                                pathINodes[inodesLen - 1];\n                                                           \n      if (!checkFileProgress(pendingFile, false)) {\n        throw new NotReplicatedYetException(\"Not replicated yet:\" + src);\n      }\n\n      // allocate new block record block locations in INode.\n      newBlock = allocateBlock(src, pathINodes, targets);\n      \n      for (DatanodeDescriptor dn : targets) {\n        dn.incBlocksScheduled();\n      }      \n    } finally {\n      writeUnlock();\n    }\n\n    // Create next block\n    LocatedBlock b = new LocatedBlock(getExtendedBlock(newBlock), targets, fileLength);\n    blockManager.setBlockToken(b, BlockTokenSecretManager.AccessMode.WRITE);\n    return b;\n  }",
            "hdfs.src.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease": "  private void checkLease(String src, String holder, INode file)\n      throws LeaseExpiredException {\n    assert hasReadOrWriteLock();\n    if (file == null || file.isDirectory()) {\n      Lease lease = leaseManager.getLease(holder);\n      throw new LeaseExpiredException(\"No lease on \" + src +\n                                      \" File does not exist. \" +\n                                      (lease != null ? lease.toString() :\n                                       \"Holder \" + holder + \n                                       \" does not have any open files.\"));\n    }\n    if (!file.isUnderConstruction()) {\n      Lease lease = leaseManager.getLease(holder);\n      throw new LeaseExpiredException(\"No lease on \" + src + \n                                      \" File is not open for writing. \" +\n                                      (lease != null ? lease.toString() :\n                                       \"Holder \" + holder + \n                                       \" does not have any open files.\"));\n    }\n    INodeFileUnderConstruction pendingFile = (INodeFileUnderConstruction)file;\n    if (holder != null && !pendingFile.getClientName().equals(holder)) {\n      throw new LeaseExpiredException(\"Lease mismatch on \" + src + \" owned by \"\n          + pendingFile.getClientName() + \" but is accessed by \" + holder);\n    }\n  }",
            "hdfs.src.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock": "  public void writeUnlock() {\n    this.fsLock.writeLock().unlock();\n  }",
            "hdfs.src.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getExtendedBlock": "  ExtendedBlock getExtendedBlock(Block blk) {\n    return new ExtendedBlock(blockPoolId, blk);\n  }",
            "hdfs.src.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkFileProgress": "  boolean checkFileProgress(INodeFile v, boolean checkall) {\n    readLock();\n    try {\n      if (checkall) {\n        //\n        // check all blocks of the file.\n        //\n        for (BlockInfo block: v.getBlocks()) {\n          if (!block.isComplete()) {\n            LOG.info(\"BLOCK* NameSystem.checkFileProgress: \"\n                + \"block \" + block + \" has not reached minimal replication \"\n                + blockManager.minReplication);\n            return false;\n          }\n        }\n      } else {\n        //\n        // check the penultimate block of this file\n        //\n        BlockInfo b = v.getPenultimateBlock();\n        if (b != null && !b.isComplete()) {\n          LOG.info(\"BLOCK* NameSystem.checkFileProgress: \"\n              + \"block \" + b + \" has not reached minimal replication \"\n              + blockManager.minReplication);\n          return false;\n        }\n      }\n      return true;\n    } finally {\n      readUnlock();\n    }\n  }",
            "hdfs.src.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.isInSafeMode": "  public boolean isInSafeMode() {\n    // safeMode is volatile, and may be set to null at any time\n    SafeModeInfo safeMode = this.safeMode;\n    if (safeMode == null)\n      return false;\n    return safeMode.isOn();\n  }",
            "hdfs.src.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPreferredBlockSize": "  long getPreferredBlockSize(String filename) \n      throws IOException, UnresolvedLinkException {\n    readLock();\n    try {\n      if (isPermissionEnabled) {\n        checkTraverse(filename);\n      }\n      return dir.getPreferredBlockSize(filename);\n    } finally {\n      readUnlock();\n    }\n  }",
            "hdfs.src.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock": "  public void writeLock() {\n    this.fsLock.writeLock().lock();\n  }",
            "hdfs.src.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.allocateBlock": "  private Block allocateBlock(String src, INode[] inodes,\n      DatanodeDescriptor targets[]) throws QuotaExceededException {\n    assert hasWriteLock();\n    Block b = new Block(DFSUtil.getRandom().nextLong(), 0, 0); \n    while(isValidBlock(b)) {\n      b.setBlockId(DFSUtil.getRandom().nextLong());\n    }\n    b.setGenerationStamp(getGenerationStamp());\n    b = dir.addBlock(src, inodes, b, targets);\n    NameNode.stateChangeLog.info(\"BLOCK* NameSystem.allocateBlock: \"\n                                 +src+ \". \" + blockPoolId + \" \"+ b);\n    return b;\n  }",
            "hdfs.src.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkBlock": "  private void checkBlock(ExtendedBlock block) throws IOException {\n    if (block != null && !this.blockPoolId.equals(block.getBlockPoolId())) {\n      throw new IOException(\"Unexpected BlockPoolId \" + block.getBlockPoolId()\n          + \" - expected \" + blockPoolId);\n    }\n  }",
            "hdfs.src.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkFsObjectLimit": "  void checkFsObjectLimit() throws IOException {\n    if (maxFsObjects != 0 &&\n        maxFsObjects <= dir.totalInodes() + getBlocksTotal()) {\n      throw new IOException(\"Exceeded the configured number of objects \" +\n                             maxFsObjects + \" in the filesystem.\");\n    }\n  }",
            "hdfs.src.java.org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock": "  public LocatedBlock addBlock(String src,\n                               String clientName,\n                               ExtendedBlock previous,\n                               DatanodeInfo[] excludedNodes)\n      throws IOException {\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \"\n          +src+\" for \"+clientName);\n    }\n    HashMap<Node, Node> excludedNodesSet = null;\n    if (excludedNodes != null) {\n      excludedNodesSet = new HashMap<Node, Node>(excludedNodes.length);\n      for (Node node:excludedNodes) {\n        excludedNodesSet.put(node, node);\n      }\n    }\n    LocatedBlock locatedBlock = \n      namesystem.getAdditionalBlock(src, clientName, previous, excludedNodesSet);\n    if (locatedBlock != null)\n      metrics.incrAddBlockOps();\n    return locatedBlock;\n  }",
            "hdfs.src.java.org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.getClientNode": "  DatanodeDescriptor getClientNode() {\n    return clientNode;\n  }"
        },
        "bug_report": {
            "Title": "BlockManager.chooseTarget(..) throws NPE",
            "Description": "{noformat}\n2011-08-10 20:20:51,350 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 8020, call: addBlock(/user/had\noopqa/passwd.1108102020.<NN hostname>.txt, DFSClient_NONMAPREDUCE_1875954430_1, null, null), rpc\n version=1, client version=68, methodsFingerPrint=-1239577025 from <gateway>:38874, error:\njava.io.IOException: java.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1225)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1455)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:786)\n        ...\n{noformat}\n"
        }
    },
    {
        "filename": "HDFS-10320.json",
        "creation_time": "2016-04-20T23:02:09.000+0000",
        "stack_trace": "```\norg.apache.hadoop.net.NetworkTopology$InvalidTopologyException: Failed to find datanode (scope=\"\" excludedScope=\"/rack_a5\").\n\tat org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:729)\n\tat org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:694)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:635)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:580)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:348)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:214)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:111)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.chooseTargets(BlockManager.java:3746)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.access$200(BlockManager.java:3711)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1400)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1306)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3682)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3634)\n\tat java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetworkTopology.chooseRandom": "  private Node chooseRandom(String scope, String excludedScope){\n    if (excludedScope != null) {\n      if (scope.startsWith(excludedScope)) {\n        return null;\n      }\n      if (!excludedScope.startsWith(scope)) {\n        excludedScope = null;\n      }\n    }\n    Node node = getNode(scope);\n    if (!(node instanceof InnerNode)) {\n      return node;\n    }\n    InnerNode innerNode = (InnerNode)node;\n    int numOfDatanodes = innerNode.getNumOfLeaves();\n    if (excludedScope == null) {\n      node = null;\n    } else {\n      node = getNode(excludedScope);\n      if (!(node instanceof InnerNode)) {\n        numOfDatanodes -= 1;\n      } else {\n        numOfDatanodes -= ((InnerNode)node).getNumOfLeaves();\n      }\n    }\n    if (numOfDatanodes == 0) {\n      throw new InvalidTopologyException(\n          \"Failed to find datanode (scope=\\\"\" + String.valueOf(scope) +\n          \"\\\" excludedScope=\\\"\" + String.valueOf(excludedScope) + \"\\\").\");\n    }\n    int leaveIndex = r.nextInt(numOfDatanodes);\n    return innerNode.getLeaf(leaveIndex, node);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetworkTopology.getLeaf": "    Node getLeaf(int leafIndex, Node excludedNode) {\n      int count=0;\n      // check if the excluded node a leaf\n      boolean isLeaf =\n        excludedNode == null || !(excludedNode instanceof InnerNode);\n      // calculate the total number of excluded leaf nodes\n      int numOfExcludedLeaves =\n        isLeaf ? 1 : ((InnerNode)excludedNode).getNumOfLeaves();\n      if (isLeafParent()) { // children are leaves\n        if (isLeaf) { // excluded node is a leaf node\n          if (excludedNode != null &&\n              childrenMap.containsKey(excludedNode.getName())) {\n            int excludedIndex = children.indexOf(excludedNode);\n            if (excludedIndex != -1 && leafIndex >= 0) {\n              // excluded node is one of the children so adjust the leaf index\n              leafIndex = leafIndex>=excludedIndex ? leafIndex+1 : leafIndex;\n            }\n          }\n        }\n        // range check\n        if (leafIndex<0 || leafIndex>=this.getNumOfChildren()) {\n          return null;\n        }\n        return children.get(leafIndex);\n      } else {\n        for(int i=0; i<children.size(); i++) {\n          InnerNode child = (InnerNode)children.get(i);\n          if (excludedNode == null || excludedNode != child) {\n            // not the excludedNode\n            int numOfLeaves = child.getNumOfLeaves();\n            if (excludedNode != null && child.isAncestor(excludedNode)) {\n              numOfLeaves -= numOfExcludedLeaves;\n            }\n            if (count+numOfLeaves > leafIndex) {\n              // the leaf is in the child subtree\n              return child.getLeaf(leafIndex-count, excludedNode);\n            } else {\n              // go to the next child\n              count = count+numOfLeaves;\n            }\n          } else { // it is the excluededNode\n            // skip it and set the excludedNode to be null\n            excludedNode = null;\n          }\n        }\n        return null;\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetworkTopology.getNode": "  public Node getNode(String loc) {\n    netlock.readLock().lock();\n    try {\n      loc = NodeBase.normalize(loc);\n      if (!NodeBase.ROOT.equals(loc))\n        loc = loc.substring(1);\n      return clusterMap.getLoc(loc);\n    } finally {\n      netlock.readLock().unlock();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetworkTopology.getNumOfLeaves": "  public int getNumOfLeaves() {\n    netlock.readLock().lock();\n    try {\n      return clusterMap.getNumOfLeaves();\n    } finally {\n      netlock.readLock().unlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom": "  protected DatanodeStorageInfo chooseRandom(int numOfReplicas,\n                            String scope,\n                            Set<Node> excludedNodes,\n                            long blocksize,\n                            int maxNodesPerRack,\n                            List<DatanodeStorageInfo> results,\n                            boolean avoidStaleNodes,\n                            EnumMap<StorageType, Integer> storageTypes)\n                            throws NotEnoughReplicasException {\n\n    int numOfAvailableNodes = clusterMap.countNumOfAvailableNodes(\n        scope, excludedNodes);\n    int refreshCounter = numOfAvailableNodes;\n    StringBuilder builder = null;\n    if (LOG.isDebugEnabled()) {\n      builder = debugLoggingBuilder.get();\n      builder.setLength(0);\n      builder.append(\"[\");\n    }\n    boolean badTarget = false;\n    DatanodeStorageInfo firstChosen = null;\n    while(numOfReplicas > 0 && numOfAvailableNodes > 0) {\n      DatanodeDescriptor chosenNode = chooseDataNode(scope);\n      if (excludedNodes.add(chosenNode)) { //was not in the excluded list\n        if (LOG.isDebugEnabled()) {\n          builder.append(\"\\nNode \").append(NodeBase.getPath(chosenNode)).append(\" [\");\n        }\n        numOfAvailableNodes--;\n        DatanodeStorageInfo storage = null;\n        if (isGoodDatanode(chosenNode, maxNodesPerRack, considerLoad,\n            results, avoidStaleNodes)) {\n          for (Iterator<Map.Entry<StorageType, Integer>> iter = storageTypes\n              .entrySet().iterator(); iter.hasNext(); ) {\n            Map.Entry<StorageType, Integer> entry = iter.next();\n            storage = chooseStorage4Block(\n                chosenNode, blocksize, results, entry.getKey());\n            if (storage != null) {\n              numOfReplicas--;\n              if (firstChosen == null) {\n                firstChosen = storage;\n              }\n              // add node and related nodes to excludedNode\n              numOfAvailableNodes -=\n                  addToExcludedNodes(chosenNode, excludedNodes);\n              int num = entry.getValue();\n              if (num == 1) {\n                iter.remove();\n              } else {\n                entry.setValue(num - 1);\n              }\n              break;\n            }\n          }\n        }\n\n        if (LOG.isDebugEnabled()) {\n          builder.append(\"\\n]\");\n        }\n\n        // If no candidate storage was found on this DN then set badTarget.\n        badTarget = (storage == null);\n      }\n      // Refresh the node count. If the live node count became smaller,\n      // but it is not reflected in this loop, it may loop forever in case\n      // the replicas/rack cannot be satisfied.\n      if (--refreshCounter == 0) {\n        numOfAvailableNodes = clusterMap.countNumOfAvailableNodes(scope,\n            excludedNodes);\n        refreshCounter = numOfAvailableNodes;\n      }\n    }\n      \n    if (numOfReplicas>0) {\n      String detail = enableDebugLogging;\n      if (LOG.isDebugEnabled()) {\n        if (badTarget && builder != null) {\n          detail = builder.toString();\n          builder.setLength(0);\n        } else {\n          detail = \"\";\n        }\n      }\n      throw new NotEnoughReplicasException(detail);\n    }\n    \n    return firstChosen;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseStorage4Block": "  DatanodeStorageInfo chooseStorage4Block(DatanodeDescriptor dnd,\n      long blockSize,\n      List<DatanodeStorageInfo> results,\n      StorageType storageType) {\n    DatanodeStorageInfo storage =\n        dnd.chooseStorage4Block(storageType, blockSize);\n    if (storage != null) {\n      results.add(storage);\n    } else {\n      logNodeIsNotChosen(dnd, \"no good storage to place the block \");\n    }\n    return storage;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.addToExcludedNodes": "  protected int addToExcludedNodes(DatanodeDescriptor localMachine,\n      Set<Node> excludedNodes) {\n    return excludedNodes.add(localMachine) ? 1 : 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.isGoodDatanode": "  boolean isGoodDatanode(DatanodeDescriptor node,\n                         int maxTargetPerRack, boolean considerLoad,\n                         List<DatanodeStorageInfo> results,\n                         boolean avoidStaleNodes) {\n    // check if the node is (being) decommissioned\n    if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n      logNodeIsNotChosen(node, \"the node is (being) decommissioned \");\n      return false;\n    }\n\n    if (avoidStaleNodes) {\n      if (node.isStale(this.staleInterval)) {\n        logNodeIsNotChosen(node, \"the node is stale \");\n        return false;\n      }\n    }\n\n    // check the communication traffic of the target machine\n    if (considerLoad) {\n      final double maxLoad = considerLoadFactor *\n          stats.getInServiceXceiverAverage();\n      final int nodeLoad = node.getXceiverCount();\n      if (nodeLoad > maxLoad) {\n        logNodeIsNotChosen(node, \"the node is too busy (load: \" + nodeLoad\n            + \" > \" + maxLoad + \") \");\n        return false;\n      }\n    }\n      \n    // check if the target rack has chosen too many nodes\n    String rackname = node.getNetworkLocation();\n    int counter=1;\n    for(DatanodeStorageInfo resultStorage : results) {\n      if (rackname.equals(\n          resultStorage.getDatanodeDescriptor().getNetworkLocation())) {\n        counter++;\n      }\n    }\n    if (counter > maxTargetPerRack) {\n      logNodeIsNotChosen(node, \"the rack has too many chosen nodes \");\n      return false;\n    }\n\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseDataNode": "  protected DatanodeDescriptor chooseDataNode(final String scope) {\n    return (DatanodeDescriptor) clusterMap.chooseRandom(scope);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack": "  protected void chooseRemoteRack(int numOfReplicas,\n                                DatanodeDescriptor localMachine,\n                                Set<Node> excludedNodes,\n                                long blocksize,\n                                int maxReplicasPerRack,\n                                List<DatanodeStorageInfo> results,\n                                boolean avoidStaleNodes,\n                                EnumMap<StorageType, Integer> storageTypes)\n                                    throws NotEnoughReplicasException {\n    int oldNumOfReplicas = results.size();\n    // randomly choose one node from remote racks\n    try {\n      chooseRandom(numOfReplicas, \"~\" + localMachine.getNetworkLocation(),\n          excludedNodes, blocksize, maxReplicasPerRack, results,\n          avoidStaleNodes, storageTypes);\n    } catch (NotEnoughReplicasException e) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Failed to choose remote rack (location = ~\"\n            + localMachine.getNetworkLocation() + \"), fallback to local rack\", e);\n      }\n      chooseRandom(numOfReplicas-(results.size()-oldNumOfReplicas),\n                   localMachine.getNetworkLocation(), excludedNodes, blocksize, \n                   maxReplicasPerRack, results, avoidStaleNodes, storageTypes);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget": "  private Node chooseTarget(int numOfReplicas,\n                            Node writer,\n                            final Set<Node> excludedNodes,\n                            final long blocksize,\n                            final int maxNodesPerRack,\n                            final List<DatanodeStorageInfo> results,\n                            final boolean avoidStaleNodes,\n                            final BlockStoragePolicy storagePolicy,\n                            final EnumSet<StorageType> unavailableStorages,\n                            final boolean newBlock) {\n    if (numOfReplicas == 0 || clusterMap.getNumOfLeaves()==0) {\n      return (writer instanceof DatanodeDescriptor) ? writer : null;\n    }\n    final int numOfResults = results.size();\n    final int totalReplicasExpected = numOfReplicas + numOfResults;\n    if ((writer == null || !(writer instanceof DatanodeDescriptor)) && !newBlock) {\n      writer = results.get(0).getDatanodeDescriptor();\n    }\n\n    // Keep a copy of original excludedNodes\n    final Set<Node> oldExcludedNodes = new HashSet<>(excludedNodes);\n\n    // choose storage types; use fallbacks for unavailable storages\n    final List<StorageType> requiredStorageTypes = storagePolicy\n        .chooseStorageTypes((short) totalReplicasExpected,\n            DatanodeStorageInfo.toStorageTypes(results),\n            unavailableStorages, newBlock);\n    final EnumMap<StorageType, Integer> storageTypes =\n        getRequiredStorageTypes(requiredStorageTypes);\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"storageTypes=\" + storageTypes);\n    }\n\n    try {\n      if ((numOfReplicas = requiredStorageTypes.size()) == 0) {\n        throw new NotEnoughReplicasException(\n            \"All required storage types are unavailable: \"\n            + \" unavailableStorages=\" + unavailableStorages\n            + \", storagePolicy=\" + storagePolicy);\n      }\n      writer = chooseTargetInOrder(numOfReplicas, writer, excludedNodes, blocksize,\n          maxNodesPerRack, results, avoidStaleNodes, newBlock, storageTypes);\n    } catch (NotEnoughReplicasException e) {\n      final String message = \"Failed to place enough replicas, still in need of \"\n          + (totalReplicasExpected - results.size()) + \" to reach \"\n          + totalReplicasExpected\n          + \" (unavailableStorages=\" + unavailableStorages\n          + \", storagePolicy=\" + storagePolicy\n          + \", newBlock=\" + newBlock + \")\";\n\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(message, e);\n      } else {\n        LOG.warn(message + \" \" + e.getMessage());\n      }\n\n      if (avoidStaleNodes) {\n        // Retry chooseTarget again, this time not avoiding stale nodes.\n\n        // excludedNodes contains the initial excludedNodes and nodes that were\n        // not chosen because they were stale, decommissioned, etc.\n        // We need to additionally exclude the nodes that were added to the \n        // result list in the successful calls to choose*() above.\n        for (DatanodeStorageInfo resultStorage : results) {\n          addToExcludedNodes(resultStorage.getDatanodeDescriptor(), oldExcludedNodes);\n        }\n        // Set numOfReplicas, since it can get out of sync with the result list\n        // if the NotEnoughReplicasException was thrown in chooseRandom().\n        numOfReplicas = totalReplicasExpected - results.size();\n        return chooseTarget(numOfReplicas, writer, oldExcludedNodes, blocksize,\n            maxNodesPerRack, results, false, storagePolicy, unavailableStorages,\n            newBlock);\n      }\n\n      boolean retry = false;\n      // simply add all the remaining types into unavailableStorages and give\n      // another try. No best effort is guaranteed here.\n      for (StorageType type : storageTypes.keySet()) {\n        if (!unavailableStorages.contains(type)) {\n          unavailableStorages.add(type);\n          retry = true;\n        }\n      }\n      if (retry) {\n        for (DatanodeStorageInfo resultStorage : results) {\n          addToExcludedNodes(resultStorage.getDatanodeDescriptor(),\n              oldExcludedNodes);\n        }\n        numOfReplicas = totalReplicasExpected - results.size();\n        return chooseTarget(numOfReplicas, writer, oldExcludedNodes, blocksize,\n            maxNodesPerRack, results, false, storagePolicy, unavailableStorages,\n            newBlock);\n      }\n    }\n    return writer;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseFavouredNodes": "  protected void chooseFavouredNodes(String src, int numOfReplicas,\n      List<DatanodeDescriptor> favoredNodes,\n      Set<Node> favoriteAndExcludedNodes, long blocksize, int maxNodesPerRack,\n      List<DatanodeStorageInfo> results, boolean avoidStaleNodes,\n      EnumMap<StorageType, Integer> storageTypes)\n      throws NotEnoughReplicasException {\n    for (int i = 0; i < favoredNodes.size() && results.size() < numOfReplicas;\n        i++) {\n      DatanodeDescriptor favoredNode = favoredNodes.get(i);\n      // Choose a single node which is local to favoredNode.\n      // 'results' is updated within chooseLocalNode\n      final DatanodeStorageInfo target =\n          chooseLocalStorage(favoredNode, favoriteAndExcludedNodes, blocksize,\n            maxNodesPerRack, results, avoidStaleNodes, storageTypes, false);\n      if (target == null) {\n        LOG.warn(\"Could not find a target for file \" + src\n            + \" with favored node \" + favoredNode);\n        continue;\n      }\n      favoriteAndExcludedNodes.add(target.getDatanodeDescriptor());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.getPipeline": "  private DatanodeStorageInfo[] getPipeline(Node writer,\n      DatanodeStorageInfo[] storages) {\n    if (storages.length == 0) {\n      return storages;\n    }\n\n    synchronized(clusterMap) {\n      int index=0;\n      if (writer == null || !clusterMap.contains(writer)) {\n        writer = storages[0].getDatanodeDescriptor();\n      }\n      for(; index < storages.length; index++) {\n        DatanodeStorageInfo shortestStorage = storages[index];\n        int shortestDistance = clusterMap.getDistance(writer,\n            shortestStorage.getDatanodeDescriptor());\n        int shortestIndex = index;\n        for(int i = index + 1; i < storages.length; i++) {\n          int currentDistance = clusterMap.getDistance(writer,\n              storages[i].getDatanodeDescriptor());\n          if (shortestDistance>currentDistance) {\n            shortestDistance = currentDistance;\n            shortestStorage = storages[i];\n            shortestIndex = i;\n          }\n        }\n        //switch position index & shortestIndex\n        if (index != shortestIndex) {\n          storages[shortestIndex] = storages[index];\n          storages[index] = shortestStorage;\n        }\n        writer = shortestStorage.getDatanodeDescriptor();\n      }\n    }\n    return storages;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.getMaxNodesPerRack": "  protected int[] getMaxNodesPerRack(int numOfChosen, int numOfReplicas) {\n    int clusterSize = clusterMap.getNumOfLeaves();\n    int totalNumOfReplicas = numOfChosen + numOfReplicas;\n    if (totalNumOfReplicas > clusterSize) {\n      numOfReplicas -= (totalNumOfReplicas-clusterSize);\n      totalNumOfReplicas = clusterSize;\n    }\n    // No calculation needed when there is only one rack or picking one node.\n    int numOfRacks = clusterMap.getNumOfRacks();\n    if (numOfRacks == 1 || totalNumOfReplicas <= 1) {\n      return new int[] {numOfReplicas, totalNumOfReplicas};\n    }\n\n    int maxNodesPerRack = (totalNumOfReplicas-1)/numOfRacks + 2;\n    // At this point, there are more than one racks and more than one replicas\n    // to store. Avoid all replicas being in the same rack.\n    //\n    // maxNodesPerRack has the following properties at this stage.\n    //   1) maxNodesPerRack >= 2\n    //   2) (maxNodesPerRack-1) * numOfRacks > totalNumOfReplicas\n    //          when numOfRacks > 1\n    //\n    // Thus, the following adjustment will still result in a value that forces\n    // multi-rack allocation and gives enough number of total nodes.\n    if (maxNodesPerRack == totalNumOfReplicas) {\n      maxNodesPerRack--;\n    }\n    return new int[] {numOfReplicas, maxNodesPerRack};\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder": "  protected Node chooseTargetInOrder(int numOfReplicas, \n                                 Node writer,\n                                 final Set<Node> excludedNodes,\n                                 final long blocksize,\n                                 final int maxNodesPerRack,\n                                 final List<DatanodeStorageInfo> results,\n                                 final boolean avoidStaleNodes,\n                                 final boolean newBlock,\n                                 EnumMap<StorageType, Integer> storageTypes)\n                                 throws NotEnoughReplicasException {\n    final int numOfResults = results.size();\n    if (numOfResults == 0) {\n      writer = chooseLocalStorage(writer, excludedNodes, blocksize,\n          maxNodesPerRack, results, avoidStaleNodes, storageTypes, true)\n          .getDatanodeDescriptor();\n      if (--numOfReplicas == 0) {\n        return writer;\n      }\n    }\n    final DatanodeDescriptor dn0 = results.get(0).getDatanodeDescriptor();\n    if (numOfResults <= 1) {\n      chooseRemoteRack(1, dn0, excludedNodes, blocksize, maxNodesPerRack,\n          results, avoidStaleNodes, storageTypes);\n      if (--numOfReplicas == 0) {\n        return writer;\n      }\n    }\n    if (numOfResults <= 2) {\n      final DatanodeDescriptor dn1 = results.get(1).getDatanodeDescriptor();\n      if (clusterMap.isOnSameRack(dn0, dn1)) {\n        chooseRemoteRack(1, dn0, excludedNodes, blocksize, maxNodesPerRack,\n            results, avoidStaleNodes, storageTypes);\n      } else if (newBlock){\n        chooseLocalRack(dn1, excludedNodes, blocksize, maxNodesPerRack,\n            results, avoidStaleNodes, storageTypes);\n      } else {\n        chooseLocalRack(writer, excludedNodes, blocksize, maxNodesPerRack,\n            results, avoidStaleNodes, storageTypes);\n      }\n      if (--numOfReplicas == 0) {\n        return writer;\n      }\n    }\n    chooseRandom(numOfReplicas, NodeBase.ROOT, excludedNodes, blocksize,\n        maxNodesPerRack, results, avoidStaleNodes, storageTypes);\n    return writer;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.getRequiredStorageTypes": "  private EnumMap<StorageType, Integer> getRequiredStorageTypes(\n      List<StorageType> types) {\n    EnumMap<StorageType, Integer> map = new EnumMap<>(StorageType.class);\n    for (StorageType type : types) {\n      if (!map.containsKey(type)) {\n        map.put(type, 1);\n      } else {\n        int num = map.get(type);\n        map.put(type, num + 1);\n      }\n    }\n    return map;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork": "  int computeDatanodeWork() {\n    // Blocks should not be replicated or removed if in safe mode.\n    // It's OK to check safe mode here w/o holding lock, in the worst\n    // case extra replications will be scheduled, and these will get\n    // fixed up later.\n    if (namesystem.isInSafeMode()) {\n      return 0;\n    }\n\n    final int numlive = heartbeatManager.getLiveDatanodeCount();\n    final int blocksToProcess = numlive\n        * this.blocksReplWorkMultiplier;\n    final int nodesToProcess = (int) Math.ceil(numlive\n        * this.blocksInvalidateWorkPct);\n\n    int workFound = this.computeBlockReconstructionWork(blocksToProcess);\n\n    // Update counters\n    namesystem.writeLock();\n    try {\n      this.updateState();\n      this.scheduledReplicationBlocksCount = workFound;\n    } finally {\n      namesystem.writeUnlock();\n    }\n    workFound += this.computeInvalidateWork(nodesToProcess);\n    return workFound;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeBlockReconstructionWork": "  int computeBlockReconstructionWork(int blocksToProcess) {\n    List<List<BlockInfo>> blocksToReconstruct = null;\n    namesystem.writeLock();\n    try {\n      // Choose the blocks to be reconstructed\n      blocksToReconstruct = neededReconstruction\n          .chooseLowRedundancyBlocks(blocksToProcess);\n    } finally {\n      namesystem.writeUnlock();\n    }\n    return computeReconstructionWorkForBlocks(blocksToReconstruct);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.isInSafeMode": "  public boolean isInSafeMode() {\n    return bmSafeMode.isInSafeMode();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.updateState": "  void updateState() {\n    pendingReplicationBlocksCount = pendingReplications.size();\n    lowRedundancyBlocksCount = neededReconstruction.size();\n    corruptReplicaBlocksCount = corruptReplicas.size();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeInvalidateWork": "  int computeInvalidateWork(int nodesToProcess) {\n    final List<DatanodeInfo> nodes = invalidateBlocks.getDatanodes();\n    Collections.shuffle(nodes);\n\n    nodesToProcess = Math.min(nodes.size(), nodesToProcess);\n\n    int blockCnt = 0;\n    for (DatanodeInfo dnInfo : nodes) {\n      int blocks = invalidateWorkForOneNode(dnInfo);\n      if (blocks > 0) {\n        blockCnt += blocks;\n        if (--nodesToProcess == 0) {\n          break;\n        }\n      }\n    }\n    return blockCnt;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.run": "    public void run() {\n      try {\n        processQueue();\n      } catch (Throwable t) {\n        ExitUtil.terminate(1,\n            getName() + \" encountered fatal exception: \" + t);\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processQueue": "    private void processQueue() {\n      while (namesystem.isRunning()) {\n        NameNodeMetrics metrics = NameNode.getNameNodeMetrics();\n        try {\n          Runnable action = queue.take();\n          // batch as many operations in the write lock until the queue\n          // runs dry, or the max lock hold is reached.\n          int processed = 0;\n          namesystem.writeLock();\n          metrics.setBlockOpsQueued(queue.size() + 1);\n          try {\n            long start = Time.monotonicNow();\n            do {\n              processed++;\n              action.run();\n              if (Time.monotonicNow() - start > MAX_LOCK_HOLD_MS) {\n                break;\n              }\n              action = queue.poll();\n            } while (action != null);\n          } finally {\n            namesystem.writeUnlock();\n            metrics.addBlockOpsBatched(processed - 1);\n          }\n        } catch (InterruptedException e) {\n          // ignore unless thread was specifically interrupted.\n          if (Thread.interrupted()) {\n            break;\n          }\n        }\n      }\n      queue.clear();\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.isPopulatingReplQueues": "  public boolean isPopulatingReplQueues() {\n    if (!shouldPopulateReplQueues()) {\n      return false;\n    }\n    return initializedReplQueues;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.rescanPostponedMisreplicatedBlocks": "  void rescanPostponedMisreplicatedBlocks() {\n    if (getPostponedMisreplicatedBlocksCount() == 0) {\n      return;\n    }\n    long startTimeRescanPostponedMisReplicatedBlocks = Time.monotonicNow();\n    long startPostponedMisReplicatedBlocksCount =\n        getPostponedMisreplicatedBlocksCount();\n    namesystem.writeLock();\n    try {\n      // blocksPerRescan is the configured number of blocks per rescan.\n      // Randomly select blocksPerRescan consecutive blocks from the HashSet\n      // when the number of blocks remaining is larger than blocksPerRescan.\n      // The reason we don't always pick the first blocksPerRescan blocks is to\n      // handle the case if for some reason some datanodes remain in\n      // content stale state for a long time and only impact the first\n      // blocksPerRescan blocks.\n      int i = 0;\n      long startIndex = 0;\n      long blocksPerRescan =\n          datanodeManager.getBlocksPerPostponedMisreplicatedBlocksRescan();\n      long base = getPostponedMisreplicatedBlocksCount() - blocksPerRescan;\n      if (base > 0) {\n        startIndex = ThreadLocalRandom.current().nextLong() % (base+1);\n        if (startIndex < 0) {\n          startIndex += (base+1);\n        }\n      }\n      Iterator<Block> it = postponedMisreplicatedBlocks.iterator();\n      for (int tmp = 0; tmp < startIndex; tmp++) {\n        it.next();\n      }\n\n      for (;it.hasNext(); i++) {\n        Block b = it.next();\n        if (i >= blocksPerRescan) {\n          break;\n        }\n\n        BlockInfo bi = getStoredBlock(b);\n        if (bi == null) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"BLOCK* rescanPostponedMisreplicatedBlocks: \" +\n                \"Postponed mis-replicated block \" + b + \" no longer found \" +\n                \"in block map.\");\n          }\n          it.remove();\n          postponedMisreplicatedBlocksCount.decrementAndGet();\n          continue;\n        }\n        MisReplicationResult res = processMisReplicatedBlock(bi);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"BLOCK* rescanPostponedMisreplicatedBlocks: \" +\n              \"Re-scanned block \" + b + \", result is \" + res);\n        }\n        if (res != MisReplicationResult.POSTPONE) {\n          it.remove();\n          postponedMisreplicatedBlocksCount.decrementAndGet();\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n      long endPostponedMisReplicatedBlocksCount =\n          getPostponedMisreplicatedBlocksCount();\n      LOG.info(\"Rescan of postponedMisreplicatedBlocks completed in \" +\n          (Time.monotonicNow() - startTimeRescanPostponedMisReplicatedBlocks) +\n          \" msecs. \" + endPostponedMisReplicatedBlocksCount +\n          \" blocks are left. \" + (startPostponedMisReplicatedBlocksCount -\n          endPostponedMisReplicatedBlocksCount) + \" blocks are removed.\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.scanAndCompactStorages": "    private void scanAndCompactStorages() throws InterruptedException {\n      ArrayList<String> datanodesAndStorages = new ArrayList<>();\n      for (DatanodeDescriptor node\n          : datanodeManager.getDatanodeListForReport(DatanodeReportType.ALL)) {\n        for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n          try {\n            namesystem.readLock();\n            double ratio = storage.treeSetFillRatio();\n            if (ratio < storageInfoDefragmentRatio) {\n              datanodesAndStorages.add(node.getDatanodeUuid());\n              datanodesAndStorages.add(storage.getStorageID());\n            }\n            LOG.info(\"StorageInfo TreeSet fill ratio {} : {}{}\",\n                     storage.getStorageID(), ratio,\n                     (ratio < storageInfoDefragmentRatio)\n                     ? \" (queued for defragmentation)\" : \"\");\n          } finally {\n            namesystem.readUnlock();\n          }\n        }\n      }\n      if (!datanodesAndStorages.isEmpty()) {\n        for (int i = 0; i < datanodesAndStorages.size(); i += 2) {\n          namesystem.writeLock();\n          try {\n            DatanodeStorageInfo storage = datanodeManager.\n                getDatanode(datanodesAndStorages.get(i)).\n                getStorageInfo(datanodesAndStorages.get(i + 1));\n            if (storage != null) {\n              boolean aborted =\n                  !storage.treeSetCompact(storageInfoDefragmentTimeout);\n              if (aborted) {\n                // Compaction timed out, reset iterator to continue with\n                // the same storage next iteration.\n                i -= 2;\n              }\n              LOG.info(\"StorageInfo TreeSet defragmented {} : {}{}\",\n                       storage.getStorageID(), storage.treeSetFillRatio(),\n                       aborted ? \" (aborted)\" : \"\");\n            }\n          } finally {\n            namesystem.writeUnlock();\n          }\n          // Wait between each iteration\n          Thread.sleep(1000);\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync": "  private void processMisReplicatesAsync() throws InterruptedException {\n    long nrInvalid = 0, nrOverReplicated = 0;\n    long nrUnderReplicated = 0, nrPostponed = 0, nrUnderConstruction = 0;\n    long startTimeMisReplicatedScan = Time.monotonicNow();\n    Iterator<BlockInfo> blocksItr = blocksMap.getBlocks().iterator();\n    long totalBlocks = blocksMap.size();\n    reconstructionQueuesInitProgress = 0;\n    long totalProcessed = 0;\n    long sleepDuration =\n        Math.max(1, Math.min(numBlocksPerIteration/1000, 10000));\n\n    while (namesystem.isRunning() && !Thread.currentThread().isInterrupted()) {\n      int processed = 0;\n      namesystem.writeLockInterruptibly();\n      try {\n        while (processed < numBlocksPerIteration && blocksItr.hasNext()) {\n          BlockInfo block = blocksItr.next();\n          MisReplicationResult res = processMisReplicatedBlock(block);\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"block \" + block + \": \" + res);\n          }\n          switch (res) {\n          case UNDER_REPLICATED:\n            nrUnderReplicated++;\n            break;\n          case OVER_REPLICATED:\n            nrOverReplicated++;\n            break;\n          case INVALID:\n            nrInvalid++;\n            break;\n          case POSTPONE:\n            nrPostponed++;\n            postponeBlock(block);\n            break;\n          case UNDER_CONSTRUCTION:\n            nrUnderConstruction++;\n            break;\n          case OK:\n            break;\n          default:\n            throw new AssertionError(\"Invalid enum value: \" + res);\n          }\n          processed++;\n        }\n        totalProcessed += processed;\n        // there is a possibility that if any of the blocks deleted/added during\n        // initialisation, then progress might be different.\n        reconstructionQueuesInitProgress = Math.min((double) totalProcessed\n            / totalBlocks, 1.0);\n\n        if (!blocksItr.hasNext()) {\n          LOG.info(\"Total number of blocks            = \" + blocksMap.size());\n          LOG.info(\"Number of invalid blocks          = \" + nrInvalid);\n          LOG.info(\"Number of under-replicated blocks = \" + nrUnderReplicated);\n          LOG.info(\"Number of  over-replicated blocks = \" + nrOverReplicated\n              + ((nrPostponed > 0) ? (\" (\" + nrPostponed + \" postponed)\") : \"\"));\n          LOG.info(\"Number of blocks being written    = \" + nrUnderConstruction);\n          NameNode.stateChangeLog\n              .info(\"STATE* Replication Queue initialization \"\n                  + \"scan for invalid, over- and under-replicated blocks \"\n                  + \"completed in \"\n                  + (Time.monotonicNow() - startTimeMisReplicatedScan)\n                  + \" msec\");\n          break;\n        }\n      } finally {\n        namesystem.writeUnlock();\n        // Make sure it is out of the write lock for sufficiently long time.\n        Thread.sleep(sleepDuration);\n      }\n    }\n    if (Thread.currentThread().isInterrupted()) {\n      LOG.info(\"Interrupted while processing replication queues.\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processPendingReplications": "  private void processPendingReplications() {\n    BlockInfo[] timedOutItems = pendingReplications.getTimedOutBlocks();\n    if (timedOutItems != null) {\n      namesystem.writeLock();\n      try {\n        for (int i = 0; i < timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we're working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi = blocksMap.getStoredBlock(timedOutItems[i]);\n          if (bi == null) {\n            continue;\n          }\n          NumberReplicas num = countNodes(timedOutItems[i]);\n          if (isNeededReconstruction(bi, num.liveReplicas())) {\n            neededReconstruction.add(bi, num.liveReplicas(),\n                num.readOnlyReplicas(), num.decommissionedAndDecommissioning(),\n                getReplication(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.toStorageTypes": "  static Iterable<StorageType> toStorageTypes(\n      final Iterable<DatanodeStorageInfo> infos) {\n    return new Iterable<StorageType>() {\n        @Override\n        public Iterator<StorageType> iterator() {\n          return new Iterator<StorageType>() {\n            final Iterator<DatanodeStorageInfo> i = infos.iterator();\n            @Override\n            public boolean hasNext() {return i.hasNext();}\n            @Override\n            public StorageType next() {return i.next().getStorageType();}\n            @Override\n            public void remove() {\n              throw new UnsupportedOperationException();\n            }\n          };\n        }\n      };\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.next": "            public StorageType next() {return i.next().getStorageType();}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.iterator": "        public Iterator<StorageType> iterator() {\n          return new Iterator<StorageType>() {\n            final Iterator<DatanodeStorageInfo> i = infos.iterator();\n            @Override\n            public boolean hasNext() {return i.hasNext();}\n            @Override\n            public StorageType next() {return i.next().getStorageType();}\n            @Override\n            public void remove() {\n              throw new UnsupportedOperationException();\n            }\n          };\n        }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.getStorageType": "  public StorageType getStorageType() {\n    return storageType;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.hasNext": "            public boolean hasNext() {return i.hasNext();}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.getDatanodeDescriptor": "  public DatanodeDescriptor getDatanodeDescriptor() {\n    return dn;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.heartbeatManager.getLiveDatanodeCount": "  synchronized int getLiveDatanodeCount() {\n    return datanodes.size();\n  }"
        },
        "bug_report": {
            "Title": "Rack failures may result in NN terminate",
            "Description": "If there're rack failures which end up leaving only 1 rack available, {{BlockPlacementPolicyDefault#chooseRandom}} may get {{InvalidTopologyException}} when calling {{NetworkTopology#chooseRandom}}, which then throws all the way out to {{BlockManager}}'s {{ReplicationMonitor}} thread and terminate the NN.\n\nLog:\n{noformat}\n2016-02-24 09:22:01,514  WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy\n\n2016-02-24 09:22:01,958  ERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor thread received Runtime exception. \norg.apache.hadoop.net.NetworkTopology$InvalidTopologyException: Failed to find datanode (scope=\"\" excludedScope=\"/rack_a5\").\n\tat org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:729)\n\tat org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:694)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:635)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:580)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:348)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:214)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:111)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.chooseTargets(BlockManager.java:3746)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.access$200(BlockManager.java:3711)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1400)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1306)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3682)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3634)\n\tat java.lang.Thread.run(Thread.java:745)\n{noformat}"
        }
    },
    {
        "filename": "HDFS-4201.json",
        "creation_time": "2012-11-16T21:28:57.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:434)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:520)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:673)\n        at java.lang.Thread.run(Thread.java:722)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat": "  HeartbeatResponse sendHeartBeat() throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat from service actor: \" + this);\n    }\n    // reports number of failed volumes\n    StorageReport[] report = { new StorageReport(bpRegistration.getStorageID(),\n        false,\n        dn.getFSDataset().getCapacity(),\n        dn.getFSDataset().getDfsUsed(),\n        dn.getFSDataset().getRemaining(),\n        dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId())) };\n    return bpNamenode.sendHeartbeat(bpRegistration, report,\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        dn.getFSDataset().getNumFailedVolumes());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService": "  private void offerService() throws Exception {\n    LOG.info(\"For namenode \" + nnAddr + \" using DELETEREPORT_INTERVAL of \"\n        + dnConf.deleteReportInterval + \" msec \" + \" BLOCKREPORT_INTERVAL of \"\n        + dnConf.blockReportInterval + \"msec\" + \" Initial delay: \"\n        + dnConf.initialBlockReportDelay + \"msec\" + \"; heartBeatInterval=\"\n        + dnConf.heartBeatInterval);\n\n    //\n    // Now loop for a long time....\n    //\n    while (shouldRun()) {\n      try {\n        long startTime = now();\n\n        //\n        // Every so often, send heartbeat or block-report\n        //\n        if (startTime - lastHeartbeat > dnConf.heartBeatInterval) {\n          //\n          // All heartbeat messages include following info:\n          // -- Datanode name\n          // -- data transfer port\n          // -- Total capacity\n          // -- Bytes remaining\n          //\n          lastHeartbeat = startTime;\n          if (!dn.areHeartbeatsDisabledForTests()) {\n            HeartbeatResponse resp = sendHeartBeat();\n            assert resp != null;\n            dn.getMetrics().addHeartbeat(now() - startTime);\n\n            // If the state of this NN has changed (eg STANDBY->ACTIVE)\n            // then let the BPOfferService update itself.\n            //\n            // Important that this happens before processCommand below,\n            // since the first heartbeat to a new active might have commands\n            // that we should actually process.\n            bpos.updateActorStatesFromHeartbeat(\n                this, resp.getNameNodeHaState());\n\n            long startProcessCommands = now();\n            if (!processCommand(resp.getCommands()))\n              continue;\n            long endProcessCommands = now();\n            if (endProcessCommands - startProcessCommands > 2000) {\n              LOG.info(\"Took \" + (endProcessCommands - startProcessCommands)\n                  + \"ms to process \" + resp.getCommands().length\n                  + \" commands from NN\");\n            }\n          }\n        }\n        if (pendingReceivedRequests > 0\n            || (startTime - lastDeletedReport > dnConf.deleteReportInterval)) {\n          reportReceivedDeletedBlocks();\n          lastDeletedReport = startTime;\n        }\n\n        DatanodeCommand cmd = blockReport();\n        processCommand(new DatanodeCommand[]{ cmd });\n\n        // Now safe to start scanning the block pool.\n        // If it has already been started, this is a no-op.\n        if (dn.blockScanner != null) {\n          dn.blockScanner.addBlockPool(bpos.getBlockPoolId());\n        }\n\n        //\n        // There is no work to do;  sleep until hearbeat timer elapses, \n        // or work arrives, and then iterate again.\n        //\n        long waitTime = dnConf.heartBeatInterval - \n        (Time.now() - lastHeartbeat);\n        synchronized(pendingIncrementalBR) {\n          if (waitTime > 0 && pendingReceivedRequests == 0) {\n            try {\n              pendingIncrementalBR.wait(waitTime);\n            } catch (InterruptedException ie) {\n              LOG.warn(\"BPOfferService for \" + this + \" interrupted\");\n            }\n          }\n        } // synchronized\n      } catch(RemoteException re) {\n        String reClass = re.getClassName();\n        if (UnregisteredNodeException.class.getName().equals(reClass) ||\n            DisallowedDatanodeException.class.getName().equals(reClass) ||\n            IncorrectVersionException.class.getName().equals(reClass)) {\n          LOG.warn(this + \" is shutting down\", re);\n          shouldServiceRun = false;\n          return;\n        }\n        LOG.warn(\"RemoteException in offerService\", re);\n        try {\n          long sleepTime = Math.min(1000, dnConf.heartBeatInterval);\n          Thread.sleep(sleepTime);\n        } catch (InterruptedException ie) {\n          Thread.currentThread().interrupt();\n        }\n      } catch (IOException e) {\n        LOG.warn(\"IOException in offerService\", e);\n      }\n    } // while (shouldRun())\n  } // offerService",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reportReceivedDeletedBlocks": "  private void reportReceivedDeletedBlocks() throws IOException {\n\n    // check if there are newly received blocks\n    ReceivedDeletedBlockInfo[] receivedAndDeletedBlockArray = null;\n    synchronized (pendingIncrementalBR) {\n      int numBlocks = pendingIncrementalBR.size();\n      if (numBlocks > 0) {\n        //\n        // Send newly-received and deleted blockids to namenode\n        //\n        receivedAndDeletedBlockArray = pendingIncrementalBR\n            .values().toArray(new ReceivedDeletedBlockInfo[numBlocks]);\n      }\n      pendingIncrementalBR.clear();\n    }\n    if (receivedAndDeletedBlockArray != null) {\n      StorageReceivedDeletedBlocks[] report = { new StorageReceivedDeletedBlocks(\n          bpRegistration.getStorageID(), receivedAndDeletedBlockArray) };\n      boolean success = false;\n      try {\n        bpNamenode.blockReceivedAndDeleted(bpRegistration, bpos.getBlockPoolId(),\n            report);\n        success = true;\n      } finally {\n        synchronized (pendingIncrementalBR) {\n          if (!success) {\n            // If we didn't succeed in sending the report, put all of the\n            // blocks back onto our queue, but only in the case where we didn't\n            // put something newer in the meantime.\n            for (ReceivedDeletedBlockInfo rdbi : receivedAndDeletedBlockArray) {\n              if (!pendingIncrementalBR.containsKey(rdbi.getBlock().getBlockId())) {\n                pendingIncrementalBR.put(rdbi.getBlock().getBlockId(), rdbi);\n              }\n            }\n          }\n          pendingReceivedRequests = pendingIncrementalBR.size();\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.shouldRun": "  private boolean shouldRun() {\n    return shouldServiceRun && dn.shouldRun();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand": "  boolean processCommand(DatanodeCommand[] cmds) {\n    if (cmds != null) {\n      for (DatanodeCommand cmd : cmds) {\n        try {\n          if (bpos.processCommandFromActor(cmd, this) == false) {\n            return false;\n          }\n        } catch (IOException ioe) {\n          LOG.warn(\"Error processing datanode Command\", ioe);\n        }\n      }\n    }\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.blockReport": "  DatanodeCommand blockReport() throws IOException {\n    // send block report if timer has expired.\n    DatanodeCommand cmd = null;\n    long startTime = now();\n    if (startTime - lastBlockReport > dnConf.blockReportInterval) {\n\n      // Flush any block information that precedes the block report. Otherwise\n      // we have a chance that we will miss the delHint information\n      // or we will report an RBW replica after the BlockReport already reports\n      // a FINALIZED one.\n      reportReceivedDeletedBlocks();\n\n      // Create block report\n      long brCreateStartTime = now();\n      BlockListAsLongs bReport = dn.getFSDataset().getBlockReport(\n          bpos.getBlockPoolId());\n\n      // Send block report\n      long brSendStartTime = now();\n      StorageBlockReport[] report = { new StorageBlockReport(\n          new DatanodeStorage(bpRegistration.getStorageID()),\n          bReport.getBlockListAsLongs()) };\n      cmd = bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), report);\n\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost = now() - brSendStartTime;\n      long brCreateCost = brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n          + \" blocks took \" + brCreateCost + \" msec to generate and \"\n          + brSendCost + \" msecs for RPC and NN processing\");\n\n      // If we have sent the first block report, then wait a random\n      // time before we start the periodic block reports.\n      if (resetBlockReportTime) {\n        lastBlockReport = startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n        resetBlockReportTime = false;\n      } else {\n        /* say the last block report was at 8:20:14. The current report\n         * should have started around 9:20:14 (default 1 hour interval).\n         * If current time is :\n         *   1) normal like 9:20:18, next report should be at 10:20:14\n         *   2) unexpected like 11:35:43, next report should be at 12:20:14\n         */\n        lastBlockReport += (now() - lastBlockReport) /\n        dnConf.blockReportInterval * dnConf.blockReportInterval;\n      }\n      LOG.info(\"sent block report, processed command:\" + cmd);\n    }\n    return cmd;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run": "  public void run() {\n    LOG.info(this + \" starting to offer service\");\n\n    try {\n      // init stuff\n      try {\n        // setup storage\n        connectToNNAndHandshake();\n      } catch (IOException ioe) {\n        // Initial handshake, storage recovery or registration failed\n        // End BPOfferService thread\n        LOG.fatal(\"Initialization failed for block pool \" + this, ioe);\n        return;\n      }\n\n      initialized = true; // bp is initialized;\n      \n      while (shouldRun()) {\n        try {\n          offerService();\n        } catch (Exception ex) {\n          LOG.error(\"Exception in BPOfferService for \" + this, ex);\n          sleepAndLogInterrupts(5000, \"offering service\");\n        }\n      }\n    } catch (Throwable ex) {\n      LOG.warn(\"Unexpected exception in block pool \" + this, ex);\n    } finally {\n      LOG.warn(\"Ending block pool service for: \" + this);\n      cleanUp();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp": "  private synchronized void cleanUp() {\n    \n    shouldServiceRun = false;\n    IOUtils.cleanup(LOG, bpNamenode);\n    bpos.shutdownActor(this);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sleepAndLogInterrupts": "  private void sleepAndLogInterrupts(int millis,\n      String stateString) {\n    try {\n      Thread.sleep(millis);\n    } catch (InterruptedException ie) {\n      LOG.info(\"BPOfferService \" + this + \" interrupted while \" + stateString);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake": "  private void connectToNNAndHandshake() throws IOException {\n    // get NN proxy\n    bpNamenode = dn.connectToNN(nnAddr);\n\n    // First phase of the handshake with NN - get the namespace\n    // info.\n    NamespaceInfo nsInfo = retrieveNamespaceInfo();\n    \n    // Verify that this matches the other NN in this HA pair.\n    // This also initializes our block pool in the DN if we are\n    // the first NN connection for this BP.\n    bpos.verifyAndSetNamespaceInfo(nsInfo);\n    \n    // Second phase of the handshake with the NN.\n    register();\n  }"
        },
        "bug_report": {
            "Title": "NPE in BPServiceActor#sendHeartBeat",
            "Description": "Saw the following NPE in a log.\n\nThink this is likely due to {{dn}} or {{dn.getFSDataset()}} being null, (not {{bpRegistration}}) due to a configuration or local directory failure.\n\n{code}\n2012-09-25 04:33:20,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode svsrs00127/11.164.162.226:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec Initial delay: 0msec; heartBeatInterval=3000\n2012-09-25 04:33:20,782 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in BPOfferService for Block pool BP-1678908700-11.164.162.226-1342785481826 (storage id DS-1031100678-11.164.162.251-5010-1341933415989) service to svsrs00127/11.164.162.226:8020\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:434)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:520)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:673)\n        at java.lang.Thread.run(Thread.java:722)\n{code}"
        }
    },
    {
        "filename": "HDFS-6904.json",
        "creation_time": "2014-08-21T10:17:21.000+0000",
        "stack_trace": "```\njava.io.IOException: Failed to renew token: Kind: WEBHDFS delegation, Service: NameNodeIP:8020, Ident: (WEBHDFS delegation token 2222 for hrt_qa)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:394)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.access$5(DelegationTokenRenewer.java:357)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.handleDTRenewerAppSubmitEvent(DelegationTokenRenewer.java:657)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.run(DelegationTokenRenewer.java:638)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Unexpected HTTP response: code=-1 != 200, op=RENEWDELEGATIONTOKEN, message=null\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:331)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$200(WebHdfsFileSystem.java:90)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:598)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:448)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:477)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:473)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.renewDelegationToken(WebHdfsFileSystem.java:1318)\n        at org.apache.hadoop.hdfs.web.TokenAspect$TokenManager.renew(TokenAspect.java:73)\n        at org.apache.hadoop.security.token.Token.renew(Token.java:377)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:477)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:1)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.renewToken(DelegationTokenRenewer.java:473)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:392)\n        ... 6 more\nCaused by: java.io.IOException: The error stream is null.\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.jsonParse(WebHdfsFileSystem.java:304)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:329)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.DelegationTokenRenewer.run": "  public void run() {\n    for(;;) {\n      RenewAction<?> action = null;\n      try {\n        action = queue.take();\n        if (action.renew()) {\n          queue.add(action);\n        }\n      } catch (InterruptedException ie) {\n        return;\n      } catch (Exception ie) {\n        action.weakFs.get().LOG.warn(\"Failed to renew token, action=\" + action,\n            ie);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.DelegationTokenRenewer.renew": "    private boolean renew() throws IOException, InterruptedException {\n      final T fs = weakFs.get();\n      final boolean b = fs != null;\n      if (b) {\n        synchronized(fs) {\n          try {\n            long expires = token.renew(fs.getConf());\n            updateRenewalTime(expires - Time.now());\n          } catch (IOException ie) {\n            try {\n              Token<?>[] tokens = fs.addDelegationTokens(null, null);\n              if (tokens.length == 0) {\n                throw new IOException(\"addDelegationTokens returned no tokens\");\n              }\n              token = tokens[0];\n              updateRenewalTime(renewCycle);\n              fs.setDelegationToken(token);\n            } catch (IOException ie2) {\n              isValid = false;\n              throw new IOException(\"Can't renew or get new delegation token \", ie);\n            }\n          }\n        }\n      }\n      return b;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse": "  private static Map<?, ?> validateResponse(final HttpOpParam.Op op,\n      final HttpURLConnection conn, boolean unwrapException) throws IOException {\n    final int code = conn.getResponseCode();\n    // server is demanding an authentication we don't support\n    if (code == HttpURLConnection.HTTP_UNAUTHORIZED) {\n      // match hdfs/rpc exception\n      throw new AccessControlException(conn.getResponseMessage());\n    }\n    if (code != op.getExpectedHttpResponseCode()) {\n      final Map<?, ?> m;\n      try {\n        m = jsonParse(conn, true);\n      } catch(Exception e) {\n        throw new IOException(\"Unexpected HTTP response: code=\" + code + \" != \"\n            + op.getExpectedHttpResponseCode() + \", \" + op.toQueryString()\n            + \", message=\" + conn.getResponseMessage(), e);\n      }\n\n      if (m == null) {\n        throw new IOException(\"Unexpected HTTP response: code=\" + code + \" != \"\n            + op.getExpectedHttpResponseCode() + \", \" + op.toQueryString()\n            + \", message=\" + conn.getResponseMessage());\n      } else if (m.get(RemoteException.class.getSimpleName()) == null) {\n        return m;\n      }\n\n      IOException re = JsonUtil.toRemoteException(m);\n      // extract UGI-related exceptions and unwrap InvalidToken\n      // the NN mangles these exceptions but the DN does not and may need\n      // to re-fetch a token if either report the token is expired\n      if (re.getMessage().startsWith(\"Failed to obtain user group information:\")) {\n        String[] parts = re.getMessage().split(\":\\\\s+\", 3);\n        re = new RemoteException(parts[1], parts[2]);\n        re = ((RemoteException)re).unwrapRemoteException(InvalidToken.class);\n      }\n      throw unwrapException? toIOException(re): re;\n    }\n    return null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException": "  private static IOException toIOException(Exception e) {\n    if (!(e instanceof IOException)) {\n      return new IOException(e);\n    }\n\n    final IOException ioe = (IOException)e;\n    if (!(ioe instanceof RemoteException)) {\n      return ioe;\n    }\n\n    return ((RemoteException)ioe).unwrapRemoteException();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.WebHdfsFileSystem.jsonParse": "  static Map<?, ?> jsonParse(final HttpURLConnection c, final boolean useErrorStream\n      ) throws IOException {\n    if (c.getContentLength() == 0) {\n      return null;\n    }\n    final InputStream in = useErrorStream? c.getErrorStream(): c.getInputStream();\n    if (in == null) {\n      throw new IOException(\"The \" + (useErrorStream? \"error\": \"input\") + \" stream is null.\");\n    }\n    final String contentType = c.getContentType();\n    if (contentType != null) {\n      final MediaType parsed = MediaType.valueOf(contentType);\n      if (!MediaType.APPLICATION_JSON_TYPE.isCompatible(parsed)) {\n        throw new IOException(\"Content-Type \\\"\" + contentType\n            + \"\\\" is incompatible with \\\"\" + MediaType.APPLICATION_JSON\n            + \"\\\" (parsed=\\\"\" + parsed + \"\\\")\");\n      }\n    }\n    return (Map<?, ?>)JSON.parse(new InputStreamReader(in, Charsets.UTF_8));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.WebHdfsFileSystem.runWithRetry": "    private T runWithRetry() throws IOException {\n      /**\n       * Do the real work.\n       *\n       * There are three cases that the code inside the loop can throw an\n       * IOException:\n       *\n       * <ul>\n       * <li>The connection has failed (e.g., ConnectException,\n       * @see FailoverOnNetworkExceptionRetry for more details)</li>\n       * <li>The namenode enters the standby state (i.e., StandbyException).</li>\n       * <li>The server returns errors for the command (i.e., RemoteException)</li>\n       * </ul>\n       *\n       * The call to shouldRetry() will conduct the retry policy. The policy\n       * examines the exception and swallows it if it decides to rerun the work.\n       */\n      for(int retry = 0; ; retry++) {\n        checkRetry = !redirected;\n        final URL url = getUrl();\n        try {\n          final HttpURLConnection conn = connect(url);\n          // output streams will validate on close\n          if (!op.getDoOutput()) {\n            validateResponse(op, conn, false);\n          }\n          return getResponse(conn);\n        } catch (AccessControlException ace) {\n          // no retries for auth failures\n          throw ace;\n        } catch (InvalidToken it) {\n          // try to replace the expired token with a new one.  the attempt\n          // to acquire a new token must be outside this operation's retry\n          // so if it fails after its own retries, this operation fails too.\n          if (op.getRequireAuth() || !replaceExpiredDelegationToken()) {\n            throw it;\n          }\n        } catch (IOException ioe) {\n          shouldRetry(ioe, retry);\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.WebHdfsFileSystem.shouldRetry": "    private void shouldRetry(final IOException ioe, final int retry\n        ) throws IOException {\n      InetSocketAddress nnAddr = getCurrentNNAddr();\n      if (checkRetry) {\n        try {\n          final RetryPolicy.RetryAction a = retryPolicy.shouldRetry(\n              ioe, retry, 0, true);\n\n          boolean isRetry = a.action == RetryPolicy.RetryAction.RetryDecision.RETRY;\n          boolean isFailoverAndRetry =\n              a.action == RetryPolicy.RetryAction.RetryDecision.FAILOVER_AND_RETRY;\n\n          if (isRetry || isFailoverAndRetry) {\n            LOG.info(\"Retrying connect to namenode: \" + nnAddr\n                + \". Already tried \" + retry + \" time(s); retry policy is \"\n                + retryPolicy + \", delay \" + a.delayMillis + \"ms.\");\n\n            if (isFailoverAndRetry) {\n              resetStateToFailOver();\n            }\n\n            Thread.sleep(a.delayMillis);\n            return;\n          }\n        } catch(Exception e) {\n          LOG.warn(\"Original exception is \", ioe);\n          throw toIOException(e);\n        }\n      }\n      throw toIOException(ioe);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getUrl": "    protected URL getUrl() {\n      return url;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getResponse": "    HttpURLConnection getResponse(HttpURLConnection conn) throws IOException {\n      return conn;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.WebHdfsFileSystem.replaceExpiredDelegationToken": "  synchronized boolean replaceExpiredDelegationToken() throws IOException {\n    boolean replaced = false;\n    if (canRefreshDelegationToken) {\n      Token<?> token = getDelegationToken(null);\n      LOG.debug(\"Replaced expired token: \" + token);\n      setDelegationToken(token);\n      replaced = (token != null);\n    }\n    return replaced;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.WebHdfsFileSystem.connect": "    protected HttpURLConnection connect(final long offset,\n        final boolean resolved) throws IOException {\n      final URL offsetUrl = offset == 0L? url\n          : new URL(url + \"&\" + new OffsetParam(offset));\n      return new URLRunner(GetOpParam.Op.OPEN, offsetUrl, resolved).run();\n    }  ",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.WebHdfsFileSystem.run": "              public T run() throws IOException {\n                return runWithRetry();\n              }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.WebHdfsFileSystem.renewDelegationToken": "  public synchronized long renewDelegationToken(final Token<?> token\n      ) throws IOException {\n    final HttpOpParam.Op op = PutOpParam.Op.RENEWDELEGATIONTOKEN;\n    return new FsPathResponseRunner<Long>(op, null,\n        new TokenArgumentParam(token.encodeToUrlString())) {\n      @Override\n      Long decodeResponse(Map<?,?> json) throws IOException {\n        return (Long) json.get(\"long\");\n      }\n    }.run();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.TokenAspect.renew": "    public long renew(Token<?> token, Configuration conf) throws IOException {\n      return getInstance(token, conf).renewDelegationToken(token);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.TokenAspect.getInstance": "    private TokenManagementDelegator getInstance(Token<?> token,\n                                                 Configuration conf)\n            throws IOException {\n      final URI uri;\n      final String scheme = getSchemeByKind(token.getKind());\n      if (HAUtil.isTokenForLogicalUri(token)) {\n        uri = HAUtil.getServiceUriFromToken(scheme, token);\n      } else {\n        final InetSocketAddress address = SecurityUtil.getTokenServiceAddr\n                (token);\n        uri = URI.create(scheme + \"://\" + NetUtils.getHostPortString(address));\n      }\n      return (TokenManagementDelegator) FileSystem.get(uri, conf);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.TokenAspect.renewDelegationToken": "    long renewDelegationToken(final Token<?> token) throws IOException;\n  }\n\n  private DelegationTokenRenewer.RenewAction<?> action;\n  private DelegationTokenRenewer dtRenewer = null;\n  private final DTSelecorByKind dtSelector;\n  private final T fs;\n  private boolean hasInitedToken;\n  private final Log LOG;\n  private final Text serviceName;\n\n  TokenAspect(T fs, final Text serviceName, final Text kind) {",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.Token.renew": "    public long renew(Token<?> token, Configuration conf) {\n      throw new UnsupportedOperationException(\"Token renewal is not supported \"+\n                                              \" for \" + token.kind + \" tokens\");\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.Token.getRenewer": "  private synchronized TokenRenewer getRenewer() throws IOException {\n    if (renewer != null) {\n      return renewer;\n    }\n    renewer = TRIVIAL_RENEWER;\n    synchronized (renewers) {\n      for (TokenRenewer canidate : renewers) {\n        if (canidate.handleKind(this.kind)) {\n          renewer = canidate;\n          return renewer;\n        }\n      }\n    }\n    LOG.warn(\"No TokenRenewer defined for token kind \" + this.kind);\n    return renewer;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException": "  public static RemoteException toRemoteException(final Map<?, ?> json) {\n    final Map<?, ?> m = (Map<?, ?>)json.get(RemoteException.class.getSimpleName());\n    final String message = (String)m.get(\"message\");\n    final String javaClassName = (String)m.get(\"javaClassName\");\n    return new RemoteException(javaClassName, message);\n  }"
        },
        "bug_report": {
            "Title": "YARN unable to renew delegation token fetched via webhdfs due to incorrect service port",
            "Description": "YARN is unable to renew delegation tokens obtained via the WebHDFS REST API. The scenario is as follows -\n\n1. User creates a delegation token using the WebHDFS REST API\n2. User passes this token to YARN as part of app submission(via the YARN REST API)\n3. When YARN tries to renew this delegation token, it fails because the token service is pointing to the RPC port but the token kind is WebHDFS.\n\nThe exception is\n\n{noformat}\n2014-08-19 03:12:54,733 WARN  security.DelegationTokenRenewer (DelegationTokenRenewer.java:handleDTRenewerAppSubmitEvent(661)) - Unable to add the application to the delegation token renewer.\njava.io.IOException: Failed to renew token: Kind: WEBHDFS delegation, Service: NameNodeIP:8020, Ident: (WEBHDFS delegation token 2222 for hrt_qa)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:394)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.access$5(DelegationTokenRenewer.java:357)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.handleDTRenewerAppSubmitEvent(DelegationTokenRenewer.java:657)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.run(DelegationTokenRenewer.java:638)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Unexpected HTTP response: code=-1 != 200, op=RENEWDELEGATIONTOKEN, message=null\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:331)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$200(WebHdfsFileSystem.java:90)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:598)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:448)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:477)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:473)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.renewDelegationToken(WebHdfsFileSystem.java:1318)\n        at org.apache.hadoop.hdfs.web.TokenAspect$TokenManager.renew(TokenAspect.java:73)\n        at org.apache.hadoop.security.token.Token.renew(Token.java:377)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:477)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:1)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.renewToken(DelegationTokenRenewer.java:473)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:392)\n        ... 6 more\nCaused by: java.io.IOException: The error stream is null.\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.jsonParse(WebHdfsFileSystem.java:304)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:329)\n        ... 24 more\n2014-08-19 03:12:54,735 DEBUG event.AsyncDispatcher (AsyncDispatcher.java:dispatch(164)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRejectedEvent.EventType: APP_REJECTED\n{noformat}\n\nI suspect the issue is that the Namenode generates a delegation token of kind WebHDFS but doesn't change the service port. When YARN tries to renew the delegation token, it ends up trying to contact WebHDFS on the RPC port.\n\nFrom NamenodeWebHdfsMethods.java\n{noformat}\n    case GETDELEGATIONTOKEN:\n    {\n      if (delegation.getValue() != null) {\n        throw new IllegalArgumentException(delegation.getName()\n            + \" parameter is not null.\");\n      }\n      final Token<? extends TokenIdentifier> token = generateDelegationToken(\n          namenode, ugi, renewer.getValue());\n      final String js = JsonUtil.toJsonString(token);\n      return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n    }\n{noformat}\nwhich in turn calls\n{noformat}\n  private Token<? extends TokenIdentifier> generateDelegationToken(\n      final NameNode namenode, final UserGroupInformation ugi,\n      final String renewer) throws IOException {\n    final Credentials c = DelegationTokenSecretManager.createCredentials(\n        namenode, ugi, renewer != null? renewer: ugi.getShortUserName());\n    final Token<? extends TokenIdentifier> t = c.getAllTokens().iterator().next();\n    Text kind = request.getScheme().equals(\"http\") ? WebHdfsFileSystem.TOKEN_KIND\n        : SWebHdfsFileSystem.TOKEN_KIND;\n    t.setKind(kind);\n    return t;\n  }\n{noformat}\n\nThe command we used to get the delegation token is -\n{noformat}\ncurl -i -k -s --negotiate -u : 'http://NameNodeHost:50070/webhdfs/v1?op=GETDELEGATIONTOKEN&renewer=yarn'\n{noformat}"
        }
    },
    {
        "filename": "HDFS-13721.json",
        "creation_time": "2018-07-05T20:11:57.000+0000",
        "stack_trace": "```\njavax.management.RuntimeMBeanException: java.lang.NullPointerException\n at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:839)\n at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrowMaybeMBeanException(DefaultMBeanServerInterceptor.java:852)\n at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:651)\n at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)\n at org.apache.hadoop.jmx.JMXJsonServlet.writeAttribute(JMXJsonServlet.java:338)\n at org.apache.hadoop.jmx.JMXJsonServlet.listBeans(JMXJsonServlet.java:316)\n at org.apache.hadoop.jmx.JMXJsonServlet.doGet(JMXJsonServlet.java:210)\n at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)\n at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)\n at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1537)\n at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\n at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\n at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\n at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\n at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\n at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\n at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\n at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n at org.eclipse.jetty.server.Server.handle(Server.java:534)\n at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\n at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n at java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n at org.apache.hadoop.hdfs.server.datanode.DataNode.getDiskBalancerStatus(DataNode.java:3146)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n at java.lang.reflect.Method.invoke(Method.java:498)\n at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)\n at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n at java.lang.reflect.Method.invoke(Method.java:498)\n at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)\n at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)\n at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)\n at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)\n at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)\n at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)\n at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)\n at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)\n at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.jmx.JMXJsonServlet.writeAttribute": "  private void writeAttribute(JsonGenerator jg, String attName, Object value) throws IOException {\n    jg.writeFieldName(attName);\n    writeObject(jg, value);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.jmx.JMXJsonServlet.writeObject": "  private void writeObject(JsonGenerator jg, Object value) throws IOException {\n    if(value == null) {\n      jg.writeNull();\n    } else {\n      Class<?> c = value.getClass();\n      if (c.isArray()) {\n        jg.writeStartArray();\n        int len = Array.getLength(value);\n        for (int j = 0; j < len; j++) {\n          Object item = Array.get(value, j);\n          writeObject(jg, item);\n        }\n        jg.writeEndArray();\n      } else if(value instanceof Number) {\n        Number n = (Number)value;\n        jg.writeNumber(n.toString());\n      } else if(value instanceof Boolean) {\n        Boolean b = (Boolean)value;\n        jg.writeBoolean(b);\n      } else if(value instanceof CompositeData) {\n        CompositeData cds = (CompositeData)value;\n        CompositeType comp = cds.getCompositeType();\n        Set<String> keys = comp.keySet();\n        jg.writeStartObject();\n        for(String key: keys) {\n          writeAttribute(jg, key, cds.get(key));\n        }\n        jg.writeEndObject();\n      } else if(value instanceof TabularData) {\n        TabularData tds = (TabularData)value;\n        jg.writeStartArray();\n        for(Object entry : tds.values()) {\n          writeObject(jg, entry);\n        }\n        jg.writeEndArray();\n      } else {\n        jg.writeString(value.toString());\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.jmx.JMXJsonServlet.listBeans": "  private void listBeans(JsonGenerator jg, ObjectName qry, String attribute, \n      HttpServletResponse response) \n  throws IOException {\n    LOG.debug(\"Listing beans for \"+qry);\n    Set<ObjectName> names = null;\n    names = mBeanServer.queryNames(qry, null);\n\n    jg.writeArrayFieldStart(\"beans\");\n    Iterator<ObjectName> it = names.iterator();\n    while (it.hasNext()) {\n      ObjectName oname = it.next();\n      MBeanInfo minfo;\n      String code = \"\";\n      Object attributeinfo = null;\n      try {\n        minfo = mBeanServer.getMBeanInfo(oname);\n        code = minfo.getClassName();\n        String prs = \"\";\n        try {\n          if (\"org.apache.commons.modeler.BaseModelMBean\".equals(code)) {\n            prs = \"modelerType\";\n            code = (String) mBeanServer.getAttribute(oname, prs);\n          }\n          if (attribute!=null) {\n            prs = attribute;\n            attributeinfo = mBeanServer.getAttribute(oname, prs);\n          }\n        } catch (AttributeNotFoundException e) {\n          // If the modelerType attribute was not found, the class name is used\n          // instead.\n          LOG.error(\"getting attribute \" + prs + \" of \" + oname\n              + \" threw an exception\", e);\n        } catch (MBeanException e) {\n          // The code inside the attribute getter threw an exception so log it,\n          // and fall back on the class name\n          LOG.error(\"getting attribute \" + prs + \" of \" + oname\n              + \" threw an exception\", e);\n        } catch (RuntimeException e) {\n          // For some reason even with an MBeanException available to them\n          // Runtime exceptionscan still find their way through, so treat them\n          // the same as MBeanException\n          LOG.error(\"getting attribute \" + prs + \" of \" + oname\n              + \" threw an exception\", e);\n        } catch ( ReflectionException e ) {\n          // This happens when the code inside the JMX bean (setter?? from the\n          // java docs) threw an exception, so log it and fall back on the \n          // class name\n          LOG.error(\"getting attribute \" + prs + \" of \" + oname\n              + \" threw an exception\", e);\n        }\n      } catch (InstanceNotFoundException e) {\n        //Ignored for some reason the bean was not found so don't output it\n        continue;\n      } catch ( IntrospectionException e ) {\n        // This is an internal error, something odd happened with reflection so\n        // log it and don't output the bean.\n        LOG.error(\"Problem while trying to process JMX query: \" + qry\n            + \" with MBean \" + oname, e);\n        continue;\n      } catch ( ReflectionException e ) {\n        // This happens when the code inside the JMX bean threw an exception, so\n        // log it and don't output the bean.\n        LOG.error(\"Problem while trying to process JMX query: \" + qry\n            + \" with MBean \" + oname, e);\n        continue;\n      }\n\n      jg.writeStartObject();\n      jg.writeStringField(\"name\", oname.toString());\n      \n      jg.writeStringField(\"modelerType\", code);\n      if ((attribute != null) && (attributeinfo == null)) {\n        jg.writeStringField(\"result\", \"ERROR\");\n        jg.writeStringField(\"message\", \"No attribute with name \" + attribute\n            + \" was found.\");\n        jg.writeEndObject();\n        jg.writeEndArray();\n        jg.close();\n        response.setStatus(HttpServletResponse.SC_NOT_FOUND);\n        return;\n      }\n      \n      if (attribute != null) {\n        writeAttribute(jg, attribute, attributeinfo);\n      } else {\n        MBeanAttributeInfo attrs[] = minfo.getAttributes();\n        for (int i = 0; i < attrs.length; i++) {\n          writeAttribute(jg, oname, attrs[i]);\n        }\n      }\n      jg.writeEndObject();\n    }\n    jg.writeEndArray();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.jmx.JMXJsonServlet.doGet": "  public void doGet(HttpServletRequest request, HttpServletResponse response) {\n    try {\n      if (!isInstrumentationAccessAllowed(request, response)) {\n        return;\n      }\n      JsonGenerator jg = null;\n      PrintWriter writer = null;\n      try {\n        writer = response.getWriter();\n \n        response.setContentType(\"application/json; charset=utf8\");\n        response.setHeader(ACCESS_CONTROL_ALLOW_METHODS, \"GET\");\n        response.setHeader(ACCESS_CONTROL_ALLOW_ORIGIN, \"*\");\n\n        jg = jsonFactory.createGenerator(writer);\n        jg.disable(JsonGenerator.Feature.AUTO_CLOSE_TARGET);\n        jg.useDefaultPrettyPrinter();\n        jg.writeStartObject();\n\n        // query per mbean attribute\n        String getmethod = request.getParameter(\"get\");\n        if (getmethod != null) {\n          String[] splitStrings = getmethod.split(\"\\\\:\\\\:\");\n          if (splitStrings.length != 2) {\n            jg.writeStringField(\"result\", \"ERROR\");\n            jg.writeStringField(\"message\", \"query format is not as expected.\");\n            jg.flush();\n            response.setStatus(HttpServletResponse.SC_BAD_REQUEST);\n            return;\n          }\n          listBeans(jg, new ObjectName(splitStrings[0]), splitStrings[1],\n              response);\n          return;\n        }\n\n        // query per mbean\n        String qry = request.getParameter(\"qry\");\n        if (qry == null) {\n          qry = \"*:*\";\n        }\n        listBeans(jg, new ObjectName(qry), null, response);\n      } finally {\n        if (jg != null) {\n          jg.close();\n        }\n        if (writer != null) {\n          writer.close();\n        }\n      }\n    } catch (IOException e) {\n      LOG.error(\"Caught an exception while processing JMX request\", e);\n      response.setStatus(HttpServletResponse.SC_INTERNAL_SERVER_ERROR);\n    } catch (MalformedObjectNameException e) {\n      LOG.error(\"Caught an exception while processing JMX request\", e);\n      response.setStatus(HttpServletResponse.SC_BAD_REQUEST);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.jmx.JMXJsonServlet.isInstrumentationAccessAllowed": "  protected boolean isInstrumentationAccessAllowed(HttpServletRequest request, \n      HttpServletResponse response) throws IOException {\n    return HttpServer2.isInstrumentationAccessAllowed(getServletContext(),\n        request, response);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.lib.StaticUserWebFilter.doFilter": "    public void doFilter(ServletRequest request, ServletResponse response,\n                         FilterChain chain\n                         ) throws IOException, ServletException {\n      HttpServletRequest httpRequest = (HttpServletRequest) request;\n      // if the user is already authenticated, don't override it\n      if (httpRequest.getRemoteUser() != null) {\n        chain.doFilter(request, response);\n      } else {\n        HttpServletRequestWrapper wrapper = \n            new HttpServletRequestWrapper(httpRequest) {\n          @Override\n          public Principal getUserPrincipal() {\n            return user;\n          }\n          @Override\n          public String getRemoteUser() {\n            return username;\n          }\n        };\n        chain.doFilter(wrapper, response);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.lib.StaticUserWebFilter.getRemoteUser": "          public String getRemoteUser() {\n            return username;\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.doFilter": "    public void doFilter(ServletRequest request,\n                         ServletResponse response,\n                         FilterChain chain\n                         ) throws IOException, ServletException {\n      HttpServletRequestWrapper quoted =\n        new RequestQuoter((HttpServletRequest) request);\n      HttpServletResponse httpResponse = (HttpServletResponse) response;\n\n      String mime = inferMimeType(request);\n      if (mime == null) {\n        httpResponse.setContentType(\"text/plain; charset=utf-8\");\n      } else if (mime.startsWith(\"text/html\")) {\n        // HTML with unspecified encoding, we want to\n        // force HTML with utf-8 encoding\n        // This is to avoid the following security issue:\n        // http://openmya.hacker.jp/hasegawa/security/utf7cs.html\n        httpResponse.setContentType(\"text/html; charset=utf-8\");\n      } else if (mime.startsWith(\"application/xml\")) {\n        httpResponse.setContentType(\"text/xml; charset=utf-8\");\n      }\n      headerMap.forEach((k, v) -> httpResponse.addHeader(k, v));\n      chain.doFilter(quoted, httpResponse);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.inferMimeType": "    private String inferMimeType(ServletRequest request) {\n      String path = ((HttpServletRequest)request).getRequestURI();\n      ServletContextHandler.Context sContext =\n          (ServletContextHandler.Context)config.getServletContext();\n      String mime = sContext.getMimeType(path);\n      return (mime == null) ? null : mime;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.NoCacheFilter.doFilter": "  public void doFilter(ServletRequest req, ServletResponse res,\n                       FilterChain chain)\n    throws IOException, ServletException {\n    HttpServletResponse httpRes = (HttpServletResponse) res;\n    httpRes.setHeader(\"Cache-Control\", \"no-cache\");\n    long now = System.currentTimeMillis();\n    httpRes.addDateHeader(\"Expires\", now);\n    httpRes.addDateHeader(\"Date\", now);\n    httpRes.addHeader(\"Pragma\", \"no-cache\");\n    chain.doFilter(req, res);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getDiskBalancerStatus": "  public String getDiskBalancerStatus() {\n    try {\n      return this.diskBalancer.queryWorkStatus().toJsonString();\n    } catch (IOException ex) {\n      LOG.debug(\"Reading diskbalancer Status failed. ex:{}\", ex);\n      return \"\";\n    }\n  }"
        },
        "bug_report": {
            "Title": "NPE in DataNode due to uninitialized DiskBalancer",
            "Description": "{noformat}\r\n2018-06-28 05:11:47,650 ERROR org.apache.hadoop.jmx.JMXJsonServlet: getting attribute DiskBalancerStatus of Hadoop:service=DataNode,name=DataNodeInfo threw an exception\r\njavax.management.RuntimeMBeanException: java.lang.NullPointerException\r\n ***** TRACEBACK 4 *****\r\n at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:839)\r\n at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrowMaybeMBeanException(DefaultMBeanServerInterceptor.java:852)\r\n at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:651)\r\n at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)\r\n at org.apache.hadoop.jmx.JMXJsonServlet.writeAttribute(JMXJsonServlet.java:338)\r\n at org.apache.hadoop.jmx.JMXJsonServlet.listBeans(JMXJsonServlet.java:316)\r\n at org.apache.hadoop.jmx.JMXJsonServlet.doGet(JMXJsonServlet.java:210)\r\n at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\r\n at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\r\n at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\r\n at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)\r\n at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)\r\n at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1537)\r\n at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\r\n at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\r\n at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\r\n at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\r\n at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\r\n at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\r\n at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\r\n at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\r\n at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\r\n at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\r\n at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\r\n at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\r\n at org.eclipse.jetty.server.Server.handle(Server.java:534)\r\n at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\r\n at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\r\n at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\r\n at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\r\n at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\r\n at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\r\n at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\r\n at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\r\n at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\r\n at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\r\n at java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.NullPointerException\r\n at org.apache.hadoop.hdfs.server.datanode.DataNode.getDiskBalancerStatus(DataNode.java:3146)\r\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n at java.lang.reflect.Method.invoke(Method.java:498)\r\n at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)\r\n at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)\r\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n at java.lang.reflect.Method.invoke(Method.java:498)\r\n at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)\r\n at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)\r\n at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)\r\n at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)\r\n at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)\r\n at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)\r\n at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)\r\n at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)\r\n at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)\r\n2018-06-28 05:12:08,400 ERROR org.apache.hadoop.jmx.JMXJsonServlet: getting attribute DiskBalancerStatus of Hadoop:service=DataNode,name=DataNodeInfo threw an exception\r\njavax.management.RuntimeMBeanException: java.lang.NullPointerException\r\n{noformat}\r\n\r\nWe have seen the above exception at datanode startup time. Should improve the NPE. Changing it to an IOE will also allow jmx to return '' correctly for \\{{getDiskBalancerStatus}}\r\n\r\n.\r\n\r\n\u00a0"
        }
    },
    {
        "filename": "HDFS-7180.json",
        "creation_time": "2014-10-02T03:07:04.000+0000",
        "stack_trace": "```\njava.io.IOException: Bad response ERROR for block BP-1960069741-10.0.3.170-1410430543652:blk_1074363564_623643 from datanode 10.0.3.176:50010\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:828)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.run": "      public void run() {\n\n        setName(\"ResponseProcessor for block \" + block);\n        PipelineAck ack = new PipelineAck();\n\n        while (!responderClosed && dfsClient.clientRunning && !isLastPacketInBlock) {\n          // process responses from datanodes.\n          try {\n            // read an ack from the pipeline\n            long begin = Time.monotonicNow();\n            ack.readFields(blockReplyStream);\n            long duration = Time.monotonicNow() - begin;\n            if (duration > dfsclientSlowLogThresholdMs\n                && ack.getSeqno() != Packet.HEART_BEAT_SEQNO) {\n              DFSClient.LOG\n                  .warn(\"Slow ReadProcessor read fields took \" + duration\n                      + \"ms (threshold=\" + dfsclientSlowLogThresholdMs + \"ms); ack: \"\n                      + ack + \", targets: \" + Arrays.asList(targets));\n            } else if (DFSClient.LOG.isDebugEnabled()) {\n              DFSClient.LOG.debug(\"DFSClient \" + ack);\n            }\n\n            long seqno = ack.getSeqno();\n            // processes response status from datanodes.\n            for (int i = ack.getNumOfReplies()-1; i >=0  && dfsClient.clientRunning; i--) {\n              final Status reply = ack.getReply(i);\n              // Restart will not be treated differently unless it is\n              // the local node or the only one in the pipeline.\n              if (PipelineAck.isRestartOOBStatus(reply) &&\n                  shouldWaitForRestart(i)) {\n                restartDeadline = dfsClient.getConf().datanodeRestartTimeout +\n                    Time.now();\n                setRestartingNodeIndex(i);\n                String message = \"A datanode is restarting: \" + targets[i];\n                DFSClient.LOG.info(message);\n               throw new IOException(message);\n              }\n              // node error\n              if (reply != SUCCESS) {\n                setErrorIndex(i); // first bad datanode\n                throw new IOException(\"Bad response \" + reply +\n                    \" for block \" + block +\n                    \" from datanode \" + \n                    targets[i]);\n              }\n            }\n            \n            assert seqno != PipelineAck.UNKOWN_SEQNO : \n              \"Ack for unknown seqno should be a failed ack: \" + ack;\n            if (seqno == Packet.HEART_BEAT_SEQNO) {  // a heartbeat ack\n              continue;\n            }\n\n            // a success ack for a data packet\n            Packet one;\n            synchronized (dataQueue) {\n              one = ackQueue.getFirst();\n            }\n            if (one.seqno != seqno) {\n              throw new IOException(\"ResponseProcessor: Expecting seqno \" +\n                                    \" for block \" + block +\n                                    one.seqno + \" but received \" + seqno);\n            }\n            isLastPacketInBlock = one.lastPacketInBlock;\n\n            // Fail the packet write for testing in order to force a\n            // pipeline recovery.\n            if (DFSClientFaultInjector.get().failPacket() &&\n                isLastPacketInBlock) {\n              failPacket = true;\n              throw new IOException(\n                    \"Failing the last packet for testing.\");\n            }\n              \n            // update bytesAcked\n            block.setNumBytes(one.getLastByteOffsetBlock());\n\n            synchronized (dataQueue) {\n              lastAckedSeqno = seqno;\n              ackQueue.removeFirst();\n              dataQueue.notifyAll();\n            }\n          } catch (Exception e) {\n            if (!responderClosed) {\n              if (e instanceof IOException) {\n                setLastException((IOException)e);\n              }\n              hasError = true;\n              // If no explicit error report was received, mark the primary\n              // node as failed.\n              tryMarkPrimaryDatanodeFailed();\n              synchronized (dataQueue) {\n                dataQueue.notifyAll();\n              }\n              if (restartingNodeIndex == -1) {\n                DFSClient.LOG.warn(\"DFSOutputStream ResponseProcessor exception \"\n                     + \" for block \" + block, e);\n              }\n              responderClosed = true;\n            }\n          }\n        }\n      }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.endBlock": "    private void endBlock() {\n      if(DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"Closing old block \" + block);\n      }\n      this.setName(\"DataStreamer for file \" + src);\n      closeResponder();\n      closeStream();\n      setPipeline(null, null, null);\n      stage = BlockConstructionStage.PIPELINE_SETUP_CREATE;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.shouldWaitForRestart": "    boolean shouldWaitForRestart(int index) {\n      // Only one node in the pipeline.\n      if (nodes.length == 1) {\n        return true;\n      }\n\n      // Is it a local node?\n      InetAddress addr = null;\n      try {\n        addr = InetAddress.getByName(nodes[index].getIpAddr());\n      } catch (java.net.UnknownHostException e) {\n        // we are passing an ip address. this should not happen.\n        assert false;\n      }\n\n      if (addr != null && NetUtils.isLocalAddress(addr)) {\n        return true;\n      }\n      return false;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.setPipeline": "    private void setPipeline(DatanodeInfo[] nodes, StorageType[] storageTypes,\n        String[] storageIDs) {\n      this.nodes = nodes;\n      this.storageTypes = storageTypes;\n      this.storageIDs = storageIDs;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.setRestartingNodeIndex": "    synchronized void setRestartingNodeIndex(int idx) {\n      restartingNodeIndex = idx;\n      // If the data streamer has already set the primary node\n      // bad, clear it. It is likely that the write failed due to\n      // the DN shutdown. Even if it was a real failure, the pipeline\n      // recovery will take care of it.\n      errorIndex = -1;      \n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.writeTo": "    void writeTo(DataOutputStream stm) throws IOException {\n      final int dataLen = dataPos - dataStart;\n      final int checksumLen = checksumPos - checksumStart;\n      final int pktLen = HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\n\n      PacketHeader header = new PacketHeader(\n        pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen, syncBlock);\n      \n      if (checksumPos != dataStart) {\n        // Move the checksum to cover the gap. This can happen for the last\n        // packet or during an hflush/hsync call.\n        System.arraycopy(buf, checksumStart, buf, \n                         dataStart - checksumLen , checksumLen); \n        checksumPos = dataStart;\n        checksumStart = checksumPos - checksumLen;\n      }\n      \n      final int headerStart = checksumStart - header.getSerializedSize();\n      assert checksumStart + 1 >= header.getSerializedSize();\n      assert checksumPos == dataStart;\n      assert headerStart >= 0;\n      assert headerStart + header.getSerializedSize() == checksumStart;\n      \n      // Copy the header data into the buffer immediately preceding the checksum\n      // data.\n      System.arraycopy(header.getBytes(), 0, buf, headerStart,\n          header.getSerializedSize());\n      \n      // corrupt the data for testing.\n      if (DFSClientFaultInjector.get().corruptPacket()) {\n        buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^= 0xff;\n      }\n\n      // Write the now contiguous full packet to the output stream.\n      stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);\n\n      // undo corruption.\n      if (DFSClientFaultInjector.get().uncorruptPacket()) {\n        buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^= 0xff;\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.setupPipelineForAppendOrRecovery": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes == null || nodes.length == 0) {\n        String msg = \"Could not get block locations. \" + \"Source file \\\"\"\n            + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed = true;\n        return false;\n      }\n      \n      boolean success = false;\n      long newGS = 0L;\n      while (!success && !streamerClosed && dfsClient.clientRunning) {\n        // Sleep before reconnect if a dn is restarting.\n        // This process will be repeated until the deadline or the datanode\n        // starts back up.\n        if (restartingNodeIndex >= 0) {\n          // 4 seconds or the configured deadline period, whichever is shorter.\n          // This is the retry interval and recovery will be retried in this\n          // interval until timeout or success.\n          long delay = Math.min(dfsClient.getConf().datanodeRestartTimeout,\n              4000L);\n          try {\n            Thread.sleep(delay);\n          } catch (InterruptedException ie) {\n            lastException.set(new IOException(\"Interrupted while waiting for \" +\n                \"datanode to restart. \" + nodes[restartingNodeIndex]));\n            streamerClosed = true;\n            return false;\n          }\n        }\n        boolean isRecovery = hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex >= 0) {\n          StringBuilder pipelineMsg = new StringBuilder();\n          for (int j = 0; j < nodes.length; j++) {\n            pipelineMsg.append(nodes[j]);\n            if (j < nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length <= 1) {\n            lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                + \" are bad. Aborting...\"));\n            streamerClosed = true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n              \" in pipeline \" + pipelineMsg + \n              \": bad datanode \" + nodes[errorIndex]);\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes = new DatanodeInfo[nodes.length-1];\n          arraycopy(nodes, newnodes, errorIndex);\n\n          final StorageType[] newStorageTypes = new StorageType[newnodes.length];\n          arraycopy(storageTypes, newStorageTypes, errorIndex);\n\n          final String[] newStorageIDs = new String[newnodes.length];\n          arraycopy(storageIDs, newStorageIDs, errorIndex);\n          \n          setPipeline(newnodes, newStorageTypes, newStorageIDs);\n\n          // Just took care of a node error while waiting for a node restart\n          if (restartingNodeIndex >= 0) {\n            // If the error came from a node further away than the restarting\n            // node, the restart must have been complete.\n            if (errorIndex > restartingNodeIndex) {\n              restartingNodeIndex = -1;\n            } else if (errorIndex < restartingNodeIndex) {\n              // the node index has shifted.\n              restartingNodeIndex--;\n            } else {\n              // this shouldn't happen...\n              assert false;\n            }\n          }\n\n          if (restartingNodeIndex == -1) {\n            hasError = false;\n          }\n          lastException.set(null);\n          errorIndex = -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n            nodes, isAppend, isHflushed)) {\n          try {\n            addDatanode2ExistingPipeline();\n          } catch(IOException ioe) {\n            if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n              throw ioe;\n            }\n            DFSClient.LOG.warn(\"Failed to replace datanode.\"\n                + \" Continue with the remaining datanodes since \"\n                + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_BEST_EFFORT_KEY\n                + \" is set to true.\", ioe);\n          }\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb = dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS = lb.getBlock().getGenerationStamp();\n        accessToken = lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        if (failPacket) { // for testing\n          success = createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n          failPacket = false;\n          try {\n            // Give DNs time to send in bad reports. In real situations,\n            // good reports should follow bad ones, if client committed\n            // with those nodes.\n            Thread.sleep(2000);\n          } catch (InterruptedException ie) {}\n        } else {\n          success = createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n        }\n\n        if (restartingNodeIndex >= 0) {\n          assert hasError == true;\n          // check errorIndex set above\n          if (errorIndex == restartingNodeIndex) {\n            // ignore, if came from the restarting node\n            errorIndex = -1;\n          }\n          // still within the deadline\n          if (Time.now() < restartDeadline) {\n            continue; // with in the deadline\n          }\n          // expired. declare the restarting node dead\n          restartDeadline = 0;\n          int expiredNodeIndex = restartingNodeIndex;\n          restartingNodeIndex = -1;\n          DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n              nodes[expiredNodeIndex]);\n          // Mark the restarting node as failed. If there is any other failed\n          // node during the last pipeline construction attempt, it will not be\n          // overwritten/dropped. In this case, the restarting node will get\n          // excluded in the following attempt, if it still does not come up.\n          if (errorIndex == -1) {\n            errorIndex = expiredNodeIndex;\n          }\n          // From this point on, normal pipeline recovery applies.\n        }\n      } // while\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock = new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n            nodes, storageIDs);\n        // update client side generation stamp\n        block = newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.close": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e = lastException.getAndSet(null);\n      if (e == null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket != null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock != 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket = new Packet(0, 0, bytesCurBlock, \n            currentSeqno++, this.checksum.getChecksumSize());\n        currentPacket.lastPacketInBlock = true;\n        currentPacket.syncBlock = shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock = streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      closed = true;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.initDataStreaming": "    private void initDataStreaming() {\n      this.setName(\"DataStreamer for file \" + src +\n          \" block \" + block);\n      response = new ResponseProcessor(nodes);\n      response.start();\n      stage = BlockConstructionStage.DATA_STREAMING;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.closeInternal": "    private void closeInternal() {\n      closeResponder();       // close and join\n      closeStream();\n      streamerClosed = true;\n      closed = true;\n      synchronized (dataQueue) {\n        dataQueue.notifyAll();\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.getLastByteOffsetBlock": "    long getLastByteOffsetBlock() {\n      return offsetInBlock + dataPos - dataStart;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.tryMarkPrimaryDatanodeFailed": "    synchronized void tryMarkPrimaryDatanodeFailed() {\n      // There should be no existing error and no ongoing restart.\n      if ((errorIndex == -1) && (restartingNodeIndex == -1)) {\n        errorIndex = 0;\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.setErrorIndex": "    synchronized void setErrorIndex(int idx) {\n      errorIndex = idx;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.processDatanodeError": "    private boolean processDatanodeError() throws IOException {\n      if (response != null) {\n        DFSClient.LOG.info(\"Error Recovery for \" + block +\n        \" waiting for responder to exit. \");\n        return true;\n      }\n      closeStream();\n\n      // move packets from ack queue to front of the data queue\n      synchronized (dataQueue) {\n        dataQueue.addAll(0, ackQueue);\n        ackQueue.clear();\n      }\n\n      // Record the new pipeline failure recovery.\n      if (lastAckedSeqnoBeforeFailure != lastAckedSeqno) {\n         lastAckedSeqnoBeforeFailure = lastAckedSeqno;\n         pipelineRecoveryCount = 1;\n      } else {\n        // If we had to recover the pipeline five times in a row for the\n        // same packet, this client likely has corrupt data or corrupting\n        // during transmission.\n        if (++pipelineRecoveryCount > 5) {\n          DFSClient.LOG.warn(\"Error recovering pipeline for writing \" +\n              block + \". Already retried 5 times for the same packet.\");\n          lastException.set(new IOException(\"Failing write. Tried pipeline \" +\n              \"recovery 5 times without success.\"));\n          streamerClosed = true;\n          return false;\n        }\n      }\n      boolean doSleep = setupPipelineForAppendOrRecovery();\n      \n      if (!streamerClosed && dfsClient.clientRunning) {\n        if (stage == BlockConstructionStage.PIPELINE_CLOSE) {\n\n          // If we had an error while closing the pipeline, we go through a fast-path\n          // where the BlockReceiver does not run. Instead, the DataNode just finalizes\n          // the block immediately during the 'connect ack' process. So, we want to pull\n          // the end-of-block packet from the dataQueue, since we don't actually have\n          // a true pipeline to send it over.\n          //\n          // We also need to set lastAckedSeqno to the end-of-block Packet's seqno, so that\n          // a client waiting on close() will be aware that the flush finished.\n          synchronized (dataQueue) {\n            Packet endOfBlockPacket = dataQueue.remove();  // remove the end of block packet\n            assert endOfBlockPacket.lastPacketInBlock;\n            assert lastAckedSeqno == endOfBlockPacket.seqno - 1;\n            lastAckedSeqno = endOfBlockPacket.seqno;\n            dataQueue.notifyAll();\n          }\n          endBlock();\n        } else {\n          initDataStreaming();\n        }\n      }\n      \n      return doSleep;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.nextBlockOutputStream": "    private LocatedBlock nextBlockOutputStream() throws IOException {\n      LocatedBlock lb = null;\n      DatanodeInfo[] nodes = null;\n      StorageType[] storageTypes = null;\n      int count = dfsClient.getConf().nBlockWriteRetry;\n      boolean success = false;\n      ExtendedBlock oldBlock = block;\n      do {\n        hasError = false;\n        lastException.set(null);\n        errorIndex = -1;\n        success = false;\n\n        long startTime = Time.now();\n        DatanodeInfo[] excluded =\n            excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n            .keySet()\n            .toArray(new DatanodeInfo[0]);\n        block = oldBlock;\n        lb = locateFollowingBlock(startTime,\n            excluded.length > 0 ? excluded : null);\n        block = lb.getBlock();\n        block.setNumBytes(0);\n        bytesSent = 0;\n        accessToken = lb.getBlockToken();\n        nodes = lb.getLocations();\n        storageTypes = lb.getStorageTypes();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success = createBlockOutputStream(nodes, storageTypes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning \" + block);\n          dfsClient.namenode.abandonBlock(block, fileId, src,\n              dfsClient.clientName);\n          block = null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n        }\n      } while (!success && --count >= 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return lb;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.setLastException": "    private void setLastException(IOException e) {\n      lastException.compareAndSet(null, e);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.isHeartbeatPacket": "    private boolean isHeartbeatPacket() {\n      return seqno == HEART_BEAT_SEQNO;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.dfsClient.getConf": "  public Conf getConf() {\n    return dfsClientConf;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClientFaultInjector.get": "  public static DFSClientFaultInjector get() {\n    return instance;\n  }"
        },
        "bug_report": {
            "Title": "NFSv3 gateway frequently gets stuck due to GC",
            "Description": "We are using Hadoop 2.5.0 (HDFS only) and start and mount the NFSv3 gateway on one node in the cluster to let users upload data with rsync.\n\nHowever, we find the NFSv3 daemon seems frequently get stuck while the HDFS seems working well. (hdfds dfs -ls and etc. works just well). The last stuck we found is after around 1 day running and several hundreds GBs of data uploaded.\n\nThe NFSv3 daemon is started on one node and on the same node the NFS is mounted.\n\nFrom the node where the NFS is mounted:\n\ndmsg shows like this:\n\n[1859245.368108] nfs: server localhost not responding, still trying\n[1859245.368111] nfs: server localhost not responding, still trying\n[1859245.368115] nfs: server localhost not responding, still trying\n[1859245.368119] nfs: server localhost not responding, still trying\n[1859245.368123] nfs: server localhost not responding, still trying\n[1859245.368127] nfs: server localhost not responding, still trying\n[1859245.368131] nfs: server localhost not responding, still trying\n[1859245.368135] nfs: server localhost not responding, still trying\n[1859245.368138] nfs: server localhost not responding, still trying\n[1859245.368142] nfs: server localhost not responding, still trying\n[1859245.368146] nfs: server localhost not responding, still trying\n[1859245.368150] nfs: server localhost not responding, still trying\n[1859245.368153] nfs: server localhost not responding, still trying\n\nThe mounted directory can not be `ls` and `df -hT` gets stuck too.\n\nThe latest lines from the nfs3 log in the hadoop logs directory:\n\n2014-10-02 05:43:20,452 INFO org.apache.hadoop.nfs.nfs3.IdUserGroup: Updated user map size: 35\n2014-10-02 05:43:20,461 INFO org.apache.hadoop.nfs.nfs3.IdUserGroup: Updated group map size: 54\n2014-10-02 05:44:40,374 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:44:40,732 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:46:06,535 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:46:26,075 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:47:56,420 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:48:56,477 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:51:46,750 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:53:23,809 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:53:24,508 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:55:57,334 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:57:07,428 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:58:32,609 INFO org.apache.hadoop.nfs.nfs3.IdUserGroup: Update cache now\n2014-10-02 05:58:32,610 INFO org.apache.hadoop.nfs.nfs3.IdUserGroup: Not doing static UID/GID mapping because '/etc/nfs.map' does not exist.\n2014-10-02 05:58:32,620 INFO org.apache.hadoop.nfs.nfs3.IdUserGroup: Updated user map size: 35\n2014-10-02 05:58:32,628 INFO org.apache.hadoop.nfs.nfs3.IdUserGroup: Updated group map size: 54\n2014-10-02 06:01:32,098 WARN org.apache.hadoop.hdfs.DFSClient: Slow ReadProcessor read fields took 60062ms (threshold=30000ms); ack: seqno: -2 status: SUCCESS status: ERROR downstreamAckTimeNanos: 0, targets: [10.0.3.172:50010, 10.0.3.176:50010]\n2014-10-02 06:01:32,099 WARN org.apache.hadoop.hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-1960069741-10.0.3.170-1410430543652:blk_1074363564_623643\njava.io.IOException: Bad response ERROR for block BP-1960069741-10.0.3.170-1410430543652:blk_1074363564_623643 from datanode 10.0.3.176:50010\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:828)\n2014-10-02 06:07:00,368 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block BP-1960069741-10.0.3.170-1410430543652:blk_1074363564_623643 in pipeline 10.0.3.172:50010, 10.0.3.176:50010: bad datanode 10.0.3.176:50010\n\nThe logs seems suggest 10.0.3.176 is bad. However, from the `hdfs dfsadmin -report`, all nodes in the cluster seems working.\n\nAny help will be appreciated. Thanks in advance."
        }
    },
    {
        "filename": "HDFS-6102.json",
        "creation_time": "2014-03-13T18:27:36.000+0000",
        "stack_trace": "```\ncom.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.\n        at com.google.protobuf.InvalidProtocolBufferException.sizeLimitExceeded(InvalidProtocolBufferException.java:110)\n        at com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:755)\n        at com.google.protobuf.CodedInputStream.readRawByte(CodedInputStream.java:769)\n        at com.google.protobuf.CodedInputStream.readRawVarint64(CodedInputStream.java:462)\n        at com.google.protobuf.CodedInputStream.readUInt64(CodedInputStream.java:188)\n        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.<init>(FsImageProto.java:9839)\n        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.<init>(FsImageProto.java:9770)\n        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$1.parsePartialFrom(FsImageProto.java:9901)\n        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$1.parsePartialFrom(FsImageProto.java:9896)\n```",
        "source_code": {},
        "bug_report": {
            "Title": "Lower the default maximum items per directory to fix PB fsimage loading",
            "Description": "Found by [~schu] during testing. We were creating a bunch of directories in a single directory to blow up the fsimage size, and it ends up we hit this error when trying to load a very large fsimage:\n\n{noformat}\n2014-03-13 13:57:03,901 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 24523605 INodes.\n2014-03-13 13:57:59,038 ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: Failed to load image from FSImageFile(file=/dfs/nn/current/fsimage_0000000000024532742, cpktTxId=0000000000024532742)\ncom.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.\n        at com.google.protobuf.InvalidProtocolBufferException.sizeLimitExceeded(InvalidProtocolBufferException.java:110)\n        at com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:755)\n        at com.google.protobuf.CodedInputStream.readRawByte(CodedInputStream.java:769)\n        at com.google.protobuf.CodedInputStream.readRawVarint64(CodedInputStream.java:462)\n        at com.google.protobuf.CodedInputStream.readUInt64(CodedInputStream.java:188)\n        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.<init>(FsImageProto.java:9839)\n        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.<init>(FsImageProto.java:9770)\n        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$1.parsePartialFrom(FsImageProto.java:9901)\n        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$1.parsePartialFrom(FsImageProto.java:9896)\n        at 52)\n...\n{noformat}\n\nSome further research reveals there's a 64MB max size per PB message, which seems to be what we're hitting here."
        }
    },
    {
        "filename": "HDFS-6250.json",
        "creation_time": "2014-04-16T16:14:32.000+0000",
        "stack_trace": "```\njava.lang.AssertionError: expected:<1800> but was:<1810>\n\tat org.junit.Assert.fail(Assert.java:93)\n\tat org.junit.Assert.failNotEquals(Assert.java:647)\n\tat org.junit.Assert.assertEquals(Assert.java:128)\n\tat org.junit.Assert.assertEquals(Assert.java:147)\n\tat org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup.testBalancerWithRackLocality(TestBalancerWithNodeGroup.java:253)\n```",
        "source_code": {},
        "bug_report": {
            "Title": "TestBalancerWithNodeGroup.testBalancerWithRackLocality fails",
            "Description": "It was seen in https://builds.apache.org/job/PreCommit-HDFS-Build/6669/\n\n{panel}\njava.lang.AssertionError: expected:<1800> but was:<1810>\n\tat org.junit.Assert.fail(Assert.java:93)\n\tat org.junit.Assert.failNotEquals(Assert.java:647)\n\tat org.junit.Assert.assertEquals(Assert.java:128)\n\tat org.junit.Assert.assertEquals(Assert.java:147)\n\tat org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup\n .testBalancerWithRackLocality(TestBalancerWithNodeGroup.java:253)\n{panel}"
        }
    },
    {
        "filename": "HDFS-11377.json",
        "creation_time": "2017-01-26T23:40:53.000+0000",
        "stack_trace": "```\njava.lang.Thread.State: TIMED_WAITING (sleeping)\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher.waitForMoveCompletion(Dispatcher.java:1043)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchBlockMoves(Dispatcher.java:1017)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchAndCheckContinue(Dispatcher.java:981)\n        at org.apache.hadoop.hdfs.server.balancer.Balancer.runOneIteration(Balancer.java:611)\n        at org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:663)\n        at org.apache.hadoop.hdfs.server.balancer.Balancer$Cli.run(Balancer.java:776)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n        at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:905)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.waitForMoveCompletion": "  public static boolean waitForMoveCompletion(\n      Iterable<? extends StorageGroup> targets) {\n    boolean hasFailure = false;\n    for(;;) {\n      boolean empty = true;\n      for (StorageGroup t : targets) {\n        if (!t.getDDatanode().isPendingQEmpty()) {\n          empty = false;\n          break;\n        } else {\n          hasFailure |= t.getDDatanode().hasFailure;\n        }\n      }\n      if (empty) {\n        return hasFailure; // all pending queues are empty\n      }\n      try {\n        Thread.sleep(1000);\n      } catch (InterruptedException ignored) {\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.getDDatanode": "      private DDatanode getDDatanode() {\n        return DDatanode.this;\n      }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.isPendingQEmpty": "    synchronized boolean isPendingQEmpty() {\n      return pendings.isEmpty();\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchBlockMoves": "  private long dispatchBlockMoves() throws InterruptedException {\n    final long bytesLastMoved = getBytesMoved();\n    final Future<?>[] futures = new Future<?>[sources.size()];\n\n    final Iterator<Source> i = sources.iterator();\n    for (int j = 0; j < futures.length; j++) {\n      final Source s = i.next();\n      futures[j] = dispatchExecutor.submit(new Runnable() {\n        @Override\n        public void run() {\n          s.dispatchBlocks();\n        }\n      });\n    }\n\n    // wait for all dispatcher threads to finish\n    for (Future<?> future : futures) {\n      try {\n        future.get();\n      } catch (ExecutionException e) {\n        LOG.warn(\"Dispatcher thread failed\", e.getCause());\n      }\n    }\n\n    // wait for all reportedBlock moving to be done\n    waitForMoveCompletion(targets);\n\n    return getBytesMoved() - bytesLastMoved;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.get": "    public G get(String datanodeUuid, StorageType storageType) {\n      return map.get(toKey(datanodeUuid, storageType));\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.size": "    int size() {\n      return map.size();\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchBlocks": "    private void dispatchBlocks() {\n      this.blocksToReceive = 2 * getScheduledSize();\n      long previousMoveTimestamp = Time.monotonicNow();\n      while (getScheduledSize() > 0 && !isIterationOver()\n          && (!srcBlocks.isEmpty() || blocksToReceive > 0)) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(this + \" blocksToReceive=\" + blocksToReceive\n              + \", scheduledSize=\" + getScheduledSize()\n              + \", srcBlocks#=\" + srcBlocks.size());\n        }\n        final PendingMove p = chooseNextMove();\n        if (p != null) {\n          // Reset previous move timestamp\n          previousMoveTimestamp = Time.monotonicNow();\n          executePendingMove(p);\n          continue;\n        }\n\n        // Since we cannot schedule any block to move,\n        // remove any moved blocks from the source block list and\n        removeMovedBlocks(); // filter already moved blocks\n        // check if we should fetch more blocks from the namenode\n        if (shouldFetchMoreBlocks()) {\n          // fetch new blocks\n          try {\n            final long received = getBlockList();\n            if (received == 0) {\n              return;\n            }\n            blocksToReceive -= received;\n            continue;\n          } catch (IOException e) {\n            LOG.warn(\"Exception while getting reportedBlock list\", e);\n            return;\n          }\n        } else {\n          // jump out of while-loop after the configured timeout.\n          long noMoveInterval = Time.monotonicNow() - previousMoveTimestamp;\n          if (noMoveInterval > maxNoMoveInterval) {\n            LOG.info(\"Failed to find a pending move for \"  + noMoveInterval\n                + \" ms.  Skipping \" + this);\n            resetScheduledSize();\n          }\n        }\n\n        // Now we can not schedule any block to move and there are\n        // no new blocks added to the source block list, so we wait.\n        try {\n          synchronized (Dispatcher.this) {\n            Dispatcher.this.wait(1000); // wait for targets/sources to be idle\n          }\n          // Didn't find a possible move in this iteration of the while loop,\n          // adding a small delay before choosing next move again.\n          Thread.sleep(100);\n        } catch (InterruptedException ignored) {\n        }\n      }\n\n      if (isIterationOver()) {\n        LOG.info(\"The maximum iteration time (\" + MAX_ITERATION_TIME/1000\n            + \" seconds) has been reached. Stopping \" + this);\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.getBytesMoved": "  long getBytesMoved() {\n    return nnc.getBytesMoved().get();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchAndCheckContinue": "  public boolean dispatchAndCheckContinue() throws InterruptedException {\n    return nnc.shouldContinue(dispatchBlockMoves());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Balancer.runOneIteration": "  Result runOneIteration() {\n    try {\n      final List<DatanodeStorageReport> reports = dispatcher.init();\n      final long bytesLeftToMove = init(reports);\n      if (bytesLeftToMove == 0) {\n        System.out.println(\"The cluster is balanced. Exiting...\");\n        return newResult(ExitStatus.SUCCESS, bytesLeftToMove, 0);\n      } else {\n        LOG.info( \"Need to move \"+ StringUtils.byteDesc(bytesLeftToMove)\n            + \" to make the cluster balanced.\" );\n      }\n\n      // Should not run the balancer during an unfinalized upgrade, since moved\n      // blocks are not deleted on the source datanode.\n      if (!runDuringUpgrade && nnc.isUpgrading()) {\n        System.err.println(\"Balancer exiting as upgrade is not finalized, \"\n            + \"please finalize the HDFS upgrade before running the balancer.\");\n        LOG.error(\"Balancer exiting as upgrade is not finalized, \"\n            + \"please finalize the HDFS upgrade before running the balancer.\");\n        return newResult(ExitStatus.UNFINALIZED_UPGRADE, bytesLeftToMove, -1);\n      }\n\n      /* Decide all the nodes that will participate in the block move and\n       * the number of bytes that need to be moved from one node to another\n       * in this iteration. Maximum bytes to be moved per node is\n       * Min(1 Band worth of bytes,  MAX_SIZE_TO_MOVE).\n       */\n      final long bytesBeingMoved = chooseStorageGroups();\n      if (bytesBeingMoved == 0) {\n        System.out.println(\"No block can be moved. Exiting...\");\n        return newResult(ExitStatus.NO_MOVE_BLOCK, bytesLeftToMove, bytesBeingMoved);\n      } else {\n        LOG.info( \"Will move \" + StringUtils.byteDesc(bytesBeingMoved) +\n            \" in this iteration\");\n      }\n\n      /* For each pair of <source, target>, start a thread that repeatedly \n       * decide a block to be moved and its proxy source, \n       * then initiates the move until all bytes are moved or no more block\n       * available to move.\n       * Exit no byte has been moved for 5 consecutive iterations.\n       */\n      if (!dispatcher.dispatchAndCheckContinue()) {\n        return newResult(ExitStatus.NO_MOVE_PROGRESS, bytesLeftToMove, bytesBeingMoved);\n      }\n\n      return newResult(ExitStatus.IN_PROGRESS, bytesLeftToMove, bytesBeingMoved);\n    } catch (IllegalArgumentException e) {\n      System.out.println(e + \".  Exiting ...\");\n      return newResult(ExitStatus.ILLEGAL_ARGUMENTS);\n    } catch (IOException e) {\n      System.out.println(e + \".  Exiting ...\");\n      return newResult(ExitStatus.IO_EXCEPTION);\n    } catch (InterruptedException e) {\n      System.out.println(e + \".  Exiting ...\");\n      return newResult(ExitStatus.INTERRUPTED);\n    } finally {\n      dispatcher.shutdownNow();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Balancer.init": "  private long init(List<DatanodeStorageReport> reports) {\n    // compute average utilization\n    for (DatanodeStorageReport r : reports) {\n      policy.accumulateSpaces(r);\n    }\n    policy.initAvgUtilization();\n\n    // create network topology and classify utilization collections: \n    //   over-utilized, above-average, below-average and under-utilized.\n    long overLoadedBytes = 0L, underLoadedBytes = 0L;\n    for(DatanodeStorageReport r : reports) {\n      final DDatanode dn = dispatcher.newDatanode(r.getDatanodeInfo());\n      final boolean isSource = Util.isIncluded(sourceNodes, dn.getDatanodeInfo());\n      for(StorageType t : StorageType.getMovableTypes()) {\n        final Double utilization = policy.getUtilization(r, t);\n        if (utilization == null) { // datanode does not have such storage type \n          continue;\n        }\n        \n        final double average = policy.getAvgUtilization(t);\n        if (utilization >= average && !isSource) {\n          LOG.info(dn + \"[\" + t + \"] has utilization=\" + utilization\n              + \" >= average=\" + average\n              + \" but it is not specified as a source; skipping it.\");\n          continue;\n        }\n\n        final double utilizationDiff = utilization - average;\n        final long capacity = getCapacity(r, t);\n        final double thresholdDiff = Math.abs(utilizationDiff) - threshold;\n        final long maxSize2Move = computeMaxSize2Move(capacity,\n            getRemaining(r, t), utilizationDiff, maxSizeToMove);\n\n        final StorageGroup g;\n        if (utilizationDiff > 0) {\n          final Source s = dn.addSource(t, maxSize2Move, dispatcher);\n          if (thresholdDiff <= 0) { // within threshold\n            aboveAvgUtilized.add(s);\n          } else {\n            overLoadedBytes += percentage2bytes(thresholdDiff, capacity);\n            overUtilized.add(s);\n          }\n          g = s;\n        } else {\n          g = dn.addTarget(t, maxSize2Move);\n          if (thresholdDiff <= 0) { // within threshold\n            belowAvgUtilized.add(g);\n          } else {\n            underLoadedBytes += percentage2bytes(thresholdDiff, capacity);\n            underUtilized.add(g);\n          }\n        }\n        dispatcher.getStorageGroupMap().put(g);\n      }\n    }\n\n    logUtilizationCollections();\n    \n    Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n        == overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n           + belowAvgUtilized.size(),\n        \"Mismatched number of storage groups\");\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Balancer.chooseStorageGroups": "  private <G extends StorageGroup, C extends StorageGroup>\n      void chooseStorageGroups(Collection<G> groups, Collection<C> candidates,\n          Matcher matcher) {\n    for(final Iterator<G> i = groups.iterator(); i.hasNext();) {\n      final G g = i.next();\n      for(; choose4One(g, candidates, matcher); );\n      if (!g.hasSpaceForScheduling()) {\n        i.remove();\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Balancer.newResult": "  Result newResult(ExitStatus exitStatus) {\n    return new Result(exitStatus, -1, -1, dispatcher.getBytesMoved());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Balancer.run": "    public int run(String[] args) {\n      final long startTime = Time.monotonicNow();\n      final Configuration conf = getConf();\n\n      try {\n        checkReplicationPolicyCompatibility(conf);\n\n        final Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);\n        return Balancer.run(namenodes, parse(args), conf);\n      } catch (IOException e) {\n        System.out.println(e + \".  Exiting ...\");\n        return ExitStatus.IO_EXCEPTION.getExitCode();\n      } catch (InterruptedException e) {\n        System.out.println(e + \".  Exiting ...\");\n        return ExitStatus.INTERRUPTED.getExitCode();\n      } finally {\n        System.out.format(\"%-24s \",\n            DateFormat.getDateTimeInstance().format(new Date()));\n        System.out.println(\"Balancing took \"\n            + time2Str(Time.monotonicNow() - startTime));\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Balancer.print": "    void print(int iteration, PrintStream out) {\n      out.printf(\"%-24s %10d  %19s  %18s  %17s%n\",\n          DateFormat.getDateTimeInstance().format(new Date()), iteration,\n          StringUtils.byteDesc(bytesAlreadyMoved),\n          StringUtils.byteDesc(bytesLeftToMove),\n          StringUtils.byteDesc(bytesBeingMoved));\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Balancer.parse": "    static BalancerParameters parse(String[] args) {\n      Set<String> excludedNodes = null;\n      Set<String> includedNodes = null;\n      BalancerParameters.Builder b = new BalancerParameters.Builder();\n\n      if (args != null) {\n        try {\n          for(int i = 0; i < args.length; i++) {\n            if (\"-threshold\".equalsIgnoreCase(args[i])) {\n              checkArgument(++i < args.length,\n                \"Threshold value is missing: args = \" + Arrays.toString(args));\n              try {\n                double threshold = Double.parseDouble(args[i]);\n                if (threshold < 1 || threshold > 100) {\n                  throw new IllegalArgumentException(\n                      \"Number out of range: threshold = \" + threshold);\n                }\n                LOG.info( \"Using a threshold of \" + threshold );\n                b.setThreshold(threshold);\n              } catch(IllegalArgumentException e) {\n                System.err.println(\n                    \"Expecting a number in the range of [1.0, 100.0]: \"\n                    + args[i]);\n                throw e;\n              }\n            } else if (\"-policy\".equalsIgnoreCase(args[i])) {\n              checkArgument(++i < args.length,\n                \"Policy value is missing: args = \" + Arrays.toString(args));\n              try {\n                b.setBalancingPolicy(BalancingPolicy.parse(args[i]));\n              } catch(IllegalArgumentException e) {\n                System.err.println(\"Illegal policy name: \" + args[i]);\n                throw e;\n              }\n            } else if (\"-exclude\".equalsIgnoreCase(args[i])) {\n              excludedNodes = new HashSet<>();\n              i = processHostList(args, i, \"exclude\", excludedNodes);\n              b.setExcludedNodes(excludedNodes);\n            } else if (\"-include\".equalsIgnoreCase(args[i])) {\n              includedNodes = new HashSet<>();\n              i = processHostList(args, i, \"include\", includedNodes);\n              b.setIncludedNodes(includedNodes);\n            } else if (\"-source\".equalsIgnoreCase(args[i])) {\n              Set<String> sourceNodes = new HashSet<>();\n              i = processHostList(args, i, \"source\", sourceNodes);\n              b.setSourceNodes(sourceNodes);\n            } else if (\"-blockpools\".equalsIgnoreCase(args[i])) {\n              checkArgument(\n                  ++i < args.length,\n                  \"blockpools value is missing: args = \"\n                      + Arrays.toString(args));\n              Set<String> blockpools = parseBlockPoolList(args[i]);\n              LOG.info(\"Balancer will run on the following blockpools: \"\n                  + blockpools.toString());\n              b.setBlockpools(blockpools);\n            } else if (\"-idleiterations\".equalsIgnoreCase(args[i])) {\n              checkArgument(++i < args.length,\n                  \"idleiterations value is missing: args = \" + Arrays\n                      .toString(args));\n              int maxIdleIteration = Integer.parseInt(args[i]);\n              LOG.info(\"Using a idleiterations of \" + maxIdleIteration);\n              b.setMaxIdleIteration(maxIdleIteration);\n            } else if (\"-runDuringUpgrade\".equalsIgnoreCase(args[i])) {\n              b.setRunDuringUpgrade(true);\n              LOG.info(\"Will run the balancer even during an ongoing HDFS \"\n                  + \"upgrade. Most users will not want to run the balancer \"\n                  + \"during an upgrade since it will not affect used space \"\n                  + \"on over-utilized machines.\");\n            } else {\n              throw new IllegalArgumentException(\"args = \"\n                  + Arrays.toString(args));\n            }\n          }\n          checkArgument(excludedNodes == null || includedNodes == null,\n              \"-exclude and -include options cannot be specified together.\");\n        } catch(RuntimeException e) {\n          printUsage(System.err);\n          throw e;\n        }\n      }\n      return b.build();\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Balancer.resetData": "  void resetData(Configuration conf) {\n    this.overUtilized.clear();\n    this.aboveAvgUtilized.clear();\n    this.belowAvgUtilized.clear();\n    this.underUtilized.clear();\n    this.policy.reset();\n    dispatcher.reset(conf);;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Balancer.checkReplicationPolicyCompatibility": "  private static void checkReplicationPolicyCompatibility(Configuration conf\n      ) throws UnsupportedActionException {\n    BlockPlacementPolicies placementPolicies =\n        new BlockPlacementPolicies(conf, null, null, null);\n    if (!(placementPolicies.getPolicy(CONTIGUOUS) instanceof\n        BlockPlacementPolicyDefault)) {\n      throw new UnsupportedActionException(\n          \"Balancer without BlockPlacementPolicyDefault\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Balancer.time2Str": "  private static String time2Str(long elapsedTime) {\n    String unit;\n    double time = elapsedTime;\n    if (elapsedTime < 1000) {\n      unit = \"milliseconds\";\n    } else if (elapsedTime < 60*1000) {\n      unit = \"seconds\";\n      time = time/1000;\n    } else if (elapsedTime < 3600*1000) {\n      unit = \"minutes\";\n      time = time/(60*1000);\n    } else {\n      unit = \"hours\";\n      time = time/(3600*1000);\n    }\n\n    return time+\" \"+unit;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Balancer.checkKeytabAndInit": "  private static void checkKeytabAndInit(Configuration conf)\n      throws IOException {\n    if (conf.getBoolean(DFSConfigKeys.DFS_BALANCER_KEYTAB_ENABLED_KEY,\n        DFSConfigKeys.DFS_BALANCER_KEYTAB_ENABLED_DEFAULT)) {\n      LOG.info(\"Keytab is configured, will login using keytab.\");\n      UserGroupInformation.setConfiguration(conf);\n      String addr = conf.get(DFSConfigKeys.DFS_BALANCER_ADDRESS_KEY,\n          DFSConfigKeys.DFS_BALANCER_ADDRESS_DEFAULT);\n      InetSocketAddress socAddr = NetUtils.createSocketAddr(addr, 0,\n          DFSConfigKeys.DFS_BALANCER_ADDRESS_KEY);\n      SecurityUtil.login(conf, DFSConfigKeys.DFS_BALANCER_KEYTAB_FILE_KEY,\n          DFSConfigKeys.DFS_BALANCER_KERBEROS_PRINCIPAL_KEY,\n          socAddr.getHostName());\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ToolRunner.run": "  public static int run(Tool tool, String[] args) \n    throws Exception{\n    return run(tool.getConf(), tool, args);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Balancer.main": "  public static void main(String[] args) {\n    if (DFSUtil.parseHelpArgument(args, USAGE, System.out, true)) {\n      System.exit(0);\n    }\n\n    try {\n      System.exit(ToolRunner.run(new HdfsConfiguration(), new Cli(), args));\n    } catch (Throwable e) {\n      LOG.error(\"Exiting balancer due an exception\", e);\n      System.exit(-1);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.dispatcher.shutdownNow": "  public void shutdownNow() {\n    if (dispatchExecutor != null) {\n      dispatchExecutor.shutdownNow();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.dispatcher.dispatchAndCheckContinue": "  public boolean dispatchAndCheckContinue() throws InterruptedException {\n    return nnc.shouldContinue(dispatchBlockMoves());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.dispatcher.dispatchBlockMoves": "  private long dispatchBlockMoves() throws InterruptedException {\n    final long bytesLastMoved = getBytesMoved();\n    final Future<?>[] futures = new Future<?>[sources.size()];\n\n    final Iterator<Source> i = sources.iterator();\n    for (int j = 0; j < futures.length; j++) {\n      final Source s = i.next();\n      futures[j] = dispatchExecutor.submit(new Runnable() {\n        @Override\n        public void run() {\n          s.dispatchBlocks();\n        }\n      });\n    }\n\n    // wait for all dispatcher threads to finish\n    for (Future<?> future : futures) {\n      try {\n        future.get();\n      } catch (ExecutionException e) {\n        LOG.warn(\"Dispatcher thread failed\", e.getCause());\n      }\n    }\n\n    // wait for all reportedBlock moving to be done\n    waitForMoveCompletion(targets);\n\n    return getBytesMoved() - bytesLastMoved;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.NameNodeConnector.newNameNodeConnectors": "  public static List<NameNodeConnector> newNameNodeConnectors(\n      Map<URI, List<Path>> namenodes, String name, Path idPath,\n      Configuration conf, int maxIdleIterations) throws IOException {\n    final List<NameNodeConnector> connectors = new ArrayList<NameNodeConnector>(\n        namenodes.size());\n    for (Map.Entry<URI, List<Path>> entry : namenodes.entrySet()) {\n      NameNodeConnector nnc = new NameNodeConnector(name, entry.getKey(),\n          idPath, entry.getValue(), conf, maxIdleIterations);\n      nnc.getKeyManager().startBlockKeyUpdater();\n      connectors.add(nnc);\n    }\n    return connectors;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.NameNodeConnector.getKeyManager": "  public KeyManager getKeyManager() {\n    return keyManager;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs": "  public String[] getRemainingArgs() {\n    return (commandLine == null) ? new String[]{} : commandLine.getArgs();\n  }"
        },
        "bug_report": {
            "Title": "Balancer hung due to no available mover threads",
            "Description": "When running balancer on large cluster which have more than 3000 Datanodes, it might be hung due to \"No mover threads available\".\nThe stack trace shows it waiting forever like below.\n{code}\n\"main\" #1 prio=5 os_prio=0 tid=0x00007ff6cc014800 nid=0x6b2c waiting on condition [0x00007ff6d1bad000]\n   java.lang.Thread.State: TIMED_WAITING (sleeping)\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher.waitForMoveCompletion(Dispatcher.java:1043)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchBlockMoves(Dispatcher.java:1017)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchAndCheckContinue(Dispatcher.java:981)\n        at org.apache.hadoop.hdfs.server.balancer.Balancer.runOneIteration(Balancer.java:611)\n        at org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:663)\n        at org.apache.hadoop.hdfs.server.balancer.Balancer$Cli.run(Balancer.java:776)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n        at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:905)\n{code}\n\nIn the log, there are lots of WARN about \"No mover threads available\".\n{quote}\n2017-01-26 15:36:40,085 WARN org.apache.hadoop.hdfs.server.balancer.Dispatcher: No mover threads available: skip moving blk_13700554102_1112815018180 with size=268435456 from 10.115.67.137:50010:DISK to 10.140.21.55:50010:DISK through 10.115.67.137:50010\n2017-01-26 15:36:40,085 WARN org.apache.hadoop.hdfs.server.balancer.Dispatcher: No mover threads available: skip moving blk_4009558842_1103118359883 with size=268435456 from 10.115.67.137:50010:DISK to 10.140.21.55:50010:DISK through 10.115.67.137:50010\n2017-01-26 15:36:40,085 WARN org.apache.hadoop.hdfs.server.balancer.Dispatcher: No mover threads available: skip moving blk_13881956058_1112996460026 with size=133509566 from 10.115.67.137:50010:DISK to 10.140.21.55:50010:DISK through 10.115.67.36:50010\n{quote}\n\nWhat happened here is, when there are no mover threads available, DDatanode.isPendingQEmpty() will return false, so Balancer hung."
        }
    },
    {
        "filename": "HDFS-6753.json",
        "creation_time": "2014-07-25T11:36:43.000+0000",
        "stack_trace": "```\norg.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=4096 B) is less than the block size (=134217728 B).\n \nat org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:60)\n\njava.io.IOException: Invalid directory or I/O error occurred for dir: /mnt/tmp_Datanode/current/BP-1384489961-XX2.XX2.XX2.XX2-845784615183/current/finalized\n \nat org.apache.hadoop.fs.FileUtil.listFiles(FileUtil.java:1164)\n \nat org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.compileReport(DirectoryScanner.java:596)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy.chooseVolume": "  public synchronized V chooseVolume(final List<V> volumes, final long blockSize\n      ) throws IOException {\n    if(volumes.size() < 1) {\n      throw new DiskOutOfSpaceException(\"No more available volumes\");\n    }\n    \n    // since volumes could've been removed because of the failure\n    // make sure we are not out of bounds\n    if(curVolume >= volumes.size()) {\n      curVolume = 0;\n    }\n    \n    int startVolume = curVolume;\n    long maxAvailable = 0;\n    \n    while (true) {\n      final V volume = volumes.get(curVolume);\n      curVolume = (curVolume + 1) % volumes.size();\n      long availableVolumeSize = volume.getAvailable();\n      if (availableVolumeSize > blockSize) { return volume; }\n      \n      if (availableVolumeSize > maxAvailable) {\n        maxAvailable = availableVolumeSize;\n      }\n      \n      if (curVolume == startVolume) {\n        throw new DiskOutOfSpaceException(\"Out of space: \"\n            + \"The volume with the most available space (=\" + maxAvailable\n            + \" B) is less than the block size (=\" + blockSize + \" B).\");\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileUtil.listFiles": "  public static File[] listFiles(File dir) throws IOException {\n    File[] files = dir.listFiles();\n    if(files == null) {\n      throw new IOException(\"Invalid directory or I/O error occurred for dir: \"\n                + dir.toString());\n    }\n    return files;\n  }  ",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.compileReport": "    private LinkedList<ScanInfo> compileReport(FsVolumeSpi vol, File dir,\n        LinkedList<ScanInfo> report) {\n      File[] files;\n      try {\n        files = FileUtil.listFiles(dir);\n      } catch (IOException ioe) {\n        LOG.warn(\"Exception occured while compiling report: \", ioe);\n        // Ignore this directory and proceed.\n        return report;\n      }\n      Arrays.sort(files);\n      /*\n       * Assumption: In the sorted list of files block file appears immediately\n       * before block metadata file. This is true for the current naming\n       * convention for block file blk_<blockid> and meta file\n       * blk_<blockid>_<genstamp>.meta\n       */\n      for (int i = 0; i < files.length; i++) {\n        if (files[i].isDirectory()) {\n          compileReport(vol, files[i], report);\n          continue;\n        }\n        if (!Block.isBlockFilename(files[i])) {\n          if (isBlockMetaFile(\"blk_\", files[i].getName())) {\n            long blockId = Block.getBlockId(files[i].getName());\n            report.add(new ScanInfo(blockId, null, files[i], vol));\n          }\n          continue;\n        }\n        File blockFile = files[i];\n        long blockId = Block.filename2id(blockFile.getName());\n        File metaFile = null;\n\n        // Skip all the files that start with block name until\n        // getting to the metafile for the block\n        while (i + 1 < files.length && files[i + 1].isFile()\n            && files[i + 1].getName().startsWith(blockFile.getName())) {\n          i++;\n          if (isBlockMetaFile(blockFile.getName(), files[i].getName())) {\n            metaFile = files[i];\n            break;\n          }\n        }\n        report.add(new ScanInfo(blockId, blockFile, metaFile, vol));\n      }\n      return report;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.isBlockMetaFile": "  private static boolean isBlockMetaFile(String blockId, String metaFile) {\n    return metaFile.startsWith(blockId)\n        && metaFile.endsWith(Block.METADATA_EXTENSION);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.getBlockId": "    long getBlockId() {\n      return blockId;\n    }"
        },
        "bug_report": {
            "Title": "Initialize checkDisk when DirectoryScanner not able to get files list for scanning",
            "Description": "Env Details :\n=============\nCluster has 3 Datanode\nCluster installed with \"Rex\" user\ndfs.datanode.failed.volumes.tolerated  = 3\ndfs.blockreport.intervalMsec                  = 18000\ndfs.datanode.directoryscan.interval     = 120\nDN_XX1.XX1.XX1.XX1 data dir                         = /mnt/tmp_Datanode,/home/REX/data/dfs1/data,/home/REX/data/dfs2/data,/opt/REX/dfs/data\n \n \n/home/REX/data/dfs1/data,/home/REX/data/dfs2/data,/opt/REX/dfs/data - permission is denied ( hence DN considered the volume as failed )\n \nExpected behavior is observed when disk is not full:\n========================================\n \nStep 1: Change the permissions of /mnt/tmp_Datanode to root\n \nStep 2: Perform write operations ( DN detects that all Volume configured is failed and gets shutdown )\n \nScenario 1: \n===========\n \nStep 1 : Make /mnt/tmp_Datanode disk full and change the permissions to root\nStep 2 : Perform client write operations ( disk full exception is thrown , but Datanode is not getting shutdown ,  eventhough all the volume configured has failed)\n \n{noformat}\n \n2014-07-21 14:10:52,814 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: XX1.XX1.XX1.XX1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /XX2.XX2.XX2.XX2:10106 dst: /XX1.XX1.XX1.XX1:50010\n \norg.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=4096 B) is less than the block size (=134217728 B).\n \nat org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:60)\n \n{noformat}\n \nObservations :\n==============\n1. Write operations does not shutdown Datanode , eventhough all the volume configured is failed ( When one of the disk is full and for all the disk permission is denied)\n \n2. Directory scannning fails , still DN is not getting shutdown\n \n \n \n{noformat}\n \n2014-07-21 14:13:00,180 WARN org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Exception occured while compiling report: \n \njava.io.IOException: Invalid directory or I/O error occurred for dir: /mnt/tmp_Datanode/current/BP-1384489961-XX2.XX2.XX2.XX2-845784615183/current/finalized\n \nat org.apache.hadoop.fs.FileUtil.listFiles(FileUtil.java:1164)\n \nat org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.compileReport(DirectoryScanner.java:596)\n \n{noformat}"
        }
    },
    {
        "filename": "HDFS-3443.json",
        "creation_time": "2012-05-18T11:30:50.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:602)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1287)\n\tat org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:63)\n\tat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1219)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:978)\n\tat org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)\n\tat org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:3633)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:427)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:916)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1692)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1688)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1686)\n\njava.nio.channels.ClosedChannelException\n\tat sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:133)\n\tat sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)\n\tat org.apache.hadoop.ipc.Server.channelWrite(Server.java:2092)\n\tat org.apache.hadoop.ipc.Server.access$2000(Server.java:107)\n\tat org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:930)\n\tat org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:994)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1738)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices": "  void startActiveServices() throws IOException {\n    LOG.info(\"Starting services required for active state\");\n    writeLock();\n    try {\n      FSEditLog editLog = dir.fsImage.getEditLog();\n      \n      if (!editLog.isOpenForWrite()) {\n        // During startup, we're already open for write during initialization.\n        editLog.initJournalsForWrite();\n        // May need to recover\n        editLog.recoverUnclosedStreams();\n        \n        LOG.info(\"Catching up to latest edits from old active before \" +\n            \"taking over writer role in edits logs.\");\n        editLogTailer.catchupDuringFailover();\n        \n        LOG.info(\"Reprocessing replication and invalidation queues...\");\n        blockManager.getDatanodeManager().markAllDatanodesStale();\n        blockManager.clearQueues();\n        blockManager.processAllPendingDNMessages();\n        blockManager.processMisReplicatedBlocks();\n        \n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"NameNode metadata after re-processing \" +\n              \"replication and invalidation queues during failover:\\n\" +\n              metaSaveAsString());\n        }\n        \n        long nextTxId = dir.fsImage.getLastAppliedTxId() + 1;\n        LOG.info(\"Will take over writing edit logs at txnid \" + \n            nextTxId);\n        editLog.setNextTxId(nextTxId);\n\n        dir.fsImage.editLog.openForWrite();\n      }\n      if (haEnabled) {\n        // Renew all of the leases before becoming active.\n        // This is because, while we were in standby mode,\n        // the leases weren't getting renewed on this NN.\n        // Give them all a fresh start here.\n        leaseManager.renewAllLeases();\n      }\n      leaseManager.startMonitor();\n      startSecretManagerIfNecessary();\n    } finally {\n      writeUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.metaSaveAsString": "  private String metaSaveAsString() {\n    StringWriter sw = new StringWriter();\n    PrintWriter pw = new PrintWriter(sw);\n    metaSave(pw);\n    pw.flush();\n    return sw.toString();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock": "  public void writeUnlock() {\n    this.fsLock.writeLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startSecretManagerIfNecessary": "  private void startSecretManagerIfNecessary() {\n    boolean shouldRun = shouldUseDelegationTokens() &&\n      !isInSafeMode() && getEditLog().isOpenForWrite();\n    boolean running = dtSecretManager.isRunning();\n    if (shouldRun && !running) {\n      startSecretManager();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock": "  public void writeLock() {\n    this.fsLock.writeLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.startActiveServices": "    public void startActiveServices() throws IOException {\n      try {\n        namesystem.startActiveServices();\n        startTrashEmptier(conf);\n      } catch (Throwable t) {\n        doImmediateShutdown(t);\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.startTrashEmptier": "  private void startTrashEmptier(Configuration conf) throws IOException {\n    long trashInterval = conf.getLong(\n        CommonConfigurationKeys.FS_TRASH_INTERVAL_KEY,\n        CommonConfigurationKeys.FS_TRASH_INTERVAL_DEFAULT);\n    if (trashInterval == 0) {\n      return;\n    } else if (trashInterval < 0) {\n      throw new IOException(\"Cannot start tresh emptier with negative interval.\"\n          + \" Set \" + CommonConfigurationKeys.FS_TRASH_INTERVAL_KEY + \" to a\"\n          + \" positive value.\");\n    }\n    this.emptier = new Thread(new Trash(conf).getEmptier(), \"Trash Emptier\");\n    this.emptier.setDaemon(true);\n    this.emptier.start();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.doImmediateShutdown": "  private synchronized void doImmediateShutdown(Throwable t)\n      throws ServiceFailedException {\n    String message = \"Error encountered requiring NN shutdown. \" +\n        \"Shutting down immediately.\";\n    try {\n      LOG.fatal(message, t);\n    } catch (Throwable ignored) {\n      // This is unlikely to happen, but there's nothing we can do if it does.\n    }\n    runtime.exit(1);\n    // This code is only reached during testing, when runtime is stubbed out.\n    throw new ServiceFailedException(message, t);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState": "  public void enterState(HAContext context) throws ServiceFailedException {\n    try {\n      context.startActiveServices();\n    } catch (IOException e) {\n      throw new ServiceFailedException(\"Failed to start active services\", e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal": "  protected final void setStateInternal(final HAContext context, final HAState s)\n      throws ServiceFailedException {\n    prepareToExitState(context);\n    s.prepareToEnterState(context);\n    context.writeLock();\n    try {\n      exitState(context);\n      context.setState(s);\n      s.enterState(context);\n    } finally {\n      context.writeUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.exitState": "  public abstract void exitState(final HAContext context)\n      throws ServiceFailedException;\n\n  /**\n   * Move from the existing state to a new state\n   * @param context HA context\n   * @param s new state\n   * @throws ServiceFailedException on failure to transition to new state.\n   */\n  public void setState(HAContext context, HAState s) throws ServiceFailedException {\n    if (this == s) { // Aleady in the new state\n      return;\n    }\n    throw new ServiceFailedException(\"Transtion from state \" + this + \" to \"\n        + s + \" is not allowed.\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.setState": "  public void setState(HAContext context, HAState s) throws ServiceFailedException {\n    if (this == s) { // Aleady in the new state\n      return;\n    }\n    throw new ServiceFailedException(\"Transtion from state \" + this + \" to \"\n        + s + \" is not allowed.\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.prepareToExitState": "  public void prepareToExitState(final HAContext context)\n      throws ServiceFailedException {}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.enterState": "  public abstract void enterState(final HAContext context)\n      throws ServiceFailedException;\n\n  /**\n   * Method to be overridden by subclasses to prepare to exit a state.\n   * This method is called <em>without</em> the context being locked.\n   * This is used by the standby state to cancel any checkpoints\n   * that are going on. It can also be used to check any preconditions\n   * for the state transition.\n   * \n   * This method should not make any destructuve changes to the state\n   * (eg stopping threads) since {@link #prepareToEnterState(HAContext)}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.prepareToEnterState": "  public void prepareToEnterState(final HAContext context)\n      throws ServiceFailedException {}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState": "  public void setState(HAContext context, HAState s) throws ServiceFailedException {\n    if (s == NameNode.ACTIVE_STATE) {\n      setStateInternal(context, s);\n      return;\n    }\n    super.setState(context, s);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive": "  synchronized void transitionToActive() \n      throws ServiceFailedException, AccessControlException {\n    namesystem.checkSuperuserPrivilege();\n    if (!haEnabled) {\n      throw new ServiceFailedException(\"HA for namenode is not enabled\");\n    }\n    state.setState(haContext, ACTIVE_STATE);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.setState": "    public void setState(HAState s) {\n      state = s;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive": "  public synchronized void transitionToActive() \n      throws ServiceFailedException, AccessControlException {\n    nn.transitionToActive();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive": "  public TransitionToActiveResponseProto transitionToActive(\n      RpcController controller, TransitionToActiveRequestProto request)\n      throws ServiceException {\n    try {\n      server.transitionToActive();\n      return TRANSITION_TO_ACTIVE_RESP;\n    } catch(IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String protocol,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcRequestWritable request = (RpcRequestWritable) writableRequest;\n        HadoopRpcRequestProto rpcRequest = request.message;\n        String methodName = rpcRequest.getMethodName();\n        String protoName = rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: protocol=\" + protocol + \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, protoName,\n            clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" + protocol\n              + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcServerException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = prototype.newBuilderForType()\n            .mergeFrom(rpcRequest.getRequest()).build();\n        Message result;\n        try {\n          long startTime = System.currentTimeMillis();\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n          int processingTime = (int) (System.currentTimeMillis() - startTime);\n          int qTime = (int) (startTime - receiveTime);\n          if (LOG.isDebugEnabled()) {\n            LOG.info(\"Served: \" + methodName + \" queueTime= \" + qTime +\n                      \" procesingTime= \" + processingTime);\n          }\n          server.rpcMetrics.addRpcQueueTime(qTime);\n          server.rpcMetrics.addRpcProcessingTime(processingTime);\n          server.rpcDetailedMetrics.addProcessingTime(methodName,\n              processingTime);\n        } catch (ServiceException e) {\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          throw e;\n        }\n        return new RpcResponseWritable(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getProtocolImpl": "      private static ProtoClassProtoImpl getProtocolImpl(RPC.Server server,\n          String protoName, long version) throws IOException {\n        ProtoNameVer pv = new ProtoNameVer(protoName, version);\n        ProtoClassProtoImpl impl = \n            server.getProtocolImplMap(RPC.RpcKind.RPC_PROTOCOL_BUFFER).get(pv);\n        if (impl == null) { // no match for Protocol AND Version\n          VerProtocolImpl highest = \n              server.getHighestSupportedProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER, \n                  protoName);\n          if (highest == null) {\n            throw new IOException(\"Unknown protocol: \" + protoName);\n          }\n          // protocol supported but not the version that client wants\n          throw new RPC.VersionMismatch(protoName, version,\n              highest.version);\n        }\n        return impl;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.call": "    public Writable call(RPC.RpcKind rpcKind, String protocol,\n        Writable rpcRequest, long receiveTime) throws Exception {\n      return getRpcInvoker(rpcKind).call(this, protocol, rpcRequest,\n          receiveTime);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.getProtocolEngine": "  static synchronized RpcEngine getProtocolEngine(Class<?> protocol,\n      Configuration conf) {\n    RpcEngine engine = PROTOCOL_ENGINES.get(protocol);\n    if (engine == null) {\n      Class<?> impl = conf.getClass(ENGINE_PROP+\".\"+protocol.getName(),\n                                    WritableRpcEngine.class);\n      engine = (RpcEngine)ReflectionUtils.newInstance(impl, conf);\n      PROTOCOL_ENGINES.put(protocol, engine);\n    }\n    return engine;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "                     public Writable run() throws Exception {\n                       // make the call\n                       return call(call.rpcKind, call.connection.protocolName, \n                                   call.rpcRequest, call.timestamp);\n\n                     }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.toString": "    public String toString() {\n      return getHostAddress() + \":\" + remotePort; \n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(ByteArrayOutputStream responseBuf,\n                             Call call, RpcStatusProto status, \n                             Writable rv, String errorClass, String error) \n  throws IOException {\n    responseBuf.reset();\n    DataOutputStream out = new DataOutputStream(responseBuf);\n    RpcResponseHeaderProto.Builder response =  \n        RpcResponseHeaderProto.newBuilder();\n    response.setCallId(call.callId);\n    response.setStatus(status);\n\n\n    if (status == RpcStatusProto.SUCCESS) {\n      try {\n        response.build().writeDelimitedTo(out);\n        rv.write(out);\n      } catch (Throwable t) {\n        LOG.warn(\"Error serializing call response for call \" + call, t);\n        // Call back to same function - this is OK since the\n        // buffer is reset at the top, and since status is changed\n        // to ERROR it won't infinite loop.\n        setupResponse(responseBuf, call, RpcStatusProto.ERROR,\n            null, t.getClass().getName(),\n            StringUtils.stringifyException(t));\n        return;\n      }\n    } else {\n      if (status == RpcStatusProto.FATAL) {\n        response.setServerIpcVersionNum(Server.CURRENT_VERSION);\n      }\n      response.build().writeDelimitedTo(out);\n      WritableUtils.writeString(out, errorClass);\n      WritableUtils.writeString(out, error);\n    }\n    if (call.connection.useWrap) {\n      wrapWithSasl(responseBuf, call);\n    }\n    call.setResponse(ByteBuffer.wrap(responseBuf.toByteArray()));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    private synchronized void close() throws IOException {\n      disposeSasl();\n      data = null;\n      dataLengthBuffer = null;\n      if (!channel.isOpen())\n        return;\n      try {socket.shutdownOutput();} catch(Exception e) {\n        LOG.warn(\"Ignoring socket shutdown exception\");\n      }\n      if (channel.isOpen()) {\n        try {channel.close();} catch(Exception e) {}\n      }\n      try {socket.close();} catch(Exception e) {}\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(RPC.RpcKind rpcKind, String protocol,\n      Writable param, long receiveTime) throws Exception;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param protocolName - the protocol\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  private void authorize(UserGroupInformation user, String protocolName,\n      InetAddress addr) throws AuthorizationException {\n    if (authorize) {\n      if (protocolName == null) {\n        throw new AuthorizationException(\"Null protocol not authorized\");\n      }\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(protocolName, getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         protocolName);\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isValid() && key.isWritable()) {\n                  doAsyncWrite(key);\n              }\n            } catch (IOException e) {\n              LOG.info(getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = System.currentTimeMillis();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<Call> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<Call>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              Call call = (Call)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n          \n          for(Call call : calls) {\n            try {\n              doPurge(call, now);\n            } catch (IOException e) {\n              LOG.warn(\"Error in purging old calls \" + e);\n            }\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeConnection": "  private void closeConnection(Connection connection) {\n    synchronized (connectionList) {\n      if (connectionList.remove(connection))\n        numConnections--;\n    }\n    try {\n      connection.close();\n    } catch (IOException e) {\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          if (LOG.isDebugEnabled())\n            LOG.debug(getName() + \": disconnecting client \" + c.getHostAddress());\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRespond": "    void doRespond(Call call) throws IOException {\n      synchronized (call.connection.responseQueue) {\n        call.connection.responseQueue.addLast(call);\n        if (call.connection.responseQueue.size() == 1) {\n          processResponse(call.connection.responseQueue, true);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws IOException,  OutOfMemoryError {\n      Connection c = null;\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        \n        Reader reader = getReader();\n        try {\n          reader.startAdd();\n          SelectionKey readKey = reader.registerChannel(channel);\n          c = new Connection(readKey, channel, System.currentTimeMillis());\n          readKey.attach(c);\n          synchronized (connectionList) {\n            connectionList.add(numConnections, c);\n            numConnections++;\n          }\n          if (LOG.isDebugEnabled())\n            LOG.debug(\"Server connection from \" + c.toString() +\n                \"; # active connections: \" + numConnections +\n                \"; # queued calls: \" + callQueue.size());          \n        } finally {\n          reader.finishAdd(); \n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.cleanupConnections": "    private void cleanupConnections(boolean force) {\n      if (force || numConnections > thresholdIdleConnections) {\n        long currentTime = System.currentTimeMillis();\n        if (!force && (currentTime - lastCleanupRunTime) < cleanupInterval) {\n          return;\n        }\n        int start = 0;\n        int end = numConnections - 1;\n        if (!force) {\n          start = rand.nextInt() % numConnections;\n          end = rand.nextInt() % numConnections;\n          int temp;\n          if (end < start) {\n            temp = start;\n            start = end;\n            end = temp;\n          }\n        }\n        int i = start;\n        int numNuked = 0;\n        while (i <= end) {\n          Connection c;\n          synchronized (connectionList) {\n            try {\n              c = connectionList.get(i);\n            } catch (Exception e) {return;}\n          }\n          if (c.timedOut(currentTime)) {\n            if (LOG.isDebugEnabled())\n              LOG.debug(getName() + \": disconnecting client \" + c.getHostAddress());\n            closeConnection(c);\n            numNuked++;\n            end--;\n            c = null;\n            if (!force && numNuked == maxConnectionsToNuke) break;\n          }\n          else i++;\n        }\n        lastCleanupRunTime = System.currentTimeMillis();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      LOG.error(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(pae,\"Unknown exception in doAs\");\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.channelWrite": "  private int channelWrite(WritableByteChannel channel, \n                           ByteBuffer buffer) throws IOException {\n    \n    int count =  (buffer.remaining() <= NIO_BUFFER_LIMIT) ?\n                 channel.write(buffer) : channelIO(null, channel, buffer);\n    if (count > 0) {\n      rpcMetrics.incrSentBytes(count);\n    }\n    return count;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.channelIO": "  private static int channelIO(ReadableByteChannel readCh, \n                               WritableByteChannel writeCh,\n                               ByteBuffer buf) throws IOException {\n    \n    int originalLimit = buf.limit();\n    int initialRemaining = buf.remaining();\n    int ret = 0;\n    \n    while (buf.remaining() > 0) {\n      try {\n        int ioSize = Math.min(buf.remaining(), NIO_BUFFER_LIMIT);\n        buf.limit(buf.position() + ioSize);\n        \n        ret = (readCh == null) ? writeCh.write(buf) : readCh.read(buf); \n        \n        if (ret < ioSize) {\n          break;\n        }\n\n      } finally {\n        buf.limit(originalLimit);        \n      }\n    }\n\n    int nBytes = initialRemaining - buf.remaining(); \n    return (nBytes > 0) ? nBytes : ret;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.write": "    void write(DataOutput out) throws IOException {\n      out.writeByte(this.ordinal());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.processResponse": "    private boolean processResponse(LinkedList<Call> responseQueue,\n                                    boolean inHandler) throws IOException {\n      boolean error = true;\n      boolean done = false;       // there is more data for this channel.\n      int numElements = 0;\n      Call call = null;\n      try {\n        synchronized (responseQueue) {\n          //\n          // If there are no items for this channel, then we are done\n          //\n          numElements = responseQueue.size();\n          if (numElements == 0) {\n            error = false;\n            return true;              // no more data for this channel.\n          }\n          //\n          // Extract the first call\n          //\n          call = responseQueue.removeFirst();\n          SocketChannel channel = call.connection.channel;\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(getName() + \": responding to #\" + call.callId + \" from \" +\n                      call.connection);\n          }\n          //\n          // Send as much data as we can in the non-blocking fashion\n          //\n          int numBytes = channelWrite(channel, call.rpcResponse);\n          if (numBytes < 0) {\n            return true;\n          }\n          if (!call.rpcResponse.hasRemaining()) {\n            call.connection.decRpcCount();\n            if (numElements == 1) {    // last call fully processes.\n              done = true;             // no more data for this channel.\n            } else {\n              done = false;            // more calls pending to be sent.\n            }\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(getName() + \": responding to #\" + call.callId + \" from \" +\n                        call.connection + \" Wrote \" + numBytes + \" bytes.\");\n            }\n          } else {\n            //\n            // If we were unable to write the entire response out, then \n            // insert in Selector queue. \n            //\n            call.connection.responseQueue.addFirst(call);\n            \n            if (inHandler) {\n              // set the serve time when the response has to be sent later\n              call.timestamp = System.currentTimeMillis();\n              \n              incPending();\n              try {\n                // Wakeup the thread blocked on select, only then can the call \n                // to channel.register() complete.\n                writeSelector.wakeup();\n                channel.register(writeSelector, SelectionKey.OP_WRITE, call);\n              } catch (ClosedChannelException e) {\n                //Its ok. channel might be closed else where.\n                done = true;\n              } finally {\n                decPending();\n              }\n            }\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(getName() + \": responding to #\" + call.callId + \" from \" +\n                        call.connection + \" Wrote partial \" + numBytes + \n                        \" bytes.\");\n            }\n          }\n          error = false;              // everything went off well\n        }\n      } finally {\n        if (error && call != null) {\n          LOG.warn(getName()+\", call \" + call + \": output error\");\n          done = true;               // error. no more data for this channel.\n          closeConnection(call.connection);\n        }\n      }\n      return done;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.decPending": "    private synchronized void decPending() { // call done enqueueing.\n      pending--;\n      notify();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.incPending": "    private synchronized void incPending() {   // call waiting to be enqueued.\n      pending++;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.recoverUnclosedStreams": "  synchronized void recoverUnclosedStreams() {\n    Preconditions.checkState(\n        state == State.BETWEEN_LOG_SEGMENTS,\n        \"May not recover segments - wrong state: %s\", state);\n    try {\n      journalSet.recoverUnfinalizedSegments();\n    } catch (IOException ex) {\n      // All journals have failed, it is handled in logSync.\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.leaseManager.startMonitor": "  void startMonitor() {\n    Preconditions.checkState(lmthread == null,\n        \"Lease Monitor already running\");\n    shouldRunMonitor = true;\n    lmthread = new Daemon(new Monitor());\n    lmthread.start();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.initJournalsForWrite": "  public synchronized void initJournalsForWrite() {\n    Preconditions.checkState(state == State.UNINITIALIZED ||\n        state == State.CLOSED, \"Unexpected state: %s\", state);\n    \n    initJournals(this.editsDirs);\n    state = State.BETWEEN_LOG_SEGMENTS;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.initJournals": "  private synchronized void initJournals(List<URI> dirs) {\n    int minimumRedundantJournals = conf.getInt(\n        DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_MINIMUM_KEY,\n        DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_MINIMUM_DEFAULT);\n\n    journalSet = new JournalSet(minimumRedundantJournals);\n    for (URI u : dirs) {\n      boolean required = FSNamesystem.getRequiredNamespaceEditsDirs(conf)\n          .contains(u);\n      if (u.getScheme().equals(NNStorage.LOCAL_URI_SCHEME)) {\n        StorageDirectory sd = storage.getStorageDirectory(u);\n        if (sd != null) {\n          journalSet.add(new FileJournalManager(sd, storage), required);\n        }\n      } else {\n        journalSet.add(createJournal(u), required);\n      }\n    }\n \n    if (journalSet.isEmpty()) {\n      LOG.error(\"No edits directories configured!\");\n    } \n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.setNextTxId": "  synchronized void setNextTxId(long nextTxId) {\n    Preconditions.checkArgument(synctxid <= txid &&\n       nextTxId >= txid,\n       \"May not decrease txid.\" +\n      \" synctxid=%s txid=%s nextTxId=%s\",\n      synctxid, txid, nextTxId);\n      \n    txid = nextTxId - 1;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.isOpenForWrite": "  synchronized boolean isOpenForWrite() {\n    return state == State.IN_SEGMENT ||\n      state == State.BETWEEN_LOG_SEGMENTS;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.leaseManager.renewAllLeases": "  synchronized void renewAllLeases() {\n    for (Lease l : leases.values()) {\n      renewLease(l);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.leaseManager.renewLease": "  synchronized void renewLease(Lease lease) {\n    if (lease != null) {\n      sortedLeases.remove(lease);\n      lease.renew();\n      sortedLeases.add(lease);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.namesystem.checkSuperuserPrivilege": "  public void checkSuperuserPrivilege() throws AccessControlException;\n\n  /** @return the block pool ID */\n  public String getBlockPoolId();\n\n  public boolean isInStandbyState();\n\n  public boolean isGenStampInFuture(long generationStamp);\n\n  public void adjustSafeModeBlockTotals(int deltaSafe, int deltaTotal);\n}"
        },
        "bug_report": {
            "Title": "Fix NPE when namenode transition to active during startup by adding checkNNStartup() in NameNodeRpcServer",
            "Description": "Start NN\nLet NN standby services be started.\nBefore the editLogTailer is initialised start ZKFC and allow the activeservices start to proceed further.\n\n\nHere editLogTailer.catchupDuringFailover() will throw NPE.\n\nvoid startActiveServices() throws IOException {\n    LOG.info(\"Starting services required for active state\");\n    writeLock();\n    try {\n      FSEditLog editLog = dir.fsImage.getEditLog();\n      \n      if (!editLog.isOpenForWrite()) {\n        // During startup, we're already open for write during initialization.\n        editLog.initJournalsForWrite();\n        // May need to recover\n        editLog.recoverUnclosedStreams();\n        \n        LOG.info(\"Catching up to latest edits from old active before \" +\n            \"taking over writer role in edits logs.\");\n        editLogTailer.catchupDuringFailover();\n\n\n{noformat}\n2012-05-18 16:51:27,585 WARN org.apache.hadoop.ipc.Server: IPC Server Responder, call org.apache.hadoop.ha.HAServiceProtocol.getServiceStatus from XX.XX.XX.55:58003: output error\n2012-05-18 16:51:27,586 WARN org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call org.apache.hadoop.ha.HAServiceProtocol.transitionToActive from XX.XX.XX.55:58004: error: java.lang.NullPointerException\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:602)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1287)\n\tat org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:63)\n\tat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1219)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:978)\n\tat org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)\n\tat org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:3633)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:427)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:916)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1692)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1688)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1686)\n2012-05-18 16:51:27,586 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020 caught an exception\njava.nio.channels.ClosedChannelException\n\tat sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:133)\n\tat sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)\n\tat org.apache.hadoop.ipc.Server.channelWrite(Server.java:2092)\n\tat org.apache.hadoop.ipc.Server.access$2000(Server.java:107)\n\tat org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:930)\n\tat org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:994)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1738)\n{noformat}"
        }
    },
    {
        "filename": "HDFS-11479.json",
        "creation_time": "2017-03-01T11:08:07.000+0000",
        "stack_trace": "```\norg.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:4242\n        at org.jboss.netty.bootstrap.ConnectionlessBootstrap.bind(ConnectionlessBootstrap.java:204)\n        at org.apache.hadoop.oncrpc.SimpleUdpServer.run(SimpleUdpServer.java:68)\n        at org.apache.hadoop.mount.MountdBase.startUDPServer(MountdBase.java:64)\n        at org.apache.hadoop.mount.MountdBase.start(MountdBase.java:97)\n        at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)\n        at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)\n        at org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:71)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)\nCaused by: java.net.BindException: Address already in use\n        at sun.nio.ch.Net.bind0(Native Method)\n        at sun.nio.ch.Net.bind(Net.java:433)\n        at sun.nio.ch.DatagramChannelImpl.bind(DatagramChannelImpl.java:691)\n        at sun.nio.ch.DatagramSocketAdaptor.bind(DatagramSocketAdaptor.java:91)\n        at org.jboss.netty.channel.socket.nio.NioDatagramPipelineSink.bind(NioDatagramPipelineSink.java:129)\n        at org.jboss.netty.channel.socket.nio.NioDatagramPipelineSink.eventSunk(NioDatagramPipelineSink.java:77)\n        at org.jboss.netty.channel.Channels.bind(Channels.java:561)\n        at org.jboss.netty.channel.AbstractChannel.bind(AbstractChannel.java:189)\n        at org.jboss.netty.bootstrap.ConnectionlessBootstrap.bind(ConnectionlessBootstrap.java:198)```",
        "source_code": {
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.SimpleUdpServer.run": "  public void run() {\n    // Configure the client.\n    DatagramChannelFactory f = new NioDatagramChannelFactory(\n        Executors.newCachedThreadPool(), workerCount);\n\n    server = new ConnectionlessBootstrap(f);\n    server.setPipeline(Channels.pipeline(RpcUtil.STAGE_RPC_MESSAGE_PARSER,\n        rpcProgram, RpcUtil.STAGE_RPC_UDP_RESPONSE));\n\n    server.setOption(\"broadcast\", \"false\");\n    server.setOption(\"sendBufferSize\", SEND_BUFFER_SIZE);\n    server.setOption(\"receiveBufferSize\", RECEIVE_BUFFER_SIZE);\n\n    // Listen to the UDP port\n    ch = server.bind(new InetSocketAddress(port));\n    InetSocketAddress socketAddr = (InetSocketAddress) ch.getLocalAddress();\n    boundPort = socketAddr.getPort();\n\n    LOG.info(\"Started listening to UDP requests at port \" + boundPort + \" for \"\n        + rpcProgram + \" with workerCount \" + workerCount);\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.mount.MountdBase.startUDPServer": "  private void startUDPServer() {\n    SimpleUdpServer udpServer = new SimpleUdpServer(rpcProgram.getPort(),\n        rpcProgram, 1);\n    rpcProgram.startDaemons();\n    try {\n      udpServer.run();\n    } catch (Throwable e) {\n      LOG.fatal(\"Failed to start the UDP server.\", e);\n      if (udpServer.getBoundPort() > 0) {\n        rpcProgram.unregister(PortmapMapping.TRANSPORT_UDP,\n            udpServer.getBoundPort());\n      }\n      udpServer.shutdown();\n      terminate(1, e);\n    }\n    udpBoundPort = udpServer.getBoundPort();\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.mount.MountdBase.run": "    public synchronized void run() {\n      rpcProgram.unregister(PortmapMapping.TRANSPORT_UDP, udpBoundPort);\n      rpcProgram.unregister(PortmapMapping.TRANSPORT_TCP, tcpBoundPort);\n    }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.mount.MountdBase.start": "  public void start(boolean register) {\n    startUDPServer();\n    startTCPServer();\n    if (register) {\n      ShutdownHookManager.get().addShutdownHook(new Unregister(),\n          SHUTDOWN_HOOK_PRIORITY);\n      try {\n        rpcProgram.register(PortmapMapping.TRANSPORT_UDP, udpBoundPort);\n        rpcProgram.register(PortmapMapping.TRANSPORT_TCP, tcpBoundPort);\n      } catch (Throwable e) {\n        LOG.fatal(\"Failed to register the MOUNT service.\", e);\n        terminate(1, e);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.mount.MountdBase.startTCPServer": "  private void startTCPServer() {\n    SimpleTcpServer tcpServer = new SimpleTcpServer(rpcProgram.getPort(),\n        rpcProgram, 1);\n    rpcProgram.startDaemons();\n    try {\n      tcpServer.run();\n    } catch (Throwable e) {\n      LOG.fatal(\"Failed to start the TCP server.\", e);\n      if (tcpServer.getBoundPort() > 0) {\n        rpcProgram.unregister(PortmapMapping.TRANSPORT_TCP,\n            tcpServer.getBoundPort());\n      }\n      tcpServer.shutdown();\n      terminate(1, e);\n    }\n    tcpBoundPort = tcpServer.getBoundPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal": "  public void startServiceInternal(boolean register) throws IOException {\n    mountd.start(register); // Start mountd\n    start(register);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService": "  static void startService(String[] args,\n      DatagramSocket registrationSocket) throws IOException {\n    StringUtils.startupShutdownMessage(Nfs3.class, args, LOG);\n    NfsConfiguration conf = new NfsConfiguration();\n    boolean allowInsecurePorts = conf.getBoolean(\n        NfsConfigKeys.DFS_NFS_PORT_MONITORING_DISABLED_KEY,\n        NfsConfigKeys.DFS_NFS_PORT_MONITORING_DISABLED_DEFAULT);\n    final Nfs3 nfsServer = new Nfs3(conf, registrationSocket,\n        allowInsecurePorts);\n    nfsServer.startServiceInternal(true);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start": "  public void start() throws Exception {\n    Nfs3.startService(args, registrationSocket);\n  }"
        },
        "bug_report": {
            "Title": "Socket re-use address option should be used in SimpleUdpServer",
            "Description": "Nfs gateway restart can fail because of bind error in SimpleUdpServer.\n\nre-use address option should be used in SimpleUdpServer to so that socket bind can happen when it is in TIME_WAIT state\n\n{noformat}\n2017-02-28 04:19:53,495 FATAL mount.MountdBase (MountdBase.java:startUDPServer(66)) - Failed to start the UDP server.\norg.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:4242\n        at org.jboss.netty.bootstrap.ConnectionlessBootstrap.bind(ConnectionlessBootstrap.java:204)\n        at org.apache.hadoop.oncrpc.SimpleUdpServer.run(SimpleUdpServer.java:68)\n        at org.apache.hadoop.mount.MountdBase.startUDPServer(MountdBase.java:64)\n        at org.apache.hadoop.mount.MountdBase.start(MountdBase.java:97)\n        at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)\n        at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)\n        at org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:71)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)\nCaused by: java.net.BindException: Address already in use\n        at sun.nio.ch.Net.bind0(Native Method)\n        at sun.nio.ch.Net.bind(Net.java:433)\n        at sun.nio.ch.DatagramChannelImpl.bind(DatagramChannelImpl.java:691)\n        at sun.nio.ch.DatagramSocketAdaptor.bind(DatagramSocketAdaptor.java:91)\n        at org.jboss.netty.channel.socket.nio.NioDatagramPipelineSink.bind(NioDatagramPipelineSink.java:129)\n        at org.jboss.netty.channel.socket.nio.NioDatagramPipelineSink.eventSunk(NioDatagramPipelineSink.java:77)\n        at org.jboss.netty.channel.Channels.bind(Channels.java:561)\n        at org.jboss.netty.channel.AbstractChannel.bind(AbstractChannel.java:189)\n        at org.jboss.netty.bootstrap.ConnectionlessBootstrap.bind(ConnectionlessBootstrap.java:198)\n        ... 11 more\n{noformat}\n"
        }
    },
    {
        "filename": "HDFS-8055.json",
        "creation_time": "2015-04-02T23:21:55.000+0000",
        "stack_trace": "```\norg.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException\n         at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedBlocks(DatanodeManager.java:359)\n         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1789)\n         at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)\n         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)\n         at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n         at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n         at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)\n         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)\n         at java.security.AccessController.doPrivileged(Native Method)\n         at javax.security.auth.Subject.doAs(Subject.java:415)\n         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)\n         at org.apache.hadoop.ipc.Client.call(Client.java:1468)\n         at org.apache.hadoop.ipc.Client.call(Client.java:1399)\n         at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n         at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)\n         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:254)\n         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n         at java.lang.reflect.Method.invoke(Method.java:606)\n         at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n         at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n         at com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)\n         at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1220)\n         at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1210)\n         at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1200)\n         at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:271)\n         at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:238)\n         at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:231)\n         at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1498)\n         at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:302)\n         at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:298)\n         at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n         at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:298)\n         at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n         at org.apache.hadoop.hbase.util.HFileV1Detector$1.call(HFileV1Detector.java:320)\n         at org.apache.hadoop.hbase.util.HFileV1Detector$1.call(HFileV1Detector.java:300)\n         at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n         at java.lang.Thread.run(Thread.java:745)\n\norg.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException\n         at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedBlocks(DatanodeManager.java:359)\n         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1789)\n         at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)\n         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedBlocks": "  public void sortLocatedBlocks(final String targethost,\n      final List<LocatedBlock> locatedblocks) {\n    //sort the blocks\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client = getDatanodeByHost(targethost);\n    if (client == null) {\n      List<String> hosts = new ArrayList<String> (1);\n      hosts.add(targethost);\n      String rName = dnsToSwitchMapping.resolve(hosts).get(0);\n      if (rName != null)\n        client = new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n    }\n    \n    Comparator<DatanodeInfo> comparator = avoidStaleDataNodesForRead ?\n        new DFSUtil.DecomStaleComparator(staleInterval) : \n        DFSUtil.DECOM_COMPARATOR;\n        \n    for (LocatedBlock b : locatedblocks) {\n      DatanodeInfo[] di = b.getLocations();\n      // Move decommissioned/stale datanodes to the bottom\n      Arrays.sort(di, comparator);\n      \n      int lastActiveIndex = di.length - 1;\n      while (lastActiveIndex > 0 && isInactive(di[lastActiveIndex])) {\n          --lastActiveIndex;\n      }\n      int activeLen = lastActiveIndex + 1;      \n      networktopology.sortByDistance(client, b.getLocations(), activeLen);\n      // must update cache since we modified locations array\n      b.updateCachedStorageInfo();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.isInactive": "  private boolean isInactive(DatanodeInfo datanode) {\n    if (datanode.isDecommissioned()) {\n      return true;\n    }\n\n    if (avoidStaleDataNodesForRead) {\n      return datanode.isStale(staleInterval);\n    }\n      \n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getDatanodeByHost": "  public DatanodeDescriptor getDatanodeByHost(final String host) {\n    return host2DatanodeMap.getDatanodeByHost(host);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations": "  GetBlockLocationsResult getBlockLocations(\n      String src, long offset, long length, boolean needBlockToken,\n      boolean checkSafeMode) throws IOException {\n    if (offset < 0) {\n      throw new HadoopIllegalArgumentException(\n          \"Negative offset is not supported. File: \" + src);\n    }\n    if (length < 0) {\n      throw new HadoopIllegalArgumentException(\n          \"Negative length is not supported. File: \" + src);\n    }\n    final GetBlockLocationsResult ret = getBlockLocationsInt(\n        src, offset, length, needBlockToken);\n\n    if (checkSafeMode && isInSafeMode()) {\n      for (LocatedBlock b : ret.blocks.getLocatedBlocks()) {\n        // if safemode & no block locations yet then throw safemodeException\n        if ((b.getLocations() == null) || (b.getLocations().length == 0)) {\n          SafeModeException se = new SafeModeException(\n              \"Zero blocklocations for \" + src, safeMode);\n          if (haEnabled && haContext != null &&\n              haContext.getState().getServiceState() == HAServiceState.ACTIVE) {\n            throw new RetriableException(se);\n          } else {\n            throw se;\n          }\n        }\n      }\n    }\n    return ret;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock": "  public void writeUnlock() {\n    this.fsLock.writeLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt": "  private GetBlockLocationsResult getBlockLocationsInt(\n      final String srcArg, long offset, long length, boolean needBlockToken)\n      throws IOException {\n    String src = srcArg;\n    FSPermissionChecker pc = getPermissionChecker();\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    src = dir.resolvePath(pc, src, pathComponents);\n    final INodesInPath iip = dir.getINodesInPath(src, true);\n    final INodeFile inode = INodeFile.valueOf(iip.getLastINode(), src);\n    if (isPermissionEnabled) {\n      dir.checkPathAccess(pc, iip, FsAction.READ);\n      checkUnreadableBySuperuser(pc, inode, iip.getPathSnapshotId());\n    }\n\n    final long fileSize = iip.isSnapshot()\n        ? inode.computeFileSize(iip.getPathSnapshotId())\n        : inode.computeFileSizeNotIncludingLastUcBlock();\n    boolean isUc = inode.isUnderConstruction();\n    if (iip.isSnapshot()) {\n      // if src indicates a snapshot file, we need to make sure the returned\n      // blocks do not exceed the size of the snapshot file.\n      length = Math.min(length, fileSize - offset);\n      isUc = false;\n    }\n\n    final FileEncryptionInfo feInfo =\n        FSDirectory.isReservedRawName(srcArg) ? null\n            : dir.getFileEncryptionInfo(inode, iip.getPathSnapshotId(), iip);\n\n    final LocatedBlocks blocks = blockManager.createLocatedBlocks(\n        inode.getBlocks(iip.getPathSnapshotId()), fileSize,\n        isUc, offset, length, needBlockToken, iip.isSnapshot(), feInfo);\n\n    // Set caching information for the located blocks.\n    for (LocatedBlock lb : blocks.getLocatedBlocks()) {\n      cacheManager.setCachedLocations(lb);\n    }\n\n    final long now = now();\n    boolean updateAccessTime = isAccessTimeSupported() && !isInSafeMode()\n        && !iip.isSnapshot()\n        && now > inode.getAccessTime() + getAccessTimePrecision();\n    return new GetBlockLocationsResult(updateAccessTime ? iip : null, blocks);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logAuditEvent": "    public void logAuditEvent(boolean succeeded, String userName,\n        InetAddress addr, String cmd, String src, String dst,\n        FileStatus status, UserGroupInformation ugi,\n        DelegationTokenSecretManager dtSecretManager) {\n      if (auditLog.isInfoEnabled()) {\n        final StringBuilder sb = auditBuffer.get();\n        sb.setLength(0);\n        sb.append(\"allowed=\").append(succeeded).append(\"\\t\");\n        sb.append(\"ugi=\").append(userName).append(\"\\t\");\n        sb.append(\"ip=\").append(addr).append(\"\\t\");\n        sb.append(\"cmd=\").append(cmd).append(\"\\t\");\n        sb.append(\"src=\").append(src).append(\"\\t\");\n        sb.append(\"dst=\").append(dst).append(\"\\t\");\n        if (null == status) {\n          sb.append(\"perm=null\");\n        } else {\n          sb.append(\"perm=\");\n          sb.append(status.getOwner()).append(\":\");\n          sb.append(status.getGroup()).append(\":\");\n          sb.append(status.getPermission());\n        }\n        if (logTokenTrackingId) {\n          sb.append(\"\\t\").append(\"trackingId=\");\n          String trackingId = null;\n          if (ugi != null && dtSecretManager != null\n              && ugi.getAuthenticationMethod() == AuthenticationMethod.TOKEN) {\n            for (TokenIdentifier tid: ugi.getTokenIdentifiers()) {\n              if (tid instanceof DelegationTokenIdentifier) {\n                DelegationTokenIdentifier dtid =\n                    (DelegationTokenIdentifier)tid;\n                trackingId = dtSecretManager.getTokenTrackingId(dtid);\n                break;\n              }\n            }\n          }\n          sb.append(trackingId);\n        }\n        sb.append(\"\\t\").append(\"proto=\");\n        sb.append(NamenodeWebHdfsMethods.isWebHdfsInvocation() ? \"webhdfs\" : \"rpc\");\n        logAuditMessage(sb.toString());\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEditLog": "  public FSEditLog getEditLog() {\n    return getFSImage().getEditLog();\n  }    ",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.isInSafeMode": "  public boolean isInSafeMode() {\n    // safeMode is volatile, and may be set to null at any time\n    SafeModeInfo safeMode = this.safeMode;\n    if (safeMode == null)\n      return false;\n    return safeMode.isOn();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updateAccessTime": "    boolean updateAccessTime() {\n      return iip != null;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAccessTimePrecision": "  long getAccessTimePrecision() {\n    return accessTimePrecision;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock": "  public void writeLock() {\n    this.fsLock.writeLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.readUnlock": "  public void readUnlock() {\n    this.fsLock.readLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation": "  public void checkOperation(OperationCategory op) throws StandbyException {\n    if (haContext != null) {\n      // null in some unit tests\n      haContext.checkOperation(op);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setTimes": "  void setTimes(String src, long mtime, long atime) throws IOException {\n    HdfsFileStatus auditStat;\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot set times \" + src);\n      auditStat = FSDirAttrOp.setTimes(dir, src, mtime, atime);\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"setTimes\", src);\n      throw e;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    logAuditEvent(true, \"setTimes\", src, null, auditStat);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.readLock": "  public void readLock() {\n    this.fsLock.readLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations": "  public LocatedBlocks getBlockLocations(String src, \n                                          long offset, \n                                          long length) \n      throws IOException {\n    checkNNStartup();\n    metrics.incrGetBlockLocations();\n    return namesystem.getBlockLocations(getClientMachine(), \n                                        src, offset, length);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkNNStartup": "  private void checkNNStartup() throws IOException {\n    if (!this.nn.isStarted()) {\n      throw new IOException(this.nn.getRole() + \" still not started\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getClientMachine": "  private static String getClientMachine() {\n    String clientMachine = NamenodeWebHdfsMethods.getRemoteAddress();\n    if (clientMachine == null) { //not a web client\n      clientMachine = Server.getRemoteAddress();\n    }\n    if (clientMachine == null) { //not a RPC client\n      clientMachine = \"\";\n    }\n    return clientMachine;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations": "  public GetBlockLocationsResponseProto getBlockLocations(\n      RpcController controller, GetBlockLocationsRequestProto req)\n      throws ServiceException {\n    try {\n      LocatedBlocks b = server.getBlockLocations(req.getSrc(), req.getOffset(),\n          req.getLength());\n      Builder builder = GetBlockLocationsResponseProto\n          .newBuilder();\n      if (b != null) {\n        builder.setLocations(PBHelper.convert(b)).build();\n      }\n      return builder.build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcRequestWrapper request = (RpcRequestWrapper) writableRequest;\n        RequestHeaderProto rpcRequest = request.requestHeader;\n        String methodName = rpcRequest.getMethodName();\n        \n        \n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = prototype.newBuilderForType()\n            .mergeFrom(request.theRequestRead).build();\n        \n        Message result;\n        long startTime = Time.now();\n        int qTime = (int) (startTime - receiveTime);\n        Exception exception = null;\n        try {\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n        } catch (ServiceException e) {\n          exception = (Exception) e.getCause();\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          exception = e;\n          throw e;\n        } finally {\n          int processingTime = (int) (Time.now() - startTime);\n          if (LOG.isDebugEnabled()) {\n            String msg = \"Served: \" + methodName + \" queueTime= \" + qTime +\n                \" procesingTime= \" + processingTime;\n            if (exception != null) {\n              msg += \" exception= \" + exception.getClass().getSimpleName();\n            }\n            LOG.debug(msg);\n          }\n          String detailedMetricsName = (exception == null) ?\n              methodName :\n              exception.getClass().getSimpleName();\n          server.rpcMetrics.addRpcQueueTime(qTime);\n          server.rpcMetrics.addRpcProcessingTime(processingTime);\n          server.rpcDetailedMetrics.addProcessingTime(detailedMetricsName,\n              processingTime);\n        }\n        return new RpcResponseWrapper(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getProtocolImpl": "      private static ProtoClassProtoImpl getProtocolImpl(RPC.Server server,\n          String protoName, long clientVersion) throws RpcServerException {\n        ProtoNameVer pv = new ProtoNameVer(protoName, clientVersion);\n        ProtoClassProtoImpl impl = \n            server.getProtocolImplMap(RPC.RpcKind.RPC_PROTOCOL_BUFFER).get(pv);\n        if (impl == null) { // no match for Protocol AND Version\n          VerProtocolImpl highest = \n              server.getHighestSupportedProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER, \n                  protoName);\n          if (highest == null) {\n            throw new RpcNoSuchProtocolException(\n                \"Unknown protocol: \" + protoName);\n          }\n          // protocol supported but not the version that client wants\n          throw new RPC.VersionMismatch(protoName, clientVersion,\n              highest.version);\n        }\n        return impl;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.call": "    public Writable call(RPC.RpcKind rpcKind, String protocol,\n        Writable rpcRequest, long receiveTime) throws Exception {\n      return getRpcInvoker(rpcKind).call(this, protocol, rpcRequest,\n          receiveTime);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.toString": "    public String toString() {\n      return getHostAddress() + \":\" + remotePort; \n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(ByteArrayOutputStream responseBuf,\n                             Call call, RpcStatusProto status, RpcErrorCodeProto erCode,\n                             Writable rv, String errorClass, String error) \n  throws IOException {\n    responseBuf.reset();\n    DataOutputStream out = new DataOutputStream(responseBuf);\n    RpcResponseHeaderProto.Builder headerBuilder =  \n        RpcResponseHeaderProto.newBuilder();\n    headerBuilder.setClientId(ByteString.copyFrom(call.clientId));\n    headerBuilder.setCallId(call.callId);\n    headerBuilder.setRetryCount(call.retryCount);\n    headerBuilder.setStatus(status);\n    headerBuilder.setServerIpcVersionNum(CURRENT_VERSION);\n\n    if (status == RpcStatusProto.SUCCESS) {\n      RpcResponseHeaderProto header = headerBuilder.build();\n      final int headerLen = header.getSerializedSize();\n      int fullLength  = CodedOutputStream.computeRawVarint32Size(headerLen) +\n          headerLen;\n      try {\n        if (rv instanceof ProtobufRpcEngine.RpcWrapper) {\n          ProtobufRpcEngine.RpcWrapper resWrapper = \n              (ProtobufRpcEngine.RpcWrapper) rv;\n          fullLength += resWrapper.getLength();\n          out.writeInt(fullLength);\n          header.writeDelimitedTo(out);\n          rv.write(out);\n        } else { // Have to serialize to buffer to get len\n          final DataOutputBuffer buf = new DataOutputBuffer();\n          rv.write(buf);\n          byte[] data = buf.getData();\n          fullLength += buf.getLength();\n          out.writeInt(fullLength);\n          header.writeDelimitedTo(out);\n          out.write(data, 0, buf.getLength());\n        }\n      } catch (Throwable t) {\n        LOG.warn(\"Error serializing call response for call \" + call, t);\n        // Call back to same function - this is OK since the\n        // buffer is reset at the top, and since status is changed\n        // to ERROR it won't infinite loop.\n        setupResponse(responseBuf, call, RpcStatusProto.ERROR,\n            RpcErrorCodeProto.ERROR_SERIALIZING_RESPONSE,\n            null, t.getClass().getName(),\n            StringUtils.stringifyException(t));\n        return;\n      }\n    } else { // Rpc Failure\n      headerBuilder.setExceptionClassName(errorClass);\n      headerBuilder.setErrorMsg(error);\n      headerBuilder.setErrorDetail(erCode);\n      RpcResponseHeaderProto header = headerBuilder.build();\n      int headerLen = header.getSerializedSize();\n      final int fullLength  = \n          CodedOutputStream.computeRawVarint32Size(headerLen) + headerLen;\n      out.writeInt(fullLength);\n      header.writeDelimitedTo(out);\n    }\n    if (call.connection.useWrap) {\n      wrapWithSasl(responseBuf, call);\n    }\n    call.setResponse(ByteBuffer.wrap(responseBuf.toByteArray()));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.remove": "    private boolean remove(Connection connection) {\n      boolean removed = connections.remove(connection);\n      if (removed) {\n        count.getAndDecrement();\n      }\n      return removed;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeAll": "    void closeAll() {\n      // use a copy of the connections to be absolutely sure the concurrent\n      // iterator doesn't miss a connection\n      for (Connection connection : toArray()) {\n        close(connection);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.isTerse": "    boolean isTerse(Class<?> t) {\n      return terseExceptions.contains(t.toString());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    boolean close(Connection connection) {\n      boolean exists = remove(connection);\n      if (exists) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(Thread.currentThread().getName() +\n              \": disconnecting client \" + connection +\n              \". Number of active connections: \"+ size());\n        }\n        // only close if actually removed to avoid double-closing due\n        // to possible races\n        connection.close();\n      }\n      return exists;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(RPC.RpcKind rpcKind, String protocol,\n      Writable param, long receiveTime) throws Exception;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param protocolName - the protocol\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  private void authorize(UserGroupInformation user, String protocolName,\n      InetAddress addr) throws AuthorizationException {\n    if (authorize) {\n      if (protocolName == null) {\n        throw new AuthorizationException(\"Null protocol not authorized\");\n      }\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(protocolName, getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         protocolName);\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isValid() && key.isWritable()) {\n                  doAsyncWrite(key);\n              }\n            } catch (IOException e) {\n              LOG.info(Thread.currentThread().getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = Time.now();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<Call> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<Call>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              Call call = (Call)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n          \n          for(Call call : calls) {\n            doPurge(call, now);\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getRpcErrorCodeProto": "    public RpcErrorCodeProto getRpcErrorCodeProto() {\n      return errCode;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.size": "    int size() {\n      return count.get();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeIdle": "    synchronized void closeIdle(boolean scanAll) {\n      long minLastContact = Time.now() - maxIdleTime;\n      // concurrent iterator might miss new connections added\n      // during the iteration, but that's ok because they won't\n      // be idle yet anyway and will be caught on next scan\n      int closed = 0;\n      for (Connection connection : connections) {\n        // stop if connections dropped below threshold unless scanning all\n        if (!scanAll && size() < idleScanThreshold) {\n          break;\n        }\n        // stop if not scanning all and max connections are closed\n        if (connection.isIdle() &&\n            connection.getLastContact() < minLastContact &&\n            close(connection) &&\n            !scanAll && (++closed == maxIdleToClose)) {\n          break;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.startIdleScan": "    void startIdleScan() {\n      scheduleIdleScanTask();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRespond": "    void doRespond(Call call) throws IOException {\n      synchronized (call.connection.responseQueue) {\n        call.connection.responseQueue.addLast(call);\n        if (call.connection.responseQueue.size() == 1) {\n          processResponse(call.connection.responseQueue, true);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.stopIdleScan": "    void stopIdleScan() {\n      idleScanTimer.cancel();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws InterruptedException, IOException,  OutOfMemoryError {\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        channel.socket().setKeepAlive(true);\n        \n        Reader reader = getReader();\n        Connection c = connectionManager.register(channel);\n        // If the connectionManager can't take it, close the connection.\n        if (c == null) {\n          if (channel.isOpen()) {\n            IOUtils.cleanup(null, channel);\n          }\n          continue;\n        }\n        key.attach(c);  // so closeCurrentConnection can get the object\n        reader.addConnection(c);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.scheduleIdleScanTask": "    private void scheduleIdleScanTask() {\n      if (!running) {\n        return;\n      }\n      TimerTask idleScanTask = new TimerTask(){\n        @Override\n        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }\n      };\n      idleScanTimer.schedule(idleScanTask, idleScanInterval);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.call": "  public Writable call(RPC.RpcKind rpcKind, Writable rpcRequest,\n      ConnectionId remoteId, int serviceClass,\n      AtomicBoolean fallbackToSimpleAuth) throws IOException {\n    final Call call = createCall(rpcKind, rpcRequest);\n    Connection connection = getConnection(remoteId, call, serviceClass,\n      fallbackToSimpleAuth);\n    try {\n      connection.sendRpcRequest(call);                 // send the rpc request\n    } catch (RejectedExecutionException e) {\n      throw new IOException(\"connection has been closed\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      LOG.warn(\"interrupted waiting to send rpc request to server\", e);\n      throw new IOException(e);\n    }\n\n    boolean interrupted = false;\n    synchronized (call) {\n      while (!call.done) {\n        try {\n          call.wait();                           // wait for the result\n        } catch (InterruptedException ie) {\n          // save the fact that we were interrupted\n          interrupted = true;\n        }\n      }\n\n      if (interrupted) {\n        // set the interrupt flag now that we are done waiting\n        Thread.currentThread().interrupt();\n      }\n\n      if (call.error != null) {\n        if (call.error instanceof RemoteException) {\n          call.error.fillInStackTrace();\n          throw call.error;\n        } else { // local exception\n          InetSocketAddress address = connection.getRemoteAddress();\n          throw NetUtils.wrapException(address.getHostName(),\n                  address.getPort(),\n                  NetUtils.getHostname(),\n                  0,\n                  call.error);\n        }\n      } else {\n        return call.getRpcResponse();\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.sendRpcRequest": "    public void sendRpcRequest(final Call call)\n        throws InterruptedException, IOException {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n\n      // Serialize the call to be sent. This is done from the actual\n      // caller thread, rather than the sendParamsExecutor thread,\n      \n      // so that if the serialization throws an error, it is reported\n      // properly. This also parallelizes the serialization.\n      //\n      // Format of a call on the wire:\n      // 0) Length of rest below (1 + 2)\n      // 1) RpcRequestHeader  - is serialized Delimited hence contains length\n      // 2) RpcRequest\n      //\n      // Items '1' and '2' are prepared here. \n      final DataOutputBuffer d = new DataOutputBuffer();\n      RpcRequestHeaderProto header = ProtoUtil.makeRpcRequestHeader(\n          call.rpcKind, OperationProto.RPC_FINAL_PACKET, call.id, call.retry,\n          clientId);\n      header.writeDelimitedTo(d);\n      call.rpcRequest.write(d);\n\n      synchronized (sendRpcRequestLock) {\n        Future<?> senderFuture = sendParamsExecutor.submit(new Runnable() {\n          @Override\n          public void run() {\n            try {\n              synchronized (Connection.this.out) {\n                if (shouldCloseConnection.get()) {\n                  return;\n                }\n                \n                if (LOG.isDebugEnabled())\n                  LOG.debug(getName() + \" sending #\" + call.id);\n         \n                byte[] data = d.getData();\n                int totalLength = d.getLength();\n                out.writeInt(totalLength); // Total Length\n                out.write(data, 0, totalLength);// RpcRequestHeader + RpcRequest\n                out.flush();\n              }\n            } catch (IOException e) {\n              // exception at this point would leave the connection in an\n              // unrecoverable state (eg half a call left on the wire).\n              // So, close the connection, killing any outstanding calls\n              markClosed(e);\n            } finally {\n              //the buffer is just an in-memory buffer, but it is still polite to\n              // close early\n              IOUtils.closeStream(d);\n            }\n          }\n        });\n      \n        try {\n          senderFuture.get();\n        } catch (ExecutionException e) {\n          Throwable cause = e.getCause();\n          \n          // cause should only be a RuntimeException as the Runnable above\n          // catches IOException\n          if (cause instanceof RuntimeException) {\n            throw (RuntimeException) cause;\n          } else {\n            throw new RuntimeException(\"unexpected checked exception\", cause);\n          }\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnectionId": "    static ConnectionId getConnectionId(InetSocketAddress addr,\n        Class<?> protocol, UserGroupInformation ticket, int rpcTimeout,\n        RetryPolicy connectionRetryPolicy, Configuration conf) throws IOException {\n\n      if (connectionRetryPolicy == null) {\n        final int max = conf.getInt(\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY,\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_DEFAULT);\n        final int retryInterval = conf.getInt(\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_RETRY_INTERVAL_KEY,\n            CommonConfigurationKeysPublic\n                .IPC_CLIENT_CONNECT_RETRY_INTERVAL_DEFAULT);\n\n        connectionRetryPolicy = RetryPolicies.retryUpToMaximumCountWithFixedSleep(\n            max, retryInterval, TimeUnit.MILLISECONDS);\n      }\n\n      return new ConnectionId(addr, protocol, ticket, rpcTimeout,\n          connectionRetryPolicy, conf);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.createCall": "  Call createCall(RPC.RpcKind rpcKind, Writable rpcRequest) {\n    return new Call(rpcKind, rpcRequest);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRpcResponse": "    public synchronized Writable getRpcResponse() {\n      return rpcResponse;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRemoteAddress": "    public InetSocketAddress getRemoteAddress() {\n      return server;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnection": "  private Connection getConnection(ConnectionId remoteId,\n      Call call, int serviceClass, AtomicBoolean fallbackToSimpleAuth)\n      throws IOException {\n    if (!running.get()) {\n      // the client is stopped\n      throw new IOException(\"The client is stopped\");\n    }\n    Connection connection;\n    /* we could avoid this allocation for each RPC by having a  \n     * connectionsId object and with set() method. We need to manage the\n     * refs for keys in HashMap properly. For now its ok.\n     */\n    do {\n      synchronized (connections) {\n        connection = connections.get(remoteId);\n        if (connection == null) {\n          connection = new Connection(remoteId, serviceClass);\n          connections.put(remoteId, connection);\n        }\n      }\n    } while (!connection.addCall(call));\n    \n    //we don't invoke the method below inside \"synchronized (connections)\"\n    //block above. The reason for that is if the server happens to be slow,\n    //it will take longer to establish a connection and that will slow the\n    //entire system down.\n    connection.setupIOstreams(fallbackToSimpleAuth);\n    return connection;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.invoke": "    public Object invoke(Object proxy, Method method, Object[] args)\n        throws ServiceException {\n      long startTime = 0;\n      if (LOG.isDebugEnabled()) {\n        startTime = Time.now();\n      }\n      \n      if (args.length != 2) { // RpcController + Message\n        throw new ServiceException(\"Too many parameters for request. Method: [\"\n            + method.getName() + \"]\" + \", Expected: 2, Actual: \"\n            + args.length);\n      }\n      if (args[1] == null) {\n        throw new ServiceException(\"null param while calling Method: [\"\n            + method.getName() + \"]\");\n      }\n\n      TraceScope traceScope = null;\n      // if Tracing is on then start a new span for this rpc.\n      // guard it in the if statement to make sure there isn't\n      // any extra string manipulation.\n      if (Trace.isTracing()) {\n        traceScope = Trace.startSpan(RpcClientUtil.methodToTraceString(method));\n      }\n\n      RequestHeaderProto rpcRequestHeader = constructRpcRequestHeader(method);\n      \n      if (LOG.isTraceEnabled()) {\n        LOG.trace(Thread.currentThread().getId() + \": Call -> \" +\n            remoteId + \": \" + method.getName() +\n            \" {\" + TextFormat.shortDebugString((Message) args[1]) + \"}\");\n      }\n\n\n      Message theRequest = (Message) args[1];\n      final RpcResponseWrapper val;\n      try {\n        val = (RpcResponseWrapper) client.call(RPC.RpcKind.RPC_PROTOCOL_BUFFER,\n            new RpcRequestWrapper(rpcRequestHeader, theRequest), remoteId,\n            fallbackToSimpleAuth);\n\n      } catch (Throwable e) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Exception <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + e + \"}\");\n        }\n        if (Trace.isTracing()) {\n          traceScope.getSpan().addTimelineAnnotation(\n              \"Call got exception: \" + e.getMessage());\n        }\n        throw new ServiceException(e);\n      } finally {\n        if (traceScope != null) traceScope.close();\n      }\n\n      if (LOG.isDebugEnabled()) {\n        long callTime = Time.now() - startTime;\n        LOG.debug(\"Call: \" + method.getName() + \" took \" + callTime + \"ms\");\n      }\n      \n      Message prototype = null;\n      try {\n        prototype = getReturnProtoType(method);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      Message returnMessage;\n      try {\n        returnMessage = prototype.newBuilderForType()\n            .mergeFrom(val.theResponseRead).build();\n\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Response <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + TextFormat.shortDebugString(returnMessage) + \"}\");\n        }\n\n      } catch (Throwable e) {\n        throw new ServiceException(e);\n      }\n      return returnMessage;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.constructRpcRequestHeader": "    private RequestHeaderProto constructRpcRequestHeader(Method method) {\n      RequestHeaderProto.Builder builder = RequestHeaderProto\n          .newBuilder();\n      builder.setMethodName(method.getName());\n     \n\n      // For protobuf, {@code protocol} used when creating client side proxy is\n      // the interface extending BlockingInterface, which has the annotations \n      // such as ProtocolName etc.\n      //\n      // Using Method.getDeclaringClass(), as in WritableEngine to get at\n      // the protocol interface will return BlockingInterface, from where \n      // the annotation ProtocolName and Version cannot be\n      // obtained.\n      //\n      // Hence we simply use the protocol class used to create the proxy.\n      // For PB this may limit the use of mixins on client side.\n      builder.setDeclaringClassProtocolName(protocolName);\n      builder.setClientProtocolVersion(clientProtocolVersion);\n      return builder.build();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getReturnProtoType": "    private Message getReturnProtoType(Method method) throws Exception {\n      if (returnTypes.containsKey(method.getName())) {\n        return returnTypes.get(method.getName());\n      }\n      \n      Class<?> returnType = method.getReturnType();\n      Method newInstMethod = returnType.getMethod(\"getDefaultInstance\");\n      newInstMethod.setAccessible(true);\n      Message prototype = (Message) newInstMethod.invoke(null, (Object[]) null);\n      returnTypes.put(method.getName(), prototype);\n      return prototype;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.close": "    public void close() throws IOException {\n      if (!isClosed) {\n        isClosed = true;\n        CLIENTS.stopClient(client);\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations": "  public LocatedBlocks getBlockLocations(String src, long offset, long length)\n      throws AccessControlException, FileNotFoundException,\n      UnresolvedLinkException, IOException {\n    GetBlockLocationsRequestProto req = GetBlockLocationsRequestProto\n        .newBuilder()\n        .setSrc(src)\n        .setOffset(offset)\n        .setLength(length)\n        .build();\n    try {\n      GetBlockLocationsResponseProto resp = rpcProxy.getBlockLocations(null,\n          req);\n      return resp.hasLocations() ? \n        PBHelper.convert(resp.getLocations()) : null;\n    } catch (ServiceException e) {\n      throw ProtobufHelper.getRemoteException(e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod": "  protected Object invokeMethod(Method method, Object[] args) throws Throwable {\n    try {\n      if (!method.isAccessible()) {\n        method.setAccessible(true);\n      }\n      return method.invoke(currentProxy.proxy, args);\n    } catch (InvocationTargetException e) {\n      throw e.getCause();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invoke": "  public Object invoke(Object proxy, Method method, Object[] args)\n    throws Throwable {\n    RetryPolicy policy = methodNameToPolicyMap.get(method.getName());\n    if (policy == null) {\n      policy = defaultPolicy;\n    }\n    \n    // The number of times this method invocation has been failed over.\n    int invocationFailoverCount = 0;\n    final boolean isRpc = isRpcInvocation(currentProxy.proxy);\n    final int callId = isRpc? Client.nextCallId(): RpcConstants.INVALID_CALL_ID;\n    int retries = 0;\n    while (true) {\n      // The number of times this invocation handler has ever been failed over,\n      // before this method invocation attempt. Used to prevent concurrent\n      // failed method invocations from triggering multiple failover attempts.\n      long invocationAttemptFailoverCount;\n      synchronized (proxyProvider) {\n        invocationAttemptFailoverCount = proxyProviderFailoverCount;\n      }\n\n      if (isRpc) {\n        Client.setCallIdAndRetryCount(callId, retries);\n      }\n      try {\n        Object ret = invokeMethod(method, args);\n        hasMadeASuccessfulCall = true;\n        return ret;\n      } catch (Exception e) {\n        boolean isIdempotentOrAtMostOnce = proxyProvider.getInterface()\n            .getMethod(method.getName(), method.getParameterTypes())\n            .isAnnotationPresent(Idempotent.class);\n        if (!isIdempotentOrAtMostOnce) {\n          isIdempotentOrAtMostOnce = proxyProvider.getInterface()\n              .getMethod(method.getName(), method.getParameterTypes())\n              .isAnnotationPresent(AtMostOnce.class);\n        }\n        RetryAction action = policy.shouldRetry(e, retries++,\n            invocationFailoverCount, isIdempotentOrAtMostOnce);\n        if (action.action == RetryAction.RetryDecision.FAIL) {\n          if (action.reason != null) {\n            LOG.warn(\"Exception while invoking \" + currentProxy.proxy.getClass()\n                + \".\" + method.getName() + \" over \" + currentProxy.proxyInfo\n                + \". Not retrying because \" + action.reason, e);\n          }\n          throw e;\n        } else { // retry or failover\n          // avoid logging the failover if this is the first call on this\n          // proxy object, and we successfully achieve the failover without\n          // any flip-flopping\n          boolean worthLogging = \n            !(invocationFailoverCount == 0 && !hasMadeASuccessfulCall);\n          worthLogging |= LOG.isDebugEnabled();\n          if (action.action == RetryAction.RetryDecision.FAILOVER_AND_RETRY &&\n              worthLogging) {\n            String msg = \"Exception while invoking \" + method.getName()\n                + \" of class \" + currentProxy.proxy.getClass().getSimpleName()\n                + \" over \" + currentProxy.proxyInfo;\n\n            if (invocationFailoverCount > 0) {\n              msg += \" after \" + invocationFailoverCount + \" fail over attempts\"; \n            }\n            msg += \". Trying to fail over \" + formatSleepMessage(action.delayMillis);\n            LOG.info(msg, e);\n          } else {\n            if(LOG.isDebugEnabled()) {\n              LOG.debug(\"Exception while invoking \" + method.getName()\n                  + \" of class \" + currentProxy.proxy.getClass().getSimpleName()\n                  + \" over \" + currentProxy.proxyInfo + \". Retrying \"\n                  + formatSleepMessage(action.delayMillis), e);\n            }\n          }\n          \n          if (action.delayMillis > 0) {\n            Thread.sleep(action.delayMillis);\n          }\n          \n          if (action.action == RetryAction.RetryDecision.FAILOVER_AND_RETRY) {\n            // Make sure that concurrent failed method invocations only cause a\n            // single actual fail over.\n            synchronized (proxyProvider) {\n              if (invocationAttemptFailoverCount == proxyProviderFailoverCount) {\n                proxyProvider.performFailover(currentProxy.proxy);\n                proxyProviderFailoverCount++;\n              } else {\n                LOG.warn(\"A failover has occurred since the start of this method\"\n                    + \" invocation attempt.\");\n              }\n              currentProxy = proxyProvider.getProxy();\n            }\n            invocationFailoverCount++;\n          }\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations": "  static LocatedBlocks callGetBlockLocations(ClientProtocol namenode,\n      String src, long start, long length) \n      throws IOException {\n    try {\n      return namenode.getBlockLocations(src, start, length);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     UnresolvedPathException.class);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.getBlockLocations": "  public BlockLocation[] getBlockLocations(String src, long start, \n        long length) throws IOException, UnresolvedLinkException {\n    TraceScope scope = getPathTraceScope(\"getBlockLocations\", src);\n    try {\n      LocatedBlocks blocks = getLocatedBlocks(src, start, length);\n      BlockLocation[] locations =  DFSUtil.locatedBlocks2Locations(blocks);\n      HdfsBlockLocation[] hdfsLocations = new HdfsBlockLocation[locations.length];\n      for (int i = 0; i < locations.length; i++) {\n        hdfsLocations[i] = new HdfsBlockLocation(locations[i], blocks.get(i));\n      }\n      return hdfsLocations;\n    } finally {\n      scope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks": "  public LocatedBlocks getLocatedBlocks(String src, long start, long length)\n      throws IOException {\n    TraceScope scope = getPathTraceScope(\"getBlockLocations\", src);\n    try {\n      return callGetBlockLocations(namenode, src, start, length);\n    } finally {\n      scope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.close": "  public synchronized void close() throws IOException {\n    if(clientRunning) {\n      closeAllFilesBeingWritten(false);\n      clientRunning = false;\n      getLeaseRenewer().closeClient(this);\n      // close connections to the namenode\n      closeConnectionToNamenode();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.getPathTraceScope": "  TraceScope getPathTraceScope(String description, String path) {\n    TraceScope scope = Trace.startSpan(description, traceSampler);\n    Span span = scope.getSpan();\n    if (span != null) {\n      if (path != null) {\n        span.addKVAnnotation(PATH,\n            path.getBytes(Charset.forName(\"UTF-8\")));\n      }\n    }\n    return scope;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength": "  private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {\n    final LocatedBlocks newInfo = dfsClient.getLocatedBlocks(src, 0);\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"newInfo = \" + newInfo);\n    }\n    if (newInfo == null) {\n      throw new IOException(\"Cannot open filename \" + src);\n    }\n\n    if (locatedBlocks != null) {\n      Iterator<LocatedBlock> oldIter = locatedBlocks.getLocatedBlocks().iterator();\n      Iterator<LocatedBlock> newIter = newInfo.getLocatedBlocks().iterator();\n      while (oldIter.hasNext() && newIter.hasNext()) {\n        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n        }\n      }\n    }\n    locatedBlocks = newInfo;\n    long lastBlockBeingWrittenLength = 0;\n    if (!locatedBlocks.isLastBlockComplete()) {\n      final LocatedBlock last = locatedBlocks.getLastLocatedBlock();\n      if (last != null) {\n        if (last.getLocations().length == 0) {\n          if (last.getBlockSize() == 0) {\n            // if the length is zero, then no data has been written to\n            // datanode. So no need to wait for the locations.\n            return 0;\n          }\n          return -1;\n        }\n        final long len = readBlockLength(last);\n        last.getBlock().setNumBytes(len);\n        lastBlockBeingWrittenLength = len; \n      }\n    }\n\n    fileEncryptionInfo = locatedBlocks.getFileEncryptionInfo();\n\n    return lastBlockBeingWrittenLength;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.getFileEncryptionInfo": "  public FileEncryptionInfo getFileEncryptionInfo() {\n    synchronized(infoLock) {\n      return fileEncryptionInfo;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.readBlockLength": "  private long readBlockLength(LocatedBlock locatedblock) throws IOException {\n    assert locatedblock != null : \"LocatedBlock cannot be null\";\n    int replicaNotFoundCount = locatedblock.getLocations().length;\n    \n    for(DatanodeInfo datanode : locatedblock.getLocations()) {\n      ClientDatanodeProtocol cdp = null;\n      \n      try {\n        cdp = DFSUtil.createClientDatanodeProtocolProxy(datanode,\n            dfsClient.getConfiguration(), dfsClient.getConf().socketTimeout,\n            dfsClient.getConf().connectToDnViaHostname, locatedblock);\n        \n        final long n = cdp.getReplicaVisibleLength(locatedblock.getBlock());\n        \n        if (n >= 0) {\n          return n;\n        }\n      }\n      catch(IOException ioe) {\n        if (ioe instanceof RemoteException &&\n          (((RemoteException) ioe).unwrapRemoteException() instanceof\n            ReplicaNotFoundException)) {\n          // special case : replica might not be on the DN, treat as 0 length\n          replicaNotFoundCount--;\n        }\n        \n        if (DFSClient.LOG.isDebugEnabled()) {\n          DFSClient.LOG.debug(\"Failed to getReplicaVisibleLength from datanode \"\n              + datanode + \" for block \" + locatedblock.getBlock(), ioe);\n        }\n      } finally {\n        if (cdp != null) {\n          RPC.stopProxy(cdp);\n        }\n      }\n    }\n\n    // Namenode told us about these locations, but none know about the replica\n    // means that we hit the race between pipeline creation start and end.\n    // we require all 3 because some other exception could have happened\n    // on a DN that has it.  we want to report that error\n    if (replicaNotFoundCount == 0) {\n      return 0;\n    }\n\n    throw new IOException(\"Cannot obtain block length for \" + locatedblock);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.openInfo": "  void openInfo() throws IOException, UnresolvedLinkException {\n    synchronized(infoLock) {\n      lastBlockBeingWrittenLength = fetchLocatedBlocksAndGetLastBlockLength();\n      int retriesForLastBlockLength = dfsClient.getConf().retryTimesForGetLastBlockLength;\n      while (retriesForLastBlockLength > 0) {\n        // Getting last block length as -1 is a special case. When cluster\n        // restarts, DNs may not report immediately. At this time partial block\n        // locations will not be available with NN for getting the length. Lets\n        // retry for 3 times to get the length.\n        if (lastBlockBeingWrittenLength == -1) {\n          DFSClient.LOG.warn(\"Last block locations not available. \"\n              + \"Datanodes might not have reported blocks completely.\"\n              + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n          waitFor(dfsClient.getConf().retryIntervalForGetLastBlockLength);\n          lastBlockBeingWrittenLength = fetchLocatedBlocksAndGetLastBlockLength();\n        } else {\n          break;\n        }\n        retriesForLastBlockLength--;\n      }\n      if (retriesForLastBlockLength == 0) {\n        throw new IOException(\"Could not obtain the last block locations.\");\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.waitFor": "  private void waitFor(int waitTime) throws IOException {\n    try {\n      Thread.sleep(waitTime);\n    } catch (InterruptedException e) {\n      throw new IOException(\n          \"Interrupted while getting the last block length.\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.open": "  public DFSInputStream open(String src, int buffersize, boolean verifyChecksum)\n      throws IOException, UnresolvedLinkException {\n    checkOpen();\n    //    Get block info from namenode\n    TraceScope scope = getPathTraceScope(\"newDFSInputStream\", src);\n    try {\n      return new DFSInputStream(this, src, verifyChecksum);\n    } finally {\n      scope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.checkOpen": "  void checkOpen() throws IOException {\n    if (!clientRunning) {\n      IOException result = new IOException(\"Filesystem closed\");\n      throw result;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.doCall": "      public Void doCall(final Path p) throws IOException {\n        dfs.checkAccess(getPathName(p), mode);\n        return null;\n      }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs": "  public boolean mkdirs(Path f, FsPermission permission) throws IOException {\n    return mkdirsInternal(f, permission, true);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setOwner": "  public void setOwner(Path p, final String username, final String groupname\n      ) throws IOException {\n    if (username == null && groupname == null) {\n      throw new IOException(\"username == null && groupname == null\");\n    }\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(p);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.setOwner(getPathName(p), username, groupname);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        fs.setOwner(p, username, groupname);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.truncate": "  public boolean truncate(Path f, final long newLength) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.truncate(getPathName(p), newLength);\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.truncate(p, newLength);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getUri": "  public URI getUri() { return uri; }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal": "  private FileStatus[] listStatusInternal(Path p) throws IOException {\n    String src = getPathName(p);\n\n    // fetch the first batch of entries in the directory\n    DirectoryListing thisListing = dfs.listPaths(\n        src, HdfsFileStatus.EMPTY_NAME);\n\n    if (thisListing == null) { // the directory does not exist\n      throw new FileNotFoundException(\"File \" + p + \" does not exist.\");\n    }\n    \n    HdfsFileStatus[] partialListing = thisListing.getPartialListing();\n    if (!thisListing.hasMore()) { // got all entries of the directory\n      FileStatus[] stats = new FileStatus[partialListing.length];\n      for (int i = 0; i < partialListing.length; i++) {\n        stats[i] = partialListing[i].makeQualified(getUri(), p);\n      }\n      statistics.incrementReadOps(1);\n      return stats;\n    }\n\n    // The directory size is too big that it needs to fetch more\n    // estimate the total number of entries in the directory\n    int totalNumEntries =\n      partialListing.length + thisListing.getRemainingEntries();\n    ArrayList<FileStatus> listing =\n      new ArrayList<FileStatus>(totalNumEntries);\n    // add the first batch of entries to the array list\n    for (HdfsFileStatus fileStatus : partialListing) {\n      listing.add(fileStatus.makeQualified(getUri(), p));\n    }\n    statistics.incrementLargeReadOps(1);\n \n    // now fetch more entries\n    do {\n      thisListing = dfs.listPaths(src, thisListing.getLastName());\n \n      if (thisListing == null) { // the directory is deleted\n        throw new FileNotFoundException(\"File \" + p + \" does not exist.\");\n      }\n \n      partialListing = thisListing.getPartialListing();\n      for (HdfsFileStatus fileStatus : partialListing) {\n        listing.add(fileStatus.makeQualified(getUri(), p));\n      }\n      statistics.incrementLargeReadOps(1);\n    } while (thisListing.hasMore());\n \n    return listing.toArray(new FileStatus[listing.size()]);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.create": "  public FSDataOutputStream create(final Path f, final FsPermission permission,\n    final EnumSet<CreateFlag> cflags, final int bufferSize,\n    final short replication, final long blockSize, final Progressable progress,\n    final ChecksumOpt checksumOpt) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FSDataOutputStream>() {\n      @Override\n      public FSDataOutputStream doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        final DFSOutputStream dfsos = dfs.create(getPathName(p), permission,\n                cflags, replication, blockSize, progress, bufferSize,\n                checksumOpt);\n        return dfs.createWrappedOutputStream(dfsos, statistics);\n      }\n      @Override\n      public FSDataOutputStream next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.create(p, permission, cflags, bufferSize,\n            replication, blockSize, progress, checksumOpt);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.removeAclEntries": "  public void removeAclEntries(Path path, final List<AclEntry> aclSpec)\n      throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.removeAclEntries(getPathName(p), aclSpec);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.removeAclEntries(p, aclSpec);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.removeXAttr": "  public void removeXAttr(Path path, final String name) throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.removeXAttr(getPathName(p), name);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.removeXAttr(p, name);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getSnapshotDiffReport": "  public SnapshotDiffReport getSnapshotDiffReport(final Path snapshotDir,\n      final String fromSnapshot, final String toSnapshot) throws IOException {\n    Path absF = fixRelativePart(snapshotDir);\n    return new FileSystemLinkResolver<SnapshotDiffReport>() {\n      @Override\n      public SnapshotDiffReport doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.getSnapshotDiffReport(getPathName(p), fromSnapshot,\n            toSnapshot);\n      }\n\n      @Override\n      public SnapshotDiffReport next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.getSnapshotDiffReport(p, fromSnapshot, toSnapshot);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + snapshotDir + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum": "  public FileChecksum getFileChecksum(Path f, final long length)\n      throws IOException {\n    statistics.incrementReadOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FileChecksum>() {\n      @Override\n      public FileChecksum doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.getFileChecksum(getPathName(p), length);\n      }\n\n      @Override\n      public FileChecksum next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          return ((DistributedFileSystem) fs).getFileChecksum(p, length);\n        } else {\n          throw new UnsupportedFileSystemException(\n              \"getFileChecksum(Path, long) is not supported by \"\n                  + fs.getClass().getSimpleName()); \n        }\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getAclStatus": "  public AclStatus getAclStatus(Path path) throws IOException {\n    final Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<AclStatus>() {\n      @Override\n      public AclStatus doCall(final Path p) throws IOException {\n        return dfs.getAclStatus(getPathName(p));\n      }\n      @Override\n      public AclStatus next(final FileSystem fs, final Path p)\n        throws IOException, UnresolvedLinkException {\n        return fs.getAclStatus(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.recoverLease": "  public boolean recoverLease(final Path f) throws IOException {\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.recoverLease(getPathName(p));\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.recoverLease(p);\n        }\n        throw new UnsupportedOperationException(\"Cannot recoverLease through\" +\n            \" a symlink to a non-DistributedFileSystem: \" + f + \" -> \" + p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.listXAttrs": "  public List<String> listXAttrs(Path path)\n          throws IOException {\n    final Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<List<String>>() {\n      @Override\n      public List<String> doCall(final Path p) throws IOException {\n        return dfs.listXAttrs(getPathName(p));\n      }\n      @Override\n      public List<String> next(final FileSystem fs, final Path p)\n              throws IOException, UnresolvedLinkException {\n        return fs.listXAttrs(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.rename": "  public void rename(Path src, Path dst, final Options.Rename... options)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    final Path absSrc = fixRelativePart(src);\n    final Path absDst = fixRelativePart(dst);\n    // Try the rename without resolving first\n    try {\n      dfs.rename(getPathName(absSrc), getPathName(absDst), options);\n    } catch (UnresolvedLinkException e) {\n      // Fully resolve the source\n      final Path source = getFileLinkStatus(absSrc).getPath();\n      // Keep trying to resolve the destination\n      new FileSystemLinkResolver<Void>() {\n        @Override\n        public Void doCall(final Path p)\n            throws IOException, UnresolvedLinkException {\n          dfs.rename(getPathName(source), getPathName(p), options);\n          return null;\n        }\n        @Override\n        public Void next(final FileSystem fs, final Path p)\n            throws IOException {\n          // Should just throw an error in FileSystem#checkPath\n          return doCall(p);\n        }\n      }.resolve(this, absDst);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.createSymlink": "  public void createSymlink(final Path target, final Path link,\n      final boolean createParent) throws AccessControlException,\n      FileAlreadyExistsException, FileNotFoundException,\n      ParentNotDirectoryException, UnsupportedFileSystemException, \n      IOException {\n    statistics.incrementWriteOps(1);\n    final Path absF = fixRelativePart(link);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException,\n          UnresolvedLinkException {\n        dfs.createSymlink(target.toString(), getPathName(p), createParent);\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException, UnresolvedLinkException {\n        fs.createSymlink(target, p, createParent);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setPermission": "  public void setPermission(Path p, final FsPermission permission\n      ) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(p);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.setPermission(getPathName(p), permission);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        fs.setPermission(p, permission);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.delete": "  public boolean delete(Path f, final boolean recursive) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.delete(getPathName(p), recursive);\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.delete(p, recursive);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.disallowSnapshot": "  public void disallowSnapshot(final Path path) throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.disallowSnapshot(getPathName(p));\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.disallowSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setReplication": "  public boolean setReplication(Path src, \n                                final short replication\n                               ) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(src);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.setReplication(getPathName(p), replication);\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.setReplication(p, replication);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setStoragePolicy": "  public void setStoragePolicy(final Path src, final String policyName)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(src);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.setStoragePolicy(getPathName(p), policyName);\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          ((DistributedFileSystem) fs).setStoragePolicy(p, policyName);\n          return null;\n        } else {\n          throw new UnsupportedOperationException(\n              \"Cannot perform setStoragePolicy on a non-DistributedFileSystem: \"\n                  + src + \" -> \" + p);\n        }\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.modifyAclEntries": "  public void modifyAclEntries(Path path, final List<AclEntry> aclSpec)\n      throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.modifyAclEntries(getPathName(p), aclSpec);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.modifyAclEntries(p, aclSpec);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.open": "  public FSDataInputStream open(Path f, final int bufferSize)\n      throws IOException {\n    statistics.incrementReadOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FSDataInputStream>() {\n      @Override\n      public FSDataInputStream doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        final DFSInputStream dfsis =\n          dfs.open(getPathName(p), bufferSize, verifyChecksum);\n        return dfs.createWrappedInputStream(dfsis);\n      }\n      @Override\n      public FSDataInputStream next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.open(p, bufferSize);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.append": "  public FSDataOutputStream append(Path f, final EnumSet<CreateFlag> flag,\n      final int bufferSize, final Progressable progress,\n      final InetSocketAddress[] favoredNodes) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FSDataOutputStream>() {\n      @Override\n      public FSDataOutputStream doCall(final Path p)\n          throws IOException {\n        return dfs.append(getPathName(p), bufferSize, flag, progress,\n            statistics, favoredNodes);\n      }\n      @Override\n      public FSDataOutputStream next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.append(p, bufferSize);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setTimes": "  public void setTimes(Path p, final long mtime, final long atime\n      ) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(p);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.setTimes(getPathName(p), mtime, atime);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        fs.setTimes(p, mtime, atime);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.deleteSnapshot": "  public void deleteSnapshot(final Path snapshotDir, final String snapshotName)\n      throws IOException {\n    Path absF = fixRelativePart(snapshotDir);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.deleteSnapshot(getPathName(p), snapshotName);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.deleteSnapshot(p, snapshotName);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + snapshotDir + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.isFileClosed": "  public boolean isFileClosed(final Path src) throws IOException {\n    Path absF = fixRelativePart(src);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.isFileClosed(getPathName(p));\n      }\n\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.isFileClosed(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot call isFileClosed\"\n              + \" on a symlink to a non-DistributedFileSystem: \"\n              + src + \" -> \" + p);\n        }\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.removeAcl": "  public void removeAcl(Path path) throws IOException {\n    final Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.removeAcl(getPathName(p));\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n        throws IOException, UnresolvedLinkException {\n        fs.removeAcl(p);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setXAttr": "  public void setXAttr(Path path, final String name, final byte[] value, \n      final EnumSet<XAttrSetFlag> flag) throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.setXAttr(getPathName(p), name, value, flag);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.setXAttr(p, name, value, flag);\n        return null;\n      }      \n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getXAttrs": "  public Map<String, byte[]> getXAttrs(Path path, final List<String> names) \n      throws IOException {\n    final Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<Map<String, byte[]>>() {\n      @Override\n      public Map<String, byte[]> doCall(final Path p) throws IOException {\n        return dfs.getXAttrs(getPathName(p), names);\n      }\n      @Override\n      public Map<String, byte[]> next(final FileSystem fs, final Path p)\n        throws IOException, UnresolvedLinkException {\n        return fs.getXAttrs(p, names);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getContentSummary": "  public ContentSummary getContentSummary(Path f) throws IOException {\n    statistics.incrementReadOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<ContentSummary>() {\n      @Override\n      public ContentSummary doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.getContentSummary(getPathName(p));\n      }\n      @Override\n      public ContentSummary next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.getContentSummary(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.removeDefaultAcl": "  public void removeDefaultAcl(Path path) throws IOException {\n    final Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.removeDefaultAcl(getPathName(p));\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n        throws IOException, UnresolvedLinkException {\n        fs.removeDefaultAcl(p);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getXAttr": "  public byte[] getXAttr(Path path, final String name) throws IOException {\n    final Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<byte[]>() {\n      @Override\n      public byte[] doCall(final Path p) throws IOException {\n        return dfs.getXAttr(getPathName(p), name);\n      }\n      @Override\n      public byte[] next(final FileSystem fs, final Path p)\n        throws IOException, UnresolvedLinkException {\n        return fs.getXAttr(p, name);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.toString": "  public String toString() {\n    return \"DFS[\" + dfs + \"]\";\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setQuota": "  public void setQuota(Path src, final long namespaceQuota,\n      final long storagespaceQuota) throws IOException {\n    Path absF = fixRelativePart(src);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.setQuota(getPathName(p), namespaceQuota, storagespaceQuota);\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        // setQuota is not defined in FileSystem, so we only can resolve\n        // within this DFS\n        return doCall(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setQuotaByStorageType": "  public void setQuotaByStorageType(\n    Path src, final StorageType type, final long quota)\n    throws IOException {\n    Path absF = fixRelativePart(src);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n        throws IOException, UnresolvedLinkException {\n        dfs.setQuotaByStorageType(getPathName(p), type, quota);\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n        throws IOException {\n        // setQuotaByStorageType is not defined in FileSystem, so we only can resolve\n        // within this DFS\n        return doCall(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.renameSnapshot": "  public void renameSnapshot(final Path path, final String snapshotOldName,\n      final String snapshotNewName) throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.renameSnapshot(getPathName(p), snapshotOldName, snapshotNewName);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.renameSnapshot(p, snapshotOldName, snapshotNewName);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setAcl": "  public void setAcl(Path path, final List<AclEntry> aclSpec) throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.setAcl(getPathName(p), aclSpec);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.setAcl(p, aclSpec);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.allowSnapshot": "  public void allowSnapshot(final Path path) throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.allowSnapshot(getPathName(p));\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.allowSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.createSnapshot": "  public Path createSnapshot(final Path path, final String snapshotName) \n      throws IOException {\n    Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<Path>() {\n      @Override\n      public Path doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return new Path(dfs.createSnapshot(getPathName(p), snapshotName));\n      }\n\n      @Override\n      public Path next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.createSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getPathName": "  private String getPathName(Path file) {\n    checkPath(file);\n    String result = file.toUri().getPath();\n    if (!DFSUtil.isValidName(result)) {\n      throw new IllegalArgumentException(\"Pathname \" + result + \" from \" +\n                                         file+\" is not a valid DFS filename.\");\n    }\n    return result;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystemLinkResolver.resolve": "  public T resolve(final FileSystem filesys, final Path path)\n      throws IOException {\n    int count = 0;\n    T in = null;\n    Path p = path;\n    // Assumes path belongs to this FileSystem.\n    // Callers validate this by passing paths through FileSystem#checkPath\n    FileSystem fs = filesys;\n    for (boolean isLink = true; isLink;) {\n      try {\n        in = doCall(p);\n        isLink = false;\n      } catch (UnresolvedLinkException e) {\n        if (!filesys.resolveSymlinks) {\n          throw new IOException(\"Path \" + path + \" contains a symlink\"\n              + \" and symlink resolution is disabled (\"\n              + CommonConfigurationKeys.FS_CLIENT_RESOLVE_REMOTE_SYMLINKS_KEY\n              + \").\", e);\n        }\n        if (count++ > FsConstants.MAX_PATH_LINKS) {\n          throw new IOException(\"Possible cyclic loop while \" +\n                                \"following symbolic link \" + path);\n        }\n        // Resolve the first unresolved path component\n        p = FSLinkResolver.qualifySymlinkTarget(fs.getUri(), p,\n            filesys.resolveLink(p));\n        fs = FileSystem.getFSofPath(p, filesys.getConf());\n        // Have to call next if it's a new FS\n        if (!fs.equals(filesys)) {\n          return next(fs, p);\n        }\n        // Else, we keep resolving with this filesystem\n      }\n    }\n    // Successful call, path was fully resolved\n    return in;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystemLinkResolver.doCall": "  abstract public T doCall(final Path p) throws IOException,\n      UnresolvedLinkException;\n\n  /**\n   * Calls the abstract FileSystem call equivalent to the specialized subclass\n   * implementation in {@link #doCall(Path)}. This is used when retrying the",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystemLinkResolver.next": "  abstract public T next(final FileSystem fs, final Path p) throws IOException;\n\n  /**\n   * Attempt calling overridden {@link #doCall(Path)} method with",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.open": "  public FSDataInputStream open(Path f) throws IOException {\n    return open(f, getConf().getInt(\"io.file.buffer.size\", 4096));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.getAccessTime": "  public final long getAccessTime() {\n    return getAccessTime(Snapshot.CURRENT_STATE_ID);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convert": "  public static BlockReportContextProto convert(BlockReportContext context) {\n    return BlockReportContextProto.newBuilder().\n        setTotalRpcs(context.getTotalRpcs()).\n        setCurRpc(context.getCurRpc()).\n        setId(context.getReportId()).\n        build();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertAclEntry": "  public static List<AclEntry> convertAclEntry(List<AclEntryProto> aclSpec) {\n    ArrayList<AclEntry> r = Lists.newArrayListWithCapacity(aclSpec.size());\n    for (AclEntryProto e : aclSpec) {\n      AclEntry.Builder builder = new AclEntry.Builder();\n      builder.setType(convert(e.getType()));\n      builder.setScope(convert(e.getScope()));\n      builder.setPermission(convert(e.getPermissions()));\n      if (e.hasName()) {\n        builder.setName(e.getName());\n      }\n      r.add(builder.build());\n    }\n    return r;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertLocatedBlock": "  public static List<LocatedBlock> convertLocatedBlock(\n      List<LocatedBlockProto> lb) {\n    if (lb == null) return null;\n    final int len = lb.size();\n    List<LocatedBlock> result = \n        new ArrayList<LocatedBlock>(len);\n    for (int i = 0; i < len; ++i) {\n      result.add(PBHelper.convert(lb.get(i)));\n    }\n    return result;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertStorageType": "  public static StorageType convertStorageType(StorageTypeProto type) {\n    switch(type) {\n      case DISK:\n        return StorageType.DISK;\n      case SSD:\n        return StorageType.SSD;\n      case ARCHIVE:\n        return StorageType.ARCHIVE;\n      case RAM_DISK:\n        return StorageType.RAM_DISK;\n      default:\n        throw new IllegalStateException(\n            \"BUG: StorageTypeProto not found, type=\" + type);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertXAttrs": "  public static List<XAttr> convertXAttrs(List<XAttrProto> xAttrSpec) {\n    ArrayList<XAttr> xAttrs = Lists.newArrayListWithCapacity(xAttrSpec.size());\n    for (XAttrProto a : xAttrSpec) {\n      XAttr.Builder builder = new XAttr.Builder();\n      builder.setNameSpace(convert(a.getNamespace()));\n      if (a.hasName()) {\n        builder.setName(a.getName());\n      }\n      if (a.hasValue()) {\n        builder.setValue(a.getValue().toByteArray());\n      }\n      xAttrs.add(builder.build());\n    }\n    return xAttrs;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertLocatedBlock2": "  public static List<LocatedBlockProto> convertLocatedBlock2(List<LocatedBlock> lb) {\n    if (lb == null) return null;\n    final int len = lb.size();\n    List<LocatedBlockProto> result = new ArrayList<LocatedBlockProto>(len);\n    for (int i = 0; i < len; ++i) {\n      result.add(PBHelper.convert(lb.get(i)));\n    }\n    return result;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.createTypeConvert": "  private static InotifyProtos.INodeType createTypeConvert(Event.CreateEvent.INodeType\n      type) {\n    switch (type) {\n    case DIRECTORY:\n      return InotifyProtos.INodeType.I_TYPE_DIRECTORY;\n    case FILE:\n      return InotifyProtos.INodeType.I_TYPE_FILE;\n    case SYMLINK:\n      return InotifyProtos.INodeType.I_TYPE_SYMLINK;\n    default:\n      return null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertAclEntryProto": "  public static List<AclEntryProto> convertAclEntryProto(\n      List<AclEntry> aclSpec) {\n    ArrayList<AclEntryProto> r = Lists.newArrayListWithCapacity(aclSpec.size());\n    for (AclEntry e : aclSpec) {\n      AclEntryProto.Builder builder = AclEntryProto.newBuilder();\n      builder.setType(convert(e.getType()));\n      builder.setScope(convert(e.getScope()));\n      builder.setPermissions(convert(e.getPermission()));\n      if (e.getName() != null) {\n        builder.setName(e.getName());\n      }\n      r.add(builder.build());\n    }\n    return r;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertState": "  private static State convertState(StorageState state) {\n    switch(state) {\n    case READ_ONLY_SHARED:\n      return DatanodeStorage.State.READ_ONLY_SHARED;\n    case NORMAL:\n    default:\n      return DatanodeStorage.State.NORMAL;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertRollingUpgradeStatus": "  public static RollingUpgradeStatusProto convertRollingUpgradeStatus(\n      RollingUpgradeStatus status) {\n    return RollingUpgradeStatusProto.newBuilder()\n        .setBlockPoolId(status.getBlockPoolId())\n        .setFinalized(status.isFinalized())\n        .build();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertStorageTypes": "  public static StorageType[] convertStorageTypes(\n      List<StorageTypeProto> storageTypesList, int expectedSize) {\n    final StorageType[] storageTypes = new StorageType[expectedSize];\n    if (storageTypesList.size() != expectedSize) { // missing storage types\n      Preconditions.checkState(storageTypesList.isEmpty());\n      Arrays.fill(storageTypes, StorageType.DEFAULT);\n    } else {\n      for (int i = 0; i < storageTypes.length; ++i) {\n        storageTypes[i] = convertStorageType(storageTypesList.get(i));\n      }\n    }\n    return storageTypes;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.castEnum": "  private static <T extends Enum<T>, U extends Enum<U>> U castEnum(T from, U[] to) {\n    return to[from.ordinal()];\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.getByteString": "  public static ByteString getByteString(byte[] bytes) {\n    return ByteString.copyFrom(bytes);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertBlockKeys": "  public static BlockKey[] convertBlockKeys(List<BlockKeyProto> list) {\n    BlockKey[] ret = new BlockKey[list.size()];\n    int i = 0;\n    for (BlockKeyProto k : list) {\n      ret[i++] = convert(k);\n    }\n    return ret;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.metadataUpdateTypeConvert": "  private static Event.MetadataUpdateEvent.MetadataType metadataUpdateTypeConvert(\n      InotifyProtos.MetadataUpdateType type) {\n    switch (type) {\n    case META_TYPE_TIMES:\n      return Event.MetadataUpdateEvent.MetadataType.TIMES;\n    case META_TYPE_REPLICATION:\n      return Event.MetadataUpdateEvent.MetadataType.REPLICATION;\n    case META_TYPE_OWNER:\n      return Event.MetadataUpdateEvent.MetadataType.OWNER;\n    case META_TYPE_PERMS:\n      return Event.MetadataUpdateEvent.MetadataType.PERMS;\n    case META_TYPE_ACLS:\n      return Event.MetadataUpdateEvent.MetadataType.ACLS;\n    case META_TYPE_XATTRS:\n      return Event.MetadataUpdateEvent.MetadataType.XATTRS;\n    default:\n      return null;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcServerException.getRpcStatusProto": "  public RpcStatusProto getRpcStatusProto() {\n    return RpcStatusProto.ERROR;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcClientUtil.methodToTraceString": "  public static String methodToTraceString(Method method) {\n    Class<?> clazz = method.getDeclaringClass();\n    while (true) {\n      Class<?> next = clazz.getEnclosingClass();\n      if (next == null || next.getEnclosingClass() == null) break;\n      clazz = next;\n    }\n    return clazz.getSimpleName() + \"#\" + method.getName();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.dfsClient.getLocatedBlocks": "  public LocatedBlocks getLocatedBlocks(String src, long start, long length)\n      throws IOException {\n    TraceScope scope = getPathTraceScope(\"getBlockLocations\", src);\n    try {\n      return callGetBlockLocations(namenode, src, start, length);\n    } finally {\n      scope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.dfsClient.close": "  public synchronized void close() throws IOException {\n    if(clientRunning) {\n      closeAllFilesBeingWritten(false);\n      clientRunning = false;\n      getLeaseRenewer().closeClient(this);\n      // close connections to the namenode\n      closeConnectionToNamenode();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.dfsClient.callGetBlockLocations": "  static LocatedBlocks callGetBlockLocations(ClientProtocol namenode,\n      String src, long start, long length) \n      throws IOException {\n    try {\n      return namenode.getBlockLocations(src, start, length);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     UnresolvedPathException.class);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.dfsClient.getPathTraceScope": "  TraceScope getPathTraceScope(String description, String path) {\n    TraceScope scope = Trace.startSpan(description, traceSampler);\n    Span span = scope.getSpan();\n    if (span != null) {\n      if (path != null) {\n        span.addKVAnnotation(PATH,\n            path.getBytes(Charset.forName(\"UTF-8\")));\n      }\n    }\n    return scope;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.dfsClient.getConf": "  public Conf getConf() {\n    return dfsClientConf;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FSLinkResolver.qualifySymlinkTarget": "  public static Path qualifySymlinkTarget(final URI pathURI,\n      Path pathWithLink, Path target) {\n    // NB: makeQualified uses the target's scheme and authority, if\n    // specified, and the scheme and authority of pathURI, if not.\n    final URI targetUri = target.toUri();\n    final String scheme = targetUri.getScheme();\n    final String auth = targetUri.getAuthority();\n    return (scheme == null && auth == null) ? target.makeQualified(pathURI,\n        pathWithLink.getParent()) : target;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getUri": "  public abstract URI getUri();\n  \n  /**\n   * Return a canonicalized form of this FileSystem's URI.\n   * \n   * The default implementation simply calls {@link #canonicalizeUri(URI)}",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getFSofPath": "  protected static FileSystem getFSofPath(final Path absOrFqPath,\n      final Configuration conf)\n      throws UnsupportedFileSystemException, IOException {\n    absOrFqPath.checkNotSchemeWithRelative();\n    absOrFqPath.checkNotRelative();\n\n    // Uses the default file system if not fully qualified\n    return get(absOrFqPath.toUri(), conf);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.get": "    FileSystem get(URI uri, Configuration conf) throws IOException{\n      Key key = new Key(uri, conf);\n      return getInternal(uri, conf, key);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.equals": "      public boolean equals(Object obj) {\n        if (obj == this) {\n          return true;\n        }\n        if (obj != null && obj instanceof Key) {\n          Key that = (Key)obj;\n          return isEqual(this.scheme, that.scheme)\n                 && isEqual(this.authority, that.authority)\n                 && isEqual(this.ugi, that.ugi)\n                 && (this.unique == that.unique);\n        }\n        return false;        \n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.isEqual": "      static boolean isEqual(Object a, Object b) {\n        return a == b || (a != null && a.equals(b));        \n      }"
        },
        "bug_report": {
            "Title": "NullPointerException when topology script is missing.",
            "Description": "We've received reports that the NameNode can get a NullPointerException when the topology script is missing. This issue tracks investigating whether or not we can improve the validation logic and give a more informative error message.\n\nHere is a sample stack trace :\nGetting NPE from HDFS:\n \n 2015-02-06 23:02:12,250 ERROR [pool-4-thread-1] util.HFileV1Detector: Got exception while reading trailer for file:hdfs://hqhd02nm01.pclc0.merkle.local:8020/hbase/.META./1028785192/info/1490a396aea448b693da563f76a28486^M\n org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException^M\n         at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedBlocks(DatanodeManager.java:359)^M\n         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1789)^M\n         at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)^M\n         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)^M\n         at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)^M\n         at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)^M\n         at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)^M\n         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)^M\n         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)^M\n         at java.security.AccessController.doPrivileged(Native Method)^M\n         at javax.security.auth.Subject.doAs(Subject.java:415)^M\n         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)^M\n         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)^M\n ^M\n         at org.apache.hadoop.ipc.Client.call(Client.java:1468)^M\n         at org.apache.hadoop.ipc.Client.call(Client.java:1399)^M\n         at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)^M\n         at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)^M\n         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:254)^M\n         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)^M\n         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)^M\n         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)^M\n         at java.lang.reflect.Method.invoke(Method.java:606)^M\n         at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)^M\n         at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)^M\n         at com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)^M\n         at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1220)^M\n         at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1210)^M\n         at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1200)^M\n         at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:271)^M\n         at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:238)^M\n         at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:231)^M\n         at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1498)^M\n         at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:302)^M\n         at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:298)^M\n         at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)^M\n         at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:298)^M\n         at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)^M\n         at org.apache.hadoop.hbase.util.HFileV1Detector$1.call(HFileV1Detector.java:320)^M\n         at org.apache.hadoop.hbase.util.HFileV1Detector$1.call(HFileV1Detector.java:300)^M\n         at java.util.concurrent.FutureTask.run(FutureTask.java:262)^M\n         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)^M\n         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)^M\n         at java.lang.Thread.run(Thread.java:745)^M\n 2015-02-06 23:02:12,263 ERROR [pool-4-thread-1] util.HFileV1Detector: Got exception while reading trailer for file:hdfs://hqhd02nm01.pclc0.merkle.local:8020/hbase/.META./1028785192/info/a06f2483f6864d818884d0a451cb91d5^M\n org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException^M\n         at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedBlocks(DatanodeManager.java:359)^M\n         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1789)^M\n         at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)^M\n         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)^M\n"
        }
    },
    {
        "filename": "HDFS-6533.json",
        "creation_time": "2014-06-14T16:15:02.000+0000",
        "stack_trace": "```\norg.mockito.exceptions.verification.WantedButNotInvoked: \nWanted but not invoked:\ndatanodeProtocolClientSideTranslatorPB.registerDatanode(\n    <any>\n);\n-> at org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBasicFunctionality(TestBPOfferService.java:175)\nActually, there were zero interactions with this mock.\n\n\tat org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBasicFunctionality(TestBPOfferService.java:175)\n```",
        "source_code": {},
        "bug_report": {
            "Title": "TestBPOfferService#testBasicFunctionalitytest fails intermittently",
            "Description": "Per https://builds.apache.org/job/Hadoop-Hdfs-trunk/1774/testReport, the following test failed. However, local rerun is successful.\n\n{code}\norg.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBasicFunctionality\n\nError Message\n\nWanted but not invoked:\ndatanodeProtocolClientSideTranslatorPB.registerDatanode(\n    <any>\n);\n-> at org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBasicFunctionality(TestBPOfferService.java:175)\nActually, there were zero interactions with this mock.\nStacktrace\n\norg.mockito.exceptions.verification.WantedButNotInvoked: \nWanted but not invoked:\ndatanodeProtocolClientSideTranslatorPB.registerDatanode(\n    <any>\n);\n-> at org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBasicFunctionality(TestBPOfferService.java:175)\nActually, there were zero interactions with this mock.\n\n\tat org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBasicFunctionality(TestBPOfferService.java:175)\nStandard Output\n\n2014-06-14 12:42:08,723 INFO  datanode.DataNode (SimulatedFSDataset.java:registerMBean(968)) - Registered FSDatasetState MBean\n2014-06-14 12:42:08,730 INFO  datanode.DataNode (BPServiceActor.java:run(805)) - Block pool <registering> (Datanode Uuid unassigned) service to 0.0.0.0/0.0.0.0:0 starting to offer service\n2014-06-14 12:42:08,730 DEBUG datanode.DataNode (BPServiceActor.java:retrieveNamespaceInfo(170)) - Block pool <registering> (Datanode Uuid unassigned) service to 0.0.0.0/0.0.0.0:0 received versionRequest response: lv=-57;cid=fake cluster;nsid=1;c=0;bpid=fake bpid\n2014-06-14 12:42:08,731 INFO  datanode.DataNode (BPServiceActor.java:register(765)) - Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0 beginning handshake with NN\n2014-06-14 12:42:08,731 INFO  datanode.DataNode (BPServiceActor.java:register(778)) - Block pool Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0 successfully registered with NN\n2014-06-14 12:42:08,732 INFO  datanode.DataNode (BPServiceActor.java:offerService(637)) - For namenode 0.0.0.0/0.0.0.0:0 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\n2014-06-14 12:42:08,732 DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(562)) - Sending heartbeat with 1 storage reports from service actor: Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0\n2014-06-14 12:42:08,734 INFO  datanode.DataNode (BPServiceActor.java:blockReport(498)) - Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 0 msecs for RPC and NN processing.  Got back commands none\n2014-06-14 12:42:08,738 INFO  datanode.DataNode (BPServiceActor.java:run(805)) - Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:1 starting to offer service\n2014-06-14 12:42:08,739 DEBUG datanode.DataNode (BPServiceActor.java:retrieveNamespaceInfo(170)) - Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:1 received versionRequest response: lv=-57;cid=fake cluster;nsid=1;c=0;bpid=fake bpid\n2014-06-14 12:42:08,739 INFO  datanode.DataNode (BPServiceActor.java:register(765)) - Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:1 beginning handshake with NN\n2014-06-14 12:42:08,740 INFO  datanode.DataNode (BPServiceActor.java:register(778)) - Block pool Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:1 successfully registered with NN\n{code}\n\n"
        }
    },
    {
        "filename": "HDFS-10609.json",
        "creation_time": "2016-07-11T17:14:39.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=557709482) doesn't exist. Current key: 1350592619\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:1308)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1272)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1433)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1147)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:632)\n\nCaused by: org.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=557709482) doesn't exist. Current key: 1350592619\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:1308)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1272)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1433)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1147)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:632)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption": "  public static SaslResponseWithNegotiatedCipherOption\n      readSaslMessageAndNegotiatedCipherOption(InputStream in)\n      throws IOException {\n    DataTransferEncryptorMessageProto proto =\n        DataTransferEncryptorMessageProto.parseFrom(vintPrefixed(in));\n    if (proto.getStatus() == DataTransferEncryptorStatus.ERROR_UNKNOWN_KEY) {\n      throw new InvalidEncryptionKeyException(proto.getMessage());\n    } else if (proto.getStatus() == DataTransferEncryptorStatus.ERROR) {\n      throw new IOException(proto.getMessage());\n    } else {\n      byte[] response = proto.getPayload().toByteArray();\n      List<CipherOption> options = PBHelperClient.convertCipherOptionProtos(\n          proto.getCipherOptionList());\n      CipherOption option = null;\n      if (options != null && !options.isEmpty()) {\n        option = options.get(0);\n      }\n      return new SaslResponseWithNegotiatedCipherOption(response, option);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake": "  private IOStreamPair doSaslHandshake(InetAddress addr,\n      OutputStream underlyingOut, InputStream underlyingIn, String userName,\n      Map<String, String> saslProps,\n      CallbackHandler callbackHandler) throws IOException {\n\n    DataOutputStream out = new DataOutputStream(underlyingOut);\n    DataInputStream in = new DataInputStream(underlyingIn);\n\n    SaslParticipant sasl= SaslParticipant.createClientSaslParticipant(userName,\n        saslProps, callbackHandler);\n\n    out.writeInt(SASL_TRANSFER_MAGIC_NUMBER);\n    out.flush();\n\n    try {\n      // Start of handshake - \"initial response\" in SASL terminology.\n      sendSaslMessage(out, new byte[0]);\n\n      // step 1\n      byte[] remoteResponse = readSaslMessage(in);\n      byte[] localResponse = sasl.evaluateChallengeOrResponse(remoteResponse);\n      List<CipherOption> cipherOptions = null;\n      String cipherSuites = conf.get(\n          DFS_ENCRYPT_DATA_TRANSFER_CIPHER_SUITES_KEY);\n      if (requestedQopContainsPrivacy(saslProps)) {\n        // Negotiate cipher suites if configured.  Currently, the only supported\n        // cipher suite is AES/CTR/NoPadding, but the protocol allows multiple\n        // values for future expansion.\n        if (cipherSuites != null && !cipherSuites.isEmpty()) {\n          if (!cipherSuites.equals(CipherSuite.AES_CTR_NOPADDING.getName())) {\n            throw new IOException(String.format(\"Invalid cipher suite, %s=%s\",\n                DFS_ENCRYPT_DATA_TRANSFER_CIPHER_SUITES_KEY, cipherSuites));\n          }\n          CipherOption option = new CipherOption(CipherSuite.AES_CTR_NOPADDING);\n          cipherOptions = Lists.newArrayListWithCapacity(1);\n          cipherOptions.add(option);\n        }\n      }\n      sendSaslMessageAndNegotiationCipherOptions(out, localResponse,\n          cipherOptions);\n\n      // step 2 (client-side only)\n      SaslResponseWithNegotiatedCipherOption response =\n          readSaslMessageAndNegotiatedCipherOption(in);\n      localResponse = sasl.evaluateChallengeOrResponse(response.payload);\n      assert localResponse == null;\n\n      // SASL handshake is complete\n      checkSaslComplete(sasl, saslProps);\n\n      CipherOption cipherOption = null;\n      if (sasl.isNegotiatedQopPrivacy()) {\n        // Unwrap the negotiated cipher option\n        cipherOption = unwrap(response.cipherOption, sasl);\n        if (LOG.isDebugEnabled()) {\n          if (cipherOption == null) {\n            // No cipher suite is negotiated\n            if (cipherSuites != null && !cipherSuites.isEmpty()) {\n              // the client accepts some cipher suites, but the server does not.\n              LOG.debug(\"Client accepts cipher suites {}, \"\n                      + \"but server {} does not accept any of them\",\n                  cipherSuites, addr.toString());\n            }\n          } else {\n            LOG.debug(\"Client using cipher suite {} with server {}\",\n                cipherOption.getCipherSuite().getName(), addr.toString());\n          }\n        }\n      }\n\n      // If negotiated cipher option is not null, we will use it to create\n      // stream pair.\n      return cipherOption != null ? createStreamPair(\n          conf, cipherOption, underlyingOut, underlyingIn, false) :\n          sasl.createStreamPair(out, in);\n    } catch (IOException ioe) {\n      sendGenericSaslErrorMessage(out, ioe.getMessage());\n      throw ioe;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams": "  private IOStreamPair getEncryptedStreams(InetAddress addr,\n      OutputStream underlyingOut,\n      InputStream underlyingIn, DataEncryptionKey encryptionKey)\n      throws IOException {\n    Map<String, String> saslProps = createSaslPropertiesForEncryption(\n        encryptionKey.encryptionAlgorithm);\n\n    LOG.debug(\"Client using encryption algorithm {}\",\n        encryptionKey.encryptionAlgorithm);\n\n    String userName = getUserNameFromEncryptionKey(encryptionKey);\n    char[] password = encryptionKeyToPassword(encryptionKey.encryptionKey);\n    CallbackHandler callbackHandler = new SaslClientCallbackHandler(userName,\n        password);\n    return doSaslHandshake(addr, underlyingOut, underlyingIn, userName,\n        saslProps, callbackHandler);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getUserNameFromEncryptionKey": "  private static String getUserNameFromEncryptionKey(\n      DataEncryptionKey encryptionKey) {\n    return encryptionKey.keyId + NAME_DELIMITER +\n        encryptionKey.blockPoolId + NAME_DELIMITER +\n        new String(Base64.encodeBase64(encryptionKey.nonce, false),\n            Charsets.UTF_8);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send": "  private IOStreamPair send(InetAddress addr, OutputStream underlyingOut,\n      InputStream underlyingIn, DataEncryptionKey encryptionKey,\n      Token<BlockTokenIdentifier> accessToken, DatanodeID datanodeId)\n      throws IOException {\n    if (encryptionKey != null) {\n      LOG.debug(\"SASL client doing encrypted handshake for addr = {}, \"\n          + \"datanodeId = {}\", addr, datanodeId);\n      return getEncryptedStreams(addr, underlyingOut, underlyingIn,\n          encryptionKey);\n    } else if (!UserGroupInformation.isSecurityEnabled()) {\n      LOG.debug(\"SASL client skipping handshake in unsecured configuration for \"\n          + \"addr = {}, datanodeId = {}\", addr, datanodeId);\n      return null;\n    } else if (SecurityUtil.isPrivilegedPort(datanodeId.getXferPort())) {\n      LOG.debug(\n          \"SASL client skipping handshake in secured configuration with \"\n              + \"privileged port for addr = {}, datanodeId = {}\",\n          addr, datanodeId);\n      return null;\n    } else if (fallbackToSimpleAuth != null && fallbackToSimpleAuth.get()) {\n      LOG.debug(\n          \"SASL client skipping handshake in secured configuration with \"\n              + \"unsecured cluster for addr = {}, datanodeId = {}\",\n          addr, datanodeId);\n      return null;\n    } else if (saslPropsResolver != null) {\n      LOG.debug(\n          \"SASL client doing general handshake for addr = {}, datanodeId = {}\",\n          addr, datanodeId);\n      return getSaslStreams(addr, underlyingOut, underlyingIn, accessToken);\n    } else {\n      // It's a secured cluster using non-privileged ports, but no SASL.  The\n      // only way this can happen is if the DataNode has\n      // ignore.secure.ports.for.testing configured so this is a rare edge case.\n      LOG.debug(\"SASL client skipping handshake in secured configuration with \"\n              + \"no SASL protection configured for addr = {}, datanodeId = {}\",\n          addr, datanodeId);\n      return null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getSaslStreams": "  private IOStreamPair getSaslStreams(InetAddress addr,\n      OutputStream underlyingOut, InputStream underlyingIn,\n      Token<BlockTokenIdentifier> accessToken)\n      throws IOException {\n    Map<String, String> saslProps = saslPropsResolver.getClientProperties(addr);\n\n    String userName = buildUserName(accessToken);\n    char[] password = buildClientPassword(accessToken);\n    CallbackHandler callbackHandler = new SaslClientCallbackHandler(userName,\n        password);\n    return doSaslHandshake(addr, underlyingOut, underlyingIn, userName,\n        saslProps, callbackHandler);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend": "  private IOStreamPair checkTrustAndSend(InetAddress addr,\n      OutputStream underlyingOut, InputStream underlyingIn,\n      DataEncryptionKeyFactory encryptionKeyFactory,\n      Token<BlockTokenIdentifier> accessToken, DatanodeID datanodeId)\n      throws IOException {\n    if (!trustedChannelResolver.isTrusted() &&\n        !trustedChannelResolver.isTrusted(addr)) {\n      // The encryption key factory only returns a key if encryption is enabled.\n      DataEncryptionKey encryptionKey =\n          encryptionKeyFactory.newDataEncryptionKey();\n      return send(addr, underlyingOut, underlyingIn, encryptionKey, accessToken,\n          datanodeId);\n    } else {\n      LOG.debug(\n          \"SASL client skipping handshake on trusted connection for addr = {}, \"\n              + \"datanodeId = {}\", addr, datanodeId);\n      return null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend": "  public IOStreamPair socketSend(Socket socket, OutputStream underlyingOut,\n      InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory,\n      Token<BlockTokenIdentifier> accessToken, DatanodeID datanodeId)\n      throws IOException {\n    IOStreamPair ios = checkTrustAndSend(socket.getInetAddress(), underlyingOut,\n        underlyingIn, encryptionKeyFactory, accessToken, datanodeId);\n    return ios != null ? ios : new IOStreamPair(underlyingIn, underlyingOut);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant.evaluateChallengeOrResponse": "  public byte[] evaluateChallengeOrResponse(byte[] challengeOrResponse)\n      throws SaslException {\n    if (saslClient != null) {\n      return saslClient.evaluateChallenge(challengeOrResponse);\n    } else {\n      return saslServer.evaluateResponse(challengeOrResponse);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant.createClientSaslParticipant": "  public static SaslParticipant createClientSaslParticipant(String userName,\n      Map<String, String> saslProps, CallbackHandler callbackHandler)\n      throws SaslException {\n    return new SaslParticipant(Sasl.createSaslClient(new String[] { MECHANISM },\n      userName, PROTOCOL, SERVER_NAME, saslProps, callbackHandler));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant.createStreamPair": "  public IOStreamPair createStreamPair(DataOutputStream out,\n      DataInputStream in) {\n    if (saslClient != null) {\n      return new IOStreamPair(\n          new SaslInputStream(in, saslClient),\n          new SaslOutputStream(out, saslClient));\n    } else {\n      return new IOStreamPair(\n          new SaslInputStream(in, saslServer),\n          new SaslOutputStream(out, saslServer));\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant.isNegotiatedQopPrivacy": "  public boolean isNegotiatedQopPrivacy() {\n    String qop = getNegotiatedQop();\n    return qop != null && \"auth-conf\".equalsIgnoreCase(qop);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant.getNegotiatedQop": "  public String getNegotiatedQop() {\n    if (saslClient != null) {\n      return (String) saslClient.getNegotiatedProperty(Sasl.QOP);\n    } else {\n      return (String) saslServer.getNegotiatedProperty(Sasl.QOP);\n    }\n  }"
        },
        "bug_report": {
            "Title": "Uncaught InvalidEncryptionKeyException during pipeline recovery may abort downstream applications",
            "Description": "In normal operations, if SASL negotiation fails due to {{InvalidEncryptionKeyException}}, it is typically a benign exception, which is caught and retried :\n\n{code:title=SaslDataTransferServer#doSaslHandshake}\n  if (ioe instanceof SaslException &&\n      ioe.getCause() != null &&\n      ioe.getCause() instanceof InvalidEncryptionKeyException) {\n    // This could just be because the client is long-lived and hasn't gotten\n    // a new encryption key from the NN in a while. Upon receiving this\n    // error, the client will get a new encryption key from the NN and retry\n    // connecting to this DN.\n    sendInvalidKeySaslErrorMessage(out, ioe.getCause().getMessage());\n  } \n{code}\n\n{code:title=DFSOutputStream.DataStreamer#createBlockOutputStream}\nif (ie instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n{code}\n\nHowever, if the exception is thrown during pipeline recovery, the corresponding code does not handle it properly, and the exception is spilled out to downstream applications, such as SOLR, aborting its operation:\n\n{quote}\n2016-07-06 12:12:51,992 ERROR org.apache.solr.update.HdfsTransactionLog: Exception closing tlog.\norg.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=557709482) doesn't exist. Current key: 1350592619\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:1308)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1272)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1433)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1147)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:632)\n2016-07-06 12:12:51,997 ERROR org.apache.solr.update.CommitTracker: auto commit error...:org.apache.solr.common.SolrException: org.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=557709482) doesn't exist. Current key: 1350592619\n        at org.apache.solr.update.HdfsTransactionLog.close(HdfsTransactionLog.java:316)\n        at org.apache.solr.update.TransactionLog.decref(TransactionLog.java:505)\n        at org.apache.solr.update.UpdateLog.addOldLog(UpdateLog.java:380)\n        at org.apache.solr.update.UpdateLog.postCommit(UpdateLog.java:676)\n        at org.apache.solr.update.DirectUpdateHandler2.commit(DirectUpdateHandler2.java:623)\n        at org.apache.solr.update.CommitTracker.run(CommitTracker.java:216)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=557709482) doesn't exist. Current key: 1350592619\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:1308)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1272)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1433)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1147)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:632)\n{quote}\n\nThis exception should be contained within HDFS, caught and retried just like in {{createBlockOutputStream()}}"
        }
    },
    {
        "filename": "HDFS-2310.json",
        "creation_time": "2011-09-05T15:31:00.000+0000",
        "stack_trace": "```\njava.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.hdfs.server.protocol.JournalProtocol\n\tat org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)\n\njava.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.hdfs.server.protocol.JournalProtocol\n\tat org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.WritableRpcEngine.call": "    public Writable call(String protocolName, Writable param, long receivedTime) \n    throws IOException {\n      try {\n        Invocation call = (Invocation)param;\n        if (verbose) log(\"Call: \" + call);\n\n        // Verify rpc version\n        if (call.getRpcVersion() != writableRpcVersion) {\n          // Client is using a different version of WritableRpc\n          throw new IOException(\n              \"WritableRpc version mismatch, client side version=\"\n                  + call.getRpcVersion() + \", server side version=\"\n                  + writableRpcVersion);\n        }\n\n        long clientVersion = call.getProtocolVersion();\n        final String protoName;\n        ProtoClassProtoImpl protocolImpl;\n        if (call.declaringClassProtocolName.equals(VersionedProtocol.class.getName())) {\n          // VersionProtocol methods are often used by client to figure out\n          // which version of protocol to use.\n          //\n          // Versioned protocol methods should go the protocolName protocol\n          // rather than the declaring class of the method since the\n          // the declaring class is VersionedProtocol which is not \n          // registered directly.\n          // Send the call to the highest  protocol version\n          protocolImpl = \n              getHighestSupportedProtocol(protocolName).protocolTarget;\n        } else {\n          protoName = call.declaringClassProtocolName;\n\n          // Find the right impl for the protocol based on client version.\n          ProtoNameVer pv = \n              new ProtoNameVer(call.declaringClassProtocolName, clientVersion);\n          protocolImpl = protocolImplMap.get(pv);\n          if (protocolImpl == null) { // no match for Protocol AND Version\n             VerProtocolImpl highest = \n                 getHighestSupportedProtocol(protoName);\n            if (highest == null) {\n              throw new IOException(\"Unknown protocol: \" + protoName);\n            } else { // protocol supported but not the version that client wants\n              throw new RPC.VersionMismatch(protoName, clientVersion,\n                highest.version);\n            }\n          }\n        }\n        \n\n        // Invoke the protocol method\n\n        long startTime = System.currentTimeMillis();\n        Method method = \n            protocolImpl.protocolClass.getMethod(call.getMethodName(),\n            call.getParameterClasses());\n        method.setAccessible(true);\n        rpcDetailedMetrics.init(protocolImpl.protocolClass);\n        Object value = \n            method.invoke(protocolImpl.protocolImpl, call.getParameters());\n        int processingTime = (int) (System.currentTimeMillis() - startTime);\n        int qTime = (int) (startTime-receivedTime);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Served: \" + call.getMethodName() +\n                    \" queueTime= \" + qTime +\n                    \" procesingTime= \" + processingTime);\n        }\n        rpcMetrics.addRpcQueueTime(qTime);\n        rpcMetrics.addRpcProcessingTime(processingTime);\n        rpcDetailedMetrics.addProcessingTime(call.getMethodName(),\n                                             processingTime);\n        if (verbose) log(\"Return: \"+value);\n\n        return new ObjectWritable(method.getReturnType(), value);\n\n      } catch (InvocationTargetException e) {\n        Throwable target = e.getTargetException();\n        if (target instanceof IOException) {\n          throw (IOException)target;\n        } else {\n          IOException ioe = new IOException(target.toString());\n          ioe.setStackTrace(target.getStackTrace());\n          throw ioe;\n        }\n      } catch (Throwable e) {\n        if (!(e instanceof IOException)) {\n          LOG.error(\"Unexpected throwable object \", e);\n        }\n        IOException ioe = new IOException(e.toString());\n        ioe.setStackTrace(e.getStackTrace());\n        throw ioe;\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.WritableRpcEngine.getProtocolVersion": "    private long getProtocolVersion() {\n      return clientVersion;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.WritableRpcEngine.getRpcVersion": "    public long getRpcVersion() {\n      return rpcVersion;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.WritableRpcEngine.toString": "    public String toString() {\n      StringBuilder buffer = new StringBuilder();\n      buffer.append(methodName);\n      buffer.append(\"(\");\n      for (int i = 0; i < parameters.length; i++) {\n        if (i != 0)\n          buffer.append(\", \");\n        buffer.append(parameters[i]);\n      }\n      buffer.append(\")\");\n      buffer.append(\", rpc version=\"+rpcVersion);\n      buffer.append(\", client version=\"+clientVersion);\n      buffer.append(\", methodsFingerPrint=\"+clientMethodsHash);\n      return buffer.toString();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.WritableRpcEngine.getParameters": "    public Object[] getParameters() { return parameters; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.WritableRpcEngine.getHighestSupportedProtocol": "    private VerProtocolImpl getHighestSupportedProtocol(String protocolName) {    \n      Long highestVersion = 0L;\n      ProtoClassProtoImpl highest = null;\n      for (Map.Entry<ProtoNameVer, ProtoClassProtoImpl> pv : protocolImplMap\n          .entrySet()) {\n        if (pv.getKey().protocol.equals(protocolName)) {\n          if ((highest == null) || (pv.getKey().version > highestVersion)) {\n            highest = pv.getValue();\n            highestVersion = pv.getKey().version;\n          } \n        }\n      }\n      if (highest == null) {\n        return null;\n      }\n      return new VerProtocolImpl(highestVersion,  highest);   \n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.WritableRpcEngine.getParameterClasses": "    public Class<?>[] getParameterClasses() { return parameterClasses; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.WritableRpcEngine.invoke": "    public Object invoke(Object proxy, Method method, Object[] args)\n      throws Throwable {\n      long startTime = 0;\n      if (LOG.isDebugEnabled()) {\n        startTime = System.currentTimeMillis();\n      }\n\n      ObjectWritable value = (ObjectWritable)\n        client.call(new Invocation(method, args), remoteId);\n      if (LOG.isDebugEnabled()) {\n        long callTime = System.currentTimeMillis() - startTime;\n        LOG.debug(\"Call: \" + method.getName() + \" \" + callTime);\n      }\n      return value.get();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.WritableRpcEngine.getMethodName": "    public String getMethodName() { return methodName; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.WritableRpcEngine.log": "  private static void log(String value) {\n    if (value!= null && value.length() > 55)\n      value = value.substring(0, 55)+\"...\";\n    LOG.info(value);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.WritableRpcEngine.getClient": "  static Client getClient(Configuration conf) {\n    return CLIENTS.getClient(conf);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "                     public Writable run() throws Exception {\n                       // make the call\n                       return call(call.connection.protocolName, \n                                   call.param, call.timestamp);\n\n                     }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.toString": "    public String toString() {\n      return getHostAddress() + \":\" + remotePort; \n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(ByteArrayOutputStream response, \n                             Call call, Status status, \n                             Writable rv, String errorClass, String error) \n  throws IOException {\n    response.reset();\n    DataOutputStream out = new DataOutputStream(response);\n    out.writeInt(call.id);                // write call id\n    out.writeInt(status.state);           // write status\n\n    if (status == Status.SUCCESS) {\n      try {\n        rv.write(out);\n      } catch (Throwable t) {\n        LOG.warn(\"Error serializing call response for call \" + call, t);\n        // Call back to same function - this is OK since the\n        // buffer is reset at the top, and since status is changed\n        // to ERROR it won't infinite loop.\n        setupResponse(response, call, Status.ERROR,\n            null, t.getClass().getName(),\n            StringUtils.stringifyException(t));\n        return;\n      }\n    } else {\n      WritableUtils.writeString(out, errorClass);\n      WritableUtils.writeString(out, error);\n    }\n    if (call.connection.useWrap) {\n      wrapWithSasl(response, call);\n    }\n    call.setResponse(ByteBuffer.wrap(response.toByteArray()));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    private synchronized void close() throws IOException {\n      disposeSasl();\n      data = null;\n      dataLengthBuffer = null;\n      if (!channel.isOpen())\n        return;\n      try {socket.shutdownOutput();} catch(Exception e) {\n        LOG.warn(\"Ignoring socket shutdown exception\");\n      }\n      if (channel.isOpen()) {\n        try {channel.close();} catch(Exception e) {}\n      }\n      try {socket.close();} catch(Exception e) {}\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(String protocol,\n                               Writable param, long receiveTime)\n  throws IOException;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param connection incoming connection\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  public void authorize(UserGroupInformation user, \n                        ConnectionHeader connection,\n                        InetAddress addr\n                        ) throws AuthorizationException {\n    if (authorize) {\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(connection.getProtocol(), getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         connection.getProtocol());\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isValid() && key.isWritable()) {\n                  doAsyncWrite(key);\n              }\n            } catch (IOException e) {\n              LOG.info(getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = System.currentTimeMillis();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<Call> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<Call>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              Call call = (Call)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n          \n          for(Call call : calls) {\n            try {\n              doPurge(call, now);\n            } catch (IOException e) {\n              LOG.warn(\"Error in purging old calls \" + e);\n            }\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeConnection": "  private void closeConnection(Connection connection) {\n    synchronized (connectionList) {\n      if (connectionList.remove(connection))\n        numConnections--;\n    }\n    try {\n      connection.close();\n    } catch (IOException e) {\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          if (LOG.isDebugEnabled())\n            LOG.debug(getName() + \": disconnecting client \" + c.getHostAddress());\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRespond": "    void doRespond(Call call) throws IOException {\n      synchronized (call.connection.responseQueue) {\n        call.connection.responseQueue.addLast(call);\n        if (call.connection.responseQueue.size() == 1) {\n          processResponse(call.connection.responseQueue, true);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws IOException,  OutOfMemoryError {\n      Connection c = null;\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        \n        Reader reader = getReader();\n        try {\n          reader.startAdd();\n          SelectionKey readKey = reader.registerChannel(channel);\n          c = new Connection(readKey, channel, System.currentTimeMillis());\n          readKey.attach(c);\n          synchronized (connectionList) {\n            connectionList.add(numConnections, c);\n            numConnections++;\n          }\n          if (LOG.isDebugEnabled())\n            LOG.debug(\"Server connection from \" + c.toString() +\n                \"; # active connections: \" + numConnections +\n                \"; # queued calls: \" + callQueue.size());          \n        } finally {\n          reader.finishAdd(); \n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.cleanupConnections": "    private void cleanupConnections(boolean force) {\n      if (force || numConnections > thresholdIdleConnections) {\n        long currentTime = System.currentTimeMillis();\n        if (!force && (currentTime - lastCleanupRunTime) < cleanupInterval) {\n          return;\n        }\n        int start = 0;\n        int end = numConnections - 1;\n        if (!force) {\n          start = rand.nextInt() % numConnections;\n          end = rand.nextInt() % numConnections;\n          int temp;\n          if (end < start) {\n            temp = start;\n            start = end;\n            end = temp;\n          }\n        }\n        int i = start;\n        int numNuked = 0;\n        while (i <= end) {\n          Connection c;\n          synchronized (connectionList) {\n            try {\n              c = connectionList.get(i);\n            } catch (Exception e) {return;}\n          }\n          if (c.timedOut(currentTime)) {\n            if (LOG.isDebugEnabled())\n              LOG.debug(getName() + \": disconnecting client \" + c.getHostAddress());\n            closeConnection(c);\n            numNuked++;\n            end--;\n            c = null;\n            if (!force && numNuked == maxConnectionsToNuke) break;\n          }\n          else i++;\n        }\n        lastCleanupRunTime = System.currentTimeMillis();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(pae,\"Unknown exception in doAs\");\n      }\n    }\n  }"
        },
        "bug_report": {
            "Title": "TestBackupNode fails since HADOOP-7524 went in.",
            "Description": "Logs give the following error. This happens because the JournalProtocol is never registered with the server.\n\n2011-09-05 10:44:36,811 INFO  ipc.Server (Server.java:run(1497)) - IPC Server handler 0 on 60758, call: startLogSegment(NamenodeRegistration(localhost:60754, role=NameNode), 3), rpc version=2, client version=1, methodsFingerPrint=-852377201 from 127.0.0.1:60760, error: \njava.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.hdfs.server.protocol.JournalProtocol\n\tat org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)\n2011-09-05 10:44:36,813 ERROR namenode.FSEditLog (FSEditLog.java:mapJournalsAndReportErrors(1048)) - Error starting log segment 3 (journal JournalAndStream(mgr=org.apache.hadoop.hdfs.server.namenode.BackupJournalManager@2876b359, stream=null))\njava.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.hdfs.server.protocol.JournalProtocol\n\tat org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)\n"
        }
    },
    {
        "filename": "HDFS-3385.json",
        "creation_time": "2012-05-08T12:45:31.000+0000",
        "stack_trace": "```\nException in thread \"main\" java.lang.ClassCastException: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo cannot be cast to org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:1787)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1584)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:1824)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:425)\n        ...\n\tat org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1150)\n\tat org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1189)\n\tat org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1177)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:221)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:1)\n\tat org.apache.hadoop.fs.FileSystem.append(FileSystem.java:981)\n\tat org.apache.hadoop.hdfs.server.datanode.DeleteMe.main(DeleteMe.java:26)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal": "  private void recoverLeaseInternal(INode fileInode, \n      String src, String holder, String clientMachine, boolean force)\n      throws IOException {\n    assert hasWriteLock();\n    if (fileInode != null && fileInode.isUnderConstruction()) {\n      INodeFileUnderConstruction pendingFile = (INodeFileUnderConstruction) fileInode;\n      //\n      // If the file is under construction , then it must be in our\n      // leases. Find the appropriate lease record.\n      //\n      Lease lease = leaseManager.getLease(holder);\n      //\n      // We found the lease for this file. And surprisingly the original\n      // holder is trying to recreate this file. This should never occur.\n      //\n      if (!force && lease != null) {\n        Lease leaseFile = leaseManager.getLeaseByPath(src);\n        if ((leaseFile != null && leaseFile.equals(lease)) ||\n            lease.getHolder().equals(holder)) { \n          throw new AlreadyBeingCreatedException(\n            \"failed to create file \" + src + \" for \" + holder +\n            \" on client \" + clientMachine + \n            \" because current leaseholder is trying to recreate file.\");\n        }\n      }\n      //\n      // Find the original holder.\n      //\n      lease = leaseManager.getLease(pendingFile.getClientName());\n      if (lease == null) {\n        throw new AlreadyBeingCreatedException(\n          \"failed to create file \" + src + \" for \" + holder +\n          \" on client \" + clientMachine + \n          \" because pendingCreates is non-null but no leases found.\");\n      }\n      if (force) {\n        // close now: no need to wait for soft lease expiration and \n        // close only the file src\n        LOG.info(\"recoverLease: recover lease \" + lease + \", src=\" + src +\n          \" from client \" + pendingFile.getClientName());\n        internalReleaseLease(lease, src, holder);\n      } else {\n        assert lease.getHolder().equals(pendingFile.getClientName()) :\n          \"Current lease holder \" + lease.getHolder() +\n          \" does not match file creator \" + pendingFile.getClientName();\n        //\n        // If the original holder has not renewed in the last SOFTLIMIT \n        // period, then start lease recovery.\n        //\n        if (lease.expiredSoftLimit()) {\n          LOG.info(\"startFile: recover lease \" + lease + \", src=\" + src +\n              \" from client \" + pendingFile.getClientName());\n          boolean isClosed = internalReleaseLease(lease, src, null);\n          if(!isClosed)\n            throw new RecoveryInProgressException(\n                \"Failed to close file \" + src +\n                \". Lease recovery is in progress. Try again later.\");\n        } else {\n          BlockInfoUnderConstruction lastBlock=pendingFile.getLastBlock();\n          if(lastBlock != null && lastBlock.getBlockUCState() ==\n            BlockUCState.UNDER_RECOVERY) {\n            throw new RecoveryInProgressException(\n              \"Recovery in progress, file [\" + src + \"], \" +\n              \"lease owner [\" + lease.getHolder() + \"]\");\n            } else {\n              throw new AlreadyBeingCreatedException(\n                \"Failed to create file [\" + src + \"] for [\" + holder +\n                \"] on client [\" + clientMachine +\n                \"], because this file is already being created by [\" +\n                pendingFile.getClientName() + \"] on [\" +\n                pendingFile.getClientMachine() + \"]\");\n            }\n         }\n      }\n    }\n\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.internalReleaseLease": "  boolean internalReleaseLease(Lease lease, String src, \n      String recoveryLeaseHolder) throws AlreadyBeingCreatedException, \n      IOException, UnresolvedLinkException {\n    LOG.info(\"Recovering lease=\" + lease + \", src=\" + src);\n    assert !isInSafeMode();\n    assert hasWriteLock();\n    INodeFile iFile = dir.getFileINode(src);\n    if (iFile == null) {\n      final String message = \"DIR* NameSystem.internalReleaseLease: \"\n        + \"attempt to release a create lock on \"\n        + src + \" file does not exist.\";\n      NameNode.stateChangeLog.warn(message);\n      throw new IOException(message);\n    }\n    if (!iFile.isUnderConstruction()) {\n      final String message = \"DIR* NameSystem.internalReleaseLease: \"\n        + \"attempt to release a create lock on \"\n        + src + \" but file is already closed.\";\n      NameNode.stateChangeLog.warn(message);\n      throw new IOException(message);\n    }\n\n    INodeFileUnderConstruction pendingFile = (INodeFileUnderConstruction) iFile;\n    int nrBlocks = pendingFile.numBlocks();\n    BlockInfo[] blocks = pendingFile.getBlocks();\n\n    int nrCompleteBlocks;\n    BlockInfo curBlock = null;\n    for(nrCompleteBlocks = 0; nrCompleteBlocks < nrBlocks; nrCompleteBlocks++) {\n      curBlock = blocks[nrCompleteBlocks];\n      if(!curBlock.isComplete())\n        break;\n      assert blockManager.checkMinReplication(curBlock) :\n              \"A COMPLETE block is not minimally replicated in \" + src;\n    }\n\n    // If there are no incomplete blocks associated with this file,\n    // then reap lease immediately and close the file.\n    if(nrCompleteBlocks == nrBlocks) {\n      finalizeINodeFileUnderConstruction(src, pendingFile);\n      NameNode.stateChangeLog.warn(\"BLOCK*\"\n        + \" internalReleaseLease: All existing blocks are COMPLETE,\"\n        + \" lease removed, file closed.\");\n      return true;  // closed!\n    }\n\n    // Only the last and the penultimate blocks may be in non COMPLETE state.\n    // If the penultimate block is not COMPLETE, then it must be COMMITTED.\n    if(nrCompleteBlocks < nrBlocks - 2 ||\n       nrCompleteBlocks == nrBlocks - 2 &&\n         curBlock != null &&\n         curBlock.getBlockUCState() != BlockUCState.COMMITTED) {\n      final String message = \"DIR* NameSystem.internalReleaseLease: \"\n        + \"attempt to release a create lock on \"\n        + src + \" but file is already closed.\";\n      NameNode.stateChangeLog.warn(message);\n      throw new IOException(message);\n    }\n\n    // no we know that the last block is not COMPLETE, and\n    // that the penultimate block if exists is either COMPLETE or COMMITTED\n    BlockInfoUnderConstruction lastBlock = pendingFile.getLastBlock();\n    BlockUCState lastBlockState = lastBlock.getBlockUCState();\n    BlockInfo penultimateBlock = pendingFile.getPenultimateBlock();\n    boolean penultimateBlockMinReplication;\n    BlockUCState penultimateBlockState;\n    if (penultimateBlock == null) {\n      penultimateBlockState = BlockUCState.COMPLETE;\n      // If penultimate block doesn't exist then its minReplication is met\n      penultimateBlockMinReplication = true;\n    } else {\n      penultimateBlockState = BlockUCState.COMMITTED;\n      penultimateBlockMinReplication = \n        blockManager.checkMinReplication(penultimateBlock);\n    }\n    assert penultimateBlockState == BlockUCState.COMPLETE ||\n           penultimateBlockState == BlockUCState.COMMITTED :\n           \"Unexpected state of penultimate block in \" + src;\n\n    switch(lastBlockState) {\n    case COMPLETE:\n      assert false : \"Already checked that the last block is incomplete\";\n      break;\n    case COMMITTED:\n      // Close file if committed blocks are minimally replicated\n      if(penultimateBlockMinReplication &&\n          blockManager.checkMinReplication(lastBlock)) {\n        finalizeINodeFileUnderConstruction(src, pendingFile);\n        NameNode.stateChangeLog.warn(\"BLOCK*\"\n          + \" internalReleaseLease: Committed blocks are minimally replicated,\"\n          + \" lease removed, file closed.\");\n        return true;  // closed!\n      }\n      // Cannot close file right now, since some blocks \n      // are not yet minimally replicated.\n      // This may potentially cause infinite loop in lease recovery\n      // if there are no valid replicas on data-nodes.\n      String message = \"DIR* NameSystem.internalReleaseLease: \" +\n          \"Failed to release lease for file \" + src +\n          \". Committed blocks are waiting to be minimally replicated.\" +\n          \" Try again later.\";\n      NameNode.stateChangeLog.warn(message);\n      throw new AlreadyBeingCreatedException(message);\n    case UNDER_CONSTRUCTION:\n    case UNDER_RECOVERY:\n      // setup the last block locations from the blockManager if not known\n      if(lastBlock.getNumExpectedLocations() == 0)\n        lastBlock.setExpectedLocations(blockManager.getNodes(lastBlock));\n      // start recovery of the last block for this file\n      long blockRecoveryId = nextGenerationStamp();\n      lease = reassignLease(lease, src, recoveryLeaseHolder, pendingFile);\n      lastBlock.initializeBlockRecovery(blockRecoveryId);\n      leaseManager.renewLease(lease);\n      // Cannot close file right now, since the last block requires recovery.\n      // This may potentially cause infinite loop in lease recovery\n      // if there are no valid replicas on data-nodes.\n      NameNode.stateChangeLog.warn(\n                \"DIR* NameSystem.internalReleaseLease: \" +\n                \"File \" + src + \" has not been closed.\" +\n               \" Lease recovery is in progress. \" +\n                \"RecoveryId = \" + blockRecoveryId + \" for block \" + lastBlock);\n      break;\n    }\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.hasWriteLock": "  public boolean hasWriteLock() {\n    return this.fsLock.isWriteLockedByCurrentThread();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal": "  private LocatedBlock startFileInternal(String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet<CreateFlag> flag, boolean createParent, short replication,\n      long blockSize) throws SafeModeException, FileAlreadyExistsException,\n      AccessControlException, UnresolvedLinkException, FileNotFoundException,\n      ParentNotDirectoryException, IOException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.startFile: src=\" + src\n          + \", holder=\" + holder\n          + \", clientMachine=\" + clientMachine\n          + \", createParent=\" + createParent\n          + \", replication=\" + replication\n          + \", createFlag=\" + flag.toString());\n    }\n    if (isInSafeMode()) {\n      throw new SafeModeException(\"Cannot create file\" + src, safeMode);\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n\n    // Verify that the destination does not exist as a directory already.\n    boolean pathExists = dir.exists(src);\n    if (pathExists && dir.isDir(src)) {\n      throw new FileAlreadyExistsException(\"Cannot create file \" + src\n          + \"; already exists as a directory.\");\n    }\n\n    boolean overwrite = flag.contains(CreateFlag.OVERWRITE);\n    boolean append = flag.contains(CreateFlag.APPEND);\n    if (isPermissionEnabled) {\n      if (append || (overwrite && pathExists)) {\n        checkPathAccess(src, FsAction.WRITE);\n      } else {\n        checkAncestorAccess(src, FsAction.WRITE);\n      }\n    }\n\n    if (!createParent) {\n      verifyParentDir(src);\n    }\n\n    try {\n      INode myFile = dir.getFileINode(src);\n      recoverLeaseInternal(myFile, src, holder, clientMachine, false);\n\n      try {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      } catch(IOException e) {\n        throw new IOException(\"failed to create \"+e.getMessage());\n      }\n      boolean create = flag.contains(CreateFlag.CREATE);\n      if (myFile == null) {\n        if (!create) {\n          throw new FileNotFoundException(\"failed to overwrite or append to non-existent file \"\n            + src + \" on client \" + clientMachine);\n        }\n      } else {\n        // File exists - must be one of append or overwrite\n        if (overwrite) {\n          delete(src, true);\n        } else if (!append) {\n          throw new FileAlreadyExistsException(\"failed to create file \" + src\n              + \" on client \" + clientMachine\n              + \" because the file exists\");\n        }\n      }\n\n      final DatanodeDescriptor clientNode = \n          blockManager.getDatanodeManager().getDatanodeByHost(clientMachine);\n\n      if (append && myFile != null) {\n        return prepareFileForWrite(\n            src, myFile, holder, clientMachine, clientNode, true);\n      } else {\n       // Now we can add the name to the filesystem. This file has no\n       // blocks associated with it.\n       //\n       checkFsObjectLimit();\n\n        // increment global generation stamp\n        long genstamp = nextGenerationStamp();\n        INodeFileUnderConstruction newNode = dir.addFile(src, permissions,\n            replication, blockSize, holder, clientMachine, clientNode, genstamp);\n        if (newNode == null) {\n          throw new IOException(\"DIR* NameSystem.startFile: \" +\n                                \"Unable to add file to namespace.\");\n        }\n        leaseManager.addLease(newNode.getClientName(), src);\n\n        // record file record in log, record new generation stamp\n        getEditLog().logOpenFile(src, newNode);\n        if (NameNode.stateChangeLog.isDebugEnabled()) {\n          NameNode.stateChangeLog.debug(\"DIR* NameSystem.startFile: \"\n                                     +\"add \"+src+\" to namespace for \"+holder);\n        }\n      }\n    } catch (IOException ie) {\n      NameNode.stateChangeLog.warn(\"DIR* NameSystem.startFile: \"\n                                   +ie.getMessage());\n      throw ie;\n    }\n    return null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete": "    boolean delete(String src, boolean recursive)\n        throws AccessControlException, SafeModeException,\n               UnresolvedLinkException, IOException {\n      if (NameNode.stateChangeLog.isDebugEnabled()) {\n        NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n      }\n      boolean status = deleteInternal(src, recursive, true);\n      if (status && auditLog.isInfoEnabled() && isExternalInvocation()) {\n        logAuditEvent(UserGroupInformation.getCurrentUser(),\n                      Server.getRemoteIp(),\n                      \"delete\", src, null, null);\n      }\n      return status;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.prepareFileForWrite": "  public LocatedBlock prepareFileForWrite(String src, INode file,\n      String leaseHolder, String clientMachine, DatanodeDescriptor clientNode,\n      boolean writeToEditLog)\n      throws UnresolvedLinkException, IOException {\n    INodeFile node = (INodeFile) file;\n    INodeFileUnderConstruction cons = new INodeFileUnderConstruction(\n                                    node.getLocalNameBytes(),\n                                    node.getReplication(),\n                                    node.getModificationTime(),\n                                    node.getPreferredBlockSize(),\n                                    node.getBlocks(),\n                                    node.getPermissionStatus(),\n                                    leaseHolder,\n                                    clientMachine,\n                                    clientNode);\n    dir.replaceNode(src, node, cons);\n    leaseManager.addLease(cons.getClientName(), src);\n    \n    LocatedBlock ret = blockManager.convertLastBlockToUnderConstruction(cons);\n    if (writeToEditLog) {\n      getEditLog().logOpenFile(src, cons);\n    }\n    return ret;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStamp": "  private long nextGenerationStamp() throws SafeModeException {\n    assert hasWriteLock();\n    if (isInSafeMode()) {\n      throw new SafeModeException(\n          \"Cannot get next generation stamp\", safeMode);\n    }\n    long gs = generationStamp.nextStamp();\n    getEditLog().logGenerationStamp(gs);\n    // NB: callers sync the log\n    return gs;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEditLog": "  public FSEditLog getEditLog() {\n    return getFSImage().getEditLog();\n  }    ",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.isInSafeMode": "  public boolean isInSafeMode() {\n    // safeMode is volatile, and may be set to null at any time\n    SafeModeInfo safeMode = this.safeMode;\n    if (safeMode == null)\n      return false;\n    return safeMode.isOn();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.verifyParentDir": "  private void verifyParentDir(String src) throws FileNotFoundException,\n      ParentNotDirectoryException, UnresolvedLinkException {\n    assert hasReadOrWriteLock();\n    Path parent = new Path(src).getParent();\n    if (parent != null) {\n      INode[] pathINodes = dir.getExistingPathINodes(parent.toString());\n      INode parentNode = pathINodes[pathINodes.length - 1];\n      if (parentNode == null) {\n        throw new FileNotFoundException(\"Parent directory doesn't exist: \"\n            + parent.toString());\n      } else if (!parentNode.isDirectory() && !parentNode.isLink()) {\n        throw new ParentNotDirectoryException(\"Parent path is not a directory: \"\n            + parent.toString());\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess": "  private FSPermissionChecker checkAncestorAccess(String path, FsAction access\n      ) throws AccessControlException, UnresolvedLinkException {\n    return checkPermission(path, false, access, null, null, null);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPathAccess": "  private FSPermissionChecker checkPathAccess(String path, FsAction access\n      ) throws AccessControlException, UnresolvedLinkException {\n    return checkPermission(path, false, null, null, access, null);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.toString": "    public String toString() {\n      return block.getBlockName() + \"\\t\" + path;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkFsObjectLimit": "  void checkFsObjectLimit() throws IOException {\n    if (maxFsObjects != 0 &&\n        maxFsObjects <= dir.totalInodes() + getBlocksTotal()) {\n      throw new IOException(\"Exceeded the configured number of objects \" +\n                             maxFsObjects + \" in the filesystem.\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile": "  LocatedBlock appendFile(String src, String holder, String clientMachine)\n      throws AccessControlException, SafeModeException,\n      FileAlreadyExistsException, FileNotFoundException,\n      ParentNotDirectoryException, IOException {\n    if (!supportAppends) {\n      throw new UnsupportedOperationException(\n          \"Append is not enabled on this NameNode. Use the \" +\n          DFS_SUPPORT_APPEND_KEY + \" configuration option to enable it.\");\n    }\n    LocatedBlock lb = null;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n\n      lb = startFileInternal(src, null, holder, clientMachine, \n                        EnumSet.of(CreateFlag.APPEND), \n                        false, blockManager.maxReplication, 0);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (lb != null) {\n      if (NameNode.stateChangeLog.isDebugEnabled()) {\n        NameNode.stateChangeLog.debug(\"DIR* NameSystem.appendFile: file \"\n            +src+\" for \"+holder+\" at \"+clientMachine\n            +\" block \" + lb.getBlock()\n            +\" block size \" + lb.getBlock().getNumBytes());\n      }\n    }\n    if (auditLog.isInfoEnabled() && isExternalInvocation()) {\n      logAuditEvent(UserGroupInformation.getCurrentUser(),\n                    Server.getRemoteIp(),\n                    \"append\", src, null, null);\n    }\n    return lb;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock": "  public void writeUnlock() {\n    this.fsLock.writeLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock": "  public void writeLock() {\n    this.fsLock.writeLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.isExternalInvocation": "  private boolean isExternalInvocation() {\n    return Server.isRpcInvocation();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation": "  public void checkOperation(OperationCategory op) throws StandbyException {\n    if (haContext != null) {\n      // null in some unit tests\n      haContext.checkOperation(op);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logAuditEvent": "  private static final void logAuditEvent(UserGroupInformation ugi,\n      InetAddress addr, String cmd, String src, String dst,\n      HdfsFileStatus stat) {\n    final StringBuilder sb = auditBuffer.get();\n    sb.setLength(0);\n    sb.append(\"ugi=\").append(ugi).append(\"\\t\");\n    sb.append(\"ip=\").append(addr).append(\"\\t\");\n    sb.append(\"cmd=\").append(cmd).append(\"\\t\");\n    sb.append(\"src=\").append(src).append(\"\\t\");\n    sb.append(\"dst=\").append(dst).append(\"\\t\");\n    if (null == stat) {\n      sb.append(\"perm=null\");\n    } else {\n      sb.append(\"perm=\");\n      sb.append(stat.getOwner()).append(\":\");\n      sb.append(stat.getGroup()).append(\":\");\n      sb.append(stat.getPermission());\n    }\n    auditLog.info(sb);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append": "  public LocatedBlock append(String src, String clientName) \n      throws IOException {\n    String clientMachine = getClientMachine();\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.append: file \"\n          +src+\" for \"+clientName+\" at \"+clientMachine);\n    }\n    LocatedBlock info = namesystem.appendFile(src, clientName, clientMachine);\n    metrics.incrFilesAppended();\n    return info;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getClientMachine": "  private static String getClientMachine() {\n    String clientMachine = NamenodeWebHdfsMethods.getRemoteAddress();\n    if (clientMachine == null) { //not a web client\n      clientMachine = Server.getRemoteAddress();\n    }\n    if (clientMachine == null) { //not a RPC client\n      clientMachine = \"\";\n    }\n    return clientMachine;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.callAppend": "  private DFSOutputStream callAppend(HdfsFileStatus stat, String src,\n      int buffersize, Progressable progress) throws IOException {\n    LocatedBlock lastBlock = null;\n    try {\n      lastBlock = namenode.append(src, clientName);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class);\n    }\n    return DFSOutputStream.newStreamForAppend(this, src, buffersize, progress,\n        lastBlock, stat, dfsClientConf.createChecksum());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.createChecksum": "    private DataChecksum createChecksum() {\n      return DataChecksum.newDataChecksum(\n          checksumType, bytesPerChecksum);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.append": "  private DFSOutputStream append(String src, int buffersize, Progressable progress) \n      throws IOException {\n    checkOpen();\n    HdfsFileStatus stat = getFileInfo(src);\n    if (stat == null) { // No file found\n      throw new FileNotFoundException(\"failed to append to non-existent file \"\n          + src + \" on client \" + clientName);\n    }\n    final DFSOutputStream result = callAppend(stat, src, buffersize, progress);\n    leaserenewer.put(src, result, this);\n    return result;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.append": "  public HdfsDataOutputStream append(Path f, int bufferSize,\n      Progressable progress) throws IOException {\n    statistics.incrementWriteOps(1);\n    return dfs.append(getPathName(f), bufferSize, progress, statistics);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getPathName": "  private String getPathName(Path file) {\n    checkPath(file);\n    String result = makeAbsolute(file).toUri().getPath();\n    if (!DFSUtil.isValidName(result)) {\n      throw new IllegalArgumentException(\"Pathname \" + result + \" from \" +\n                                         file+\" is not a valid DFS filename.\");\n    }\n    return result;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.append": "  public abstract FSDataOutputStream append(Path f, int bufferSize,\n      Progressable progress) throws IOException;\n\n /**\n   * Get replication.\n   * \n   * @deprecated Use getFileStatus() instead\n   * @param src file name\n   * @return file replication\n   * @throws IOException\n   */ \n  @Deprecated\n  public short getReplication(Path src) throws IOException {\n    return getFileStatus(src).getReplication();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.leaseManager.getLease": "  Lease getLease(String holder) {\n    return leases.get(holder);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.getClientMachine": "  String getClientMachine() {\n    return clientMachine;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.leaseManager.getLeaseByPath": "  public Lease getLeaseByPath(String src) {return sortedLeasesByPath.get(src);}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.getClientName": "  String getClientName() {\n    return clientName;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.leaseManager.addLease": "  synchronized Lease addLease(String holder, String src) {\n    Lease lease = getLease(holder);\n    if (lease == null) {\n      lease = new Lease(holder);\n      leases.put(holder, lease);\n      sortedLeases.add(lease);\n    } else {\n      renewLease(lease);\n    }\n    sortedLeasesByPath.put(src, lease);\n    lease.paths.add(src);\n    return lease;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.leaseManager.renewLease": "  synchronized void renewLease(Lease lease) {\n    if (lease != null) {\n      sortedLeases.remove(lease);\n      lease.renew();\n      sortedLeases.add(lease);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.newStreamForAppend": "  static DFSOutputStream newStreamForAppend(DFSClient dfsClient, String src,\n      int buffersize, Progressable progress, LocatedBlock lastBlock,\n      HdfsFileStatus stat, DataChecksum checksum) throws IOException {\n    final DFSOutputStream out = new DFSOutputStream(dfsClient, src, buffersize,\n        progress, lastBlock, stat, checksum);\n    out.streamer.start();\n    return out;\n  }"
        },
        "bug_report": {
            "Title": "ClassCastException when trying to append a file",
            "Description": "When I try to append a file I got \n\n{noformat}\n2012-05-08 18:13:40,506 WARN  util.KerberosName (KerberosName.java:<clinit>(87)) - Kerberos krb5 configuration not found, setting default realm to empty\nException in thread \"main\" java.lang.ClassCastException: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo cannot be cast to org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:1787)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1584)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:1824)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:425)\n        ...\n\tat org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1150)\n\tat org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1189)\n\tat org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1177)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:221)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:1)\n\tat org.apache.hadoop.fs.FileSystem.append(FileSystem.java:981)\n\tat org.apache.hadoop.hdfs.server.datanode.DeleteMe.main(DeleteMe.java:26)\n{noformat}"
        }
    },
    {
        "filename": "HDFS-4006.json",
        "creation_time": "2012-10-04T22:05:14.000+0000",
        "stack_trace": "```\norg.apache.hadoop.util.ExitUtil$ExitException: Fatal exception with message null\njava.lang.NullPointerException\nat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:480)\nat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:331)\nat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2.run(SecondaryNameNode.java:298)\nat org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:452)\nat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:294)\nat java.lang.Thread.run(Thread.java:662)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint": "  boolean doCheckpoint() throws IOException {\n    checkpointImage.ensureCurrentDirExists();\n    NNStorage dstStorage = checkpointImage.getStorage();\n    \n    // Tell the namenode to start logging transactions in a new edit file\n    // Returns a token that would be used to upload the merged image.\n    CheckpointSignature sig = namenode.rollEditLog();\n    \n    if ((checkpointImage.getNamespaceID() == 0) ||\n        (sig.isSameCluster(checkpointImage) &&\n         !sig.storageVersionMatches(checkpointImage.getStorage()))) {\n      // if we're a fresh 2NN, or if we're on the same cluster and our storage\n      // needs an upgrade, just take the storage info from the server.\n      dstStorage.setStorageInfo(sig);\n      dstStorage.setClusterID(sig.getClusterID());\n      dstStorage.setBlockPoolID(sig.getBlockpoolID());\n    }\n    sig.validateStorageInfo(checkpointImage);\n\n    // error simulation code for junit test\n    CheckpointFaultInjector.getInstance().afterSecondaryCallsRollEditLog();\n\n    RemoteEditLogManifest manifest =\n      namenode.getEditLogManifest(sig.mostRecentCheckpointTxId + 1);\n\n    boolean loadImage = downloadCheckpointFiles(\n        fsName, checkpointImage, sig, manifest);   // Fetch fsimage and edits\n    doMerge(sig, manifest, loadImage, checkpointImage, namesystem);\n    \n    //\n    // Upload the new image into the NameNode. Then tell the Namenode\n    // to make this new uploaded image as the most current image.\n    //\n    long txid = checkpointImage.getLastAppliedTxId();\n    TransferFsImage.uploadImageFromStorage(fsName, getImageListenAddress(),\n        dstStorage, txid);\n\n    // error simulation code for junit test\n    CheckpointFaultInjector.getInstance().afterSecondaryUploadsNewImage();\n\n    LOG.warn(\"Checkpoint done. New Image Size: \" \n             + dstStorage.getFsImageName(txid).length());\n    \n    return loadImage;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doMerge": "  static void doMerge(\n      CheckpointSignature sig, RemoteEditLogManifest manifest,\n      boolean loadImage, FSImage dstImage, FSNamesystem dstNamesystem)\n      throws IOException {   \n    NNStorage dstStorage = dstImage.getStorage();\n    \n    dstStorage.setStorageInfo(sig);\n    if (loadImage) {\n      File file = dstStorage.findImageFile(sig.mostRecentCheckpointTxId);\n      if (file == null) {\n        throw new IOException(\"Couldn't find image file at txid \" + \n            sig.mostRecentCheckpointTxId + \" even though it should have \" +\n            \"just been downloaded\");\n      }\n      dstImage.reloadFromImageFile(file, dstNamesystem);\n    }\n    \n    Checkpointer.rollForwardByApplyingLogs(manifest, dstImage, dstNamesystem);\n    // The following has the side effect of purging old fsimages/edit logs.\n    dstImage.saveFSImageInAllDirs(dstNamesystem, dstImage.getLastAppliedTxId());\n    dstStorage.writeAll();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.ensureCurrentDirExists": "    void ensureCurrentDirExists() throws IOException {\n      for (Iterator<StorageDirectory> it\n             = storage.dirIterator(); it.hasNext();) {\n        StorageDirectory sd = it.next();\n        File curDir = sd.getCurrentDir();\n        if (!curDir.exists() && !curDir.mkdirs()) {\n          throw new IOException(\"Could not create directory \" + curDir);\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.getImageListenAddress": "  private InetSocketAddress getImageListenAddress() {\n    return new InetSocketAddress(infoBindAddress, infoPort);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.downloadCheckpointFiles": "  static boolean downloadCheckpointFiles(\n      final String nnHostPort,\n      final FSImage dstImage,\n      final CheckpointSignature sig,\n      final RemoteEditLogManifest manifest\n  ) throws IOException {\n    \n    // Sanity check manifest - these could happen if, eg, someone on the\n    // NN side accidentally rmed the storage directories\n    if (manifest.getLogs().isEmpty()) {\n      throw new IOException(\"Found no edit logs to download on NN since txid \" \n          + sig.mostRecentCheckpointTxId);\n    }\n    \n    long expectedTxId = sig.mostRecentCheckpointTxId + 1;\n    if (manifest.getLogs().get(0).getStartTxId() != expectedTxId) {\n      throw new IOException(\"Bad edit log manifest (expected txid = \" +\n          expectedTxId + \": \" + manifest);\n    }\n\n    try {\n        Boolean b = UserGroupInformation.getCurrentUser().doAs(\n            new PrivilegedExceptionAction<Boolean>() {\n  \n          @Override\n          public Boolean run() throws Exception {\n            dstImage.getStorage().cTime = sig.cTime;\n\n            // get fsimage\n            boolean downloadImage = true;\n            if (sig.mostRecentCheckpointTxId ==\n                dstImage.getStorage().getMostRecentCheckpointTxId()) {\n              downloadImage = false;\n              LOG.info(\"Image has not changed. Will not download image.\");\n            } else {\n              LOG.info(\"Image has changed. Downloading updated image from NN.\");\n              MD5Hash downloadedHash = TransferFsImage.downloadImageToStorage(\n                  nnHostPort, sig.mostRecentCheckpointTxId, dstImage.getStorage(), true);\n              dstImage.saveDigestAndRenameCheckpointImage(\n                  sig.mostRecentCheckpointTxId, downloadedHash);\n            }\n        \n            // get edits file\n            for (RemoteEditLog log : manifest.getLogs()) {\n              TransferFsImage.downloadEditsToStorage(\n                  nnHostPort, log, dstImage.getStorage());\n            }\n        \n            return Boolean.valueOf(downloadImage);\n          }\n        });\n        return b.booleanValue();\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork": "  public void doWork() {\n    //\n    // Poll the Namenode (once every checkpointCheckPeriod seconds) to find the\n    // number of transactions in the edit log that haven't yet been checkpointed.\n    //\n    long period = checkpointConf.getCheckPeriod();\n\n    while (shouldRun) {\n      try {\n        Thread.sleep(1000 * period);\n      } catch (InterruptedException ie) {\n        // do nothing\n      }\n      if (!shouldRun) {\n        break;\n      }\n      try {\n        // We may have lost our ticket since last checkpoint, log in again, just in case\n        if(UserGroupInformation.isSecurityEnabled())\n          UserGroupInformation.getCurrentUser().reloginFromKeytab();\n        \n        long now = Time.now();\n\n        if (shouldCheckpointBasedOnCount() ||\n            now >= lastCheckpointTime + 1000 * checkpointConf.getPeriod()) {\n          doCheckpoint();\n          lastCheckpointTime = now;\n        }\n      } catch (IOException e) {\n        LOG.error(\"Exception in doCheckpoint\", e);\n        e.printStackTrace();\n      } catch (Throwable e) {\n        LOG.fatal(\"Throwable Exception in doCheckpoint\", e);\n        e.printStackTrace();\n        terminate(1, e);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount": "  boolean shouldCheckpointBasedOnCount() throws IOException {\n    return countUncheckpointedTxns() >= checkpointConf.getTxnCount();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run": "          public Boolean run() throws Exception {\n            dstImage.getStorage().cTime = sig.cTime;\n\n            // get fsimage\n            boolean downloadImage = true;\n            if (sig.mostRecentCheckpointTxId ==\n                dstImage.getStorage().getMostRecentCheckpointTxId()) {\n              downloadImage = false;\n              LOG.info(\"Image has not changed. Will not download image.\");\n            } else {\n              LOG.info(\"Image has changed. Downloading updated image from NN.\");\n              MD5Hash downloadedHash = TransferFsImage.downloadImageToStorage(\n                  nnHostPort, sig.mostRecentCheckpointTxId, dstImage.getStorage(), true);\n              dstImage.saveDigestAndRenameCheckpointImage(\n                  sig.mostRecentCheckpointTxId, downloadedHash);\n            }\n        \n            // get edits file\n            for (RemoteEditLog log : manifest.getLogs()) {\n              TransferFsImage.downloadEditsToStorage(\n                  nnHostPort, log, dstImage.getStorage());\n            }\n        \n            return Boolean.valueOf(downloadImage);\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal": "  public static <T> T doAsLoginUserOrFatal(PrivilegedAction<T> action) { \n    if (UserGroupInformation.isSecurityEnabled()) {\n      UserGroupInformation ugi = null;\n      try { \n        ugi = UserGroupInformation.getLoginUser();\n      } catch (IOException e) {\n        LOG.fatal(\"Exception while getting login user\", e);\n        e.printStackTrace();\n        Runtime.getRuntime().exit(-1);\n      }\n      return ugi.doAs(action);\n    } else {\n      return action.run();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.isSameCluster": "  boolean isSameCluster(FSImage si) {\n    return namespaceID == si.getStorage().namespaceID &&\n      clusterID.equals(si.getClusterID()) &&\n      blockpoolID.equals(si.getBlockPoolID());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.getClusterID": "  public String getClusterID() {\n    return clusterID;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.equals": "  public boolean equals(Object o) {\n    if (!(o instanceof CheckpointSignature)) {\n      return false;\n    }\n    return compareTo((CheckpointSignature)o) == 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.setBlockPoolID": "  private void setBlockPoolID(File storage, String bpid)\n      throws InconsistentFSStateException {\n    if (bpid == null || bpid.equals(\"\")) {\n      throw new InconsistentFSStateException(storage, \"file \"\n          + Storage.STORAGE_FILE_VERSION + \" has no block pool Id.\");\n    }\n    \n    if (!blockpoolID.equals(\"\") && !blockpoolID.equals(bpid)) {\n      throw new InconsistentFSStateException(storage,\n          \"Unexepcted blockpoolID \" + bpid + \" . Expected \" + blockpoolID);\n    }\n    setBlockPoolID(bpid);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.storageVersionMatches": "  boolean storageVersionMatches(StorageInfo si) throws IOException {\n    return (layoutVersion == si.layoutVersion) && (cTime == si.cTime);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.TransferFsImage.uploadImageFromStorage": "  public static void uploadImageFromStorage(String fsName,\n      InetSocketAddress imageListenAddress,\n      Storage storage, long txid) throws IOException {\n    \n    String fileid = GetImageServlet.getParamStringToPutImage(\n        txid, imageListenAddress, storage);\n    // this doesn't directly upload an image, but rather asks the NN\n    // to connect back to the 2NN to download the specified image.\n    try {\n      TransferFsImage.getFileClient(fsName, fileid, null, null, false);\n    } catch (HttpGetFailedException e) {\n      if (e.getResponseCode() == HttpServletResponse.SC_CONFLICT) {\n        // this is OK - this means that a previous attempt to upload\n        // this checkpoint succeeded even though we thought it failed.\n        LOG.info(\"Image upload with txid \" + txid + \n            \" conflicted with a previous image upload to the \" +\n            \"same NameNode. Continuing...\", e);\n        return;\n      } else {\n        throw e;\n      }\n    }\n    LOG.info(\"Uploaded image with txid \" + txid + \" to namenode at \" +\n    \t\tfsName);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient": "  static MD5Hash getFileClient(String nnHostPort,\n      String queryString, List<File> localPaths,\n      Storage dstStorage, boolean getChecksum) throws IOException {\n\n    String str = HttpConfig.getSchemePrefix() + nnHostPort + \"/getimage?\" +\n        queryString;\n    LOG.info(\"Opening connection to \" + str);\n    //\n    // open connection to remote server\n    //\n    URL url = new URL(str);\n    return doGetUrl(url, localPaths, dstStorage, getChecksum);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getResponseCode": "    public int getResponseCode() {\n      return responseCode;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.CheckpointFaultInjector.getInstance": "  static CheckpointFaultInjector getInstance() {\n    return instance;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.getFsImageName": "  public File getFsImageName(long txid) {\n    StorageDirectory sd = null;\n    for (Iterator<StorageDirectory> it =\n      dirIterator(NameNodeDirType.IMAGE); it.hasNext();) {\n      sd = it.next();\n      File fsImage = getStorageFile(sd, NameNodeFile.IMAGE, txid);\n      if(sd.getRoot().canRead() && fsImage.exists())\n        return fsImage;\n    }\n    return null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.getStorageFile": "  static File getStorageFile(StorageDirectory sd, NameNodeFile type) {\n    return new File(sd.getCurrentDir(), type.getName());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo": "  void validateStorageInfo(FSImage si) throws IOException {\n    if (!isSameCluster(si)\n        || !storageVersionMatches(si.getStorage())) {\n      throw new IOException(\"Inconsistent checkpoint fields.\\n\"\n          + \"LV = \" + layoutVersion + \" namespaceID = \" + namespaceID\n          + \" cTime = \" + cTime\n          + \" ; clusterId = \" + clusterID\n          + \" ; blockpoolId = \" + blockpoolID\n          + \".\\nExpecting respectively: \"\n          + si.getStorage().layoutVersion + \"; \" \n          + si.getStorage().namespaceID + \"; \" + si.getStorage().cTime\n          + \"; \" + si.getClusterID() + \"; \" \n          + si.getBlockPoolID() + \".\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.getBlockpoolID": "  public String getBlockpoolID() {\n    return blockpoolID;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.setClusterID": "  void setClusterID(String cid) {\n    clusterID = cid;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.checkpointConf.getPeriod": "  public long getPeriod() {\n    return checkpointPeriod;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.checkpointConf.getCheckPeriod": "  public long getCheckPeriod() {\n    return Math.min(checkpointCheckPeriod, checkpointPeriod);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.TransferFsImage.downloadEditsToStorage": "  static void downloadEditsToStorage(String fsName, RemoteEditLog log,\n      NNStorage dstStorage) throws IOException {\n    assert log.getStartTxId() > 0 && log.getEndTxId() > 0 :\n      \"bad log: \" + log;\n    String fileid = GetImageServlet.getParamStringForLog(\n        log, dstStorage);\n    String fileName = NNStorage.getFinalizedEditsFileName(\n        log.getStartTxId(), log.getEndTxId());\n\n    List<File> dstFiles = dstStorage.getFiles(NameNodeDirType.EDITS, fileName);\n    assert !dstFiles.isEmpty() : \"No checkpoint targets.\";\n    \n    for (File f : dstFiles) {\n      if (f.exists() && f.canRead()) {\n        LOG.info(\"Skipping download of remote edit log \" +\n            log + \" since it already is stored locally at \" + f);\n        return;\n      } else {\n        LOG.debug(\"Dest file: \" + f);\n      }\n    }\n\n    getFileClient(fsName, fileid, dstFiles, dstStorage, false);\n    LOG.info(\"Downloaded file \" + dstFiles.get(0).getName() + \" size \" +\n        dstFiles.get(0).length() + \" bytes.\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.TransferFsImage.downloadImageToStorage": "  public static MD5Hash downloadImageToStorage(\n      String fsName, long imageTxId, Storage dstStorage, boolean needDigest)\n      throws IOException {\n    String fileid = GetImageServlet.getParamStringForImage(\n        imageTxId, dstStorage);\n    String fileName = NNStorage.getCheckpointImageFileName(imageTxId);\n    \n    List<File> dstFiles = dstStorage.getFiles(\n        NameNodeDirType.IMAGE, fileName);\n    if (dstFiles.isEmpty()) {\n      throw new IOException(\"No targets in destination storage!\");\n    }\n    \n    MD5Hash hash = getFileClient(fsName, fileid, dstFiles, dstStorage, needDigest);\n    LOG.info(\"Downloaded file \" + dstFiles.get(0).getName() + \" size \" +\n        dstFiles.get(0).length() + \" bytes.\");\n    return hash;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getLoginUser": "  static UserGroupInformation getLoginUser() throws IOException {\n    if (loginUser == null) {\n      try {\n        Subject subject = new Subject();\n        LoginContext login;\n        if (isSecurityEnabled()) {\n          login = newLoginContext(HadoopConfiguration.USER_KERBEROS_CONFIG_NAME,\n              subject, new HadoopConfiguration());\n        } else {\n          login = newLoginContext(HadoopConfiguration.SIMPLE_CONFIG_NAME, \n              subject, new HadoopConfiguration());\n        }\n        login.login();\n        loginUser = new UserGroupInformation(subject);\n        loginUser.setLogin(login);\n        loginUser.setAuthenticationMethod(isSecurityEnabled() ?\n                                          AuthenticationMethod.KERBEROS :\n                                          AuthenticationMethod.SIMPLE);\n        loginUser = new UserGroupInformation(login.getSubject());\n        String fileLocation = System.getenv(HADOOP_TOKEN_FILE_LOCATION);\n        if (fileLocation != null) {\n          // load the token storage file and put all of the tokens into the\n          // user.\n          Credentials cred = Credentials.readTokenStorageFile(\n              new Path(\"file:///\" + fileLocation), conf);\n          loginUser.addCredentials(cred);\n        }\n        loginUser.spawnAutoRenewalThreadForUserCreds();\n      } catch (LoginException le) {\n        throw new IOException(\"failure to login\", le);\n      }\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"UGI loginUser:\"+loginUser);\n      }\n    }\n    return loginUser;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.addCredentials": "  public synchronized void addCredentials(Credentials credentials) {\n    getCredentialsInternal().addAll(credentials);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.setLogin": "  private void setLogin(LoginContext login) {\n    user.setLogin(login);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getSubject": "  protected Subject getSubject() {\n    return subject;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.login": "    public boolean login() throws LoginException {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"hadoop login\");\n      }\n      return true;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.newLoginContext": "  private static LoginContext\n  newLoginContext(String appName, Subject subject,\n    javax.security.auth.login.Configuration loginConf)\n      throws LoginException {\n    // Temporarily switch the thread's ContextClassLoader to match this\n    // class's classloader, so that we can properly load HadoopLoginModule\n    // from the JAAS libraries.\n    Thread t = Thread.currentThread();\n    ClassLoader oldCCL = t.getContextClassLoader();\n    t.setContextClassLoader(HadoopLoginModule.class.getClassLoader());\n    try {\n      return new LoginContext(appName, subject, null, loginConf);\n    } finally {\n      t.setContextClassLoader(oldCCL);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.setAuthenticationMethod": "  void setAuthenticationMethod(AuthenticationMethod authMethod) {\n    user.setAuthenticationMethod(authMethod);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled": "  public static boolean isSecurityEnabled() {\n    ensureInitialized();\n    return useKerberos;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.spawnAutoRenewalThreadForUserCreds": "  private void spawnAutoRenewalThreadForUserCreds() {\n    if (isSecurityEnabled()) {\n      //spawn thread only if we have kerb credentials\n      if (user.getAuthenticationMethod() == AuthenticationMethod.KERBEROS &&\n          !isKeytab) {\n        Thread t = new Thread(new Runnable() {\n          \n          @Override\n          public void run() {\n            String cmd = conf.get(\"hadoop.kerberos.kinit.command\",\n                                  \"kinit\");\n            KerberosTicket tgt = getTGT();\n            if (tgt == null) {\n              return;\n            }\n            long nextRefresh = getRefreshTime(tgt);\n            while (true) {\n              try {\n                long now = Time.now();\n                if(LOG.isDebugEnabled()) {\n                  LOG.debug(\"Current time is \" + now);\n                  LOG.debug(\"Next refresh is \" + nextRefresh);\n                }\n                if (now < nextRefresh) {\n                  Thread.sleep(nextRefresh - now);\n                }\n                Shell.execCommand(cmd, \"-R\");\n                if(LOG.isDebugEnabled()) {\n                  LOG.debug(\"renewed ticket\");\n                }\n                reloginFromTicketCache();\n                tgt = getTGT();\n                if (tgt == null) {\n                  LOG.warn(\"No TGT after renewal. Aborting renew thread for \" +\n                           getUserName());\n                  return;\n                }\n                nextRefresh = Math.max(getRefreshTime(tgt),\n                                       now + kerberosMinSecondsBeforeRelogin);\n              } catch (InterruptedException ie) {\n                LOG.warn(\"Terminating renewal thread\");\n                return;\n              } catch (IOException ie) {\n                LOG.warn(\"Exception encountered while running the\" +\n                    \" renewal command. Aborting renew thread. \" + ie);\n                return;\n              }\n            }\n          }\n        });\n        t.setDaemon(true);\n        t.setName(\"TGT Renewer for \" + getUserName());\n        t.start();\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      LOG.error(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(pae,\"Unknown exception in doAs\");\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }"
        },
        "bug_report": {
            "Title": "TestCheckpoint#testSecondaryHasVeryOutOfDateImage occasionally fails due to unexpected exit",
            "Description": "TestCheckpoint#testSecondaryHasVeryOutOfDateImage occasionally fails due to unexpected exit, due to an NPE while checkpointing. It looks like the background checkpoint fails, conflicts with the explicit checkpoints done by the tests (note the backtrace is not for the doCheckpoint calls in the tests.\n\n{noformat}\n2012-09-16 01:55:05,901 FATAL hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1355)) - Test resulted in an unexpected exit\norg.apache.hadoop.util.ExitUtil$ExitException: Fatal exception with message null\nstack trace\njava.lang.NullPointerException\nat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:480)\nat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:331)\nat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2.run(SecondaryNameNode.java:298)\nat org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:452)\nat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:294)\nat java.lang.Thread.run(Thread.java:662)\n{noformat}"
        }
    },
    {
        "filename": "HDFS-6715.json",
        "creation_time": "2014-07-21T21:26:25.000+0000",
        "stack_trace": "```\njava.io.IOException: Namenode is in startup mode\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n```",
        "source_code": {},
        "bug_report": {
            "Title": "webhdfs wont fail over when it gets java.io.IOException: Namenode is in startup mode",
            "Description": "Noticed in our HA testing when we run MR job with webhdfs file system we some times run into \n\n{code}\n2014-04-17 05:08:06,346 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1397710493213_0001_r_000008_0: Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n\n2014-04-17 05:08:10,205 ERROR [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Could not commit job\njava.io.IOException: Namenode is in startup mode\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n{code}"
        }
    },
    {
        "filename": "HDFS-2392.json",
        "creation_time": "2011-09-30T20:29:09.000+0000",
        "stack_trace": "```\njava.io.IOException: Copied: 0 Skipped: 0 Failed: 1\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n        at org.apache.hadoop.mapred.Child.main(Child.java:249)\n\njava.io.IOException: Copied: 0 Skipped: 0 Failed: 1\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n        at org.apache.hadoop.mapred.Child.main(Child.java:249)\n\njava.io.IOException: Copied: 0 Skipped: 0 Failed: 1\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n        at org.apache.hadoop.mapred.Child.main(Child.java:249)\n\nCopy failed: java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1257)\n        at org.apache.hadoop.tools.DistCp.copy(DistCp.java:667)\n        at org.apache.hadoop.tools.DistCp.run(DistCp.java:881)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n        at org.apache.hadoop.tools.DistCp.main(DistCp.java:908)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(pae,\"Unknown exception in doAs\");\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ToolRunner.run": "  public static int run(Tool tool, String[] args) \n    throws Exception{\n    return run(tool.getConf(), tool, args);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs": "  public String[] getRemainingArgs() {\n    return (commandLine == null) ? new String[]{} : commandLine.getArgs();\n  }"
        },
        "bug_report": {
            "Title": "Dist with hftp is failing again",
            "Description": "$ hadoop distcp hftp://<NN Hostname>:50070/user/hadoopqa/input1/part-00000 /user/hadoopqa/out3\n11/09/30 18:57:59 INFO tools.DistCp: srcPaths=[hftp://<NN Hostname>:50070/user/hadoopqa/input1/part-00000]\n11/09/30 18:57:59 INFO tools.DistCp: destPath=/user/hadoopqa/out3\n11/09/30 18:58:00 INFO security.TokenCache: Got dt for\nhftp://<NN Hostname>:50070/user/hadoopqa/input1/part-00000;uri=<NN IP>:50470;t.service=<NN IP>:50470\n11/09/30 18:58:00 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 24 for hadoopqa on <NN IP>:8020\n11/09/30 18:58:00 INFO security.TokenCache: Got dt for\n/user/hadoopqa/out3;uri=<NN IP>:8020;t.service=<NN IP>:8020\n11/09/30 18:58:00 INFO tools.DistCp: /user/hadoopqa/out3 does not exist.\n11/09/30 18:58:00 INFO tools.DistCp: sourcePathsCount=1\n11/09/30 18:58:00 INFO tools.DistCp: filesToCopyCount=1\n11/09/30 18:58:00 INFO tools.DistCp: bytesToCopyCount=1.0g\n11/09/30 18:58:01 INFO mapred.JobClient: Running job: job_201109300819_0007\n11/09/30 18:58:02 INFO mapred.JobClient:  map 0% reduce 0%\n11/09/30 18:58:25 INFO mapred.JobClient: Task Id : attempt_201109300819_0007_m_000000_0, Status : FAILED\njava.io.IOException: Copied: 0 Skipped: 0 Failed: 1\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n        at org.apache.hadoop.mapred.Child.main(Child.java:249)\n\n11/09/30 18:58:41 INFO mapred.JobClient: Task Id : attempt_201109300819_0007_m_000000_1, Status : FAILED\njava.io.IOException: Copied: 0 Skipped: 0 Failed: 1\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n        at org.apache.hadoop.mapred.Child.main(Child.java:249)\n\n11/09/30 18:58:56 INFO mapred.JobClient: Task Id : attempt_201109300819_0007_m_000000_2, Status : FAILED\njava.io.IOException: Copied: 0 Skipped: 0 Failed: 1\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n        at org.apache.hadoop.mapred.Child.main(Child.java:249)\n\n11/09/30 18:59:14 INFO mapred.JobClient: Job complete: job_201109300819_0007\n11/09/30 18:59:14 INFO mapred.JobClient: Counters: 6\n11/09/30 18:59:14 INFO mapred.JobClient:   Job Counters \n11/09/30 18:59:14 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=62380\n11/09/30 18:59:14 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0\n11/09/30 18:59:14 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0\n11/09/30 18:59:14 INFO mapred.JobClient:     Launched map tasks=4\n11/09/30 18:59:14 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=0\n11/09/30 18:59:14 INFO mapred.JobClient:     Failed map tasks=1\n11/09/30 18:59:14 INFO mapred.JobClient: Job Failed: # of failed Map Tasks exceeded allowed limit. FailedCount: 1.\nLastFailedTask: task_201109300819_0007_m_000000\nWith failures, global counters are inaccurate; consider running with -i\nCopy failed: java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1257)\n        at org.apache.hadoop.tools.DistCp.copy(DistCp.java:667)\n        at org.apache.hadoop.tools.DistCp.run(DistCp.java:881)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n        at org.apache.hadoop.tools.DistCp.main(DistCp.java:908)\n\n\n\n\n\n"
        }
    },
    {
        "filename": "HDFS-11472.json",
        "creation_time": "2017-02-28T18:39:13.000+0000",
        "stack_trace": "```\njava.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: getBytesOnDisk() < getVisibleLength(), rip=ReplicaBeingWritten, blk_2526438952_1101394519586, RBW\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2284)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2260)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.initReplicaRecovery(DataNode.java:2566)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.callInitReplicaRecovery(DataNode.java:2577)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2645)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.access$400(DataNode.java:245)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode$5.run(DataNode.java:2551)\n        at java.lang.Thread.run(Thread.java:745)\n\njava.nio.channels.ClosedByInterruptException\n        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n        at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:269)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.adjustCrcChannelPosition(FsDatasetImpl.java:1484)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.adjustCrcFilePosition(BlockReceiver.java:994)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:670)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:857)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:797)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:244)\n        at java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery": "  static ReplicaRecoveryInfo initReplicaRecovery(String bpid, ReplicaMap map,\n      Block block, long recoveryId, long xceiverStopTimeout) throws IOException {\n    while (true) {\n      try {\n        try (AutoCloseableLock lock = map.getLock().acquire()) {\n          return initReplicaRecoveryImpl(bpid, map, block, recoveryId);\n        }\n      } catch (MustStopExistingWriter e) {\n        e.getReplicaInPipeline().stopWriter(xceiverStopTimeout);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecoveryImpl": "  static ReplicaRecoveryInfo initReplicaRecoveryImpl(String bpid, ReplicaMap map,\n      Block block, long recoveryId)\n          throws IOException, MustStopExistingWriter {\n    final ReplicaInfo replica = map.get(bpid, block.getBlockId());\n    LOG.info(\"initReplicaRecovery: \" + block + \", recoveryId=\" + recoveryId\n        + \", replica=\" + replica);\n\n    //check replica\n    if (replica == null) {\n      return null;\n    }\n\n    //stop writer if there is any\n    if (replica.getState() == ReplicaState.TEMPORARY ||\n        replica.getState() == ReplicaState.RBW) {\n      final ReplicaInPipeline rip = (ReplicaInPipeline)replica;\n      if (!rip.attemptToSetWriter(null, Thread.currentThread())) {\n        throw new MustStopExistingWriter(rip);\n      }\n\n      //check replica bytes on disk.\n      if (replica.getBytesOnDisk() < replica.getVisibleLength()) {\n        throw new IOException(\"THIS IS NOT SUPPOSED TO HAPPEN:\"\n            + \" getBytesOnDisk() < getVisibleLength(), rip=\" + replica);\n      }\n\n      //check the replica's files\n      checkReplicaFiles(replica);\n    }\n\n    //check generation stamp\n    if (replica.getGenerationStamp() < block.getGenerationStamp()) {\n      throw new IOException(\n          \"replica.getGenerationStamp() < block.getGenerationStamp(), block=\"\n          + block + \", replica=\" + replica);\n    }\n\n    //check recovery id\n    if (replica.getGenerationStamp() >= recoveryId) {\n      throw new IOException(\"THIS IS NOT SUPPOSED TO HAPPEN:\"\n          + \" replica.getGenerationStamp() >= recoveryId = \" + recoveryId\n          + \", block=\" + block + \", replica=\" + replica);\n    }\n\n    //check RUR\n    final ReplicaInfo rur;\n    if (replica.getState() == ReplicaState.RUR) {\n      rur = replica;\n      if (rur.getRecoveryID() >= recoveryId) {\n        throw new RecoveryInProgressException(\n            \"rur.getRecoveryID() >= recoveryId = \" + recoveryId\n            + \", block=\" + block + \", rur=\" + rur);\n      }\n      final long oldRecoveryID = rur.getRecoveryID();\n      rur.setRecoveryID(recoveryId);\n      LOG.info(\"initReplicaRecovery: update recovery id for \" + block\n          + \" from \" + oldRecoveryID + \" to \" + recoveryId);\n    }\n    else {\n      rur = new ReplicaBuilder(ReplicaState.RUR)\n          .from(replica).setRecoveryId(recoveryId).build();\n      map.add(bpid, rur);\n      LOG.info(\"initReplicaRecovery: changing replica state for \"\n          + block + \" from \" + replica.getState()\n          + \" to \" + rur.getState());\n    }\n    return rur.createInfo();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInPipeline": "    ReplicaInPipeline getReplicaInPipeline() {\n      return rip;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.initReplicaRecovery": "  public ReplicaRecoveryInfo initReplicaRecovery(RecoveringBlock rBlock)\n      throws IOException {\n    return data.initReplicaRecovery(rBlock);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.run": "      @Override public void run() {\n        if (!shutdownForUpgrade) {\n          // Delay the shutdown a bit if not doing for restart.\n          try {\n            Thread.sleep(1000);\n          } catch (InterruptedException ie) { }\n        }\n        shutdown();\n      }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown": "  public void shutdown() {\n    stopMetricsLogger();\n    if (plugins != null) {\n      for (ServicePlugin p : plugins) {\n        try {\n          p.stop();\n          LOG.info(\"Stopped plug-in \" + p);\n        } catch (Throwable t) {\n          LOG.warn(\"ServicePlugin \" + p + \" could not be stopped\", t);\n        }\n      }\n    }\n    \n    List<BPOfferService> bposArray = (this.blockPoolManager == null)\n        ? new ArrayList<BPOfferService>()\n        : this.blockPoolManager.getAllNamenodeThreads();\n    // If shutdown is not for restart, set shouldRun to false early. \n    if (!shutdownForUpgrade) {\n      shouldRun = false;\n    }\n\n    // When shutting down for restart, DataXceiverServer is interrupted\n    // in order to avoid any further acceptance of requests, but the peers\n    // for block writes are not closed until the clients are notified.\n    if (dataXceiverServer != null) {\n      try {\n        xserver.sendOOBToPeers();\n        ((DataXceiverServer) this.dataXceiverServer.getRunnable()).kill();\n        this.dataXceiverServer.interrupt();\n      } catch (Exception e) {\n        // Ignore, since the out of band messaging is advisory.\n        LOG.trace(\"Exception interrupting DataXceiverServer: \", e);\n      }\n    }\n\n    // Record the time of initial notification\n    long timeNotified = Time.monotonicNow();\n\n    if (localDataXceiverServer != null) {\n      ((DataXceiverServer) this.localDataXceiverServer.getRunnable()).kill();\n      this.localDataXceiverServer.interrupt();\n    }\n\n    // Terminate directory scanner and block scanner\n    shutdownPeriodicScanners();\n    shutdownDiskBalancer();\n\n    // Stop the web server\n    if (httpServer != null) {\n      try {\n        httpServer.close();\n      } catch (Exception e) {\n        LOG.warn(\"Exception shutting down DataNode HttpServer\", e);\n      }\n    }\n\n    volumeChecker.shutdownAndWait(1, TimeUnit.SECONDS);\n\n    if (storageLocationChecker != null) {\n      storageLocationChecker.shutdownAndWait(1, TimeUnit.SECONDS);\n    }\n\n    if (pauseMonitor != null) {\n      pauseMonitor.stop();\n    }\n\n    // shouldRun is set to false here to prevent certain threads from exiting\n    // before the restart prep is done.\n    this.shouldRun = false;\n    \n    // wait reconfiguration thread, if any, to exit\n    shutdownReconfigurationTask();\n\n    // wait for all data receiver threads to exit\n    if (this.threadGroup != null) {\n      int sleepMs = 2;\n      while (true) {\n        // When shutting down for restart, wait 2.5 seconds before forcing\n        // termination of receiver threads.\n        if (!this.shutdownForUpgrade ||\n            (this.shutdownForUpgrade && (Time.monotonicNow() - timeNotified\n                > 1000))) {\n          this.threadGroup.interrupt();\n          break;\n        }\n        LOG.info(\"Waiting for threadgroup to exit, active threads is \" +\n                 this.threadGroup.activeCount());\n        if (this.threadGroup.activeCount() == 0) {\n          break;\n        }\n        try {\n          Thread.sleep(sleepMs);\n        } catch (InterruptedException e) {}\n        sleepMs = sleepMs * 3 / 2; // exponential backoff\n        if (sleepMs > 200) {\n          sleepMs = 200;\n        }\n      }\n      this.threadGroup = null;\n    }\n    if (this.dataXceiverServer != null) {\n      // wait for dataXceiverServer to terminate\n      try {\n        this.dataXceiverServer.join();\n      } catch (InterruptedException ie) {\n      }\n    }\n    if (this.localDataXceiverServer != null) {\n      // wait for localDataXceiverServer to terminate\n      try {\n        this.localDataXceiverServer.join();\n      } catch (InterruptedException ie) {\n      }\n    }\n    if (metrics != null) {\n      metrics.setDataNodeActiveXceiversCount(0);\n    }\n\n   // IPC server needs to be shutdown late in the process, otherwise\n   // shutdown command response won't get sent.\n   if (ipcServer != null) {\n      ipcServer.stop();\n    }\n\n    if(blockPoolManager != null) {\n      try {\n        this.blockPoolManager.shutDownAll(bposArray);\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Received exception in BlockPoolManager#shutDownAll: \", ie);\n      }\n    }\n    \n    if (storage != null) {\n      try {\n        this.storage.unlockAll();\n      } catch (IOException ie) {\n        LOG.warn(\"Exception when unlocking storage: \" + ie, ie);\n      }\n    }\n    if (data != null) {\n      data.shutdown();\n    }\n    if (metrics != null) {\n      metrics.shutdown();\n    }\n    if (dataNodeInfoBeanName != null) {\n      MBeans.unregister(dataNodeInfoBeanName);\n      dataNodeInfoBeanName = null;\n    }\n    if (shortCircuitRegistry != null) shortCircuitRegistry.shutdown();\n    LOG.info(\"Shutdown complete.\");\n    synchronized(this) {\n      // it is already false, but setting it again to avoid a findbug warning.\n      this.shouldRun = false;\n      // Notify the main thread.\n      notifyAll();\n    }\n    tracer.close();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.createSocketAddr": "  public static InetSocketAddress createSocketAddr(String target) {\n    return NetUtils.createSocketAddr(target);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getDataEncryptionKeyFactoryForBlock": "  public DataEncryptionKeyFactory getDataEncryptionKeyFactoryForBlock(\n      final ExtendedBlock block) {\n    return new DataEncryptionKeyFactory() {\n      @Override\n      public DataEncryptionKey newDataEncryptionKey() {\n        return dnConf.encryptDataTransfer ?\n          blockPoolTokenSecretManager.generateDataEncryptionKey(\n            block.getBlockPoolId()) : null;\n      }\n    };\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.incrementXmitsInProgress": "  public void incrementXmitsInProgress() {\n    xmitsInProgress.getAndIncrement();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getDisplayName": "  public String getDisplayName() {\n    // NB: our DatanodeID may not be set yet\n    return hostName + \":\" + getXferPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.decrementXmitsInProgress": "  public void decrementXmitsInProgress() {\n    xmitsInProgress.getAndDecrement();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.newSocket": "  public Socket newSocket() throws IOException {\n    return socketFactory.createSocket();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getBlockAccessToken": "  public Token<BlockTokenIdentifier> getBlockAccessToken(ExtendedBlock b,\n      EnumSet<AccessMode> mode) throws IOException {\n    Token<BlockTokenIdentifier> accessToken = \n        BlockTokenSecretManager.DUMMY_TOKEN;\n    if (isBlockTokenEnabled) {\n      accessToken = blockPoolTokenSecretManager.generateToken(b, mode);\n    }\n    return accessToken;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.adjustCrcChannelPosition": "  public void adjustCrcChannelPosition(ExtendedBlock b, ReplicaOutputStreams streams, \n      int checksumSize) throws IOException {\n    FileOutputStream file = (FileOutputStream)streams.getChecksumOut();\n    FileChannel channel = file.getChannel();\n    long oldPos = channel.position();\n    long newPos = oldPos - checksumSize;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Changing meta file offset of block \" + b + \" from \" +\n          oldPos + \" to \" + newPos);\n    }\n    channel.position(newPos);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.adjustCrcFilePosition": "  private void adjustCrcFilePosition() throws IOException {\n    streams.flushDataOut();\n    if (checksumOut != null) {\n      checksumOut.flush();\n    }\n\n    // rollback the position of the meta file\n    datanode.data.adjustCrcChannelPosition(block, streams, checksumSize);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket": "  private int receivePacket() throws IOException {\n    // read the next packet\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader header = packetReceiver.getHeader();\n    if (LOG.isDebugEnabled()){\n      LOG.debug(\"Receiving one packet for block \" + block +\n                \": \" + header);\n    }\n\n    // Sanity check the header\n    if (header.getOffsetInBlock() > replicaInfo.getNumBytes()) {\n      throw new IOException(\"Received an out-of-sequence packet for \" + block + \n          \"from \" + inAddr + \" at offset \" + header.getOffsetInBlock() +\n          \". Expecting packet starting at \" + replicaInfo.getNumBytes());\n    }\n    if (header.getDataLen() < 0) {\n      throw new IOException(\"Got wrong length during writeBlock(\" + block + \n                            \") from \" + inAddr + \" at offset \" + \n                            header.getOffsetInBlock() + \": \" +\n                            header.getDataLen()); \n    }\n\n    long offsetInBlock = header.getOffsetInBlock();\n    long seqno = header.getSeqno();\n    boolean lastPacketInBlock = header.isLastPacketInBlock();\n    final int len = header.getDataLen();\n    boolean syncBlock = header.getSyncBlock();\n\n    // avoid double sync'ing on close\n    if (syncBlock && lastPacketInBlock) {\n      this.syncOnClose = false;\n    }\n\n    // update received bytes\n    final long firstByteInBlock = offsetInBlock;\n    offsetInBlock += len;\n    if (replicaInfo.getNumBytes() < offsetInBlock) {\n      replicaInfo.setNumBytes(offsetInBlock);\n    }\n    \n    // put in queue for pending acks, unless sync was requested\n    if (responder != null && !syncBlock && !shouldVerifyChecksum()) {\n      ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n          lastPacketInBlock, offsetInBlock, Status.SUCCESS);\n    }\n\n    // Drop heartbeat for testing.\n    if (seqno < 0 && len == 0 &&\n        DataNodeFaultInjector.get().dropHeartbeatPacket()) {\n      return 0;\n    }\n\n    //First write the packet to the mirror:\n    if (mirrorOut != null && !mirrorError) {\n      try {\n        long begin = Time.monotonicNow();\n        // For testing. Normally no-op.\n        DataNodeFaultInjector.get().stopSendingPacketDownstream(mirrorAddr);\n        packetReceiver.mirrorPacketTo(mirrorOut);\n        mirrorOut.flush();\n        long now = Time.monotonicNow();\n        setLastSentTime(now);\n        long duration = now - begin;\n        DataNodeFaultInjector.get().logDelaySendingPacketDownstream(\n            mirrorAddr,\n            duration);\n        trackSendPacketToLastNodeInPipeline(duration);\n        if (duration > datanodeSlowLogThresholdMs) {\n          LOG.warn(\"Slow BlockReceiver write packet to mirror took \" + duration\n              + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms)\");\n        }\n      } catch (IOException e) {\n        handleMirrorOutError(e);\n      }\n    }\n    \n    ByteBuffer dataBuf = packetReceiver.getDataSlice();\n    ByteBuffer checksumBuf = packetReceiver.getChecksumSlice();\n    \n    if (lastPacketInBlock || len == 0) {\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"Receiving an empty packet or the end of the block \" + block);\n      }\n      // sync block if requested\n      if (syncBlock) {\n        flushOrSync(true);\n      }\n    } else {\n      final int checksumLen = diskChecksum.getChecksumSize(len);\n      final int checksumReceivedLen = checksumBuf.capacity();\n\n      if (checksumReceivedLen > 0 && checksumReceivedLen != checksumLen) {\n        throw new IOException(\"Invalid checksum length: received length is \"\n            + checksumReceivedLen + \" but expected length is \" + checksumLen);\n      }\n\n      if (checksumReceivedLen > 0 && shouldVerifyChecksum()) {\n        try {\n          verifyChunks(dataBuf, checksumBuf);\n        } catch (IOException ioe) {\n          // checksum error detected locally. there is no reason to continue.\n          if (responder != null) {\n            try {\n              ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n                  lastPacketInBlock, offsetInBlock,\n                  Status.ERROR_CHECKSUM);\n              // Wait until the responder sends back the response\n              // and interrupt this thread.\n              Thread.sleep(3000);\n            } catch (InterruptedException e) { }\n          }\n          throw new IOException(\"Terminating due to a checksum error.\" + ioe);\n        }\n \n        if (needsChecksumTranslation) {\n          // overwrite the checksums in the packet buffer with the\n          // appropriate polynomial for the disk storage.\n          translateChunks(dataBuf, checksumBuf);\n        }\n      }\n\n      if (checksumReceivedLen == 0 && !streams.isTransientStorage()) {\n        // checksum is missing, need to calculate it\n        checksumBuf = ByteBuffer.allocate(checksumLen);\n        diskChecksum.calculateChunkedSums(dataBuf, checksumBuf);\n      }\n      \n      // by this point, the data in the buffer uses the disk checksum\n\n      final boolean shouldNotWriteChecksum = checksumReceivedLen == 0\n          && streams.isTransientStorage();\n      try {\n        long onDiskLen = replicaInfo.getBytesOnDisk();\n        if (onDiskLen<offsetInBlock) {\n          // Normally the beginning of an incoming packet is aligned with the\n          // existing data on disk. If the beginning packet data offset is not\n          // checksum chunk aligned, the end of packet will not go beyond the\n          // next chunk boundary.\n          // When a failure-recovery is involved, the client state and the\n          // the datanode state may not exactly agree. I.e. the client may\n          // resend part of data that is already on disk. Correct number of\n          // bytes should be skipped when writing the data and checksum\n          // buffers out to disk.\n          long partialChunkSizeOnDisk = onDiskLen % bytesPerChecksum;\n          long lastChunkBoundary = onDiskLen - partialChunkSizeOnDisk;\n          boolean alignedOnDisk = partialChunkSizeOnDisk == 0;\n          boolean alignedInPacket = firstByteInBlock % bytesPerChecksum == 0;\n\n          // If the end of the on-disk data is not chunk-aligned, the last\n          // checksum needs to be overwritten.\n          boolean overwriteLastCrc = !alignedOnDisk && !shouldNotWriteChecksum;\n          // If the starting offset of the packat data is at the last chunk\n          // boundary of the data on disk, the partial checksum recalculation\n          // can be skipped and the checksum supplied by the client can be used\n          // instead. This reduces disk reads and cpu load.\n          boolean doCrcRecalc = overwriteLastCrc &&\n              (lastChunkBoundary != firstByteInBlock);\n\n          // If this is a partial chunk, then verify that this is the only\n          // chunk in the packet. If the starting offset is not chunk\n          // aligned, the packet should terminate at or before the next\n          // chunk boundary.\n          if (!alignedInPacket && len > bytesPerChecksum) {\n            throw new IOException(\"Unexpected packet data length for \"\n                +  block + \" from \" + inAddr + \": a partial chunk must be \"\n                + \" sent in an individual packet (data length = \" + len\n                +  \" > bytesPerChecksum = \" + bytesPerChecksum + \")\");\n          }\n\n          // If the last portion of the block file is not a full chunk,\n          // then read in pre-existing partial data chunk and recalculate\n          // the checksum so that the checksum calculation can continue\n          // from the right state. If the client provided the checksum for\n          // the whole chunk, this is not necessary.\n          Checksum partialCrc = null;\n          if (doCrcRecalc) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"receivePacket for \" + block \n                  + \": previous write did not end at the chunk boundary.\"\n                  + \" onDiskLen=\" + onDiskLen);\n            }\n            long offsetInChecksum = BlockMetadataHeader.getHeaderSize() +\n                onDiskLen / bytesPerChecksum * checksumSize;\n            partialCrc = computePartialChunkCrc(onDiskLen, offsetInChecksum);\n          }\n\n          // The data buffer position where write will begin. If the packet\n          // data and on-disk data have no overlap, this will not be at the\n          // beginning of the buffer.\n          int startByteToDisk = (int)(onDiskLen-firstByteInBlock) \n              + dataBuf.arrayOffset() + dataBuf.position();\n\n          // Actual number of data bytes to write.\n          int numBytesToDisk = (int)(offsetInBlock-onDiskLen);\n          \n          // Write data to disk.\n          long begin = Time.monotonicNow();\n          streams.writeDataToDisk(dataBuf.array(),\n              startByteToDisk, numBytesToDisk);\n          long duration = Time.monotonicNow() - begin;\n\n          if (duration > maxWriteToDiskMs) {\n            maxWriteToDiskMs = duration;\n          }\n\n          final byte[] lastCrc;\n          if (shouldNotWriteChecksum) {\n            lastCrc = null;\n          } else {\n            int skip = 0;\n            byte[] crcBytes = null;\n\n            // First, prepare to overwrite the partial crc at the end.\n            if (overwriteLastCrc) { // not chunk-aligned on disk\n              // prepare to overwrite last checksum\n              adjustCrcFilePosition();\n            }\n\n            // The CRC was recalculated for the last partial chunk. Update the\n            // CRC by reading the rest of the chunk, then write it out.\n            if (doCrcRecalc) {\n              // Calculate new crc for this chunk.\n              int bytesToReadForRecalc =\n                  (int)(bytesPerChecksum - partialChunkSizeOnDisk);\n              if (numBytesToDisk < bytesToReadForRecalc) {\n                bytesToReadForRecalc = numBytesToDisk;\n              }\n\n              partialCrc.update(dataBuf.array(), startByteToDisk,\n                  bytesToReadForRecalc);\n              byte[] buf = FSOutputSummer.convertToByteStream(partialCrc,\n                  checksumSize);\n              crcBytes = copyLastChunkChecksum(buf, checksumSize, buf.length);\n              checksumOut.write(buf);\n              if(LOG.isDebugEnabled()) {\n                LOG.debug(\"Writing out partial crc for data len \" + len +\n                    \", skip=\" + skip);\n              }\n              skip++; //  For the partial chunk that was just read.\n            }\n\n            // Determine how many checksums need to be skipped up to the last\n            // boundary. The checksum after the boundary was already counted\n            // above. Only count the number of checksums skipped up to the\n            // boundary here.\n            long skippedDataBytes = lastChunkBoundary - firstByteInBlock;\n\n            if (skippedDataBytes > 0) {\n              skip += (int)(skippedDataBytes / bytesPerChecksum) +\n                  ((skippedDataBytes % bytesPerChecksum == 0) ? 0 : 1);\n            }\n            skip *= checksumSize; // Convert to number of bytes\n\n            // write the rest of checksum\n            final int offset = checksumBuf.arrayOffset() +\n                checksumBuf.position() + skip;\n            final int end = offset + checksumLen - skip;\n            // If offset >= end, there is no more checksum to write.\n            // I.e. a partial chunk checksum rewrite happened and there is no\n            // more to write after that.\n            if (offset >= end && doCrcRecalc) {\n              lastCrc = crcBytes;\n            } else {\n              final int remainingBytes = checksumLen - skip;\n              lastCrc = copyLastChunkChecksum(checksumBuf.array(),\n                  checksumSize, end);\n              checksumOut.write(checksumBuf.array(), offset, remainingBytes);\n            }\n          }\n\n          /// flush entire packet, sync if requested\n          flushOrSync(syncBlock);\n          \n          replicaInfo.setLastChecksumAndDataLen(offsetInBlock, lastCrc);\n\n          datanode.metrics.incrBytesWritten(len);\n          datanode.metrics.incrTotalWriteTime(duration);\n\n          manageWriterOsCache(offsetInBlock);\n        }\n      } catch (IOException iex) {\n        // Volume error check moved to FileIoProvider\n        throw iex;\n      }\n    }\n\n    // if sync was requested, put in queue for pending acks here\n    // (after the fsync finished)\n    if (responder != null && (syncBlock || shouldVerifyChecksum())) {\n      ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n          lastPacketInBlock, offsetInBlock, Status.SUCCESS);\n    }\n\n    /*\n     * Send in-progress responses for the replaceBlock() calls back to caller to\n     * avoid timeouts due to balancer throttling. HDFS-6247\n     */\n    if (isReplaceBlock\n        && (Time.monotonicNow() - lastResponseTime > responseInterval)) {\n      BlockOpResponseProto.Builder response = BlockOpResponseProto.newBuilder()\n          .setStatus(Status.IN_PROGRESS);\n      response.build().writeDelimitedTo(replyOut);\n      replyOut.flush();\n\n      lastResponseTime = Time.monotonicNow();\n    }\n\n    if (throttler != null) { // throttle I/O\n      throttler.throttle(len);\n    }\n    \n    return lastPacketInBlock?-1:len;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.translateChunks": "  private void translateChunks(ByteBuffer dataBuf, ByteBuffer checksumBuf) {\n    diskChecksum.calculateChunkedSums(dataBuf, checksumBuf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.handleMirrorOutError": "  private void handleMirrorOutError(IOException ioe) throws IOException {\n    String bpid = block.getBlockPoolId();\n    LOG.info(datanode.getDNRegistrationForBP(bpid)\n        + \":Exception writing \" + block + \" to mirror \" + mirrorAddr, ioe);\n    if (Thread.interrupted()) { // shut down if the thread is interrupted\n      throw ioe;\n    } else { // encounter an error while writing to mirror\n      // continue to run even if can not write to mirror\n      // notify client of the error\n      // and wait for the client to shut down the pipeline\n      mirrorError = true;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.manageWriterOsCache": "  private void manageWriterOsCache(long offsetInBlock) {\n    try {\n      if (streams.getOutFd() != null &&\n          offsetInBlock > lastCacheManagementOffset + CACHE_DROP_LAG_BYTES) {\n        long begin = Time.monotonicNow();\n        //\n        // For SYNC_FILE_RANGE_WRITE, we want to sync from\n        // lastCacheManagementOffset to a position \"two windows ago\"\n        //\n        //                         <========= sync ===========>\n        // +-----------------------O--------------------------X\n        // start                  last                      curPos\n        // of file                 \n        //\n        if (syncBehindWrites) {\n          if (syncBehindWritesInBackground) {\n            this.datanode.getFSDataset().submitBackgroundSyncFileRangeRequest(\n                block, streams, lastCacheManagementOffset,\n                offsetInBlock - lastCacheManagementOffset,\n                SYNC_FILE_RANGE_WRITE);\n          } else {\n            streams.syncFileRangeIfPossible(lastCacheManagementOffset,\n                offsetInBlock - lastCacheManagementOffset,\n                SYNC_FILE_RANGE_WRITE);\n          }\n        }\n        //\n        // For POSIX_FADV_DONTNEED, we want to drop from the beginning \n        // of the file to a position prior to the current position.\n        //\n        // <=== drop =====> \n        //                 <---W--->\n        // +--------------+--------O--------------------------X\n        // start        dropPos   last                      curPos\n        // of file             \n        //                     \n        long dropPos = lastCacheManagementOffset - CACHE_DROP_LAG_BYTES;\n        if (dropPos > 0 && dropCacheBehindWrites) {\n          streams.dropCacheBehindWrites(block.getBlockName(), 0, dropPos,\n              POSIX_FADV_DONTNEED);\n        }\n        lastCacheManagementOffset = offsetInBlock;\n        long duration = Time.monotonicNow() - begin;\n        if (duration > datanodeSlowLogThresholdMs) {\n          LOG.warn(\"Slow manageWriterOsCache took \" + duration\n              + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms)\");\n        }\n      }\n    } catch (Throwable t) {\n      LOG.warn(\"Error managing cache for writer of block \" + block, t);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.copyLastChunkChecksum": "  private static byte[] copyLastChunkChecksum(byte[] array, int size, int end) {\n    return Arrays.copyOfRange(array, end - size, end);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.trackSendPacketToLastNodeInPipeline": "  private void trackSendPacketToLastNodeInPipeline(final long elapsedMs) {\n    final DataNodePeerMetrics peerMetrics = datanode.getPeerMetrics();\n    if (peerMetrics != null && isPenultimateNode) {\n      peerMetrics.addSendPacketDownstream(mirrorNameForMetrics, elapsedMs);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.setLastSentTime": "  synchronized void setLastSentTime(long sentTime) {\n    lastSentTime = sentTime;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.verifyChunks": "  private void verifyChunks(ByteBuffer dataBuf, ByteBuffer checksumBuf)\n      throws IOException {\n    try {\n      clientChecksum.verifyChunkedSums(dataBuf, checksumBuf, clientname, 0);\n    } catch (ChecksumException ce) {\n      PacketHeader header = packetReceiver.getHeader();\n      String specificOffset = \"specific offsets are:\"\n          + \" offsetInBlock = \" + header.getOffsetInBlock()\n          + \" offsetInPacket = \" + ce.getPos();\n      LOG.warn(\"Checksum error in block \"\n          + block + \" from \" + inAddr\n          + \", \" + specificOffset, ce);\n      // No need to report to namenode when client is writing.\n      if (srcDataNode != null && isDatanode) {\n        try {\n          LOG.info(\"report corrupt \" + block + \" from datanode \" +\n                    srcDataNode + \" to namenode\");\n          datanode.reportRemoteBadBlock(srcDataNode, block);\n        } catch (IOException e) {\n          LOG.warn(\"Failed to report bad \" + block + \n                    \" from datanode \" + srcDataNode + \" to namenode\");\n        }\n      }\n      throw new IOException(\"Unexpected checksum mismatch while writing \"\n          + block + \" from \" + inAddr);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.computePartialChunkCrc": "  private Checksum computePartialChunkCrc(long blkoff, long ckoff)\n      throws IOException {\n\n    // find offset of the beginning of partial chunk.\n    //\n    int sizePartialChunk = (int) (blkoff % bytesPerChecksum);\n    blkoff = blkoff - sizePartialChunk;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"computePartialChunkCrc for \" + block\n          + \": sizePartialChunk=\" + sizePartialChunk\n          + \", block offset=\" + blkoff\n          + \", metafile offset=\" + ckoff);\n    }\n\n    // create an input stream from the block file\n    // and read in partial crc chunk into temporary buffer\n    //\n    byte[] buf = new byte[sizePartialChunk];\n    byte[] crcbuf = new byte[checksumSize];\n    try (ReplicaInputStreams instr =\n        datanode.data.getTmpInputStreams(block, blkoff, ckoff)) {\n      instr.readDataFully(buf, 0, sizePartialChunk);\n\n      // open meta file and read in crc value computer earlier\n      instr.readChecksumFully(crcbuf, 0, crcbuf.length);\n    }\n\n    // compute crc of partial chunk from data read in the block file.\n    final Checksum partialCrc = DataChecksum.newDataChecksum(\n        diskChecksum.getChecksumType(), diskChecksum.getBytesPerChecksum());\n    partialCrc.update(buf, 0, sizePartialChunk);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Read in partial CRC chunk from disk for \" + block);\n    }\n\n    // paranoia! verify that the pre-computed crc matches what we\n    // recalculated just now\n    if (partialCrc.getValue() != checksum2long(crcbuf)) {\n      String msg = \"Partial CRC \" + partialCrc.getValue() +\n                   \" does not match value computed the \" +\n                   \" last time file was closed \" +\n                   checksum2long(crcbuf);\n      throw new IOException(msg);\n    }\n    return partialCrc;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.shouldVerifyChecksum": "  private boolean shouldVerifyChecksum() {\n    return (mirrorOut == null || isDatanode || needsChecksumTranslation);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.flushOrSync": "  void flushOrSync(boolean isSync) throws IOException {\n    long flushTotalNanos = 0;\n    long begin = Time.monotonicNow();\n    if (checksumOut != null) {\n      long flushStartNanos = System.nanoTime();\n      checksumOut.flush();\n      long flushEndNanos = System.nanoTime();\n      if (isSync) {\n        streams.syncChecksumOut();\n        datanode.metrics.addFsyncNanos(System.nanoTime() - flushEndNanos);\n      }\n      flushTotalNanos += flushEndNanos - flushStartNanos;\n    }\n    if (streams.getDataOut() != null) {\n      long flushStartNanos = System.nanoTime();\n      streams.flushDataOut();\n      long flushEndNanos = System.nanoTime();\n      if (isSync) {\n        long fsyncStartNanos = flushEndNanos;\n        streams.syncDataOut();\n        datanode.metrics.addFsyncNanos(System.nanoTime() - fsyncStartNanos);\n      }\n      flushTotalNanos += flushEndNanos - flushStartNanos;\n    }\n    if (checksumOut != null || streams.getDataOut() != null) {\n      datanode.metrics.addFlushNanos(flushTotalNanos);\n      if (isSync) {\n        datanode.metrics.incrFsyncCount();\n      }\n    }\n    long duration = Time.monotonicNow() - begin;\n    if (duration > datanodeSlowLogThresholdMs) {\n      LOG.warn(\"Slow flushOrSync took \" + duration + \"ms (threshold=\"\n          + datanodeSlowLogThresholdMs + \"ms), isSync:\" + isSync + \", flushTotalNanos=\"\n          + flushTotalNanos + \"ns\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock": "  void receiveBlock(\n      DataOutputStream mirrOut, // output to next datanode\n      DataInputStream mirrIn,   // input from next datanode\n      DataOutputStream replyOut,  // output to previous datanode\n      String mirrAddr, DataTransferThrottler throttlerArg,\n      DatanodeInfo[] downstreams,\n      boolean isReplaceBlock) throws IOException {\n\n    syncOnClose = datanode.getDnConf().syncOnClose;\n    boolean responderClosed = false;\n    mirrorOut = mirrOut;\n    mirrorAddr = mirrAddr;\n    isPenultimateNode = ((downstreams != null) && (downstreams.length == 1));\n    if (isPenultimateNode) {\n      mirrorNameForMetrics = (downstreams[0].getInfoSecurePort() != 0 ?\n          downstreams[0].getInfoSecureAddr() : downstreams[0].getInfoAddr());\n      LOG.debug(\"Will collect peer metrics for downstream node {}\",\n          mirrorNameForMetrics);\n    }\n    throttler = throttlerArg;\n\n    this.replyOut = replyOut;\n    this.isReplaceBlock = isReplaceBlock;\n\n    try {\n      if (isClient && !isTransfer) {\n        responder = new Daemon(datanode.threadGroup, \n            new PacketResponder(replyOut, mirrIn, downstreams));\n        responder.start(); // start thread to processes responses\n      }\n\n      while (receivePacket() >= 0) { /* Receive until the last packet */ }\n\n      // wait for all outstanding packet responses. And then\n      // indicate responder to gracefully shutdown.\n      // Mark that responder has been closed for future processing\n      if (responder != null) {\n        ((PacketResponder)responder.getRunnable()).close();\n        responderClosed = true;\n      }\n\n      // If this write is for a replication or transfer-RBW/Finalized,\n      // then finalize block or convert temporary to RBW.\n      // For client-writes, the block is finalized in the PacketResponder.\n      if (isDatanode || isTransfer) {\n        // Hold a volume reference to finalize block.\n        try (ReplicaHandler handler = claimReplicaHandler()) {\n          // close the block/crc files\n          close();\n          block.setNumBytes(replicaInfo.getNumBytes());\n\n          if (stage == BlockConstructionStage.TRANSFER_RBW) {\n            // for TRANSFER_RBW, convert temporary to RBW\n            datanode.data.convertTemporaryToRbw(block);\n          } else {\n            // for isDatnode or TRANSFER_FINALIZED\n            // Finalize the block.\n            datanode.data.finalizeBlock(block);\n          }\n        }\n        datanode.metrics.incrBlocksWritten();\n      }\n\n    } catch (IOException ioe) {\n      replicaInfo.releaseAllBytesReserved();\n      if (datanode.isRestarting()) {\n        // Do not throw if shutting down for restart. Otherwise, it will cause\n        // premature termination of responder.\n        LOG.info(\"Shutting down for restart (\" + block + \").\");\n      } else {\n        LOG.info(\"Exception for \" + block, ioe);\n        throw ioe;\n      }\n    } finally {\n      // Clear the previous interrupt state of this thread.\n      Thread.interrupted();\n\n      // If a shutdown for restart was initiated, upstream needs to be notified.\n      // There is no need to do anything special if the responder was closed\n      // normally.\n      if (!responderClosed) { // Data transfer was not complete.\n        if (responder != null) {\n          // In case this datanode is shutting down for quick restart,\n          // send a special ack upstream.\n          if (datanode.isRestarting() && isClient && !isTransfer) {\n            try (Writer out = new OutputStreamWriter(\n                replicaInfo.createRestartMetaStream(), \"UTF-8\")) {\n              // write out the current time.\n              out.write(Long.toString(Time.now() + restartBudget));\n              out.flush();\n            } catch (IOException ioe) {\n              // The worst case is not recovering this RBW replica. \n              // Client will fall back to regular pipeline recovery.\n            } finally {\n              IOUtils.closeStream(streams.getDataOut());\n            }\n            try {              \n              // Even if the connection is closed after the ack packet is\n              // flushed, the client can react to the connection closure \n              // first. Insert a delay to lower the chance of client \n              // missing the OOB ack.\n              Thread.sleep(1000);\n            } catch (InterruptedException ie) {\n              // It is already going down. Ignore this.\n            }\n          }\n          responder.interrupt();\n        }\n        IOUtils.closeStream(this);\n        cleanupBlock();\n      }\n      if (responder != null) {\n        try {\n          responder.interrupt();\n          // join() on the responder should timeout a bit earlier than the\n          // configured deadline. Otherwise, the join() on this thread will\n          // likely timeout as well.\n          long joinTimeout = datanode.getDnConf().getXceiverStopTimeout();\n          joinTimeout = joinTimeout > 1  ? joinTimeout*8/10 : joinTimeout;\n          responder.join(joinTimeout);\n          if (responder.isAlive()) {\n            String msg = \"Join on responder thread \" + responder\n                + \" timed out\";\n            LOG.warn(msg + \"\\n\" + StringUtils.getStackTrace(responder));\n            throw new IOException(msg);\n          }\n        } catch (InterruptedException e) {\n          responder.interrupt();\n          // do not throw if shutting down for restart.\n          if (!datanode.isRestarting()) {\n            throw new IOException(\"Interrupted receiveBlock\");\n          }\n        }\n        responder = null;\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.claimReplicaHandler": "  private ReplicaHandler claimReplicaHandler() {\n    ReplicaHandler handler = replicaHandler;\n    replicaHandler = null;\n    return handler;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.cleanupBlock": "  private void cleanupBlock() throws IOException {\n    if (isDatanode) {\n      datanode.data.unfinalizeBlock(block);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.toString": "    public String toString() {\n      return getClass().getSimpleName() + \"(seqno=\" + seqno\n        + \", lastPacketInBlock=\" + lastPacketInBlock\n        + \", offsetInBlock=\" + offsetInBlock\n        + \", ackEnqueueNanoTime=\" + ackEnqueueNanoTime\n        + \", ackStatus=\" + ackStatus\n        + \")\";\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.close": "    public void close() {\n      synchronized(ackQueue) {\n        while (isRunning() && ackQueue.size() != 0) {\n          try {\n            ackQueue.wait();\n          } catch (InterruptedException e) {\n            running = false;\n            Thread.currentThread().interrupt();\n          }\n        }\n        if(LOG.isDebugEnabled()) {\n          LOG.debug(myString + \": closing\");\n        }\n        running = false;\n        ackQueue.notifyAll();\n      }\n\n      synchronized(this) {\n        running = false;\n        notifyAll();\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock": "  public void writeBlock(final ExtendedBlock block,\n      final StorageType storageType, \n      final Token<BlockTokenIdentifier> blockToken,\n      final String clientname,\n      final DatanodeInfo[] targets,\n      final StorageType[] targetStorageTypes, \n      final DatanodeInfo srcDataNode,\n      final BlockConstructionStage stage,\n      final int pipelineSize,\n      final long minBytesRcvd,\n      final long maxBytesRcvd,\n      final long latestGenerationStamp,\n      DataChecksum requestedChecksum,\n      CachingStrategy cachingStrategy,\n      boolean allowLazyPersist,\n      final boolean pinning,\n      final boolean[] targetPinnings) throws IOException {\n    previousOpClientName = clientname;\n    updateCurrentThreadName(\"Receiving block \" + block);\n    final boolean isDatanode = clientname.length() == 0;\n    final boolean isClient = !isDatanode;\n    final boolean isTransfer = stage == BlockConstructionStage.TRANSFER_RBW\n        || stage == BlockConstructionStage.TRANSFER_FINALIZED;\n    allowLazyPersist = allowLazyPersist &&\n        (dnConf.getAllowNonLocalLazyPersist() || peer.isLocal());\n    long size = 0;\n    // reply to upstream datanode or client \n    final DataOutputStream replyOut = getBufferedOutputStream();\n    checkAccess(replyOut, isClient, block, blockToken,\n        Op.WRITE_BLOCK, BlockTokenIdentifier.AccessMode.WRITE);\n    // check single target for transfer-RBW/Finalized \n    if (isTransfer && targets.length > 0) {\n      throw new IOException(stage + \" does not support multiple targets \"\n          + Arrays.asList(targets));\n    }\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"opWriteBlock: stage=\" + stage + \", clientname=\" + clientname \n      \t\t+ \"\\n  block  =\" + block + \", newGs=\" + latestGenerationStamp\n      \t\t+ \", bytesRcvd=[\" + minBytesRcvd + \", \" + maxBytesRcvd + \"]\"\n          + \"\\n  targets=\" + Arrays.asList(targets)\n          + \"; pipelineSize=\" + pipelineSize + \", srcDataNode=\" + srcDataNode\n          + \", pinning=\" + pinning);\n      LOG.debug(\"isDatanode=\" + isDatanode\n          + \", isClient=\" + isClient\n          + \", isTransfer=\" + isTransfer);\n      LOG.debug(\"writeBlock receive buf size \" + peer.getReceiveBufferSize() +\n                \" tcp no delay \" + peer.getTcpNoDelay());\n    }\n\n    // We later mutate block's generation stamp and length, but we need to\n    // forward the original version of the block to downstream mirrors, so\n    // make a copy here.\n    final ExtendedBlock originalBlock = new ExtendedBlock(block);\n    if (block.getNumBytes() == 0) {\n      block.setNumBytes(dataXceiverServer.estimateBlockSize);\n    }\n    LOG.info(\"Receiving \" + block + \" src: \" + remoteAddress + \" dest: \"\n        + localAddress);\n\n    DataOutputStream mirrorOut = null;  // stream to next target\n    DataInputStream mirrorIn = null;    // reply from next target\n    Socket mirrorSock = null;           // socket to next target\n    String mirrorNode = null;           // the name:port of next target\n    String firstBadLink = \"\";           // first datanode that failed in connection setup\n    Status mirrorInStatus = SUCCESS;\n    final String storageUuid;\n    final boolean isOnTransientStorage;\n    try {\n      final Replica replica;\n      if (isDatanode || \n          stage != BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        // open a block receiver\n        setCurrentBlockReceiver(getBlockReceiver(block, storageType, in,\n            peer.getRemoteAddressString(),\n            peer.getLocalAddressString(),\n            stage, latestGenerationStamp, minBytesRcvd, maxBytesRcvd,\n            clientname, srcDataNode, datanode, requestedChecksum,\n            cachingStrategy, allowLazyPersist, pinning));\n        replica = blockReceiver.getReplica();\n      } else {\n        replica = datanode.data.recoverClose(\n            block, latestGenerationStamp, minBytesRcvd);\n      }\n      storageUuid = replica.getStorageUuid();\n      isOnTransientStorage = replica.isOnTransientStorage();\n\n      //\n      // Connect to downstream machine, if appropriate\n      //\n      if (targets.length > 0) {\n        InetSocketAddress mirrorTarget = null;\n        // Connect to backup machine\n        mirrorNode = targets[0].getXferAddr(connectToDnViaHostname);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Connecting to datanode \" + mirrorNode);\n        }\n        mirrorTarget = NetUtils.createSocketAddr(mirrorNode);\n        mirrorSock = datanode.newSocket();\n        try {\n\n          DataNodeFaultInjector.get().failMirrorConnection();\n\n          int timeoutValue = dnConf.socketTimeout +\n              (HdfsConstants.READ_TIMEOUT_EXTENSION * targets.length);\n          int writeTimeout = dnConf.socketWriteTimeout +\n              (HdfsConstants.WRITE_TIMEOUT_EXTENSION * targets.length);\n          NetUtils.connect(mirrorSock, mirrorTarget, timeoutValue);\n          mirrorSock.setTcpNoDelay(dnConf.getDataTransferServerTcpNoDelay());\n          mirrorSock.setSoTimeout(timeoutValue);\n          mirrorSock.setKeepAlive(true);\n          if (dnConf.getTransferSocketSendBufferSize() > 0) {\n            mirrorSock.setSendBufferSize(\n                dnConf.getTransferSocketSendBufferSize());\n          }\n\n          OutputStream unbufMirrorOut = NetUtils.getOutputStream(mirrorSock,\n              writeTimeout);\n          InputStream unbufMirrorIn = NetUtils.getInputStream(mirrorSock);\n          DataEncryptionKeyFactory keyFactory =\n            datanode.getDataEncryptionKeyFactoryForBlock(block);\n          IOStreamPair saslStreams = datanode.saslClient.socketSend(mirrorSock,\n            unbufMirrorOut, unbufMirrorIn, keyFactory, blockToken, targets[0]);\n          unbufMirrorOut = saslStreams.out;\n          unbufMirrorIn = saslStreams.in;\n          mirrorOut = new DataOutputStream(new BufferedOutputStream(unbufMirrorOut,\n              smallBufferSize));\n          mirrorIn = new DataInputStream(unbufMirrorIn);\n\n          if (targetPinnings != null && targetPinnings.length > 0) {\n            new Sender(mirrorOut).writeBlock(originalBlock, targetStorageTypes[0],\n              blockToken, clientname, targets, targetStorageTypes, srcDataNode,\n              stage, pipelineSize, minBytesRcvd, maxBytesRcvd,\n              latestGenerationStamp, requestedChecksum, cachingStrategy,\n                allowLazyPersist, targetPinnings[0], targetPinnings);\n          } else {\n            new Sender(mirrorOut).writeBlock(originalBlock, targetStorageTypes[0],\n              blockToken, clientname, targets, targetStorageTypes, srcDataNode,\n              stage, pipelineSize, minBytesRcvd, maxBytesRcvd,\n              latestGenerationStamp, requestedChecksum, cachingStrategy,\n                allowLazyPersist, false, targetPinnings);\n          }\n\n          mirrorOut.flush();\n\n          DataNodeFaultInjector.get().writeBlockAfterFlush();\n\n          // read connect ack (only for clients, not for replication req)\n          if (isClient) {\n            BlockOpResponseProto connectAck =\n              BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(mirrorIn));\n            mirrorInStatus = connectAck.getStatus();\n            firstBadLink = connectAck.getFirstBadLink();\n            if (LOG.isDebugEnabled() || mirrorInStatus != SUCCESS) {\n              LOG.debug(\"Datanode \" + targets.length +\n                       \" got response for connect ack \" +\n                       \" from downstream datanode with firstbadlink as \" +\n                       firstBadLink);\n            }\n          }\n\n        } catch (IOException e) {\n          if (isClient) {\n            BlockOpResponseProto.newBuilder()\n              .setStatus(ERROR)\n               // NB: Unconditionally using the xfer addr w/o hostname\n              .setFirstBadLink(targets[0].getXferAddr())\n              .build()\n              .writeDelimitedTo(replyOut);\n            replyOut.flush();\n          }\n          IOUtils.closeStream(mirrorOut);\n          mirrorOut = null;\n          IOUtils.closeStream(mirrorIn);\n          mirrorIn = null;\n          IOUtils.closeSocket(mirrorSock);\n          mirrorSock = null;\n          if (isClient) {\n            LOG.error(datanode + \":Exception transfering block \" +\n                      block + \" to mirror \" + mirrorNode + \": \" + e);\n            throw e;\n          } else {\n            LOG.info(datanode + \":Exception transfering \" +\n                     block + \" to mirror \" + mirrorNode +\n                     \"- continuing without the mirror\", e);\n            incrDatanodeNetworkErrors();\n          }\n        }\n      }\n\n      // send connect-ack to source for clients and not transfer-RBW/Finalized\n      if (isClient && !isTransfer) {\n        if (LOG.isDebugEnabled() || mirrorInStatus != SUCCESS) {\n          LOG.debug(\"Datanode \" + targets.length +\n                   \" forwarding connect ack to upstream firstbadlink is \" +\n                   firstBadLink);\n        }\n        BlockOpResponseProto.newBuilder()\n          .setStatus(mirrorInStatus)\n          .setFirstBadLink(firstBadLink)\n          .build()\n          .writeDelimitedTo(replyOut);\n        replyOut.flush();\n      }\n\n      // receive the block and mirror to the next target\n      if (blockReceiver != null) {\n        String mirrorAddr = (mirrorSock == null) ? null : mirrorNode;\n        blockReceiver.receiveBlock(mirrorOut, mirrorIn, replyOut,\n            mirrorAddr, null, targets, false);\n\n        // send close-ack for transfer-RBW/Finalized \n        if (isTransfer) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"TRANSFER: send close-ack\");\n          }\n          writeResponse(SUCCESS, null, replyOut);\n        }\n      }\n\n      // update its generation stamp\n      if (isClient && \n          stage == BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        block.setGenerationStamp(latestGenerationStamp);\n        block.setNumBytes(minBytesRcvd);\n      }\n      \n      // if this write is for a replication request or recovering\n      // a failed close for client, then confirm block. For other client-writes,\n      // the block is finalized in the PacketResponder.\n      if (isDatanode ||\n          stage == BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        datanode.closeBlock(block, null, storageUuid, isOnTransientStorage);\n        LOG.info(\"Received \" + block + \" src: \" + remoteAddress + \" dest: \"\n            + localAddress + \" of size \" + block.getNumBytes());\n      }\n\n      if(isClient) {\n        size = block.getNumBytes();\n      }\n    } catch (IOException ioe) {\n      LOG.info(\"opWriteBlock \" + block + \" received exception \" + ioe);\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      // close all opened streams\n      IOUtils.closeStream(mirrorOut);\n      IOUtils.closeStream(mirrorIn);\n      IOUtils.closeStream(replyOut);\n      IOUtils.closeSocket(mirrorSock);\n      IOUtils.closeStream(blockReceiver);\n      setCurrentBlockReceiver(null);\n    }\n\n    //update metrics\n    datanode.getMetrics().addWriteBlockOp(elapsed());\n    datanode.getMetrics().incrWritesFromClient(peer.isLocal(), size);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBlockReceiver": "  BlockReceiver getBlockReceiver(\n      final ExtendedBlock block, final StorageType storageType,\n      final DataInputStream in,\n      final String inAddr, final String myAddr,\n      final BlockConstructionStage stage,\n      final long newGs, final long minBytesRcvd, final long maxBytesRcvd,\n      final String clientname, final DatanodeInfo srcDataNode,\n      final DataNode dn, DataChecksum requestedChecksum,\n      CachingStrategy cachingStrategy,\n      final boolean allowLazyPersist,\n      final boolean pinning) throws IOException {\n    return new BlockReceiver(block, storageType, in,\n        inAddr, myAddr, stage, newGs, minBytesRcvd, maxBytesRcvd,\n        clientname, srcDataNode, dn, requestedChecksum,\n        cachingStrategy, allowLazyPersist, pinning);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.incrDatanodeNetworkErrors": "  private void incrDatanodeNetworkErrors() {\n    datanode.incrDatanodeNetworkErrors(remoteAddressWithoutPort);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBufferedOutputStream": "  DataOutputStream getBufferedOutputStream() {\n    return new DataOutputStream(\n        new BufferedOutputStream(getOutputStream(), smallBufferSize));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.elapsed": "  private long elapsed() {\n    return monotonicNow() - opStartTime;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeResponse": "  private static void writeResponse(Status status, String message, OutputStream out)\n  throws IOException {\n    BlockOpResponseProto.Builder response = BlockOpResponseProto.newBuilder()\n      .setStatus(status);\n    if (message != null) {\n      response.setMessage(message);\n    }\n    response.build().writeDelimitedTo(out);\n    out.flush();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess": "  private void checkAccess(OutputStream out, final boolean reply, \n      final ExtendedBlock blk,\n      final Token<BlockTokenIdentifier> t,\n      final Op op,\n      final BlockTokenIdentifier.AccessMode mode) throws IOException {\n    checkAndWaitForBP(blk);\n    if (datanode.isBlockTokenEnabled) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Checking block access token for block '\" + blk.getBlockId()\n            + \"' with mode '\" + mode + \"'\");\n      }\n      try {\n        datanode.blockPoolTokenSecretManager.checkAccess(t, null, blk, mode);\n      } catch(InvalidToken e) {\n        try {\n          if (reply) {\n            BlockOpResponseProto.Builder resp = BlockOpResponseProto.newBuilder()\n              .setStatus(ERROR_ACCESS_TOKEN);\n            if (mode == BlockTokenIdentifier.AccessMode.WRITE) {\n              DatanodeRegistration dnR = \n                datanode.getDNRegistrationForBP(blk.getBlockPoolId());\n              // NB: Unconditionally using the xfer addr w/o hostname\n              resp.setFirstBadLink(dnR.getXferAddr());\n            }\n            resp.build().writeDelimitedTo(out);\n            out.flush();\n          }\n          LOG.warn(\"Block token verification failed: op=\" + op\n              + \", remoteAddress=\" + remoteAddress\n              + \", message=\" + e.getLocalizedMessage());\n          throw e;\n        } finally {\n          IOUtils.closeStream(out);\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.updateCurrentThreadName": "  private void updateCurrentThreadName(String status) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"DataXceiver for client \");\n    if (previousOpClientName != null) {\n      sb.append(previousOpClientName).append(\" at \");\n    }\n    sb.append(remoteAddress);\n    if (status != null) {\n      sb.append(\" [\").append(status).append(\"]\");\n    }\n    Thread.currentThread().setName(sb.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.getOutputStream": "  private OutputStream getOutputStream() {\n    return socketOut;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.setCurrentBlockReceiver": "  private synchronized void setCurrentBlockReceiver(BlockReceiver br) {\n    blockReceiver = br;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock": "  private void opWriteBlock(DataInputStream in) throws IOException {\n    final OpWriteBlockProto proto = OpWriteBlockProto.parseFrom(vintPrefixed(in));\n    final DatanodeInfo[] targets = PBHelperClient.convert(proto.getTargetsList());\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      writeBlock(PBHelperClient.convert(proto.getHeader().getBaseHeader().getBlock()),\n          PBHelperClient.convertStorageType(proto.getStorageType()),\n          PBHelperClient.convert(proto.getHeader().getBaseHeader().getToken()),\n          proto.getHeader().getClientName(),\n          targets,\n          PBHelperClient.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length),\n          PBHelperClient.convert(proto.getSource()),\n          fromProto(proto.getStage()),\n          proto.getPipelineSize(),\n          proto.getMinBytesRcvd(), proto.getMaxBytesRcvd(),\n          proto.getLatestGenerationStamp(),\n          fromProto(proto.getRequestedChecksum()),\n          (proto.hasCachingStrategy() ?\n              getCachingStrategy(proto.getCachingStrategy()) :\n            CachingStrategy.newDefaultStrategy()),\n          (proto.hasAllowLazyPersist() ? proto.getAllowLazyPersist() : false),\n          (proto.hasPinning() ? proto.getPinning(): false),\n          (PBHelperClient.convertBooleanList(proto.getTargetPinningsList())));\n    } finally {\n     if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.continueTraceSpan": "  private TraceScope continueTraceSpan(BaseHeaderProto header,\n                                             String description) {\n    return continueTraceSpan(header.getTraceInfo(), description);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.getCachingStrategy": "  static private CachingStrategy getCachingStrategy(CachingStrategyProto strategy) {\n    Boolean dropBehind = strategy.hasDropBehind() ?\n        strategy.getDropBehind() : null;\n    Long readahead = strategy.hasReadahead() ?\n        strategy.getReadahead() : null;\n    return new CachingStrategy(dropBehind, readahead);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp": "  protected final void processOp(Op op) throws IOException {\n    switch(op) {\n    case READ_BLOCK:\n      opReadBlock();\n      break;\n    case WRITE_BLOCK:\n      opWriteBlock(in);\n      break;\n    case REPLACE_BLOCK:\n      opReplaceBlock(in);\n      break;\n    case COPY_BLOCK:\n      opCopyBlock(in);\n      break;\n    case BLOCK_CHECKSUM:\n      opBlockChecksum(in);\n      break;\n    case BLOCK_GROUP_CHECKSUM:\n      opStripedBlockChecksum(in);\n      break;\n    case TRANSFER_BLOCK:\n      opTransferBlock(in);\n      break;\n    case REQUEST_SHORT_CIRCUIT_FDS:\n      opRequestShortCircuitFds(in);\n      break;\n    case RELEASE_SHORT_CIRCUIT_FDS:\n      opReleaseShortCircuitFds(in);\n      break;\n    case REQUEST_SHORT_CIRCUIT_SHM:\n      opRequestShortCircuitShm(in);\n      break;\n    default:\n      throw new IOException(\"Unknown op \" + op + \" in data stream\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReplaceBlock": "  private void opReplaceBlock(DataInputStream in) throws IOException {\n    OpReplaceBlockProto proto = OpReplaceBlockProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      replaceBlock(PBHelperClient.convert(proto.getHeader().getBlock()),\n          PBHelperClient.convertStorageType(proto.getStorageType()),\n          PBHelperClient.convert(proto.getHeader().getToken()),\n          proto.getDelHint(),\n          PBHelperClient.convert(proto.getSource()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitFds": "  private void opRequestShortCircuitFds(DataInputStream in) throws IOException {\n    final OpRequestShortCircuitAccessProto proto =\n      OpRequestShortCircuitAccessProto.parseFrom(vintPrefixed(in));\n    SlotId slotId = (proto.hasSlotId()) ? \n        PBHelperClient.convert(proto.getSlotId()) : null;\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      requestShortCircuitFds(PBHelperClient.convert(proto.getHeader().getBlock()),\n          PBHelperClient.convert(proto.getHeader().getToken()),\n          slotId, proto.getMaxVersion(),\n          proto.getSupportsReceiptVerification());\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitShm": "  private void opRequestShortCircuitShm(DataInputStream in) throws IOException {\n    final ShortCircuitShmRequestProto proto =\n        ShortCircuitShmRequestProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getTraceInfo(),\n        proto.getClass().getSimpleName());\n    try {\n      requestShortCircuitShm(proto.getClientName());\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opBlockChecksum": "  private void opBlockChecksum(DataInputStream in) throws IOException {\n    OpBlockChecksumProto proto = OpBlockChecksumProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n    blockChecksum(PBHelperClient.convert(proto.getHeader().getBlock()),\n        PBHelperClient.convert(proto.getHeader().getToken()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReleaseShortCircuitFds": "  private void opReleaseShortCircuitFds(DataInputStream in)\n      throws IOException {\n    final ReleaseShortCircuitAccessRequestProto proto =\n      ReleaseShortCircuitAccessRequestProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getTraceInfo(),\n        proto.getClass().getSimpleName());\n    try {\n      releaseShortCircuitFds(PBHelperClient.convert(proto.getSlotId()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum": "  private void opStripedBlockChecksum(DataInputStream dis) throws IOException {\n    OpBlockGroupChecksumProto proto =\n        OpBlockGroupChecksumProto.parseFrom(vintPrefixed(dis));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    StripedBlockInfo stripedBlockInfo = new StripedBlockInfo(\n        PBHelperClient.convert(proto.getHeader().getBlock()),\n        PBHelperClient.convert(proto.getDatanodes()),\n        PBHelperClient.convertTokens(proto.getBlockTokensList()),\n        PBHelperClient.convertBlockIndices(proto.getBlockIndicesList()),\n        PBHelperClient.convertErasureCodingPolicy(proto.getEcPolicy())\n    );\n\n    try {\n      blockGroupChecksum(stripedBlockInfo,\n          PBHelperClient.convert(proto.getHeader().getToken()),\n          proto.getRequestedNumBytes());\n    } finally {\n      if (traceScope != null) {\n        traceScope.close();\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opTransferBlock": "  private void opTransferBlock(DataInputStream in) throws IOException {\n    final OpTransferBlockProto proto =\n      OpTransferBlockProto.parseFrom(vintPrefixed(in));\n    final DatanodeInfo[] targets = PBHelperClient.convert(proto.getTargetsList());\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      transferBlock(PBHelperClient.convert(proto.getHeader().getBaseHeader().getBlock()),\n          PBHelperClient.convert(proto.getHeader().getBaseHeader().getToken()),\n          proto.getHeader().getClientName(),\n          targets,\n          PBHelperClient.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opCopyBlock": "  private void opCopyBlock(DataInputStream in) throws IOException {\n    OpCopyBlockProto proto = OpCopyBlockProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      copyBlock(PBHelperClient.convert(proto.getHeader().getBlock()),\n          PBHelperClient.convert(proto.getHeader().getToken()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock": "  private void opReadBlock() throws IOException {\n    OpReadBlockProto proto = OpReadBlockProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      readBlock(PBHelperClient.convert(proto.getHeader().getBaseHeader().getBlock()),\n        PBHelperClient.convert(proto.getHeader().getBaseHeader().getToken()),\n        proto.getHeader().getClientName(),\n        proto.getOffset(),\n        proto.getLen(),\n        proto.getSendChecksums(),\n        (proto.hasCachingStrategy() ?\n            getCachingStrategy(proto.getCachingStrategy()) :\n          CachingStrategy.newDefaultStrategy()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.run": "  public void run() {\n    int opsProcessed = 0;\n    Op op = null;\n\n    try {\n      synchronized(this) {\n        xceiver = Thread.currentThread();\n      }\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input = socketIn;\n      try {\n        IOStreamPair saslStreams = datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input = new BufferedInputStream(saslStreams.in,\n            smallBufferSize);\n        socketOut = saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at \" + peer.getRemoteAddressString() + \n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\");\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed != 0) {\n            assert dnConf.socketKeepaliveTimeout > 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op = readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (EOFException | ClosedChannelException e) {\n          // Since we optimistically expect the next op, it's quite normal to\n          // get EOF here.\n          LOG.debug(\"Cached {} closing after {} ops.  \" +\n              \"This message is usually benign.\", peer, opsProcessed);\n          break;\n        } catch (IOException err) {\n          incrDatanodeNetworkErrors();\n          throw err;\n        }\n\n        // restore normal timeout\n        if (opsProcessed != 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime = monotonicNow();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer != null) &&\n          (!peer.isClosed() && dnConf.socketKeepaliveTimeout > 0));\n    } catch (Throwable t) {\n      String s = datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op == null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op == Op.WRITE_BLOCK && t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op == Op.READ_BLOCK && t instanceof SocketTimeoutException) {\n        String s1 =\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 += \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else if (t instanceof InvalidToken) {\n        // The InvalidToken exception has already been logged in\n        // checkAccess() method and this is not a server error.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      collectThreadLocalStates();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer != null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.collectThreadLocalStates": "  private void collectThreadLocalStates() {\n    if (datanode.getPeerMetrics() != null) {\n      datanode.getPeerMetrics().collectThreadLocalStates();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DatanodeUtil.getCauseIfDiskError": "  static IOException getCauseIfDiskError(IOException ioe) {\n    if (ioe.getMessage()!=null && ioe.getMessage().startsWith(DISK_ERROR)) {\n      return (IOException)ioe.getCause();\n    } else {\n      return null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    final TraceScope scope = datanode.getTracer().\n        newScope(\"sendBlock_\" + block.getBlockId());\n    try {\n      return doSendBlock(out, baseStream, throttler);\n    } finally {\n      scope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock": "  private long doSendBlock(DataOutputStream out, OutputStream baseStream,\n        DataTransferThrottler throttler) throws IOException {\n    if (out == null) {\n      throw new IOException( \"out stream is null\" );\n    }\n    initialOffset = offset;\n    long totalRead = 0;\n    OutputStream streamForSendChunks = out;\n    \n    lastCacheDropOffset = initialOffset;\n\n    if (isLongRead() && ris.getDataInFd() != null) {\n      // Advise that this file descriptor will be accessed sequentially.\n      ris.dropCacheBehindReads(block.getBlockName(), 0, 0,\n          POSIX_FADV_SEQUENTIAL);\n    }\n    \n    // Trigger readahead of beginning of file if configured.\n    manageOsCache();\n\n    final long startTime = ClientTraceLog.isDebugEnabled() ? System.nanoTime() : 0;\n    try {\n      int maxChunksPerPacket;\n      int pktBufSize = PacketHeader.PKT_MAX_HEADER_LEN;\n      boolean transferTo = transferToAllowed && !verifyChecksum\n          && baseStream instanceof SocketOutputStream\n          && ris.getDataIn() instanceof FileInputStream;\n      if (transferTo) {\n        FileChannel fileChannel =\n            ((FileInputStream)ris.getDataIn()).getChannel();\n        blockInPosition = fileChannel.position();\n        streamForSendChunks = baseStream;\n        maxChunksPerPacket = numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n        \n        // Smaller packet size to only hold checksum when doing transferTo\n        pktBufSize += checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket = Math.max(1,\n            numberOfChunks(IO_FILE_BUFFER_SIZE));\n        // Packet size includes both checksum and data\n        pktBufSize += (chunkSize + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf = ByteBuffer.allocate(pktBufSize);\n\n      while (endOffset > offset && !Thread.currentThread().isInterrupted()) {\n        manageOsCache();\n        long len = sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n            transferTo, throttler);\n        offset += len;\n        totalRead += len + (numberOfChunks(len) * checksumSize);\n        seqno++;\n      }\n      // If this thread was interrupted, then it did not send the full block.\n      if (!Thread.currentThread().isInterrupted()) {\n        try {\n          // send an empty packet to mark the end of the block\n          sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n              throttler);\n          out.flush();\n        } catch (IOException e) { //socket error\n          throw ioeToSocketException(e);\n        }\n\n        sentEntireByteRange = true;\n      }\n    } finally {\n      if ((clientTraceFmt != null) && ClientTraceLog.isDebugEnabled()) {\n        final long endTime = System.nanoTime();\n        ClientTraceLog.debug(String.format(clientTraceFmt, totalRead,\n            initialOffset, endTime - startTime));\n      }\n      close();\n    }\n    return totalRead;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.close": "  public void close() throws IOException {\n    if (ris.getDataInFd() != null &&\n        ((dropCacheBehindAllReads) ||\n         (dropCacheBehindLargeReads && isLongRead()))) {\n      try {\n        ris.dropCacheBehindReads(block.getBlockName(), lastCacheDropOffset,\n            offset - lastCacheDropOffset, POSIX_FADV_DONTNEED);\n      } catch (Exception e) {\n        LOG.warn(\"Unable to drop cache on file close\", e);\n      }\n    }\n    if (curReadahead != null) {\n      curReadahead.cancel();\n    }\n\n    try {\n      ris.closeStreams();\n    } finally {\n      IOUtils.closeStream(ris);\n      ris = null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockScanner.markSuspectBlock": "  synchronized void markSuspectBlock(String storageId, ExtendedBlock block) {\n    if (!isEnabled()) {\n      LOG.debug(\"Not scanning suspicious block {} on {}, because the block \" +\n          \"scanner is disabled.\", block, storageId);\n      return;\n    }\n    VolumeScanner scanner = scanners.get(storageId);\n    if (scanner == null) {\n      // This could happen if the volume is in the process of being removed.\n      // The removal process shuts down the VolumeScanner, but the volume\n      // object stays around as long as there are references to it (which\n      // should not be that long.)\n      LOG.info(\"Not scanning suspicious block {} on {}, because there is no \" +\n          \"volume scanner for that storageId.\", block, storageId);\n      return;\n    }\n    scanner.markSuspectBlock(block);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockScanner.isEnabled": "  public boolean isEnabled() {\n    return (conf.scanPeriodMs > 0) && (conf.targetBytesPerSec > 0);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dnConf.getDataTransferServerTcpNoDelay": "  public boolean getDataTransferServerTcpNoDelay() {\n    return tcpNoDelay;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.getChecksum": "  DataChecksum getChecksum() {\n    return checksum;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader.getHeaderSize": "  public static int getHeaderSize() {\n    return Short.SIZE/Byte.SIZE + DataChecksum.getChecksumHeaderSize();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNodeFaultInjector.get": "  public static DataNodeFaultInjector get() {\n    return instance;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDnConf": "  public DNConf getDnConf() {\n    return dnConf;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.isRestarting": "  boolean isRestarting() {\n    return shutdownForUpgrade;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.newSocket": "  public Socket newSocket() throws IOException {\n    return socketFactory.createSocket();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dnConf.getAllowNonLocalLazyPersist": "  public boolean getAllowNonLocalLazyPersist() {\n    return allowNonLocalLazyPersist;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockReceiver.receiveBlock": "  void receiveBlock(\n      DataOutputStream mirrOut, // output to next datanode\n      DataInputStream mirrIn,   // input from next datanode\n      DataOutputStream replyOut,  // output to previous datanode\n      String mirrAddr, DataTransferThrottler throttlerArg,\n      DatanodeInfo[] downstreams,\n      boolean isReplaceBlock) throws IOException {\n\n    syncOnClose = datanode.getDnConf().syncOnClose;\n    boolean responderClosed = false;\n    mirrorOut = mirrOut;\n    mirrorAddr = mirrAddr;\n    isPenultimateNode = ((downstreams != null) && (downstreams.length == 1));\n    if (isPenultimateNode) {\n      mirrorNameForMetrics = (downstreams[0].getInfoSecurePort() != 0 ?\n          downstreams[0].getInfoSecureAddr() : downstreams[0].getInfoAddr());\n      LOG.debug(\"Will collect peer metrics for downstream node {}\",\n          mirrorNameForMetrics);\n    }\n    throttler = throttlerArg;\n\n    this.replyOut = replyOut;\n    this.isReplaceBlock = isReplaceBlock;\n\n    try {\n      if (isClient && !isTransfer) {\n        responder = new Daemon(datanode.threadGroup, \n            new PacketResponder(replyOut, mirrIn, downstreams));\n        responder.start(); // start thread to processes responses\n      }\n\n      while (receivePacket() >= 0) { /* Receive until the last packet */ }\n\n      // wait for all outstanding packet responses. And then\n      // indicate responder to gracefully shutdown.\n      // Mark that responder has been closed for future processing\n      if (responder != null) {\n        ((PacketResponder)responder.getRunnable()).close();\n        responderClosed = true;\n      }\n\n      // If this write is for a replication or transfer-RBW/Finalized,\n      // then finalize block or convert temporary to RBW.\n      // For client-writes, the block is finalized in the PacketResponder.\n      if (isDatanode || isTransfer) {\n        // Hold a volume reference to finalize block.\n        try (ReplicaHandler handler = claimReplicaHandler()) {\n          // close the block/crc files\n          close();\n          block.setNumBytes(replicaInfo.getNumBytes());\n\n          if (stage == BlockConstructionStage.TRANSFER_RBW) {\n            // for TRANSFER_RBW, convert temporary to RBW\n            datanode.data.convertTemporaryToRbw(block);\n          } else {\n            // for isDatnode or TRANSFER_FINALIZED\n            // Finalize the block.\n            datanode.data.finalizeBlock(block);\n          }\n        }\n        datanode.metrics.incrBlocksWritten();\n      }\n\n    } catch (IOException ioe) {\n      replicaInfo.releaseAllBytesReserved();\n      if (datanode.isRestarting()) {\n        // Do not throw if shutting down for restart. Otherwise, it will cause\n        // premature termination of responder.\n        LOG.info(\"Shutting down for restart (\" + block + \").\");\n      } else {\n        LOG.info(\"Exception for \" + block, ioe);\n        throw ioe;\n      }\n    } finally {\n      // Clear the previous interrupt state of this thread.\n      Thread.interrupted();\n\n      // If a shutdown for restart was initiated, upstream needs to be notified.\n      // There is no need to do anything special if the responder was closed\n      // normally.\n      if (!responderClosed) { // Data transfer was not complete.\n        if (responder != null) {\n          // In case this datanode is shutting down for quick restart,\n          // send a special ack upstream.\n          if (datanode.isRestarting() && isClient && !isTransfer) {\n            try (Writer out = new OutputStreamWriter(\n                replicaInfo.createRestartMetaStream(), \"UTF-8\")) {\n              // write out the current time.\n              out.write(Long.toString(Time.now() + restartBudget));\n              out.flush();\n            } catch (IOException ioe) {\n              // The worst case is not recovering this RBW replica. \n              // Client will fall back to regular pipeline recovery.\n            } finally {\n              IOUtils.closeStream(streams.getDataOut());\n            }\n            try {              \n              // Even if the connection is closed after the ack packet is\n              // flushed, the client can react to the connection closure \n              // first. Insert a delay to lower the chance of client \n              // missing the OOB ack.\n              Thread.sleep(1000);\n            } catch (InterruptedException ie) {\n              // It is already going down. Ignore this.\n            }\n          }\n          responder.interrupt();\n        }\n        IOUtils.closeStream(this);\n        cleanupBlock();\n      }\n      if (responder != null) {\n        try {\n          responder.interrupt();\n          // join() on the responder should timeout a bit earlier than the\n          // configured deadline. Otherwise, the join() on this thread will\n          // likely timeout as well.\n          long joinTimeout = datanode.getDnConf().getXceiverStopTimeout();\n          joinTimeout = joinTimeout > 1  ? joinTimeout*8/10 : joinTimeout;\n          responder.join(joinTimeout);\n          if (responder.isAlive()) {\n            String msg = \"Join on responder thread \" + responder\n                + \" timed out\";\n            LOG.warn(msg + \"\\n\" + StringUtils.getStackTrace(responder));\n            throw new IOException(msg);\n          }\n        } catch (InterruptedException e) {\n          responder.interrupt();\n          // do not throw if shutting down for restart.\n          if (!datanode.isRestarting()) {\n            throw new IOException(\"Interrupted receiveBlock\");\n          }\n        }\n        responder = null;\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockReceiver.claimReplicaHandler": "  private ReplicaHandler claimReplicaHandler() {\n    ReplicaHandler handler = replicaHandler;\n    replicaHandler = null;\n    return handler;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockReceiver.cleanupBlock": "  private void cleanupBlock() throws IOException {\n    if (isDatanode) {\n      datanode.data.unfinalizeBlock(block);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockReceiver.toString": "    public String toString() {\n      return getClass().getSimpleName() + \"(seqno=\" + seqno\n        + \", lastPacketInBlock=\" + lastPacketInBlock\n        + \", offsetInBlock=\" + offsetInBlock\n        + \", ackEnqueueNanoTime=\" + ackEnqueueNanoTime\n        + \", ackStatus=\" + ackStatus\n        + \")\";\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockReceiver.close": "    public void close() {\n      synchronized(ackQueue) {\n        while (isRunning() && ackQueue.size() != 0) {\n          try {\n            ackQueue.wait();\n          } catch (InterruptedException e) {\n            running = false;\n            Thread.currentThread().interrupt();\n          }\n        }\n        if(LOG.isDebugEnabled()) {\n          LOG.debug(myString + \": closing\");\n        }\n        running = false;\n        ackQueue.notifyAll();\n      }\n\n      synchronized(this) {\n        running = false;\n        notifyAll();\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockReceiver.receivePacket": "  private int receivePacket() throws IOException {\n    // read the next packet\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader header = packetReceiver.getHeader();\n    if (LOG.isDebugEnabled()){\n      LOG.debug(\"Receiving one packet for block \" + block +\n                \": \" + header);\n    }\n\n    // Sanity check the header\n    if (header.getOffsetInBlock() > replicaInfo.getNumBytes()) {\n      throw new IOException(\"Received an out-of-sequence packet for \" + block + \n          \"from \" + inAddr + \" at offset \" + header.getOffsetInBlock() +\n          \". Expecting packet starting at \" + replicaInfo.getNumBytes());\n    }\n    if (header.getDataLen() < 0) {\n      throw new IOException(\"Got wrong length during writeBlock(\" + block + \n                            \") from \" + inAddr + \" at offset \" + \n                            header.getOffsetInBlock() + \": \" +\n                            header.getDataLen()); \n    }\n\n    long offsetInBlock = header.getOffsetInBlock();\n    long seqno = header.getSeqno();\n    boolean lastPacketInBlock = header.isLastPacketInBlock();\n    final int len = header.getDataLen();\n    boolean syncBlock = header.getSyncBlock();\n\n    // avoid double sync'ing on close\n    if (syncBlock && lastPacketInBlock) {\n      this.syncOnClose = false;\n    }\n\n    // update received bytes\n    final long firstByteInBlock = offsetInBlock;\n    offsetInBlock += len;\n    if (replicaInfo.getNumBytes() < offsetInBlock) {\n      replicaInfo.setNumBytes(offsetInBlock);\n    }\n    \n    // put in queue for pending acks, unless sync was requested\n    if (responder != null && !syncBlock && !shouldVerifyChecksum()) {\n      ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n          lastPacketInBlock, offsetInBlock, Status.SUCCESS);\n    }\n\n    // Drop heartbeat for testing.\n    if (seqno < 0 && len == 0 &&\n        DataNodeFaultInjector.get().dropHeartbeatPacket()) {\n      return 0;\n    }\n\n    //First write the packet to the mirror:\n    if (mirrorOut != null && !mirrorError) {\n      try {\n        long begin = Time.monotonicNow();\n        // For testing. Normally no-op.\n        DataNodeFaultInjector.get().stopSendingPacketDownstream(mirrorAddr);\n        packetReceiver.mirrorPacketTo(mirrorOut);\n        mirrorOut.flush();\n        long now = Time.monotonicNow();\n        setLastSentTime(now);\n        long duration = now - begin;\n        DataNodeFaultInjector.get().logDelaySendingPacketDownstream(\n            mirrorAddr,\n            duration);\n        trackSendPacketToLastNodeInPipeline(duration);\n        if (duration > datanodeSlowLogThresholdMs) {\n          LOG.warn(\"Slow BlockReceiver write packet to mirror took \" + duration\n              + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms)\");\n        }\n      } catch (IOException e) {\n        handleMirrorOutError(e);\n      }\n    }\n    \n    ByteBuffer dataBuf = packetReceiver.getDataSlice();\n    ByteBuffer checksumBuf = packetReceiver.getChecksumSlice();\n    \n    if (lastPacketInBlock || len == 0) {\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"Receiving an empty packet or the end of the block \" + block);\n      }\n      // sync block if requested\n      if (syncBlock) {\n        flushOrSync(true);\n      }\n    } else {\n      final int checksumLen = diskChecksum.getChecksumSize(len);\n      final int checksumReceivedLen = checksumBuf.capacity();\n\n      if (checksumReceivedLen > 0 && checksumReceivedLen != checksumLen) {\n        throw new IOException(\"Invalid checksum length: received length is \"\n            + checksumReceivedLen + \" but expected length is \" + checksumLen);\n      }\n\n      if (checksumReceivedLen > 0 && shouldVerifyChecksum()) {\n        try {\n          verifyChunks(dataBuf, checksumBuf);\n        } catch (IOException ioe) {\n          // checksum error detected locally. there is no reason to continue.\n          if (responder != null) {\n            try {\n              ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n                  lastPacketInBlock, offsetInBlock,\n                  Status.ERROR_CHECKSUM);\n              // Wait until the responder sends back the response\n              // and interrupt this thread.\n              Thread.sleep(3000);\n            } catch (InterruptedException e) { }\n          }\n          throw new IOException(\"Terminating due to a checksum error.\" + ioe);\n        }\n \n        if (needsChecksumTranslation) {\n          // overwrite the checksums in the packet buffer with the\n          // appropriate polynomial for the disk storage.\n          translateChunks(dataBuf, checksumBuf);\n        }\n      }\n\n      if (checksumReceivedLen == 0 && !streams.isTransientStorage()) {\n        // checksum is missing, need to calculate it\n        checksumBuf = ByteBuffer.allocate(checksumLen);\n        diskChecksum.calculateChunkedSums(dataBuf, checksumBuf);\n      }\n      \n      // by this point, the data in the buffer uses the disk checksum\n\n      final boolean shouldNotWriteChecksum = checksumReceivedLen == 0\n          && streams.isTransientStorage();\n      try {\n        long onDiskLen = replicaInfo.getBytesOnDisk();\n        if (onDiskLen<offsetInBlock) {\n          // Normally the beginning of an incoming packet is aligned with the\n          // existing data on disk. If the beginning packet data offset is not\n          // checksum chunk aligned, the end of packet will not go beyond the\n          // next chunk boundary.\n          // When a failure-recovery is involved, the client state and the\n          // the datanode state may not exactly agree. I.e. the client may\n          // resend part of data that is already on disk. Correct number of\n          // bytes should be skipped when writing the data and checksum\n          // buffers out to disk.\n          long partialChunkSizeOnDisk = onDiskLen % bytesPerChecksum;\n          long lastChunkBoundary = onDiskLen - partialChunkSizeOnDisk;\n          boolean alignedOnDisk = partialChunkSizeOnDisk == 0;\n          boolean alignedInPacket = firstByteInBlock % bytesPerChecksum == 0;\n\n          // If the end of the on-disk data is not chunk-aligned, the last\n          // checksum needs to be overwritten.\n          boolean overwriteLastCrc = !alignedOnDisk && !shouldNotWriteChecksum;\n          // If the starting offset of the packat data is at the last chunk\n          // boundary of the data on disk, the partial checksum recalculation\n          // can be skipped and the checksum supplied by the client can be used\n          // instead. This reduces disk reads and cpu load.\n          boolean doCrcRecalc = overwriteLastCrc &&\n              (lastChunkBoundary != firstByteInBlock);\n\n          // If this is a partial chunk, then verify that this is the only\n          // chunk in the packet. If the starting offset is not chunk\n          // aligned, the packet should terminate at or before the next\n          // chunk boundary.\n          if (!alignedInPacket && len > bytesPerChecksum) {\n            throw new IOException(\"Unexpected packet data length for \"\n                +  block + \" from \" + inAddr + \": a partial chunk must be \"\n                + \" sent in an individual packet (data length = \" + len\n                +  \" > bytesPerChecksum = \" + bytesPerChecksum + \")\");\n          }\n\n          // If the last portion of the block file is not a full chunk,\n          // then read in pre-existing partial data chunk and recalculate\n          // the checksum so that the checksum calculation can continue\n          // from the right state. If the client provided the checksum for\n          // the whole chunk, this is not necessary.\n          Checksum partialCrc = null;\n          if (doCrcRecalc) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"receivePacket for \" + block \n                  + \": previous write did not end at the chunk boundary.\"\n                  + \" onDiskLen=\" + onDiskLen);\n            }\n            long offsetInChecksum = BlockMetadataHeader.getHeaderSize() +\n                onDiskLen / bytesPerChecksum * checksumSize;\n            partialCrc = computePartialChunkCrc(onDiskLen, offsetInChecksum);\n          }\n\n          // The data buffer position where write will begin. If the packet\n          // data and on-disk data have no overlap, this will not be at the\n          // beginning of the buffer.\n          int startByteToDisk = (int)(onDiskLen-firstByteInBlock) \n              + dataBuf.arrayOffset() + dataBuf.position();\n\n          // Actual number of data bytes to write.\n          int numBytesToDisk = (int)(offsetInBlock-onDiskLen);\n          \n          // Write data to disk.\n          long begin = Time.monotonicNow();\n          streams.writeDataToDisk(dataBuf.array(),\n              startByteToDisk, numBytesToDisk);\n          long duration = Time.monotonicNow() - begin;\n\n          if (duration > maxWriteToDiskMs) {\n            maxWriteToDiskMs = duration;\n          }\n\n          final byte[] lastCrc;\n          if (shouldNotWriteChecksum) {\n            lastCrc = null;\n          } else {\n            int skip = 0;\n            byte[] crcBytes = null;\n\n            // First, prepare to overwrite the partial crc at the end.\n            if (overwriteLastCrc) { // not chunk-aligned on disk\n              // prepare to overwrite last checksum\n              adjustCrcFilePosition();\n            }\n\n            // The CRC was recalculated for the last partial chunk. Update the\n            // CRC by reading the rest of the chunk, then write it out.\n            if (doCrcRecalc) {\n              // Calculate new crc for this chunk.\n              int bytesToReadForRecalc =\n                  (int)(bytesPerChecksum - partialChunkSizeOnDisk);\n              if (numBytesToDisk < bytesToReadForRecalc) {\n                bytesToReadForRecalc = numBytesToDisk;\n              }\n\n              partialCrc.update(dataBuf.array(), startByteToDisk,\n                  bytesToReadForRecalc);\n              byte[] buf = FSOutputSummer.convertToByteStream(partialCrc,\n                  checksumSize);\n              crcBytes = copyLastChunkChecksum(buf, checksumSize, buf.length);\n              checksumOut.write(buf);\n              if(LOG.isDebugEnabled()) {\n                LOG.debug(\"Writing out partial crc for data len \" + len +\n                    \", skip=\" + skip);\n              }\n              skip++; //  For the partial chunk that was just read.\n            }\n\n            // Determine how many checksums need to be skipped up to the last\n            // boundary. The checksum after the boundary was already counted\n            // above. Only count the number of checksums skipped up to the\n            // boundary here.\n            long skippedDataBytes = lastChunkBoundary - firstByteInBlock;\n\n            if (skippedDataBytes > 0) {\n              skip += (int)(skippedDataBytes / bytesPerChecksum) +\n                  ((skippedDataBytes % bytesPerChecksum == 0) ? 0 : 1);\n            }\n            skip *= checksumSize; // Convert to number of bytes\n\n            // write the rest of checksum\n            final int offset = checksumBuf.arrayOffset() +\n                checksumBuf.position() + skip;\n            final int end = offset + checksumLen - skip;\n            // If offset >= end, there is no more checksum to write.\n            // I.e. a partial chunk checksum rewrite happened and there is no\n            // more to write after that.\n            if (offset >= end && doCrcRecalc) {\n              lastCrc = crcBytes;\n            } else {\n              final int remainingBytes = checksumLen - skip;\n              lastCrc = copyLastChunkChecksum(checksumBuf.array(),\n                  checksumSize, end);\n              checksumOut.write(checksumBuf.array(), offset, remainingBytes);\n            }\n          }\n\n          /// flush entire packet, sync if requested\n          flushOrSync(syncBlock);\n          \n          replicaInfo.setLastChecksumAndDataLen(offsetInBlock, lastCrc);\n\n          datanode.metrics.incrBytesWritten(len);\n          datanode.metrics.incrTotalWriteTime(duration);\n\n          manageWriterOsCache(offsetInBlock);\n        }\n      } catch (IOException iex) {\n        // Volume error check moved to FileIoProvider\n        throw iex;\n      }\n    }\n\n    // if sync was requested, put in queue for pending acks here\n    // (after the fsync finished)\n    if (responder != null && (syncBlock || shouldVerifyChecksum())) {\n      ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n          lastPacketInBlock, offsetInBlock, Status.SUCCESS);\n    }\n\n    /*\n     * Send in-progress responses for the replaceBlock() calls back to caller to\n     * avoid timeouts due to balancer throttling. HDFS-6247\n     */\n    if (isReplaceBlock\n        && (Time.monotonicNow() - lastResponseTime > responseInterval)) {\n      BlockOpResponseProto.Builder response = BlockOpResponseProto.newBuilder()\n          .setStatus(Status.IN_PROGRESS);\n      response.build().writeDelimitedTo(replyOut);\n      replyOut.flush();\n\n      lastResponseTime = Time.monotonicNow();\n    }\n\n    if (throttler != null) { // throttle I/O\n      throttler.throttle(len);\n    }\n    \n    return lastPacketInBlock?-1:len;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getMetrics": "  public DataNodeMetrics getMetrics() {\n    return metrics;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockReceiver.getReplica": "  Replica getReplica() {\n    return replicaInfo;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.Replica.isOnTransientStorage": "  public boolean isOnTransientStorage();\n}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dnConf.getTransferSocketSendBufferSize": "  public int getTransferSocketSendBufferSize() {\n    return transferSocketSendBufferSize;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.closeBlock": "  void closeBlock(ExtendedBlock block, String delHint, String storageUuid,\n      boolean isTransientStorage) {\n    metrics.incrBlocksWritten();\n    notifyNamenodeReceivedBlock(block, delHint, storageUuid,\n        isTransientStorage);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.notifyNamenodeReceivedBlock": "  public void notifyNamenodeReceivedBlock(ExtendedBlock block, String delHint,\n      String storageUuid, boolean isOnTransientStorage) {\n    BPOfferService bpos = blockPoolManager.get(block.getBlockPoolId());\n    if(bpos != null) {\n      bpos.notifyNamenodeReceivedBlock(block, delHint, storageUuid,\n          isOnTransientStorage);\n    } else {\n      LOG.error(\"Cannot find BPOfferService for reporting block received for bpid=\"\n          + block.getBlockPoolId());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.Replica.getStorageUuid": "  public String getStorageUuid();\n\n  /**\n   * Return true if the target volume is backed by RAM.\n   */\n  public boolean isOnTransientStorage();\n}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDataEncryptionKeyFactoryForBlock": "  public DataEncryptionKeyFactory getDataEncryptionKeyFactoryForBlock(\n      final ExtendedBlock block) {\n    return new DataEncryptionKeyFactory() {\n      @Override\n      public DataEncryptionKey newDataEncryptionKey() {\n        return dnConf.encryptDataTransfer ?\n          blockPoolTokenSecretManager.generateDataEncryptionKey(\n            block.getBlockPoolId()) : null;\n      }\n    };\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXceiverCount": "  public int getXceiverCount() {\n    return threadGroup == null ? 0 : threadGroup.activeCount();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDisplayName": "  public String getDisplayName() {\n    // NB: our DatanodeID may not be set yet\n    return hostName + \":\" + getXferPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXferPort": "  public int getXferPort() {\n    return streamingAddr.getPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dataXceiverServer.closePeer": "  synchronized void closePeer(Peer peer) {\n    peers.remove(peer);\n    peersXceiver.remove(peer);\n    datanode.metrics.decrDataNodeActiveXceiversCount();\n    IOUtils.cleanup(null, peer);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dataXceiverServer.addPeer": "  synchronized void addPeer(Peer peer, Thread t, DataXceiver xceiver)\n      throws IOException {\n    if (closed) {\n      throw new IOException(\"Server closed.\");\n    }\n    peers.put(peer, t);\n    peersXceiver.put(peer, xceiver);\n    datanode.metrics.incrDataNodeActiveXceiversCount();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDatanodeId": "  public DatanodeID getDatanodeId() {\n    return id;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXferAddress": "  public InetSocketAddress getXferAddress() {\n    return streamingAddr;\n  }"
        },
        "bug_report": {
            "Title": "Fix inconsistent replica size after a data pipeline failure",
            "Description": "We observed a case where a replica's on disk length is less than acknowledged length, breaking the assumption in recovery code.\n\n{noformat}\n2017-01-08 01:41:03,532 WARN org.apache.hadoop.hdfs.server.protocol.InterDatanodeProtocol: Failed to obtain replica info for block (=BP-947993742-10.204.0.136-1362248978912:blk_2526438952_1101394519586) from datanode (=DatanodeInfoWithStorage[10.204.138.17:1004,null,null])\njava.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: getBytesOnDisk() < getVisibleLength(), rip=ReplicaBeingWritten, blk_2526438952_1101394519586, RBW\n  getNumBytes()     = 27530\n  getBytesOnDisk()  = 27006\n  getVisibleLength()= 27268\n  getVolume()       = /data/6/hdfs/datanode/current\n  getBlockFile()    = /data/6/hdfs/datanode/current/BP-947993742-10.204.0.136-1362248978912/current/rbw/blk_2526438952\n  bytesAcked=27268\n  bytesOnDisk=27006\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2284)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2260)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.initReplicaRecovery(DataNode.java:2566)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.callInitReplicaRecovery(DataNode.java:2577)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2645)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.access$400(DataNode.java:245)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode$5.run(DataNode.java:2551)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\n\nIt turns out that if an exception is thrown within {{BlockReceiver#receivePacket}}, the in-memory replica on disk length may not be updated, but the data is written to disk anyway.\n\nFor example, here's one exception we observed\n{noformat}\n2017-01-08 01:40:59,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception for BP-947993742-10.204.0.136-1362248978912:blk_2526438952_1101394499067\njava.nio.channels.ClosedByInterruptException\n        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n        at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:269)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.adjustCrcChannelPosition(FsDatasetImpl.java:1484)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.adjustCrcFilePosition(BlockReceiver.java:994)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:670)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:857)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:797)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:244)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\n\nThere are potentially other places and causes where an exception is thrown within {{BlockReceiver#receivePacket}}, so it may not make much sense to alleviate it for this particular exception. Instead, we should improve replica recovery code to handle the case where ondisk size is less than acknowledged size, and update in-memory checksum accordingly."
        }
    },
    {
        "filename": "HDFS-10760.json",
        "creation_time": "2016-08-12T08:29:24.000+0000",
        "stack_trace": "```\norg.apache.hadoop.security.token.SecretManager$InvalidToken: Block token with block_token_identifier (expiryDate=1470850746803, keyId=-2093956963, userId=hbase, blockPoolId=BP-641703426-10.17.1.2-1468517918886, blockId=1077120201, access modes=[READ]) is expired.\n        at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:280)\n        at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:301)\n        at org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager.checkAccess(BlockPoolTokenSecretManager.java:97)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess(DataXceiver.java:1236)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:481)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:242)\n        at java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess": "  public void checkAccess(Token<BlockTokenIdentifier> token, String userId,\n      ExtendedBlock block, BlockTokenIdentifier.AccessMode mode) throws InvalidToken {\n    BlockTokenIdentifier id = new BlockTokenIdentifier();\n    try {\n      id.readFields(new DataInputStream(new ByteArrayInputStream(token\n          .getIdentifier())));\n    } catch (IOException e) {\n      throw new InvalidToken(\n          \"Unable to de-serialize block token identifier for user=\" + userId\n              + \", block=\" + block + \", access mode=\" + mode);\n    }\n    checkAccess(id, userId, block, mode);\n    if (!Arrays.equals(retrievePassword(id), token.getPassword())) {\n      throw new InvalidToken(\"Block token with \" + id.toString()\n          + \" doesn't have the correct token password\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.isExpired": "  private static boolean isExpired(long expiryDate) {\n    return Time.now() > expiryDate;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.retrievePassword": "  public byte[] retrievePassword(BlockTokenIdentifier identifier)\n      throws InvalidToken {\n    if (isExpired(identifier.getExpiryDate())) {\n      throw new InvalidToken(\"Block token with \" + identifier.toString()\n          + \" is expired.\");\n    }\n    BlockKey key = null;\n    synchronized (this) {\n      key = allKeys.get(identifier.getKeyId());\n    }\n    if (key == null) {\n      throw new InvalidToken(\"Can't re-compute password for \"\n          + identifier.toString() + \", since the required block key (keyID=\"\n          + identifier.getKeyId() + \") doesn't exist.\");\n    }\n    return createPassword(identifier.getBytes(), key.getKey());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager.checkAccess": "  public void checkAccess(Token<BlockTokenIdentifier> token,\n      String userId, ExtendedBlock block, AccessMode mode) throws InvalidToken {\n    get(block.getBlockPoolId()).checkAccess(token, userId, block, mode);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager.get": "  synchronized BlockTokenSecretManager get(String bpid) {\n    BlockTokenSecretManager secretMgr = map.get(bpid);\n    if (secretMgr == null) {\n      throw new IllegalArgumentException(\"Block pool \" + bpid\n          + \" is not found\");\n    }\n    return secretMgr;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess": "  private void checkAccess(OutputStream out, final boolean reply, \n      final ExtendedBlock blk,\n      final Token<BlockTokenIdentifier> t,\n      final Op op,\n      final BlockTokenIdentifier.AccessMode mode) throws IOException {\n    checkAndWaitForBP(blk);\n    if (datanode.isBlockTokenEnabled) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Checking block access token for block '\" + blk.getBlockId()\n            + \"' with mode '\" + mode + \"'\");\n      }\n      try {\n        datanode.blockPoolTokenSecretManager.checkAccess(t, null, blk, mode);\n      } catch(InvalidToken e) {\n        try {\n          if (reply) {\n            BlockOpResponseProto.Builder resp = BlockOpResponseProto.newBuilder()\n              .setStatus(ERROR_ACCESS_TOKEN);\n            if (mode == BlockTokenIdentifier.AccessMode.WRITE) {\n              DatanodeRegistration dnR = \n                datanode.getDNRegistrationForBP(blk.getBlockPoolId());\n              // NB: Unconditionally using the xfer addr w/o hostname\n              resp.setFirstBadLink(dnR.getXferAddr());\n            }\n            resp.build().writeDelimitedTo(out);\n            out.flush();\n          }\n          LOG.warn(\"Block token verification failed: op=\" + op\n              + \", remoteAddress=\" + remoteAddress\n              + \", message=\" + e.getLocalizedMessage());\n          throw e;\n        } finally {\n          IOUtils.closeStream(out);\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAndWaitForBP": "  void checkAndWaitForBP(final ExtendedBlock block)\n      throws IOException {\n    String bpId = block.getBlockPoolId();\n\n    // The registration is only missing in relatively short time window.\n    // Optimistically perform this first.\n    try {\n      datanode.getDNRegistrationForBP(bpId);\n      return;\n    } catch (IOException ioe) {\n      // not registered\n    }\n\n    // retry\n    long bpReadyTimeout = dnConf.getBpReadyTimeout();\n    StopWatch sw = new StopWatch();\n    sw.start();\n    while (sw.now(TimeUnit.SECONDS) <= bpReadyTimeout) {\n      try {\n        datanode.getDNRegistrationForBP(bpId);\n        return;\n      } catch (IOException ioe) {\n        // not registered\n      }\n      // sleep before trying again\n      try {\n        Thread.sleep(1000);\n      } catch (InterruptedException ie) {\n        throw new IOException(\"Interrupted while serving request. Aborting.\");\n      }\n    }\n    // failed to obtain registration.\n    throw new IOException(\"Not ready to serve the block pool, \" + bpId + \".\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock": "  public void readBlock(final ExtendedBlock block,\n      final Token<BlockTokenIdentifier> blockToken,\n      final String clientName,\n      final long blockOffset,\n      final long length,\n      final boolean sendChecksum,\n      final CachingStrategy cachingStrategy) throws IOException {\n    previousOpClientName = clientName;\n    long read = 0;\n    updateCurrentThreadName(\"Sending block \" + block);\n    OutputStream baseStream = getOutputStream();\n    DataOutputStream out = getBufferedOutputStream();\n    checkAccess(out, true, block, blockToken,\n        Op.READ_BLOCK, BlockTokenIdentifier.AccessMode.READ);\n  \n    // send the block\n    BlockSender blockSender = null;\n    DatanodeRegistration dnR = \n      datanode.getDNRegistrationForBP(block.getBlockPoolId());\n    final String clientTraceFmt =\n      clientName.length() > 0 && ClientTraceLog.isInfoEnabled()\n        ? String.format(DN_CLIENTTRACE_FORMAT, localAddress, remoteAddress,\n            \"%d\", \"HDFS_READ\", clientName, \"%d\",\n            dnR.getDatanodeUuid(), block, \"%d\")\n        : dnR + \" Served block \" + block + \" to \" +\n            remoteAddress;\n\n    try {\n      try {\n        blockSender = new BlockSender(block, blockOffset, length,\n            true, false, sendChecksum, datanode, clientTraceFmt,\n            cachingStrategy);\n      } catch(IOException e) {\n        String msg = \"opReadBlock \" + block + \" received exception \" + e; \n        LOG.info(msg);\n        sendResponse(ERROR, msg);\n        throw e;\n      }\n      \n      // send op status\n      writeSuccessWithChecksumInfo(blockSender, new DataOutputStream(getOutputStream()));\n\n      long beginRead = Time.monotonicNow();\n      read = blockSender.sendBlock(out, baseStream, null); // send data\n      long duration = Time.monotonicNow() - beginRead;\n      if (blockSender.didSendEntireByteRange()) {\n        // If we sent the entire range, then we should expect the client\n        // to respond with a Status enum.\n        try {\n          ClientReadStatusProto stat = ClientReadStatusProto.parseFrom(\n              PBHelperClient.vintPrefixed(in));\n          if (!stat.hasStatus()) {\n            LOG.warn(\"Client \" + peer.getRemoteAddressString() +\n                \" did not send a valid status code after reading. \" +\n                \"Will close connection.\");\n            IOUtils.closeStream(out);\n          }\n        } catch (IOException ioe) {\n          LOG.debug(\"Error reading client status response. Will close connection.\", ioe);\n          IOUtils.closeStream(out);\n          incrDatanodeNetworkErrors();\n        }\n      } else {\n        IOUtils.closeStream(out);\n      }\n      datanode.metrics.incrBytesRead((int) read);\n      datanode.metrics.incrBlocksRead();\n      datanode.metrics.incrTotalReadTime(duration);\n    } catch ( SocketException ignored ) {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(dnR + \":Ignoring exception while serving \" + block + \" to \" +\n            remoteAddress, ignored);\n      }\n      // Its ok for remote side to close the connection anytime.\n      datanode.metrics.incrBlocksRead();\n      IOUtils.closeStream(out);\n    } catch ( IOException ioe ) {\n      /* What exactly should we do here?\n       * Earlier version shutdown() datanode if there is disk error.\n       */\n      if (!(ioe instanceof SocketTimeoutException)) {\n        LOG.warn(dnR + \":Got exception while serving \" + block + \" to \"\n          + remoteAddress, ioe);\n        incrDatanodeNetworkErrors();\n      }\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(blockSender);\n    }\n\n    //update metrics\n    datanode.metrics.addReadBlockOp(elapsed());\n    datanode.metrics.incrReadsFromClient(peer.isLocal(), read);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeSuccessWithChecksumInfo": "  private void writeSuccessWithChecksumInfo(BlockSender blockSender,\n      DataOutputStream out) throws IOException {\n\n    ReadOpChecksumInfoProto ckInfo = ReadOpChecksumInfoProto.newBuilder()\n      .setChecksum(DataTransferProtoUtil.toProto(blockSender.getChecksum()))\n      .setChunkOffset(blockSender.getOffset())\n      .build();\n      \n    BlockOpResponseProto response = BlockOpResponseProto.newBuilder()\n      .setStatus(SUCCESS)\n      .setReadOpChecksumInfo(ckInfo)\n      .build();\n    response.writeDelimitedTo(out);\n    out.flush();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.incrDatanodeNetworkErrors": "  private void incrDatanodeNetworkErrors() {\n    datanode.incrDatanodeNetworkErrors(remoteAddressWithoutPort);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBufferedOutputStream": "  DataOutputStream getBufferedOutputStream() {\n    return new DataOutputStream(\n        new BufferedOutputStream(getOutputStream(), smallBufferSize));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.elapsed": "  private long elapsed() {\n    return monotonicNow() - opStartTime;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.getOutputStream": "  private OutputStream getOutputStream() {\n    return socketOut;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.sendResponse": "  private void sendResponse(Status status,\n      String message) throws IOException {\n    writeResponse(status, message, getOutputStream());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.updateCurrentThreadName": "  private void updateCurrentThreadName(String status) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"DataXceiver for client \");\n    if (previousOpClientName != null) {\n      sb.append(previousOpClientName).append(\" at \");\n    }\n    sb.append(remoteAddress);\n    if (status != null) {\n      sb.append(\" [\").append(status).append(\"]\");\n    }\n    Thread.currentThread().setName(sb.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock": "  private void opReadBlock() throws IOException {\n    OpReadBlockProto proto = OpReadBlockProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      readBlock(PBHelperClient.convert(proto.getHeader().getBaseHeader().getBlock()),\n        PBHelperClient.convert(proto.getHeader().getBaseHeader().getToken()),\n        proto.getHeader().getClientName(),\n        proto.getOffset(),\n        proto.getLen(),\n        proto.getSendChecksums(),\n        (proto.hasCachingStrategy() ?\n            getCachingStrategy(proto.getCachingStrategy()) :\n          CachingStrategy.newDefaultStrategy()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.continueTraceSpan": "  private TraceScope continueTraceSpan(BaseHeaderProto header,\n                                             String description) {\n    return continueTraceSpan(header.getTraceInfo(), description);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.getCachingStrategy": "  static private CachingStrategy getCachingStrategy(CachingStrategyProto strategy) {\n    Boolean dropBehind = strategy.hasDropBehind() ?\n        strategy.getDropBehind() : null;\n    Long readahead = strategy.hasReadahead() ?\n        strategy.getReadahead() : null;\n    return new CachingStrategy(dropBehind, readahead);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp": "  protected final void processOp(Op op) throws IOException {\n    switch(op) {\n    case READ_BLOCK:\n      opReadBlock();\n      break;\n    case WRITE_BLOCK:\n      opWriteBlock(in);\n      break;\n    case REPLACE_BLOCK:\n      opReplaceBlock(in);\n      break;\n    case COPY_BLOCK:\n      opCopyBlock(in);\n      break;\n    case BLOCK_CHECKSUM:\n      opBlockChecksum(in);\n      break;\n    case BLOCK_GROUP_CHECKSUM:\n      opStripedBlockChecksum(in);\n      break;\n    case TRANSFER_BLOCK:\n      opTransferBlock(in);\n      break;\n    case REQUEST_SHORT_CIRCUIT_FDS:\n      opRequestShortCircuitFds(in);\n      break;\n    case RELEASE_SHORT_CIRCUIT_FDS:\n      opReleaseShortCircuitFds(in);\n      break;\n    case REQUEST_SHORT_CIRCUIT_SHM:\n      opRequestShortCircuitShm(in);\n      break;\n    default:\n      throw new IOException(\"Unknown op \" + op + \" in data stream\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReplaceBlock": "  private void opReplaceBlock(DataInputStream in) throws IOException {\n    OpReplaceBlockProto proto = OpReplaceBlockProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      replaceBlock(PBHelperClient.convert(proto.getHeader().getBlock()),\n          PBHelperClient.convertStorageType(proto.getStorageType()),\n          PBHelperClient.convert(proto.getHeader().getToken()),\n          proto.getDelHint(),\n          PBHelperClient.convert(proto.getSource()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitFds": "  private void opRequestShortCircuitFds(DataInputStream in) throws IOException {\n    final OpRequestShortCircuitAccessProto proto =\n      OpRequestShortCircuitAccessProto.parseFrom(vintPrefixed(in));\n    SlotId slotId = (proto.hasSlotId()) ? \n        PBHelperClient.convert(proto.getSlotId()) : null;\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      requestShortCircuitFds(PBHelperClient.convert(proto.getHeader().getBlock()),\n          PBHelperClient.convert(proto.getHeader().getToken()),\n          slotId, proto.getMaxVersion(),\n          proto.getSupportsReceiptVerification());\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitShm": "  private void opRequestShortCircuitShm(DataInputStream in) throws IOException {\n    final ShortCircuitShmRequestProto proto =\n        ShortCircuitShmRequestProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getTraceInfo(),\n        proto.getClass().getSimpleName());\n    try {\n      requestShortCircuitShm(proto.getClientName());\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opBlockChecksum": "  private void opBlockChecksum(DataInputStream in) throws IOException {\n    OpBlockChecksumProto proto = OpBlockChecksumProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n    blockChecksum(PBHelperClient.convert(proto.getHeader().getBlock()),\n        PBHelperClient.convert(proto.getHeader().getToken()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReleaseShortCircuitFds": "  private void opReleaseShortCircuitFds(DataInputStream in)\n      throws IOException {\n    final ReleaseShortCircuitAccessRequestProto proto =\n      ReleaseShortCircuitAccessRequestProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getTraceInfo(),\n        proto.getClass().getSimpleName());\n    try {\n      releaseShortCircuitFds(PBHelperClient.convert(proto.getSlotId()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum": "  private void opStripedBlockChecksum(DataInputStream dis) throws IOException {\n    OpBlockGroupChecksumProto proto =\n        OpBlockGroupChecksumProto.parseFrom(vintPrefixed(dis));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    StripedBlockInfo stripedBlockInfo = new StripedBlockInfo(\n        PBHelperClient.convert(proto.getHeader().getBlock()),\n        PBHelperClient.convert(proto.getDatanodes()),\n        PBHelperClient.convertTokens(proto.getBlockTokensList()),\n        PBHelperClient.convertBlockIndices(proto.getBlockIndicesList()),\n        PBHelperClient.convertErasureCodingPolicy(proto.getEcPolicy())\n    );\n\n    try {\n      blockGroupChecksum(stripedBlockInfo,\n          PBHelperClient.convert(proto.getHeader().getToken()),\n          proto.getRequestedNumBytes());\n    } finally {\n      if (traceScope != null) {\n        traceScope.close();\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opTransferBlock": "  private void opTransferBlock(DataInputStream in) throws IOException {\n    final OpTransferBlockProto proto =\n      OpTransferBlockProto.parseFrom(vintPrefixed(in));\n    final DatanodeInfo[] targets = PBHelperClient.convert(proto.getTargetsList());\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      transferBlock(PBHelperClient.convert(proto.getHeader().getBaseHeader().getBlock()),\n          PBHelperClient.convert(proto.getHeader().getBaseHeader().getToken()),\n          proto.getHeader().getClientName(),\n          targets,\n          PBHelperClient.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opCopyBlock": "  private void opCopyBlock(DataInputStream in) throws IOException {\n    OpCopyBlockProto proto = OpCopyBlockProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      copyBlock(PBHelperClient.convert(proto.getHeader().getBlock()),\n          PBHelperClient.convert(proto.getHeader().getToken()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock": "  private void opWriteBlock(DataInputStream in) throws IOException {\n    final OpWriteBlockProto proto = OpWriteBlockProto.parseFrom(vintPrefixed(in));\n    final DatanodeInfo[] targets = PBHelperClient.convert(proto.getTargetsList());\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      writeBlock(PBHelperClient.convert(proto.getHeader().getBaseHeader().getBlock()),\n          PBHelperClient.convertStorageType(proto.getStorageType()),\n          PBHelperClient.convert(proto.getHeader().getBaseHeader().getToken()),\n          proto.getHeader().getClientName(),\n          targets,\n          PBHelperClient.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length),\n          PBHelperClient.convert(proto.getSource()),\n          fromProto(proto.getStage()),\n          proto.getPipelineSize(),\n          proto.getMinBytesRcvd(), proto.getMaxBytesRcvd(),\n          proto.getLatestGenerationStamp(),\n          fromProto(proto.getRequestedChecksum()),\n          (proto.hasCachingStrategy() ?\n              getCachingStrategy(proto.getCachingStrategy()) :\n            CachingStrategy.newDefaultStrategy()),\n          (proto.hasAllowLazyPersist() ? proto.getAllowLazyPersist() : false),\n          (proto.hasPinning() ? proto.getPinning(): false),\n          (PBHelperClient.convertBooleanList(proto.getTargetPinningsList())));\n    } finally {\n     if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.run": "  public void run() {\n    int opsProcessed = 0;\n    Op op = null;\n\n    try {\n      synchronized(this) {\n        xceiver = Thread.currentThread();\n      }\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input = socketIn;\n      try {\n        IOStreamPair saslStreams = datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input = new BufferedInputStream(saslStreams.in,\n            smallBufferSize);\n        socketOut = saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at \" + peer.getRemoteAddressString() + \n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\");\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed != 0) {\n            assert dnConf.socketKeepaliveTimeout > 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op = readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (EOFException | ClosedChannelException e) {\n          // Since we optimistically expect the next op, it's quite normal to\n          // get EOF here.\n          LOG.debug(\"Cached {} closing after {} ops.  \" +\n              \"This message is usually benign.\", peer, opsProcessed);\n          break;\n        } catch (IOException err) {\n          incrDatanodeNetworkErrors();\n          throw err;\n        }\n\n        // restore normal timeout\n        if (opsProcessed != 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime = monotonicNow();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer != null) &&\n          (!peer.isClosed() && dnConf.socketKeepaliveTimeout > 0));\n    } catch (Throwable t) {\n      String s = datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op == null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op == Op.WRITE_BLOCK && t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op == Op.READ_BLOCK && t instanceof SocketTimeoutException) {\n        String s1 =\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 += \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer != null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier.getBlockPoolId": "  public String getBlockPoolId() {\n    return blockPoolId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier.getAccessModes": "  public EnumSet<AccessMode> getAccessModes() {\n    return modes;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier.toString": "  public String toString() {\n    return \"block_token_identifier (expiryDate=\" + this.getExpiryDate()\n        + \", keyId=\" + this.getKeyId() + \", userId=\" + this.getUserId()\n        + \", blockPoolId=\" + this.getBlockPoolId()\n        + \", blockId=\" + this.getBlockId() + \", access modes=\"\n        + this.getAccessModes() + \")\";\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier.getKeyId": "  public int getKeyId() {\n    return this.keyId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier.getExpiryDate": "  public long getExpiryDate() {\n    return expiryDate;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier.getUserId": "  public String getUserId() {\n    return userId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier.getBlockId": "  public long getBlockId() {\n    return blockId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier.readFields": "  public void readFields(DataInput in) throws IOException {\n    this.cache = null;\n    expiryDate = WritableUtils.readVLong(in);\n    keyId = WritableUtils.readVInt(in);\n    userId = WritableUtils.readString(in);\n    blockPoolId = WritableUtils.readString(in);\n    blockId = WritableUtils.readVLong(in);\n    int length = WritableUtils.readVIntInRange(in, 0,\n        AccessMode.class.getEnumConstants().length);\n    for (int i = 0; i < length; i++) {\n      modes.add(WritableUtils.readEnum(in, AccessMode.class));\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDNRegistrationForBP": "  public DatanodeRegistration getDNRegistrationForBP(String bpid) \n  throws IOException {\n    DataNodeFaultInjector.get().noRegistration();\n    BPOfferService bpos = blockPoolManager.get(bpid);\n    if(bpos==null || bpos.bpRegistration==null) {\n      throw new IOException(\"cannot find BPOfferService for bpid=\"+bpid);\n    }\n    return bpos.bpRegistration;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    final TraceScope scope = datanode.getTracer().\n        newScope(\"sendBlock_\" + block.getBlockId());\n    try {\n      return doSendBlock(out, baseStream, throttler);\n    } finally {\n      scope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock": "  private long doSendBlock(DataOutputStream out, OutputStream baseStream,\n        DataTransferThrottler throttler) throws IOException {\n    if (out == null) {\n      throw new IOException( \"out stream is null\" );\n    }\n    initialOffset = offset;\n    long totalRead = 0;\n    OutputStream streamForSendChunks = out;\n    \n    lastCacheDropOffset = initialOffset;\n\n    if (isLongRead() && blockInFd != null) {\n      // Advise that this file descriptor will be accessed sequentially.\n      NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n          block.getBlockName(), blockInFd, 0, 0, POSIX_FADV_SEQUENTIAL);\n    }\n    \n    // Trigger readahead of beginning of file if configured.\n    manageOsCache();\n\n    final long startTime = ClientTraceLog.isDebugEnabled() ? System.nanoTime() : 0;\n    try {\n      int maxChunksPerPacket;\n      int pktBufSize = PacketHeader.PKT_MAX_HEADER_LEN;\n      boolean transferTo = transferToAllowed && !verifyChecksum\n          && baseStream instanceof SocketOutputStream\n          && blockIn instanceof FileInputStream;\n      if (transferTo) {\n        FileChannel fileChannel = ((FileInputStream)blockIn).getChannel();\n        blockInPosition = fileChannel.position();\n        streamForSendChunks = baseStream;\n        maxChunksPerPacket = numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n        \n        // Smaller packet size to only hold checksum when doing transferTo\n        pktBufSize += checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket = Math.max(1,\n            numberOfChunks(IO_FILE_BUFFER_SIZE));\n        // Packet size includes both checksum and data\n        pktBufSize += (chunkSize + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf = ByteBuffer.allocate(pktBufSize);\n\n      while (endOffset > offset && !Thread.currentThread().isInterrupted()) {\n        manageOsCache();\n        long len = sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n            transferTo, throttler);\n        offset += len;\n        totalRead += len + (numberOfChunks(len) * checksumSize);\n        seqno++;\n      }\n      // If this thread was interrupted, then it did not send the full block.\n      if (!Thread.currentThread().isInterrupted()) {\n        try {\n          // send an empty packet to mark the end of the block\n          sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n              throttler);\n          out.flush();\n        } catch (IOException e) { //socket error\n          throw ioeToSocketException(e);\n        }\n\n        sentEntireByteRange = true;\n      }\n    } finally {\n      if ((clientTraceFmt != null) && ClientTraceLog.isDebugEnabled()) {\n        final long endTime = System.nanoTime();\n        ClientTraceLog.debug(String.format(clientTraceFmt, totalRead,\n            initialOffset, endTime - startTime));\n      }\n      close();\n    }\n    return totalRead;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.close": "  public void close() throws IOException {\n    if (blockInFd != null &&\n        ((dropCacheBehindAllReads) ||\n         (dropCacheBehindLargeReads && isLongRead()))) {\n      try {\n        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n            block.getBlockName(), blockInFd, lastCacheDropOffset,\n            offset - lastCacheDropOffset, POSIX_FADV_DONTNEED);\n      } catch (Exception e) {\n        LOG.warn(\"Unable to drop cache on file close\", e);\n      }\n    }\n    if (curReadahead != null) {\n      curReadahead.cancel();\n    }\n    \n    IOException ioe = null;\n    if(checksumIn!=null) {\n      try {\n        checksumIn.close(); // close checksum file\n      } catch (IOException e) {\n        ioe = e;\n      }\n      checksumIn = null;\n    }   \n    if(blockIn!=null) {\n      try {\n        blockIn.close(); // close data file\n      } catch (IOException e) {\n        ioe = e;\n      }\n      blockIn = null;\n      blockInFd = null;\n    }\n    if (volumeRef != null) {\n      IOUtils.cleanup(null, volumeRef);\n      volumeRef = null;\n    }\n    // throw IOException if there is any\n    if(ioe!= null) {\n      throw ioe;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.didSendEntireByteRange": "  boolean didSendEntireByteRange() {\n    return sentEntireByteRange;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXceiverCount": "  public int getXceiverCount() {\n    return threadGroup == null ? 0 : threadGroup.activeCount();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDisplayName": "  public String getDisplayName() {\n    // NB: our DatanodeID may not be set yet\n    return hostName + \":\" + getXferPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXferPort": "  public int getXferPort() {\n    return streamingAddr.getPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dataXceiverServer.closePeer": "  synchronized void closePeer(Peer peer) {\n    peers.remove(peer);\n    peersXceiver.remove(peer);\n    datanode.metrics.decrDataNodeActiveXceiversCount();\n    IOUtils.cleanup(null, peer);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dataXceiverServer.addPeer": "  synchronized void addPeer(Peer peer, Thread t, DataXceiver xceiver)\n      throws IOException {\n    if (closed) {\n      throw new IOException(\"Server closed.\");\n    }\n    peers.put(peer, t);\n    peersXceiver.put(peer, xceiver);\n    datanode.metrics.incrDataNodeActiveXceiversCount();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDatanodeId": "  public DatanodeID getDatanodeId() {\n    return id;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXferAddress": "  public InetSocketAddress getXferAddress() {\n    return streamingAddr;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDnConf": "  public DNConf getDnConf() {\n    return dnConf;\n  }"
        },
        "bug_report": {
            "Title": "DataXceiver#run() should not log InvalidToken exception as an error",
            "Description": "DataXceiver#run() just log InvalidToken exception as an error.\nWhen client has an expired token and just refetch a new token, the DN log will has an error like below:\n{noformat}\n2016-08-11 02:41:09,817 ERROR datanode.DataNode (DataXceiver.java:run(269)) - XXXXXXX:50010:DataXceiver error processing READ_BLOCK operation  src: /10.17.1.5:38844 dst: /10.17.1.5:50010\norg.apache.hadoop.security.token.SecretManager$InvalidToken: Block token with block_token_identifier (expiryDate=1470850746803, keyId=-2093956963, userId=hbase, blockPoolId=BP-641703426-10.17.1.2-1468517918886, blockId=1077120201, access modes=[READ]) is expired.\n        at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:280)\n        at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:301)\n        at org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager.checkAccess(BlockPoolTokenSecretManager.java:97)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess(DataXceiver.java:1236)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:481)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:242)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\nThis is not a server error and the DataXceiver#checkAccess() has already loged the InvalidToken as a warning.\nA simple fix by catching the InvalidToken exception in DataXceiver#run(), only keeping the warning logged by DataXceiver#checkAccess() in the DN log."
        }
    },
    {
        "filename": "HDFS-13635.json",
        "creation_time": "2018-05-29T19:54:50.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Cannot append to a non-existent replica BP-725378529-10.236.236.8-1410027444173:13276792346\n at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo(FsDatasetImpl.java:792)\n at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaVisibleLength(FsDatasetImpl.java:2588)\n at org.apache.hadoop.hdfs.server.datanode.DataNode.getReplicaVisibleLength(DataNode.java:2756)\n at org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolServerSideTranslatorPB.getReplicaVisibleLength(ClientDatanodeProtocolServerSideTranslatorPB.java:107)\n at org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ClientDatanodeProtocolService$2.callBlockingMethod(ClientDatanodeProtocolProtos.java:17873)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2217)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2213)\n at java.security.AccessController.doPrivileged(Native Method)\n at javax.security.auth.Subject.doAs(Subject.java:422)\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2211)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo": "  ReplicaInfo getReplicaInfo(String bpid, long blkid)\n      throws ReplicaNotFoundException {\n    ReplicaInfo info = volumeMap.get(bpid, blkid);\n    if (info == null) {\n      throw new ReplicaNotFoundException(\n          ReplicaNotFoundException.NON_EXISTENT_REPLICA + bpid + \":\" + blkid);\n    }\n    return info;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaVisibleLength": "  public long getReplicaVisibleLength(final ExtendedBlock block)\n  throws IOException {\n    try (AutoCloseableLock lock = datasetLock.acquire()) {\n      final Replica replica = getReplicaInfo(block.getBlockPoolId(),\n          block.getBlockId());\n      if (replica.getGenerationStamp() < block.getGenerationStamp()) {\n        throw new IOException(\n            \"replica.getGenerationStamp() < block.getGenerationStamp(), block=\"\n                + block + \", replica=\" + replica);\n      }\n      return replica.getVisibleLength();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getReplicaVisibleLength": "  public long getReplicaVisibleLength(final ExtendedBlock block) throws IOException {\n    checkReadAccess(block);\n    return data.getReplicaVisibleLength(block);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.checkReadAccess": "  private void checkReadAccess(final ExtendedBlock block) throws IOException {\n    // Make sure this node has registered for the block pool.\n    try {\n      getDNRegistrationForBP(block.getBlockPoolId());\n    } catch (IOException e) {\n      // if it has not registered with the NN, throw an exception back.\n      throw new org.apache.hadoop.ipc.RetriableException(\n          \"Datanode not registered. Try again later.\");\n    }\n\n    if (isBlockTokenEnabled) {\n      Set<TokenIdentifier> tokenIds = UserGroupInformation.getCurrentUser()\n          .getTokenIdentifiers();\n      if (tokenIds.size() != 1) {\n        throw new IOException(\"Can't continue since none or more than one \"\n            + \"BlockTokenIdentifier is found.\");\n      }\n      for (TokenIdentifier tokenId : tokenIds) {\n        BlockTokenIdentifier id = (BlockTokenIdentifier) tokenId;\n        LOG.debug(\"Got: {}\", id);\n        blockPoolTokenSecretManager.checkAccess(id, null, block,\n            BlockTokenIdentifier.AccessMode.READ, null, null);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolServerSideTranslatorPB.getReplicaVisibleLength": "  public GetReplicaVisibleLengthResponseProto getReplicaVisibleLength(\n      RpcController unused, GetReplicaVisibleLengthRequestProto request)\n      throws ServiceException {\n    long len;\n    try {\n      len = impl.getReplicaVisibleLength(PBHelperClient.convert(request.getBlock()));\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n    return GetReplicaVisibleLengthResponseProto.newBuilder().setLength(len)\n        .build();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcProtobufRequest request = (RpcProtobufRequest) writableRequest;\n        RequestHeaderProto rpcRequest = request.getRequestHeader();\n        String methodName = rpcRequest.getMethodName();\n\n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = request.getValue(prototype);\n\n        Message result;\n        long startTime = Time.now();\n        int qTime = (int) (startTime - receiveTime);\n        Exception exception = null;\n        boolean isDeferred = false;\n        try {\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          currentCallInfo.set(new CallInfo(server, methodName));\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n          // Check if this needs to be a deferred response,\n          // by checking the ThreadLocal callback being set\n          if (currentCallback.get() != null) {\n            Server.getCurCall().get().deferResponse();\n            isDeferred = true;\n            currentCallback.set(null);\n            return null;\n          }\n        } catch (ServiceException e) {\n          exception = (Exception) e.getCause();\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          exception = e;\n          throw e;\n        } finally {\n          currentCallInfo.set(null);\n          int processingTime = (int) (Time.now() - startTime);\n          if (LOG.isDebugEnabled()) {\n            String msg =\n                \"Served: \" + methodName + (isDeferred ? \", deferred\" : \"\") +\n                    \", queueTime= \" + qTime +\n                    \" procesingTime= \" + processingTime;\n            if (exception != null) {\n              msg += \" exception= \" + exception.getClass().getSimpleName();\n            }\n            LOG.debug(msg);\n          }\n          String detailedMetricsName = (exception == null) ?\n              methodName :\n              exception.getClass().getSimpleName();\n          server.updateMetrics(detailedMetricsName, qTime, processingTime,\n              isDeferred);\n        }\n        return RpcWritable.wrap(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.get": "          public Message get(long timeout, TimeUnit unit) throws Exception {\n            return getReturnMessage(method, arr.get(timeout, unit));\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getRequestHeader": "    RequestHeaderProto getRequestHeader() throws IOException {\n      if (getByteBuffer() != null && requestHeader == null) {\n        requestHeader = getValue(RequestHeaderProto.getDefaultInstance());\n      }\n      return requestHeader;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getProtocolImpl": "      private static ProtoClassProtoImpl getProtocolImpl(RPC.Server server,\n          String protoName, long clientVersion) throws RpcServerException {\n        ProtoNameVer pv = new ProtoNameVer(protoName, clientVersion);\n        ProtoClassProtoImpl impl = \n            server.getProtocolImplMap(RPC.RpcKind.RPC_PROTOCOL_BUFFER).get(pv);\n        if (impl == null) { // no match for Protocol AND Version\n          VerProtocolImpl highest = \n              server.getHighestSupportedProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER, \n                  protoName);\n          if (highest == null) {\n            throw new RpcNoSuchProtocolException(\n                \"Unknown protocol: \" + protoName);\n          }\n          // protocol supported but not the version that client wants\n          throw new RPC.VersionMismatch(protoName, clientVersion,\n              highest.version);\n        }\n        return impl;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.call": "    public Writable call(RPC.RpcKind rpcKind, String protocol,\n        Writable rpcRequest, long receiveTime) throws Exception {\n      return getRpcInvoker(rpcKind).call(this, protocol, rpcRequest,\n          receiveTime);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(RpcCall call,\n      RpcResponseHeaderProto header, Writable rv) throws IOException {\n    final byte[] response;\n    if (rv == null || (rv instanceof RpcWritable.ProtobufWrapper)) {\n      response = setupResponseForProtobuf(header, rv);\n    } else {\n      response = setupResponseForWritable(header, rv);\n    }\n    if (response.length > maxRespSize) {\n      LOG.warn(\"Large response size \" + response.length + \" for call \"\n          + call.toString());\n    }\n    call.setResponse(ByteBuffer.wrap(response));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.isResponseDeferred": "    public boolean isResponseDeferred() {\n      return this.deferredResponse;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.remove": "    private boolean remove(Connection connection) {\n      boolean removed = connections.remove(connection);\n      if (removed) {\n        count.getAndDecrement();\n      }\n      return removed;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeAll": "    void closeAll() {\n      // use a copy of the connections to be absolutely sure the concurrent\n      // iterator doesn't miss a connection\n      for (Connection connection : toArray()) {\n        close(connection);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(RPC.RpcKind rpcKind, String protocol,\n      Writable param, long receiveTime) throws Exception;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param protocolName - the protocol\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  private void authorize(UserGroupInformation user, String protocolName,\n      InetAddress addr) throws AuthorizationException {\n    if (authorize) {\n      if (protocolName == null) {\n        throw new AuthorizationException(\"Null protocol not authorized\");\n      }\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(protocolName, getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         protocolName);\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    boolean close(Connection connection) {\n      boolean exists = remove(connection);\n      if (exists) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(Thread.currentThread().getName() +\n              \": disconnecting client \" + connection +\n              \". Number of active connections: \"+ size());\n        }\n        // only close if actually removed to avoid double-closing due\n        // to possible races\n        connection.close();\n        // Remove authorized users only\n        if (connection.user != null && connection.connectionContextRead) {\n          decrUserConnections(connection.user.getShortUserName());\n        }\n      }\n      return exists;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isWritable()) {\n                doAsyncWrite(key);\n              }\n            } catch (CancelledKeyException cke) {\n              // something else closed the connection, ex. reader or the\n              // listener doing an idle scan.  ignore it and let them clean\n              // up\n              RpcCall call = (RpcCall)key.attachment();\n              if (call != null) {\n                LOG.info(Thread.currentThread().getName() +\n                    \": connection aborted from \" + call.connection);\n              }\n            } catch (IOException e) {\n              LOG.info(Thread.currentThread().getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = Time.now();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<RpcCall> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<RpcCall>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              RpcCall call = (RpcCall)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n\n          for (RpcCall call : calls) {\n            doPurge(call, now);\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeIdle": "    synchronized void closeIdle(boolean scanAll) {\n      long minLastContact = Time.now() - maxIdleTime;\n      // concurrent iterator might miss new connections added\n      // during the iteration, but that's ok because they won't\n      // be idle yet anyway and will be caught on next scan\n      int closed = 0;\n      for (Connection connection : connections) {\n        // stop if connections dropped below threshold unless scanning all\n        if (!scanAll && size() < idleScanThreshold) {\n          break;\n        }\n        // stop if not scanning all and max connections are closed\n        if (connection.isIdle() &&\n            connection.getLastContact() < minLastContact &&\n            close(connection) &&\n            !scanAll && (++closed == maxIdleToClose)) {\n          break;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.startIdleScan": "    void startIdleScan() {\n      scheduleIdleScanTask();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.sendResponse": "    private void sendResponse(RpcCall call) throws IOException {\n      responder.doRespond(call);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.stopIdleScan": "    void stopIdleScan() {\n      idleScanTimer.cancel();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws InterruptedException, IOException,  OutOfMemoryError {\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        channel.socket().setKeepAlive(true);\n        \n        Reader reader = getReader();\n        Connection c = connectionManager.register(channel);\n        // If the connectionManager can't take it, close the connection.\n        if (c == null) {\n          if (channel.isOpen()) {\n            IOUtils.cleanup(null, channel);\n          }\n          connectionManager.droppedConnections.getAndIncrement();\n          continue;\n        }\n        key.attach(c);  // so closeCurrentConnection can get the object\n        reader.addConnection(c);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.scheduleIdleScanTask": "    private void scheduleIdleScanTask() {\n      if (!running) {\n        return;\n      }\n      TimerTask idleScanTask = new TimerTask(){\n        @Override\n        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }\n      };\n      idleScanTimer.schedule(idleScanTask, idleScanInterval);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getRemoteUser": "    public UserGroupInformation getRemoteUser() {\n      return connection.user;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.populateResponseParamsOnError": "    private void populateResponseParamsOnError(Throwable t,\n                                               ResponseParams responseParams) {\n      if (t instanceof UndeclaredThrowableException) {\n        t = t.getCause();\n      }\n      logException(Server.LOG, t, this);\n      if (t instanceof RpcServerException) {\n        RpcServerException rse = ((RpcServerException) t);\n        responseParams.returnStatus = rse.getRpcStatusProto();\n        responseParams.detailedErr = rse.getRpcErrorCodeProto();\n      } else {\n        responseParams.returnStatus = RpcStatusProto.ERROR;\n        responseParams.detailedErr = RpcErrorCodeProto.ERROR_APPLICATION;\n      }\n      responseParams.errorClass = t.getClass().getName();\n      responseParams.error = StringUtils.stringifyException(t);\n      // Remove redundant error class name from the beginning of the\n      // stack trace\n      String exceptionHdr = responseParams.errorClass + \": \";\n      if (responseParams.error.startsWith(exceptionHdr)) {\n        responseParams.error =\n            responseParams.error.substring(exceptionHdr.length());\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause == null) {\n        throw new RuntimeException(\"PrivilegedActionException with no \" +\n                \"underlying cause. UGI [\" + this + \"]\" +\": \" + pae, pae);\n      } else if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convert": "  public static HdfsProtos.ProvidedStorageLocationProto convert(\n      ProvidedStorageLocation providedStorageLocation) {\n    String path = providedStorageLocation.getPath().toString();\n    return HdfsProtos.ProvidedStorageLocationProto.newBuilder()\n        .setPath(path)\n        .setLength(providedStorageLocation.getLength())\n        .setOffset(providedStorageLocation.getOffset())\n        .setNonce(ByteString.copyFrom(providedStorageLocation.getNonce()))\n        .build();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.addStorageTypes": "  private static void addStorageTypes(\n      HdfsProtos.StorageTypeQuotaInfosProto typeQuotaInfos,\n      QuotaUsage.Builder builder) {\n    for (HdfsProtos.StorageTypeQuotaInfoProto info :\n        typeQuotaInfos.getTypeQuotaInfoList()) {\n      StorageType type = convertStorageType(info.getType());\n      builder.typeConsumed(type, info.getConsumed());\n      builder.typeQuota(type, info.getQuota());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertAclEntry": "  public static List<AclEntry> convertAclEntry(List<AclEntryProto> aclSpec) {\n    ArrayList<AclEntry> r = Lists.newArrayListWithCapacity(aclSpec.size());\n    for (AclEntryProto e : aclSpec) {\n      AclEntry.Builder builder = new AclEntry.Builder();\n      builder.setType(convert(e.getType()));\n      builder.setScope(convert(e.getScope()));\n      builder.setPermission(convert(e.getPermissions()));\n      if (e.hasName()) {\n        builder.setName(e.getName());\n      }\n      r.add(builder.build());\n    }\n    return r;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertState": "  private static StorageState convertState(State state) {\n    switch(state) {\n    case READ_ONLY_SHARED:\n      return StorageState.READ_ONLY_SHARED;\n    case NORMAL:\n    default:\n      return StorageState.NORMAL;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertFlags": "  private static EnumSet<HdfsFileStatus.Flags> convertFlags(\n      FsPermissionProto pbp) {\n    EnumSet<HdfsFileStatus.Flags> f =\n        EnumSet.noneOf(HdfsFileStatus.Flags.class);\n    FsPermission p = new FsPermissionExtension((short)pbp.getPerm());\n    if (p.getAclBit()) {\n      f.add(HdfsFileStatus.Flags.HAS_ACL);\n    }\n    if (p.getEncryptedBit()) {\n      f.add(HdfsFileStatus.Flags.HAS_CRYPT);\n    }\n    if (p.getErasureCodedBit()) {\n      f.add(HdfsFileStatus.Flags.HAS_EC);\n    }\n    return f;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertStorageType": "  public static StorageType convertStorageType(StorageTypeProto type) {\n    switch(type) {\n    case DISK:\n      return StorageType.DISK;\n    case SSD:\n      return StorageType.SSD;\n    case ARCHIVE:\n      return StorageType.ARCHIVE;\n    case RAM_DISK:\n      return StorageType.RAM_DISK;\n    case PROVIDED:\n      return StorageType.PROVIDED;\n    default:\n      throw new IllegalStateException(\n          \"BUG: StorageTypeProto not found, type=\" + type);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertLocatedBlocks2": "  public static List<LocatedBlockProto> convertLocatedBlocks2(\n      List<LocatedBlock> lb) {\n    if (lb == null) return null;\n    final int len = lb.size();\n    List<LocatedBlockProto> result = new ArrayList<>(len);\n    for (LocatedBlock aLb : lb) {\n      result.add(convertLocatedBlock(aLb));\n    }\n    return result;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertLocatedBlock": "  public static LocatedBlock[] convertLocatedBlock(LocatedBlockProto[] lb) {\n    if (lb == null) return null;\n    return convertLocatedBlock(Arrays.asList(lb)).toArray(\n        new LocatedBlock[lb.length]);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertLocatedBlocks": "  public static List<LocatedBlock> convertLocatedBlocks(\n      List<LocatedBlockProto> lb) {\n    if (lb == null) return null;\n    final int len = lb.size();\n    List<LocatedBlock> result = new ArrayList<>(len);\n    for (LocatedBlockProto aLb : lb) {\n      result.add(convertLocatedBlockProto(aLb));\n    }\n    return result;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertXAttrs": "  public static List<XAttr> convertXAttrs(List<XAttrProto> xAttrSpec) {\n    ArrayList<XAttr> xAttrs = Lists.newArrayListWithCapacity(xAttrSpec.size());\n    for (XAttrProto a : xAttrSpec) {\n      XAttr.Builder builder = new XAttr.Builder();\n      builder.setNameSpace(convert(a.getNamespace()));\n      if (a.hasName()) {\n        builder.setName(a.getName());\n      }\n      if (a.hasValue()) {\n        builder.setValue(a.getValue().toByteArray());\n      }\n      xAttrs.add(builder.build());\n    }\n    return xAttrs;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.createTypeConvert": "  private static Event.CreateEvent.INodeType createTypeConvert(\n      InotifyProtos.INodeType type) {\n    switch (type) {\n    case I_TYPE_DIRECTORY:\n      return Event.CreateEvent.INodeType.DIRECTORY;\n    case I_TYPE_FILE:\n      return Event.CreateEvent.INodeType.FILE;\n    case I_TYPE_SYMLINK:\n      return Event.CreateEvent.INodeType.SYMLINK;\n    default:\n      return null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertAclEntryProto": "  public static List<AclEntryProto> convertAclEntryProto(\n      List<AclEntry> aclSpec) {\n    ArrayList<AclEntryProto> r = Lists.newArrayListWithCapacity(aclSpec.size());\n    for (AclEntry e : aclSpec) {\n      AclEntryProto.Builder builder = AclEntryProto.newBuilder();\n      builder.setType(convert(e.getType()));\n      builder.setScope(convert(e.getScope()));\n      builder.setPermissions(convert(e.getPermission()));\n      if (e.getName() != null) {\n        builder.setName(e.getName());\n      }\n      r.add(builder.build());\n    }\n    return r;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.getFixedByteString": "  private static ByteString getFixedByteString(String key) {\n    ByteString value = fixedByteStringCache.get(key);\n    if (value == null) {\n      value = ByteString.copyFromUtf8(key);\n      fixedByteStringCache.put(key, value);\n    }\n    return value;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertRollingUpgradeStatus": "  public static RollingUpgradeStatusProto convertRollingUpgradeStatus(\n      RollingUpgradeStatus status) {\n    return RollingUpgradeStatusProto.newBuilder()\n        .setBlockPoolId(status.getBlockPoolId())\n        .setFinalized(status.isFinalized())\n        .build();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertLocatedBlockProto": "  public static LocatedBlock convertLocatedBlockProto(LocatedBlockProto proto) {\n    if (proto == null) return null;\n    List<DatanodeInfoProto> locs = proto.getLocsList();\n    DatanodeInfo[] targets = new DatanodeInfo[locs.size()];\n    for (int i = 0; i < locs.size(); i++) {\n      targets[i] = convert(locs.get(i));\n    }\n\n    final StorageType[] storageTypes = convertStorageTypes(\n        proto.getStorageTypesList(), locs.size());\n\n    final int storageIDsCount = proto.getStorageIDsCount();\n    final String[] storageIDs;\n    if (storageIDsCount == 0) {\n      storageIDs = null;\n    } else {\n      Preconditions.checkState(storageIDsCount == locs.size());\n      storageIDs = proto.getStorageIDsList()\n          .toArray(new String[storageIDsCount]);\n    }\n\n    byte[] indices = null;\n    if (proto.hasBlockIndices()) {\n      indices = proto.getBlockIndices().toByteArray();\n    }\n\n    // Set values from the isCached list, re-using references from loc\n    List<DatanodeInfo> cachedLocs = new ArrayList<>(locs.size());\n    List<Boolean> isCachedList = proto.getIsCachedList();\n    for (int i=0; i<isCachedList.size(); i++) {\n      if (isCachedList.get(i)) {\n        cachedLocs.add(targets[i]);\n      }\n    }\n\n    final LocatedBlock lb;\n    if (indices == null) {\n      lb = new LocatedBlock(PBHelperClient.convert(proto.getB()), targets,\n          storageIDs, storageTypes, proto.getOffset(), proto.getCorrupt(),\n          cachedLocs.toArray(new DatanodeInfo[cachedLocs.size()]));\n    } else {\n      lb = new LocatedStripedBlock(PBHelperClient.convert(proto.getB()),\n          targets, storageIDs, storageTypes, indices, proto.getOffset(),\n          proto.getCorrupt(),\n          cachedLocs.toArray(new DatanodeInfo[cachedLocs.size()]));\n      List<TokenProto> tokenProtos = proto.getBlockTokensList();\n      Token<BlockTokenIdentifier>[] blockTokens =\n          convertTokens(tokenProtos);\n      ((LocatedStripedBlock) lb).setBlockTokens(blockTokens);\n    }\n    lb.setBlockToken(convert(proto.getBlockToken()));\n\n    return lb;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertStorageTypes": "  public static StorageType[] convertStorageTypes(\n      List<StorageTypeProto> storageTypesList, int expectedSize) {\n    final StorageType[] storageTypes = new StorageType[expectedSize];\n    if (storageTypesList.size() != expectedSize) {\n      // missing storage types\n      Preconditions.checkState(storageTypesList.isEmpty());\n      Arrays.fill(storageTypes, StorageType.DEFAULT);\n    } else {\n      for (int i = 0; i < storageTypes.length; ++i) {\n        storageTypes[i] = convertStorageType(storageTypesList.get(i));\n      }\n    }\n    return storageTypes;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertErasureCodingPolicy": "  public static ErasureCodingPolicyProto convertErasureCodingPolicy(\n      ErasureCodingPolicyInfo info) {\n    final ErasureCodingPolicyProto.Builder builder =\n        createECPolicyProtoBuilder(info.getPolicy());\n    builder.setState(convertECState(info.getState()));\n    return builder.build();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.castEnum": "  static <T extends Enum<T>, U extends Enum<U>> U castEnum(T from, U[] to) {\n    return to[from.ordinal()];\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.getByteString": "  public static ByteString getByteString(byte[] bytes) {\n    // return singleton to reduce object allocation\n    return (bytes.length == 0) ? ByteString.EMPTY : ByteString.copyFrom(bytes);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.metadataUpdateTypeConvert": "  private static Event.MetadataUpdateEvent.MetadataType metadataUpdateTypeConvert(\n      InotifyProtos.MetadataUpdateType type) {\n    switch (type) {\n    case META_TYPE_TIMES:\n      return Event.MetadataUpdateEvent.MetadataType.TIMES;\n    case META_TYPE_REPLICATION:\n      return Event.MetadataUpdateEvent.MetadataType.REPLICATION;\n    case META_TYPE_OWNER:\n      return Event.MetadataUpdateEvent.MetadataType.OWNER;\n    case META_TYPE_PERMS:\n      return Event.MetadataUpdateEvent.MetadataType.PERMS;\n    case META_TYPE_ACLS:\n      return Event.MetadataUpdateEvent.MetadataType.ACLS;\n    case META_TYPE_XATTRS:\n      return Event.MetadataUpdateEvent.MetadataType.XATTRS;\n    default:\n      return null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.getBuilder": "  private static HdfsProtos.StorageTypeQuotaInfosProto.Builder getBuilder(\n      QuotaUsage qu) {\n    HdfsProtos.StorageTypeQuotaInfosProto.Builder isb =\n            HdfsProtos.StorageTypeQuotaInfosProto.newBuilder();\n    for (StorageType t: StorageType.getTypesSupportingQuota()) {\n      HdfsProtos.StorageTypeQuotaInfoProto info =\n          HdfsProtos.StorageTypeQuotaInfoProto.newBuilder().\n              setType(convertStorageType(t)).\n              setConsumed(qu.getTypeConsumed(t)).\n              setQuota(qu.getTypeQuota(t)).\n              build();\n      isb.addTypeQuotaInfo(info);\n    }\n    return isb;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getCurCall": "  public static ThreadLocal<Call> getCurCall() {\n    return CurCall;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcWritable.wrap": "    public static Buffer wrap(ByteBuffer bb) {\n      return new Buffer(bb);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.updateMetrics": "  void updateMetrics(String name, int queueTime, int processingTime,\n                     boolean deferredCall) {\n    rpcMetrics.addRpcQueueTime(queueTime);\n    if (!deferredCall) {\n      rpcMetrics.addRpcProcessingTime(processingTime);\n      rpcDetailedMetrics.addProcessingTime(name, processingTime);\n      callQueue.addResponseTime(name, getPriorityLevel(), queueTime,\n          processingTime);\n      if (isLogSlowRPC()) {\n        logSlowRpcCalls(name, processingTime);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.logSlowRpcCalls": "  void logSlowRpcCalls(String methodName, int processingTime) {\n    final int deviation = 3;\n\n    // 1024 for minSampleSize just a guess -- not a number computed based on\n    // sample size analysis. It is chosen with the hope that this\n    // number is high enough to avoid spurious logging, yet useful\n    // in practice.\n    final int minSampleSize = 1024;\n    final double threeSigma = rpcMetrics.getProcessingMean() +\n        (rpcMetrics.getProcessingStdDev() * deviation);\n\n    if ((rpcMetrics.getProcessingSampleCount() > minSampleSize) &&\n        (processingTime > threeSigma)) {\n      if(LOG.isWarnEnabled()) {\n        String client = CurCall.get().toString();\n        LOG.warn(\n            \"Slow RPC : \" + methodName + \" took \" + processingTime +\n                \" milliseconds to process from client \" + client);\n      }\n      rpcMetrics.incrSlowRpc();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.getPriorityLevel": "    public int getPriorityLevel() {\n      return this.priorityLevel;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.isLogSlowRPC": "  protected boolean isLogSlowRPC() {\n    return logSlowRPC;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.CallerContext.setCurrent": "  public static void setCurrent(CallerContext callerContext) {\n    CurrentCallerContextHolder.CALLER_CONTEXT.set(callerContext);\n  }"
        },
        "bug_report": {
            "Title": "Incorrect message when block is not found",
            "Description": "When client opens a file, it asks DataNode to\u00a0check the blocks' visible length. If somehow the block is not on the DN, it throws \"Cannot append to a non-existent replica\" message, which is incorrect, because\u00a0getReplicaVisibleLength() is called for different use, just not for appending to a block. It should just state \"block is not found\"\r\n\r\nThe following stacktrace comes from a CDH5.13, but it looks like the same warning exists in Apache Hadoop trunk.\r\n{noformat}\r\n2018-05-29 09:23:41,966 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getReplicaVisibleLength from 10.0.0.14:53217 Call#38334117 Retry#0\r\norg.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Cannot append to a non-existent replica BP-725378529-10.236.236.8-1410027444173:13276792346\r\n at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo(FsDatasetImpl.java:792)\r\n at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaVisibleLength(FsDatasetImpl.java:2588)\r\n at org.apache.hadoop.hdfs.server.datanode.DataNode.getReplicaVisibleLength(DataNode.java:2756)\r\n at org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolServerSideTranslatorPB.getReplicaVisibleLength(ClientDatanodeProtocolServerSideTranslatorPB.java:107)\r\n at org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ClientDatanodeProtocolService$2.callBlockingMethod(ClientDatanodeProtocolProtos.java:17873)\r\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\r\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\r\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2217)\r\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2213)\r\n at java.security.AccessController.doPrivileged(Native Method)\r\n at javax.security.auth.Subject.doAs(Subject.java:422)\r\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\r\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2211){noformat}"
        }
    },
    {
        "filename": "HDFS-11608.json",
        "creation_time": "2017-03-31T17:37:20.000+0000",
        "stack_trace": "```\njava.io.IOException: Incorrect value for packet payload size: 2147483128\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:159)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:502)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:898)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:806)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)\n        at java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead": "  private void doRead(ReadableByteChannel ch, InputStream in)\n      throws IOException {\n    // Each packet looks like:\n    //   PLEN    HLEN      HEADER     CHECKSUMS  DATA\n    //   32-bit  16-bit   <protobuf>  <variable length>\n    //\n    // PLEN:      Payload length\n    //            = length(PLEN) + length(CHECKSUMS) + length(DATA)\n    //            This length includes its own encoded length in\n    //            the sum for historical reasons.\n    //\n    // HLEN:      Header length\n    //            = length(HEADER)\n    //\n    // HEADER:    the actual packet header fields, encoded in protobuf\n    // CHECKSUMS: the crcs for the data chunk. May be missing if\n    //            checksums were not requested\n    // DATA       the actual block data\n    Preconditions.checkState(curHeader == null || !curHeader.isLastPacketInBlock());\n\n    curPacketBuf.clear();\n    curPacketBuf.limit(PacketHeader.PKT_LENGTHS_LEN);\n    doReadFully(ch, in, curPacketBuf);\n    curPacketBuf.flip();\n    int payloadLen = curPacketBuf.getInt();\n\n    if (payloadLen < Ints.BYTES) {\n      // The \"payload length\" includes its own length. Therefore it\n      // should never be less than 4 bytes\n      throw new IOException(\"Invalid payload length \" +\n          payloadLen);\n    }\n    int dataPlusChecksumLen = payloadLen - Ints.BYTES;\n    int headerLen = curPacketBuf.getShort();\n    if (headerLen < 0) {\n      throw new IOException(\"Invalid header length \" + headerLen);\n    }\n\n    LOG.trace(\"readNextPacket: dataPlusChecksumLen={}, headerLen={}\",\n        dataPlusChecksumLen, headerLen);\n\n    // Sanity check the buffer size so we don't allocate too much memory\n    // and OOME.\n    int totalLen = payloadLen + headerLen;\n    if (totalLen < 0 || totalLen > MAX_PACKET_SIZE) {\n      throw new IOException(\"Incorrect value for packet payload size: \" +\n                            payloadLen);\n    }\n\n    // Make sure we have space for the whole packet, and\n    // read it.\n    reallocPacketBuf(PacketHeader.PKT_LENGTHS_LEN +\n        dataPlusChecksumLen + headerLen);\n    curPacketBuf.clear();\n    curPacketBuf.position(PacketHeader.PKT_LENGTHS_LEN);\n    curPacketBuf.limit(PacketHeader.PKT_LENGTHS_LEN +\n        dataPlusChecksumLen + headerLen);\n    doReadFully(ch, in, curPacketBuf);\n    curPacketBuf.flip();\n    curPacketBuf.position(PacketHeader.PKT_LENGTHS_LEN);\n\n    // Extract the header from the front of the buffer (after the length prefixes)\n    byte[] headerBuf = new byte[headerLen];\n    curPacketBuf.get(headerBuf);\n    if (curHeader == null) {\n      curHeader = new PacketHeader();\n    }\n    curHeader.setFieldsFromData(payloadLen, headerBuf);\n\n    // Compute the sub-slices of the packet\n    int checksumLen = dataPlusChecksumLen - curHeader.getDataLen();\n    if (checksumLen < 0) {\n      throw new IOException(\"Invalid packet: data length in packet header \" +\n          \"exceeds data length received. dataPlusChecksumLen=\" +\n          dataPlusChecksumLen + \" header: \" + curHeader);\n    }\n\n    reslicePacket(headerLen, checksumLen, curHeader.getDataLen());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.reslicePacket": "  private void reslicePacket(\n      int headerLen, int checksumsLen, int dataLen) {\n    // Packet structure (refer to doRead() for details):\n    //   PLEN    HLEN      HEADER     CHECKSUMS  DATA\n    //   32-bit  16-bit   <protobuf>  <variable length>\n    //   |--- lenThroughHeader ----|\n    //   |----------- lenThroughChecksums   ----|\n    //   |------------------- lenThroughData    ------|\n    int lenThroughHeader = PacketHeader.PKT_LENGTHS_LEN + headerLen;\n    int lenThroughChecksums = lenThroughHeader + checksumsLen;\n    int lenThroughData = lenThroughChecksums + dataLen;\n\n    assert dataLen >= 0 : \"invalid datalen: \" + dataLen;\n    assert curPacketBuf.position() == lenThroughHeader;\n    assert curPacketBuf.limit() == lenThroughData :\n      \"headerLen= \" + headerLen + \" clen=\" + checksumsLen + \" dlen=\" + dataLen +\n      \" rem=\" + curPacketBuf.remaining();\n\n    // Slice the checksums.\n    curPacketBuf.position(lenThroughHeader);\n    curPacketBuf.limit(lenThroughChecksums);\n    curChecksumSlice = curPacketBuf.slice();\n\n    // Slice the data.\n    curPacketBuf.position(lenThroughChecksums);\n    curPacketBuf.limit(lenThroughData);\n    curDataSlice = curPacketBuf.slice();\n\n    // Reset buffer to point to the entirety of the packet (including\n    // length prefixes)\n    curPacketBuf.position(0);\n    curPacketBuf.limit(lenThroughData);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully": "  private static void doReadFully(ReadableByteChannel ch, InputStream in,\n      ByteBuffer buf) throws IOException {\n    if (ch != null) {\n      readChannelFully(ch, buf);\n    } else {\n      Preconditions.checkState(!buf.isDirect(),\n          \"Must not use direct buffers with InputStream API\");\n      IOUtils.readFully(in, buf.array(),\n          buf.arrayOffset() + buf.position(),\n          buf.remaining());\n      buf.position(buf.position() + buf.remaining());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.reallocPacketBuf": "  private void reallocPacketBuf(int atLeastCapacity) {\n    // Realloc the buffer if this packet is longer than the previous\n    // one.\n    if (curPacketBuf == null ||\n        curPacketBuf.capacity() < atLeastCapacity) {\n      ByteBuffer newBuf;\n      if (useDirectBuffers) {\n        newBuf = bufferPool.getBuffer(atLeastCapacity);\n      } else {\n        newBuf = ByteBuffer.allocate(atLeastCapacity);\n      }\n      // If reallocing an existing buffer, copy the old packet length\n      // prefixes over\n      if (curPacketBuf != null) {\n        curPacketBuf.flip();\n        newBuf.put(curPacketBuf);\n      }\n\n      returnPacketBufToPool();\n      curPacketBuf = newBuf;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket": "  public void receiveNextPacket(InputStream in) throws IOException {\n    doRead(null, in);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket": "  private int receivePacket() throws IOException {\n    // read the next packet\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader header = packetReceiver.getHeader();\n    if (LOG.isDebugEnabled()){\n      LOG.debug(\"Receiving one packet for block \" + block +\n                \": \" + header);\n    }\n\n    // Sanity check the header\n    if (header.getOffsetInBlock() > replicaInfo.getNumBytes()) {\n      throw new IOException(\"Received an out-of-sequence packet for \" + block + \n          \"from \" + inAddr + \" at offset \" + header.getOffsetInBlock() +\n          \". Expecting packet starting at \" + replicaInfo.getNumBytes());\n    }\n    if (header.getDataLen() < 0) {\n      throw new IOException(\"Got wrong length during writeBlock(\" + block + \n                            \") from \" + inAddr + \" at offset \" + \n                            header.getOffsetInBlock() + \": \" +\n                            header.getDataLen()); \n    }\n\n    long offsetInBlock = header.getOffsetInBlock();\n    long seqno = header.getSeqno();\n    boolean lastPacketInBlock = header.isLastPacketInBlock();\n    final int len = header.getDataLen();\n    boolean syncBlock = header.getSyncBlock();\n\n    // avoid double sync'ing on close\n    if (syncBlock && lastPacketInBlock) {\n      this.syncOnClose = false;\n    }\n\n    // update received bytes\n    final long firstByteInBlock = offsetInBlock;\n    offsetInBlock += len;\n    if (replicaInfo.getNumBytes() < offsetInBlock) {\n      replicaInfo.setNumBytes(offsetInBlock);\n    }\n    \n    // put in queue for pending acks, unless sync was requested\n    if (responder != null && !syncBlock && !shouldVerifyChecksum()) {\n      ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n          lastPacketInBlock, offsetInBlock, Status.SUCCESS);\n    }\n\n    // Drop heartbeat for testing.\n    if (seqno < 0 && len == 0 &&\n        DataNodeFaultInjector.get().dropHeartbeatPacket()) {\n      return 0;\n    }\n\n    //First write the packet to the mirror:\n    if (mirrorOut != null && !mirrorError) {\n      try {\n        long begin = Time.monotonicNow();\n        // For testing. Normally no-op.\n        DataNodeFaultInjector.get().stopSendingPacketDownstream(mirrorAddr);\n        packetReceiver.mirrorPacketTo(mirrorOut);\n        mirrorOut.flush();\n        long now = Time.monotonicNow();\n        setLastSentTime(now);\n        long duration = now - begin;\n        DataNodeFaultInjector.get().logDelaySendingPacketDownstream(\n            mirrorAddr,\n            duration);\n        trackSendPacketToLastNodeInPipeline(duration);\n        if (duration > datanodeSlowLogThresholdMs) {\n          LOG.warn(\"Slow BlockReceiver write packet to mirror took \" + duration\n              + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms)\");\n        }\n      } catch (IOException e) {\n        handleMirrorOutError(e);\n      }\n    }\n    \n    ByteBuffer dataBuf = packetReceiver.getDataSlice();\n    ByteBuffer checksumBuf = packetReceiver.getChecksumSlice();\n    \n    if (lastPacketInBlock || len == 0) {\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"Receiving an empty packet or the end of the block \" + block);\n      }\n      // sync block if requested\n      if (syncBlock) {\n        flushOrSync(true);\n      }\n    } else {\n      final int checksumLen = diskChecksum.getChecksumSize(len);\n      final int checksumReceivedLen = checksumBuf.capacity();\n\n      if (checksumReceivedLen > 0 && checksumReceivedLen != checksumLen) {\n        throw new IOException(\"Invalid checksum length: received length is \"\n            + checksumReceivedLen + \" but expected length is \" + checksumLen);\n      }\n\n      if (checksumReceivedLen > 0 && shouldVerifyChecksum()) {\n        try {\n          verifyChunks(dataBuf, checksumBuf);\n        } catch (IOException ioe) {\n          // checksum error detected locally. there is no reason to continue.\n          if (responder != null) {\n            try {\n              ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n                  lastPacketInBlock, offsetInBlock,\n                  Status.ERROR_CHECKSUM);\n              // Wait until the responder sends back the response\n              // and interrupt this thread.\n              Thread.sleep(3000);\n            } catch (InterruptedException e) { }\n          }\n          throw new IOException(\"Terminating due to a checksum error.\" + ioe);\n        }\n \n        if (needsChecksumTranslation) {\n          // overwrite the checksums in the packet buffer with the\n          // appropriate polynomial for the disk storage.\n          translateChunks(dataBuf, checksumBuf);\n        }\n      }\n\n      if (checksumReceivedLen == 0 && !streams.isTransientStorage()) {\n        // checksum is missing, need to calculate it\n        checksumBuf = ByteBuffer.allocate(checksumLen);\n        diskChecksum.calculateChunkedSums(dataBuf, checksumBuf);\n      }\n      \n      // by this point, the data in the buffer uses the disk checksum\n\n      final boolean shouldNotWriteChecksum = checksumReceivedLen == 0\n          && streams.isTransientStorage();\n      try {\n        long onDiskLen = replicaInfo.getBytesOnDisk();\n        if (onDiskLen<offsetInBlock) {\n          // Normally the beginning of an incoming packet is aligned with the\n          // existing data on disk. If the beginning packet data offset is not\n          // checksum chunk aligned, the end of packet will not go beyond the\n          // next chunk boundary.\n          // When a failure-recovery is involved, the client state and the\n          // the datanode state may not exactly agree. I.e. the client may\n          // resend part of data that is already on disk. Correct number of\n          // bytes should be skipped when writing the data and checksum\n          // buffers out to disk.\n          long partialChunkSizeOnDisk = onDiskLen % bytesPerChecksum;\n          long lastChunkBoundary = onDiskLen - partialChunkSizeOnDisk;\n          boolean alignedOnDisk = partialChunkSizeOnDisk == 0;\n          boolean alignedInPacket = firstByteInBlock % bytesPerChecksum == 0;\n\n          // If the end of the on-disk data is not chunk-aligned, the last\n          // checksum needs to be overwritten.\n          boolean overwriteLastCrc = !alignedOnDisk && !shouldNotWriteChecksum;\n          // If the starting offset of the packat data is at the last chunk\n          // boundary of the data on disk, the partial checksum recalculation\n          // can be skipped and the checksum supplied by the client can be used\n          // instead. This reduces disk reads and cpu load.\n          boolean doCrcRecalc = overwriteLastCrc &&\n              (lastChunkBoundary != firstByteInBlock);\n\n          // If this is a partial chunk, then verify that this is the only\n          // chunk in the packet. If the starting offset is not chunk\n          // aligned, the packet should terminate at or before the next\n          // chunk boundary.\n          if (!alignedInPacket && len > bytesPerChecksum) {\n            throw new IOException(\"Unexpected packet data length for \"\n                +  block + \" from \" + inAddr + \": a partial chunk must be \"\n                + \" sent in an individual packet (data length = \" + len\n                +  \" > bytesPerChecksum = \" + bytesPerChecksum + \")\");\n          }\n\n          // If the last portion of the block file is not a full chunk,\n          // then read in pre-existing partial data chunk and recalculate\n          // the checksum so that the checksum calculation can continue\n          // from the right state. If the client provided the checksum for\n          // the whole chunk, this is not necessary.\n          Checksum partialCrc = null;\n          if (doCrcRecalc) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"receivePacket for \" + block \n                  + \": previous write did not end at the chunk boundary.\"\n                  + \" onDiskLen=\" + onDiskLen);\n            }\n            long offsetInChecksum = BlockMetadataHeader.getHeaderSize() +\n                onDiskLen / bytesPerChecksum * checksumSize;\n            partialCrc = computePartialChunkCrc(onDiskLen, offsetInChecksum);\n          }\n\n          // The data buffer position where write will begin. If the packet\n          // data and on-disk data have no overlap, this will not be at the\n          // beginning of the buffer.\n          int startByteToDisk = (int)(onDiskLen-firstByteInBlock) \n              + dataBuf.arrayOffset() + dataBuf.position();\n\n          // Actual number of data bytes to write.\n          int numBytesToDisk = (int)(offsetInBlock-onDiskLen);\n          \n          // Write data to disk.\n          long begin = Time.monotonicNow();\n          streams.writeDataToDisk(dataBuf.array(),\n              startByteToDisk, numBytesToDisk);\n          long duration = Time.monotonicNow() - begin;\n          if (duration > datanodeSlowLogThresholdMs) {\n            LOG.warn(\"Slow BlockReceiver write data to disk cost:\" + duration\n                + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms)\");\n          }\n\n          if (duration > maxWriteToDiskMs) {\n            maxWriteToDiskMs = duration;\n          }\n\n          final byte[] lastCrc;\n          if (shouldNotWriteChecksum) {\n            lastCrc = null;\n          } else {\n            int skip = 0;\n            byte[] crcBytes = null;\n\n            // First, prepare to overwrite the partial crc at the end.\n            if (overwriteLastCrc) { // not chunk-aligned on disk\n              // prepare to overwrite last checksum\n              adjustCrcFilePosition();\n            }\n\n            // The CRC was recalculated for the last partial chunk. Update the\n            // CRC by reading the rest of the chunk, then write it out.\n            if (doCrcRecalc) {\n              // Calculate new crc for this chunk.\n              int bytesToReadForRecalc =\n                  (int)(bytesPerChecksum - partialChunkSizeOnDisk);\n              if (numBytesToDisk < bytesToReadForRecalc) {\n                bytesToReadForRecalc = numBytesToDisk;\n              }\n\n              partialCrc.update(dataBuf.array(), startByteToDisk,\n                  bytesToReadForRecalc);\n              byte[] buf = FSOutputSummer.convertToByteStream(partialCrc,\n                  checksumSize);\n              crcBytes = copyLastChunkChecksum(buf, checksumSize, buf.length);\n              checksumOut.write(buf);\n              if(LOG.isDebugEnabled()) {\n                LOG.debug(\"Writing out partial crc for data len \" + len +\n                    \", skip=\" + skip);\n              }\n              skip++; //  For the partial chunk that was just read.\n            }\n\n            // Determine how many checksums need to be skipped up to the last\n            // boundary. The checksum after the boundary was already counted\n            // above. Only count the number of checksums skipped up to the\n            // boundary here.\n            long skippedDataBytes = lastChunkBoundary - firstByteInBlock;\n\n            if (skippedDataBytes > 0) {\n              skip += (int)(skippedDataBytes / bytesPerChecksum) +\n                  ((skippedDataBytes % bytesPerChecksum == 0) ? 0 : 1);\n            }\n            skip *= checksumSize; // Convert to number of bytes\n\n            // write the rest of checksum\n            final int offset = checksumBuf.arrayOffset() +\n                checksumBuf.position() + skip;\n            final int end = offset + checksumLen - skip;\n            // If offset >= end, there is no more checksum to write.\n            // I.e. a partial chunk checksum rewrite happened and there is no\n            // more to write after that.\n            if (offset >= end && doCrcRecalc) {\n              lastCrc = crcBytes;\n            } else {\n              final int remainingBytes = checksumLen - skip;\n              lastCrc = copyLastChunkChecksum(checksumBuf.array(),\n                  checksumSize, end);\n              checksumOut.write(checksumBuf.array(), offset, remainingBytes);\n            }\n          }\n\n          /// flush entire packet, sync if requested\n          flushOrSync(syncBlock);\n          \n          replicaInfo.setLastChecksumAndDataLen(offsetInBlock, lastCrc);\n\n          datanode.metrics.incrBytesWritten(len);\n          datanode.metrics.incrTotalWriteTime(duration);\n\n          manageWriterOsCache(offsetInBlock);\n        }\n      } catch (IOException iex) {\n        // Volume error check moved to FileIoProvider\n        throw iex;\n      }\n    }\n\n    // if sync was requested, put in queue for pending acks here\n    // (after the fsync finished)\n    if (responder != null && (syncBlock || shouldVerifyChecksum())) {\n      ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n          lastPacketInBlock, offsetInBlock, Status.SUCCESS);\n    }\n\n    /*\n     * Send in-progress responses for the replaceBlock() calls back to caller to\n     * avoid timeouts due to balancer throttling. HDFS-6247\n     */\n    if (isReplaceBlock\n        && (Time.monotonicNow() - lastResponseTime > responseInterval)) {\n      BlockOpResponseProto.Builder response = BlockOpResponseProto.newBuilder()\n          .setStatus(Status.IN_PROGRESS);\n      response.build().writeDelimitedTo(replyOut);\n      replyOut.flush();\n\n      lastResponseTime = Time.monotonicNow();\n    }\n\n    if (throttler != null) { // throttle I/O\n      throttler.throttle(len);\n    }\n    \n    return lastPacketInBlock?-1:len;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.translateChunks": "  private void translateChunks(ByteBuffer dataBuf, ByteBuffer checksumBuf) {\n    diskChecksum.calculateChunkedSums(dataBuf, checksumBuf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.adjustCrcFilePosition": "  private void adjustCrcFilePosition() throws IOException {\n    streams.flushDataOut();\n    if (checksumOut != null) {\n      checksumOut.flush();\n    }\n\n    // rollback the position of the meta file\n    datanode.data.adjustCrcChannelPosition(block, streams, checksumSize);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.handleMirrorOutError": "  private void handleMirrorOutError(IOException ioe) throws IOException {\n    String bpid = block.getBlockPoolId();\n    LOG.info(datanode.getDNRegistrationForBP(bpid)\n        + \":Exception writing \" + block + \" to mirror \" + mirrorAddr, ioe);\n    if (Thread.interrupted()) { // shut down if the thread is interrupted\n      throw ioe;\n    } else { // encounter an error while writing to mirror\n      // continue to run even if can not write to mirror\n      // notify client of the error\n      // and wait for the client to shut down the pipeline\n      mirrorError = true;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.manageWriterOsCache": "  private void manageWriterOsCache(long offsetInBlock) {\n    try {\n      if (streams.getOutFd() != null &&\n          offsetInBlock > lastCacheManagementOffset + CACHE_DROP_LAG_BYTES) {\n        long begin = Time.monotonicNow();\n        //\n        // For SYNC_FILE_RANGE_WRITE, we want to sync from\n        // lastCacheManagementOffset to a position \"two windows ago\"\n        //\n        //                         <========= sync ===========>\n        // +-----------------------O--------------------------X\n        // start                  last                      curPos\n        // of file                 \n        //\n        if (syncBehindWrites) {\n          if (syncBehindWritesInBackground) {\n            this.datanode.getFSDataset().submitBackgroundSyncFileRangeRequest(\n                block, streams, lastCacheManagementOffset,\n                offsetInBlock - lastCacheManagementOffset,\n                SYNC_FILE_RANGE_WRITE);\n          } else {\n            streams.syncFileRangeIfPossible(lastCacheManagementOffset,\n                offsetInBlock - lastCacheManagementOffset,\n                SYNC_FILE_RANGE_WRITE);\n          }\n        }\n        //\n        // For POSIX_FADV_DONTNEED, we want to drop from the beginning \n        // of the file to a position prior to the current position.\n        //\n        // <=== drop =====> \n        //                 <---W--->\n        // +--------------+--------O--------------------------X\n        // start        dropPos   last                      curPos\n        // of file             \n        //                     \n        long dropPos = lastCacheManagementOffset - CACHE_DROP_LAG_BYTES;\n        if (dropPos > 0 && dropCacheBehindWrites) {\n          streams.dropCacheBehindWrites(block.getBlockName(), 0, dropPos,\n              POSIX_FADV_DONTNEED);\n        }\n        lastCacheManagementOffset = offsetInBlock;\n        long duration = Time.monotonicNow() - begin;\n        if (duration > datanodeSlowLogThresholdMs) {\n          LOG.warn(\"Slow manageWriterOsCache took \" + duration\n              + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms)\");\n        }\n      }\n    } catch (Throwable t) {\n      LOG.warn(\"Error managing cache for writer of block \" + block, t);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.copyLastChunkChecksum": "  private static byte[] copyLastChunkChecksum(byte[] array, int size, int end) {\n    return Arrays.copyOfRange(array, end - size, end);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.trackSendPacketToLastNodeInPipeline": "  private void trackSendPacketToLastNodeInPipeline(final long elapsedMs) {\n    final DataNodePeerMetrics peerMetrics = datanode.getPeerMetrics();\n    if (peerMetrics != null && isPenultimateNode) {\n      peerMetrics.addSendPacketDownstream(mirrorNameForMetrics, elapsedMs);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.setLastSentTime": "  synchronized void setLastSentTime(long sentTime) {\n    lastSentTime = sentTime;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.verifyChunks": "  private void verifyChunks(ByteBuffer dataBuf, ByteBuffer checksumBuf)\n      throws IOException {\n    try {\n      clientChecksum.verifyChunkedSums(dataBuf, checksumBuf, clientname, 0);\n    } catch (ChecksumException ce) {\n      PacketHeader header = packetReceiver.getHeader();\n      String specificOffset = \"specific offsets are:\"\n          + \" offsetInBlock = \" + header.getOffsetInBlock()\n          + \" offsetInPacket = \" + ce.getPos();\n      LOG.warn(\"Checksum error in block \"\n          + block + \" from \" + inAddr\n          + \", \" + specificOffset, ce);\n      // No need to report to namenode when client is writing.\n      if (srcDataNode != null && isDatanode) {\n        try {\n          LOG.info(\"report corrupt \" + block + \" from datanode \" +\n                    srcDataNode + \" to namenode\");\n          datanode.reportRemoteBadBlock(srcDataNode, block);\n        } catch (IOException e) {\n          LOG.warn(\"Failed to report bad \" + block + \n                    \" from datanode \" + srcDataNode + \" to namenode\");\n        }\n      }\n      throw new IOException(\"Unexpected checksum mismatch while writing \"\n          + block + \" from \" + inAddr);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.computePartialChunkCrc": "  private Checksum computePartialChunkCrc(long blkoff, long ckoff)\n      throws IOException {\n\n    // find offset of the beginning of partial chunk.\n    //\n    int sizePartialChunk = (int) (blkoff % bytesPerChecksum);\n    blkoff = blkoff - sizePartialChunk;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"computePartialChunkCrc for \" + block\n          + \": sizePartialChunk=\" + sizePartialChunk\n          + \", block offset=\" + blkoff\n          + \", metafile offset=\" + ckoff);\n    }\n\n    // create an input stream from the block file\n    // and read in partial crc chunk into temporary buffer\n    //\n    byte[] buf = new byte[sizePartialChunk];\n    byte[] crcbuf = new byte[checksumSize];\n    try (ReplicaInputStreams instr =\n        datanode.data.getTmpInputStreams(block, blkoff, ckoff)) {\n      instr.readDataFully(buf, 0, sizePartialChunk);\n\n      // open meta file and read in crc value computer earlier\n      instr.readChecksumFully(crcbuf, 0, crcbuf.length);\n    }\n\n    // compute crc of partial chunk from data read in the block file.\n    final Checksum partialCrc = DataChecksum.newDataChecksum(\n        diskChecksum.getChecksumType(), diskChecksum.getBytesPerChecksum());\n    partialCrc.update(buf, 0, sizePartialChunk);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Read in partial CRC chunk from disk for \" + block);\n    }\n\n    // paranoia! verify that the pre-computed crc matches what we\n    // recalculated just now\n    if (partialCrc.getValue() != checksum2long(crcbuf)) {\n      String msg = \"Partial CRC \" + partialCrc.getValue() +\n                   \" does not match value computed the \" +\n                   \" last time file was closed \" +\n                   checksum2long(crcbuf);\n      throw new IOException(msg);\n    }\n    return partialCrc;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.shouldVerifyChecksum": "  private boolean shouldVerifyChecksum() {\n    return (mirrorOut == null || isDatanode || needsChecksumTranslation);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.flushOrSync": "  void flushOrSync(boolean isSync) throws IOException {\n    long flushTotalNanos = 0;\n    long begin = Time.monotonicNow();\n    if (checksumOut != null) {\n      long flushStartNanos = System.nanoTime();\n      checksumOut.flush();\n      long flushEndNanos = System.nanoTime();\n      if (isSync) {\n        streams.syncChecksumOut();\n        datanode.metrics.addFsyncNanos(System.nanoTime() - flushEndNanos);\n      }\n      flushTotalNanos += flushEndNanos - flushStartNanos;\n    }\n    if (streams.getDataOut() != null) {\n      long flushStartNanos = System.nanoTime();\n      streams.flushDataOut();\n      long flushEndNanos = System.nanoTime();\n      if (isSync) {\n        long fsyncStartNanos = flushEndNanos;\n        streams.syncDataOut();\n        datanode.metrics.addFsyncNanos(System.nanoTime() - fsyncStartNanos);\n      }\n      flushTotalNanos += flushEndNanos - flushStartNanos;\n    }\n    if (checksumOut != null || streams.getDataOut() != null) {\n      datanode.metrics.addFlushNanos(flushTotalNanos);\n      if (isSync) {\n        datanode.metrics.incrFsyncCount();\n      }\n    }\n    long duration = Time.monotonicNow() - begin;\n    if (duration > datanodeSlowLogThresholdMs) {\n      LOG.warn(\"Slow flushOrSync took \" + duration + \"ms (threshold=\"\n          + datanodeSlowLogThresholdMs + \"ms), isSync:\" + isSync + \", flushTotalNanos=\"\n          + flushTotalNanos + \"ns\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock": "  void receiveBlock(\n      DataOutputStream mirrOut, // output to next datanode\n      DataInputStream mirrIn,   // input from next datanode\n      DataOutputStream replyOut,  // output to previous datanode\n      String mirrAddr, DataTransferThrottler throttlerArg,\n      DatanodeInfo[] downstreams,\n      boolean isReplaceBlock) throws IOException {\n\n    syncOnClose = datanode.getDnConf().syncOnClose;\n    boolean responderClosed = false;\n    mirrorOut = mirrOut;\n    mirrorAddr = mirrAddr;\n    isPenultimateNode = ((downstreams != null) && (downstreams.length == 1));\n    if (isPenultimateNode) {\n      mirrorNameForMetrics = (downstreams[0].getInfoSecurePort() != 0 ?\n          downstreams[0].getInfoSecureAddr() : downstreams[0].getInfoAddr());\n      LOG.debug(\"Will collect peer metrics for downstream node {}\",\n          mirrorNameForMetrics);\n    }\n    throttler = throttlerArg;\n\n    this.replyOut = replyOut;\n    this.isReplaceBlock = isReplaceBlock;\n\n    try {\n      if (isClient && !isTransfer) {\n        responder = new Daemon(datanode.threadGroup, \n            new PacketResponder(replyOut, mirrIn, downstreams));\n        responder.start(); // start thread to processes responses\n      }\n\n      while (receivePacket() >= 0) { /* Receive until the last packet */ }\n\n      // wait for all outstanding packet responses. And then\n      // indicate responder to gracefully shutdown.\n      // Mark that responder has been closed for future processing\n      if (responder != null) {\n        ((PacketResponder)responder.getRunnable()).close();\n        responderClosed = true;\n      }\n\n      // If this write is for a replication or transfer-RBW/Finalized,\n      // then finalize block or convert temporary to RBW.\n      // For client-writes, the block is finalized in the PacketResponder.\n      if (isDatanode || isTransfer) {\n        // Hold a volume reference to finalize block.\n        try (ReplicaHandler handler = claimReplicaHandler()) {\n          // close the block/crc files\n          close();\n          block.setNumBytes(replicaInfo.getNumBytes());\n\n          if (stage == BlockConstructionStage.TRANSFER_RBW) {\n            // for TRANSFER_RBW, convert temporary to RBW\n            datanode.data.convertTemporaryToRbw(block);\n          } else {\n            // for isDatnode or TRANSFER_FINALIZED\n            // Finalize the block.\n            datanode.data.finalizeBlock(block);\n          }\n        }\n        datanode.metrics.incrBlocksWritten();\n      }\n\n    } catch (IOException ioe) {\n      replicaInfo.releaseAllBytesReserved();\n      if (datanode.isRestarting()) {\n        // Do not throw if shutting down for restart. Otherwise, it will cause\n        // premature termination of responder.\n        LOG.info(\"Shutting down for restart (\" + block + \").\");\n      } else {\n        LOG.info(\"Exception for \" + block, ioe);\n        throw ioe;\n      }\n    } finally {\n      // Clear the previous interrupt state of this thread.\n      Thread.interrupted();\n\n      // If a shutdown for restart was initiated, upstream needs to be notified.\n      // There is no need to do anything special if the responder was closed\n      // normally.\n      if (!responderClosed) { // Data transfer was not complete.\n        if (responder != null) {\n          // In case this datanode is shutting down for quick restart,\n          // send a special ack upstream.\n          if (datanode.isRestarting() && isClient && !isTransfer) {\n            try (Writer out = new OutputStreamWriter(\n                replicaInfo.createRestartMetaStream(), \"UTF-8\")) {\n              // write out the current time.\n              out.write(Long.toString(Time.now() + restartBudget));\n              out.flush();\n            } catch (IOException ioe) {\n              // The worst case is not recovering this RBW replica. \n              // Client will fall back to regular pipeline recovery.\n            } finally {\n              IOUtils.closeStream(streams.getDataOut());\n            }\n            try {              \n              // Even if the connection is closed after the ack packet is\n              // flushed, the client can react to the connection closure \n              // first. Insert a delay to lower the chance of client \n              // missing the OOB ack.\n              Thread.sleep(1000);\n            } catch (InterruptedException ie) {\n              // It is already going down. Ignore this.\n            }\n          }\n          responder.interrupt();\n        }\n        IOUtils.closeStream(this);\n        cleanupBlock();\n      }\n      if (responder != null) {\n        try {\n          responder.interrupt();\n          // join() on the responder should timeout a bit earlier than the\n          // configured deadline. Otherwise, the join() on this thread will\n          // likely timeout as well.\n          long joinTimeout = datanode.getDnConf().getXceiverStopTimeout();\n          joinTimeout = joinTimeout > 1  ? joinTimeout*8/10 : joinTimeout;\n          responder.join(joinTimeout);\n          if (responder.isAlive()) {\n            String msg = \"Join on responder thread \" + responder\n                + \" timed out\";\n            LOG.warn(msg + \"\\n\" + StringUtils.getStackTrace(responder));\n            throw new IOException(msg);\n          }\n        } catch (InterruptedException e) {\n          responder.interrupt();\n          // do not throw if shutting down for restart.\n          if (!datanode.isRestarting()) {\n            throw new IOException(\"Interrupted receiveBlock\");\n          }\n        }\n        responder = null;\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.claimReplicaHandler": "  private ReplicaHandler claimReplicaHandler() {\n    ReplicaHandler handler = replicaHandler;\n    replicaHandler = null;\n    return handler;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.cleanupBlock": "  private void cleanupBlock() throws IOException {\n    if (isDatanode) {\n      datanode.data.unfinalizeBlock(block);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.toString": "    public String toString() {\n      return getClass().getSimpleName() + \"(seqno=\" + seqno\n        + \", lastPacketInBlock=\" + lastPacketInBlock\n        + \", offsetInBlock=\" + offsetInBlock\n        + \", ackEnqueueNanoTime=\" + ackEnqueueNanoTime\n        + \", ackStatus=\" + ackStatus\n        + \")\";\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.close": "    public void close() {\n      synchronized(ackQueue) {\n        while (isRunning() && ackQueue.size() != 0) {\n          try {\n            ackQueue.wait();\n          } catch (InterruptedException e) {\n            running = false;\n            Thread.currentThread().interrupt();\n          }\n        }\n        if(LOG.isDebugEnabled()) {\n          LOG.debug(myString + \": closing\");\n        }\n        running = false;\n        ackQueue.notifyAll();\n      }\n\n      synchronized(this) {\n        running = false;\n        notifyAll();\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock": "  public void writeBlock(final ExtendedBlock block,\n      final StorageType storageType, \n      final Token<BlockTokenIdentifier> blockToken,\n      final String clientname,\n      final DatanodeInfo[] targets,\n      final StorageType[] targetStorageTypes, \n      final DatanodeInfo srcDataNode,\n      final BlockConstructionStage stage,\n      final int pipelineSize,\n      final long minBytesRcvd,\n      final long maxBytesRcvd,\n      final long latestGenerationStamp,\n      DataChecksum requestedChecksum,\n      CachingStrategy cachingStrategy,\n      boolean allowLazyPersist,\n      final boolean pinning,\n      final boolean[] targetPinnings) throws IOException {\n    previousOpClientName = clientname;\n    updateCurrentThreadName(\"Receiving block \" + block);\n    final boolean isDatanode = clientname.length() == 0;\n    final boolean isClient = !isDatanode;\n    final boolean isTransfer = stage == BlockConstructionStage.TRANSFER_RBW\n        || stage == BlockConstructionStage.TRANSFER_FINALIZED;\n    allowLazyPersist = allowLazyPersist &&\n        (dnConf.getAllowNonLocalLazyPersist() || peer.isLocal());\n    long size = 0;\n    // reply to upstream datanode or client \n    final DataOutputStream replyOut = getBufferedOutputStream();\n    checkAccess(replyOut, isClient, block, blockToken,\n        Op.WRITE_BLOCK, BlockTokenIdentifier.AccessMode.WRITE);\n    // check single target for transfer-RBW/Finalized \n    if (isTransfer && targets.length > 0) {\n      throw new IOException(stage + \" does not support multiple targets \"\n          + Arrays.asList(targets));\n    }\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"opWriteBlock: stage=\" + stage + \", clientname=\" + clientname \n      \t\t+ \"\\n  block  =\" + block + \", newGs=\" + latestGenerationStamp\n      \t\t+ \", bytesRcvd=[\" + minBytesRcvd + \", \" + maxBytesRcvd + \"]\"\n          + \"\\n  targets=\" + Arrays.asList(targets)\n          + \"; pipelineSize=\" + pipelineSize + \", srcDataNode=\" + srcDataNode\n          + \", pinning=\" + pinning);\n      LOG.debug(\"isDatanode=\" + isDatanode\n          + \", isClient=\" + isClient\n          + \", isTransfer=\" + isTransfer);\n      LOG.debug(\"writeBlock receive buf size \" + peer.getReceiveBufferSize() +\n                \" tcp no delay \" + peer.getTcpNoDelay());\n    }\n\n    // We later mutate block's generation stamp and length, but we need to\n    // forward the original version of the block to downstream mirrors, so\n    // make a copy here.\n    final ExtendedBlock originalBlock = new ExtendedBlock(block);\n    if (block.getNumBytes() == 0) {\n      block.setNumBytes(dataXceiverServer.estimateBlockSize);\n    }\n    LOG.info(\"Receiving \" + block + \" src: \" + remoteAddress + \" dest: \"\n        + localAddress);\n\n    DataOutputStream mirrorOut = null;  // stream to next target\n    DataInputStream mirrorIn = null;    // reply from next target\n    Socket mirrorSock = null;           // socket to next target\n    String mirrorNode = null;           // the name:port of next target\n    String firstBadLink = \"\";           // first datanode that failed in connection setup\n    Status mirrorInStatus = SUCCESS;\n    final String storageUuid;\n    final boolean isOnTransientStorage;\n    try {\n      final Replica replica;\n      if (isDatanode || \n          stage != BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        // open a block receiver\n        setCurrentBlockReceiver(getBlockReceiver(block, storageType, in,\n            peer.getRemoteAddressString(),\n            peer.getLocalAddressString(),\n            stage, latestGenerationStamp, minBytesRcvd, maxBytesRcvd,\n            clientname, srcDataNode, datanode, requestedChecksum,\n            cachingStrategy, allowLazyPersist, pinning));\n        replica = blockReceiver.getReplica();\n      } else {\n        replica = datanode.data.recoverClose(\n            block, latestGenerationStamp, minBytesRcvd);\n      }\n      storageUuid = replica.getStorageUuid();\n      isOnTransientStorage = replica.isOnTransientStorage();\n\n      //\n      // Connect to downstream machine, if appropriate\n      //\n      if (targets.length > 0) {\n        InetSocketAddress mirrorTarget = null;\n        // Connect to backup machine\n        mirrorNode = targets[0].getXferAddr(connectToDnViaHostname);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Connecting to datanode \" + mirrorNode);\n        }\n        mirrorTarget = NetUtils.createSocketAddr(mirrorNode);\n        mirrorSock = datanode.newSocket();\n        try {\n\n          DataNodeFaultInjector.get().failMirrorConnection();\n\n          int timeoutValue = dnConf.socketTimeout +\n              (HdfsConstants.READ_TIMEOUT_EXTENSION * targets.length);\n          int writeTimeout = dnConf.socketWriteTimeout +\n              (HdfsConstants.WRITE_TIMEOUT_EXTENSION * targets.length);\n          NetUtils.connect(mirrorSock, mirrorTarget, timeoutValue);\n          mirrorSock.setTcpNoDelay(dnConf.getDataTransferServerTcpNoDelay());\n          mirrorSock.setSoTimeout(timeoutValue);\n          mirrorSock.setKeepAlive(true);\n          if (dnConf.getTransferSocketSendBufferSize() > 0) {\n            mirrorSock.setSendBufferSize(\n                dnConf.getTransferSocketSendBufferSize());\n          }\n\n          OutputStream unbufMirrorOut = NetUtils.getOutputStream(mirrorSock,\n              writeTimeout);\n          InputStream unbufMirrorIn = NetUtils.getInputStream(mirrorSock);\n          DataEncryptionKeyFactory keyFactory =\n            datanode.getDataEncryptionKeyFactoryForBlock(block);\n          IOStreamPair saslStreams = datanode.saslClient.socketSend(mirrorSock,\n            unbufMirrorOut, unbufMirrorIn, keyFactory, blockToken, targets[0]);\n          unbufMirrorOut = saslStreams.out;\n          unbufMirrorIn = saslStreams.in;\n          mirrorOut = new DataOutputStream(new BufferedOutputStream(unbufMirrorOut,\n              smallBufferSize));\n          mirrorIn = new DataInputStream(unbufMirrorIn);\n\n          if (targetPinnings != null && targetPinnings.length > 0) {\n            new Sender(mirrorOut).writeBlock(originalBlock, targetStorageTypes[0],\n              blockToken, clientname, targets, targetStorageTypes, srcDataNode,\n              stage, pipelineSize, minBytesRcvd, maxBytesRcvd,\n              latestGenerationStamp, requestedChecksum, cachingStrategy,\n                allowLazyPersist, targetPinnings[0], targetPinnings);\n          } else {\n            new Sender(mirrorOut).writeBlock(originalBlock, targetStorageTypes[0],\n              blockToken, clientname, targets, targetStorageTypes, srcDataNode,\n              stage, pipelineSize, minBytesRcvd, maxBytesRcvd,\n              latestGenerationStamp, requestedChecksum, cachingStrategy,\n                allowLazyPersist, false, targetPinnings);\n          }\n\n          mirrorOut.flush();\n\n          DataNodeFaultInjector.get().writeBlockAfterFlush();\n\n          // read connect ack (only for clients, not for replication req)\n          if (isClient) {\n            BlockOpResponseProto connectAck =\n              BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(mirrorIn));\n            mirrorInStatus = connectAck.getStatus();\n            firstBadLink = connectAck.getFirstBadLink();\n            if (LOG.isDebugEnabled() || mirrorInStatus != SUCCESS) {\n              LOG.debug(\"Datanode \" + targets.length +\n                       \" got response for connect ack \" +\n                       \" from downstream datanode with firstbadlink as \" +\n                       firstBadLink);\n            }\n          }\n\n        } catch (IOException e) {\n          if (isClient) {\n            BlockOpResponseProto.newBuilder()\n              .setStatus(ERROR)\n               // NB: Unconditionally using the xfer addr w/o hostname\n              .setFirstBadLink(targets[0].getXferAddr())\n              .build()\n              .writeDelimitedTo(replyOut);\n            replyOut.flush();\n          }\n          IOUtils.closeStream(mirrorOut);\n          mirrorOut = null;\n          IOUtils.closeStream(mirrorIn);\n          mirrorIn = null;\n          IOUtils.closeSocket(mirrorSock);\n          mirrorSock = null;\n          if (isClient) {\n            LOG.error(datanode + \":Exception transfering block \" +\n                      block + \" to mirror \" + mirrorNode + \": \" + e);\n            throw e;\n          } else {\n            LOG.info(datanode + \":Exception transfering \" +\n                     block + \" to mirror \" + mirrorNode +\n                     \"- continuing without the mirror\", e);\n            incrDatanodeNetworkErrors();\n          }\n        }\n      }\n\n      // send connect-ack to source for clients and not transfer-RBW/Finalized\n      if (isClient && !isTransfer) {\n        if (LOG.isDebugEnabled() || mirrorInStatus != SUCCESS) {\n          LOG.debug(\"Datanode \" + targets.length +\n                   \" forwarding connect ack to upstream firstbadlink is \" +\n                   firstBadLink);\n        }\n        BlockOpResponseProto.newBuilder()\n          .setStatus(mirrorInStatus)\n          .setFirstBadLink(firstBadLink)\n          .build()\n          .writeDelimitedTo(replyOut);\n        replyOut.flush();\n      }\n\n      // receive the block and mirror to the next target\n      if (blockReceiver != null) {\n        String mirrorAddr = (mirrorSock == null) ? null : mirrorNode;\n        blockReceiver.receiveBlock(mirrorOut, mirrorIn, replyOut,\n            mirrorAddr, null, targets, false);\n\n        // send close-ack for transfer-RBW/Finalized \n        if (isTransfer) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"TRANSFER: send close-ack\");\n          }\n          writeResponse(SUCCESS, null, replyOut);\n        }\n      }\n\n      // update its generation stamp\n      if (isClient && \n          stage == BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        block.setGenerationStamp(latestGenerationStamp);\n        block.setNumBytes(minBytesRcvd);\n      }\n      \n      // if this write is for a replication request or recovering\n      // a failed close for client, then confirm block. For other client-writes,\n      // the block is finalized in the PacketResponder.\n      if (isDatanode ||\n          stage == BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        datanode.closeBlock(block, null, storageUuid, isOnTransientStorage);\n        LOG.info(\"Received \" + block + \" src: \" + remoteAddress + \" dest: \"\n            + localAddress + \" of size \" + block.getNumBytes());\n      }\n\n      if(isClient) {\n        size = block.getNumBytes();\n      }\n    } catch (IOException ioe) {\n      LOG.info(\"opWriteBlock \" + block + \" received exception \" + ioe);\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      // close all opened streams\n      IOUtils.closeStream(mirrorOut);\n      IOUtils.closeStream(mirrorIn);\n      IOUtils.closeStream(replyOut);\n      IOUtils.closeSocket(mirrorSock);\n      IOUtils.closeStream(blockReceiver);\n      setCurrentBlockReceiver(null);\n    }\n\n    //update metrics\n    datanode.getMetrics().addWriteBlockOp(elapsed());\n    datanode.getMetrics().incrWritesFromClient(peer.isLocal(), size);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBlockReceiver": "  BlockReceiver getBlockReceiver(\n      final ExtendedBlock block, final StorageType storageType,\n      final DataInputStream in,\n      final String inAddr, final String myAddr,\n      final BlockConstructionStage stage,\n      final long newGs, final long minBytesRcvd, final long maxBytesRcvd,\n      final String clientname, final DatanodeInfo srcDataNode,\n      final DataNode dn, DataChecksum requestedChecksum,\n      CachingStrategy cachingStrategy,\n      final boolean allowLazyPersist,\n      final boolean pinning) throws IOException {\n    return new BlockReceiver(block, storageType, in,\n        inAddr, myAddr, stage, newGs, minBytesRcvd, maxBytesRcvd,\n        clientname, srcDataNode, dn, requestedChecksum,\n        cachingStrategy, allowLazyPersist, pinning);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.incrDatanodeNetworkErrors": "  private void incrDatanodeNetworkErrors() {\n    datanode.incrDatanodeNetworkErrors(remoteAddressWithoutPort);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBufferedOutputStream": "  DataOutputStream getBufferedOutputStream() {\n    return new DataOutputStream(\n        new BufferedOutputStream(getOutputStream(), smallBufferSize));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.elapsed": "  private long elapsed() {\n    return monotonicNow() - opStartTime;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeResponse": "  private static void writeResponse(Status status, String message, OutputStream out)\n  throws IOException {\n    BlockOpResponseProto.Builder response = BlockOpResponseProto.newBuilder()\n      .setStatus(status);\n    if (message != null) {\n      response.setMessage(message);\n    }\n    response.build().writeDelimitedTo(out);\n    out.flush();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess": "  private void checkAccess(OutputStream out, final boolean reply, \n      final ExtendedBlock blk,\n      final Token<BlockTokenIdentifier> t,\n      final Op op,\n      final BlockTokenIdentifier.AccessMode mode) throws IOException {\n    checkAndWaitForBP(blk);\n    if (datanode.isBlockTokenEnabled) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Checking block access token for block '\" + blk.getBlockId()\n            + \"' with mode '\" + mode + \"'\");\n      }\n      try {\n        datanode.blockPoolTokenSecretManager.checkAccess(t, null, blk, mode);\n      } catch(InvalidToken e) {\n        try {\n          if (reply) {\n            BlockOpResponseProto.Builder resp = BlockOpResponseProto.newBuilder()\n              .setStatus(ERROR_ACCESS_TOKEN);\n            if (mode == BlockTokenIdentifier.AccessMode.WRITE) {\n              DatanodeRegistration dnR = \n                datanode.getDNRegistrationForBP(blk.getBlockPoolId());\n              // NB: Unconditionally using the xfer addr w/o hostname\n              resp.setFirstBadLink(dnR.getXferAddr());\n            }\n            resp.build().writeDelimitedTo(out);\n            out.flush();\n          }\n          LOG.warn(\"Block token verification failed: op=\" + op\n              + \", remoteAddress=\" + remoteAddress\n              + \", message=\" + e.getLocalizedMessage());\n          throw e;\n        } finally {\n          IOUtils.closeStream(out);\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.updateCurrentThreadName": "  private void updateCurrentThreadName(String status) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"DataXceiver for client \");\n    if (previousOpClientName != null) {\n      sb.append(previousOpClientName).append(\" at \");\n    }\n    sb.append(remoteAddress);\n    if (status != null) {\n      sb.append(\" [\").append(status).append(\"]\");\n    }\n    Thread.currentThread().setName(sb.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.getOutputStream": "  private OutputStream getOutputStream() {\n    return socketOut;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.setCurrentBlockReceiver": "  private synchronized void setCurrentBlockReceiver(BlockReceiver br) {\n    blockReceiver = br;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock": "  private void opWriteBlock(DataInputStream in) throws IOException {\n    final OpWriteBlockProto proto = OpWriteBlockProto.parseFrom(vintPrefixed(in));\n    final DatanodeInfo[] targets = PBHelperClient.convert(proto.getTargetsList());\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      writeBlock(PBHelperClient.convert(proto.getHeader().getBaseHeader().getBlock()),\n          PBHelperClient.convertStorageType(proto.getStorageType()),\n          PBHelperClient.convert(proto.getHeader().getBaseHeader().getToken()),\n          proto.getHeader().getClientName(),\n          targets,\n          PBHelperClient.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length),\n          PBHelperClient.convert(proto.getSource()),\n          fromProto(proto.getStage()),\n          proto.getPipelineSize(),\n          proto.getMinBytesRcvd(), proto.getMaxBytesRcvd(),\n          proto.getLatestGenerationStamp(),\n          fromProto(proto.getRequestedChecksum()),\n          (proto.hasCachingStrategy() ?\n              getCachingStrategy(proto.getCachingStrategy()) :\n            CachingStrategy.newDefaultStrategy()),\n          (proto.hasAllowLazyPersist() ? proto.getAllowLazyPersist() : false),\n          (proto.hasPinning() ? proto.getPinning(): false),\n          (PBHelperClient.convertBooleanList(proto.getTargetPinningsList())));\n    } finally {\n     if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.continueTraceSpan": "  private TraceScope continueTraceSpan(BaseHeaderProto header,\n                                             String description) {\n    return continueTraceSpan(header.getTraceInfo(), description);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.getCachingStrategy": "  static private CachingStrategy getCachingStrategy(CachingStrategyProto strategy) {\n    Boolean dropBehind = strategy.hasDropBehind() ?\n        strategy.getDropBehind() : null;\n    Long readahead = strategy.hasReadahead() ?\n        strategy.getReadahead() : null;\n    return new CachingStrategy(dropBehind, readahead);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp": "  protected final void processOp(Op op) throws IOException {\n    switch(op) {\n    case READ_BLOCK:\n      opReadBlock();\n      break;\n    case WRITE_BLOCK:\n      opWriteBlock(in);\n      break;\n    case REPLACE_BLOCK:\n      opReplaceBlock(in);\n      break;\n    case COPY_BLOCK:\n      opCopyBlock(in);\n      break;\n    case BLOCK_CHECKSUM:\n      opBlockChecksum(in);\n      break;\n    case BLOCK_GROUP_CHECKSUM:\n      opStripedBlockChecksum(in);\n      break;\n    case TRANSFER_BLOCK:\n      opTransferBlock(in);\n      break;\n    case REQUEST_SHORT_CIRCUIT_FDS:\n      opRequestShortCircuitFds(in);\n      break;\n    case RELEASE_SHORT_CIRCUIT_FDS:\n      opReleaseShortCircuitFds(in);\n      break;\n    case REQUEST_SHORT_CIRCUIT_SHM:\n      opRequestShortCircuitShm(in);\n      break;\n    default:\n      throw new IOException(\"Unknown op \" + op + \" in data stream\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReplaceBlock": "  private void opReplaceBlock(DataInputStream in) throws IOException {\n    OpReplaceBlockProto proto = OpReplaceBlockProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      replaceBlock(PBHelperClient.convert(proto.getHeader().getBlock()),\n          PBHelperClient.convertStorageType(proto.getStorageType()),\n          PBHelperClient.convert(proto.getHeader().getToken()),\n          proto.getDelHint(),\n          PBHelperClient.convert(proto.getSource()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitFds": "  private void opRequestShortCircuitFds(DataInputStream in) throws IOException {\n    final OpRequestShortCircuitAccessProto proto =\n      OpRequestShortCircuitAccessProto.parseFrom(vintPrefixed(in));\n    SlotId slotId = (proto.hasSlotId()) ? \n        PBHelperClient.convert(proto.getSlotId()) : null;\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      requestShortCircuitFds(PBHelperClient.convert(proto.getHeader().getBlock()),\n          PBHelperClient.convert(proto.getHeader().getToken()),\n          slotId, proto.getMaxVersion(),\n          proto.getSupportsReceiptVerification());\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitShm": "  private void opRequestShortCircuitShm(DataInputStream in) throws IOException {\n    final ShortCircuitShmRequestProto proto =\n        ShortCircuitShmRequestProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getTraceInfo(),\n        proto.getClass().getSimpleName());\n    try {\n      requestShortCircuitShm(proto.getClientName());\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opBlockChecksum": "  private void opBlockChecksum(DataInputStream in) throws IOException {\n    OpBlockChecksumProto proto = OpBlockChecksumProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n    blockChecksum(PBHelperClient.convert(proto.getHeader().getBlock()),\n        PBHelperClient.convert(proto.getHeader().getToken()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReleaseShortCircuitFds": "  private void opReleaseShortCircuitFds(DataInputStream in)\n      throws IOException {\n    final ReleaseShortCircuitAccessRequestProto proto =\n      ReleaseShortCircuitAccessRequestProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getTraceInfo(),\n        proto.getClass().getSimpleName());\n    try {\n      releaseShortCircuitFds(PBHelperClient.convert(proto.getSlotId()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum": "  private void opStripedBlockChecksum(DataInputStream dis) throws IOException {\n    OpBlockGroupChecksumProto proto =\n        OpBlockGroupChecksumProto.parseFrom(vintPrefixed(dis));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    StripedBlockInfo stripedBlockInfo = new StripedBlockInfo(\n        PBHelperClient.convert(proto.getHeader().getBlock()),\n        PBHelperClient.convert(proto.getDatanodes()),\n        PBHelperClient.convertTokens(proto.getBlockTokensList()),\n        PBHelperClient.convertBlockIndices(proto.getBlockIndicesList()),\n        PBHelperClient.convertErasureCodingPolicy(proto.getEcPolicy())\n    );\n\n    try {\n      blockGroupChecksum(stripedBlockInfo,\n          PBHelperClient.convert(proto.getHeader().getToken()),\n          proto.getRequestedNumBytes());\n    } finally {\n      if (traceScope != null) {\n        traceScope.close();\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opTransferBlock": "  private void opTransferBlock(DataInputStream in) throws IOException {\n    final OpTransferBlockProto proto =\n      OpTransferBlockProto.parseFrom(vintPrefixed(in));\n    final DatanodeInfo[] targets = PBHelperClient.convert(proto.getTargetsList());\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      transferBlock(PBHelperClient.convert(proto.getHeader().getBaseHeader().getBlock()),\n          PBHelperClient.convert(proto.getHeader().getBaseHeader().getToken()),\n          proto.getHeader().getClientName(),\n          targets,\n          PBHelperClient.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opCopyBlock": "  private void opCopyBlock(DataInputStream in) throws IOException {\n    OpCopyBlockProto proto = OpCopyBlockProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      copyBlock(PBHelperClient.convert(proto.getHeader().getBlock()),\n          PBHelperClient.convert(proto.getHeader().getToken()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock": "  private void opReadBlock() throws IOException {\n    OpReadBlockProto proto = OpReadBlockProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      readBlock(PBHelperClient.convert(proto.getHeader().getBaseHeader().getBlock()),\n        PBHelperClient.convert(proto.getHeader().getBaseHeader().getToken()),\n        proto.getHeader().getClientName(),\n        proto.getOffset(),\n        proto.getLen(),\n        proto.getSendChecksums(),\n        (proto.hasCachingStrategy() ?\n            getCachingStrategy(proto.getCachingStrategy()) :\n          CachingStrategy.newDefaultStrategy()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.run": "  public void run() {\n    int opsProcessed = 0;\n    Op op = null;\n\n    try {\n      synchronized(this) {\n        xceiver = Thread.currentThread();\n      }\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input = socketIn;\n      try {\n        IOStreamPair saslStreams = datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input = new BufferedInputStream(saslStreams.in,\n            smallBufferSize);\n        socketOut = saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at \" + peer.getRemoteAddressString() + \n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\");\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed != 0) {\n            assert dnConf.socketKeepaliveTimeout > 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op = readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (EOFException | ClosedChannelException e) {\n          // Since we optimistically expect the next op, it's quite normal to\n          // get EOF here.\n          LOG.debug(\"Cached {} closing after {} ops.  \" +\n              \"This message is usually benign.\", peer, opsProcessed);\n          break;\n        } catch (IOException err) {\n          incrDatanodeNetworkErrors();\n          throw err;\n        }\n\n        // restore normal timeout\n        if (opsProcessed != 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime = monotonicNow();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer != null) &&\n          (!peer.isClosed() && dnConf.socketKeepaliveTimeout > 0));\n    } catch (Throwable t) {\n      String s = datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op == null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op == Op.WRITE_BLOCK && t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op == Op.READ_BLOCK && t instanceof SocketTimeoutException) {\n        String s1 =\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 += \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else if (t instanceof InvalidToken) {\n        // The InvalidToken exception has already been logged in\n        // checkAccess() method and this is not a server error.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      collectThreadLocalStates();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer != null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.collectThreadLocalStates": "  private void collectThreadLocalStates() {\n    if (datanode.getPeerMetrics() != null) {\n      datanode.getPeerMetrics().collectThreadLocalStates();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader.getHeaderSize": "  public static int getHeaderSize() {\n    return Short.SIZE/Byte.SIZE + DataChecksum.getChecksumHeaderSize();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNodeFaultInjector.get": "  public static DataNodeFaultInjector get() {\n    return instance;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDnConf": "  public DNConf getDnConf() {\n    return dnConf;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.isRestarting": "  boolean isRestarting() {\n    return shutdownForUpgrade;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.newSocket": "  public Socket newSocket() throws IOException {\n    return socketFactory.createSocket();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dnConf.getAllowNonLocalLazyPersist": "  public boolean getAllowNonLocalLazyPersist() {\n    return allowNonLocalLazyPersist;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockReceiver.receiveBlock": "  void receiveBlock(\n      DataOutputStream mirrOut, // output to next datanode\n      DataInputStream mirrIn,   // input from next datanode\n      DataOutputStream replyOut,  // output to previous datanode\n      String mirrAddr, DataTransferThrottler throttlerArg,\n      DatanodeInfo[] downstreams,\n      boolean isReplaceBlock) throws IOException {\n\n    syncOnClose = datanode.getDnConf().syncOnClose;\n    boolean responderClosed = false;\n    mirrorOut = mirrOut;\n    mirrorAddr = mirrAddr;\n    isPenultimateNode = ((downstreams != null) && (downstreams.length == 1));\n    if (isPenultimateNode) {\n      mirrorNameForMetrics = (downstreams[0].getInfoSecurePort() != 0 ?\n          downstreams[0].getInfoSecureAddr() : downstreams[0].getInfoAddr());\n      LOG.debug(\"Will collect peer metrics for downstream node {}\",\n          mirrorNameForMetrics);\n    }\n    throttler = throttlerArg;\n\n    this.replyOut = replyOut;\n    this.isReplaceBlock = isReplaceBlock;\n\n    try {\n      if (isClient && !isTransfer) {\n        responder = new Daemon(datanode.threadGroup, \n            new PacketResponder(replyOut, mirrIn, downstreams));\n        responder.start(); // start thread to processes responses\n      }\n\n      while (receivePacket() >= 0) { /* Receive until the last packet */ }\n\n      // wait for all outstanding packet responses. And then\n      // indicate responder to gracefully shutdown.\n      // Mark that responder has been closed for future processing\n      if (responder != null) {\n        ((PacketResponder)responder.getRunnable()).close();\n        responderClosed = true;\n      }\n\n      // If this write is for a replication or transfer-RBW/Finalized,\n      // then finalize block or convert temporary to RBW.\n      // For client-writes, the block is finalized in the PacketResponder.\n      if (isDatanode || isTransfer) {\n        // Hold a volume reference to finalize block.\n        try (ReplicaHandler handler = claimReplicaHandler()) {\n          // close the block/crc files\n          close();\n          block.setNumBytes(replicaInfo.getNumBytes());\n\n          if (stage == BlockConstructionStage.TRANSFER_RBW) {\n            // for TRANSFER_RBW, convert temporary to RBW\n            datanode.data.convertTemporaryToRbw(block);\n          } else {\n            // for isDatnode or TRANSFER_FINALIZED\n            // Finalize the block.\n            datanode.data.finalizeBlock(block);\n          }\n        }\n        datanode.metrics.incrBlocksWritten();\n      }\n\n    } catch (IOException ioe) {\n      replicaInfo.releaseAllBytesReserved();\n      if (datanode.isRestarting()) {\n        // Do not throw if shutting down for restart. Otherwise, it will cause\n        // premature termination of responder.\n        LOG.info(\"Shutting down for restart (\" + block + \").\");\n      } else {\n        LOG.info(\"Exception for \" + block, ioe);\n        throw ioe;\n      }\n    } finally {\n      // Clear the previous interrupt state of this thread.\n      Thread.interrupted();\n\n      // If a shutdown for restart was initiated, upstream needs to be notified.\n      // There is no need to do anything special if the responder was closed\n      // normally.\n      if (!responderClosed) { // Data transfer was not complete.\n        if (responder != null) {\n          // In case this datanode is shutting down for quick restart,\n          // send a special ack upstream.\n          if (datanode.isRestarting() && isClient && !isTransfer) {\n            try (Writer out = new OutputStreamWriter(\n                replicaInfo.createRestartMetaStream(), \"UTF-8\")) {\n              // write out the current time.\n              out.write(Long.toString(Time.now() + restartBudget));\n              out.flush();\n            } catch (IOException ioe) {\n              // The worst case is not recovering this RBW replica. \n              // Client will fall back to regular pipeline recovery.\n            } finally {\n              IOUtils.closeStream(streams.getDataOut());\n            }\n            try {              \n              // Even if the connection is closed after the ack packet is\n              // flushed, the client can react to the connection closure \n              // first. Insert a delay to lower the chance of client \n              // missing the OOB ack.\n              Thread.sleep(1000);\n            } catch (InterruptedException ie) {\n              // It is already going down. Ignore this.\n            }\n          }\n          responder.interrupt();\n        }\n        IOUtils.closeStream(this);\n        cleanupBlock();\n      }\n      if (responder != null) {\n        try {\n          responder.interrupt();\n          // join() on the responder should timeout a bit earlier than the\n          // configured deadline. Otherwise, the join() on this thread will\n          // likely timeout as well.\n          long joinTimeout = datanode.getDnConf().getXceiverStopTimeout();\n          joinTimeout = joinTimeout > 1  ? joinTimeout*8/10 : joinTimeout;\n          responder.join(joinTimeout);\n          if (responder.isAlive()) {\n            String msg = \"Join on responder thread \" + responder\n                + \" timed out\";\n            LOG.warn(msg + \"\\n\" + StringUtils.getStackTrace(responder));\n            throw new IOException(msg);\n          }\n        } catch (InterruptedException e) {\n          responder.interrupt();\n          // do not throw if shutting down for restart.\n          if (!datanode.isRestarting()) {\n            throw new IOException(\"Interrupted receiveBlock\");\n          }\n        }\n        responder = null;\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockReceiver.claimReplicaHandler": "  private ReplicaHandler claimReplicaHandler() {\n    ReplicaHandler handler = replicaHandler;\n    replicaHandler = null;\n    return handler;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockReceiver.cleanupBlock": "  private void cleanupBlock() throws IOException {\n    if (isDatanode) {\n      datanode.data.unfinalizeBlock(block);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockReceiver.toString": "    public String toString() {\n      return getClass().getSimpleName() + \"(seqno=\" + seqno\n        + \", lastPacketInBlock=\" + lastPacketInBlock\n        + \", offsetInBlock=\" + offsetInBlock\n        + \", ackEnqueueNanoTime=\" + ackEnqueueNanoTime\n        + \", ackStatus=\" + ackStatus\n        + \")\";\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockReceiver.close": "    public void close() {\n      synchronized(ackQueue) {\n        while (isRunning() && ackQueue.size() != 0) {\n          try {\n            ackQueue.wait();\n          } catch (InterruptedException e) {\n            running = false;\n            Thread.currentThread().interrupt();\n          }\n        }\n        if(LOG.isDebugEnabled()) {\n          LOG.debug(myString + \": closing\");\n        }\n        running = false;\n        ackQueue.notifyAll();\n      }\n\n      synchronized(this) {\n        running = false;\n        notifyAll();\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockReceiver.receivePacket": "  private int receivePacket() throws IOException {\n    // read the next packet\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader header = packetReceiver.getHeader();\n    if (LOG.isDebugEnabled()){\n      LOG.debug(\"Receiving one packet for block \" + block +\n                \": \" + header);\n    }\n\n    // Sanity check the header\n    if (header.getOffsetInBlock() > replicaInfo.getNumBytes()) {\n      throw new IOException(\"Received an out-of-sequence packet for \" + block + \n          \"from \" + inAddr + \" at offset \" + header.getOffsetInBlock() +\n          \". Expecting packet starting at \" + replicaInfo.getNumBytes());\n    }\n    if (header.getDataLen() < 0) {\n      throw new IOException(\"Got wrong length during writeBlock(\" + block + \n                            \") from \" + inAddr + \" at offset \" + \n                            header.getOffsetInBlock() + \": \" +\n                            header.getDataLen()); \n    }\n\n    long offsetInBlock = header.getOffsetInBlock();\n    long seqno = header.getSeqno();\n    boolean lastPacketInBlock = header.isLastPacketInBlock();\n    final int len = header.getDataLen();\n    boolean syncBlock = header.getSyncBlock();\n\n    // avoid double sync'ing on close\n    if (syncBlock && lastPacketInBlock) {\n      this.syncOnClose = false;\n    }\n\n    // update received bytes\n    final long firstByteInBlock = offsetInBlock;\n    offsetInBlock += len;\n    if (replicaInfo.getNumBytes() < offsetInBlock) {\n      replicaInfo.setNumBytes(offsetInBlock);\n    }\n    \n    // put in queue for pending acks, unless sync was requested\n    if (responder != null && !syncBlock && !shouldVerifyChecksum()) {\n      ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n          lastPacketInBlock, offsetInBlock, Status.SUCCESS);\n    }\n\n    // Drop heartbeat for testing.\n    if (seqno < 0 && len == 0 &&\n        DataNodeFaultInjector.get().dropHeartbeatPacket()) {\n      return 0;\n    }\n\n    //First write the packet to the mirror:\n    if (mirrorOut != null && !mirrorError) {\n      try {\n        long begin = Time.monotonicNow();\n        // For testing. Normally no-op.\n        DataNodeFaultInjector.get().stopSendingPacketDownstream(mirrorAddr);\n        packetReceiver.mirrorPacketTo(mirrorOut);\n        mirrorOut.flush();\n        long now = Time.monotonicNow();\n        setLastSentTime(now);\n        long duration = now - begin;\n        DataNodeFaultInjector.get().logDelaySendingPacketDownstream(\n            mirrorAddr,\n            duration);\n        trackSendPacketToLastNodeInPipeline(duration);\n        if (duration > datanodeSlowLogThresholdMs) {\n          LOG.warn(\"Slow BlockReceiver write packet to mirror took \" + duration\n              + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms)\");\n        }\n      } catch (IOException e) {\n        handleMirrorOutError(e);\n      }\n    }\n    \n    ByteBuffer dataBuf = packetReceiver.getDataSlice();\n    ByteBuffer checksumBuf = packetReceiver.getChecksumSlice();\n    \n    if (lastPacketInBlock || len == 0) {\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"Receiving an empty packet or the end of the block \" + block);\n      }\n      // sync block if requested\n      if (syncBlock) {\n        flushOrSync(true);\n      }\n    } else {\n      final int checksumLen = diskChecksum.getChecksumSize(len);\n      final int checksumReceivedLen = checksumBuf.capacity();\n\n      if (checksumReceivedLen > 0 && checksumReceivedLen != checksumLen) {\n        throw new IOException(\"Invalid checksum length: received length is \"\n            + checksumReceivedLen + \" but expected length is \" + checksumLen);\n      }\n\n      if (checksumReceivedLen > 0 && shouldVerifyChecksum()) {\n        try {\n          verifyChunks(dataBuf, checksumBuf);\n        } catch (IOException ioe) {\n          // checksum error detected locally. there is no reason to continue.\n          if (responder != null) {\n            try {\n              ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n                  lastPacketInBlock, offsetInBlock,\n                  Status.ERROR_CHECKSUM);\n              // Wait until the responder sends back the response\n              // and interrupt this thread.\n              Thread.sleep(3000);\n            } catch (InterruptedException e) { }\n          }\n          throw new IOException(\"Terminating due to a checksum error.\" + ioe);\n        }\n \n        if (needsChecksumTranslation) {\n          // overwrite the checksums in the packet buffer with the\n          // appropriate polynomial for the disk storage.\n          translateChunks(dataBuf, checksumBuf);\n        }\n      }\n\n      if (checksumReceivedLen == 0 && !streams.isTransientStorage()) {\n        // checksum is missing, need to calculate it\n        checksumBuf = ByteBuffer.allocate(checksumLen);\n        diskChecksum.calculateChunkedSums(dataBuf, checksumBuf);\n      }\n      \n      // by this point, the data in the buffer uses the disk checksum\n\n      final boolean shouldNotWriteChecksum = checksumReceivedLen == 0\n          && streams.isTransientStorage();\n      try {\n        long onDiskLen = replicaInfo.getBytesOnDisk();\n        if (onDiskLen<offsetInBlock) {\n          // Normally the beginning of an incoming packet is aligned with the\n          // existing data on disk. If the beginning packet data offset is not\n          // checksum chunk aligned, the end of packet will not go beyond the\n          // next chunk boundary.\n          // When a failure-recovery is involved, the client state and the\n          // the datanode state may not exactly agree. I.e. the client may\n          // resend part of data that is already on disk. Correct number of\n          // bytes should be skipped when writing the data and checksum\n          // buffers out to disk.\n          long partialChunkSizeOnDisk = onDiskLen % bytesPerChecksum;\n          long lastChunkBoundary = onDiskLen - partialChunkSizeOnDisk;\n          boolean alignedOnDisk = partialChunkSizeOnDisk == 0;\n          boolean alignedInPacket = firstByteInBlock % bytesPerChecksum == 0;\n\n          // If the end of the on-disk data is not chunk-aligned, the last\n          // checksum needs to be overwritten.\n          boolean overwriteLastCrc = !alignedOnDisk && !shouldNotWriteChecksum;\n          // If the starting offset of the packat data is at the last chunk\n          // boundary of the data on disk, the partial checksum recalculation\n          // can be skipped and the checksum supplied by the client can be used\n          // instead. This reduces disk reads and cpu load.\n          boolean doCrcRecalc = overwriteLastCrc &&\n              (lastChunkBoundary != firstByteInBlock);\n\n          // If this is a partial chunk, then verify that this is the only\n          // chunk in the packet. If the starting offset is not chunk\n          // aligned, the packet should terminate at or before the next\n          // chunk boundary.\n          if (!alignedInPacket && len > bytesPerChecksum) {\n            throw new IOException(\"Unexpected packet data length for \"\n                +  block + \" from \" + inAddr + \": a partial chunk must be \"\n                + \" sent in an individual packet (data length = \" + len\n                +  \" > bytesPerChecksum = \" + bytesPerChecksum + \")\");\n          }\n\n          // If the last portion of the block file is not a full chunk,\n          // then read in pre-existing partial data chunk and recalculate\n          // the checksum so that the checksum calculation can continue\n          // from the right state. If the client provided the checksum for\n          // the whole chunk, this is not necessary.\n          Checksum partialCrc = null;\n          if (doCrcRecalc) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"receivePacket for \" + block \n                  + \": previous write did not end at the chunk boundary.\"\n                  + \" onDiskLen=\" + onDiskLen);\n            }\n            long offsetInChecksum = BlockMetadataHeader.getHeaderSize() +\n                onDiskLen / bytesPerChecksum * checksumSize;\n            partialCrc = computePartialChunkCrc(onDiskLen, offsetInChecksum);\n          }\n\n          // The data buffer position where write will begin. If the packet\n          // data and on-disk data have no overlap, this will not be at the\n          // beginning of the buffer.\n          int startByteToDisk = (int)(onDiskLen-firstByteInBlock) \n              + dataBuf.arrayOffset() + dataBuf.position();\n\n          // Actual number of data bytes to write.\n          int numBytesToDisk = (int)(offsetInBlock-onDiskLen);\n          \n          // Write data to disk.\n          long begin = Time.monotonicNow();\n          streams.writeDataToDisk(dataBuf.array(),\n              startByteToDisk, numBytesToDisk);\n          long duration = Time.monotonicNow() - begin;\n          if (duration > datanodeSlowLogThresholdMs) {\n            LOG.warn(\"Slow BlockReceiver write data to disk cost:\" + duration\n                + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms)\");\n          }\n\n          if (duration > maxWriteToDiskMs) {\n            maxWriteToDiskMs = duration;\n          }\n\n          final byte[] lastCrc;\n          if (shouldNotWriteChecksum) {\n            lastCrc = null;\n          } else {\n            int skip = 0;\n            byte[] crcBytes = null;\n\n            // First, prepare to overwrite the partial crc at the end.\n            if (overwriteLastCrc) { // not chunk-aligned on disk\n              // prepare to overwrite last checksum\n              adjustCrcFilePosition();\n            }\n\n            // The CRC was recalculated for the last partial chunk. Update the\n            // CRC by reading the rest of the chunk, then write it out.\n            if (doCrcRecalc) {\n              // Calculate new crc for this chunk.\n              int bytesToReadForRecalc =\n                  (int)(bytesPerChecksum - partialChunkSizeOnDisk);\n              if (numBytesToDisk < bytesToReadForRecalc) {\n                bytesToReadForRecalc = numBytesToDisk;\n              }\n\n              partialCrc.update(dataBuf.array(), startByteToDisk,\n                  bytesToReadForRecalc);\n              byte[] buf = FSOutputSummer.convertToByteStream(partialCrc,\n                  checksumSize);\n              crcBytes = copyLastChunkChecksum(buf, checksumSize, buf.length);\n              checksumOut.write(buf);\n              if(LOG.isDebugEnabled()) {\n                LOG.debug(\"Writing out partial crc for data len \" + len +\n                    \", skip=\" + skip);\n              }\n              skip++; //  For the partial chunk that was just read.\n            }\n\n            // Determine how many checksums need to be skipped up to the last\n            // boundary. The checksum after the boundary was already counted\n            // above. Only count the number of checksums skipped up to the\n            // boundary here.\n            long skippedDataBytes = lastChunkBoundary - firstByteInBlock;\n\n            if (skippedDataBytes > 0) {\n              skip += (int)(skippedDataBytes / bytesPerChecksum) +\n                  ((skippedDataBytes % bytesPerChecksum == 0) ? 0 : 1);\n            }\n            skip *= checksumSize; // Convert to number of bytes\n\n            // write the rest of checksum\n            final int offset = checksumBuf.arrayOffset() +\n                checksumBuf.position() + skip;\n            final int end = offset + checksumLen - skip;\n            // If offset >= end, there is no more checksum to write.\n            // I.e. a partial chunk checksum rewrite happened and there is no\n            // more to write after that.\n            if (offset >= end && doCrcRecalc) {\n              lastCrc = crcBytes;\n            } else {\n              final int remainingBytes = checksumLen - skip;\n              lastCrc = copyLastChunkChecksum(checksumBuf.array(),\n                  checksumSize, end);\n              checksumOut.write(checksumBuf.array(), offset, remainingBytes);\n            }\n          }\n\n          /// flush entire packet, sync if requested\n          flushOrSync(syncBlock);\n          \n          replicaInfo.setLastChecksumAndDataLen(offsetInBlock, lastCrc);\n\n          datanode.metrics.incrBytesWritten(len);\n          datanode.metrics.incrTotalWriteTime(duration);\n\n          manageWriterOsCache(offsetInBlock);\n        }\n      } catch (IOException iex) {\n        // Volume error check moved to FileIoProvider\n        throw iex;\n      }\n    }\n\n    // if sync was requested, put in queue for pending acks here\n    // (after the fsync finished)\n    if (responder != null && (syncBlock || shouldVerifyChecksum())) {\n      ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n          lastPacketInBlock, offsetInBlock, Status.SUCCESS);\n    }\n\n    /*\n     * Send in-progress responses for the replaceBlock() calls back to caller to\n     * avoid timeouts due to balancer throttling. HDFS-6247\n     */\n    if (isReplaceBlock\n        && (Time.monotonicNow() - lastResponseTime > responseInterval)) {\n      BlockOpResponseProto.Builder response = BlockOpResponseProto.newBuilder()\n          .setStatus(Status.IN_PROGRESS);\n      response.build().writeDelimitedTo(replyOut);\n      replyOut.flush();\n\n      lastResponseTime = Time.monotonicNow();\n    }\n\n    if (throttler != null) { // throttle I/O\n      throttler.throttle(len);\n    }\n    \n    return lastPacketInBlock?-1:len;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getMetrics": "  public DataNodeMetrics getMetrics() {\n    return metrics;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockReceiver.getReplica": "  Replica getReplica() {\n    return replicaInfo;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.Replica.isOnTransientStorage": "  public boolean isOnTransientStorage();\n}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dnConf.getTransferSocketSendBufferSize": "  public int getTransferSocketSendBufferSize() {\n    return transferSocketSendBufferSize;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.closeBlock": "  void closeBlock(ExtendedBlock block, String delHint, String storageUuid,\n      boolean isTransientStorage) {\n    metrics.incrBlocksWritten();\n    notifyNamenodeReceivedBlock(block, delHint, storageUuid,\n        isTransientStorage);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.notifyNamenodeReceivedBlock": "  public void notifyNamenodeReceivedBlock(ExtendedBlock block, String delHint,\n      String storageUuid, boolean isOnTransientStorage) {\n    BPOfferService bpos = blockPoolManager.get(block.getBlockPoolId());\n    if(bpos != null) {\n      bpos.notifyNamenodeReceivedBlock(block, delHint, storageUuid,\n          isOnTransientStorage);\n    } else {\n      LOG.error(\"Cannot find BPOfferService for reporting block received for bpid=\"\n          + block.getBlockPoolId());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dnConf.getDataTransferServerTcpNoDelay": "  public boolean getDataTransferServerTcpNoDelay() {\n    return tcpNoDelay;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.Replica.getStorageUuid": "  public String getStorageUuid();\n\n  /**\n   * Return true if the target volume is backed by RAM.\n   */\n  public boolean isOnTransientStorage();\n}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDataEncryptionKeyFactoryForBlock": "  public DataEncryptionKeyFactory getDataEncryptionKeyFactoryForBlock(\n      final ExtendedBlock block) {\n    return new DataEncryptionKeyFactory() {\n      @Override\n      public DataEncryptionKey newDataEncryptionKey() {\n        return dnConf.encryptDataTransfer ?\n          blockPoolTokenSecretManager.generateDataEncryptionKey(\n            block.getBlockPoolId()) : null;\n      }\n    };\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXceiverCount": "  public int getXceiverCount() {\n    return threadGroup == null ? 0 : threadGroup.activeCount();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDisplayName": "  public String getDisplayName() {\n    // NB: our DatanodeID may not be set yet\n    return hostName + \":\" + getXferPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXferPort": "  public int getXferPort() {\n    return streamingAddr.getPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dataXceiverServer.closePeer": "  synchronized void closePeer(Peer peer) {\n    peers.remove(peer);\n    peersXceiver.remove(peer);\n    datanode.metrics.decrDataNodeActiveXceiversCount();\n    IOUtils.cleanup(null, peer);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dataXceiverServer.addPeer": "  synchronized void addPeer(Peer peer, Thread t, DataXceiver xceiver)\n      throws IOException {\n    if (closed) {\n      throw new IOException(\"Server closed.\");\n    }\n    peers.put(peer, t);\n    peersXceiver.put(peer, xceiver);\n    datanode.metrics.incrDataNodeActiveXceiversCount();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDatanodeId": "  public DatanodeID getDatanodeId() {\n    return id;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXferAddress": "  public InetSocketAddress getXferAddress() {\n    return streamingAddr;\n  }"
        },
        "bug_report": {
            "Title": "HDFS write crashed with block size greater than 2 GB",
            "Description": "We've seen HDFS write crashes in the case of huge block size. For example, writing a 3 GB file using block size > 2 GB (e.g., 3 GB), HDFS client throws out of memory exception. DataNode gives out IOException. After changing heap size limit,  DFSOutputStream ResponseProcessor exception is seen followed by Broken pipe and pipeline recovery.\n\nGive below:\nDN exception,\n{noformat}\n2017-03-30 16:34:33,828 ERROR datanode.DataNode (DataXceiver.java:run(278)) - c6401.ambari.apache.org:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.64.101:47167 dst: /192.168.64.101:50010\njava.io.IOException: Incorrect value for packet payload size: 2147483128\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:159)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:502)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:898)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:806)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}"
        }
    },
    {
        "filename": "HDFS-12638.json",
        "creation_time": "2017-10-11T11:29:04.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:55)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1532)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1491)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3792)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3744)\n        at java.lang.Thread.run(Thread.java:834)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets": "  void chooseTargets(BlockPlacementPolicy blockplacement,\n      BlockStoragePolicySuite storagePolicySuite,\n      Set<Node> excludedNodes) {\n    assert getSrcNodes().length > 0\n        : \"At least 1 source node should have been selected\";\n    try {\n      DatanodeStorageInfo[] chosenTargets = blockplacement.chooseTarget(\n          getBc().getName(), getAdditionalReplRequired(), getSrcNodes()[0],\n          getLiveReplicaStorages(), false, excludedNodes,\n          getBlock().getNumBytes(),\n          storagePolicySuite.getPolicy(getBc().getStoragePolicyID()),\n          null);\n      setTargets(chosenTargets);\n    } finally {\n      getSrcNodes()[0].decrementPendingReplicationWithoutTargets();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork": "  int computeDatanodeWork() {\n    // Blocks should not be replicated or removed if in safe mode.\n    // It's OK to check safe mode here w/o holding lock, in the worst\n    // case extra replications will be scheduled, and these will get\n    // fixed up later.\n    if (namesystem.isInSafeMode()) {\n      return 0;\n    }\n\n    final int numlive = heartbeatManager.getLiveDatanodeCount();\n    final int blocksToProcess = numlive\n        * this.blocksReplWorkMultiplier;\n    final int nodesToProcess = (int) Math.ceil(numlive\n        * this.blocksInvalidateWorkPct);\n\n    int workFound = this.computeBlockReconstructionWork(blocksToProcess);\n\n    // Update counters\n    namesystem.writeLock();\n    try {\n      this.updateState();\n      this.scheduledReplicationBlocksCount = workFound;\n    } finally {\n      namesystem.writeUnlock();\n    }\n    workFound += this.computeInvalidateWork(nodesToProcess);\n    return workFound;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeBlockReconstructionWork": "  int computeBlockReconstructionWork(int blocksToProcess) {\n    List<List<BlockInfo>> blocksToReconstruct = null;\n    namesystem.writeLock();\n    try {\n      // Choose the blocks to be reconstructed\n      blocksToReconstruct = neededReconstruction\n          .chooseLowRedundancyBlocks(blocksToProcess);\n    } finally {\n      namesystem.writeUnlock();\n    }\n    return computeReconstructionWorkForBlocks(blocksToReconstruct);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.isInSafeMode": "  public boolean isInSafeMode() {\n    return bmSafeMode.isInSafeMode();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.updateState": "  void updateState() {\n    pendingReconstructionBlocksCount = pendingReconstruction.size();\n    lowRedundancyBlocksCount = neededReconstruction.size();\n    corruptReplicaBlocksCount = corruptReplicas.size();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeInvalidateWork": "  int computeInvalidateWork(int nodesToProcess) {\n    final List<DatanodeInfo> nodes = invalidateBlocks.getDatanodes();\n    Collections.shuffle(nodes);\n\n    nodesToProcess = Math.min(nodes.size(), nodesToProcess);\n\n    int blockCnt = 0;\n    for (DatanodeInfo dnInfo : nodes) {\n      int blocks = invalidateWorkForOneNode(dnInfo);\n      if (blocks > 0) {\n        blockCnt += blocks;\n        if (--nodesToProcess == 0) {\n          break;\n        }\n      }\n    }\n    return blockCnt;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.run": "    public void run() {\n      try {\n        processQueue();\n      } catch (Throwable t) {\n        ExitUtil.terminate(1,\n            getName() + \" encountered fatal exception: \" + t);\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processQueue": "    private void processQueue() {\n      while (namesystem.isRunning()) {\n        NameNodeMetrics metrics = NameNode.getNameNodeMetrics();\n        try {\n          Runnable action = queue.take();\n          // batch as many operations in the write lock until the queue\n          // runs dry, or the max lock hold is reached.\n          int processed = 0;\n          namesystem.writeLock();\n          metrics.setBlockOpsQueued(queue.size() + 1);\n          try {\n            long start = Time.monotonicNow();\n            do {\n              processed++;\n              action.run();\n              if (Time.monotonicNow() - start > MAX_LOCK_HOLD_MS) {\n                break;\n              }\n              action = queue.poll();\n            } while (action != null);\n          } finally {\n            namesystem.writeUnlock();\n            metrics.addBlockOpsBatched(processed - 1);\n          }\n        } catch (InterruptedException e) {\n          // ignore unless thread was specifically interrupted.\n          if (Thread.interrupted()) {\n            break;\n          }\n        }\n      }\n      queue.clear();\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.isPopulatingReplQueues": "  public boolean isPopulatingReplQueues() {\n    if (!shouldPopulateReplQueues()) {\n      return false;\n    }\n    return initializedReplQueues;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.rescanPostponedMisreplicatedBlocks": "  void rescanPostponedMisreplicatedBlocks() {\n    if (getPostponedMisreplicatedBlocksCount() == 0) {\n      return;\n    }\n    namesystem.writeLock();\n    long startTime = Time.monotonicNow();\n    long startSize = postponedMisreplicatedBlocks.size();\n    try {\n      Iterator<Block> it = postponedMisreplicatedBlocks.iterator();\n      for (int i=0; i < blocksPerPostpondedRescan && it.hasNext(); i++) {\n        Block b = it.next();\n        it.remove();\n\n        BlockInfo bi = getStoredBlock(b);\n        if (bi == null) {\n          LOG.debug(\"BLOCK* rescanPostponedMisreplicatedBlocks: \" +\n              \"Postponed mis-replicated block {} no longer found \" +\n              \"in block map.\", b);\n          continue;\n        }\n        MisReplicationResult res = processMisReplicatedBlock(bi);\n        LOG.debug(\"BLOCK* rescanPostponedMisreplicatedBlocks: \" +\n            \"Re-scanned block {}, result is {}\", b, res);\n        if (res == MisReplicationResult.POSTPONE) {\n          rescannedMisreplicatedBlocks.add(b);\n        }\n      }\n    } finally {\n      postponedMisreplicatedBlocks.addAll(rescannedMisreplicatedBlocks);\n      rescannedMisreplicatedBlocks.clear();\n      long endSize = postponedMisreplicatedBlocks.size();\n      namesystem.writeUnlock();\n      LOG.info(\"Rescan of postponedMisreplicatedBlocks completed in {}\" +\n          \" msecs. {} blocks are left. {} blocks were removed.\",\n          (Time.monotonicNow() - startTime), endSize, (startSize - endSize));\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processPendingReconstructions": "  private void processPendingReconstructions() {\n    BlockInfo[] timedOutItems = pendingReconstruction.getTimedOutBlocks();\n    if (timedOutItems != null) {\n      namesystem.writeLock();\n      try {\n        for (int i = 0; i < timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we're working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi = blocksMap.getStoredBlock(timedOutItems[i]);\n          if (bi == null) {\n            continue;\n          }\n          NumberReplicas num = countNodes(timedOutItems[i]);\n          if (isNeededReconstruction(bi, num)) {\n            neededReconstruction.add(bi, num.liveReplicas(),\n                num.readOnlyReplicas(), num.outOfServiceReplicas(),\n                getExpectedRedundancyNum(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync": "  private void processMisReplicatesAsync() throws InterruptedException {\n    long nrInvalid = 0, nrOverReplicated = 0;\n    long nrUnderReplicated = 0, nrPostponed = 0, nrUnderConstruction = 0;\n    long startTimeMisReplicatedScan = Time.monotonicNow();\n    Iterator<BlockInfo> blocksItr = blocksMap.getBlocks().iterator();\n    long totalBlocks = blocksMap.size();\n    reconstructionQueuesInitProgress = 0;\n    long totalProcessed = 0;\n    long sleepDuration =\n        Math.max(1, Math.min(numBlocksPerIteration/1000, 10000));\n\n    while (namesystem.isRunning() && !Thread.currentThread().isInterrupted()) {\n      int processed = 0;\n      namesystem.writeLockInterruptibly();\n      try {\n        while (processed < numBlocksPerIteration && blocksItr.hasNext()) {\n          BlockInfo block = blocksItr.next();\n          MisReplicationResult res = processMisReplicatedBlock(block);\n          switch (res) {\n          case UNDER_REPLICATED:\n            LOG.trace(\"under replicated block {}: {}\", block, res);\n            nrUnderReplicated++;\n            break;\n          case OVER_REPLICATED:\n            LOG.trace(\"over replicated block {}: {}\", block, res);\n            nrOverReplicated++;\n            break;\n          case INVALID:\n            LOG.trace(\"invalid block {}: {}\", block, res);\n            nrInvalid++;\n            break;\n          case POSTPONE:\n            LOG.trace(\"postpone block {}: {}\", block, res);\n            nrPostponed++;\n            postponeBlock(block);\n            break;\n          case UNDER_CONSTRUCTION:\n            LOG.trace(\"under construction block {}: {}\", block, res);\n            nrUnderConstruction++;\n            break;\n          case OK:\n            break;\n          default:\n            throw new AssertionError(\"Invalid enum value: \" + res);\n          }\n          processed++;\n        }\n        totalProcessed += processed;\n        // there is a possibility that if any of the blocks deleted/added during\n        // initialisation, then progress might be different.\n        reconstructionQueuesInitProgress = Math.min((double) totalProcessed\n            / totalBlocks, 1.0);\n\n        if (!blocksItr.hasNext()) {\n          LOG.info(\"Total number of blocks            = {}\", blocksMap.size());\n          LOG.info(\"Number of invalid blocks          = {}\", nrInvalid);\n          LOG.info(\"Number of under-replicated blocks = {}\", nrUnderReplicated);\n          LOG.info(\"Number of  over-replicated blocks = {}{}\", nrOverReplicated,\n              ((nrPostponed > 0) ? (\" (\" + nrPostponed + \" postponed)\") : \"\"));\n          LOG.info(\"Number of blocks being written    = {}\",\n                   nrUnderConstruction);\n          NameNode.stateChangeLog\n              .info(\"STATE* Replication Queue initialization \"\n                  + \"scan for invalid, over- and under-replicated blocks \"\n                  + \"completed in \"\n                  + (Time.monotonicNow() - startTimeMisReplicatedScan)\n                  + \" msec\");\n          break;\n        }\n      } finally {\n        namesystem.writeUnlock();\n        // Make sure it is out of the write lock for sufficiently long time.\n        Thread.sleep(sleepDuration);\n      }\n    }\n    if (Thread.currentThread().isInterrupted()) {\n      LOG.info(\"Interrupted while processing replication queues.\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.scanAndCompactStorages": "    private void scanAndCompactStorages() throws InterruptedException {\n      ArrayList<String> datanodesAndStorages = new ArrayList<>();\n      for (DatanodeDescriptor node\n          : datanodeManager.getDatanodeListForReport(DatanodeReportType.ALL)) {\n        for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n          try {\n            namesystem.readLock();\n            double ratio = storage.treeSetFillRatio();\n            if (ratio < storageInfoDefragmentRatio) {\n              datanodesAndStorages.add(node.getDatanodeUuid());\n              datanodesAndStorages.add(storage.getStorageID());\n            }\n            LOG.info(\"StorageInfo TreeSet fill ratio {} : {}{}\",\n                     storage.getStorageID(), ratio,\n                     (ratio < storageInfoDefragmentRatio)\n                     ? \" (queued for defragmentation)\" : \"\");\n          } finally {\n            namesystem.readUnlock();\n          }\n        }\n      }\n      if (!datanodesAndStorages.isEmpty()) {\n        for (int i = 0; i < datanodesAndStorages.size(); i += 2) {\n          namesystem.writeLock();\n          try {\n            final DatanodeDescriptor dn = datanodeManager.\n                getDatanode(datanodesAndStorages.get(i));\n            if (dn == null) {\n              continue;\n            }\n            final DatanodeStorageInfo storage = dn.\n                getStorageInfo(datanodesAndStorages.get(i + 1));\n            if (storage != null) {\n              boolean aborted =\n                  !storage.treeSetCompact(storageInfoDefragmentTimeout);\n              if (aborted) {\n                // Compaction timed out, reset iterator to continue with\n                // the same storage next iteration.\n                i -= 2;\n              }\n              LOG.info(\"StorageInfo TreeSet defragmented {} : {}{}\",\n                       storage.getStorageID(), storage.treeSetFillRatio(),\n                       aborted ? \" (aborted)\" : \"\");\n            }\n          } finally {\n            namesystem.writeUnlock();\n          }\n          // Wait between each iteration\n          Thread.sleep(1000);\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.heartbeatManager.getLiveDatanodeCount": "  synchronized int getLiveDatanodeCount() {\n    return datanodes.size();\n  }"
        },
        "bug_report": {
            "Title": "Delete copy-on-truncate block along with the original block, when deleting a file being truncated",
            "Description": "Active NamNode exit due to NPE, I can confirm that the BlockCollection passed in when creating ReplicationWork is null, but I do not know why BlockCollection is null, By view history I found [HDFS-9754|https://issues.apache.org/jira/browse/HDFS-9754] remove judging  whether  BlockCollection is null.\r\n\r\nNN logs are as following:\r\n{code:java}\r\n2017-10-11 16:29:06,161 ERROR [ReplicationMonitor] org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor thread received Runtime exception.\r\njava.lang.NullPointerException\r\n        at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:55)\r\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1532)\r\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1491)\r\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3792)\r\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3744)\r\n        at java.lang.Thread.run(Thread.java:834)\r\n{code}"
        }
    },
    {
        "filename": "HDFS-12383.json",
        "creation_time": "2017-08-31T20:43:50.000+0000",
        "stack_trace": "```\njava.util.concurrent.CancellationException\n        at java.util.concurrent.FutureTask.report(FutureTask.java:121)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:192)\n        at org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.takeAndProcessTasks(ReencryptionUpdater.java:404)\n        at org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.run(ReencryptionUpdater.java:250)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.takeAndProcessTasks": "  private void takeAndProcessTasks() throws Exception {\n    final Future<ReencryptionTask> completed = batchService.take();\n    throttle();\n    checkPauseForTesting();\n    ReencryptionTask task = completed.get();\n    if (completed.isCancelled()) {\n      LOG.debug(\"Skipped canceled re-encryption task for zone {}, last: {}\",\n          task.zoneId, task.lastFile);\n    }\n\n    boolean shouldRetry;\n    do {\n      dir.getFSNamesystem().writeLock();\n      try {\n        throttleTimerLocked.start();\n        processTask(task);\n        shouldRetry = false;\n      } catch (RetriableException | SafeModeException re) {\n        // Keep retrying until succeed.\n        LOG.info(\"Exception when processing re-encryption task for zone {}, \"\n                + \"retrying...\", task.zoneId, re);\n        shouldRetry = true;\n        Thread.sleep(faultRetryInterval);\n      } catch (IOException ioe) {\n        LOG.warn(\"Failure processing re-encryption task for zone {}\",\n            task.zoneId, ioe);\n        ++task.numFailures;\n        task.processed = true;\n        shouldRetry = false;\n      } finally {\n        dir.getFSNamesystem().writeUnlock(\"reencryptUpdater\");\n        throttleTimerLocked.stop();\n      }\n      // logSync regardless, to prevent edit log buffer overflow triggering\n      // logSync inside FSN writelock.\n      dir.getEditLog().logSync();\n    } while (shouldRetry);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.throttle": "  private void throttle() throws InterruptedException {\n    if (throttleLimitRatio >= 1.0) {\n      return;\n    }\n\n    final long expect = (long) (throttleTimerAll.now(TimeUnit.MILLISECONDS)\n        * throttleLimitRatio);\n    final long actual = throttleTimerLocked.now(TimeUnit.MILLISECONDS);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Re-encryption updater throttling expect: {}, actual: {},\"\n              + \" throttleTimerAll:{}\", expect, actual,\n          throttleTimerAll.now(TimeUnit.MILLISECONDS));\n    }\n    if (expect - actual < 0) {\n      // in case throttleLimitHandlerRatio is very small, expect will be 0.\n      // so sleepMs should not be calculated from expect, to really meet the\n      // ratio. e.g. if ratio is 0.001, expect = 0 and actual = 1, sleepMs\n      // should be 1000 - throttleTimerAll.now()\n      final long sleepMs =\n          (long) (actual / throttleLimitRatio) - throttleTimerAll\n              .now(TimeUnit.MILLISECONDS);\n      LOG.debug(\"Throttling re-encryption, sleeping for {} ms\", sleepMs);\n      Thread.sleep(sleepMs);\n    }\n    throttleTimerAll.reset().start();\n    throttleTimerLocked.reset();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.processTask": "  private void processTask(ReencryptionTask task)\n      throws InterruptedException, ExecutionException, IOException {\n    final List<XAttr> xAttrs;\n    final String zonePath;\n    dir.writeLock();\n    try {\n      handler.checkZoneReady(task.zoneId);\n      final INode zoneNode = dir.getInode(task.zoneId);\n      if (zoneNode == null) {\n        // ez removed.\n        return;\n      }\n      zonePath = zoneNode.getFullPathName();\n      LOG.info(\"Processing returned re-encryption task for zone {}({}), \"\n              + \"batch size {}, start:{}\", zonePath, task.zoneId,\n          task.batch.size(), task.batch.getFirstFilePath());\n      final ZoneSubmissionTracker tracker =\n          handler.getTracker(zoneNode.getId());\n      Preconditions.checkNotNull(tracker, \"zone tracker not found \" + zonePath);\n      tracker.numFutureDone++;\n      EncryptionFaultInjector.getInstance().reencryptUpdaterProcessOneTask();\n      processTaskEntries(zonePath, task);\n      EncryptionFaultInjector.getInstance().reencryptUpdaterProcessCheckpoint();\n      xAttrs = processCheckpoints(zoneNode, tracker);\n    } finally {\n      dir.writeUnlock();\n    }\n    FSDirEncryptionZoneOp.saveFileXAttrsForBatch(dir, task.batch.getBatch());\n    if (!xAttrs.isEmpty()) {\n      dir.getEditLog().logSetXAttrs(zonePath, xAttrs, false);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.checkPauseForTesting": "  private synchronized void checkPauseForTesting() throws InterruptedException {\n    assert !dir.hasWriteLock();\n    assert !dir.getFSNamesystem().hasWriteLock();\n    if (pauseAfterNthCheckpoint != 0) {\n      ZoneSubmissionTracker tracker =\n          handler.unprotectedGetTracker(pauseZoneId);\n      if (tracker != null) {\n        if (tracker.numFutureDone == pauseAfterNthCheckpoint) {\n          shouldPauseForTesting = true;\n          pauseAfterNthCheckpoint = 0;\n        }\n      }\n    }\n    while (shouldPauseForTesting) {\n      LOG.info(\"Sleeping in the re-encryption updater for unit test.\");\n      wait();\n      LOG.info(\"Continuing re-encryption updater after pausing.\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.run": "  public void run() {\n    throttleTimerAll.start();\n    while (true) {\n      try {\n        // Assuming single-threaded updater.\n        takeAndProcessTasks();\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Re-encryption updater thread interrupted. Exiting.\");\n        Thread.currentThread().interrupt();\n        return;\n      } catch (IOException ioe) {\n        LOG.warn(\"Re-encryption updater thread exception.\", ioe);\n      } catch (Throwable t) {\n        LOG.error(\"Re-encryption updater thread exiting.\", t);\n        return;\n      }\n    }\n  }"
        },
        "bug_report": {
            "Title": "Re-encryption updater should handle canceled tasks better",
            "Description": "Seen an instance where the re-encryption updater exited due to an exception, and later tasks no longer executes. Logs below:\n{noformat}\n2017-08-31 09:54:08,104 INFO org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager: Zone /tmp/encryption-zone-3(16819) is submitted for re-encryption.\n2017-08-31 09:54:08,104 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler: Executing re-encrypt commands on zone 16819. Current zones:[zone:16787 state:Completed lastProcessed:null filesReencrypted:1 fileReencryptionFailures:0][zone:16813 state:Completed lastProcessed:null filesReencrypted:1 fileReencryptionFailures:0][zone:16819 state:Submitted lastProcessed:null filesReencrypted:0 fileReencryptionFailures:0]\n2017-08-31 09:54:08,105 INFO org.apache.hadoop.hdfs.protocol.ReencryptionStatus: Zone 16819 starts re-encryption processing\n2017-08-31 09:54:08,105 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler: Re-encrypting zone /tmp/encryption-zone-3(id=16819)\n2017-08-31 09:54:08,105 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler: Submitted batch (start:/tmp/encryption-zone-3/data1, size:1) of zone 16819 to re-encrypt.\n2017-08-31 09:54:08,105 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler: Submission completed of zone 16819 for re-encryption.\n2017-08-31 09:54:08,105 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler: Processing batched re-encryption for zone 16819, batch size 1, start:/tmp/encryption-zone-3/data1\n2017-08-31 09:54:08,979 INFO BlockStateChange: BLOCK* BlockManager: ask 172.26.1.71:20002 to delete [blk_1073742291_1467]\n2017-08-31 09:54:18,295 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater: Cancelling 1 re-encryption tasks\n2017-08-31 09:54:18,295 INFO org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager: Cancelled zone /tmp/encryption-zone-3(16819) for re-encryption.\n2017-08-31 09:54:18,295 INFO org.apache.hadoop.hdfs.protocol.ReencryptionStatus: Zone 16819 completed re-encryption.\n2017-08-31 09:54:18,296 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler: Completed re-encrypting one batch of 1 edeks from KMS, time consumed: 10.19 s, start: /tmp/encryption-zone-3/data1.\n2017-08-31 09:54:18,296 ERROR org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater: Re-encryption updater thread exiting.\njava.util.concurrent.CancellationException\n        at java.util.concurrent.FutureTask.report(FutureTask.java:121)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:192)\n        at org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.takeAndProcessTasks(ReencryptionUpdater.java:404)\n        at org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.run(ReencryptionUpdater.java:250)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\n\nUpdater should be fixed to handle canceled tasks better."
        }
    },
    {
        "filename": "HDFS-5322.json",
        "creation_time": "2013-10-07T22:52:54.000+0000",
        "stack_trace": "```\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)\nError: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 11 for hrt_qa) can't be found in cache\nat org.apache.hadoop.ipc.Client.call(Client.java:1347)\nat org.apache.hadoop.ipc.Client.call(Client.java:1300)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\nat com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      LOG.error(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.call": "  public Writable call(RPC.RpcKind rpcKind, Writable rpcRequest,\n      ConnectionId remoteId, int serviceClass) throws IOException {\n    final Call call = createCall(rpcKind, rpcRequest);\n    Connection connection = getConnection(remoteId, call, serviceClass);\n    try {\n      connection.sendRpcRequest(call);                 // send the rpc request\n    } catch (RejectedExecutionException e) {\n      throw new IOException(\"connection has been closed\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      LOG.warn(\"interrupted waiting to send rpc request to server\", e);\n      throw new IOException(e);\n    }\n\n    boolean interrupted = false;\n    synchronized (call) {\n      while (!call.done) {\n        try {\n          call.wait();                           // wait for the result\n        } catch (InterruptedException ie) {\n          // save the fact that we were interrupted\n          interrupted = true;\n        }\n      }\n\n      if (interrupted) {\n        // set the interrupt flag now that we are done waiting\n        Thread.currentThread().interrupt();\n      }\n\n      if (call.error != null) {\n        if (call.error instanceof RemoteException) {\n          call.error.fillInStackTrace();\n          throw call.error;\n        } else { // local exception\n          InetSocketAddress address = connection.getRemoteAddress();\n          throw NetUtils.wrapException(address.getHostName(),\n                  address.getPort(),\n                  NetUtils.getHostname(),\n                  0,\n                  call.error);\n        }\n      } else {\n        return call.getRpcResponse();\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.sendRpcRequest": "    public void sendRpcRequest(final Call call)\n        throws InterruptedException, IOException {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n\n      // Serialize the call to be sent. This is done from the actual\n      // caller thread, rather than the sendParamsExecutor thread,\n      \n      // so that if the serialization throws an error, it is reported\n      // properly. This also parallelizes the serialization.\n      //\n      // Format of a call on the wire:\n      // 0) Length of rest below (1 + 2)\n      // 1) RpcRequestHeader  - is serialized Delimited hence contains length\n      // 2) RpcRequest\n      //\n      // Items '1' and '2' are prepared here. \n      final DataOutputBuffer d = new DataOutputBuffer();\n      RpcRequestHeaderProto header = ProtoUtil.makeRpcRequestHeader(\n          call.rpcKind, OperationProto.RPC_FINAL_PACKET, call.id, call.retry,\n          clientId);\n      header.writeDelimitedTo(d);\n      call.rpcRequest.write(d);\n\n      synchronized (sendRpcRequestLock) {\n        Future<?> senderFuture = sendParamsExecutor.submit(new Runnable() {\n          @Override\n          public void run() {\n            try {\n              synchronized (Connection.this.out) {\n                if (shouldCloseConnection.get()) {\n                  return;\n                }\n                \n                if (LOG.isDebugEnabled())\n                  LOG.debug(getName() + \" sending #\" + call.id);\n         \n                byte[] data = d.getData();\n                int totalLength = d.getLength();\n                out.writeInt(totalLength); // Total Length\n                out.write(data, 0, totalLength);// RpcRequestHeader + RpcRequest\n                out.flush();\n              }\n            } catch (IOException e) {\n              // exception at this point would leave the connection in an\n              // unrecoverable state (eg half a call left on the wire).\n              // So, close the connection, killing any outstanding calls\n              markClosed(e);\n            } finally {\n              //the buffer is just an in-memory buffer, but it is still polite to\n              // close early\n              IOUtils.closeStream(d);\n            }\n          }\n        });\n      \n        try {\n          senderFuture.get();\n        } catch (ExecutionException e) {\n          Throwable cause = e.getCause();\n          \n          // cause should only be a RuntimeException as the Runnable above\n          // catches IOException\n          if (cause instanceof RuntimeException) {\n            throw (RuntimeException) cause;\n          } else {\n            throw new RuntimeException(\"unexpected checked exception\", cause);\n          }\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnectionId": "    static ConnectionId getConnectionId(InetSocketAddress addr,\n        Class<?> protocol, UserGroupInformation ticket, int rpcTimeout,\n        RetryPolicy connectionRetryPolicy, Configuration conf) throws IOException {\n\n      if (connectionRetryPolicy == null) {\n        final int max = conf.getInt(\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY,\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_DEFAULT);\n        connectionRetryPolicy = RetryPolicies.retryUpToMaximumCountWithFixedSleep(\n            max, 1, TimeUnit.SECONDS);\n      }\n\n      boolean doPing =\n        conf.getBoolean(CommonConfigurationKeys.IPC_CLIENT_PING_KEY, true);\n      return new ConnectionId(addr, protocol, ticket, rpcTimeout,\n          conf.getInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECTION_MAXIDLETIME_KEY,\n              CommonConfigurationKeysPublic.IPC_CLIENT_CONNECTION_MAXIDLETIME_DEFAULT),\n          connectionRetryPolicy,\n          conf.getInt(\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_KEY,\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_DEFAULT),\n          conf.getBoolean(CommonConfigurationKeysPublic.IPC_CLIENT_TCPNODELAY_KEY,\n              CommonConfigurationKeysPublic.IPC_CLIENT_TCPNODELAY_DEFAULT),\n          doPing, \n          (doPing ? Client.getPingInterval(conf) : 0));\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.createCall": "  Call createCall(RPC.RpcKind rpcKind, Writable rpcRequest) {\n    return new Call(rpcKind, rpcRequest);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRpcResponse": "    public synchronized Writable getRpcResponse() {\n      return rpcResponse;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRemoteAddress": "    public InetSocketAddress getRemoteAddress() {\n      return server;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnection": "  private Connection getConnection(ConnectionId remoteId,\n      Call call, int serviceClass) throws IOException {\n    if (!running.get()) {\n      // the client is stopped\n      throw new IOException(\"The client is stopped\");\n    }\n    Connection connection;\n    /* we could avoid this allocation for each RPC by having a  \n     * connectionsId object and with set() method. We need to manage the\n     * refs for keys in HashMap properly. For now its ok.\n     */\n    do {\n      synchronized (connections) {\n        connection = connections.get(remoteId);\n        if (connection == null) {\n          connection = new Connection(remoteId, serviceClass);\n          connections.put(remoteId, connection);\n        }\n      }\n    } while (!connection.addCall(call));\n    \n    //we don't invoke the method below inside \"synchronized (connections)\"\n    //block above. The reason for that is if the server happens to be slow,\n    //it will take longer to establish a connection and that will slow the\n    //entire system down.\n    connection.setupIOstreams();\n    return connection;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.invoke": "    public Object invoke(Object proxy, Method method, Object[] args)\n        throws ServiceException {\n      long startTime = 0;\n      if (LOG.isDebugEnabled()) {\n        startTime = Time.now();\n      }\n      \n      if (args.length != 2) { // RpcController + Message\n        throw new ServiceException(\"Too many parameters for request. Method: [\"\n            + method.getName() + \"]\" + \", Expected: 2, Actual: \"\n            + args.length);\n      }\n      if (args[1] == null) {\n        throw new ServiceException(\"null param while calling Method: [\"\n            + method.getName() + \"]\");\n      }\n\n      RequestHeaderProto rpcRequestHeader = constructRpcRequestHeader(method);\n      \n      if (LOG.isTraceEnabled()) {\n        LOG.trace(Thread.currentThread().getId() + \": Call -> \" +\n            remoteId + \": \" + method.getName() +\n            \" {\" + TextFormat.shortDebugString((Message) args[1]) + \"}\");\n      }\n\n\n      Message theRequest = (Message) args[1];\n      final RpcResponseWrapper val;\n      try {\n        val = (RpcResponseWrapper) client.call(RPC.RpcKind.RPC_PROTOCOL_BUFFER,\n            new RpcRequestWrapper(rpcRequestHeader, theRequest), remoteId);\n\n      } catch (Throwable e) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Exception <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + e + \"}\");\n        }\n\n        throw new ServiceException(e);\n      }\n\n      if (LOG.isDebugEnabled()) {\n        long callTime = Time.now() - startTime;\n        LOG.debug(\"Call: \" + method.getName() + \" took \" + callTime + \"ms\");\n      }\n      \n      Message prototype = null;\n      try {\n        prototype = getReturnProtoType(method);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      Message returnMessage;\n      try {\n        returnMessage = prototype.newBuilderForType()\n            .mergeFrom(val.theResponseRead).build();\n\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Response <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + TextFormat.shortDebugString(returnMessage) + \"}\");\n        }\n\n      } catch (Throwable e) {\n        throw new ServiceException(e);\n      }\n      return returnMessage;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcRequestWrapper request = (RpcRequestWrapper) writableRequest;\n        RequestHeaderProto rpcRequest = request.requestHeader;\n        String methodName = rpcRequest.getMethodName();\n        \n        \n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = prototype.newBuilderForType()\n            .mergeFrom(request.theRequestRead).build();\n        \n        Message result;\n        try {\n          long startTime = Time.now();\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n          int processingTime = (int) (Time.now() - startTime);\n          int qTime = (int) (startTime - receiveTime);\n          if (LOG.isDebugEnabled()) {\n            LOG.info(\"Served: \" + methodName + \" queueTime= \" + qTime +\n                      \" procesingTime= \" + processingTime);\n          }\n          server.rpcMetrics.addRpcQueueTime(qTime);\n          server.rpcMetrics.addRpcProcessingTime(processingTime);\n          server.rpcDetailedMetrics.addProcessingTime(methodName,\n              processingTime);\n        } catch (ServiceException e) {\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          throw e;\n        }\n        return new RpcResponseWrapper(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.constructRpcRequestHeader": "    private RequestHeaderProto constructRpcRequestHeader(Method method) {\n      RequestHeaderProto.Builder builder = RequestHeaderProto\n          .newBuilder();\n      builder.setMethodName(method.getName());\n     \n\n      // For protobuf, {@code protocol} used when creating client side proxy is\n      // the interface extending BlockingInterface, which has the annotations \n      // such as ProtocolName etc.\n      //\n      // Using Method.getDeclaringClass(), as in WritableEngine to get at\n      // the protocol interface will return BlockingInterface, from where \n      // the annotation ProtocolName and Version cannot be\n      // obtained.\n      //\n      // Hence we simply use the protocol class used to create the proxy.\n      // For PB this may limit the use of mixins on client side.\n      builder.setDeclaringClassProtocolName(protocolName);\n      builder.setClientProtocolVersion(clientProtocolVersion);\n      return builder.build();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getReturnProtoType": "    private Message getReturnProtoType(Method method) throws Exception {\n      if (returnTypes.containsKey(method.getName())) {\n        return returnTypes.get(method.getName());\n      }\n      \n      Class<?> returnType = method.getReturnType();\n      Method newInstMethod = returnType.getMethod(\"getDefaultInstance\");\n      newInstMethod.setAccessible(true);\n      Message prototype = (Message) newInstMethod.invoke(null, (Object[]) null);\n      returnTypes.put(method.getName(), prototype);\n      return prototype;\n    }"
        },
        "bug_report": {
            "Title": "HDFS delegation token not found in cache errors seen on secure HA clusters",
            "Description": "While running HA tests we have seen issues were we see HDFS delegation token not found in cache errors causing jobs running to fail.\n\n{code}\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)\n|2013-10-06 20:14:51,193 INFO  [main] mapreduce.Job: Task Id : attempt_1381090351344_0001_m_000007_0, Status : FAILED\nError: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 11 for hrt_qa) can't be found in cache\nat org.apache.hadoop.ipc.Client.call(Client.java:1347)\nat org.apache.hadoop.ipc.Client.call(Client.java:1300)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\nat com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source)\n{code}"
        }
    },
    {
        "filename": "HDFS-11741.json",
        "creation_time": "2017-05-02T17:57:25.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=1005215027) doesn't exist. Current key: 1005215030\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.dispatch(Dispatcher.java:311)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.access$2300(Dispatcher.java:182)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher$1.run(Dispatcher.java:899)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption": "  public static SaslResponseWithNegotiatedCipherOption\n      readSaslMessageAndNegotiatedCipherOption(InputStream in)\n      throws IOException {\n    DataTransferEncryptorMessageProto proto =\n        DataTransferEncryptorMessageProto.parseFrom(vintPrefixed(in));\n    if (proto.getStatus() == DataTransferEncryptorStatus.ERROR_UNKNOWN_KEY) {\n      throw new InvalidEncryptionKeyException(proto.getMessage());\n    } else if (proto.getStatus() == DataTransferEncryptorStatus.ERROR) {\n      throw new IOException(proto.getMessage());\n    } else {\n      byte[] response = proto.getPayload().toByteArray();\n      List<CipherOption> options = PBHelperClient.convertCipherOptionProtos(\n          proto.getCipherOptionList());\n      CipherOption option = null;\n      if (options != null && !options.isEmpty()) {\n        option = options.get(0);\n      }\n      return new SaslResponseWithNegotiatedCipherOption(response, option);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake": "  private IOStreamPair doSaslHandshake(InetAddress addr,\n      OutputStream underlyingOut, InputStream underlyingIn, String userName,\n      Map<String, String> saslProps,\n      CallbackHandler callbackHandler) throws IOException {\n\n    DataOutputStream out = new DataOutputStream(underlyingOut);\n    DataInputStream in = new DataInputStream(underlyingIn);\n\n    SaslParticipant sasl= SaslParticipant.createClientSaslParticipant(userName,\n        saslProps, callbackHandler);\n\n    out.writeInt(SASL_TRANSFER_MAGIC_NUMBER);\n    out.flush();\n\n    try {\n      // Start of handshake - \"initial response\" in SASL terminology.\n      sendSaslMessage(out, new byte[0]);\n\n      // step 1\n      byte[] remoteResponse = readSaslMessage(in);\n      byte[] localResponse = sasl.evaluateChallengeOrResponse(remoteResponse);\n      List<CipherOption> cipherOptions = null;\n      String cipherSuites = conf.get(\n          DFS_ENCRYPT_DATA_TRANSFER_CIPHER_SUITES_KEY);\n      if (requestedQopContainsPrivacy(saslProps)) {\n        // Negotiate cipher suites if configured.  Currently, the only supported\n        // cipher suite is AES/CTR/NoPadding, but the protocol allows multiple\n        // values for future expansion.\n        if (cipherSuites != null && !cipherSuites.isEmpty()) {\n          if (!cipherSuites.equals(CipherSuite.AES_CTR_NOPADDING.getName())) {\n            throw new IOException(String.format(\"Invalid cipher suite, %s=%s\",\n                DFS_ENCRYPT_DATA_TRANSFER_CIPHER_SUITES_KEY, cipherSuites));\n          }\n          CipherOption option = new CipherOption(CipherSuite.AES_CTR_NOPADDING);\n          cipherOptions = Lists.newArrayListWithCapacity(1);\n          cipherOptions.add(option);\n        }\n      }\n      sendSaslMessageAndNegotiationCipherOptions(out, localResponse,\n          cipherOptions);\n\n      // step 2 (client-side only)\n      SaslResponseWithNegotiatedCipherOption response =\n          readSaslMessageAndNegotiatedCipherOption(in);\n      localResponse = sasl.evaluateChallengeOrResponse(response.payload);\n      assert localResponse == null;\n\n      // SASL handshake is complete\n      checkSaslComplete(sasl, saslProps);\n\n      CipherOption cipherOption = null;\n      if (sasl.isNegotiatedQopPrivacy()) {\n        // Unwrap the negotiated cipher option\n        cipherOption = unwrap(response.cipherOption, sasl);\n        if (LOG.isDebugEnabled()) {\n          if (cipherOption == null) {\n            // No cipher suite is negotiated\n            if (cipherSuites != null && !cipherSuites.isEmpty()) {\n              // the client accepts some cipher suites, but the server does not.\n              LOG.debug(\"Client accepts cipher suites {}, \"\n                      + \"but server {} does not accept any of them\",\n                  cipherSuites, addr.toString());\n            }\n          } else {\n            LOG.debug(\"Client using cipher suite {} with server {}\",\n                cipherOption.getCipherSuite().getName(), addr.toString());\n          }\n        }\n      }\n\n      // If negotiated cipher option is not null, we will use it to create\n      // stream pair.\n      return cipherOption != null ? createStreamPair(\n          conf, cipherOption, underlyingOut, underlyingIn, false) :\n          sasl.createStreamPair(out, in);\n    } catch (IOException ioe) {\n      sendGenericSaslErrorMessage(out, ioe.getMessage());\n      throw ioe;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams": "  private IOStreamPair getEncryptedStreams(InetAddress addr,\n      OutputStream underlyingOut,\n      InputStream underlyingIn, DataEncryptionKey encryptionKey)\n      throws IOException {\n    Map<String, String> saslProps = createSaslPropertiesForEncryption(\n        encryptionKey.encryptionAlgorithm);\n\n    LOG.debug(\"Client using encryption algorithm {}\",\n        encryptionKey.encryptionAlgorithm);\n\n    String userName = getUserNameFromEncryptionKey(encryptionKey);\n    char[] password = encryptionKeyToPassword(encryptionKey.encryptionKey);\n    CallbackHandler callbackHandler = new SaslClientCallbackHandler(userName,\n        password);\n    return doSaslHandshake(addr, underlyingOut, underlyingIn, userName,\n        saslProps, callbackHandler);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getUserNameFromEncryptionKey": "  private static String getUserNameFromEncryptionKey(\n      DataEncryptionKey encryptionKey) {\n    return encryptionKey.keyId + NAME_DELIMITER +\n        encryptionKey.blockPoolId + NAME_DELIMITER +\n        new String(Base64.encodeBase64(encryptionKey.nonce, false),\n            Charsets.UTF_8);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send": "  private IOStreamPair send(InetAddress addr, OutputStream underlyingOut,\n      InputStream underlyingIn, DataEncryptionKey encryptionKey,\n      Token<BlockTokenIdentifier> accessToken, DatanodeID datanodeId)\n      throws IOException {\n    if (encryptionKey != null) {\n      LOG.debug(\"SASL client doing encrypted handshake for addr = {}, \"\n          + \"datanodeId = {}\", addr, datanodeId);\n      return getEncryptedStreams(addr, underlyingOut, underlyingIn,\n          encryptionKey);\n    } else if (!UserGroupInformation.isSecurityEnabled()) {\n      LOG.debug(\"SASL client skipping handshake in unsecured configuration for \"\n          + \"addr = {}, datanodeId = {}\", addr, datanodeId);\n      return null;\n    } else if (SecurityUtil.isPrivilegedPort(datanodeId.getXferPort())) {\n      LOG.debug(\n          \"SASL client skipping handshake in secured configuration with \"\n              + \"privileged port for addr = {}, datanodeId = {}\",\n          addr, datanodeId);\n      return null;\n    } else if (fallbackToSimpleAuth != null && fallbackToSimpleAuth.get()) {\n      LOG.debug(\n          \"SASL client skipping handshake in secured configuration with \"\n              + \"unsecured cluster for addr = {}, datanodeId = {}\",\n          addr, datanodeId);\n      return null;\n    } else if (saslPropsResolver != null) {\n      LOG.debug(\n          \"SASL client doing general handshake for addr = {}, datanodeId = {}\",\n          addr, datanodeId);\n      return getSaslStreams(addr, underlyingOut, underlyingIn, accessToken);\n    } else {\n      // It's a secured cluster using non-privileged ports, but no SASL.  The\n      // only way this can happen is if the DataNode has\n      // ignore.secure.ports.for.testing configured so this is a rare edge case.\n      LOG.debug(\"SASL client skipping handshake in secured configuration with \"\n              + \"no SASL protection configured for addr = {}, datanodeId = {}\",\n          addr, datanodeId);\n      return null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getSaslStreams": "  private IOStreamPair getSaslStreams(InetAddress addr,\n      OutputStream underlyingOut, InputStream underlyingIn,\n      Token<BlockTokenIdentifier> accessToken)\n      throws IOException {\n    Map<String, String> saslProps = saslPropsResolver.getClientProperties(addr);\n\n    String userName = buildUserName(accessToken);\n    char[] password = buildClientPassword(accessToken);\n    CallbackHandler callbackHandler = new SaslClientCallbackHandler(userName,\n        password);\n    return doSaslHandshake(addr, underlyingOut, underlyingIn, userName,\n        saslProps, callbackHandler);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend": "  private IOStreamPair checkTrustAndSend(InetAddress addr,\n      OutputStream underlyingOut, InputStream underlyingIn,\n      DataEncryptionKeyFactory encryptionKeyFactory,\n      Token<BlockTokenIdentifier> accessToken, DatanodeID datanodeId)\n      throws IOException {\n    if (!trustedChannelResolver.isTrusted() &&\n        !trustedChannelResolver.isTrusted(addr)) {\n      // The encryption key factory only returns a key if encryption is enabled.\n      DataEncryptionKey encryptionKey =\n          encryptionKeyFactory.newDataEncryptionKey();\n      return send(addr, underlyingOut, underlyingIn, encryptionKey, accessToken,\n          datanodeId);\n    } else {\n      LOG.debug(\n          \"SASL client skipping handshake on trusted connection for addr = {}, \"\n              + \"datanodeId = {}\", addr, datanodeId);\n      return null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend": "  public IOStreamPair socketSend(Socket socket, OutputStream underlyingOut,\n      InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory,\n      Token<BlockTokenIdentifier> accessToken, DatanodeID datanodeId)\n      throws IOException {\n    IOStreamPair ios = checkTrustAndSend(socket.getInetAddress(), underlyingOut,\n        underlyingIn, encryptionKeyFactory, accessToken, datanodeId);\n    return ios != null ? ios : new IOStreamPair(underlyingIn, underlyingOut);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatch": "    private void dispatch() {\n      LOG.info(\"Start moving \" + this);\n      assert !(reportedBlock instanceof DBlockStriped);\n\n      Socket sock = new Socket();\n      DataOutputStream out = null;\n      DataInputStream in = null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().\n                getXferAddr(Dispatcher.this.connectToDnViaHostname)),\n                HdfsConstants.READ_TIMEOUT);\n\n        // Set read timeout so that it doesn't hang forever against\n        // unresponsive nodes. Datanode normally sends IN_PROGRESS response\n        // twice within the client read timeout period (every 30 seconds by\n        // default). Here, we make it give up after 5 minutes of no response.\n        sock.setSoTimeout(HdfsConstants.READ_TIMEOUT * 5);\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut = sock.getOutputStream();\n        InputStream unbufIn = sock.getInputStream();\n        ExtendedBlock eb = new ExtendedBlock(nnc.getBlockpoolID(),\n            reportedBlock.getBlock());\n        final KeyManager km = nnc.getKeyManager(); \n        Token<BlockTokenIdentifier> accessToken = km.getAccessToken(eb,\n            new StorageType[]{target.storageType});\n        IOStreamPair saslStreams = saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut = saslStreams.out;\n        unbufIn = saslStreams.in;\n        out = new DataOutputStream(new BufferedOutputStream(unbufOut,\n            ioFileBufferSize));\n        in = new DataInputStream(new BufferedInputStream(unbufIn,\n            ioFileBufferSize));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n        target.getDDatanode().setHasSuccess();\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this, e);\n        target.getDDatanode().setHasFailure();\n        // Check that the failure is due to block pinning errors.\n        if (e instanceof BlockPinningException) {\n          // Pinned block can't be moved. Add this block into failure list.\n          // Later in the next iteration mover will exclude these blocks from\n          // pending moves.\n          target.getDDatanode().addBlockPinningFailures(this);\n          return;\n        }\n\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.activateDelay": "    synchronized private void activateDelay(long delta) {\n      delayUntil = Time.monotonicNow() + delta;\n      LOG.info(this + \" activateDelay \" + delta/1000.0 + \" seconds\");\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.getBytesMoved": "  long getBytesMoved() {\n    return nnc.getBytesMoved().get();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.removePendingBlock": "    synchronized boolean removePendingBlock(PendingMove pendingBlock) {\n      return pendings.remove(pendingBlock);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.setHasSuccess": "    void setHasSuccess() {\n      this.hasSuccess = true;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.getDatanodeInfo": "    public DatanodeInfo getDatanodeInfo() {\n      return datanode;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.getDDatanode": "      private DDatanode getDDatanode() {\n        return DDatanode.this;\n      }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.setHasFailure": "    void setHasFailure() {\n      this.hasFailure = true;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.receiveResponse": "    private void receiveResponse(DataInputStream in) throws IOException {\n      long startTime = Time.monotonicNow();\n      BlockOpResponseProto response =\n          BlockOpResponseProto.parseFrom(vintPrefixed(in));\n      while (response.getStatus() == Status.IN_PROGRESS) {\n        // read intermediate responses\n        response = BlockOpResponseProto.parseFrom(vintPrefixed(in));\n        // Stop waiting for slow block moves. Even if it stops waiting,\n        // the actual move may continue.\n        if (stopWaitingForResponse(startTime)) {\n          throw new IOException(\"Block move timed out\");\n        }\n      }\n      String logInfo = \"reportedBlock move is failed\";\n      DataTransferProtoUtil.checkBlockOpStatus(response, logInfo, true);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.addBlockPinningFailures": "    void addBlockPinningFailures(PendingMove pendingBlock) {\n      synchronized (blockPinningFailures) {\n        long blockId = pendingBlock.reportedBlock.getBlock().getBlockId();\n        Set<DatanodeInfo> pinnedLocations = blockPinningFailures.get(blockId);\n        if (pinnedLocations == null) {\n          pinnedLocations = new HashSet<>();\n          blockPinningFailures.put(blockId, pinnedLocations);\n        }\n        pinnedLocations.add(pendingBlock.getSource());\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.sendRequest": "    private void sendRequest(DataOutputStream out, ExtendedBlock eb,\n        Token<BlockTokenIdentifier> accessToken) throws IOException {\n      new Sender(out).replaceBlock(eb, target.storageType, accessToken,\n          source.getDatanodeInfo().getDatanodeUuid(), proxySource.datanode);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.getNumBytes": "    public long getNumBytes(StorageGroup storage) {\n      return getInternalBlock(storage).getNumBytes();\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.reset": "  void reset(Configuration conf) {\n    cluster = NetworkTopology.getInstance(conf);\n    storageGroupMap.clear();\n    sources.clear();\n\n    moverThreadAllocator.reset();\n    for(StorageGroup t : targets) {\n      t.getDDatanode().shutdownMoveExecutor();\n    }\n    targets.clear();\n    globalBlocks.removeAllButRetain(movedBlocks);\n    movedBlocks.cleanup();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.run": "        public void run() {\n          s.dispatchBlocks(delay);\n        }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchBlocks": "    private void dispatchBlocks(long delay) {\n      this.blocksToReceive = 2 * getScheduledSize();\n      long previousMoveTimestamp = Time.monotonicNow();\n      while (getScheduledSize() > 0 && !isIterationOver()\n          && (!srcBlocks.isEmpty() || blocksToReceive > 0)) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(this + \" blocksToReceive=\" + blocksToReceive\n              + \", scheduledSize=\" + getScheduledSize()\n              + \", srcBlocks#=\" + srcBlocks.size());\n        }\n        final PendingMove p = chooseNextMove();\n        if (p != null) {\n          // Reset previous move timestamp\n          previousMoveTimestamp = Time.monotonicNow();\n          executePendingMove(p);\n          continue;\n        }\n\n        // Since we cannot schedule any block to move,\n        // remove any moved blocks from the source block list and\n        removeMovedBlocks(); // filter already moved blocks\n        // check if we should fetch more blocks from the namenode\n        if (shouldFetchMoreBlocks()) {\n          // fetch new blocks\n          try {\n            if(delay > 0) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Sleeping \" + delay + \"  msec.\");\n              }\n              Thread.sleep(delay);\n            }\n            final long received = getBlockList();\n            if (received == 0) {\n              return;\n            }\n            blocksToReceive -= received;\n            continue;\n          } catch (InterruptedException ignored) {\n            // nothing to do\n          } catch (IOException e) {\n            LOG.warn(\"Exception while getting reportedBlock list\", e);\n            return;\n          } finally {\n            delay = 0L;\n          }\n        } else {\n          // jump out of while-loop after the configured timeout.\n          long noMoveInterval = Time.monotonicNow() - previousMoveTimestamp;\n          if (noMoveInterval > maxNoMoveInterval) {\n            LOG.info(\"Failed to find a pending move for \"  + noMoveInterval\n                + \" ms.  Skipping \" + this);\n            resetScheduledSize();\n          }\n        }\n\n        // Now we can not schedule any block to move and there are\n        // no new blocks added to the source block list, so we wait.\n        try {\n          synchronized (Dispatcher.this) {\n            Dispatcher.this.wait(1000); // wait for targets/sources to be idle\n          }\n          // Didn't find a possible move in this iteration of the while loop,\n          // adding a small delay before choosing next move again.\n          Thread.sleep(100);\n        } catch (InterruptedException ignored) {\n        }\n      }\n\n      if (isIterationOver()) {\n        LOG.info(\"The maximum iteration time (\" + MAX_ITERATION_TIME/1000\n            + \" seconds) has been reached. Stopping \" + this);\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant.evaluateChallengeOrResponse": "  public byte[] evaluateChallengeOrResponse(byte[] challengeOrResponse)\n      throws SaslException {\n    if (saslClient != null) {\n      return saslClient.evaluateChallenge(challengeOrResponse);\n    } else {\n      return saslServer.evaluateResponse(challengeOrResponse);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant.createClientSaslParticipant": "  public static SaslParticipant createClientSaslParticipant(String userName,\n      Map<String, String> saslProps, CallbackHandler callbackHandler)\n      throws SaslException {\n    return new SaslParticipant(Sasl.createSaslClient(new String[] { MECHANISM },\n      userName, PROTOCOL, SERVER_NAME, saslProps, callbackHandler));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant.createStreamPair": "  public IOStreamPair createStreamPair(DataOutputStream out,\n      DataInputStream in) {\n    if (saslClient != null) {\n      return new IOStreamPair(\n          new SaslInputStream(in, saslClient),\n          new SaslOutputStream(out, saslClient));\n    } else {\n      return new IOStreamPair(\n          new SaslInputStream(in, saslServer),\n          new SaslOutputStream(out, saslServer));\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant.isNegotiatedQopPrivacy": "  public boolean isNegotiatedQopPrivacy() {\n    String qop = getNegotiatedQop();\n    return qop != null && \"auth-conf\".equalsIgnoreCase(qop);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant.getNegotiatedQop": "  public String getNegotiatedQop() {\n    if (saslClient != null) {\n      return (String) saslClient.getNegotiatedProperty(Sasl.QOP);\n    } else {\n      return (String) saslServer.getNegotiatedProperty(Sasl.QOP);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.KeyManager.getAccessToken": "  public Token<BlockTokenIdentifier> getAccessToken(ExtendedBlock eb,\n      StorageType[] storageTypes) throws IOException {\n    if (!isBlockTokenEnabled) {\n      return BlockTokenSecretManager.DUMMY_TOKEN;\n    } else {\n      if (!shouldRun) {\n        throw new IOException(\n            \"Cannot get access token since BlockKeyUpdater is not running\");\n      }\n      return blockTokenSecretManager.generateToken(null, eb,\n          EnumSet.of(BlockTokenIdentifier.AccessMode.REPLACE,\n              BlockTokenIdentifier.AccessMode.COPY), storageTypes);\n    }\n  }"
        },
        "bug_report": {
            "Title": "Long running balancer may fail due to expired DataEncryptionKey",
            "Description": "We found a long running balancer may fail despite using keytab, because KeyManager returns expired DataEncryptionKey, and it throws the following exception:\n\n{noformat}\n2017-04-30 05:03:58,661 WARN  [pool-1464-thread-10] balancer.Dispatcher (Dispatcher.java:dispatch(325)) - Failed to move blk_1067352712_3913241 with size=546650 from 10.0.0.134:50010:DISK to 10.0.0.98:50010:DISK through 10.0.0.134:50010\norg.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=1005215027) doesn't exist. Current key: 1005215030\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.dispatch(Dispatcher.java:311)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.access$2300(Dispatcher.java:182)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher$1.run(Dispatcher.java:899)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\n\nThis bug is similar in nature to HDFS-10609. While balancer KeyManager actively synchronizes itself with NameNode w.r.t block keys, it does not update DataEncryptionKey accordingly.\n\nIn a specific cluster, with Kerberos ticket life time 10 hours, and default block token expiration/life time 10 hours, a long running balancer failed after 20~30 hours."
        }
    },
    {
        "filename": "HDFS-3936.json",
        "creation_time": "2012-09-14T05:35:12.000+0000",
        "stack_trace": "```\norg.apache.hadoop.util.ExitUtil$ExitException: Fatal exception with message null\nstack trace\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap.getBlockCollection(BlocksMap.java:101)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1132)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1107)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3061)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3023)\n\tat java.lang.Thread.run(Thread.java:662)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap.getBlockCollection": "  BlockCollection getBlockCollection(Block b) {\n    BlockInfo info = blocks.get(b);\n    return (info != null) ? info.getBlockCollection() : null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks": "  int computeReplicationWorkForBlocks(List<List<Block>> blocksToReplicate) {\n    int requiredReplication, numEffectiveReplicas;\n    List<DatanodeDescriptor> containingNodes, liveReplicaNodes;\n    DatanodeDescriptor srcNode;\n    BlockCollection bc = null;\n    int additionalReplRequired;\n\n    int scheduledWork = 0;\n    List<ReplicationWork> work = new LinkedList<ReplicationWork>();\n\n    namesystem.writeLock();\n    try {\n      synchronized (neededReplications) {\n        for (int priority = 0; priority < blocksToReplicate.size(); priority++) {\n          for (Block block : blocksToReplicate.get(priority)) {\n            // block should belong to a file\n            bc = blocksMap.getBlockCollection(block);\n            // abandoned block or block reopened for append\n            if(bc == null || bc instanceof MutableBlockCollection) {\n              neededReplications.remove(block, priority); // remove from neededReplications\n              neededReplications.decrementReplicationIndex(priority);\n              continue;\n            }\n\n            requiredReplication = bc.getReplication();\n\n            // get a source data-node\n            containingNodes = new ArrayList<DatanodeDescriptor>();\n            liveReplicaNodes = new ArrayList<DatanodeDescriptor>();\n            NumberReplicas numReplicas = new NumberReplicas();\n            srcNode = chooseSourceDatanode(\n                block, containingNodes, liveReplicaNodes, numReplicas);\n            if(srcNode == null) { // block can not be replicated from any node\n              LOG.debug(\"Block \" + block + \" cannot be repl from any node\");\n              continue;\n          }\n\n            assert liveReplicaNodes.size() == numReplicas.liveReplicas();\n            // do not schedule more if enough replicas is already pending\n            numEffectiveReplicas = numReplicas.liveReplicas() +\n                                    pendingReplications.getNumReplicas(block);\n      \n            if (numEffectiveReplicas >= requiredReplication) {\n              if ( (pendingReplications.getNumReplicas(block) > 0) ||\n                   (blockHasEnoughRacks(block)) ) {\n                neededReplications.remove(block, priority); // remove from neededReplications\n                neededReplications.decrementReplicationIndex(priority);\n                NameNode.stateChangeLog.info(\"BLOCK* \"\n                    + \"Removing block \" + block\n                    + \" from neededReplications as it has enough replicas.\");\n                continue;\n              }\n            }\n\n            if (numReplicas.liveReplicas() < requiredReplication) {\n              additionalReplRequired = requiredReplication\n                  - numEffectiveReplicas;\n            } else {\n              additionalReplRequired = 1; // Needed on a new rack\n            }\n            work.add(new ReplicationWork(block, bc, srcNode,\n                containingNodes, liveReplicaNodes, additionalReplRequired,\n                priority));\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    HashMap<Node, Node> excludedNodes\n        = new HashMap<Node, Node>();\n    for(ReplicationWork rw : work){\n      // Exclude all of the containing nodes from being targets.\n      // This list includes decommissioning or corrupt nodes.\n      excludedNodes.clear();\n      for (DatanodeDescriptor dn : rw.containingNodes) {\n        excludedNodes.put(dn, dn);\n      }\n\n      // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n      // It is costly to extract the filename for which chooseTargets is called,\n      // so for now we pass in the block collection itself.\n      rw.targets = blockplacement.chooseTarget(rw.bc,\n          rw.additionalReplRequired, rw.srcNode, rw.liveReplicaNodes,\n          excludedNodes, rw.block.getNumBytes());\n    }\n\n    namesystem.writeLock();\n    try {\n      for(ReplicationWork rw : work){\n        DatanodeDescriptor[] targets = rw.targets;\n        if(targets == null || targets.length == 0){\n          rw.targets = null;\n          continue;\n        }\n\n        synchronized (neededReplications) {\n          Block block = rw.block;\n          int priority = rw.priority;\n          // Recheck since global lock was released\n          // block should belong to a file\n          bc = blocksMap.getBlockCollection(block);\n          // abandoned block or block reopened for append\n          if(bc == null || bc instanceof MutableBlockCollection) {\n            neededReplications.remove(block, priority); // remove from neededReplications\n            rw.targets = null;\n            neededReplications.decrementReplicationIndex(priority);\n            continue;\n          }\n          requiredReplication = bc.getReplication();\n\n          // do not schedule more if enough replicas is already pending\n          NumberReplicas numReplicas = countNodes(block);\n          numEffectiveReplicas = numReplicas.liveReplicas() +\n            pendingReplications.getNumReplicas(block);\n\n          if (numEffectiveReplicas >= requiredReplication) {\n            if ( (pendingReplications.getNumReplicas(block) > 0) ||\n                 (blockHasEnoughRacks(block)) ) {\n              neededReplications.remove(block, priority); // remove from neededReplications\n              neededReplications.decrementReplicationIndex(priority);\n              rw.targets = null;\n              NameNode.stateChangeLog.info(\"BLOCK* \"\n                  + \"Removing block \" + block\n                  + \" from neededReplications as it has enough replicas.\");\n              continue;\n            }\n          }\n\n          if ( (numReplicas.liveReplicas() >= requiredReplication) &&\n               (!blockHasEnoughRacks(block)) ) {\n            if (rw.srcNode.getNetworkLocation().equals(targets[0].getNetworkLocation())) {\n              //No use continuing, unless a new rack in this case\n              continue;\n            }\n          }\n\n          // Add block to the to be replicated list\n          rw.srcNode.addBlockToBeReplicated(block, targets);\n          scheduledWork++;\n\n          for (DatanodeDescriptor dn : targets) {\n            dn.incBlocksScheduled();\n          }\n\n          // Move the block-replication into a \"pending\" state.\n          // The reason we use 'pending' is so we can retry\n          // replications that fail after an appropriate amount of time.\n          pendingReplications.add(block, targets.length);\n          if(NameNode.stateChangeLog.isDebugEnabled()) {\n            NameNode.stateChangeLog.debug(\n                \"BLOCK* block \" + block\n                + \" is moved from neededReplications to pendingReplications\");\n          }\n\n          // remove from neededReplications\n          if(numEffectiveReplicas + targets.length >= requiredReplication) {\n            neededReplications.remove(block, priority); // remove from neededReplications\n            neededReplications.decrementReplicationIndex(priority);\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    if (NameNode.stateChangeLog.isInfoEnabled()) {\n      // log which blocks have been scheduled for replication\n      for(ReplicationWork rw : work){\n        DatanodeDescriptor[] targets = rw.targets;\n        if (targets != null && targets.length != 0) {\n          StringBuilder targetList = new StringBuilder(\"datanode(s)\");\n          for (int k = 0; k < targets.length; k++) {\n            targetList.append(' ');\n            targetList.append(targets[k]);\n          }\n          NameNode.stateChangeLog.info(\n                  \"BLOCK* ask \"\n                  + rw.srcNode + \" to replicate \"\n                  + rw.block + \" to \" + targetList);\n        }\n      }\n    }\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n        NameNode.stateChangeLog.debug(\n          \"BLOCK* neededReplications = \" + neededReplications.size()\n          + \" pendingReplications = \" + pendingReplications.size());\n    }\n\n    return scheduledWork;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget": "  public DatanodeDescriptor[] chooseTarget(final String src,\n      final int numOfReplicas, final DatanodeDescriptor client,\n      final HashMap<Node, Node> excludedNodes,\n      final long blocksize) throws IOException {\n    // choose targets for the new block to be allocated.\n    final DatanodeDescriptor targets[] = blockplacement.chooseTarget(src,\n        numOfReplicas, client, new ArrayList<DatanodeDescriptor>(), false,\n        excludedNodes, blocksize);\n    if (targets.length < minReplication) {\n      throw new IOException(\"File \" + src + \" could only be replicated to \"\n          + targets.length + \" nodes instead of minReplication (=\"\n          + minReplication + \").  There are \"\n          + getDatanodeManager().getNetworkTopology().getNumOfLeaves()\n          + \" datanode(s) running and \"\n          + (excludedNodes == null? \"no\": excludedNodes.size())\n          + \" node(s) are excluded in this operation.\");\n    }\n    return targets;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.getBlockCollection": "  public BlockCollection getBlockCollection(Block b) {\n    return blocksMap.getBlockCollection(b);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.countNodes": "  public NumberReplicas countNodes(Block b) {\n    int decommissioned = 0;\n    int live = 0;\n    int corrupt = 0;\n    int excess = 0;\n    int stale = 0;\n    Iterator<DatanodeDescriptor> nodeIter = blocksMap.nodeIterator(b);\n    Collection<DatanodeDescriptor> nodesCorrupt = corruptReplicas.getNodes(b);\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node = nodeIter.next();\n      if ((nodesCorrupt != null) && (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        decommissioned++;\n      } else {\n        LightWeightLinkedSet<Block> blocksExcess = excessReplicateMap.get(node\n            .getStorageID());\n        if (blocksExcess != null && blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n      if (node.areBlockContentsStale()) {\n        stale++;\n      }\n    }\n    return new NumberReplicas(live, decommissioned, corrupt, excess, stale);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseSourceDatanode": "  private DatanodeDescriptor chooseSourceDatanode(\n                                    Block block,\n                                    List<DatanodeDescriptor> containingNodes,\n                                    List<DatanodeDescriptor> nodesContainingLiveReplicas,\n                                    NumberReplicas numReplicas) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    DatanodeDescriptor srcNode = null;\n    int live = 0;\n    int decommissioned = 0;\n    int corrupt = 0;\n    int excess = 0;\n    Iterator<DatanodeDescriptor> it = blocksMap.nodeIterator(block);\n    Collection<DatanodeDescriptor> nodesCorrupt = corruptReplicas.getNodes(block);\n    while(it.hasNext()) {\n      DatanodeDescriptor node = it.next();\n      LightWeightLinkedSet<Block> excessBlocks =\n        excessReplicateMap.get(node.getStorageID());\n      if ((nodesCorrupt != null) && (nodesCorrupt.contains(node)))\n        corrupt++;\n      else if (node.isDecommissionInProgress() || node.isDecommissioned())\n        decommissioned++;\n      else if (excessBlocks != null && excessBlocks.contains(block)) {\n        excess++;\n      } else {\n        nodesContainingLiveReplicas.add(node);\n        live++;\n      }\n      containingNodes.add(node);\n      // Check if this replica is corrupt\n      // If so, do not select the node as src node\n      if ((nodesCorrupt != null) && nodesCorrupt.contains(node))\n        continue;\n      if(node.getNumberOfBlocksToBeReplicated() >= maxReplicationStreams)\n        continue; // already reached replication limit\n      // the block must not be scheduled for removal on srcNode\n      if(excessBlocks != null && excessBlocks.contains(block))\n        continue;\n      // never use already decommissioned nodes\n      if(node.isDecommissioned())\n        continue;\n      // we prefer nodes that are in DECOMMISSION_INPROGRESS state\n      if(node.isDecommissionInProgress() || srcNode == null) {\n        srcNode = node;\n        continue;\n      }\n      if(srcNode.isDecommissionInProgress())\n        continue;\n      // switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if(DFSUtil.getRandom().nextBoolean())\n        srcNode = node;\n    }\n    if(numReplicas != null)\n      numReplicas.initialize(live, decommissioned, corrupt, excess, 0);\n    return srcNode;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.blockHasEnoughRacks": "  boolean blockHasEnoughRacks(Block b) {\n    if (!this.shouldCheckForEnoughRacks) {\n      return true;\n    }\n    boolean enoughRacks = false;;\n    Collection<DatanodeDescriptor> corruptNodes = \n                                  corruptReplicas.getNodes(b);\n    int numExpectedReplicas = getReplication(b);\n    String rackName = null;\n    for (Iterator<DatanodeDescriptor> it = blocksMap.nodeIterator(b); \n         it.hasNext();) {\n      DatanodeDescriptor cur = it.next();\n      if (!cur.isDecommissionInProgress() && !cur.isDecommissioned()) {\n        if ((corruptNodes == null ) || !corruptNodes.contains(cur)) {\n          if (numExpectedReplicas == 1 ||\n              (numExpectedReplicas > 1 &&\n                  !datanodeManager.hasClusterEverBeenMultiRack())) {\n            enoughRacks = true;\n            break;\n          }\n          String rackNameNew = cur.getNetworkLocation();\n          if (rackName == null) {\n            rackName = rackNameNew;\n          } else if (!rackName.equals(rackNameNew)) {\n            enoughRacks = true;\n            break;\n          }\n        }\n      }\n    }\n    return enoughRacks;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.getReplication": "  private int getReplication(Block block) {\n    BlockCollection bc = blocksMap.getBlockCollection(block);\n    if (bc == null) { // block does not belong to any file\n      return 0;\n    }\n    return bc.getReplication();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork": "  int computeReplicationWork(int blocksToProcess) {\n    List<List<Block>> blocksToReplicate = null;\n    namesystem.writeLock();\n    try {\n      // Choose the blocks to be replicated\n      blocksToReplicate = neededReplications\n          .chooseUnderReplicatedBlocks(blocksToProcess);\n    } finally {\n      namesystem.writeUnlock();\n    }\n    return computeReplicationWorkForBlocks(blocksToReplicate);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork": "  int computeDatanodeWork() {\n    // Blocks should not be replicated or removed if in safe mode.\n    // It's OK to check safe mode here w/o holding lock, in the worst\n    // case extra replications will be scheduled, and these will get\n    // fixed up later.\n    if (namesystem.isInSafeMode()) {\n      return 0;\n    }\n\n    final int numlive = heartbeatManager.getLiveDatanodeCount();\n    final int blocksToProcess = numlive\n        * this.blocksReplWorkMultiplier;\n    final int nodesToProcess = (int) Math.ceil(numlive\n        * this.blocksInvalidateWorkPct);\n\n    int workFound = this.computeReplicationWork(blocksToProcess);\n\n    // Update counters\n    namesystem.writeLock();\n    try {\n      this.updateState();\n      this.scheduledReplicationBlocksCount = workFound;\n    } finally {\n      namesystem.writeUnlock();\n    }\n    workFound += this.computeInvalidateWork(nodesToProcess);\n    return workFound;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.updateState": "  void updateState() {\n    pendingReplicationBlocksCount = pendingReplications.size();\n    underReplicatedBlocksCount = neededReplications.size();\n    corruptReplicaBlocksCount = corruptReplicas.size();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeInvalidateWork": "  int computeInvalidateWork(int nodesToProcess) {\n    final List<String> nodes = invalidateBlocks.getStorageIDs();\n    Collections.shuffle(nodes);\n\n    nodesToProcess = Math.min(nodes.size(), nodesToProcess);\n\n    int blockCnt = 0;\n    for(int nodeCnt = 0; nodeCnt < nodesToProcess; nodeCnt++ ) {\n      blockCnt += invalidateWorkForOneNode(nodes.get(nodeCnt));\n    }\n    return blockCnt;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.run": "    public void run() {\n      while (namesystem.isRunning()) {\n        try {\n          computeDatanodeWork();\n          processPendingReplications();\n          Thread.sleep(replicationRecheckInterval);\n        } catch (InterruptedException ie) {\n          LOG.warn(\"ReplicationMonitor thread received InterruptedException.\", ie);\n          break;\n        } catch (Throwable t) {\n          LOG.fatal(\"ReplicationMonitor thread received Runtime exception. \", t);\n          terminate(1, t);\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processPendingReplications": "  private void processPendingReplications() {\n    Block[] timedOutItems = pendingReplications.getTimedOutBlocks();\n    if (timedOutItems != null) {\n      namesystem.writeLock();\n      try {\n        for (int i = 0; i < timedOutItems.length; i++) {\n          NumberReplicas num = countNodes(timedOutItems[i]);\n          if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                 num.liveReplicas())) {\n            neededReplications.add(timedOutItems[i],\n                                   num.liveReplicas(),\n                                   num.decommissionedReplicas(),\n                                   getReplication(timedOutItems[i]));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas.liveReplicas": "  public int liveReplicas() {\n    return liveReplicas;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.heartbeatManager.getLiveDatanodeCount": "  synchronized int getLiveDatanodeCount() {\n    return datanodes.size();\n  }"
        },
        "bug_report": {
            "Title": "MiniDFSCluster shutdown races with BlocksMap usage",
            "Description": "Looks like HDFS-3664 didn't fix the whole issue because the added join times out because the thread closing the BM (FSN#stopCommonServices) holds the FSN lock while closing the BM and the BM is block uninterruptedly trying to aquire the FSN lock.\n\n{noformat}\n2012-09-13 18:54:12,526 FATAL hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1355)) - Test resulted in an unexpected exit\norg.apache.hadoop.util.ExitUtil$ExitException: Fatal exception with message null\nstack trace\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap.getBlockCollection(BlocksMap.java:101)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1132)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1107)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3061)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3023)\n\tat java.lang.Thread.run(Thread.java:662)\n{noformat}"
        }
    },
    {
        "filename": "HDFS-6348.json",
        "creation_time": "2014-05-07T12:34:54.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1900)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.getInstance(BlockPlacementPolicy.java:199)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:256)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:635)\n\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:260)\n\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init>(SecondaryNameNode.java:205)\n\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:695)\nCaused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1868)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1892)\n\t... 6 more\nCaused by: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1774)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1866)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getClass": "  public <U> Class<? extends U> getClass(String name, \n                                         Class<? extends U> defaultValue, \n                                         Class<U> xface) {\n    try {\n      Class<?> theClass = getClass(name, defaultValue);\n      if (theClass != null && !xface.isAssignableFrom(theClass))\n        throw new RuntimeException(theClass+\" not \"+xface.getName());\n      else if (theClass != null)\n        return theClass.asSubclass(xface);\n      else\n        return null;\n    } catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getTrimmed": "  public String getTrimmed(String name, String defaultValue) {\n    String ret = getTrimmed(name);\n    return ret == null ? defaultValue : ret;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getName": "    public String getName(){\n      return name;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.getClassByName": "  public Class<?> getClassByName(String name) throws ClassNotFoundException {\n    Class<?> ret = getClassByNameOrNull(name);\n    if (ret == null) {\n      throw new ClassNotFoundException(\"Class \" + name + \" not found\");\n    }\n    return ret;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.getInstance": "  public static BlockPlacementPolicy getInstance(Configuration conf, \n                                                 FSClusterStats stats,\n                                                 NetworkTopology clusterMap,\n                                                 Host2NodesMap host2datanodeMap) {\n    final Class<? extends BlockPlacementPolicy> replicatorClass = conf.getClass(\n        DFSConfigKeys.DFS_BLOCK_REPLICATOR_CLASSNAME_KEY,\n        DFSConfigKeys.DFS_BLOCK_REPLICATOR_CLASSNAME_DEFAULT,\n        BlockPlacementPolicy.class);\n    final BlockPlacementPolicy replicator = ReflectionUtils.newInstance(\n        replicatorClass, conf);\n    replicator.initialize(conf, stats, clusterMap, host2datanodeMap);\n    return replicator;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.initialize": "  abstract protected void initialize(Configuration conf,  FSClusterStats stats, \n                                     NetworkTopology clusterMap, \n                                     Host2NodesMap host2datanodeMap);\n    \n  /**\n   * Get an instance of the configured Block Placement Policy based on the\n   * the configuration property\n   * {@link  DFSConfigKeys#DFS_BLOCK_REPLICATOR_CLASSNAME_KEY}.",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize": "  private void initialize(final Configuration conf,\n      CommandLineOpts commandLineOpts) throws IOException {\n    final InetSocketAddress infoSocAddr = getHttpAddress(conf);\n    final String infoBindAddress = infoSocAddr.getHostName();\n    UserGroupInformation.setConfiguration(conf);\n    if (UserGroupInformation.isSecurityEnabled()) {\n      SecurityUtil.login(conf,\n          DFSConfigKeys.DFS_SECONDARY_NAMENODE_KEYTAB_FILE_KEY,\n          DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY, infoBindAddress);\n    }\n    // initiate Java VM metrics\n    DefaultMetricsSystem.initialize(\"SecondaryNameNode\");\n    JvmMetrics.create(\"SecondaryNameNode\",\n        conf.get(DFSConfigKeys.DFS_METRICS_SESSION_ID_KEY),\n        DefaultMetricsSystem.instance());\n\n    // Create connection to the namenode.\n    shouldRun = true;\n    nameNodeAddr = NameNode.getServiceAddress(conf, true);\n\n    this.conf = conf;\n    this.namenode = NameNodeProxies.createNonHAProxy(conf, nameNodeAddr, \n        NamenodeProtocol.class, UserGroupInformation.getCurrentUser(),\n        true).getProxy();\n\n    // initialize checkpoint directories\n    fsName = getInfoServer();\n    checkpointDirs = FSImage.getCheckpointDirs(conf,\n                                  \"/tmp/hadoop/dfs/namesecondary\");\n    checkpointEditsDirs = FSImage.getCheckpointEditsDirs(conf, \n                                  \"/tmp/hadoop/dfs/namesecondary\");    \n    checkpointImage = new CheckpointStorage(conf, checkpointDirs, checkpointEditsDirs);\n    checkpointImage.recoverCreate(commandLineOpts.shouldFormat());\n    checkpointImage.deleteTempEdits();\n    \n    namesystem = new FSNamesystem(conf, checkpointImage, true);\n\n    // Disable quota checks\n    namesystem.dir.disableQuotaChecks();\n\n    // Initialize other scheduling parameters from the configuration\n    checkpointConf = new CheckpointConf(conf);\n\n    final InetSocketAddress httpAddr = infoSocAddr;\n\n    final String httpsAddrString = conf.get(\n        DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTPS_ADDRESS_KEY,\n        DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTPS_ADDRESS_DEFAULT);\n    InetSocketAddress httpsAddr = NetUtils.createSocketAddr(httpsAddrString);\n\n    HttpServer2.Builder builder = DFSUtil.httpServerTemplateForNNAndJN(conf,\n        httpAddr, httpsAddr, \"secondary\",\n        DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_INTERNAL_SPNEGO_PRINCIPAL_KEY,\n        DFSConfigKeys.DFS_SECONDARY_NAMENODE_KEYTAB_FILE_KEY);\n\n    nameNodeStatusBeanName = MBeans.register(\"SecondaryNameNode\",\n            \"SecondaryNameNodeInfo\", this);\n\n    infoServer = builder.build();\n\n    infoServer.setAttribute(\"secondary.name.node\", this);\n    infoServer.setAttribute(\"name.system.image\", checkpointImage);\n    infoServer.setAttribute(JspHelper.CURRENT_CONF, conf);\n    infoServer.addInternalServlet(\"imagetransfer\", ImageServlet.PATH_SPEC,\n        ImageServlet.class, true);\n    infoServer.start();\n\n    LOG.info(\"Web server init done\");\n\n    HttpConfig.Policy policy = DFSUtil.getHttpPolicy(conf);\n    int connIdx = 0;\n    if (policy.isHttpEnabled()) {\n      InetSocketAddress httpAddress = infoServer.getConnectorAddress(connIdx++);\n      conf.set(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,\n          NetUtils.getHostPortString(httpAddress));\n    }\n\n    if (policy.isHttpsEnabled()) {\n      InetSocketAddress httpsAddress = infoServer.getConnectorAddress(connIdx);\n      conf.set(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTPS_ADDRESS_KEY,\n          NetUtils.getHostPortString(httpsAddress));\n    }\n\n    LOG.info(\"Checkpoint Period   :\" + checkpointConf.getPeriod() + \" secs \"\n        + \"(\" + checkpointConf.getPeriod() / 60 + \" min)\");\n    LOG.info(\"Log Size Trigger    :\" + checkpointConf.getTxnCount() + \" txns\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.recoverCreate": "    void recoverCreate(boolean format) throws IOException {\n      storage.attemptRestoreRemovedStorage();\n      storage.unlockAll();\n\n      for (Iterator<StorageDirectory> it = \n                   storage.dirIterator(); it.hasNext();) {\n        StorageDirectory sd = it.next();\n        boolean isAccessible = true;\n        try { // create directories if don't exist yet\n          if(!sd.getRoot().mkdirs()) {\n            // do nothing, directory is already created\n          }\n        } catch(SecurityException se) {\n          isAccessible = false;\n        }\n        if(!isAccessible)\n          throw new InconsistentFSStateException(sd.getRoot(),\n              \"cannot access checkpoint directory.\");\n        \n        if (format) {\n          // Don't confirm, since this is just the secondary namenode.\n          LOG.info(\"Formatting storage directory \" + sd);\n          sd.clearDirectory();\n        }\n        \n        StorageState curState;\n        try {\n          curState = sd.analyzeStorage(HdfsServerConstants.StartupOption.REGULAR, storage);\n          // sd is locked but not opened\n          switch(curState) {\n          case NON_EXISTENT:\n            // fail if any of the configured checkpoint dirs are inaccessible \n            throw new InconsistentFSStateException(sd.getRoot(),\n                  \"checkpoint directory does not exist or is not accessible.\");\n          case NOT_FORMATTED:\n            break;  // it's ok since initially there is no current and VERSION\n          case NORMAL:\n            // Read the VERSION file. This verifies that:\n            // (a) the VERSION file for each of the directories is the same,\n            // and (b) when we connect to a NN, we can verify that the remote\n            // node matches the same namespace that we ran on previously.\n            storage.readProperties(sd);\n            break;\n          default:  // recovery is possible\n            sd.doRecover(curState);\n          }\n        } catch (IOException ioe) {\n          sd.unlock();\n          throw ioe;\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.deleteTempEdits": "    void deleteTempEdits() throws IOException {\n      FilenameFilter filter = new FilenameFilter() {\n        @Override\n        public boolean accept(File dir, String name) {\n          return name.matches(NameNodeFile.EDITS_TMP.getName()\n              + \"_(\\\\d+)-(\\\\d+)_(\\\\d+)\");\n        }\n      };\n      Iterator<StorageDirectory> it = storage.dirIterator(NameNodeDirType.EDITS);\n      for (;it.hasNext();) {\n        StorageDirectory dir = it.next();\n        File[] tempEdits = dir.getCurrentDir().listFiles(filter);\n        if (tempEdits != null) {\n          for (File t : tempEdits) {\n            boolean success = t.delete();\n            if (!success) {\n              LOG.warn(\"Failed to delete temporary edits file: \"\n                  + t.getAbsolutePath());\n            }\n          }\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.getInfoServer": "  private URL getInfoServer() throws IOException {\n    URI fsName = FileSystem.getDefaultUri(conf);\n    if (!HdfsConstants.HDFS_URI_SCHEME.equalsIgnoreCase(fsName.getScheme())) {\n      throw new IOException(\"This is not a DFS\");\n    }\n\n    final String scheme = DFSUtil.getHttpClientScheme(conf);\n    URI address = DFSUtil.getInfoServerWithDefaultHost(fsName.getHost(), conf,\n        scheme);\n    LOG.debug(\"Will connect to NameNode at \" + address);\n    return address.toURL();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldFormat": "    public boolean shouldFormat() {\n      return shouldFormat;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.getHttpAddress": "  public static InetSocketAddress getHttpAddress(Configuration conf) {\n    return NetUtils.createSocketAddr(conf.get(\n        DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,\n        DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_DEFAULT));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main": "  public static void main(String[] argv) throws Exception {\n    CommandLineOpts opts = SecondaryNameNode.parseArgs(argv);\n    if (opts == null) {\n      LOG.fatal(\"Failed to parse options\");\n      terminate(1);\n    } else if (opts.shouldPrintHelp()) {\n      opts.usage();\n      System.exit(0);\n    }\n    \n    StringUtils.startupShutdownMessage(SecondaryNameNode.class, argv, LOG);\n    Configuration tconf = new HdfsConfiguration();\n    SecondaryNameNode secondary = null;\n    try {\n      secondary = new SecondaryNameNode(tconf, opts);\n    } catch (IOException ioe) {\n      LOG.fatal(\"Failed to start secondary namenode\", ioe);\n      terminate(1);\n    }\n\n    if (opts != null && opts.getCommand() != null) {\n      int ret = secondary.processStartupCommand(opts);\n      terminate(ret);\n    }\n\n    if (secondary != null) {\n      secondary.startCheckpointThread();\n      secondary.join();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.usage": "    void usage() {\n      String header = \"The Secondary NameNode is a helper \"\n          + \"to the primary NameNode. The Secondary is responsible \"\n          + \"for supporting periodic checkpoints of the HDFS metadata. \"\n          + \"The current design allows only one Secondary NameNode \"\n          + \"per HDFS cluster.\";\n      HelpFormatter formatter = new HelpFormatter();\n      formatter.printHelp(\"secondarynamenode\", header, options, \"\", false);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.startCheckpointThread": "  public void startCheckpointThread() {\n    Preconditions.checkState(checkpointThread == null,\n        \"Should not already have a thread\");\n    Preconditions.checkState(shouldRun, \"shouldRun should be true\");\n    \n    checkpointThread = new Daemon(this);\n    checkpointThread.start();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.getCommand": "    public Command getCommand() {\n      return cmd;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.join": "  private void join() {\n    try {\n      infoServer.join();\n    } catch (InterruptedException ie) {\n      LOG.debug(\"Exception \", ie);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.parseArgs": "  private static CommandLineOpts parseArgs(String[] argv) {\n    CommandLineOpts opts = new CommandLineOpts();\n    try {\n      opts.parse(argv);\n    } catch (ParseException pe) {\n      LOG.error(pe.getMessage());\n      opts.usage();\n      return null;\n    }\n    return opts;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.processStartupCommand": "  private int processStartupCommand(CommandLineOpts opts) throws Exception {\n    if (opts.getCommand() == null) {\n      return 0;\n    }\n    \n    String cmd = opts.getCommand().toString().toLowerCase();\n    \n    int exitCode = 0;\n    try {\n      switch (opts.getCommand()) {\n      case CHECKPOINT:\n        long count = countUncheckpointedTxns();\n        if (count > checkpointConf.getTxnCount() ||\n            opts.shouldForceCheckpoint()) {\n          doCheckpoint();\n        } else {\n          System.err.println(\"EditLog size \" + count + \" transactions is \" +\n                             \"smaller than configured checkpoint \" +\n                             \"interval \" + checkpointConf.getTxnCount() + \" transactions.\");\n          System.err.println(\"Skipping checkpoint.\");\n        }\n        break;\n      case GETEDITSIZE:\n        long uncheckpointed = countUncheckpointedTxns();\n        System.out.println(\"NameNode has \" + uncheckpointed +\n            \" uncheckpointed transactions\");\n        break;\n      default:\n        throw new AssertionError(\"bad command enum: \" + opts.getCommand());\n      }\n      \n    } catch (RemoteException e) {\n      //\n      // This is a error returned by hadoop server. Print\n      // out the first line of the error mesage, ignore the stack trace.\n      exitCode = 1;\n      try {\n        String[] content;\n        content = e.getLocalizedMessage().split(\"\\n\");\n        LOG.error(cmd + \": \" + content[0]);\n      } catch (Exception ex) {\n        LOG.error(cmd + \": \" + ex.getLocalizedMessage());\n      }\n    } catch (IOException e) {\n      //\n      // IO exception encountered locally.\n      //\n      exitCode = 1;\n      LOG.error(cmd + \": \" + e.getLocalizedMessage());\n    } finally {\n      // Does the RPC connection need to be closed?\n    }\n    return exitCode;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldPrintHelp": "    public boolean shouldPrintHelp() {\n      return shouldPrintHelp;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.checkpointConf.getTxnCount": "  public long getTxnCount() {\n    return checkpointTxnCount;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getCheckpointEditsDirs": "  static List<URI> getCheckpointEditsDirs(Configuration conf,\n      String defaultName) {\n    Collection<String> dirNames = conf.getTrimmedStringCollection(\n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_EDITS_DIR_KEY);\n    if (dirNames.size() == 0 && defaultName != null) {\n      dirNames.add(defaultName);\n    }\n    return Util.stringCollectionAsURIs(dirNames);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress": "  public static InetSocketAddress getServiceAddress(Configuration conf,\n                                                        boolean fallback) {\n    String addr = conf.get(DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY);\n    if (addr == null || addr.isEmpty()) {\n      return fallback ? getAddress(conf) : null;\n    }\n    return getAddress(addr);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress": "  public static InetSocketAddress getAddress(URI filesystemURI) {\n    String authority = filesystemURI.getAuthority();\n    if (authority == null) {\n      throw new IllegalArgumentException(String.format(\n          \"Invalid URI for NameNode address (check %s): %s has no authority.\",\n          FileSystem.FS_DEFAULT_NAME_KEY, filesystemURI.toString()));\n    }\n    if (!HdfsConstants.HDFS_URI_SCHEME.equalsIgnoreCase(\n        filesystemURI.getScheme())) {\n      throw new IllegalArgumentException(String.format(\n          \"Invalid URI for NameNode address (check %s): %s is not of scheme '%s'.\",\n          FileSystem.FS_DEFAULT_NAME_KEY, filesystemURI.toString(),\n          HdfsConstants.HDFS_URI_SCHEME));\n    }\n    return getAddress(authority);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.checkpointConf.getPeriod": "  public long getPeriod() {\n    return checkpointPeriod;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getCheckpointDirs": "  static Collection<URI> getCheckpointDirs(Configuration conf,\n      String defaultValue) {\n    Collection<String> dirNames = conf.getTrimmedStringCollection(\n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_DIR_KEY);\n    if (dirNames.size() == 0 && defaultValue != null) {\n      dirNames.add(defaultValue);\n    }\n    return Util.stringCollectionAsURIs(dirNames);\n  }"
        },
        "bug_report": {
            "Title": "SecondaryNameNode not terminating properly on runtime exceptions",
            "Description": "Secondary Namenode is not exiting when there is RuntimeException occurred during startup.\n\nSay I configured wrong configuration, due to that validation failed and thrown RuntimeException as shown below. But when I check the environment SecondaryNamenode process is alive. When analysed, RMI Thread is still alive, since it is not a daemon thread JVM is nit exiting. \n\nI'm attaching threaddump to this JIRA for more details about the thread.\n{code}\njava.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1900)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.getInstance(BlockPlacementPolicy.java:199)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:256)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:635)\n\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:260)\n\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init>(SecondaryNameNode.java:205)\n\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:695)\nCaused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1868)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1892)\n\t... 6 more\nCaused by: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1774)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1866)\n\t... 7 more\n2014-05-07 14:27:04,666 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for active state\n2014-05-07 14:27:04,666 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state\n2014-05-07 14:31:04,926 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: \n{code}\n\n"
        }
    },
    {
        "filename": "HDFS-7884.json",
        "creation_time": "2015-03-04T15:47:50.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:264)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:506)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:249)\n\tat java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock": "  public void readBlock(final ExtendedBlock block,\n      final Token<BlockTokenIdentifier> blockToken,\n      final String clientName,\n      final long blockOffset,\n      final long length,\n      final boolean sendChecksum,\n      final CachingStrategy cachingStrategy) throws IOException {\n    previousOpClientName = clientName;\n    long read = 0;\n    OutputStream baseStream = getOutputStream();\n    DataOutputStream out = new DataOutputStream(new BufferedOutputStream(\n        baseStream, HdfsConstants.SMALL_BUFFER_SIZE));\n    checkAccess(out, true, block, blockToken,\n        Op.READ_BLOCK, BlockTokenSecretManager.AccessMode.READ);\n  \n    // send the block\n    BlockSender blockSender = null;\n    DatanodeRegistration dnR = \n      datanode.getDNRegistrationForBP(block.getBlockPoolId());\n    final String clientTraceFmt =\n      clientName.length() > 0 && ClientTraceLog.isInfoEnabled()\n        ? String.format(DN_CLIENTTRACE_FORMAT, localAddress, remoteAddress,\n            \"%d\", \"HDFS_READ\", clientName, \"%d\",\n            dnR.getDatanodeUuid(), block, \"%d\")\n        : dnR + \" Served block \" + block + \" to \" +\n            remoteAddress;\n\n    updateCurrentThreadName(\"Sending block \" + block);\n    try {\n      try {\n        blockSender = new BlockSender(block, blockOffset, length,\n            true, false, sendChecksum, datanode, clientTraceFmt,\n            cachingStrategy);\n      } catch(IOException e) {\n        String msg = \"opReadBlock \" + block + \" received exception \" + e; \n        LOG.info(msg);\n        sendResponse(ERROR, msg);\n        throw e;\n      }\n      \n      // send op status\n      writeSuccessWithChecksumInfo(blockSender, new DataOutputStream(getOutputStream()));\n\n      long beginRead = Time.monotonicNow();\n      read = blockSender.sendBlock(out, baseStream, null); // send data\n      long duration = Time.monotonicNow() - beginRead;\n      if (blockSender.didSendEntireByteRange()) {\n        // If we sent the entire range, then we should expect the client\n        // to respond with a Status enum.\n        try {\n          ClientReadStatusProto stat = ClientReadStatusProto.parseFrom(\n              PBHelper.vintPrefixed(in));\n          if (!stat.hasStatus()) {\n            LOG.warn(\"Client \" + peer.getRemoteAddressString() +\n                \" did not send a valid status code after reading. \" +\n                \"Will close connection.\");\n            IOUtils.closeStream(out);\n          }\n        } catch (IOException ioe) {\n          LOG.debug(\"Error reading client status response. Will close connection.\", ioe);\n          IOUtils.closeStream(out);\n          incrDatanodeNetworkErrors();\n        }\n      } else {\n        IOUtils.closeStream(out);\n      }\n      datanode.metrics.incrBytesRead((int) read);\n      datanode.metrics.incrBlocksRead();\n      datanode.metrics.incrTotalReadTime(duration);\n    } catch ( SocketException ignored ) {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(dnR + \":Ignoring exception while serving \" + block + \" to \" +\n            remoteAddress, ignored);\n      }\n      // Its ok for remote side to close the connection anytime.\n      datanode.metrics.incrBlocksRead();\n      IOUtils.closeStream(out);\n    } catch ( IOException ioe ) {\n      /* What exactly should we do here?\n       * Earlier version shutdown() datanode if there is disk error.\n       */\n      if (!(ioe instanceof SocketTimeoutException)) {\n        LOG.warn(dnR + \":Got exception while serving \" + block + \" to \"\n          + remoteAddress, ioe);\n        incrDatanodeNetworkErrors();\n      }\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(blockSender);\n    }\n\n    //update metrics\n    datanode.metrics.addReadBlockOp(elapsed());\n    datanode.metrics.incrReadsFromClient(peer.isLocal(), read);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeSuccessWithChecksumInfo": "  private void writeSuccessWithChecksumInfo(BlockSender blockSender,\n      DataOutputStream out) throws IOException {\n\n    ReadOpChecksumInfoProto ckInfo = ReadOpChecksumInfoProto.newBuilder()\n      .setChecksum(DataTransferProtoUtil.toProto(blockSender.getChecksum()))\n      .setChunkOffset(blockSender.getOffset())\n      .build();\n      \n    BlockOpResponseProto response = BlockOpResponseProto.newBuilder()\n      .setStatus(SUCCESS)\n      .setReadOpChecksumInfo(ckInfo)\n      .build();\n    response.writeDelimitedTo(out);\n    out.flush();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.incrDatanodeNetworkErrors": "  private void incrDatanodeNetworkErrors() {\n    datanode.incrDatanodeNetworkErrors(remoteAddressWithoutPort);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.elapsed": "  private long elapsed() {\n    return now() - opStartTime;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.getOutputStream": "  private OutputStream getOutputStream() {\n    return socketOut;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess": "  private void checkAccess(OutputStream out, final boolean reply, \n      final ExtendedBlock blk,\n      final Token<BlockTokenIdentifier> t,\n      final Op op,\n      final BlockTokenSecretManager.AccessMode mode) throws IOException {\n    if (datanode.isBlockTokenEnabled) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Checking block access token for block '\" + blk.getBlockId()\n            + \"' with mode '\" + mode + \"'\");\n      }\n      try {\n        datanode.blockPoolTokenSecretManager.checkAccess(t, null, blk, mode);\n      } catch(InvalidToken e) {\n        try {\n          if (reply) {\n            BlockOpResponseProto.Builder resp = BlockOpResponseProto.newBuilder()\n              .setStatus(ERROR_ACCESS_TOKEN);\n            if (mode == BlockTokenSecretManager.AccessMode.WRITE) {\n              DatanodeRegistration dnR = \n                datanode.getDNRegistrationForBP(blk.getBlockPoolId());\n              // NB: Unconditionally using the xfer addr w/o hostname\n              resp.setFirstBadLink(dnR.getXferAddr());\n            }\n            resp.build().writeDelimitedTo(out);\n            out.flush();\n          }\n          LOG.warn(\"Block token verification failed: op=\" + op\n              + \", remoteAddress=\" + remoteAddress\n              + \", message=\" + e.getLocalizedMessage());\n          throw e;\n        } finally {\n          IOUtils.closeStream(out);\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.sendResponse": "  private void sendResponse(Status status,\n      String message) throws IOException {\n    writeResponse(status, message, getOutputStream());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.updateCurrentThreadName": "  private void updateCurrentThreadName(String status) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"DataXceiver for client \");\n    if (previousOpClientName != null) {\n      sb.append(previousOpClientName).append(\" at \");\n    }\n    sb.append(remoteAddress);\n    if (status != null) {\n      sb.append(\" [\").append(status).append(\"]\");\n    }\n    Thread.currentThread().setName(sb.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock": "  private void opReadBlock() throws IOException {\n    OpReadBlockProto proto = OpReadBlockProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      readBlock(PBHelper.convert(proto.getHeader().getBaseHeader().getBlock()),\n        PBHelper.convert(proto.getHeader().getBaseHeader().getToken()),\n        proto.getHeader().getClientName(),\n        proto.getOffset(),\n        proto.getLen(),\n        proto.getSendChecksums(),\n        (proto.hasCachingStrategy() ?\n            getCachingStrategy(proto.getCachingStrategy()) :\n          CachingStrategy.newDefaultStrategy()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.getCachingStrategy": "  static private CachingStrategy getCachingStrategy(CachingStrategyProto strategy) {\n    Boolean dropBehind = strategy.hasDropBehind() ?\n        strategy.getDropBehind() : null;\n    Long readahead = strategy.hasReadahead() ?\n        strategy.getReadahead() : null;\n    return new CachingStrategy(dropBehind, readahead);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp": "  protected final void processOp(Op op) throws IOException {\n    switch(op) {\n    case READ_BLOCK:\n      opReadBlock();\n      break;\n    case WRITE_BLOCK:\n      opWriteBlock(in);\n      break;\n    case REPLACE_BLOCK:\n      opReplaceBlock(in);\n      break;\n    case COPY_BLOCK:\n      opCopyBlock(in);\n      break;\n    case BLOCK_CHECKSUM:\n      opBlockChecksum(in);\n      break;\n    case TRANSFER_BLOCK:\n      opTransferBlock(in);\n      break;\n    case REQUEST_SHORT_CIRCUIT_FDS:\n      opRequestShortCircuitFds(in);\n      break;\n    case RELEASE_SHORT_CIRCUIT_FDS:\n      opReleaseShortCircuitFds(in);\n      break;\n    case REQUEST_SHORT_CIRCUIT_SHM:\n      opRequestShortCircuitShm(in);\n      break;\n    default:\n      throw new IOException(\"Unknown op \" + op + \" in data stream\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReplaceBlock": "  private void opReplaceBlock(DataInputStream in) throws IOException {\n    OpReplaceBlockProto proto = OpReplaceBlockProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      replaceBlock(PBHelper.convert(proto.getHeader().getBlock()),\n          PBHelper.convertStorageType(proto.getStorageType()),\n          PBHelper.convert(proto.getHeader().getToken()),\n          proto.getDelHint(),\n          PBHelper.convert(proto.getSource()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitFds": "  private void opRequestShortCircuitFds(DataInputStream in) throws IOException {\n    final OpRequestShortCircuitAccessProto proto =\n      OpRequestShortCircuitAccessProto.parseFrom(vintPrefixed(in));\n    SlotId slotId = (proto.hasSlotId()) ? \n        PBHelper.convert(proto.getSlotId()) : null;\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      requestShortCircuitFds(PBHelper.convert(proto.getHeader().getBlock()),\n          PBHelper.convert(proto.getHeader().getToken()),\n          slotId, proto.getMaxVersion());\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitShm": "  private void opRequestShortCircuitShm(DataInputStream in) throws IOException {\n    final ShortCircuitShmRequestProto proto =\n        ShortCircuitShmRequestProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getTraceInfo(),\n        proto.getClass().getSimpleName());\n    try {\n      requestShortCircuitShm(proto.getClientName());\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opBlockChecksum": "  private void opBlockChecksum(DataInputStream in) throws IOException {\n    OpBlockChecksumProto proto = OpBlockChecksumProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n    blockChecksum(PBHelper.convert(proto.getHeader().getBlock()),\n        PBHelper.convert(proto.getHeader().getToken()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReleaseShortCircuitFds": "  private void opReleaseShortCircuitFds(DataInputStream in)\n      throws IOException {\n    final ReleaseShortCircuitAccessRequestProto proto =\n      ReleaseShortCircuitAccessRequestProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getTraceInfo(),\n        proto.getClass().getSimpleName());\n    try {\n      releaseShortCircuitFds(PBHelper.convert(proto.getSlotId()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opTransferBlock": "  private void opTransferBlock(DataInputStream in) throws IOException {\n    final OpTransferBlockProto proto =\n      OpTransferBlockProto.parseFrom(vintPrefixed(in));\n    final DatanodeInfo[] targets = PBHelper.convert(proto.getTargetsList());\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      transferBlock(PBHelper.convert(proto.getHeader().getBaseHeader().getBlock()),\n          PBHelper.convert(proto.getHeader().getBaseHeader().getToken()),\n          proto.getHeader().getClientName(),\n          targets,\n          PBHelper.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opCopyBlock": "  private void opCopyBlock(DataInputStream in) throws IOException {\n    OpCopyBlockProto proto = OpCopyBlockProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      copyBlock(PBHelper.convert(proto.getHeader().getBlock()),\n          PBHelper.convert(proto.getHeader().getToken()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock": "  private void opWriteBlock(DataInputStream in) throws IOException {\n    final OpWriteBlockProto proto = OpWriteBlockProto.parseFrom(vintPrefixed(in));\n    final DatanodeInfo[] targets = PBHelper.convert(proto.getTargetsList());\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      writeBlock(PBHelper.convert(proto.getHeader().getBaseHeader().getBlock()),\n          PBHelper.convertStorageType(proto.getStorageType()),\n          PBHelper.convert(proto.getHeader().getBaseHeader().getToken()),\n          proto.getHeader().getClientName(),\n          targets,\n          PBHelper.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length),\n          PBHelper.convert(proto.getSource()),\n          fromProto(proto.getStage()),\n          proto.getPipelineSize(),\n          proto.getMinBytesRcvd(), proto.getMaxBytesRcvd(),\n          proto.getLatestGenerationStamp(),\n          fromProto(proto.getRequestedChecksum()),\n          (proto.hasCachingStrategy() ?\n              getCachingStrategy(proto.getCachingStrategy()) :\n            CachingStrategy.newDefaultStrategy()),\n          (proto.hasAllowLazyPersist() ? proto.getAllowLazyPersist() : false),\n          (proto.hasPinning() ? proto.getPinning(): false),\n          (PBHelper.convertBooleanList(proto.getTargetPinningsList())));\n    } finally {\n     if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.run": "  public void run() {\n    int opsProcessed = 0;\n    Op op = null;\n\n    try {\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input = socketIn;\n      try {\n        IOStreamPair saslStreams = datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input = new BufferedInputStream(saslStreams.in,\n          HdfsConstants.SMALL_BUFFER_SIZE);\n        socketOut = saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at \" + peer.getRemoteAddressString() + \n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\");\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed != 0) {\n            assert dnConf.socketKeepaliveTimeout > 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op = readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it's quite normal to get EOF here.\n          if (opsProcessed > 0 &&\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            incrDatanodeNetworkErrors();\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed != 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime = now();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer != null) &&\n          (!peer.isClosed() && dnConf.socketKeepaliveTimeout > 0));\n    } catch (Throwable t) {\n      String s = datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op == null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op == Op.WRITE_BLOCK && t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op == Op.READ_BLOCK && t instanceof SocketTimeoutException) {\n        String s1 =\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 += \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer != null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    TraceScope scope =\n        Trace.startSpan(\"sendBlock_\" + block.getBlockId(), Sampler.NEVER);\n    try {\n      return doSendBlock(out, baseStream, throttler);\n    } finally {\n      scope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.close": "  public void close() throws IOException {\n    if (blockInFd != null &&\n        ((dropCacheBehindAllReads) ||\n         (dropCacheBehindLargeReads && isLongRead()))) {\n      try {\n        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n            block.getBlockName(), blockInFd, lastCacheDropOffset,\n            offset - lastCacheDropOffset,\n            NativeIO.POSIX.POSIX_FADV_DONTNEED);\n      } catch (Exception e) {\n        LOG.warn(\"Unable to drop cache on file close\", e);\n      }\n    }\n    if (curReadahead != null) {\n      curReadahead.cancel();\n    }\n    \n    IOException ioe = null;\n    if(checksumIn!=null) {\n      try {\n        checksumIn.close(); // close checksum file\n      } catch (IOException e) {\n        ioe = e;\n      }\n      checksumIn = null;\n    }   \n    if(blockIn!=null) {\n      try {\n        blockIn.close(); // close data file\n      } catch (IOException e) {\n        ioe = e;\n      }\n      blockIn = null;\n      blockInFd = null;\n    }\n    if (volumeRef != null) {\n      IOUtils.cleanup(null, volumeRef);\n      volumeRef = null;\n    }\n    // throw IOException if there is any\n    if(ioe!= null) {\n      throw ioe;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock": "  private long doSendBlock(DataOutputStream out, OutputStream baseStream,\n        DataTransferThrottler throttler) throws IOException {\n    if (out == null) {\n      throw new IOException( \"out stream is null\" );\n    }\n    initialOffset = offset;\n    long totalRead = 0;\n    OutputStream streamForSendChunks = out;\n    \n    lastCacheDropOffset = initialOffset;\n\n    if (isLongRead() && blockInFd != null) {\n      // Advise that this file descriptor will be accessed sequentially.\n      NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n          block.getBlockName(), blockInFd, 0, 0,\n          NativeIO.POSIX.POSIX_FADV_SEQUENTIAL);\n    }\n    \n    // Trigger readahead of beginning of file if configured.\n    manageOsCache();\n\n    final long startTime = ClientTraceLog.isDebugEnabled() ? System.nanoTime() : 0;\n    try {\n      int maxChunksPerPacket;\n      int pktBufSize = PacketHeader.PKT_MAX_HEADER_LEN;\n      boolean transferTo = transferToAllowed && !verifyChecksum\n          && baseStream instanceof SocketOutputStream\n          && blockIn instanceof FileInputStream;\n      if (transferTo) {\n        FileChannel fileChannel = ((FileInputStream)blockIn).getChannel();\n        blockInPosition = fileChannel.position();\n        streamForSendChunks = baseStream;\n        maxChunksPerPacket = numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n        \n        // Smaller packet size to only hold checksum when doing transferTo\n        pktBufSize += checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket = Math.max(1,\n            numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n        // Packet size includes both checksum and data\n        pktBufSize += (chunkSize + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf = ByteBuffer.allocate(pktBufSize);\n\n      while (endOffset > offset && !Thread.currentThread().isInterrupted()) {\n        manageOsCache();\n        long len = sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n            transferTo, throttler);\n        offset += len;\n        totalRead += len + (numberOfChunks(len) * checksumSize);\n        seqno++;\n      }\n      // If this thread was interrupted, then it did not send the full block.\n      if (!Thread.currentThread().isInterrupted()) {\n        try {\n          // send an empty packet to mark the end of the block\n          sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n              throttler);\n          out.flush();\n        } catch (IOException e) { //socket error\n          throw ioeToSocketException(e);\n        }\n\n        sentEntireByteRange = true;\n      }\n    } finally {\n      if ((clientTraceFmt != null) && ClientTraceLog.isDebugEnabled()) {\n        final long endTime = System.nanoTime();\n        ClientTraceLog.debug(String.format(clientTraceFmt, totalRead,\n            initialOffset, endTime - startTime));\n      }\n      close();\n    }\n    return totalRead;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDNRegistrationForBP": "  public DatanodeRegistration getDNRegistrationForBP(String bpid) \n  throws IOException {\n    BPOfferService bpos = blockPoolManager.get(bpid);\n    if(bpos==null || bpos.bpRegistration==null) {\n      throw new IOException(\"cannot find BPOfferService for bpid=\"+bpid);\n    }\n    return bpos.bpRegistration;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.didSendEntireByteRange": "  boolean didSendEntireByteRange() {\n    return sentEntireByteRange;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXceiverCount": "  public int getXceiverCount() {\n    return threadGroup == null ? 0 : threadGroup.activeCount();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDisplayName": "  public String getDisplayName() {\n    // NB: our DatanodeID may not be set yet\n    return hostName + \":\" + getXferPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXferPort": "  public int getXferPort() {\n    return streamingAddr.getPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dataXceiverServer.closePeer": "  synchronized void closePeer(Peer peer) {\n    peers.remove(peer);\n    peersXceiver.remove(peer);\n    IOUtils.cleanup(null, peer);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dataXceiverServer.addPeer": "  synchronized void addPeer(Peer peer, Thread t, DataXceiver xceiver)\n      throws IOException {\n    if (closed) {\n      throw new IOException(\"Server closed.\");\n    }\n    peers.put(peer, t);\n    peersXceiver.put(peer, xceiver);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDatanodeId": "  public DatanodeID getDatanodeId() {\n    return id;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXferAddress": "  public InetSocketAddress getXferAddress() {\n    return streamingAddr;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDnConf": "  public DNConf getDnConf() {\n    return dnConf;\n  }"
        },
        "bug_report": {
            "Title": "NullPointerException in BlockSender",
            "Description": "{noformat}\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:264)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:506)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:249)\n\tat java.lang.Thread.run(Thread.java:745)\n{noformat}\nBlockSender.java:264 is shown below\n{code}\n      this.volumeRef = datanode.data.getVolume(block).obtainReference();\n{code}"
        }
    },
    {
        "filename": "HDFS-7996.json",
        "creation_time": "2015-03-26T21:19:55.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Cannot append to a non-existent replica BP-51301509-10.20.202.114-1427296597742:blk_1073742979_2160\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo(FsDatasetImpl.java:615)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.finalizeBlock(FsDatasetImpl.java:1362)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.finalizeBlock(BlockReceiver.java:1281)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1241)\n        at java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo": "  private ReplicaInfo getReplicaInfo(String bpid, long blkid)\n      throws ReplicaNotFoundException {\n    ReplicaInfo info = volumeMap.get(bpid, blkid);\n    if (info == null) {\n      throw new ReplicaNotFoundException(\n          ReplicaNotFoundException.NON_EXISTENT_REPLICA + bpid + \":\" + blkid);\n    }\n    return info;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.finalizeBlock": "  public synchronized void finalizeBlock(ExtendedBlock b) throws IOException {\n    if (Thread.interrupted()) {\n      // Don't allow data modifications from interrupted threads\n      throw new IOException(\"Cannot finalize block from Interrupted Thread\");\n    }\n    ReplicaInfo replicaInfo = getReplicaInfo(b);\n    if (replicaInfo.getState() == ReplicaState.FINALIZED) {\n      // this is legal, when recovery happens on a file that has\n      // been opened for append but never modified\n      return;\n    }\n    finalizeReplica(b.getBlockPoolId(), replicaInfo);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.finalizeReplica": "  private synchronized FinalizedReplica finalizeReplica(String bpid,\n      ReplicaInfo replicaInfo) throws IOException {\n    FinalizedReplica newReplicaInfo = null;\n    if (replicaInfo.getState() == ReplicaState.RUR &&\n       ((ReplicaUnderRecovery)replicaInfo).getOriginalReplica().getState() == \n         ReplicaState.FINALIZED) {\n      newReplicaInfo = (FinalizedReplica)\n             ((ReplicaUnderRecovery)replicaInfo).getOriginalReplica();\n    } else {\n      FsVolumeImpl v = (FsVolumeImpl)replicaInfo.getVolume();\n      File f = replicaInfo.getBlockFile();\n      if (v == null) {\n        throw new IOException(\"No volume for temporary file \" + f + \n            \" for block \" + replicaInfo);\n      }\n\n      File dest = v.addFinalizedBlock(\n          bpid, replicaInfo, f, replicaInfo.getBytesReserved());\n      newReplicaInfo = new FinalizedReplica(replicaInfo, v, dest.getParentFile());\n\n      if (v.isTransientStorage()) {\n        ramDiskReplicaTracker.addReplica(bpid, replicaInfo.getBlockId(), v);\n        datanode.getMetrics().addRamDiskBytesWrite(replicaInfo.getNumBytes());\n      }\n    }\n    volumeMap.add(bpid, newReplicaInfo);\n\n    return newReplicaInfo;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.finalizeBlock": "    private void finalizeBlock(long startTime) throws IOException {\n      BlockReceiver.this.close();\n      final long endTime = ClientTraceLog.isInfoEnabled() ? System.nanoTime()\n          : 0;\n      block.setNumBytes(replicaInfo.getNumBytes());\n      datanode.data.finalizeBlock(block);\n      \n      if (pinning) {\n        datanode.data.setPinning(block);\n      }\n      \n      datanode.closeBlock(\n          block, DataNode.EMPTY_DEL_HINT, replicaInfo.getStorageUuid());\n      if (ClientTraceLog.isInfoEnabled() && isClient) {\n        long offset = 0;\n        DatanodeRegistration dnR = datanode.getDNRegistrationForBP(block\n            .getBlockPoolId());\n        ClientTraceLog.info(String.format(DN_CLIENTTRACE_FORMAT, inAddr,\n            myAddr, block.getNumBytes(), \"HDFS_WRITE\", clientname, offset,\n            dnR.getDatanodeUuid(), block, endTime - startTime));\n      } else {\n        LOG.info(\"Received \" + block + \" size \" + block.getNumBytes()\n            + \" from \" + inAddr);\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.getStorageUuid": "  String getStorageUuid() {\n    return replicaInfo.getStorageUuid();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.close": "    public void close() {\n      synchronized(ackQueue) {\n        while (isRunning() && ackQueue.size() != 0) {\n          try {\n            ackQueue.wait();\n          } catch (InterruptedException e) {\n            running = false;\n            Thread.currentThread().interrupt();\n          }\n        }\n        if(LOG.isDebugEnabled()) {\n          LOG.debug(myString + \": closing\");\n        }\n        running = false;\n        ackQueue.notifyAll();\n      }\n\n      synchronized(this) {\n        running = false;\n        notifyAll();\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.run": "    public void run() {\n      boolean lastPacketInBlock = false;\n      final long startTime = ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n      while (isRunning() && !lastPacketInBlock) {\n        long totalAckTimeNanos = 0;\n        boolean isInterrupted = false;\n        try {\n          Packet pkt = null;\n          long expected = -2;\n          PipelineAck ack = new PipelineAck();\n          long seqno = PipelineAck.UNKOWN_SEQNO;\n          long ackRecvNanoTime = 0;\n          try {\n            if (type != PacketResponderType.LAST_IN_PIPELINE && !mirrorError) {\n              // read an ack from downstream datanode\n              ack.readFields(downstreamIn);\n              ackRecvNanoTime = System.nanoTime();\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(myString + \" got \" + ack);\n              }\n              // Process an OOB ACK.\n              Status oobStatus = ack.getOOBStatus();\n              if (oobStatus != null) {\n                LOG.info(\"Relaying an out of band ack of type \" + oobStatus);\n                sendAckUpstream(ack, PipelineAck.UNKOWN_SEQNO, 0L, 0L,\n                    PipelineAck.combineHeader(datanode.getECN(),\n                      Status.SUCCESS));\n                continue;\n              }\n              seqno = ack.getSeqno();\n            }\n            if (seqno != PipelineAck.UNKOWN_SEQNO\n                || type == PacketResponderType.LAST_IN_PIPELINE) {\n              pkt = waitForAckHead(seqno);\n              if (!isRunning()) {\n                break;\n              }\n              expected = pkt.seqno;\n              if (type == PacketResponderType.HAS_DOWNSTREAM_IN_PIPELINE\n                  && seqno != expected) {\n                throw new IOException(myString + \"seqno: expected=\" + expected\n                    + \", received=\" + seqno);\n              }\n              if (type == PacketResponderType.HAS_DOWNSTREAM_IN_PIPELINE) {\n                // The total ack time includes the ack times of downstream\n                // nodes.\n                // The value is 0 if this responder doesn't have a downstream\n                // DN in the pipeline.\n                totalAckTimeNanos = ackRecvNanoTime - pkt.ackEnqueueNanoTime;\n                // Report the elapsed time from ack send to ack receive minus\n                // the downstream ack time.\n                long ackTimeNanos = totalAckTimeNanos\n                    - ack.getDownstreamAckTimeNanos();\n                if (ackTimeNanos < 0) {\n                  if (LOG.isDebugEnabled()) {\n                    LOG.debug(\"Calculated invalid ack time: \" + ackTimeNanos\n                        + \"ns.\");\n                  }\n                } else {\n                  datanode.metrics.addPacketAckRoundTripTimeNanos(ackTimeNanos);\n                }\n              }\n              lastPacketInBlock = pkt.lastPacketInBlock;\n            }\n          } catch (InterruptedException ine) {\n            isInterrupted = true;\n          } catch (IOException ioe) {\n            if (Thread.interrupted()) {\n              isInterrupted = true;\n            } else {\n              // continue to run even if can not read from mirror\n              // notify client of the error\n              // and wait for the client to shut down the pipeline\n              mirrorError = true;\n              LOG.info(myString, ioe);\n            }\n          }\n\n          if (Thread.interrupted() || isInterrupted) {\n            /*\n             * The receiver thread cancelled this thread. We could also check\n             * any other status updates from the receiver thread (e.g. if it is\n             * ok to write to replyOut). It is prudent to not send any more\n             * status back to the client because this datanode has a problem.\n             * The upstream datanode will detect that this datanode is bad, and\n             * rightly so.\n             *\n             * The receiver thread can also interrupt this thread for sending\n             * an out-of-band response upstream.\n             */\n            LOG.info(myString + \": Thread is interrupted.\");\n            running = false;\n            continue;\n          }\n\n          if (lastPacketInBlock) {\n            // Finalize the block and close the block file\n            finalizeBlock(startTime);\n          }\n\n          Status myStatus = pkt != null ? pkt.ackStatus : Status.SUCCESS;\n          sendAckUpstream(ack, expected, totalAckTimeNanos,\n            (pkt != null ? pkt.offsetInBlock : 0),\n            PipelineAck.combineHeader(datanode.getECN(), myStatus));\n          if (pkt != null) {\n            // remove the packet from the ack queue\n            removeAckHead();\n          }\n        } catch (IOException e) {\n          LOG.warn(\"IOException in BlockReceiver.run(): \", e);\n          if (running) {\n            datanode.checkDiskErrorAsync();\n            LOG.info(myString, e);\n            running = false;\n            if (!Thread.interrupted()) { // failure not caused by interruption\n              receiverThread.interrupt();\n            }\n          }\n        } catch (Throwable e) {\n          if (running) {\n            LOG.info(myString, e);\n            running = false;\n            receiverThread.interrupt();\n          }\n        }\n      }\n      LOG.info(myString + \" terminating\");\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.isRunning": "    private boolean isRunning() {\n      // When preparing for a restart, it should continue to run until\n      // interrupted by the receiver thread.\n      return running && (datanode.shouldRun || datanode.isRestarting());\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.removeAckHead": "    private void removeAckHead() {\n      synchronized(ackQueue) {\n        ackQueue.removeFirst();\n        ackQueue.notifyAll();\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.sendAckUpstream": "    private void sendAckUpstream(PipelineAck ack, long seqno,\n        long totalAckTimeNanos, long offsetInBlock,\n        int myHeader) throws IOException {\n      try {\n        // Wait for other sender to finish. Unless there is an OOB being sent,\n        // the responder won't have to wait.\n        synchronized(this) {\n          while(sending) {\n            wait();\n          }\n          sending = true;\n        }\n\n        try {\n          if (!running) return;\n          sendAckUpstreamUnprotected(ack, seqno, totalAckTimeNanos,\n              offsetInBlock, myHeader);\n        } finally {\n          synchronized(this) {\n            sending = false;\n            notify();\n          }\n        }\n      } catch (InterruptedException ie) {\n        // The responder was interrupted. Make it go down without\n        // interrupting the receiver(writer) thread.  \n        running = false;\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.waitForAckHead": "    Packet waitForAckHead(long seqno) throws InterruptedException {\n      synchronized(ackQueue) {\n        while (isRunning() && ackQueue.size() == 0) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(myString + \": seqno=\" + seqno +\n                      \" waiting for local datanode to finish write.\");\n          }\n          ackQueue.wait();\n        }\n        return isRunning() ? ackQueue.getFirst() : null;\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.closeBlock": "  void closeBlock(ExtendedBlock block, String delHint, String storageUuid) {\n    metrics.incrBlocksWritten();\n    BPOfferService bpos = blockPoolManager.get(block.getBlockPoolId());\n    if(bpos != null) {\n      bpos.notifyNamenodeReceivedBlock(block, delHint, storageUuid);\n    } else {\n      LOG.warn(\"Cannot find BPOfferService for reporting block received for bpid=\"\n          + block.getBlockPoolId());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.notifyNamenodeReceivedBlock": "  public void notifyNamenodeReceivedBlock(\n      ExtendedBlock block, String delHint, String storageUuid) {\n    BPOfferService bpos = blockPoolManager.get(block.getBlockPoolId());\n    if(bpos != null) {\n      bpos.notifyNamenodeReceivedBlock(block, delHint, storageUuid);\n    } else {\n      LOG.error(\"Cannot find BPOfferService for reporting block received for bpid=\"\n          + block.getBlockPoolId());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDNRegistrationForBP": "  public DatanodeRegistration getDNRegistrationForBP(String bpid) \n  throws IOException {\n    BPOfferService bpos = blockPoolManager.get(bpid);\n    if(bpos==null || bpos.bpRegistration==null) {\n      throw new IOException(\"cannot find BPOfferService for bpid=\"+bpid);\n    }\n    return bpos.bpRegistration;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getECN": "  public PipelineAck.ECN getECN() {\n    return pipelineSupportECN ? PipelineAck.ECN.SUPPORTED : PipelineAck.ECN\n      .DISABLED;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.checkDiskErrorAsync": "  public void checkDiskErrorAsync() {\n    synchronized(checkDiskErrorMutex) {\n      checkDiskErrorFlag = true;\n      if(checkDiskErrorThread == null) {\n        startCheckDiskErrorThread();\n        checkDiskErrorThread.start();\n        LOG.info(\"Starting CheckDiskError Thread\");\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.startCheckDiskErrorThread": "  private void startCheckDiskErrorThread() {\n    checkDiskErrorThread = new Thread(new Runnable() {\n          @Override\n          public void run() {\n            while(shouldRun) {\n              boolean tempFlag ;\n              synchronized(checkDiskErrorMutex) {\n                tempFlag = checkDiskErrorFlag;\n                checkDiskErrorFlag = false;\n              }\n              if(tempFlag) {\n                try {\n                  checkDiskError();\n                } catch (Exception e) {\n                  LOG.warn(\"Unexpected exception occurred while checking disk error  \" + e);\n                  checkDiskErrorThread = null;\n                  return;\n                }\n                synchronized(checkDiskErrorMutex) {\n                  lastDiskErrorCheck = Time.monotonicNow();\n                }\n              }\n              try {\n                Thread.sleep(checkDiskErrorInterval);\n              } catch (InterruptedException e) {\n                LOG.debug(\"InterruptedException in check disk error thread\", e);\n                checkDiskErrorThread = null;\n                return;\n              }\n            }\n          }\n    });\n  }"
        },
        "bug_report": {
            "Title": "After swapping a volume, BlockReceiver reports ReplicaNotFoundException",
            "Description": "When removing a disk from an actively writing DataNode, the BlockReceiver working on the disk throws {{ReplicaNotFoundException}} because the replicas are removed from the memory:\n\n{code}\n2015-03-26 08:02:43,154 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Removed volume: /data/2/dfs/dn/current\n2015-03-26 08:02:43,163 INFO org.apache.hadoop.hdfs.server.common.Storage: Removing block level storage: /data/2/dfs/dn/current/BP-51301509-10.20.202.114-1427296597742\n2015-03-26 08:02:43,163 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in BlockReceiver.run():\norg.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Cannot append to a non-existent replica BP-51301509-10.20.202.114-1427296597742:blk_1073742979_2160\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo(FsDatasetImpl.java:615)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.finalizeBlock(FsDatasetImpl.java:1362)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.finalizeBlock(BlockReceiver.java:1281)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1241)\n        at java.lang.Thread.run(Thread.java:745)\n{code}\n\n{{FsVolumeList#removeVolume}} waits all threads release {{FsVolumeReference}} on the volume to be removed, however, in {{PacketResponder#finalizeBlock()}}, it calls\n\n{code}\nprivate void finalizeBlock(long startTime) throws IOException {\n      BlockReceiver.this.close();\n      final long endTime = ClientTraceLog.isInfoEnabled() ? System.nanoTime()\n          : 0;\n      block.setNumBytes(replicaInfo.getNumBytes());\n      datanode.data.finalizeBlock(block);\n{code}\n\nThe {{FsVolumeReference}} was released in {{BlockReceiver.this.close()}} before calling {{datanode.data.finalizeBlock(block)}}."
        }
    },
    {
        "filename": "HDFS-4302.json",
        "creation_time": "2012-12-11T19:44:59.000+0000",
        "stack_trace": "```\nFATAL namenode.NameNode (NameNode.java:main(1224)) - Exception in namenode join\njava.lang.IllegalStateException: must get input stream before length is available\n        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)\n        at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$URLLog.length(EditLogFileInputStream.java:405)\n        at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.length(EditLogFileInputStream.java:258)\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:125)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:88)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:697)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:259)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:604)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:447)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:409)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:400)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:434)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:606)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:591)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1153)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1219)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.length": "    public long length() {\n      Preconditions.checkState(advertisedSize != -1,\n          \"must get input stream before length is available\");\n      return advertisedSize;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length": "  public long length() throws IOException {\n    return streams[curIdx].length();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords": "  long loadEditRecords(EditLogInputStream in, boolean closeOnExit,\n                      long expectedStartingTxId, MetaRecoveryContext recovery)\n      throws IOException {\n    FSDirectory fsDir = fsNamesys.dir;\n\n    EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts =\n      new EnumMap<FSEditLogOpCodes, Holder<Integer>>(FSEditLogOpCodes.class);\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Acquiring write lock to replay edit log\");\n    }\n\n    fsNamesys.writeLock();\n    fsDir.writeLock();\n\n    long recentOpcodeOffsets[] = new long[4];\n    Arrays.fill(recentOpcodeOffsets, -1);\n    \n    long expectedTxId = expectedStartingTxId;\n    long numEdits = 0;\n    long lastTxId = in.getLastTxId();\n    long numTxns = (lastTxId - expectedStartingTxId) + 1;\n    long lastLogTime = now();\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"edit log length: \" + in.length() + \", start txid: \"\n          + expectedStartingTxId + \", last txid: \" + lastTxId);\n    }\n    try {\n      while (true) {\n        try {\n          FSEditLogOp op;\n          try {\n            op = in.readOp();\n            if (op == null) {\n              break;\n            }\n          } catch (Throwable e) {\n            // Handle a problem with our input\n            check203UpgradeFailure(in.getVersion(), e);\n            String errorMessage =\n              formatEditLogReplayError(in, recentOpcodeOffsets, expectedTxId);\n            FSImage.LOG.error(errorMessage, e);\n            if (recovery == null) {\n               // We will only try to skip over problematic opcodes when in\n               // recovery mode.\n              throw new EditLogInputException(errorMessage, e, numEdits);\n            }\n            MetaRecoveryContext.editLogLoaderPrompt(\n                \"We failed to read txId \" + expectedTxId,\n                recovery, \"skipping the bad section in the log\");\n            in.resync();\n            continue;\n          }\n          recentOpcodeOffsets[(int)(numEdits % recentOpcodeOffsets.length)] =\n            in.getPosition();\n          if (op.hasTransactionId()) {\n            if (op.getTransactionId() > expectedTxId) { \n              MetaRecoveryContext.editLogLoaderPrompt(\"There appears \" +\n                  \"to be a gap in the edit log.  We expected txid \" +\n                  expectedTxId + \", but got txid \" +\n                  op.getTransactionId() + \".\", recovery, \"ignoring missing \" +\n                  \" transaction IDs\");\n            } else if (op.getTransactionId() < expectedTxId) { \n              MetaRecoveryContext.editLogLoaderPrompt(\"There appears \" +\n                  \"to be an out-of-order edit in the edit log.  We \" +\n                  \"expected txid \" + expectedTxId + \", but got txid \" +\n                  op.getTransactionId() + \".\", recovery,\n                  \"skipping the out-of-order edit\");\n              continue;\n            }\n          }\n          try {\n            applyEditLogOp(op, fsDir, in.getVersion());\n          } catch (Throwable e) {\n            LOG.error(\"Encountered exception on operation \" + op, e);\n            MetaRecoveryContext.editLogLoaderPrompt(\"Failed to \" +\n             \"apply edit log operation \" + op + \": error \" +\n             e.getMessage(), recovery, \"applying edits\");\n          }\n          // Now that the operation has been successfully decoded and\n          // applied, update our bookkeeping.\n          incrOpCount(op.opCode, opCounts);\n          if (op.hasTransactionId()) {\n            lastAppliedTxId = op.getTransactionId();\n            expectedTxId = lastAppliedTxId + 1;\n          } else {\n            expectedTxId = lastAppliedTxId = expectedStartingTxId;\n          }\n          // log progress\n          if (op.hasTransactionId()) {\n            long now = now();\n            if (now - lastLogTime > REPLAY_TRANSACTION_LOG_INTERVAL) {\n              long deltaTxId = lastAppliedTxId - expectedStartingTxId + 1;\n              int percent = Math.round((float) deltaTxId / numTxns * 100);\n              LOG.info(\"replaying edit log: \" + deltaTxId + \"/\" + numTxns\n                  + \" transactions completed. (\" + percent + \"%)\");\n              lastLogTime = now;\n            }\n          }\n          numEdits++;\n        } catch (MetaRecoveryContext.RequestStopException e) {\n          MetaRecoveryContext.LOG.warn(\"Stopped reading edit log at \" +\n              in.getPosition() + \"/\"  + in.length());\n          break;\n        }\n      }\n    } finally {\n      if(closeOnExit) {\n        in.close();\n      }\n      fsDir.writeUnlock();\n      fsNamesys.writeUnlock();\n\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"replaying edit log finished\");\n      }\n\n      if (FSImage.LOG.isDebugEnabled()) {\n        dumpOpCounts(opCounts);\n      }\n    }\n    return numEdits;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.formatEditLogReplayError": "  private static String formatEditLogReplayError(EditLogInputStream in,\n      long recentOpcodeOffsets[], long txid) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"Error replaying edit log at offset \" + in.getPosition());\n    sb.append(\".  Expected transaction ID was \").append(txid);\n    if (recentOpcodeOffsets[0] != -1) {\n      Arrays.sort(recentOpcodeOffsets);\n      sb.append(\"\\nRecent opcode offsets:\");\n      for (long offset : recentOpcodeOffsets) {\n        if (offset != -1) {\n          sb.append(' ').append(offset);\n        }\n      }\n    }\n    return sb.toString();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.check203UpgradeFailure": "  private void check203UpgradeFailure(int logVersion, Throwable e)\n      throws IOException {\n    // 0.20.203 version version has conflicting opcodes with the later releases.\n    // The editlog must be emptied by restarting the namenode, before proceeding\n    // with the upgrade.\n    if (Storage.is203LayoutVersion(logVersion)\n        && logVersion != HdfsConstants.LAYOUT_VERSION) {\n      String msg = \"During upgrade failed to load the editlog version \"\n          + logVersion + \" from release 0.20.203. Please go back to the old \"\n          + \" release and restart the namenode. This empties the editlog \"\n          + \" and saves the namespace. Resume the upgrade after this step.\";\n      throw new IOException(msg, e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp": "  private void applyEditLogOp(FSEditLogOp op, FSDirectory fsDir,\n      int logVersion) throws IOException {\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"replaying edit log: \" + op);\n    }\n\n    switch (op.opCode) {\n    case OP_ADD: {\n      AddCloseOp addCloseOp = (AddCloseOp)op;\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + addCloseOp.path +\n            \" numblocks : \" + addCloseOp.blocks.length +\n            \" clientHolder \" + addCloseOp.clientName +\n            \" clientMachine \" + addCloseOp.clientMachine);\n      }\n      // There three cases here:\n      // 1. OP_ADD to create a new file\n      // 2. OP_ADD to update file blocks\n      // 3. OP_ADD to open file for append\n\n      // See if the file already exists (persistBlocks call)\n      INodeFile oldFile = getINodeFile(fsDir, addCloseOp.path);\n      INodeFile newFile = oldFile;\n      if (oldFile == null) { // this is OP_ADD on a new file (case 1)\n        // versions > 0 support per file replication\n        // get name and replication\n        final short replication  = fsNamesys.getBlockManager(\n            ).adjustReplication(addCloseOp.replication);\n        assert addCloseOp.blocks.length == 0;\n\n        // add to the file tree\n        newFile = (INodeFile)fsDir.unprotectedAddFile(\n            addCloseOp.path, addCloseOp.permissions,\n            replication, addCloseOp.mtime,\n            addCloseOp.atime, addCloseOp.blockSize,\n            true, addCloseOp.clientName, addCloseOp.clientMachine);\n        fsNamesys.leaseManager.addLease(addCloseOp.clientName, addCloseOp.path);\n\n      } else { // This is OP_ADD on an existing file\n        if (!oldFile.isUnderConstruction()) {\n          // This is case 3: a call to append() on an already-closed file.\n          if (FSNamesystem.LOG.isDebugEnabled()) {\n            FSNamesystem.LOG.debug(\"Reopening an already-closed file \" +\n                \"for append\");\n          }\n          fsNamesys.prepareFileForWrite(addCloseOp.path, oldFile,\n              addCloseOp.clientName, addCloseOp.clientMachine, null,\n              false);\n          newFile = getINodeFile(fsDir, addCloseOp.path);\n        }\n      }\n      // Fall-through for case 2.\n      // Regardless of whether it's a new file or an updated file,\n      // update the block list.\n      \n      // Update the salient file attributes.\n      newFile.setAccessTime(addCloseOp.atime);\n      newFile.setModificationTimeForce(addCloseOp.mtime);\n      updateBlocks(fsDir, addCloseOp, newFile);\n      break;\n    }\n    case OP_CLOSE: {\n      AddCloseOp addCloseOp = (AddCloseOp)op;\n      \n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + addCloseOp.path +\n            \" numblocks : \" + addCloseOp.blocks.length +\n            \" clientHolder \" + addCloseOp.clientName +\n            \" clientMachine \" + addCloseOp.clientMachine);\n      }\n\n      INodeFile oldFile = getINodeFile(fsDir, addCloseOp.path);\n      if (oldFile == null) {\n        throw new IOException(\"Operation trying to close non-existent file \" +\n            addCloseOp.path);\n      }\n      \n      // Update the salient file attributes.\n      oldFile.setAccessTime(addCloseOp.atime);\n      oldFile.setModificationTimeForce(addCloseOp.mtime);\n      updateBlocks(fsDir, addCloseOp, oldFile);\n\n      // Now close the file\n      if (!oldFile.isUnderConstruction() &&\n          logVersion <= LayoutVersion.BUGFIX_HDFS_2991_VERSION) {\n        // There was a bug (HDFS-2991) in hadoop < 0.23.1 where OP_CLOSE\n        // could show up twice in a row. But after that version, this\n        // should be fixed, so we should treat it as an error.\n        throw new IOException(\n            \"File is not under construction: \" + addCloseOp.path);\n      }\n      // One might expect that you could use removeLease(holder, path) here,\n      // but OP_CLOSE doesn't serialize the holder. So, remove by path.\n      if (oldFile.isUnderConstruction()) {\n        INodeFileUnderConstruction ucFile = (INodeFileUnderConstruction) oldFile;\n        fsNamesys.leaseManager.removeLeaseWithPrefixPath(addCloseOp.path);\n        INodeFile newFile = ucFile.convertToInodeFile();\n        fsDir.unprotectedReplaceNode(addCloseOp.path, ucFile, newFile);\n      }\n      break;\n    }\n    case OP_UPDATE_BLOCKS: {\n      UpdateBlocksOp updateOp = (UpdateBlocksOp)op;\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + updateOp.path +\n            \" numblocks : \" + updateOp.blocks.length);\n      }\n      INodeFile oldFile = getINodeFile(fsDir, updateOp.path);\n      if (oldFile == null) {\n        throw new IOException(\n            \"Operation trying to update blocks in non-existent file \" +\n            updateOp.path);\n      }\n      \n      // Update in-memory data structures\n      updateBlocks(fsDir, updateOp, oldFile);\n      break;\n    }\n      \n    case OP_SET_REPLICATION: {\n      SetReplicationOp setReplicationOp = (SetReplicationOp)op;\n      short replication = fsNamesys.getBlockManager().adjustReplication(\n          setReplicationOp.replication);\n      fsDir.unprotectedSetReplication(setReplicationOp.path,\n                                      replication, null);\n      break;\n    }\n    case OP_CONCAT_DELETE: {\n      ConcatDeleteOp concatDeleteOp = (ConcatDeleteOp)op;\n      fsDir.unprotectedConcat(concatDeleteOp.trg, concatDeleteOp.srcs,\n          concatDeleteOp.timestamp);\n      break;\n    }\n    case OP_RENAME_OLD: {\n      RenameOldOp renameOp = (RenameOldOp)op;\n      fsDir.unprotectedRenameTo(renameOp.src, renameOp.dst,\n                                renameOp.timestamp);\n      break;\n    }\n    case OP_DELETE: {\n      DeleteOp deleteOp = (DeleteOp)op;\n      fsDir.unprotectedDelete(deleteOp.path, deleteOp.timestamp);\n      break;\n    }\n    case OP_MKDIR: {\n      MkdirOp mkdirOp = (MkdirOp)op;\n      fsDir.unprotectedMkdir(mkdirOp.path, mkdirOp.permissions,\n                             mkdirOp.timestamp);\n      break;\n    }\n    case OP_SET_GENSTAMP: {\n      SetGenstampOp setGenstampOp = (SetGenstampOp)op;\n      fsNamesys.setGenerationStamp(setGenstampOp.genStamp);\n      break;\n    }\n    case OP_SET_PERMISSIONS: {\n      SetPermissionsOp setPermissionsOp = (SetPermissionsOp)op;\n      fsDir.unprotectedSetPermission(setPermissionsOp.src,\n                                     setPermissionsOp.permissions);\n      break;\n    }\n    case OP_SET_OWNER: {\n      SetOwnerOp setOwnerOp = (SetOwnerOp)op;\n      fsDir.unprotectedSetOwner(setOwnerOp.src, setOwnerOp.username,\n                                setOwnerOp.groupname);\n      break;\n    }\n    case OP_SET_NS_QUOTA: {\n      SetNSQuotaOp setNSQuotaOp = (SetNSQuotaOp)op;\n      fsDir.unprotectedSetQuota(setNSQuotaOp.src,\n                                setNSQuotaOp.nsQuota,\n                                HdfsConstants.QUOTA_DONT_SET);\n      break;\n    }\n    case OP_CLEAR_NS_QUOTA: {\n      ClearNSQuotaOp clearNSQuotaOp = (ClearNSQuotaOp)op;\n      fsDir.unprotectedSetQuota(clearNSQuotaOp.src,\n                                HdfsConstants.QUOTA_RESET,\n                                HdfsConstants.QUOTA_DONT_SET);\n      break;\n    }\n\n    case OP_SET_QUOTA:\n      SetQuotaOp setQuotaOp = (SetQuotaOp)op;\n      fsDir.unprotectedSetQuota(setQuotaOp.src,\n                                setQuotaOp.nsQuota,\n                                setQuotaOp.dsQuota);\n      break;\n\n    case OP_TIMES: {\n      TimesOp timesOp = (TimesOp)op;\n\n      fsDir.unprotectedSetTimes(timesOp.path,\n                                timesOp.mtime,\n                                timesOp.atime, true);\n      break;\n    }\n    case OP_SYMLINK: {\n      SymlinkOp symlinkOp = (SymlinkOp)op;\n      fsDir.unprotectedAddSymlink(symlinkOp.path, symlinkOp.value,\n                               symlinkOp.mtime, symlinkOp.atime,\n                               symlinkOp.permissionStatus);\n      break;\n    }\n    case OP_RENAME: {\n      RenameOp renameOp = (RenameOp)op;\n      fsDir.unprotectedRenameTo(renameOp.src, renameOp.dst,\n                                renameOp.timestamp, renameOp.options);\n      break;\n    }\n    case OP_GET_DELEGATION_TOKEN: {\n      GetDelegationTokenOp getDelegationTokenOp\n        = (GetDelegationTokenOp)op;\n\n      fsNamesys.getDelegationTokenSecretManager()\n        .addPersistedDelegationToken(getDelegationTokenOp.token,\n                                     getDelegationTokenOp.expiryTime);\n      break;\n    }\n    case OP_RENEW_DELEGATION_TOKEN: {\n      RenewDelegationTokenOp renewDelegationTokenOp\n        = (RenewDelegationTokenOp)op;\n      fsNamesys.getDelegationTokenSecretManager()\n        .updatePersistedTokenRenewal(renewDelegationTokenOp.token,\n                                     renewDelegationTokenOp.expiryTime);\n      break;\n    }\n    case OP_CANCEL_DELEGATION_TOKEN: {\n      CancelDelegationTokenOp cancelDelegationTokenOp\n        = (CancelDelegationTokenOp)op;\n      fsNamesys.getDelegationTokenSecretManager()\n          .updatePersistedTokenCancellation(\n              cancelDelegationTokenOp.token);\n      break;\n    }\n    case OP_UPDATE_MASTER_KEY: {\n      UpdateMasterKeyOp updateMasterKeyOp = (UpdateMasterKeyOp)op;\n      fsNamesys.getDelegationTokenSecretManager()\n        .updatePersistedMasterKey(updateMasterKeyOp.key);\n      break;\n    }\n    case OP_REASSIGN_LEASE: {\n      ReassignLeaseOp reassignLeaseOp = (ReassignLeaseOp)op;\n\n      Lease lease = fsNamesys.leaseManager.getLease(\n          reassignLeaseOp.leaseHolder);\n      INodeFileUnderConstruction pendingFile =\n          INodeFileUnderConstruction.valueOf( \n              fsDir.getINode(reassignLeaseOp.path), reassignLeaseOp.path);\n      fsNamesys.reassignLeaseInternal(lease,\n          reassignLeaseOp.path, reassignLeaseOp.newHolder, pendingFile);\n      break;\n    }\n    case OP_START_LOG_SEGMENT:\n    case OP_END_LOG_SEGMENT: {\n      // no data in here currently.\n      break;\n    }\n    default:\n      throw new IOException(\"Invalid operation read \" + op.opCode);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.dumpOpCounts": "  private static void dumpOpCounts(\n      EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"Summary of operations loaded from edit log:\\n  \");\n    Joiner.on(\"\\n  \").withKeyValueSeparator(\"=\").appendTo(sb, opCounts);\n    FSImage.LOG.debug(sb.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.incrOpCount": "  private void incrOpCount(FSEditLogOpCodes opCode,\n      EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts) {\n    Holder<Integer> holder = opCounts.get(opCode);\n    if (holder == null) {\n      holder = new Holder<Integer>(1);\n      opCounts.put(opCode, holder);\n    } else {\n      holder.held++;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits": "  long loadFSEdits(EditLogInputStream edits, long expectedStartingTxId,\n      MetaRecoveryContext recovery) throws IOException {\n    fsNamesys.writeLock();\n    try {\n      long startTime = now();\n      long numEdits = loadEditRecords(edits, false, \n                                 expectedStartingTxId, recovery);\n      FSImage.LOG.info(\"Edits file \" + edits.getName() \n          + \" of size \" + edits.length() + \" edits # \" + numEdits \n          + \" loaded in \" + (now()-startTime)/1000 + \" seconds\");\n      return numEdits;\n    } finally {\n      edits.close();\n      fsNamesys.writeUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits": "  public long loadEdits(Iterable<EditLogInputStream> editStreams,\n      FSNamesystem target, MetaRecoveryContext recovery) throws IOException {\n    LOG.debug(\"About to load edits:\\n  \" + Joiner.on(\"\\n  \").join(editStreams));\n    \n    long prevLastAppliedTxId = lastAppliedTxId;  \n    try {    \n      FSEditLogLoader loader = new FSEditLogLoader(target, lastAppliedTxId);\n      \n      // Load latest edits\n      for (EditLogInputStream editIn : editStreams) {\n        LOG.info(\"Reading \" + editIn + \" expecting start txid #\" +\n              (lastAppliedTxId + 1));\n        try {\n          loader.loadFSEdits(editIn, lastAppliedTxId + 1, recovery);\n        } finally {\n          // Update lastAppliedTxId even in case of error, since some ops may\n          // have been successfully applied before the error.\n          lastAppliedTxId = loader.getLastAppliedTxId();\n        }\n        // If we are in recovery mode, we may have skipped over some txids.\n        if (editIn.getLastTxId() != HdfsConstants.INVALID_TXID) {\n          lastAppliedTxId = editIn.getLastTxId();\n        }\n      }\n    } finally {\n      FSEditLog.closeAllStreams(editStreams);\n      // update the counts\n      target.dir.updateCountForINodeWithQuota();   \n    }\n    return lastAppliedTxId - prevLastAppliedTxId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getLastAppliedTxId": "  public synchronized long getLastAppliedTxId() {\n    return lastAppliedTxId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage": "  private void loadFSImage(File curFile, MD5Hash expectedMd5,\n      FSNamesystem target, MetaRecoveryContext recovery) throws IOException {\n    FSImageFormat.Loader loader = new FSImageFormat.Loader(\n        conf, target);\n    loader.load(curFile);\n    target.setBlockPoolId(this.getBlockPoolID());\n\n    // Check that the image digest we loaded matches up with what\n    // we expected\n    MD5Hash readImageMd5 = loader.getLoadedImageMd5();\n    if (expectedMd5 != null &&\n        !expectedMd5.equals(readImageMd5)) {\n      throw new IOException(\"Image file \" + curFile +\n          \" is corrupt with MD5 checksum of \" + readImageMd5 +\n          \" but expecting \" + expectedMd5);\n    }\n\n    long txId = loader.getLoadedImageTxId();\n    LOG.info(\"Loaded image for txid \" + txId + \" from \" + curFile);\n    lastAppliedTxId = txId;\n    storage.setMostRecentCheckpointInfo(txId, curFile.lastModified());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getBlockPoolID": "  public String getBlockPoolID() {\n    return storage.getBlockPoolID();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.initEditLog": "  public void initEditLog() {\n    Preconditions.checkState(getNamespaceID() != 0,\n        \"Must know namespace ID before initting edit log\");\n    String nameserviceId = DFSUtil.getNamenodeNameServiceId(conf);\n    if (!HAUtil.isHAEnabled(conf, nameserviceId)) {\n      editLog.initJournalsForWrite();\n      editLog.recoverUnclosedStreams();\n    } else {\n      editLog.initSharedJournalsForRead();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getLayoutVersion": "  public int getLayoutVersion() {\n    return storage.getLayoutVersion();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.needsResaveBasedOnStaleCheckpoint": "  private boolean needsResaveBasedOnStaleCheckpoint(\n      File imageFile, long numEditsLoaded) {\n    final long checkpointPeriod = conf.getLong(\n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_KEY, \n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_DEFAULT);\n    final long checkpointTxnCount = conf.getLong(\n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_KEY, \n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_DEFAULT);\n    long checkpointAge = Time.now() - imageFile.lastModified();\n\n    return (checkpointAge > checkpointPeriod * 1000) ||\n           (numEditsLoaded > checkpointTxnCount);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.isUpgradeFinalized": "  boolean isUpgradeFinalized() {\n    return isUpgradeFinalized;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead": "  boolean recoverTransitionRead(StartupOption startOpt, FSNamesystem target,\n      MetaRecoveryContext recovery) throws IOException {\n    assert startOpt != StartupOption.FORMAT : \n      \"NameNode formatting should be performed before reading the image\";\n    \n    Collection<URI> imageDirs = storage.getImageDirectories();\n    Collection<URI> editsDirs = editLog.getEditURIs();\n\n    // none of the data dirs exist\n    if((imageDirs.size() == 0 || editsDirs.size() == 0) \n                             && startOpt != StartupOption.IMPORT)  \n      throw new IOException(\n          \"All specified directories are not accessible or do not exist.\");\n    \n    // 1. For each data directory calculate its state and \n    // check whether all is consistent before transitioning.\n    Map<StorageDirectory, StorageState> dataDirStates = \n             new HashMap<StorageDirectory, StorageState>();\n    boolean isFormatted = recoverStorageDirs(startOpt, dataDirStates);\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Data dir states:\\n  \" +\n        Joiner.on(\"\\n  \").withKeyValueSeparator(\": \")\n        .join(dataDirStates));\n    }\n    \n    if (!isFormatted && startOpt != StartupOption.ROLLBACK \n                     && startOpt != StartupOption.IMPORT) {\n      throw new IOException(\"NameNode is not formatted.\");      \n    }\n\n\n    int layoutVersion = storage.getLayoutVersion();\n    if (layoutVersion < Storage.LAST_PRE_UPGRADE_LAYOUT_VERSION) {\n      NNStorage.checkVersionUpgradable(storage.getLayoutVersion());\n    }\n    if (startOpt != StartupOption.UPGRADE\n        && layoutVersion < Storage.LAST_PRE_UPGRADE_LAYOUT_VERSION\n        && layoutVersion != HdfsConstants.LAYOUT_VERSION) {\n      throw new IOException(\n          \"\\nFile system image contains an old layout version \" \n          + storage.getLayoutVersion() + \".\\nAn upgrade to version \"\n          + HdfsConstants.LAYOUT_VERSION + \" is required.\\n\"\n          + \"Please restart NameNode with -upgrade option.\");\n    }\n    \n    storage.processStartupOptionsForUpgrade(startOpt, layoutVersion);\n\n    // 2. Format unformatted dirs.\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      StorageState curState = dataDirStates.get(sd);\n      switch(curState) {\n      case NON_EXISTENT:\n        throw new IOException(StorageState.NON_EXISTENT + \n                              \" state cannot be here\");\n      case NOT_FORMATTED:\n        LOG.info(\"Storage directory \" + sd.getRoot() + \" is not formatted.\");\n        LOG.info(\"Formatting ...\");\n        sd.clearDirectory(); // create empty currrent dir\n        break;\n      default:\n        break;\n      }\n    }\n\n    // 3. Do transitions\n    switch(startOpt) {\n    case UPGRADE:\n      doUpgrade(target);\n      return false; // upgrade saved image already\n    case IMPORT:\n      doImportCheckpoint(target);\n      return false; // import checkpoint saved image already\n    case ROLLBACK:\n      doRollback();\n      break;\n    case REGULAR:\n      // just load the image\n    }\n    \n    return loadFSImage(target, recovery);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.doRollback": "  private void doRollback() throws IOException {\n    // Rollback is allowed only if there is \n    // a previous fs states in at least one of the storage directories.\n    // Directories that don't have previous state do not rollback\n    boolean canRollback = false;\n    FSImage prevState = new FSImage(conf);\n    prevState.getStorage().layoutVersion = HdfsConstants.LAYOUT_VERSION;\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      File prevDir = sd.getPreviousDir();\n      if (!prevDir.exists()) {  // use current directory then\n        LOG.info(\"Storage directory \" + sd.getRoot()\n                 + \" does not contain previous fs state.\");\n        // read and verify consistency with other directories\n        storage.readProperties(sd);\n        continue;\n      }\n\n      // read and verify consistency of the prev dir\n      prevState.getStorage().readPreviousVersionProperties(sd);\n\n      if (prevState.getLayoutVersion() != HdfsConstants.LAYOUT_VERSION) {\n        throw new IOException(\n          \"Cannot rollback to storage version \" +\n          prevState.getLayoutVersion() +\n          \" using this version of the NameNode, which uses storage version \" +\n          HdfsConstants.LAYOUT_VERSION + \". \" +\n          \"Please use the previous version of HDFS to perform the rollback.\");\n      }\n      canRollback = true;\n    }\n    if (!canRollback)\n      throw new IOException(\"Cannot rollback. None of the storage \"\n                            + \"directories contain previous fs state.\");\n\n    // Now that we know all directories are going to be consistent\n    // Do rollback for each directory containing previous state\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      File prevDir = sd.getPreviousDir();\n      if (!prevDir.exists())\n        continue;\n\n      LOG.info(\"Rolling back storage directory \" + sd.getRoot()\n               + \".\\n   new LV = \" + prevState.getStorage().getLayoutVersion()\n               + \"; new CTime = \" + prevState.getStorage().getCTime());\n      File tmpDir = sd.getRemovedTmp();\n      assert !tmpDir.exists() : \"removed.tmp directory must not exist.\";\n      // rename current to tmp\n      File curDir = sd.getCurrentDir();\n      assert curDir.exists() : \"Current directory must exist.\";\n      NNStorage.rename(curDir, tmpDir);\n      // rename previous to current\n      NNStorage.rename(prevDir, curDir);\n\n      // delete tmp dir\n      NNStorage.deleteDir(tmpDir);\n      LOG.info(\"Rollback of \" + sd.getRoot()+ \" is complete.\");\n    }\n    isUpgradeFinalized = true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs": "  private boolean recoverStorageDirs(StartupOption startOpt,\n      Map<StorageDirectory, StorageState> dataDirStates) throws IOException {\n    boolean isFormatted = false;\n    for (Iterator<StorageDirectory> it = \n                      storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      StorageState curState;\n      try {\n        curState = sd.analyzeStorage(startOpt, storage);\n        String nameserviceId = DFSUtil.getNamenodeNameServiceId(conf);\n        if (curState != StorageState.NORMAL && HAUtil.isHAEnabled(conf, nameserviceId)) {\n          throw new IOException(\"Cannot start an HA namenode with name dirs \" +\n              \"that need recovery. Dir: \" + sd + \" state: \" + curState);\n        }\n        // sd is locked but not opened\n        switch(curState) {\n        case NON_EXISTENT:\n          // name-node fails if any of the configured storage dirs are missing\n          throw new InconsistentFSStateException(sd.getRoot(),\n                      \"storage directory does not exist or is not accessible.\");\n        case NOT_FORMATTED:\n          break;\n        case NORMAL:\n          break;\n        default:  // recovery is possible\n          sd.doRecover(curState);      \n        }\n        if (curState != StorageState.NOT_FORMATTED \n            && startOpt != StartupOption.ROLLBACK) {\n          // read and verify consistency with other directories\n          storage.readProperties(sd);\n          isFormatted = true;\n        }\n        if (startOpt == StartupOption.IMPORT && isFormatted)\n          // import of a checkpoint is allowed only into empty image directories\n          throw new IOException(\"Cannot import image from a checkpoint. \" \n              + \" NameNode already contains an image in \" + sd.getRoot());\n      } catch (IOException ioe) {\n        sd.unlock();\n        throw ioe;\n      }\n      dataDirStates.put(sd,curState);\n    }\n    return isFormatted;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade": "  private void doUpgrade(FSNamesystem target) throws IOException {\n    // Upgrade is allowed only if there are \n    // no previous fs states in any of the directories\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      if (sd.getPreviousDir().exists())\n        throw new InconsistentFSStateException(sd.getRoot(),\n            \"previous fs state should not exist during upgrade. \"\n            + \"Finalize or rollback first.\");\n    }\n\n    // load the latest image\n    this.loadFSImage(target, null);\n\n    // Do upgrade for each directory\n    long oldCTime = storage.getCTime();\n    storage.cTime = now();  // generate new cTime for the state\n    int oldLV = storage.getLayoutVersion();\n    storage.layoutVersion = HdfsConstants.LAYOUT_VERSION;\n    \n    List<StorageDirectory> errorSDs =\n      Collections.synchronizedList(new ArrayList<StorageDirectory>());\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      LOG.info(\"Starting upgrade of image directory \" + sd.getRoot()\n               + \".\\n   old LV = \" + oldLV\n               + \"; old CTime = \" + oldCTime\n               + \".\\n   new LV = \" + storage.getLayoutVersion()\n               + \"; new CTime = \" + storage.getCTime());\n      try {\n        File curDir = sd.getCurrentDir();\n        File prevDir = sd.getPreviousDir();\n        File tmpDir = sd.getPreviousTmp();\n        assert curDir.exists() : \"Current directory must exist.\";\n        assert !prevDir.exists() : \"previous directory must not exist.\";\n        assert !tmpDir.exists() : \"previous.tmp directory must not exist.\";\n        assert !editLog.isSegmentOpen() : \"Edits log must not be open.\";\n\n        // rename current to tmp\n        NNStorage.rename(curDir, tmpDir);\n        \n        if (!curDir.mkdir()) {\n          throw new IOException(\"Cannot create directory \" + curDir);\n        }\n      } catch (Exception e) {\n        LOG.error(\"Failed to move aside pre-upgrade storage \" +\n            \"in image directory \" + sd.getRoot(), e);\n        errorSDs.add(sd);\n        continue;\n      }\n    }\n    storage.reportErrorsOnDirectories(errorSDs);\n    errorSDs.clear();\n\n    saveFSImageInAllDirs(target, editLog.getLastWrittenTxId());\n\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      try {\n        // Write the version file, since saveFsImage above only makes the\n        // fsimage_<txid>, and the directory is otherwise empty.\n        storage.writeProperties(sd);\n        \n        File prevDir = sd.getPreviousDir();\n        File tmpDir = sd.getPreviousTmp();\n        // rename tmp to previous\n        NNStorage.rename(tmpDir, prevDir);\n      } catch (IOException ioe) {\n        LOG.error(\"Unable to rename temp to previous for \" + sd.getRoot(), ioe);\n        errorSDs.add(sd);\n        continue;\n      }\n      LOG.info(\"Upgrade of \" + sd.getRoot() + \" is complete.\");\n    }\n    storage.reportErrorsOnDirectories(errorSDs);\n\n    isUpgradeFinalized = false;\n    if (!storage.getRemovedStorageDirs().isEmpty()) {\n      //during upgrade, it's a fatal error to fail any storage directory\n      throw new IOException(\"Upgrade failed in \"\n          + storage.getRemovedStorageDirs().size()\n          + \" storage directory(ies), previously logged.\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.doImportCheckpoint": "  void doImportCheckpoint(FSNamesystem target) throws IOException {\n    Collection<URI> checkpointDirs =\n      FSImage.getCheckpointDirs(conf, null);\n    List<URI> checkpointEditsDirs =\n      FSImage.getCheckpointEditsDirs(conf, null);\n\n    if (checkpointDirs == null || checkpointDirs.isEmpty()) {\n      throw new IOException(\"Cannot import image from a checkpoint. \"\n                            + \"\\\"dfs.namenode.checkpoint.dir\\\" is not set.\" );\n    }\n    \n    if (checkpointEditsDirs == null || checkpointEditsDirs.isEmpty()) {\n      throw new IOException(\"Cannot import image from a checkpoint. \"\n                            + \"\\\"dfs.namenode.checkpoint.dir\\\" is not set.\" );\n    }\n\n    FSImage realImage = target.getFSImage();\n    FSImage ckptImage = new FSImage(conf, \n                                    checkpointDirs, checkpointEditsDirs);\n    target.dir.fsImage = ckptImage;\n    // load from the checkpoint dirs\n    try {\n      ckptImage.recoverTransitionRead(StartupOption.REGULAR, target, null);\n    } finally {\n      ckptImage.close();\n    }\n    // return back the real image\n    realImage.getStorage().setStorageInfo(ckptImage.getStorage());\n    realImage.getEditLog().setNextTxId(ckptImage.getEditLog().getLastWrittenTxId()+1);\n    realImage.initEditLog();\n\n    target.dir.fsImage = realImage;\n    realImage.getStorage().setBlockPoolID(ckptImage.getBlockPoolID());\n\n    // and save it but keep the same checkpointTime\n    saveNamespace(target);\n    getStorage().writeAll();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage": "  void loadFSImage(StartupOption startOpt, FSImage fsImage, boolean haEnabled)\n      throws IOException {\n    // format before starting up if requested\n    if (startOpt == StartupOption.FORMAT) {\n      \n      fsImage.format(this, fsImage.getStorage().determineClusterId());// reuse current id\n\n      startOpt = StartupOption.REGULAR;\n    }\n    boolean success = false;\n    writeLock();\n    try {\n      // We shouldn't be calling saveNamespace if we've come up in standby state.\n      MetaRecoveryContext recovery = startOpt.createRecoveryContext();\n      if (fsImage.recoverTransitionRead(startOpt, this, recovery) && !haEnabled) {\n        fsImage.saveNamespace(this);\n      }\n      // This will start a new log segment and write to the seen_txid file, so\n      // we shouldn't do it when coming up in standby state\n      if (!haEnabled) {\n        fsImage.openEditLogForWrite();\n      }\n      \n      success = true;\n    } finally {\n      if (!success) {\n        fsImage.close();\n      }\n      writeUnlock();\n    }\n    dir.imageLoadComplete();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.close": "  void close() {\n    fsRunning = false;\n    try {\n      stopCommonServices();\n      if (smmthread != null) smmthread.interrupt();\n    } finally {\n      // using finally to ensure we also wait for lease daemon\n      try {\n        stopActiveServices();\n        stopStandbyServices();\n        if (dir != null) {\n          dir.close();\n        }\n      } catch (IOException ie) {\n        LOG.error(\"Error closing FSDirectory\", ie);\n        IOUtils.cleanup(LOG, dir);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock": "  public void writeUnlock() {\n    this.fsLock.writeLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock": "  public void writeLock() {\n    this.fsLock.writeLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveNamespace": "  void saveNamespace() throws AccessControlException, IOException {\n    readLock();\n    try {\n      checkSuperuserPrivilege();\n      if (!isInSafeMode()) {\n        throw new IOException(\"Safe mode should be turned ON \" +\n                              \"in order to create namespace image.\");\n      }\n      getFSImage().saveNamespace(this);\n      LOG.info(\"New namespace image has been created\");\n    } finally {\n      readUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk": "  public static FSNamesystem loadFromDisk(Configuration conf,\n      Collection<URI> namespaceDirs, List<URI> namespaceEditsDirs)\n      throws IOException {\n\n    if (namespaceDirs.size() == 1) {\n      LOG.warn(\"Only one image storage directory (\"\n          + DFS_NAMENODE_NAME_DIR_KEY + \") configured. Beware of dataloss\"\n          + \" due to lack of redundant storage directories!\");\n    }\n    if (namespaceEditsDirs.size() == 1) {\n      LOG.warn(\"Only one namespace edits storage directory (\"\n          + DFS_NAMENODE_EDITS_DIR_KEY + \") configured. Beware of dataloss\"\n          + \" due to lack of redundant storage directories!\");\n    }\n\n    FSImage fsImage = new FSImage(conf, namespaceDirs, namespaceEditsDirs);\n    FSNamesystem namesystem = new FSNamesystem(conf, fsImage);\n    StartupOption startOpt = NameNode.getStartupOption(conf);\n    if (startOpt == StartupOption.RECOVER) {\n      namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n    }\n\n    long loadStart = now();\n    String nameserviceId = DFSUtil.getNamenodeNameServiceId(conf);\n    namesystem.loadFSImage(startOpt, fsImage,\n      HAUtil.isHAEnabled(conf, nameserviceId));\n    long timeTakenToLoadFSImage = now() - loadStart;\n    LOG.info(\"Finished loading FSImage in \" + timeTakenToLoadFSImage + \" msecs\");\n    NameNodeMetrics nnMetrics = NameNode.getNameNodeMetrics();\n    if (nnMetrics != null) {\n      nnMetrics.setFsImageLoadTime((int) timeTakenToLoadFSImage);\n    }\n    return namesystem;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNamespaceEditsDirs": "  public static List<URI> getNamespaceEditsDirs(Configuration conf,\n      boolean includeShared)\n      throws IOException {\n    // Use a LinkedHashSet so that order is maintained while we de-dup\n    // the entries.\n    LinkedHashSet<URI> editsDirs = new LinkedHashSet<URI>();\n    \n    if (includeShared) {\n      List<URI> sharedDirs = getSharedEditsDirs(conf);\n  \n      // Fail until multiple shared edits directories are supported (HDFS-2782)\n      if (sharedDirs.size() > 1) {\n        throw new IOException(\n            \"Multiple shared edits directories are not yet supported\");\n      }\n  \n      // First add the shared edits dirs. It's critical that the shared dirs\n      // are added first, since JournalSet syncs them in the order they are listed,\n      // and we need to make sure all edits are in place in the shared storage\n      // before they are replicated locally. See HDFS-2874.\n      for (URI dir : sharedDirs) {\n        if (!editsDirs.add(dir)) {\n          LOG.warn(\"Edits URI \" + dir + \" listed multiple times in \" + \n              DFS_NAMENODE_SHARED_EDITS_DIR_KEY + \". Ignoring duplicates.\");\n        }\n      }\n    }    \n    // Now add the non-shared dirs.\n    for (URI dir : getStorageDirs(conf, DFS_NAMENODE_EDITS_DIR_KEY)) {\n      if (!editsDirs.add(dir)) {\n        LOG.warn(\"Edits URI \" + dir + \" listed multiple times in \" + \n            DFS_NAMENODE_SHARED_EDITS_DIR_KEY + \" and \" +\n            DFS_NAMENODE_EDITS_DIR_KEY + \". Ignoring duplicates.\");\n      }\n    }\n\n    if (editsDirs.isEmpty()) {\n      // If this is the case, no edit dirs have been explicitly configured.\n      // Image dirs are to be used for edits too.\n      return Lists.newArrayList(getNamespaceDirs(conf));\n    } else {\n      return Lists.newArrayList(editsDirs);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setSafeMode": "  boolean setSafeMode(SafeModeAction action) throws IOException {\n    if (action != SafeModeAction.SAFEMODE_GET) {\n      checkSuperuserPrivilege();\n      switch(action) {\n      case SAFEMODE_LEAVE: // leave safe mode\n        leaveSafeMode();\n        break;\n      case SAFEMODE_ENTER: // enter safe mode\n        enterSafeMode(false);\n        break;\n      }\n    }\n    return isInSafeMode();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNamespaceDirs": "  public static Collection<URI> getNamespaceDirs(Configuration conf) {\n    return getStorageDirs(conf, DFS_NAMENODE_NAME_DIR_KEY);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem": "  protected void loadNamesystem(Configuration conf) throws IOException {\n    this.namesystem = FSNamesystem.loadFromDisk(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initialize": "  protected void initialize(Configuration conf) throws IOException {\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    loadNamesystem(conf);\n\n    rpcServer = createRpcServer(conf);\n    \n    try {\n      validateConfigurationSettings(conf);\n    } catch (IOException e) {\n      LOG.fatal(e.toString());\n      throw e;\n    }\n\n    startCommonServices(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getRole": "  public NamenodeRole getRole() {\n    return role;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.startCommonServices": "  private void startCommonServices(Configuration conf) throws IOException {\n    namesystem.startCommonServices(conf, haContext);\n    startHttpServer(conf);\n    rpcServer.start();\n    plugins = conf.getInstances(DFS_NAMENODE_PLUGINS_KEY,\n        ServicePlugin.class);\n    for (ServicePlugin p: plugins) {\n      try {\n        p.start(this);\n      } catch (Throwable t) {\n        LOG.warn(\"ServicePlugin \" + p + \" could not be started\", t);\n      }\n    }\n    LOG.info(getRole() + \" RPC up at: \" + rpcServer.getRpcAddress());\n    if (rpcServer.getServiceRpcAddress() != null) {\n      LOG.info(getRole() + \" service RPC up at: \"\n          + rpcServer.getServiceRpcAddress());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initMetrics": "  static void initMetrics(Configuration conf, NamenodeRole role) {\n    metrics = NameNodeMetrics.create(conf, role);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer": "  protected NameNodeRpcServer createRpcServer(Configuration conf)\n      throws IOException {\n    return new NameNodeRpcServer(conf, this);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.loginAsNameNodeUser": "  void loginAsNameNodeUser(Configuration conf) throws IOException {\n    InetSocketAddress socAddr = getRpcServerAddress(conf);\n    SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n        DFS_NAMENODE_USER_NAME_KEY, socAddr.getHostName());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.validateConfigurationSettings": "  protected void validateConfigurationSettings(final Configuration conf) \n      throws IOException {\n    // check to make sure the web port and rpc port do not match \n    if(getHttpServerAddress(conf).getPort() \n        == getRpcServerAddress(conf).getPort()) {\n      String errMsg = \"dfs.namenode.rpc-address \" +\n          \"(\"+ getRpcServerAddress(conf) + \") and \" +\n          \"dfs.namenode.http-address (\"+ getHttpServerAddress(conf) + \") \" +\n          \"configuration keys are bound to the same port, unable to start \" +\n          \"NameNode. Port: \" + getRpcServerAddress(conf).getPort();\n      throw new IOException(errMsg);\n    } \n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf == null)\n      conf = new HdfsConfiguration();\n    // Parse out some generic args into Configuration.\n    GenericOptionsParser hParser = new GenericOptionsParser(conf, argv);\n    argv = hParser.getRemainingArgs();\n    // Parse the rest, NN specific args.\n    StartupOption startOpt = parseArguments(argv);\n    if (startOpt == null) {\n      printUsage(System.err);\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n    \n    if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) &&\n        (startOpt == StartupOption.UPGRADE ||\n         startOpt == StartupOption.ROLLBACK ||\n         startOpt == StartupOption.FINALIZE)) {\n      throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n          \"Cannot perform DFS upgrade with HA enabled.\");\n    }\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted = format(conf, startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        terminate(0);\n        return null;\n      }\n      case FINALIZE: {\n        boolean aborted = finalize(conf, true);\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] = Arrays.copyOfRange(argv, 1, argv.length);\n        int rc = BootstrapStandby.run(toolArgs, conf);\n        terminate(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted = initializeSharedEdits(conf,\n            startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role = startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      case RECOVER: {\n        NameNode.doRecovery(startOpt, conf);\n        return null;\n      }\n      default: {\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.printUsage": "  private static void printUsage(PrintStream out) {\n    out.println(USAGE + \"\\n\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits": "  private static boolean initializeSharedEdits(Configuration conf,\n      boolean force, boolean interactive) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    \n    if (conf.get(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY) == null) {\n      LOG.fatal(\"No shared edits directory configured for namespace \" +\n          nsId + \" namenode \" + namenodeId);\n      return false;\n    }\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      InetSocketAddress socAddr = getAddress(conf);\n      SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n          DFS_NAMENODE_USER_NAME_KEY, socAddr.getHostName());\n    }\n\n    NNStorage existingStorage = null;\n    try {\n      Configuration confWithoutShared = new Configuration(conf);\n      confWithoutShared.unset(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY);\n      FSNamesystem fsns = FSNamesystem.loadFromDisk(confWithoutShared,\n          FSNamesystem.getNamespaceDirs(conf),\n          FSNamesystem.getNamespaceEditsDirs(conf, false));\n      \n      existingStorage = fsns.getFSImage().getStorage();\n      NamespaceInfo nsInfo = existingStorage.getNamespaceInfo();\n      \n      List<URI> sharedEditsDirs = FSNamesystem.getSharedEditsDirs(conf);\n      \n      FSImage sharedEditsImage = new FSImage(conf,\n          Lists.<URI>newArrayList(),\n          sharedEditsDirs);\n      sharedEditsImage.getEditLog().initJournalsForWrite();\n      \n      if (!sharedEditsImage.confirmFormat(force, interactive)) {\n        return true; // abort\n      }\n      \n      NNStorage newSharedStorage = sharedEditsImage.getStorage();\n      // Call Storage.format instead of FSImage.format here, since we don't\n      // actually want to save a checkpoint - just prime the dirs with\n      // the existing namespace info\n      newSharedStorage.format(nsInfo);\n      sharedEditsImage.getEditLog().formatNonFileJournals(nsInfo);\n\n      // Need to make sure the edit log segments are in good shape to initialize\n      // the shared edits dir.\n      fsns.getFSImage().getEditLog().close();\n      fsns.getFSImage().getEditLog().initJournalsForWrite();\n      fsns.getFSImage().getEditLog().recoverUnclosedStreams();\n\n      copyEditLogSegmentsToSharedDir(fsns, sharedEditsDirs, newSharedStorage,\n          conf);\n    } catch (IOException ioe) {\n      LOG.error(\"Could not initialize shared edits dir\", ioe);\n      return true; // aborted\n    } finally {\n      // Have to unlock storage explicitly for the case when we're running in a\n      // unit test, which runs in the same JVM as NNs.\n      if (existingStorage != null) {\n        try {\n          existingStorage.unlockAll();\n        } catch (IOException ioe) {\n          LOG.warn(\"Could not unlock storage directories\", ioe);\n          return true; // aborted\n        }\n      }\n    }\n    return false; // did not abort\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.setStartupOption": "  private static void setStartupOption(Configuration conf, StartupOption opt) {\n    conf.set(DFS_NAMENODE_STARTUP_KEY, opt.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.run": "          public FileSystem run() throws IOException {\n            return FileSystem.get(conf);\n          }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.parseArguments": "  private static StartupOption parseArguments(String args[]) {\n    int argsLen = (args == null) ? 0 : args.length;\n    StartupOption startOpt = StartupOption.REGULAR;\n    for(int i=0; i < argsLen; i++) {\n      String cmd = args[i];\n      if (StartupOption.FORMAT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.FORMAT;\n        for (i = i + 1; i < argsLen; i++) {\n          if (args[i].equalsIgnoreCase(StartupOption.CLUSTERID.getName())) {\n            i++;\n            if (i >= argsLen) {\n              // if no cluster id specified, return null\n              LOG.fatal(\"Must specify a valid cluster ID after the \"\n                  + StartupOption.CLUSTERID.getName() + \" flag\");\n              return null;\n            }\n            String clusterId = args[i];\n            // Make sure an id is specified and not another flag\n            if (clusterId.isEmpty() ||\n                clusterId.equalsIgnoreCase(StartupOption.FORCE.getName()) ||\n                clusterId.equalsIgnoreCase(\n                    StartupOption.NONINTERACTIVE.getName())) {\n              LOG.fatal(\"Must specify a valid cluster ID after the \"\n                  + StartupOption.CLUSTERID.getName() + \" flag\");\n              return null;\n            }\n            startOpt.setClusterId(clusterId);\n          }\n\n          if (args[i].equalsIgnoreCase(StartupOption.FORCE.getName())) {\n            startOpt.setForceFormat(true);\n          }\n\n          if (args[i].equalsIgnoreCase(StartupOption.NONINTERACTIVE.getName())) {\n            startOpt.setInteractiveFormat(false);\n          }\n        }\n      } else if (StartupOption.GENCLUSTERID.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.GENCLUSTERID;\n      } else if (StartupOption.REGULAR.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.REGULAR;\n      } else if (StartupOption.BACKUP.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.BACKUP;\n      } else if (StartupOption.CHECKPOINT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.CHECKPOINT;\n      } else if (StartupOption.UPGRADE.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.UPGRADE;\n        // might be followed by two args\n        if (i + 2 < argsLen\n            && args[i + 1].equalsIgnoreCase(StartupOption.CLUSTERID.getName())) {\n          i += 2;\n          startOpt.setClusterId(args[i]);\n        }\n      } else if (StartupOption.ROLLBACK.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.ROLLBACK;\n      } else if (StartupOption.FINALIZE.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.FINALIZE;\n      } else if (StartupOption.IMPORT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.IMPORT;\n      } else if (StartupOption.BOOTSTRAPSTANDBY.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.BOOTSTRAPSTANDBY;\n        return startOpt;\n      } else if (StartupOption.INITIALIZESHAREDEDITS.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.INITIALIZESHAREDEDITS;\n        for (i = i + 1 ; i < argsLen; i++) {\n          if (StartupOption.NONINTERACTIVE.getName().equals(args[i])) {\n            startOpt.setInteractiveFormat(false);\n          } else if (StartupOption.FORCE.getName().equals(args[i])) {\n            startOpt.setForceFormat(true);\n          } else {\n            LOG.fatal(\"Invalid argument: \" + args[i]);\n            return null;\n          }\n        }\n        return startOpt;\n      } else if (StartupOption.RECOVER.getName().equalsIgnoreCase(cmd)) {\n        if (startOpt != StartupOption.REGULAR) {\n          throw new RuntimeException(\"Can't combine -recover with \" +\n              \"other startup options.\");\n        }\n        startOpt = StartupOption.RECOVER;\n        while (++i < argsLen) {\n          if (args[i].equalsIgnoreCase(\n                StartupOption.FORCE.getName())) {\n            startOpt.setForce(MetaRecoveryContext.FORCE_FIRST_CHOICE);\n          } else {\n            throw new RuntimeException(\"Error parsing recovery options: \" + \n              \"can't understand option \\\"\" + args[i] + \"\\\"\");\n          }\n        }\n      } else {\n        return null;\n      }\n    }\n    return startOpt;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.finalize": "  private static boolean finalize(Configuration conf,\n                               boolean isConfirmationNeeded\n                               ) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n\n    FSNamesystem nsys = new FSNamesystem(conf, new FSImage(conf));\n    System.err.print(\n        \"\\\"finalize\\\" will remove the previous state of the files system.\\n\"\n        + \"Recent upgrade will become permanent.\\n\"\n        + \"Rollback option will not be available anymore.\\n\");\n    if (isConfirmationNeeded) {\n      if (!confirmPrompt(\"Finalize filesystem state?\")) {\n        System.err.println(\"Finalize aborted.\");\n        return true;\n      }\n    }\n    nsys.dir.fsImage.finalizeUpgrade();\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.doRecovery": "  private static void doRecovery(StartupOption startOpt, Configuration conf)\n      throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    if (startOpt.getForce() < MetaRecoveryContext.FORCE_ALL) {\n      if (!confirmPrompt(\"You have selected Metadata Recovery mode.  \" +\n          \"This mode is intended to recover lost metadata on a corrupt \" +\n          \"filesystem.  Metadata recovery mode often permanently deletes \" +\n          \"data from your HDFS filesystem.  Please back up your edit log \" +\n          \"and fsimage before trying this!\\n\\n\" +\n          \"Are you ready to proceed? (Y/N)\\n\")) {\n        System.err.println(\"Recovery aborted at user request.\\n\");\n        return;\n      }\n    }\n    MetaRecoveryContext.LOG.info(\"starting recovery...\");\n    UserGroupInformation.setConfiguration(conf);\n    NameNode.initMetrics(conf, startOpt.toNodeRole());\n    FSNamesystem fsn = null;\n    try {\n      fsn = FSNamesystem.loadFromDisk(conf);\n      fsn.saveNamespace();\n      MetaRecoveryContext.LOG.info(\"RECOVERY COMPLETE\");\n    } catch (IOException e) {\n      MetaRecoveryContext.LOG.info(\"RECOVERY FAILED: caught exception\", e);\n      throw e;\n    } catch (RuntimeException e) {\n      MetaRecoveryContext.LOG.info(\"RECOVERY FAILED: caught exception\", e);\n      throw e;\n    } finally {\n      if (fsn != null)\n        fsn.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.format": "  private static boolean format(Configuration conf, boolean force,\n      boolean isInteractive) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    checkAllowFormat(conf);\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      InetSocketAddress socAddr = getAddress(conf);\n      SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n          DFS_NAMENODE_USER_NAME_KEY, socAddr.getHostName());\n    }\n    \n    Collection<URI> nameDirsToFormat = FSNamesystem.getNamespaceDirs(conf);\n    List<URI> sharedDirs = FSNamesystem.getSharedEditsDirs(conf);\n    List<URI> dirsToPrompt = new ArrayList<URI>();\n    dirsToPrompt.addAll(nameDirsToFormat);\n    dirsToPrompt.addAll(sharedDirs);\n    List<URI> editDirsToFormat = \n                 FSNamesystem.getNamespaceEditsDirs(conf);\n\n    // if clusterID is not provided - see if you can find the current one\n    String clusterId = StartupOption.FORMAT.getClusterId();\n    if(clusterId == null || clusterId.equals(\"\")) {\n      //Generate a new cluster id\n      clusterId = NNStorage.newClusterID();\n    }\n    System.out.println(\"Formatting using clusterid: \" + clusterId);\n    \n    FSImage fsImage = new FSImage(conf, nameDirsToFormat, editDirsToFormat);\n    FSNamesystem fsn = new FSNamesystem(conf, fsImage);\n    fsImage.getEditLog().initJournalsForWrite();\n    \n    if (!fsImage.confirmFormat(force, isInteractive)) {\n      return true; // aborted\n    }\n    \n    fsImage.format(fsn, clusterId);\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.main": "  public static void main(String argv[]) throws Exception {\n    if (DFSUtil.parseHelpArgument(argv, NameNode.USAGE, System.out, true)) {\n      System.exit(0);\n    }\n\n    try {\n      StringUtils.startupShutdownMessage(NameNode.class, argv, LOG);\n      NameNode namenode = createNameNode(argv, null);\n      if (namenode != null) {\n        namenode.join();\n      }\n    } catch (Throwable e) {\n      LOG.fatal(\"Exception in namenode join\", e);\n      terminate(1, e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.join": "  public void join() {\n    try {\n      rpcServer.join();\n    } catch (InterruptedException ie) {\n      LOG.info(\"Caught interrupted exception \", ie);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.editLogLoaderPrompt": "  public static void editLogLoaderPrompt(String prompt,\n        MetaRecoveryContext recovery, String contStr)\n        throws IOException, RequestStopException\n  {\n    if (recovery == null) {\n      throw new IOException(prompt);\n    }\n    LOG.error(prompt);\n    String answer = recovery.ask(\"\\nEnter 'c' to continue, \" + contStr + \"\\n\" +\n      \"Enter 's' to stop reading the edit log here, abandoning any later \" +\n        \"edits\\n\" +\n      \"Enter 'q' to quit without saving\\n\" +\n      \"Enter 'a' to always select the first choice in the future \" +\n      \"without prompting. \" + \n      \"(c/s/q/a)\\n\", \"c\", \"s\", \"q\", \"a\");\n    if (answer.equals(\"c\")) {\n      LOG.info(\"Continuing\");\n      return;\n    } else if (answer.equals(\"s\")) {\n      throw new RequestStopException(\"user requested stop\");\n    } else if (answer.equals(\"q\")) {\n      recovery.quit();\n    } else {\n      recovery.setForce(FORCE_FIRST_CHOICE);\n      return;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.quit": "  public void quit() {\n    LOG.error(\"Exiting on user request.\");\n    System.exit(0);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.ask": "  public String ask(String prompt, String firstChoice, String... choices) \n      throws IOException {\n    while (true) {\n      LOG.info(prompt);\n      if (force > FORCE_NONE) {\n        LOG.info(\"automatically choosing \" + firstChoice);\n        return firstChoice;\n      }\n      StringBuilder responseBuilder = new StringBuilder();\n      while (true) {\n        int c = System.in.read();\n        if (c == -1 || c == '\\r' || c == '\\n') {\n          break;\n        }\n        responseBuilder.append((char)c);\n      }\n      String response = responseBuilder.toString();\n      if (response.equalsIgnoreCase(firstChoice))\n        return firstChoice;\n      for (String c : choices) {\n        if (response.equalsIgnoreCase(c)) {\n          return c;\n        }\n      }\n      LOG.error(\"I'm sorry, I cannot understand your response.\\n\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.setForce": "  public void setForce(int force) {\n    this.force = force;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.hasTransactionId": "  public boolean hasTransactionId() {\n    return (txid != HdfsConstants.INVALID_TXID);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.writeUnlock": "  void writeUnlock() {\n    this.dirLock.writeLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.writeLock": "  void writeLock() {\n    this.dirLock.writeLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.getTransactionId": "  public long getTransactionId() {\n    Preconditions.checkState(txid != HdfsConstants.INVALID_TXID);\n    return txid;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.closeAllStreams": "  static void closeAllStreams(Iterable<EditLogInputStream> streams) {\n    for (EditLogInputStream s : streams) {\n      IOUtils.closeStream(s);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditLogStreams": "  static Iterable<EditLogInputStream> getEditLogStreams(NNStorage storage)\n      throws IOException {\n    FSImagePreTransactionalStorageInspector inspector \n      = new FSImagePreTransactionalStorageInspector();\n    storage.inspectStorageDirs(inspector);\n\n    List<EditLogInputStream> editStreams = new ArrayList<EditLogInputStream>();\n    for (File f : inspector.getLatestEditsFiles()) {\n      editStreams.add(new EditLogFileInputStream(f));\n    }\n    return editStreams;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getLatestEditsFiles": "  private List<File> getLatestEditsFiles() {\n    if (latestNameCheckpointTime > latestEditsCheckpointTime) {\n      // the image is already current, discard edits\n      LOG.debug(\n          \"Name checkpoint time is newer than edits, not loading edits.\");\n      return Collections.<File>emptyList();\n    }\n    \n    return getEditsInStorageDir(latestEditsSD);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getCheckpointTxId": "    public long getCheckpointTxId() {\n      return txId;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getLatestImage": "  abstract FSImageFile getLatestImage() throws IOException;\n\n  /** \n   * Get the minimum tx id which should be loaded with this set of images.\n   */\n  abstract long getMaxSeenTxId();\n\n  /**\n   * @return true if the directories are in such a state that the image should be re-saved\n   * following the load\n   */\n  abstract boolean needToSave();\n\n  /**\n   * Record of an image that has been located and had its filename parsed.\n   */\n  static class FSImageFile {\n    final StorageDirectory sd;    \n    final long txId;\n    private final File file;\n    \n    FSImageFile(StorageDirectory sd, File file, long txId) {\n      assert txId >= 0 || txId == HdfsConstants.INVALID_TXID \n        : \"Invalid txid on \" + file +\": \" + txId;\n      \n      this.sd = sd;\n      this.txId = txId;\n      this.file = file;\n    } \n    \n    File getFile() {\n      return file;\n    }\n\n    public long getCheckpointTxId() {\n      return txId;\n    }\n    \n    @Override\n    public String toString() {\n      return String.format(\"FSImageFile(file=%s, cpktTxId=%019d)\", \n                           file.toString(), txId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getFile": "    File getFile() {\n      return file;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.needToSave": "  abstract boolean needToSave();\n\n  /**\n   * Record of an image that has been located and had its filename parsed.\n   */\n  static class FSImageFile {\n    final StorageDirectory sd;    \n    final long txId;\n    private final File file;\n    \n    FSImageFile(StorageDirectory sd, File file, long txId) {\n      assert txId >= 0 || txId == HdfsConstants.INVALID_TXID \n        : \"Invalid txid on \" + file +\": \" + txId;\n      \n      this.sd = sd;\n      this.txId = txId;\n      this.file = file;\n    } \n    \n    File getFile() {\n      return file;\n    }\n\n    public long getCheckpointTxId() {\n      return txId;\n    }\n    \n    @Override\n    public String toString() {\n      return String.format(\"FSImageFile(file=%s, cpktTxId=%019d)\", \n                           file.toString(), txId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getMaxSeenTxId": "  abstract long getMaxSeenTxId();\n\n  /**\n   * @return true if the directories are in such a state that the image should be re-saved\n   * following the load\n   */\n  abstract boolean needToSave();\n\n  /**\n   * Record of an image that has been located and had its filename parsed.\n   */\n  static class FSImageFile {\n    final StorageDirectory sd;    \n    final long txId;\n    private final File file;\n    \n    FSImageFile(StorageDirectory sd, File file, long txId) {\n      assert txId >= 0 || txId == HdfsConstants.INVALID_TXID \n        : \"Invalid txid on \" + file +\": \" + txId;\n      \n      this.sd = sd;\n      this.txId = txId;\n      this.file = file;\n    } \n    \n    File getFile() {\n      return file;\n    }\n\n    public long getCheckpointTxId() {\n      return txId;\n    }\n    \n    @Override\n    public String toString() {\n      return String.format(\"FSImageFile(file=%s, cpktTxId=%019d)\", \n                           file.toString(), txId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.format": "  void format(FSNamesystem fsn, String clusterId) throws IOException {\n    long fileCount = fsn.getTotalFiles();\n    // Expect 1 file, which is the root inode\n    Preconditions.checkState(fileCount == 1,\n        \"FSImage.format should be called with an uninitialized namesystem, has \" +\n        fileCount + \" files\");\n    NamespaceInfo ns = NNStorage.newNamespaceInfo();\n    ns.clusterID = clusterId;\n    \n    storage.format(ns);\n    editLog.formatNonFileJournals(ns);\n    saveFSImageInAllDirs(fsn, 0);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImageInAllDirs": "  protected synchronized void saveFSImageInAllDirs(FSNamesystem source, long txid,\n      Canceler canceler)\n      throws IOException {    \n    if (storage.getNumStorageDirs(NameNodeDirType.IMAGE) == 0) {\n      throw new IOException(\"No image directories available!\");\n    }\n    if (canceler == null) {\n      canceler = new Canceler();\n    }\n    SaveNamespaceContext ctx = new SaveNamespaceContext(\n        source, txid, canceler);\n    \n    try {\n      List<Thread> saveThreads = new ArrayList<Thread>();\n      // save images into current\n      for (Iterator<StorageDirectory> it\n             = storage.dirIterator(NameNodeDirType.IMAGE); it.hasNext();) {\n        StorageDirectory sd = it.next();\n        FSImageSaver saver = new FSImageSaver(ctx, sd);\n        Thread saveThread = new Thread(saver, saver.toString());\n        saveThreads.add(saveThread);\n        saveThread.start();\n      }\n      waitForThreads(saveThreads);\n      saveThreads.clear();\n      storage.reportErrorsOnDirectories(ctx.getErrorSDs());\n  \n      if (storage.getNumStorageDirs(NameNodeDirType.IMAGE) == 0) {\n        throw new IOException(\n          \"Failed to save in any storage directories while saving namespace.\");\n      }\n      if (canceler.isCancelled()) {\n        deleteCancelledCheckpoint(txid);\n        ctx.checkCancelled(); // throws\n        assert false : \"should have thrown above!\";\n      }\n  \n      renameCheckpoint(txid);\n  \n      // Since we now have a new checkpoint, we can clean up some\n      // old edit logs and checkpoints.\n      purgeOldStorage();\n    } finally {\n      // Notify any threads waiting on the checkpoint to be canceled\n      // that it is complete.\n      ctx.markComplete();\n      ctx = null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.openEditLogForWrite": "  void openEditLogForWrite() throws IOException {\n    assert editLog != null : \"editLog must be initialized\";\n    editLog.openForWrite();\n    storage.writeTransactionIdFileToStorage(editLog.getCurSegmentTxId());\n  };",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getStorage": "  public NNStorage getStorage() {\n    return storage;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getStartupOption": "  static StartupOption getStartupOption(Configuration conf) {\n    return StartupOption.valueOf(conf.get(DFS_NAMENODE_STARTUP_KEY,\n                                          StartupOption.REGULAR.toString()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getNameNodeMetrics": "  public static NameNodeMetrics getNameNodeMetrics() {\n    return metrics;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.newClusterID": "  public static String newClusterID() {\n    return \"CID-\" + UUID.randomUUID().toString();\n  }"
        },
        "bug_report": {
            "Title": "Precondition in EditLogFileInputStream's length() method is checked too early in NameNode startup, causing fatal exception",
            "Description": "When bringing up a namenode in standby mode, where DEBUG is enabled for namenode, the namenode will hit the following code in {{hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java}}:\n\n{code}\n if (LOG.isDebugEnabled()) {\n      LOG.debug(\"edit log length: \" + in.length() + \", start txid: \"\n          + expectedStartingTxId + \", last txid: \" + lastTxId);\n    }\n{code}.\n\nHowever, if {{in}} has an {{EditLogFileInputStream}} as its {{streams[0]}}, this code is hit before the {{EditLogFileInputStream}}'s {{advertizedSize}} is initialized (before the HTTP client connects to the remote edit log server (i.e. the journal node)). This causes the following precondition to fail in {{EditLogFileInputStream:length()}}:\n\n{code}\n      Preconditions.checkState(advertisedSize != -1,\n          \"must get input stream before length is available\");\n{code}\n\nwhich shuts down the namenode with the following log messages and stack trace:\n\n{code}\n2012-12-11 10:45:33,319 DEBUG ipc.ProtobufRpcEngine (ProtobufRpcEngine.java:invoke(217)) - Call: getEditLogManifest took 88ms\n2012-12-11 10:45:33,336 DEBUG client.QuorumJournalManager (QuorumJournalManager.java:selectInputStreams(459)) - selectInputStream manifests:\n172.16.175.1:8485: [[1,3]]\n2012-12-11 10:45:33,351 DEBUG namenode.FSImage (FSImage.java:loadFSImage(605)) - Planning to load image :\nFSImageFile(file=/tmp/hadoop-data/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)\n2012-12-11 10:45:33,351 DEBUG namenode.FSImage (FSImage.java:loadFSImage(607)) - Planning to load edit log stream: org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@465098f9\n2012-12-11 10:45:33,355 INFO  namenode.FSImage (FSImageFormat.java:load(168)) - Loading image file /tmp/hadoop-data/dfs/name/current/fsimage_0000000000000000000 using no compression\n2012-12-11 10:45:33,355 INFO  namenode.FSImage (FSImageFormat.java:load(171)) - Number of files = 1\n2012-12-11 10:45:33,356 INFO  namenode.FSImage (FSImageFormat.java:loadFilesUnderConstruction(383)) - Number of files under construction = 0\n2012-12-11 10:45:33,357 INFO  namenode.FSImage (FSImageFormat.java:load(193)) - Image file of size 119 loaded in 0 seconds.\n2012-12-11 10:45:33,357 INFO  namenode.FSImage (FSImage.java:loadFSImage(753)) - Loaded image for txid 0 from /tmp/hadoop-data/dfs/name/current/fsimage_0000000000000000000\n2012-12-11 10:45:33,357 DEBUG namenode.FSImage (FSImage.java:loadEdits(686)) - About to load edits:\n  org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@465098f9\n2012-12-11 10:45:33,359 INFO  namenode.FSImage (FSImage.java:loadEdits(694)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@465098f9 expecting start txid #1\n2012-12-11 10:45:33,361 DEBUG ipc.Client (Client.java:stop(1060)) - Stopping client\n2012-12-11 10:45:33,363 DEBUG ipc.Client (Client.java:close(1016)) - IPC Client (1462017562) connection to Eugenes-MacBook-Pro.local/172.16.175.1:8485 from hdfs/eugenes-macbook-pro.local@EXAMPLE.COM: closed\n2012-12-11 10:45:33,363 DEBUG ipc.Client (Client.java:run(848)) - IPC Client (1462017562) connection to Eugenes-MacBook-Pro.local/172.16.175.1:8485 from hdfs/eugenes-macbook-pro.local@EXAMPLE.COM: stopped, remaining connections 0\n2012-12-11 10:45:33,464 FATAL namenode.NameNode (NameNode.java:main(1224)) - Exception in namenode join\njava.lang.IllegalStateException: must get input stream before length is available\n        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)\n        at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$URLLog.length(EditLogFileInputStream.java:405)\n        at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.length(EditLogFileInputStream.java:258)\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:125)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:88)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:697)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:259)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:604)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:447)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:409)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:400)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:434)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:606)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:591)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1153)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1219)\n2012-12-11 10:45:33,470 INFO  util.ExitUtil (ExitUtil.java:terminate(84)) - Exiting with status 1\n2012-12-11 10:45:33,471 INFO  namenode.NameNode (StringUtils.java:run(620)) - SHUTDOWN_MSG:\n/************************************************************\nSHUTDOWN_MSG: Shutting down NameNode at Eugenes-MacBook-Pro.local/172.16.175.1\n************************************************************/\n{code}\n\n\n"
        }
    },
    {
        "filename": "HDFS-11849.json",
        "creation_time": "2017-05-18T11:31:48.000+0000",
        "stack_trace": "```\nException in thread \"main\" java.io.IOException: Login failure for xxx/yyyy@ZZZZ.COM from keytab dummy.keytab: javax.security.auth.login.LoginException: host1\n        at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:994)\n        at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:281)\n        at org.apache.hadoop.hdfs.qjournal.server.JournalNode.start(JournalNode.java:153)\n        at org.apache.hadoop.hdfs.qjournal.server.JournalNode.run(JournalNode.java:132)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)\n        at org.apache.hadoop.hdfs.qjournal.server.JournalNode.main(JournalNode.java:318)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab": "  static void loginUserFromKeytab(String user,\n                                  String path\n                                  ) throws IOException {\n    if (!isSecurityEnabled())\n      return;\n\n    keytabFile = path;\n    keytabPrincipal = user;\n    Subject subject = new Subject();\n    LoginContext login; \n    long start = 0;\n    try {\n      login = newLoginContext(HadoopConfiguration.KEYTAB_KERBEROS_CONFIG_NAME,\n            subject, new HadoopConfiguration());\n      start = Time.now();\n      login.login();\n      metrics.loginSuccess.add(Time.now() - start);\n      loginUser = new UserGroupInformation(subject, false);\n      loginUser.setLogin(login);\n      loginUser.setAuthenticationMethod(AuthenticationMethod.KERBEROS);\n    } catch (LoginException le) {\n      if (start > 0) {\n        metrics.loginFailure.add(Time.now() - start);\n      }\n      KerberosAuthException kae = new KerberosAuthException(LOGIN_FAILURE, le);\n      kae.setUser(user);\n      kae.setKeytabFile(path);\n      throw kae;\n    }\n    LOG.info(\"Login successful for user \" + keytabPrincipal\n        + \" using keytab file \" + keytabFile);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.setLogin": "  private void setLogin(LoginContext login) {\n    user.setLogin(login);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.login": "    public boolean login() throws LoginException {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"hadoop login\");\n      }\n      return true;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.newLoginContext": "  private static LoginContext\n  newLoginContext(String appName, Subject subject,\n    javax.security.auth.login.Configuration loginConf)\n      throws LoginException {\n    // Temporarily switch the thread's ContextClassLoader to match this\n    // class's classloader, so that we can properly load HadoopLoginModule\n    // from the JAAS libraries.\n    Thread t = Thread.currentThread();\n    ClassLoader oldCCL = t.getContextClassLoader();\n    t.setContextClassLoader(HadoopLoginModule.class.getClassLoader());\n    try {\n      return new LoginContext(appName, subject, null, loginConf);\n    } finally {\n      t.setContextClassLoader(oldCCL);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.setAuthenticationMethod": "  public void setAuthenticationMethod(AuthMethod authMethod) {\n    user.setAuthenticationMethod(AuthenticationMethod.valueOf(authMethod));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled": "  public static boolean isSecurityEnabled() {\n    return !isAuthenticationMethodEnabled(AuthenticationMethod.SIMPLE);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SecurityUtil.login": "  public static void login(final Configuration conf,\n      final String keytabFileKey, final String userNameKey, String hostname)\n      throws IOException {\n    \n    if(! UserGroupInformation.isSecurityEnabled()) \n      return;\n    \n    String keytabFilename = conf.get(keytabFileKey);\n    if (keytabFilename == null || keytabFilename.length() == 0) {\n      throw new IOException(\"Running in secure mode, but config doesn't have a keytab\");\n    }\n\n    String principalConfig = conf.get(userNameKey, System\n        .getProperty(\"user.name\"));\n    String principalName = SecurityUtil.getServerPrincipal(principalConfig,\n        hostname);\n    UserGroupInformation.loginUserFromKeytab(principalName, keytabFilename);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SecurityUtil.getServerPrincipal": "  public static String getServerPrincipal(String principalConfig,\n      InetAddress addr) throws IOException {\n    String[] components = getComponents(principalConfig);\n    if (components == null || components.length != 3\n        || !components[1].equals(HOSTNAME_PATTERN)) {\n      return principalConfig;\n    } else {\n      if (addr == null) {\n        throw new IOException(\"Can't replace \" + HOSTNAME_PATTERN\n            + \" pattern since client address is null\");\n      }\n      return replacePattern(components, addr.getCanonicalHostName());\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SecurityUtil.getLocalHostName": "  static String getLocalHostName(@Nullable Configuration conf)\n      throws UnknownHostException {\n    if (conf != null) {\n      String dnsInterface = conf.get(HADOOP_SECURITY_DNS_INTERFACE_KEY);\n      String nameServer = conf.get(HADOOP_SECURITY_DNS_NAMESERVER_KEY);\n\n      if (dnsInterface != null) {\n        return DNS.getDefaultHost(dnsInterface, nameServer, true);\n      } else if (nameServer != null) {\n        throw new IllegalArgumentException(HADOOP_SECURITY_DNS_NAMESERVER_KEY +\n            \" requires \" + HADOOP_SECURITY_DNS_INTERFACE_KEY + \". Check your\" +\n            \"configuration.\");\n      }\n    }\n\n    // Fallback to querying the default hostname as we did before.\n    return InetAddress.getLocalHost().getCanonicalHostName();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.JournalNode.start": "  public void start() throws IOException {\n    Preconditions.checkState(!isStarted(), \"JN already running\");\n    \n    validateAndCreateJournalDir(localDir);\n    \n    DefaultMetricsSystem.initialize(\"JournalNode\");\n    JvmMetrics.create(\"JournalNode\",\n        conf.get(DFSConfigKeys.DFS_METRICS_SESSION_ID_KEY),\n        DefaultMetricsSystem.instance());\n\n    InetSocketAddress socAddr = JournalNodeRpcServer.getAddress(conf);\n    SecurityUtil.login(conf, DFSConfigKeys.DFS_JOURNALNODE_KEYTAB_FILE_KEY,\n        DFSConfigKeys.DFS_JOURNALNODE_KERBEROS_PRINCIPAL_KEY, socAddr.getHostName());\n    \n    registerJNMXBean();\n    \n    httpServer = new JournalNodeHttpServer(conf, this);\n    httpServer.start();\n\n    httpServerURI = httpServer.getServerURI().toString();\n\n    rpcServer = new JournalNodeRpcServer(conf, this);\n    rpcServer.start();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.JournalNode.registerJNMXBean": "  private void registerJNMXBean() {\n    journalNodeInfoBeanName = MBeans.register(\"JournalNode\", \"JournalNodeInfo\", this);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.JournalNode.validateAndCreateJournalDir": "  private static void validateAndCreateJournalDir(File dir) throws IOException {\n    if (!dir.isAbsolute()) {\n      throw new IllegalArgumentException(\n          \"Journal dir '\" + dir + \"' should be an absolute path\");\n    }\n\n    DiskChecker.checkDir(dir);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.JournalNode.isStarted": "  public boolean isStarted() {\n    return rpcServer != null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.JournalNode.run": "  public int run(String[] args) throws Exception {\n    start();\n    return join();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.JournalNode.join": "  int join() throws InterruptedException {\n    if (rpcServer != null) {\n      rpcServer.join();\n    }\n    return resultCode;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ToolRunner.run": "  public static int run(Tool tool, String[] args) \n    throws Exception{\n    return run(tool.getConf(), tool, args);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.JournalNode.main": "  public static void main(String[] args) throws Exception {\n    StringUtils.startupShutdownMessage(JournalNode.class, args, LOG);\n    System.exit(ToolRunner.run(new JournalNode(), args));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.KerberosAuthException.setUser": "  public void setUser(final String u) {\n    user = u;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.KerberosAuthException.setKeytabFile": "  public void setKeytabFile(final String k) {\n    keytabFile = k;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.getAddress": "  static InetSocketAddress getAddress(Configuration conf) {\n    String addr = conf.get(\n        DFSConfigKeys.DFS_JOURNALNODE_RPC_ADDRESS_KEY,\n        DFSConfigKeys.DFS_JOURNALNODE_RPC_ADDRESS_DEFAULT);\n    return NetUtils.createSocketAddr(addr, 0,\n        DFSConfigKeys.DFS_JOURNALNODE_RPC_ADDRESS_KEY);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs": "  public String[] getRemainingArgs() {\n    return (commandLine == null) ? new String[]{} : commandLine.getArgs();\n  }"
        },
        "bug_report": {
            "Title": "JournalNode startup failure exception should be logged in log file",
            "Description": "JournalNode failed to start because of kerberos login. \n{noformat}\nException in thread \"main\" java.io.IOException: Login failure for xxx/yyyy@ZZZZ.COM from keytab dummy.keytab: javax.security.auth.login.LoginException: host1\n        at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:994)\n        at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:281)\n        at org.apache.hadoop.hdfs.qjournal.server.JournalNode.start(JournalNode.java:153)\n        at org.apache.hadoop.hdfs.qjournal.server.JournalNode.run(JournalNode.java:132)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)\n        at org.apache.hadoop.hdfs.qjournal.server.JournalNode.main(JournalNode.java:318)\n{noformat}\n\nbut this exception is not written in log file.\n\n{noformat}\nSTARTUP_MSG:   java = 1.x.x\n************************************************************/\n2017-05-18 16:08:14,961 INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: registered UNIX signal handlers for [TERM, HUP, INT]\n2017-05-18 16:08:15,511 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2017-05-18 16:08:15,660 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2017-05-18 16:08:15,660 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: JournalNode metrics system started\n2017-05-18 16:08:16,429 INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: SHUTDOWN_MSG:\n/************************************************************\nSHUTDOWN_MSG: Shutting down JournalNode at w-x-y-z\n************************************************************/\n{noformat}"
        }
    },
    {
        "filename": "HDFS-4841.json",
        "creation_time": "2013-05-22T17:01:36.000+0000",
        "stack_trace": "```\njava.lang.IllegalStateException: Shutdown in progress, cannot add a shutdownHook\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:152)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2400)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2372)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:352)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$DtRenewer.getWebHdfs(WebHdfsFileSystem.java:1001)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$DtRenewer.cancel(WebHdfsFileSystem.java:1013)\n\tat org.apache.hadoop.security.token.Token.cancel(Token.java:382)\n\tat org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction.cancel(DelegationTokenRenewer.java:152)\n\tat org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction.access$200(DelegationTokenRenewer.java:58)\n\tat org.apache.hadoop.fs.DelegationTokenRenewer.removeRenewAction(DelegationTokenRenewer.java:241)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.close(WebHdfsFileSystem.java:822)\n\tat org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2446)\n\tat org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2463)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ShutdownHookManager.addShutdownHook": "  public void addShutdownHook(Runnable shutdownHook, int priority) {\n    if (shutdownHook == null) {\n      throw new IllegalArgumentException(\"shutdownHook cannot be NULL\");\n    }\n    if (shutdownInProgress.get()) {\n      throw new IllegalStateException(\"Shutdown in progress, cannot add a shutdownHook\");\n    }\n    hooks.add(new HookEntry(shutdownHook, priority));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ShutdownHookManager.get": "  public static ShutdownHookManager get() {\n    return MGR;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getInternal": "    private FileSystem getInternal(URI uri, Configuration conf, Key key) throws IOException{\n      FileSystem fs;\n      synchronized (this) {\n        fs = map.get(key);\n      }\n      if (fs != null) {\n        return fs;\n      }\n\n      fs = createFileSystem(uri, conf);\n      synchronized (this) { // refetch the lock again\n        FileSystem oldfs = map.get(key);\n        if (oldfs != null) { // a file system is created while lock is releasing\n          fs.close(); // close the new file system\n          return oldfs;  // return the old file system\n        }\n        \n        // now insert the new file system into the map\n        if (map.isEmpty() ) {\n          ShutdownHookManager.get().addShutdownHook(clientFinalizer, SHUTDOWN_HOOK_PRIORITY);\n        }\n        fs.key = key;\n        map.put(key, fs);\n        if (conf.getBoolean(\"fs.automatic.close\", true)) {\n          toAutoClose.add(key);\n        }\n        return fs;\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.createFileSystem": "  private static FileSystem createFileSystem(URI uri, Configuration conf\n      ) throws IOException {\n    Class<?> clazz = getFileSystemClass(uri.getScheme(), conf);\n    if (clazz == null) {\n      throw new IOException(\"No FileSystem for scheme: \" + uri.getScheme());\n    }\n    FileSystem fs = (FileSystem)ReflectionUtils.newInstance(clazz, conf);\n    fs.initialize(uri, conf);\n    return fs;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.close": "  public void close() throws IOException {\n    // delete all files that were marked as delete-on-exit.\n    processDeleteOnExit();\n    CACHE.remove(this.key, this);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.get": "    FileSystem get(URI uri, Configuration conf) throws IOException{\n      Key key = new Key(uri, conf);\n      return getInternal(uri, conf, key);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getWebHdfs": "    private static WebHdfsFileSystem getWebHdfs(\n        final Token<?> token, final Configuration conf) throws IOException {\n      \n      final InetSocketAddress nnAddr = SecurityUtil.getTokenServiceAddr(token);\n      final URI uri = DFSUtil.createUri(WebHdfsFileSystem.SCHEME, nnAddr);\n      return (WebHdfsFileSystem)FileSystem.get(uri, conf);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.WebHdfsFileSystem.cancel": "    public void cancel(final Token<?> token, final Configuration conf\n        ) throws IOException, InterruptedException {\n      getWebHdfs(token, conf).cancelDelegationToken(token);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.WebHdfsFileSystem.cancelDelegationToken": "  private synchronized void cancelDelegationToken(final Token<?> token\n      ) throws IOException {\n    final HttpOpParam.Op op = PutOpParam.Op.CANCELDELEGATIONTOKEN;\n    TokenArgumentParam dtargParam = new TokenArgumentParam(\n        token.encodeToUrlString());\n    run(op, null, dtargParam);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.DelegationTokenRenewer.cancel": "    private void cancel() throws IOException, InterruptedException {\n      final T fs = weakFs.get();\n      if (fs != null) {\n        token.cancel(fs.getConf());\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.DelegationTokenRenewer.removeRenewAction": "  public <T extends FileSystem & Renewable> void removeRenewAction(\n      final T fs) throws IOException {\n    RenewAction<T> action = new RenewAction<T>(fs);\n    if (queue.remove(action)) {\n      try {\n        action.cancel();\n      } catch (InterruptedException ie) {\n        LOG.error(\"Interrupted while canceling token for \" + fs.getUri()\n            + \"filesystem\");\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(ie.getStackTrace());\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.WebHdfsFileSystem.close": "  public void close() throws IOException {\n    super.close();\n    if (dtRenewer != null) {\n      dtRenewer.removeRenewAction(this); // blocks\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.WebHdfsFileSystem.disconnect": "    private void disconnect() {\n      if (conn != null) {\n        conn.disconnect();\n        conn = null;\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse": "  private static Map<?, ?> validateResponse(final HttpOpParam.Op op,\n      final HttpURLConnection conn, boolean unwrapException) throws IOException {\n    final int code = conn.getResponseCode();\n    if (code != op.getExpectedHttpResponseCode()) {\n      final Map<?, ?> m;\n      try {\n        m = jsonParse(conn, true);\n      } catch(Exception e) {\n        throw new IOException(\"Unexpected HTTP response: code=\" + code + \" != \"\n            + op.getExpectedHttpResponseCode() + \", \" + op.toQueryString()\n            + \", message=\" + conn.getResponseMessage(), e);\n      }\n\n      if (m == null) {\n        throw new IOException(\"Unexpected HTTP response: code=\" + code + \" != \"\n            + op.getExpectedHttpResponseCode() + \", \" + op.toQueryString()\n            + \", message=\" + conn.getResponseMessage());\n      } else if (m.get(RemoteException.class.getSimpleName()) == null) {\n        return m;\n      }\n\n      final RemoteException re = JsonUtil.toRemoteException(m);\n      throw unwrapException? toIOException(re): re;\n    }\n    return null;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.closeAll": "    synchronized void closeAll(UserGroupInformation ugi) throws IOException {\n      List<FileSystem> targetFSList = new ArrayList<FileSystem>();\n      //Make a pass over the list and collect the filesystems to close\n      //we cannot close inline since close() removes the entry from the Map\n      for (Map.Entry<Key, FileSystem> entry : map.entrySet()) {\n        final Key key = entry.getKey();\n        final FileSystem fs = entry.getValue();\n        if (ugi.equals(key.ugi) && fs != null) {\n          targetFSList.add(fs);   \n        }\n      }\n      List<IOException> exceptions = new ArrayList<IOException>();\n      //now make a pass over the target list and close each\n      for (FileSystem fs : targetFSList) {\n        try {\n          fs.close();\n        }\n        catch(IOException ioe) {\n          exceptions.add(ioe);\n        }\n      }\n      if (!exceptions.isEmpty()) {\n        throw MultipleIOException.createIOException(exceptions);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.remove": "    synchronized void remove(Key key, FileSystem fs) {\n      if (map.containsKey(key) && fs == map.get(key)) {\n        map.remove(key);\n        toAutoClose.remove(key);\n        }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.equals": "      public boolean equals(Object obj) {\n        if (obj == this) {\n          return true;\n        }\n        if (obj != null && obj instanceof Key) {\n          Key that = (Key)obj;\n          return isEqual(this.scheme, that.scheme)\n                 && isEqual(this.authority, that.authority)\n                 && isEqual(this.ugi, that.ugi)\n                 && (this.unique == that.unique);\n        }\n        return false;        \n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.run": "      public synchronized void run() {\n        try {\n          closeAll(true);\n        } catch (IOException e) {\n          LOG.info(\"FileSystem.Cache.closeAll() threw an exception:\\n\" + e);\n        }\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.newInstance": "  public static FileSystem newInstance(Configuration conf) throws IOException {\n    return newInstance(getDefaultUri(conf), conf);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ShutdownHookManager.run": "        public void run() {\n          MGR.shutdownInProgress.set(true);\n          for (Runnable hook: MGR.getShutdownHooksInOrder()) {\n            try {\n              hook.run();\n            } catch (Throwable ex) {\n              LOG.warn(\"ShutdownHook '\" + hook.getClass().getSimpleName() +\n                       \"' failed, \" + ex.toString(), ex);\n            }\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ShutdownHookManager.getShutdownHooksInOrder": "  List<Runnable> getShutdownHooksInOrder() {\n    List<HookEntry> list;\n    synchronized (MGR.hooks) {\n      list = new ArrayList<HookEntry>(MGR.hooks);\n    }\n    Collections.sort(list, new Comparator<HookEntry>() {\n\n      //reversing comparison so highest priority hooks are first\n      @Override\n      public int compare(HookEntry o1, HookEntry o2) {\n        return o2.priority - o1.priority;\n      }\n    });\n    List<Runnable> ordered = new ArrayList<Runnable>();\n    for (HookEntry entry: list) {\n      ordered.add(entry.hook);\n    }\n    return ordered;\n  }"
        },
        "bug_report": {
            "Title": "FsShell commands using secure webhfds fail ClientFinalizer shutdown hook",
            "Description": "Hadoop version:\n{code}\nbash-4.1$ $HADOOP_HOME/bin/hadoop version\nHadoop 3.0.0-SNAPSHOT\nSubversion git://github.com/apache/hadoop-common.git -r d5373b9c550a355d4e91330ba7cc8f4c7c3aac51\nCompiled by root on 2013-05-22T08:06Z\nFrom source with checksum 8c4cc9b1e8d6e8361431e00f64483f\nThis command was run using /var/lib/hadoop-hdfs/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/hadoop-common-3.0.0-SNAPSHOT.jar\n{code}\n\nI'm seeing a problem when issuing FsShell commands using the webhdfs:// URI when security is enabled. The command completes but leaves a warning that ShutdownHook 'ClientFinalizer' failed.\n\n{code}\nbash-4.1$ hadoop-3.0.0-SNAPSHOT/bin/hadoop fs -ls webhdfs://hdfs-upgrade-pseudo.ent.cloudera.com:50070/\n2013-05-22 09:46:55,710 INFO  [main] util.Shell (Shell.java:isSetsidSupported(311)) - setsid exited with exit code 0\nFound 3 items\ndrwxr-xr-x   - hbase supergroup          0 2013-05-22 09:46 webhdfs://hdfs-upgrade-pseudo.ent.cloudera.com:50070/hbase\ndrwxr-xr-x   - hdfs  supergroup          0 2013-05-22 09:46 webhdfs://hdfs-upgrade-pseudo.ent.cloudera.com:50070/tmp\ndrwxr-xr-x   - hdfs  supergroup          0 2013-05-22 09:46 webhdfs://hdfs-upgrade-pseudo.ent.cloudera.com:50070/user\n2013-05-22 09:46:58,660 WARN  [Thread-3] util.ShutdownHookManager (ShutdownHookManager.java:run(56)) - ShutdownHook 'ClientFinalizer' failed, java.lang.IllegalStateException: Shutdown in progress, cannot add a shutdownHook\njava.lang.IllegalStateException: Shutdown in progress, cannot add a shutdownHook\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:152)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2400)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2372)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:352)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$DtRenewer.getWebHdfs(WebHdfsFileSystem.java:1001)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$DtRenewer.cancel(WebHdfsFileSystem.java:1013)\n\tat org.apache.hadoop.security.token.Token.cancel(Token.java:382)\n\tat org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction.cancel(DelegationTokenRenewer.java:152)\n\tat org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction.access$200(DelegationTokenRenewer.java:58)\n\tat org.apache.hadoop.fs.DelegationTokenRenewer.removeRenewAction(DelegationTokenRenewer.java:241)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.close(WebHdfsFileSystem.java:822)\n\tat org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2446)\n\tat org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2463)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n{code}\n\nI've checked that FsShell + hdfs:// commands and WebHDFS operations through curl work successfully:\n\n{code}\nbash-4.1$ hadoop-3.0.0-SNAPSHOT/bin/hadoop fs -ls /\n2013-05-22 09:46:43,663 INFO  [main] util.Shell (Shell.java:isSetsidSupported(311)) - setsid exited with exit code 0\nFound 3 items\ndrwxr-xr-x   - hbase supergroup          0 2013-05-22 09:46 /hbase\ndrwxr-xr-x   - hdfs  supergroup          0 2013-05-22 09:46 /tmp\ndrwxr-xr-x   - hdfs  supergroup          0 2013-05-22 09:46 /user\nbash-4.1$ curl -i --negotiate -u : \"http://hdfs-upgrade-pseudo.ent.cloudera.com:50070/webhdfs/v1/?op=GETHOMEDIRECTORY\"\nHTTP/1.1 401 \nCache-Control: must-revalidate,no-cache,no-store\nDate: Wed, 22 May 2013 16:47:14 GMT\nPragma: no-cache\nDate: Wed, 22 May 2013 16:47:14 GMT\nPragma: no-cache\nContent-Type: text/html; charset=iso-8859-1\nWWW-Authenticate: Negotiate\nSet-Cookie: hadoop.auth=;Path=/;Expires=Thu, 01-Jan-1970 00:00:00 GMT\nContent-Length: 1358\nServer: Jetty(6.1.26)\n\nHTTP/1.1 200 OK\nCache-Control: no-cache\nExpires: Thu, 01-Jan-1970 00:00:00 GMT\nDate: Wed, 22 May 2013 16:47:14 GMT\nPragma: no-cache\nDate: Wed, 22 May 2013 16:47:14 GMT\nPragma: no-cache\nContent-Type: application/json\nSet-Cookie: hadoop.auth=\"u=hdfs&p=hdfs/hdfs-upgrade-pseudo.ent.cloudera.com@ENT.CLOUDERA.COM&t=kerberos&e=1369277234852&s=m3vJ7/pV831tBLkpOBb0Naa5N+g=\";Path=/\nTransfer-Encoding: chunked\nServer: Jetty(6.1.26)\n\n{\"Path\":\"/user/hdfs\"}bash-4.1$ \n{code}\n\nWhen I disable security, the warning goes away.\n\nI'll attach my core-site.xml, hdfs-site.xml, NN and DN output logs.\n"
        }
    },
    {
        "filename": "HDFS-3384.json",
        "creation_time": "2012-05-08T09:01:10.000+0000",
        "stack_trace": "```\njava.io.EOFException: Premature EOF: no length prefix available\n\tat org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1039)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:939)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:510)\n\njava.io.IOException: All datanodes 10.18.40.20:50010 are bad. Aborting...\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:908)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n\njava.io.IOException: All datanodes 10.18.40.20:50010 are bad. Aborting...\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:908)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed": "  public static InputStream vintPrefixed(final InputStream input)\n  throws IOException {\n    final int firstByte = input.read();\n    if (firstByte == -1) {\n      throw new EOFException(\"Premature EOF: no length prefix available\");\n    }\n    \n    int size = CodedInputStream.readRawVarint32(firstByte, input);\n    assert size >= 0;\n  \n    return new ExactSizeInputStream(input, size);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.createBlockOutputStream": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      Status pipelineStatus = SUCCESS;\n      String firstBadLink = \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i = 0; i < nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline = \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      boolean result = false;\n      DataOutputStream out = null;\n      try {\n        assert null == s : \"Previous socket unclosed\";\n        s = createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout = dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        //\n        // Xmit header info to datanode\n        //\n        out = new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(s, writeTimeout),\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        \n        assert null == blockReplyStream : \"Previous blockReplyStream unclosed\";\n        blockReplyStream = new DataInputStream(NetUtils.getInputStream(s));\n\n        // send the request\n        new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n            nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n            nodes.length, block.getNumBytes(), bytesSent, newGS, checksum);\n\n        // receive ack for connect\n        BlockOpResponseProto resp = BlockOpResponseProto.parseFrom(\n            HdfsProtoUtil.vintPrefixed(blockReplyStream));\n        pipelineStatus = resp.getStatus();\n        firstBadLink = resp.getFirstBadLink();\n        \n        if (pipelineStatus != SUCCESS) {\n          if (pipelineStatus == Status.ERROR_ACCESS_TOKEN) {\n            throw new InvalidBlockTokenException(\n                \"Got access token error for connect ack with firstBadLink as \"\n                    + firstBadLink);\n          } else {\n            throw new IOException(\"Bad connect ack with firstBadLink as \"\n                + firstBadLink);\n          }\n        }\n        assert null == blockStream : \"Previous blockStream unclosed\";\n        blockStream = out;\n        result =  true; // success\n\n      } catch (IOException ie) {\n\n        DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n\n        // find the datanode that matches\n        if (firstBadLink.length() != 0) {\n          for (int i = 0; i < nodes.length; i++) {\n            if (nodes[i].getXferAddr().equals(firstBadLink)) {\n              errorIndex = i;\n              break;\n            }\n          }\n        } else {\n          errorIndex = 0;\n        }\n        hasError = true;\n        setLastException(ie);\n        result =  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s = null;\n          IOUtils.closeStream(out);\n          out = null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream = null;\n        }\n      }\n      return result;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.closeStream": "    private void closeStream() {\n      if (blockStream != null) {\n        try {\n          blockStream.close();\n        } catch (IOException e) {\n          setLastException(e);\n        } finally {\n          blockStream = null;\n        }\n      }\n      if (blockReplyStream != null) {\n        try {\n          blockReplyStream.close();\n        } catch (IOException e) {\n          setLastException(e);\n        } finally {\n          blockReplyStream = null;\n        }\n      }\n      if (null != s) {\n        try {\n          s.close();\n        } catch (IOException e) {\n          setLastException(e);\n        } finally {\n          s = null;\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Connecting to datanode \" + first);\n    }\n    final InetSocketAddress isa =\n      NetUtils.createSocketAddr(first.getXferAddr());\n    final Socket sock = client.socketFactory.createSocket();\n    final int timeout = client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), timeout);\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n    }\n    return sock;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.setLastException": "    private void setLastException(IOException e) {\n      if (lastException == null) {\n        lastException = e;\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.setupPipelineForAppendOrRecovery": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes == null || nodes.length == 0) {\n        String msg = \"Could not get block locations. \" + \"Source file \\\"\"\n            + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed = true;\n        return false;\n      }\n      \n      boolean success = false;\n      long newGS = 0L;\n      while (!success && !streamerClosed && dfsClient.clientRunning) {\n        boolean isRecovery = hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex >= 0) {\n          StringBuilder pipelineMsg = new StringBuilder();\n          for (int j = 0; j < nodes.length; j++) {\n            pipelineMsg.append(nodes[j]);\n            if (j < nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length <= 1) {\n            lastException = new IOException(\"All datanodes \" + pipelineMsg\n                + \" are bad. Aborting...\");\n            streamerClosed = true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n              \" in pipeline \" + pipelineMsg + \n              \": bad datanode \" + nodes[errorIndex]);\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes = new DatanodeInfo[nodes.length-1];\n          System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n          System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n              newnodes.length-errorIndex);\n          nodes = newnodes;\n          hasError = false;\n          lastException = null;\n          errorIndex = -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n            nodes, isAppend, isHflushed)) {\n          addDatanode2ExistingPipeline();\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb = dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS = lb.getBlock().getGenerationStamp();\n        accessToken = lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        success = createBlockOutputStream(nodes, newGS, isRecovery);\n      }\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock = new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock, nodes);\n        // update client side generation stamp\n        block = newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.getBlockToken": "  synchronized Token<BlockTokenIdentifier> getBlockToken() {\n    return streamer.getBlockToken();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.addDatanode2ExistingPipeline": "    private void addDatanode2ExistingPipeline() throws IOException {\n      if (DataTransferProtocol.LOG.isDebugEnabled()) {\n        DataTransferProtocol.LOG.debug(\"lastAckedSeqno = \" + lastAckedSeqno);\n      }\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       * \n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens \n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       * \n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n      if (!isAppend && lastAckedSeqno < 0\n          && stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n        //no data have been written\n        return;\n      } else if (stage == BlockConstructionStage.PIPELINE_CLOSE\n          || stage == BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        //pipeline is closing\n        return;\n      }\n\n      //get a new datanode\n      final DatanodeInfo[] original = nodes;\n      final LocatedBlock lb = dfsClient.namenode.getAdditionalDatanode(\n          src, block, nodes, failed.toArray(new DatanodeInfo[failed.size()]),\n          1, dfsClient.clientName);\n      nodes = lb.getLocations();\n\n      //find the new datanode\n      final int d = findNewDatanode(original);\n\n      //transfer replica\n      final DatanodeInfo src = d == 0? nodes[1]: nodes[d - 1];\n      final DatanodeInfo[] targets = {nodes[d]};\n      transfer(src, targets, lb.getBlockToken());\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.getBlock": "    ExtendedBlock getBlock() {\n      return block;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.run": "      public void run() {\n\n        setName(\"ResponseProcessor for block \" + block);\n        PipelineAck ack = new PipelineAck();\n\n        while (!responderClosed && dfsClient.clientRunning && !isLastPacketInBlock) {\n          // process responses from datanodes.\n          try {\n            // read an ack from the pipeline\n            ack.readFields(blockReplyStream);\n            if (DFSClient.LOG.isDebugEnabled()) {\n              DFSClient.LOG.debug(\"DFSClient \" + ack);\n            }\n            \n            long seqno = ack.getSeqno();\n            // processes response status from datanodes.\n            for (int i = ack.getNumOfReplies()-1; i >=0  && dfsClient.clientRunning; i--) {\n              final Status reply = ack.getReply(i);\n              if (reply != SUCCESS) {\n                errorIndex = i; // first bad datanode\n                throw new IOException(\"Bad response \" + reply +\n                    \" for block \" + block +\n                    \" from datanode \" + \n                    targets[i]);\n              }\n            }\n            \n            assert seqno != PipelineAck.UNKOWN_SEQNO : \n              \"Ack for unkown seqno should be a failed ack: \" + ack;\n            if (seqno == Packet.HEART_BEAT_SEQNO) {  // a heartbeat ack\n              continue;\n            }\n\n            // a success ack for a data packet\n            Packet one = null;\n            synchronized (dataQueue) {\n              one = ackQueue.getFirst();\n            }\n            if (one.seqno != seqno) {\n              throw new IOException(\"Responseprocessor: Expecting seqno \" +\n                                    \" for block \" + block +\n                                    one.seqno + \" but received \" + seqno);\n            }\n            isLastPacketInBlock = one.lastPacketInBlock;\n            // update bytesAcked\n            block.setNumBytes(one.getLastByteOffsetBlock());\n\n            synchronized (dataQueue) {\n              lastAckedSeqno = seqno;\n              ackQueue.removeFirst();\n              dataQueue.notifyAll();\n            }\n          } catch (Exception e) {\n            if (!responderClosed) {\n              if (e instanceof IOException) {\n                setLastException((IOException)e);\n              }\n              hasError = true;\n              errorIndex = errorIndex==-1 ? 0 : errorIndex;\n              synchronized (dataQueue) {\n                dataQueue.notifyAll();\n              }\n              DFSClient.LOG.warn(\"DFSOutputStream ResponseProcessor exception \"\n                  + \" for block \" + block, e);\n              responderClosed = true;\n            }\n          }\n        }\n      }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.endBlock": "    private void endBlock() {\n      if(DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"Closing old block \" + block);\n      }\n      this.setName(\"DataStreamer for file \" + src);\n      closeResponder();\n      closeStream();\n      nodes = null;\n      stage = BlockConstructionStage.PIPELINE_SETUP_CREATE;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.getBuffer": "    ByteBuffer getBuffer() {\n      /* Once this is called, no more data can be added to the packet.\n       * setting 'buf' to null ensures that.\n       * This is called only when the packet is ready to be sent.\n       */\n      if (buffer != null) {\n        return buffer;\n      }\n      \n      //prepare the header and close any gap between checksum and data.\n      \n      int dataLen = dataPos - dataStart;\n      int checksumLen = checksumPos - checksumStart;\n      \n      if (checksumPos != dataStart) {\n        /* move the checksum to cover the gap.\n         * This can happen for the last packet.\n         */\n        System.arraycopy(buf, checksumStart, buf, \n                         dataStart - checksumLen , checksumLen); \n      }\n      \n      int pktLen = HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\n      \n      //normally dataStart == checksumPos, i.e., offset is zero.\n      buffer = ByteBuffer.wrap(\n        buf, dataStart - checksumPos,\n        PacketHeader.PKT_HEADER_LEN + pktLen - HdfsConstants.BYTES_IN_INTEGER);\n      buf = null;\n      buffer.mark();\n\n      PacketHeader header = new PacketHeader(\n        pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen);\n      header.putInBuffer(buffer);\n      \n      buffer.reset();\n      return buffer;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.close": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e = lastException;\n      if (e == null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket != null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock != 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket = new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n            bytesCurBlock);\n        currentPacket.lastPacketInBlock = true;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock = streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.leaserenewer.closeFile(src, dfsClient);\n    } finally {\n      closed = true;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.initDataStreaming": "    private void initDataStreaming() {\n      this.setName(\"DataStreamer for file \" + src +\n          \" block \" + block);\n      response = new ResponseProcessor(nodes);\n      response.start();\n      stage = BlockConstructionStage.DATA_STREAMING;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.closeInternal": "    private void closeInternal() {\n      closeResponder();       // close and join\n      closeStream();\n      streamerClosed = true;\n      closed = true;\n      synchronized (dataQueue) {\n        dataQueue.notifyAll();\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.getLastByteOffsetBlock": "    long getLastByteOffsetBlock() {\n      return offsetInBlock + dataPos - dataStart;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.processDatanodeError": "    private boolean processDatanodeError() throws IOException {\n      if (response != null) {\n        DFSClient.LOG.info(\"Error Recovery for block \" + block +\n        \" waiting for responder to exit. \");\n        return true;\n      }\n      closeStream();\n\n      // move packets from ack queue to front of the data queue\n      synchronized (dataQueue) {\n        dataQueue.addAll(0, ackQueue);\n        ackQueue.clear();\n      }\n\n      boolean doSleep = setupPipelineForAppendOrRecovery();\n      \n      if (!streamerClosed && dfsClient.clientRunning) {\n        if (stage == BlockConstructionStage.PIPELINE_CLOSE) {\n\n          // If we had an error while closing the pipeline, we go through a fast-path\n          // where the BlockReceiver does not run. Instead, the DataNode just finalizes\n          // the block immediately during the 'connect ack' process. So, we want to pull\n          // the end-of-block packet from the dataQueue, since we don't actually have\n          // a true pipeline to send it over.\n          //\n          // We also need to set lastAckedSeqno to the end-of-block Packet's seqno, so that\n          // a client waiting on close() will be aware that the flush finished.\n          synchronized (dataQueue) {\n            assert dataQueue.size() == 1;\n            Packet endOfBlockPacket = dataQueue.remove();  // remove the end of block packet\n            assert endOfBlockPacket.lastPacketInBlock;\n            assert lastAckedSeqno == endOfBlockPacket.seqno - 1;\n            lastAckedSeqno = endOfBlockPacket.seqno;\n            dataQueue.notifyAll();\n          }\n          endBlock();\n        } else {\n          initDataStreaming();\n        }\n      }\n      \n      return doSleep;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.nextBlockOutputStream": "    private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n      LocatedBlock lb = null;\n      DatanodeInfo[] nodes = null;\n      int count = dfsClient.getConf().nBlockWriteRetry;\n      boolean success = false;\n      do {\n        hasError = false;\n        lastException = null;\n        errorIndex = -1;\n        success = false;\n\n        long startTime = System.currentTimeMillis();\n        DatanodeInfo[] w = excludedNodes.toArray(\n            new DatanodeInfo[excludedNodes.size()]);\n        lb = locateFollowingBlock(startTime, w.length > 0 ? w : null);\n        block = lb.getBlock();\n        block.setNumBytes(0);\n        accessToken = lb.getBlockToken();\n        nodes = lb.getLocations();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success = createBlockOutputStream(nodes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning block \" + block);\n          dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n          block = null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.add(nodes[errorIndex]);\n        }\n      } while (!success && --count >= 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return nodes;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.isHeartbeatPacket": "    private boolean isHeartbeatPacket() {\n      return seqno == HEART_BEAT_SEQNO;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.dfsClient.getDatanodeWriteTimeout": "  int getDatanodeWriteTimeout(int numNodes) {\n    return (dfsClientConf.confTime > 0) ?\n      (dfsClientConf.confTime + HdfsServerConstants.WRITE_TIMEOUT_EXTENSION * numNodes) : 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.dfsClient.getConf": "  Conf getConf() {\n    return dfsClientConf;\n  }"
        },
        "bug_report": {
            "Title": "DataStreamer thread should be closed immediatly when failed to setup a PipelineForAppendOrRecovery",
            "Description": "Scenraio:\n=========\nwrite a file\ncorrupt block manually\ncall append..\n\n{noformat}\n\n2012-04-19 09:33:10,776 INFO  hdfs.DFSClient (DFSOutputStream.java:createBlockOutputStream(1059)) - Exception in createBlockOutputStream\njava.io.EOFException: Premature EOF: no length prefix available\n\tat org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1039)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:939)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n2012-04-19 09:33:10,807 WARN  hdfs.DFSClient (DFSOutputStream.java:run(549)) - DataStreamer Exception\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:510)\n2012-04-19 09:33:10,807 WARN  hdfs.DFSClient (DFSOutputStream.java:hflush(1511)) - Error while syncing\njava.io.IOException: All datanodes 10.18.40.20:50010 are bad. Aborting...\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:908)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\njava.io.IOException: All datanodes 10.18.40.20:50010 are bad. Aborting...\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:908)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n{noformat}"
        }
    },
    {
        "filename": "HDFS-5657.json",
        "creation_time": "2013-12-11T21:59:46.000+0000",
        "stack_trace": "```\nERROR nfs3.AsyncDataService (AsyncDataService.java:run(136)) - Asyn data service got error:java.lang.IllegalStateException: The openFileCtx has false async status\n        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)\n        at org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.executeWriteBack(OpenFileCtx.java:890)\n        at org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService$WriteBackTask.run(AsyncDataService.java:134)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:662)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.executeWriteBack": "  void executeWriteBack() {\n    Preconditions.checkState(asyncStatus,\n        \"The openFileCtx has false async status\");\n    try {\n      while (activeState) {\n        WriteCtx toWrite = offerNextToWrite();\n        if (toWrite != null) {\n          // Do the write\n          doSingleWrite(toWrite);\n          updateLastAccessTime();\n        } else {\n          break;\n        }\n      }\n      \n      if (!activeState && LOG.isDebugEnabled()) {\n        LOG.debug(\"The openFileCtx is not active anymore, fileId: \"\n            + latestAttr.getFileId());\n      }\n    } finally {\n      // make sure we reset asyncStatus to false\n      asyncStatus = false;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.offerNextToWrite": "  private synchronized WriteCtx offerNextToWrite() {\n    if (pendingWrites.isEmpty()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"The asyn write task has no pending writes, fileId: \"\n            + latestAttr.getFileId());\n      }\n      // process pending commit again to handle this race: a commit is added\n      // to pendingCommits map just after the last doSingleWrite returns.\n      // There is no pending write and the commit should be handled by the\n      // last doSingleWrite. Due to the race, the commit is left along and\n      // can't be processed until cleanup. Therefore, we should do another\n      // processCommits to fix the race issue.\n      processCommits(nextOffset.get()); // nextOffset has same value as\n                                        // flushedOffset\n      this.asyncStatus = false;\n      return null;\n    } \n    \n      Entry<OffsetRange, WriteCtx> lastEntry = pendingWrites.lastEntry();\n      OffsetRange range = lastEntry.getKey();\n      WriteCtx toWrite = lastEntry.getValue();\n      \n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"range.getMin()=\" + range.getMin() + \" nextOffset=\"\n            + nextOffset);\n      }\n      \n      long offset = nextOffset.get();\n      if (range.getMin() > offset) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"The next sequencial write has not arrived yet\");\n        }\n        processCommits(nextOffset.get()); // handle race\n        this.asyncStatus = false;\n      } else if (range.getMin() < offset && range.getMax() > offset) {\n        // shouldn't happen since we do sync for overlapped concurrent writers\n        LOG.warn(\"Got a overlapping write (\" + range.getMin() + \",\"\n            + range.getMax() + \"), nextOffset=\" + offset\n            + \". Silently drop it now\");\n        pendingWrites.remove(range);\n        processCommits(nextOffset.get()); // handle race\n      } else {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Remove write(\" + range.getMin() + \"-\" + range.getMax()\n              + \") from the list\");\n        }\n        // after writing, remove the WriteCtx from cache \n        pendingWrites.remove(range);\n        // update nextOffset\n        nextOffset.addAndGet(toWrite.getCount());\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Change nextOffset to \" + nextOffset.get());\n        }\n        return toWrite;\n      }\n    \n    return null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.updateLastAccessTime": "  private void updateLastAccessTime() {\n    lastAccessTime = System.currentTimeMillis();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.doSingleWrite": "  private void doSingleWrite(final WriteCtx writeCtx) {\n    Channel channel = writeCtx.getChannel();\n    int xid = writeCtx.getXid();\n\n    long offset = writeCtx.getOffset();\n    int count = writeCtx.getCount();\n    WriteStableHow stableHow = writeCtx.getStableHow();\n    \n    FileHandle handle = writeCtx.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"do write, fileId: \" + handle.getFileId() + \" offset: \"\n          + offset + \" length:\" + count + \" stableHow:\" + stableHow.name());\n    }\n\n    try {\n      // The write is not protected by lock. asyncState is used to make sure\n      // there is one thread doing write back at any time    \n      writeCtx.writeData(fos);\n      \n      long flushedOffset = getFlushedOffset();\n      if (flushedOffset != (offset + count)) {\n        throw new IOException(\"output stream is out of sync, pos=\"\n            + flushedOffset + \" and nextOffset should be\"\n            + (offset + count));\n      }\n      \n\n      // Reduce memory occupation size if request was allowed dumped\n      if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {\n        synchronized (writeCtx) {\n          if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {\n            writeCtx.setDataState(WriteCtx.DataState.NO_DUMP);\n            updateNonSequentialWriteInMemory(-count);\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"After writing \" + handle.getFileId() + \" at offset \"\n                  + offset + \", updated the memory count, new value:\"\n                  + nonSequentialWriteInMemory.get());\n            }\n          }\n        }\n      }\n      \n      if (!writeCtx.getReplied()) {\n        if (stableHow != WriteStableHow.UNSTABLE) {\n          LOG.info(\"Do sync for stable write:\" + writeCtx);\n          try {\n            if (stableHow == WriteStableHow.DATA_SYNC) {\n              fos.hsync();\n            } else {\n              Preconditions.checkState(stableHow == WriteStableHow.FILE_SYNC,\n                  \"Unknown WriteStableHow:\" + stableHow);\n              // Sync file data and length\n              fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n            }\n          } catch (IOException e) {\n            LOG.error(\"hsync failed with writeCtx:\" + writeCtx + \" error:\" + e);\n            throw e;\n          }\n        }\n        \n        WccAttr preOpAttr = latestAttr.getWccAttr();\n        WccData fileWcc = new WccData(preOpAttr, latestAttr);\n        if (writeCtx.getOriginalCount() != WriteCtx.INVALID_ORIGINAL_COUNT) {\n          LOG.warn(\"Return original count:\" + writeCtx.getOriginalCount()\n              + \" instead of real data count:\" + count);\n          count = writeCtx.getOriginalCount();\n        }\n        WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3_OK,\n            fileWcc, count, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n        Nfs3Utils.writeChannel(channel, response.writeHeaderAndResponse(\n            new XDR(), xid, new VerifierNone()), xid);\n      }\n      \n      // Handle the waiting commits without holding any lock\n      processCommits(writeCtx.getOffset() + writeCtx.getCount());\n     \n    } catch (IOException e) {\n      LOG.error(\"Error writing to fileId \" + handle.getFileId() + \" at offset \"\n          + offset + \" and length \" + count, e);\n      if (!writeCtx.getReplied()) {\n        WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3ERR_IO);\n        Nfs3Utils.writeChannel(channel, response.writeHeaderAndResponse(\n            new XDR(), xid, new VerifierNone()), xid);\n        // Keep stream open. Either client retries or SteamMonitor closes it.\n      }\n\n      LOG.info(\"Clean up open file context for fileId: \"\n          + latestAttr.getFileid());\n      cleanup();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService.run": "    public void run() {\n      try {\n        openFileCtx.executeWriteBack();\n      } catch (Throwable t) {\n        LOG.error(\"Asyn data service got error:\"\n            + ExceptionUtils.getFullStackTrace(t));\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.openFileCtx.executeWriteBack": "  void executeWriteBack() {\n    Preconditions.checkState(asyncStatus,\n        \"The openFileCtx has false async status\");\n    try {\n      while (activeState) {\n        WriteCtx toWrite = offerNextToWrite();\n        if (toWrite != null) {\n          // Do the write\n          doSingleWrite(toWrite);\n          updateLastAccessTime();\n        } else {\n          break;\n        }\n      }\n      \n      if (!activeState && LOG.isDebugEnabled()) {\n        LOG.debug(\"The openFileCtx is not active anymore, fileId: \"\n            + latestAttr.getFileId());\n      }\n    } finally {\n      // make sure we reset asyncStatus to false\n      asyncStatus = false;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.openFileCtx.offerNextToWrite": "  private synchronized WriteCtx offerNextToWrite() {\n    if (pendingWrites.isEmpty()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"The asyn write task has no pending writes, fileId: \"\n            + latestAttr.getFileId());\n      }\n      // process pending commit again to handle this race: a commit is added\n      // to pendingCommits map just after the last doSingleWrite returns.\n      // There is no pending write and the commit should be handled by the\n      // last doSingleWrite. Due to the race, the commit is left along and\n      // can't be processed until cleanup. Therefore, we should do another\n      // processCommits to fix the race issue.\n      processCommits(nextOffset.get()); // nextOffset has same value as\n                                        // flushedOffset\n      this.asyncStatus = false;\n      return null;\n    } \n    \n      Entry<OffsetRange, WriteCtx> lastEntry = pendingWrites.lastEntry();\n      OffsetRange range = lastEntry.getKey();\n      WriteCtx toWrite = lastEntry.getValue();\n      \n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"range.getMin()=\" + range.getMin() + \" nextOffset=\"\n            + nextOffset);\n      }\n      \n      long offset = nextOffset.get();\n      if (range.getMin() > offset) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"The next sequencial write has not arrived yet\");\n        }\n        processCommits(nextOffset.get()); // handle race\n        this.asyncStatus = false;\n      } else if (range.getMin() < offset && range.getMax() > offset) {\n        // shouldn't happen since we do sync for overlapped concurrent writers\n        LOG.warn(\"Got a overlapping write (\" + range.getMin() + \",\"\n            + range.getMax() + \"), nextOffset=\" + offset\n            + \". Silently drop it now\");\n        pendingWrites.remove(range);\n        processCommits(nextOffset.get()); // handle race\n      } else {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Remove write(\" + range.getMin() + \"-\" + range.getMax()\n              + \") from the list\");\n        }\n        // after writing, remove the WriteCtx from cache \n        pendingWrites.remove(range);\n        // update nextOffset\n        nextOffset.addAndGet(toWrite.getCount());\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Change nextOffset to \" + nextOffset.get());\n        }\n        return toWrite;\n      }\n    \n    return null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.openFileCtx.updateLastAccessTime": "  private void updateLastAccessTime() {\n    lastAccessTime = System.currentTimeMillis();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.openFileCtx.doSingleWrite": "  private void doSingleWrite(final WriteCtx writeCtx) {\n    Channel channel = writeCtx.getChannel();\n    int xid = writeCtx.getXid();\n\n    long offset = writeCtx.getOffset();\n    int count = writeCtx.getCount();\n    WriteStableHow stableHow = writeCtx.getStableHow();\n    \n    FileHandle handle = writeCtx.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"do write, fileId: \" + handle.getFileId() + \" offset: \"\n          + offset + \" length:\" + count + \" stableHow:\" + stableHow.name());\n    }\n\n    try {\n      // The write is not protected by lock. asyncState is used to make sure\n      // there is one thread doing write back at any time    \n      writeCtx.writeData(fos);\n      \n      long flushedOffset = getFlushedOffset();\n      if (flushedOffset != (offset + count)) {\n        throw new IOException(\"output stream is out of sync, pos=\"\n            + flushedOffset + \" and nextOffset should be\"\n            + (offset + count));\n      }\n      \n\n      // Reduce memory occupation size if request was allowed dumped\n      if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {\n        synchronized (writeCtx) {\n          if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {\n            writeCtx.setDataState(WriteCtx.DataState.NO_DUMP);\n            updateNonSequentialWriteInMemory(-count);\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"After writing \" + handle.getFileId() + \" at offset \"\n                  + offset + \", updated the memory count, new value:\"\n                  + nonSequentialWriteInMemory.get());\n            }\n          }\n        }\n      }\n      \n      if (!writeCtx.getReplied()) {\n        if (stableHow != WriteStableHow.UNSTABLE) {\n          LOG.info(\"Do sync for stable write:\" + writeCtx);\n          try {\n            if (stableHow == WriteStableHow.DATA_SYNC) {\n              fos.hsync();\n            } else {\n              Preconditions.checkState(stableHow == WriteStableHow.FILE_SYNC,\n                  \"Unknown WriteStableHow:\" + stableHow);\n              // Sync file data and length\n              fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n            }\n          } catch (IOException e) {\n            LOG.error(\"hsync failed with writeCtx:\" + writeCtx + \" error:\" + e);\n            throw e;\n          }\n        }\n        \n        WccAttr preOpAttr = latestAttr.getWccAttr();\n        WccData fileWcc = new WccData(preOpAttr, latestAttr);\n        if (writeCtx.getOriginalCount() != WriteCtx.INVALID_ORIGINAL_COUNT) {\n          LOG.warn(\"Return original count:\" + writeCtx.getOriginalCount()\n              + \" instead of real data count:\" + count);\n          count = writeCtx.getOriginalCount();\n        }\n        WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3_OK,\n            fileWcc, count, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n        Nfs3Utils.writeChannel(channel, response.writeHeaderAndResponse(\n            new XDR(), xid, new VerifierNone()), xid);\n      }\n      \n      // Handle the waiting commits without holding any lock\n      processCommits(writeCtx.getOffset() + writeCtx.getCount());\n     \n    } catch (IOException e) {\n      LOG.error(\"Error writing to fileId \" + handle.getFileId() + \" at offset \"\n          + offset + \" and length \" + count, e);\n      if (!writeCtx.getReplied()) {\n        WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3ERR_IO);\n        Nfs3Utils.writeChannel(channel, response.writeHeaderAndResponse(\n            new XDR(), xid, new VerifierNone()), xid);\n        // Keep stream open. Either client retries or SteamMonitor closes it.\n      }\n\n      LOG.info(\"Clean up open file context for fileId: \"\n          + latestAttr.getFileid());\n      cleanup();\n    }\n  }"
        },
        "bug_report": {
            "Title": "race condition causes writeback state error in NFS gateway",
            "Description": "A race condition between NFS gateway writeback executor thread and new write handler thread can cause writeback state check failure, e.g.,\n{noformat}\n2013-11-26 10:34:07,859 DEBUG nfs3.RpcProgramNfs3 (Nfs3Utils.java:writeChannel(113)) - WRITE_RPC_CALL_END______957880843\n2013-11-26 10:34:07,863 DEBUG nfs3.OpenFileCtx (OpenFileCtx.java:offerNextToWrite(832)) - The asyn write task has no pending writes, fileId: 30938\n2013-11-26 10:34:07,871 ERROR nfs3.AsyncDataService (AsyncDataService.java:run(136)) - Asyn data service got error:java.lang.IllegalStateException: The openFileCtx has false async status\n        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)\n        at org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.executeWriteBack(OpenFileCtx.java:890)\n        at org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService$WriteBackTask.run(AsyncDataService.java:134)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:662)\n\n2013-11-26 10:34:07,901 DEBUG nfs3.RpcProgramNfs3 (RpcProgramNfs3.java:write(707)) - requesed offset=917504 and current filesize=917504\n2013-11-26 10:34:07,902 DEBUG nfs3.WriteManager (WriteManager.java:handleWrite(131)) - handleWrite fileId: 30938 offset: 917504 length:65536 stableHow:0\n{noformat}"
        }
    },
    {
        "filename": "HDFS-11827.json",
        "creation_time": "2017-05-16T06:28:49.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:666)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:607)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:375)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:446)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:446)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:240)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:119)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithMultiDC.chooseTargetInternal(BlockPlacementPolicyWithMultiDC.java:263)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithMultiDC.chooseTarget(BlockPlacementPolicyWithMultiDC.java:214)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.chooseTargets(BlockManager.java:3836)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.access$200(BlockManager.java:3801)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1394)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1300)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3764)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3711)\n        at java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom": "  protected DatanodeStorageInfo chooseRandom(int numOfReplicas,\n                            String scope,\n                            Set<Node> excludedNodes,\n                            long blocksize,\n                            int maxNodesPerRack,\n                            List<DatanodeStorageInfo> results,\n                            boolean avoidStaleNodes,\n                            EnumMap<StorageType, Integer> storageTypes)\n                            throws NotEnoughReplicasException {\n    StringBuilder builder = null;\n    if (LOG.isDebugEnabled()) {\n      builder = debugLoggingBuilder.get();\n      builder.setLength(0);\n      builder.append(\"[\");\n    }\n    boolean badTarget = false;\n    DatanodeStorageInfo firstChosen = null;\n    while (numOfReplicas > 0) {\n      // the storage type that current node has\n      StorageType includeType = null;\n      DatanodeDescriptor chosenNode = null;\n      if (clusterMap instanceof DFSNetworkTopology) {\n        for (StorageType type : storageTypes.keySet()) {\n          chosenNode = chooseDataNode(scope, excludedNodes, type);\n\n          if (chosenNode != null) {\n            includeType = type;\n            break;\n          }\n        }\n      } else {\n        chosenNode = chooseDataNode(scope, excludedNodes);\n      }\n\n      if (chosenNode == null) {\n        break;\n      }\n      Preconditions.checkState(excludedNodes.add(chosenNode), \"chosenNode \"\n          + chosenNode + \" is already in excludedNodes \" + excludedNodes);\n      if (LOG.isDebugEnabled()) {\n        builder.append(\"\\nNode \").append(NodeBase.getPath(chosenNode))\n            .append(\" [\");\n      }\n      DatanodeStorageInfo storage = null;\n      if (isGoodDatanode(chosenNode, maxNodesPerRack, considerLoad,\n          results, avoidStaleNodes)) {\n        for (Iterator<Map.Entry<StorageType, Integer>> iter = storageTypes\n            .entrySet().iterator(); iter.hasNext();) {\n          Map.Entry<StorageType, Integer> entry = iter.next();\n\n          // If there is one storage type the node has already contained,\n          // then no need to loop through other storage type.\n          if (includeType != null && entry.getKey() != includeType) {\n            continue;\n          }\n\n          storage = chooseStorage4Block(\n              chosenNode, blocksize, results, entry.getKey());\n          if (storage != null) {\n            numOfReplicas--;\n            if (firstChosen == null) {\n              firstChosen = storage;\n            }\n            // add node (subclasses may also add related nodes) to excludedNode\n            addToExcludedNodes(chosenNode, excludedNodes);\n            int num = entry.getValue();\n            if (num == 1) {\n              iter.remove();\n            } else {\n              entry.setValue(num - 1);\n            }\n            break;\n          }\n        }\n\n        if (LOG.isDebugEnabled()) {\n          builder.append(\"\\n]\");\n        }\n\n        // If no candidate storage was found on this DN then set badTarget.\n        badTarget = (storage == null);\n      }\n    }\n    if (numOfReplicas>0) {\n      String detail = enableDebugLogging;\n      if (LOG.isDebugEnabled()) {\n        if (badTarget && builder != null) {\n          detail = builder.toString();\n          builder.setLength(0);\n        } else {\n          detail = \"\";\n        }\n      }\n      throw new NotEnoughReplicasException(detail);\n    }\n    \n    return firstChosen;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseStorage4Block": "  DatanodeStorageInfo chooseStorage4Block(DatanodeDescriptor dnd,\n      long blockSize,\n      List<DatanodeStorageInfo> results,\n      StorageType storageType) {\n    DatanodeStorageInfo storage =\n        dnd.chooseStorage4Block(storageType, blockSize);\n    if (storage != null) {\n      results.add(storage);\n    } else {\n      logNodeIsNotChosen(dnd, \"no good storage to place the block \");\n    }\n    return storage;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.addToExcludedNodes": "  protected int addToExcludedNodes(DatanodeDescriptor localMachine,\n      Set<Node> excludedNodes) {\n    return excludedNodes.add(localMachine) ? 1 : 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.isGoodDatanode": "  boolean isGoodDatanode(DatanodeDescriptor node,\n                         int maxTargetPerRack, boolean considerLoad,\n                         List<DatanodeStorageInfo> results,\n                         boolean avoidStaleNodes) {\n    // check if the node is (being) decommissioned\n    if (!node.isInService()) {\n      logNodeIsNotChosen(node, \"the node isn't in service.\");\n      return false;\n    }\n\n    if (avoidStaleNodes) {\n      if (node.isStale(this.staleInterval)) {\n        logNodeIsNotChosen(node, \"the node is stale \");\n        return false;\n      }\n    }\n\n    // check the communication traffic of the target machine\n    if (considerLoad) {\n      final double maxLoad = considerLoadFactor *\n          stats.getInServiceXceiverAverage();\n      final int nodeLoad = node.getXceiverCount();\n      if (nodeLoad > maxLoad) {\n        logNodeIsNotChosen(node, \"the node is too busy (load: \" + nodeLoad\n            + \" > \" + maxLoad + \") \");\n        return false;\n      }\n    }\n      \n    // check if the target rack has chosen too many nodes\n    String rackname = node.getNetworkLocation();\n    int counter=1;\n    for(DatanodeStorageInfo resultStorage : results) {\n      if (rackname.equals(\n          resultStorage.getDatanodeDescriptor().getNetworkLocation())) {\n        counter++;\n      }\n    }\n    if (counter > maxTargetPerRack) {\n      logNodeIsNotChosen(node, \"the rack has too many chosen nodes \");\n      return false;\n    }\n\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseDataNode": "  protected DatanodeDescriptor chooseDataNode(final String scope,\n      final Collection<Node> excludedNodes, StorageType type) {\n    return (DatanodeDescriptor) ((DFSNetworkTopology) clusterMap)\n        .chooseRandomWithStorageTypeTwoTrial(scope, excludedNodes, type);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack": "  protected void chooseRemoteRack(int numOfReplicas,\n                                DatanodeDescriptor localMachine,\n                                Set<Node> excludedNodes,\n                                long blocksize,\n                                int maxReplicasPerRack,\n                                List<DatanodeStorageInfo> results,\n                                boolean avoidStaleNodes,\n                                EnumMap<StorageType, Integer> storageTypes)\n                                    throws NotEnoughReplicasException {\n    int oldNumOfReplicas = results.size();\n    // randomly choose one node from remote racks\n    try {\n      chooseRandom(numOfReplicas, \"~\" + localMachine.getNetworkLocation(),\n          excludedNodes, blocksize, maxReplicasPerRack, results,\n          avoidStaleNodes, storageTypes);\n    } catch (NotEnoughReplicasException e) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Failed to choose remote rack (location = ~\"\n            + localMachine.getNetworkLocation() + \"), fallback to local rack\", e);\n      }\n      chooseRandom(numOfReplicas-(results.size()-oldNumOfReplicas),\n                   localMachine.getNetworkLocation(), excludedNodes, blocksize, \n                   maxReplicasPerRack, results, avoidStaleNodes, storageTypes);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget": "  private Node chooseTarget(int numOfReplicas,\n                            Node writer,\n                            final Set<Node> excludedNodes,\n                            final long blocksize,\n                            final int maxNodesPerRack,\n                            final List<DatanodeStorageInfo> results,\n                            final boolean avoidStaleNodes,\n                            final BlockStoragePolicy storagePolicy,\n                            final EnumSet<StorageType> unavailableStorages,\n                            final boolean newBlock) {\n    if (numOfReplicas == 0 || clusterMap.getNumOfLeaves()==0) {\n      return (writer instanceof DatanodeDescriptor) ? writer : null;\n    }\n    final int numOfResults = results.size();\n    final int totalReplicasExpected = numOfReplicas + numOfResults;\n    if ((writer == null || !(writer instanceof DatanodeDescriptor)) && !newBlock) {\n      writer = results.get(0).getDatanodeDescriptor();\n    }\n\n    // Keep a copy of original excludedNodes\n    final Set<Node> oldExcludedNodes = new HashSet<>(excludedNodes);\n\n    // choose storage types; use fallbacks for unavailable storages\n    final List<StorageType> requiredStorageTypes = storagePolicy\n        .chooseStorageTypes((short) totalReplicasExpected,\n            DatanodeStorageInfo.toStorageTypes(results),\n            unavailableStorages, newBlock);\n    final EnumMap<StorageType, Integer> storageTypes =\n        getRequiredStorageTypes(requiredStorageTypes);\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"storageTypes=\" + storageTypes);\n    }\n\n    try {\n      if ((numOfReplicas = requiredStorageTypes.size()) == 0) {\n        throw new NotEnoughReplicasException(\n            \"All required storage types are unavailable: \"\n            + \" unavailableStorages=\" + unavailableStorages\n            + \", storagePolicy=\" + storagePolicy);\n      }\n      writer = chooseTargetInOrder(numOfReplicas, writer, excludedNodes, blocksize,\n          maxNodesPerRack, results, avoidStaleNodes, newBlock, storageTypes);\n    } catch (NotEnoughReplicasException e) {\n      final String message = \"Failed to place enough replicas, still in need of \"\n          + (totalReplicasExpected - results.size()) + \" to reach \"\n          + totalReplicasExpected\n          + \" (unavailableStorages=\" + unavailableStorages\n          + \", storagePolicy=\" + storagePolicy\n          + \", newBlock=\" + newBlock + \")\";\n\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(message, e);\n      } else {\n        LOG.warn(message + \" \" + e.getMessage());\n      }\n\n      if (avoidStaleNodes) {\n        // Retry chooseTarget again, this time not avoiding stale nodes.\n\n        // excludedNodes contains the initial excludedNodes and nodes that were\n        // not chosen because they were stale, decommissioned, etc.\n        // We need to additionally exclude the nodes that were added to the \n        // result list in the successful calls to choose*() above.\n        for (DatanodeStorageInfo resultStorage : results) {\n          addToExcludedNodes(resultStorage.getDatanodeDescriptor(), oldExcludedNodes);\n        }\n        // Set numOfReplicas, since it can get out of sync with the result list\n        // if the NotEnoughReplicasException was thrown in chooseRandom().\n        numOfReplicas = totalReplicasExpected - results.size();\n        return chooseTarget(numOfReplicas, writer, oldExcludedNodes, blocksize,\n            maxNodesPerRack, results, false, storagePolicy, unavailableStorages,\n            newBlock);\n      }\n\n      boolean retry = false;\n      // simply add all the remaining types into unavailableStorages and give\n      // another try. No best effort is guaranteed here.\n      for (StorageType type : storageTypes.keySet()) {\n        if (!unavailableStorages.contains(type)) {\n          unavailableStorages.add(type);\n          retry = true;\n        }\n      }\n      if (retry) {\n        for (DatanodeStorageInfo resultStorage : results) {\n          addToExcludedNodes(resultStorage.getDatanodeDescriptor(),\n              oldExcludedNodes);\n        }\n        numOfReplicas = totalReplicasExpected - results.size();\n        return chooseTarget(numOfReplicas, writer, oldExcludedNodes, blocksize,\n            maxNodesPerRack, results, false, storagePolicy, unavailableStorages,\n            newBlock);\n      }\n    }\n    return writer;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseFavouredNodes": "  protected void chooseFavouredNodes(String src, int numOfReplicas,\n      List<DatanodeDescriptor> favoredNodes,\n      Set<Node> favoriteAndExcludedNodes, long blocksize, int maxNodesPerRack,\n      List<DatanodeStorageInfo> results, boolean avoidStaleNodes,\n      EnumMap<StorageType, Integer> storageTypes)\n      throws NotEnoughReplicasException {\n    for (int i = 0; i < favoredNodes.size() && results.size() < numOfReplicas;\n        i++) {\n      DatanodeDescriptor favoredNode = favoredNodes.get(i);\n      // Choose a single node which is local to favoredNode.\n      // 'results' is updated within chooseLocalNode\n      final DatanodeStorageInfo target =\n          chooseLocalStorage(favoredNode, favoriteAndExcludedNodes, blocksize,\n            maxNodesPerRack, results, avoidStaleNodes, storageTypes, false);\n      if (target == null) {\n        LOG.warn(\"Could not find a target for file \" + src\n            + \" with favored node \" + favoredNode);\n        continue;\n      }\n      favoriteAndExcludedNodes.add(target.getDatanodeDescriptor());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.getPipeline": "  private DatanodeStorageInfo[] getPipeline(Node writer,\n      DatanodeStorageInfo[] storages) {\n    if (storages.length == 0) {\n      return storages;\n    }\n\n    synchronized(clusterMap) {\n      int index=0;\n      if (writer == null || !clusterMap.contains(writer)) {\n        writer = storages[0].getDatanodeDescriptor();\n      }\n      for(; index < storages.length; index++) {\n        DatanodeStorageInfo shortestStorage = storages[index];\n        int shortestDistance = clusterMap.getDistance(writer,\n            shortestStorage.getDatanodeDescriptor());\n        int shortestIndex = index;\n        for(int i = index + 1; i < storages.length; i++) {\n          int currentDistance = clusterMap.getDistance(writer,\n              storages[i].getDatanodeDescriptor());\n          if (shortestDistance>currentDistance) {\n            shortestDistance = currentDistance;\n            shortestStorage = storages[i];\n            shortestIndex = i;\n          }\n        }\n        //switch position index & shortestIndex\n        if (index != shortestIndex) {\n          storages[shortestIndex] = storages[index];\n          storages[index] = shortestStorage;\n        }\n        writer = shortestStorage.getDatanodeDescriptor();\n      }\n    }\n    return storages;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.getMaxNodesPerRack": "  protected int[] getMaxNodesPerRack(int numOfChosen, int numOfReplicas) {\n    int clusterSize = clusterMap.getNumOfLeaves();\n    int totalNumOfReplicas = numOfChosen + numOfReplicas;\n    if (totalNumOfReplicas > clusterSize) {\n      numOfReplicas -= (totalNumOfReplicas-clusterSize);\n      totalNumOfReplicas = clusterSize;\n    }\n    // No calculation needed when there is only one rack or picking one node.\n    int numOfRacks = clusterMap.getNumOfRacks();\n    if (numOfRacks == 1 || totalNumOfReplicas <= 1) {\n      return new int[] {numOfReplicas, totalNumOfReplicas};\n    }\n\n    int maxNodesPerRack = (totalNumOfReplicas-1)/numOfRacks + 2;\n    // At this point, there are more than one racks and more than one replicas\n    // to store. Avoid all replicas being in the same rack.\n    //\n    // maxNodesPerRack has the following properties at this stage.\n    //   1) maxNodesPerRack >= 2\n    //   2) (maxNodesPerRack-1) * numOfRacks > totalNumOfReplicas\n    //          when numOfRacks > 1\n    //\n    // Thus, the following adjustment will still result in a value that forces\n    // multi-rack allocation and gives enough number of total nodes.\n    if (maxNodesPerRack == totalNumOfReplicas) {\n      maxNodesPerRack--;\n    }\n    return new int[] {numOfReplicas, maxNodesPerRack};\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder": "  protected Node chooseTargetInOrder(int numOfReplicas, \n                                 Node writer,\n                                 final Set<Node> excludedNodes,\n                                 final long blocksize,\n                                 final int maxNodesPerRack,\n                                 final List<DatanodeStorageInfo> results,\n                                 final boolean avoidStaleNodes,\n                                 final boolean newBlock,\n                                 EnumMap<StorageType, Integer> storageTypes)\n                                 throws NotEnoughReplicasException {\n    final int numOfResults = results.size();\n    if (numOfResults == 0) {\n      writer = chooseLocalStorage(writer, excludedNodes, blocksize,\n          maxNodesPerRack, results, avoidStaleNodes, storageTypes, true)\n          .getDatanodeDescriptor();\n      if (--numOfReplicas == 0) {\n        return writer;\n      }\n    }\n    final DatanodeDescriptor dn0 = results.get(0).getDatanodeDescriptor();\n    if (numOfResults <= 1) {\n      chooseRemoteRack(1, dn0, excludedNodes, blocksize, maxNodesPerRack,\n          results, avoidStaleNodes, storageTypes);\n      if (--numOfReplicas == 0) {\n        return writer;\n      }\n    }\n    if (numOfResults <= 2) {\n      final DatanodeDescriptor dn1 = results.get(1).getDatanodeDescriptor();\n      if (clusterMap.isOnSameRack(dn0, dn1)) {\n        chooseRemoteRack(1, dn0, excludedNodes, blocksize, maxNodesPerRack,\n            results, avoidStaleNodes, storageTypes);\n      } else if (newBlock){\n        chooseLocalRack(dn1, excludedNodes, blocksize, maxNodesPerRack,\n            results, avoidStaleNodes, storageTypes);\n      } else {\n        chooseLocalRack(writer, excludedNodes, blocksize, maxNodesPerRack,\n            results, avoidStaleNodes, storageTypes);\n      }\n      if (--numOfReplicas == 0) {\n        return writer;\n      }\n    }\n    chooseRandom(numOfReplicas, NodeBase.ROOT, excludedNodes, blocksize,\n        maxNodesPerRack, results, avoidStaleNodes, storageTypes);\n    return writer;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.getRequiredStorageTypes": "  private EnumMap<StorageType, Integer> getRequiredStorageTypes(\n      List<StorageType> types) {\n    EnumMap<StorageType, Integer> map = new EnumMap<>(StorageType.class);\n    for (StorageType type : types) {\n      if (!map.containsKey(type)) {\n        map.put(type, 1);\n      } else {\n        int num = map.get(type);\n        map.put(type, num + 1);\n      }\n    }\n    return map;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork": "  int computeDatanodeWork() {\n    // Blocks should not be replicated or removed if in safe mode.\n    // It's OK to check safe mode here w/o holding lock, in the worst\n    // case extra replications will be scheduled, and these will get\n    // fixed up later.\n    if (namesystem.isInSafeMode()) {\n      return 0;\n    }\n\n    final int numlive = heartbeatManager.getLiveDatanodeCount();\n    final int blocksToProcess = numlive\n        * this.blocksReplWorkMultiplier;\n    final int nodesToProcess = (int) Math.ceil(numlive\n        * this.blocksInvalidateWorkPct);\n\n    int workFound = this.computeBlockReconstructionWork(blocksToProcess);\n\n    // Update counters\n    namesystem.writeLock();\n    try {\n      this.updateState();\n      this.scheduledReplicationBlocksCount = workFound;\n    } finally {\n      namesystem.writeUnlock();\n    }\n    workFound += this.computeInvalidateWork(nodesToProcess);\n    return workFound;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeBlockReconstructionWork": "  int computeBlockReconstructionWork(int blocksToProcess) {\n    List<List<BlockInfo>> blocksToReconstruct = null;\n    namesystem.writeLock();\n    try {\n      // Choose the blocks to be reconstructed\n      blocksToReconstruct = neededReconstruction\n          .chooseLowRedundancyBlocks(blocksToProcess);\n    } finally {\n      namesystem.writeUnlock();\n    }\n    return computeReconstructionWorkForBlocks(blocksToReconstruct);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.isInSafeMode": "  public boolean isInSafeMode() {\n    return bmSafeMode.isInSafeMode();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.updateState": "  void updateState() {\n    pendingReconstructionBlocksCount = pendingReconstruction.size();\n    lowRedundancyBlocksCount = neededReconstruction.size();\n    corruptReplicaBlocksCount = corruptReplicas.size();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeInvalidateWork": "  int computeInvalidateWork(int nodesToProcess) {\n    final List<DatanodeInfo> nodes = invalidateBlocks.getDatanodes();\n    Collections.shuffle(nodes);\n\n    nodesToProcess = Math.min(nodes.size(), nodesToProcess);\n\n    int blockCnt = 0;\n    for (DatanodeInfo dnInfo : nodes) {\n      int blocks = invalidateWorkForOneNode(dnInfo);\n      if (blocks > 0) {\n        blockCnt += blocks;\n        if (--nodesToProcess == 0) {\n          break;\n        }\n      }\n    }\n    return blockCnt;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.run": "    public void run() {\n      try {\n        processQueue();\n      } catch (Throwable t) {\n        ExitUtil.terminate(1,\n            getName() + \" encountered fatal exception: \" + t);\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processQueue": "    private void processQueue() {\n      while (namesystem.isRunning()) {\n        NameNodeMetrics metrics = NameNode.getNameNodeMetrics();\n        try {\n          Runnable action = queue.take();\n          // batch as many operations in the write lock until the queue\n          // runs dry, or the max lock hold is reached.\n          int processed = 0;\n          namesystem.writeLock();\n          metrics.setBlockOpsQueued(queue.size() + 1);\n          try {\n            long start = Time.monotonicNow();\n            do {\n              processed++;\n              action.run();\n              if (Time.monotonicNow() - start > MAX_LOCK_HOLD_MS) {\n                break;\n              }\n              action = queue.poll();\n            } while (action != null);\n          } finally {\n            namesystem.writeUnlock();\n            metrics.addBlockOpsBatched(processed - 1);\n          }\n        } catch (InterruptedException e) {\n          // ignore unless thread was specifically interrupted.\n          if (Thread.interrupted()) {\n            break;\n          }\n        }\n      }\n      queue.clear();\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.isPopulatingReplQueues": "  public boolean isPopulatingReplQueues() {\n    if (!shouldPopulateReplQueues()) {\n      return false;\n    }\n    return initializedReplQueues;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.rescanPostponedMisreplicatedBlocks": "  void rescanPostponedMisreplicatedBlocks() {\n    if (getPostponedMisreplicatedBlocksCount() == 0) {\n      return;\n    }\n    namesystem.writeLock();\n    long startTime = Time.monotonicNow();\n    long startSize = postponedMisreplicatedBlocks.size();\n    try {\n      Iterator<Block> it = postponedMisreplicatedBlocks.iterator();\n      for (int i=0; i < blocksPerPostpondedRescan && it.hasNext(); i++) {\n        Block b = it.next();\n        it.remove();\n\n        BlockInfo bi = getStoredBlock(b);\n        if (bi == null) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"BLOCK* rescanPostponedMisreplicatedBlocks: \" +\n                \"Postponed mis-replicated block \" + b + \" no longer found \" +\n                \"in block map.\");\n          }\n          continue;\n        }\n        MisReplicationResult res = processMisReplicatedBlock(bi);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"BLOCK* rescanPostponedMisreplicatedBlocks: \" +\n              \"Re-scanned block \" + b + \", result is \" + res);\n        }\n        if (res == MisReplicationResult.POSTPONE) {\n          rescannedMisreplicatedBlocks.add(b);\n        }\n      }\n    } finally {\n      postponedMisreplicatedBlocks.addAll(rescannedMisreplicatedBlocks);\n      rescannedMisreplicatedBlocks.clear();\n      long endSize = postponedMisreplicatedBlocks.size();\n      namesystem.writeUnlock();\n      LOG.info(\"Rescan of postponedMisreplicatedBlocks completed in \" +\n          (Time.monotonicNow() - startTime) + \" msecs. \" +\n          endSize + \" blocks are left. \" +\n          (startSize - endSize) + \" blocks were removed.\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processPendingReconstructions": "  private void processPendingReconstructions() {\n    BlockInfo[] timedOutItems = pendingReconstruction.getTimedOutBlocks();\n    if (timedOutItems != null) {\n      namesystem.writeLock();\n      try {\n        for (int i = 0; i < timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we're working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi = blocksMap.getStoredBlock(timedOutItems[i]);\n          if (bi == null) {\n            continue;\n          }\n          NumberReplicas num = countNodes(timedOutItems[i]);\n          if (isNeededReconstruction(bi, num)) {\n            neededReconstruction.add(bi, num.liveReplicas(),\n                num.readOnlyReplicas(), num.outOfServiceReplicas(),\n                getExpectedRedundancyNum(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync": "  private void processMisReplicatesAsync() throws InterruptedException {\n    long nrInvalid = 0, nrOverReplicated = 0;\n    long nrUnderReplicated = 0, nrPostponed = 0, nrUnderConstruction = 0;\n    long startTimeMisReplicatedScan = Time.monotonicNow();\n    Iterator<BlockInfo> blocksItr = blocksMap.getBlocks().iterator();\n    long totalBlocks = blocksMap.size();\n    reconstructionQueuesInitProgress = 0;\n    long totalProcessed = 0;\n    long sleepDuration =\n        Math.max(1, Math.min(numBlocksPerIteration/1000, 10000));\n\n    while (namesystem.isRunning() && !Thread.currentThread().isInterrupted()) {\n      int processed = 0;\n      namesystem.writeLockInterruptibly();\n      try {\n        while (processed < numBlocksPerIteration && blocksItr.hasNext()) {\n          BlockInfo block = blocksItr.next();\n          MisReplicationResult res = processMisReplicatedBlock(block);\n          switch (res) {\n          case UNDER_REPLICATED:\n            LOG.trace(\"under replicated block {}: {}\", block, res);\n            nrUnderReplicated++;\n            break;\n          case OVER_REPLICATED:\n            LOG.trace(\"over replicated block {}: {}\", block, res);\n            nrOverReplicated++;\n            break;\n          case INVALID:\n            LOG.trace(\"invalid block {}: {}\", block, res);\n            nrInvalid++;\n            break;\n          case POSTPONE:\n            LOG.trace(\"postpone block {}: {}\", block, res);\n            nrPostponed++;\n            postponeBlock(block);\n            break;\n          case UNDER_CONSTRUCTION:\n            LOG.trace(\"under construction block {}: {}\", block, res);\n            nrUnderConstruction++;\n            break;\n          case OK:\n            break;\n          default:\n            throw new AssertionError(\"Invalid enum value: \" + res);\n          }\n          processed++;\n        }\n        totalProcessed += processed;\n        // there is a possibility that if any of the blocks deleted/added during\n        // initialisation, then progress might be different.\n        reconstructionQueuesInitProgress = Math.min((double) totalProcessed\n            / totalBlocks, 1.0);\n\n        if (!blocksItr.hasNext()) {\n          LOG.info(\"Total number of blocks            = \" + blocksMap.size());\n          LOG.info(\"Number of invalid blocks          = \" + nrInvalid);\n          LOG.info(\"Number of under-replicated blocks = \" + nrUnderReplicated);\n          LOG.info(\"Number of  over-replicated blocks = \" + nrOverReplicated\n              + ((nrPostponed > 0) ? (\" (\" + nrPostponed + \" postponed)\") : \"\"));\n          LOG.info(\"Number of blocks being written    = \" + nrUnderConstruction);\n          NameNode.stateChangeLog\n              .info(\"STATE* Replication Queue initialization \"\n                  + \"scan for invalid, over- and under-replicated blocks \"\n                  + \"completed in \"\n                  + (Time.monotonicNow() - startTimeMisReplicatedScan)\n                  + \" msec\");\n          break;\n        }\n      } finally {\n        namesystem.writeUnlock();\n        // Make sure it is out of the write lock for sufficiently long time.\n        Thread.sleep(sleepDuration);\n      }\n    }\n    if (Thread.currentThread().isInterrupted()) {\n      LOG.info(\"Interrupted while processing replication queues.\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.scanAndCompactStorages": "    private void scanAndCompactStorages() throws InterruptedException {\n      ArrayList<String> datanodesAndStorages = new ArrayList<>();\n      for (DatanodeDescriptor node\n          : datanodeManager.getDatanodeListForReport(DatanodeReportType.ALL)) {\n        for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n          try {\n            namesystem.readLock();\n            double ratio = storage.treeSetFillRatio();\n            if (ratio < storageInfoDefragmentRatio) {\n              datanodesAndStorages.add(node.getDatanodeUuid());\n              datanodesAndStorages.add(storage.getStorageID());\n            }\n            LOG.info(\"StorageInfo TreeSet fill ratio {} : {}{}\",\n                     storage.getStorageID(), ratio,\n                     (ratio < storageInfoDefragmentRatio)\n                     ? \" (queued for defragmentation)\" : \"\");\n          } finally {\n            namesystem.readUnlock();\n          }\n        }\n      }\n      if (!datanodesAndStorages.isEmpty()) {\n        for (int i = 0; i < datanodesAndStorages.size(); i += 2) {\n          namesystem.writeLock();\n          try {\n            DatanodeStorageInfo storage = datanodeManager.\n                getDatanode(datanodesAndStorages.get(i)).\n                getStorageInfo(datanodesAndStorages.get(i + 1));\n            if (storage != null) {\n              boolean aborted =\n                  !storage.treeSetCompact(storageInfoDefragmentTimeout);\n              if (aborted) {\n                // Compaction timed out, reset iterator to continue with\n                // the same storage next iteration.\n                i -= 2;\n              }\n              LOG.info(\"StorageInfo TreeSet defragmented {} : {}{}\",\n                       storage.getStorageID(), storage.treeSetFillRatio(),\n                       aborted ? \" (aborted)\" : \"\");\n            }\n          } finally {\n            namesystem.writeUnlock();\n          }\n          // Wait between each iteration\n          Thread.sleep(1000);\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.toStorageTypes": "  static Iterable<StorageType> toStorageTypes(\n      final Iterable<DatanodeStorageInfo> infos) {\n    return new Iterable<StorageType>() {\n        @Override\n        public Iterator<StorageType> iterator() {\n          return new Iterator<StorageType>() {\n            final Iterator<DatanodeStorageInfo> i = infos.iterator();\n            @Override\n            public boolean hasNext() {return i.hasNext();}\n            @Override\n            public StorageType next() {return i.next().getStorageType();}\n            @Override\n            public void remove() {\n              throw new UnsupportedOperationException();\n            }\n          };\n        }\n      };\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.next": "            public StorageType next() {return i.next().getStorageType();}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.iterator": "        public Iterator<StorageType> iterator() {\n          return new Iterator<StorageType>() {\n            final Iterator<DatanodeStorageInfo> i = infos.iterator();\n            @Override\n            public boolean hasNext() {return i.hasNext();}\n            @Override\n            public StorageType next() {return i.next().getStorageType();}\n            @Override\n            public void remove() {\n              throw new UnsupportedOperationException();\n            }\n          };\n        }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.getStorageType": "  public StorageType getStorageType() {\n    return storageType;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.hasNext": "            public boolean hasNext() {return i.hasNext();}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.getDatanodeDescriptor": "  public DatanodeDescriptor getDatanodeDescriptor() {\n    return dn;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.heartbeatManager.getLiveDatanodeCount": "  synchronized int getLiveDatanodeCount() {\n    return datanodes.size();\n  }"
        },
        "bug_report": {
            "Title": "NPE is thrown when log level changed in BlockPlacementPolicyDefault#chooseRandom() method ",
            "Description": "This issue was found by my colleague when changing log-level of BlockPlacementPolicy using \"hadoop daemonlog\" command. \n\nThe exception stack trace is below:\n{noformat}\n2017-05-16 13:36:51,053 ERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor thread received Runtime exception.\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:666)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:607)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:375)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:446)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:446)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:240)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:119)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithMultiDC.chooseTargetInternal(BlockPlacementPolicyWithMultiDC.java:263)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithMultiDC.chooseTarget(BlockPlacementPolicyWithMultiDC.java:214)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.chooseTargets(BlockManager.java:3836)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.access$200(BlockManager.java:3801)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1394)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1300)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3764)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3711)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\n\n\nAfter checking _BlockPlacementPolicyDefault_ code of trunk, I found that _BlockPlacementPolicyDefault_ class missed some NPE check code in _chooseRandom()_ method. "
        }
    },
    {
        "filename": "HDFS-6804.json",
        "creation_time": "2014-08-01T05:37:23.000+0000",
        "stack_trace": "```\njava.io.IOException: Terminating due to a checksum error.java.io.IOException: Unexpected checksum mismatch while writing BP-2072804351-192.168.2.104-1406008383435:blk_1073741997_9248 from /192.168.2.101:39495\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:536)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:703)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:575)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:115)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:68)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)\n        at java.lang.Thread.run(Thread.java:744)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket": "  private int receivePacket() throws IOException {\n    // read the next packet\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader header = packetReceiver.getHeader();\n    if (LOG.isDebugEnabled()){\n      LOG.debug(\"Receiving one packet for block \" + block +\n                \": \" + header);\n    }\n\n    // Sanity check the header\n    if (header.getOffsetInBlock() > replicaInfo.getNumBytes()) {\n      throw new IOException(\"Received an out-of-sequence packet for \" + block + \n          \"from \" + inAddr + \" at offset \" + header.getOffsetInBlock() +\n          \". Expecting packet starting at \" + replicaInfo.getNumBytes());\n    }\n    if (header.getDataLen() < 0) {\n      throw new IOException(\"Got wrong length during writeBlock(\" + block + \n                            \") from \" + inAddr + \" at offset \" + \n                            header.getOffsetInBlock() + \": \" +\n                            header.getDataLen()); \n    }\n\n    long offsetInBlock = header.getOffsetInBlock();\n    long seqno = header.getSeqno();\n    boolean lastPacketInBlock = header.isLastPacketInBlock();\n    int len = header.getDataLen();\n    boolean syncBlock = header.getSyncBlock();\n\n    // avoid double sync'ing on close\n    if (syncBlock && lastPacketInBlock) {\n      this.syncOnClose = false;\n    }\n\n    // update received bytes\n    long firstByteInBlock = offsetInBlock;\n    offsetInBlock += len;\n    if (replicaInfo.getNumBytes() < offsetInBlock) {\n      replicaInfo.setNumBytes(offsetInBlock);\n    }\n    \n    // put in queue for pending acks, unless sync was requested\n    if (responder != null && !syncBlock && !shouldVerifyChecksum()) {\n      ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n          lastPacketInBlock, offsetInBlock, Status.SUCCESS);\n    }\n\n    //First write the packet to the mirror:\n    if (mirrorOut != null && !mirrorError) {\n      try {\n        long begin = Time.monotonicNow();\n        packetReceiver.mirrorPacketTo(mirrorOut);\n        mirrorOut.flush();\n        long duration = Time.monotonicNow() - begin;\n        if (duration > datanodeSlowLogThresholdMs) {\n          LOG.warn(\"Slow BlockReceiver write packet to mirror took \" + duration\n              + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms)\");\n        }\n      } catch (IOException e) {\n        handleMirrorOutError(e);\n      }\n    }\n    \n    ByteBuffer dataBuf = packetReceiver.getDataSlice();\n    ByteBuffer checksumBuf = packetReceiver.getChecksumSlice();\n    \n    if (lastPacketInBlock || len == 0) {\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"Receiving an empty packet or the end of the block \" + block);\n      }\n      // sync block if requested\n      if (syncBlock) {\n        flushOrSync(true);\n      }\n    } else {\n      int checksumLen = ((len + bytesPerChecksum - 1)/bytesPerChecksum)*\n                                                            checksumSize;\n\n      if ( checksumBuf.capacity() != checksumLen) {\n        throw new IOException(\"Length of checksums in packet \" +\n            checksumBuf.capacity() + \" does not match calculated checksum \" +\n            \"length \" + checksumLen);\n      }\n\n      if (shouldVerifyChecksum()) {\n        try {\n          verifyChunks(dataBuf, checksumBuf);\n        } catch (IOException ioe) {\n          // checksum error detected locally. there is no reason to continue.\n          if (responder != null) {\n            try {\n              ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n                  lastPacketInBlock, offsetInBlock,\n                  Status.ERROR_CHECKSUM);\n              // Wait until the responder sends back the response\n              // and interrupt this thread.\n              Thread.sleep(3000);\n            } catch (InterruptedException e) { }\n          }\n          throw new IOException(\"Terminating due to a checksum error.\" + ioe);\n        }\n \n        if (needsChecksumTranslation) {\n          // overwrite the checksums in the packet buffer with the\n          // appropriate polynomial for the disk storage.\n          translateChunks(dataBuf, checksumBuf);\n        }\n      }\n      \n      // by this point, the data in the buffer uses the disk checksum\n\n      byte[] lastChunkChecksum;\n      \n      try {\n        long onDiskLen = replicaInfo.getBytesOnDisk();\n        if (onDiskLen<offsetInBlock) {\n          //finally write to the disk :\n          \n          if (onDiskLen % bytesPerChecksum != 0) { \n            // prepare to overwrite last checksum\n            adjustCrcFilePosition();\n          }\n          \n          // If this is a partial chunk, then read in pre-existing checksum\n          if (firstByteInBlock % bytesPerChecksum != 0) {\n            LOG.info(\"Packet starts at \" + firstByteInBlock +\n                     \" for \" + block +\n                     \" which is not a multiple of bytesPerChecksum \" +\n                     bytesPerChecksum);\n            long offsetInChecksum = BlockMetadataHeader.getHeaderSize() +\n                onDiskLen / bytesPerChecksum * checksumSize;\n            computePartialChunkCrc(onDiskLen, offsetInChecksum, bytesPerChecksum);\n          }\n\n          int startByteToDisk = (int)(onDiskLen-firstByteInBlock) \n              + dataBuf.arrayOffset() + dataBuf.position();\n\n          int numBytesToDisk = (int)(offsetInBlock-onDiskLen);\n          \n          // Write data to disk.\n          long begin = Time.monotonicNow();\n          out.write(dataBuf.array(), startByteToDisk, numBytesToDisk);\n          long duration = Time.monotonicNow() - begin;\n          if (duration > datanodeSlowLogThresholdMs) {\n            LOG.warn(\"Slow BlockReceiver write data to disk cost:\" + duration\n                + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms)\");\n          }\n\n          // If this is a partial chunk, then verify that this is the only\n          // chunk in the packet. Calculate new crc for this chunk.\n          if (partialCrc != null) {\n            if (len > bytesPerChecksum) {\n              throw new IOException(\"Got wrong length during writeBlock(\" + \n                                    block + \") from \" + inAddr + \" \" +\n                                    \"A packet can have only one partial chunk.\"+\n                                    \" len = \" + len + \n                                    \" bytesPerChecksum \" + bytesPerChecksum);\n            }\n            partialCrc.update(dataBuf.array(), startByteToDisk, numBytesToDisk);\n            byte[] buf = FSOutputSummer.convertToByteStream(partialCrc, checksumSize);\n            lastChunkChecksum = Arrays.copyOfRange(\n              buf, buf.length - checksumSize, buf.length\n            );\n            checksumOut.write(buf);\n            if(LOG.isDebugEnabled()) {\n              LOG.debug(\"Writing out partial crc for data len \" + len);\n            }\n            partialCrc = null;\n          } else {\n            lastChunkChecksum = Arrays.copyOfRange(\n                checksumBuf.array(),\n                checksumBuf.arrayOffset() + checksumBuf.position() + checksumLen - checksumSize,\n                checksumBuf.arrayOffset() + checksumBuf.position() + checksumLen);\n            checksumOut.write(checksumBuf.array(),\n                checksumBuf.arrayOffset() + checksumBuf.position(),\n                checksumLen);\n          }\n          /// flush entire packet, sync if requested\n          flushOrSync(syncBlock);\n          \n          replicaInfo.setLastChecksumAndDataLen(\n            offsetInBlock, lastChunkChecksum\n          );\n\n          datanode.metrics.incrBytesWritten(len);\n\n          manageWriterOsCache(offsetInBlock);\n        }\n      } catch (IOException iex) {\n        datanode.checkDiskError();\n        throw iex;\n      }\n    }\n\n    // if sync was requested, put in queue for pending acks here\n    // (after the fsync finished)\n    if (responder != null && (syncBlock || shouldVerifyChecksum())) {\n      ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n          lastPacketInBlock, offsetInBlock, Status.SUCCESS);\n    }\n\n    if (throttler != null) { // throttle I/O\n      throttler.throttle(len);\n    }\n    \n    return lastPacketInBlock?-1:len;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.translateChunks": "  private void translateChunks(ByteBuffer dataBuf, ByteBuffer checksumBuf) {\n    diskChecksum.calculateChunkedSums(dataBuf, checksumBuf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.adjustCrcFilePosition": "  private void adjustCrcFilePosition() throws IOException {\n    if (out != null) {\n     out.flush();\n    }\n    if (checksumOut != null) {\n      checksumOut.flush();\n    }\n\n    // rollback the position of the meta file\n    datanode.data.adjustCrcChannelPosition(block, streams, checksumSize);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.handleMirrorOutError": "  private void handleMirrorOutError(IOException ioe) throws IOException {\n    String bpid = block.getBlockPoolId();\n    LOG.info(datanode.getDNRegistrationForBP(bpid)\n        + \":Exception writing \" + block + \" to mirror \" + mirrorAddr, ioe);\n    if (Thread.interrupted()) { // shut down if the thread is interrupted\n      throw ioe;\n    } else { // encounter an error while writing to mirror\n      // continue to run even if can not write to mirror\n      // notify client of the error\n      // and wait for the client to shut down the pipeline\n      mirrorError = true;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.manageWriterOsCache": "  private void manageWriterOsCache(long offsetInBlock) {\n    try {\n      if (outFd != null &&\n          offsetInBlock > lastCacheManagementOffset + CACHE_DROP_LAG_BYTES) {\n        long begin = Time.monotonicNow();\n        //\n        // For SYNC_FILE_RANGE_WRITE, we want to sync from\n        // lastCacheManagementOffset to a position \"two windows ago\"\n        //\n        //                         <========= sync ===========>\n        // +-----------------------O--------------------------X\n        // start                  last                      curPos\n        // of file                 \n        //\n        if (syncBehindWrites) {\n          if (syncBehindWritesInBackground) {\n            this.datanode.getFSDataset().submitBackgroundSyncFileRangeRequest(\n                block, outFd, lastCacheManagementOffset,\n                offsetInBlock - lastCacheManagementOffset,\n                NativeIO.POSIX.SYNC_FILE_RANGE_WRITE);\n          } else {\n            NativeIO.POSIX.syncFileRangeIfPossible(outFd,\n                lastCacheManagementOffset, offsetInBlock\n                    - lastCacheManagementOffset,\n                NativeIO.POSIX.SYNC_FILE_RANGE_WRITE);\n          }\n        }\n        //\n        // For POSIX_FADV_DONTNEED, we want to drop from the beginning \n        // of the file to a position prior to the current position.\n        //\n        // <=== drop =====> \n        //                 <---W--->\n        // +--------------+--------O--------------------------X\n        // start        dropPos   last                      curPos\n        // of file             \n        //                     \n        long dropPos = lastCacheManagementOffset - CACHE_DROP_LAG_BYTES;\n        if (dropPos > 0 && dropCacheBehindWrites) {\n          NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n              block.getBlockName(), outFd, 0, dropPos,\n              NativeIO.POSIX.POSIX_FADV_DONTNEED);\n        }\n        lastCacheManagementOffset = offsetInBlock;\n        long duration = Time.monotonicNow() - begin;\n        if (duration > datanodeSlowLogThresholdMs) {\n          LOG.warn(\"Slow manageWriterOsCache took \" + duration\n              + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms)\");\n        }\n      }\n    } catch (Throwable t) {\n      LOG.warn(\"Error managing cache for writer of block \" + block, t);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.verifyChunks": "  private void verifyChunks(ByteBuffer dataBuf, ByteBuffer checksumBuf)\n      throws IOException {\n    try {\n      clientChecksum.verifyChunkedSums(dataBuf, checksumBuf, clientname, 0);\n    } catch (ChecksumException ce) {\n      LOG.warn(\"Checksum error in block \" + block + \" from \" + inAddr, ce);\n      // No need to report to namenode when client is writing.\n      if (srcDataNode != null && isDatanode) {\n        try {\n          LOG.info(\"report corrupt \" + block + \" from datanode \" +\n                    srcDataNode + \" to namenode\");\n          datanode.reportRemoteBadBlock(srcDataNode, block);\n        } catch (IOException e) {\n          LOG.warn(\"Failed to report bad \" + block + \n                    \" from datanode \" + srcDataNode + \" to namenode\");\n        }\n      }\n      throw new IOException(\"Unexpected checksum mismatch while writing \"\n          + block + \" from \" + inAddr);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.computePartialChunkCrc": "  private void computePartialChunkCrc(long blkoff, long ckoff, \n                                      int bytesPerChecksum) throws IOException {\n\n    // find offset of the beginning of partial chunk.\n    //\n    int sizePartialChunk = (int) (blkoff % bytesPerChecksum);\n    int checksumSize = diskChecksum.getChecksumSize();\n    blkoff = blkoff - sizePartialChunk;\n    LOG.info(\"computePartialChunkCrc sizePartialChunk \" + \n              sizePartialChunk + \" \" + block +\n              \" block offset \" + blkoff +\n              \" metafile offset \" + ckoff);\n\n    // create an input stream from the block file\n    // and read in partial crc chunk into temporary buffer\n    //\n    byte[] buf = new byte[sizePartialChunk];\n    byte[] crcbuf = new byte[checksumSize];\n    ReplicaInputStreams instr = null;\n    try { \n      instr = datanode.data.getTmpInputStreams(block, blkoff, ckoff);\n      IOUtils.readFully(instr.getDataIn(), buf, 0, sizePartialChunk);\n\n      // open meta file and read in crc value computer earlier\n      IOUtils.readFully(instr.getChecksumIn(), crcbuf, 0, crcbuf.length);\n    } finally {\n      IOUtils.closeStream(instr);\n    }\n\n    // compute crc of partial chunk from data read in the block file.\n    partialCrc = DataChecksum.newDataChecksum(\n        diskChecksum.getChecksumType(), diskChecksum.getBytesPerChecksum());\n    partialCrc.update(buf, 0, sizePartialChunk);\n    LOG.info(\"Read in partial CRC chunk from disk for \" + block);\n\n    // paranoia! verify that the pre-computed crc matches what we\n    // recalculated just now\n    if (partialCrc.getValue() != checksum2long(crcbuf)) {\n      String msg = \"Partial CRC \" + partialCrc.getValue() +\n                   \" does not match value computed the \" +\n                   \" last time file was closed \" +\n                   checksum2long(crcbuf);\n      throw new IOException(msg);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.shouldVerifyChecksum": "  private boolean shouldVerifyChecksum() {\n    return (mirrorOut == null || isDatanode || needsChecksumTranslation);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.flushOrSync": "  void flushOrSync(boolean isSync) throws IOException {\n    long flushTotalNanos = 0;\n    long begin = Time.monotonicNow();\n    if (checksumOut != null) {\n      long flushStartNanos = System.nanoTime();\n      checksumOut.flush();\n      long flushEndNanos = System.nanoTime();\n      if (isSync) {\n        long fsyncStartNanos = flushEndNanos;\n        streams.syncChecksumOut();\n        datanode.metrics.addFsyncNanos(System.nanoTime() - fsyncStartNanos);\n      }\n      flushTotalNanos += flushEndNanos - flushStartNanos;\n    }\n    if (out != null) {\n      long flushStartNanos = System.nanoTime();\n      out.flush();\n      long flushEndNanos = System.nanoTime();\n      if (isSync) {\n        long fsyncStartNanos = flushEndNanos;\n        streams.syncDataOut();\n        datanode.metrics.addFsyncNanos(System.nanoTime() - fsyncStartNanos);\n      }\n      flushTotalNanos += flushEndNanos - flushStartNanos;\n    }\n    if (checksumOut != null || out != null) {\n      datanode.metrics.addFlushNanos(flushTotalNanos);\n      if (isSync) {\n    \t  datanode.metrics.incrFsyncCount();      \n      }\n    }\n    long duration = Time.monotonicNow() - begin;\n    if (duration > datanodeSlowLogThresholdMs) {\n      LOG.warn(\"Slow flushOrSync took \" + duration + \"ms (threshold=\"\n          + datanodeSlowLogThresholdMs + \"ms), isSync:\" + isSync + \", flushTotalNanos=\"\n          + flushTotalNanos + \"ns\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock": "  void receiveBlock(\n      DataOutputStream mirrOut, // output to next datanode\n      DataInputStream mirrIn,   // input from next datanode\n      DataOutputStream replyOut,  // output to previous datanode\n      String mirrAddr, DataTransferThrottler throttlerArg,\n      DatanodeInfo[] downstreams) throws IOException {\n\n      syncOnClose = datanode.getDnConf().syncOnClose;\n      boolean responderClosed = false;\n      mirrorOut = mirrOut;\n      mirrorAddr = mirrAddr;\n      throttler = throttlerArg;\n\n    try {\n      if (isClient && !isTransfer) {\n        responder = new Daemon(datanode.threadGroup, \n            new PacketResponder(replyOut, mirrIn, downstreams));\n        responder.start(); // start thread to processes responses\n      }\n\n      while (receivePacket() >= 0) { /* Receive until the last packet */ }\n\n      // wait for all outstanding packet responses. And then\n      // indicate responder to gracefully shutdown.\n      // Mark that responder has been closed for future processing\n      if (responder != null) {\n        ((PacketResponder)responder.getRunnable()).close();\n        responderClosed = true;\n      }\n\n      // If this write is for a replication or transfer-RBW/Finalized,\n      // then finalize block or convert temporary to RBW.\n      // For client-writes, the block is finalized in the PacketResponder.\n      if (isDatanode || isTransfer) {\n        // close the block/crc files\n        close();\n        block.setNumBytes(replicaInfo.getNumBytes());\n\n        if (stage == BlockConstructionStage.TRANSFER_RBW) {\n          // for TRANSFER_RBW, convert temporary to RBW\n          datanode.data.convertTemporaryToRbw(block);\n        } else {\n          // for isDatnode or TRANSFER_FINALIZED\n          // Finalize the block.\n          datanode.data.finalizeBlock(block);\n        }\n        datanode.metrics.incrBlocksWritten();\n      }\n\n    } catch (IOException ioe) {\n      if (datanode.isRestarting()) {\n        // Do not throw if shutting down for restart. Otherwise, it will cause\n        // premature termination of responder.\n        LOG.info(\"Shutting down for restart (\" + block + \").\");\n      } else {\n        LOG.info(\"Exception for \" + block, ioe);\n        throw ioe;\n      }\n    } finally {\n      // Clear the previous interrupt state of this thread.\n      Thread.interrupted();\n\n      // If a shutdown for restart was initiated, upstream needs to be notified.\n      // There is no need to do anything special if the responder was closed\n      // normally.\n      if (!responderClosed) { // Data transfer was not complete.\n        if (responder != null) {\n          // In case this datanode is shutting down for quick restart,\n          // send a special ack upstream.\n          if (datanode.isRestarting() && isClient && !isTransfer) {\n            File blockFile = ((ReplicaInPipeline)replicaInfo).getBlockFile();\n            File restartMeta = new File(blockFile.getParent()  + \n                File.pathSeparator + \".\" + blockFile.getName() + \".restart\");\n            if (restartMeta.exists() && !restartMeta.delete()) {\n              LOG.warn(\"Failed to delete restart meta file: \" +\n                  restartMeta.getPath());\n            }\n            try {\n              FileWriter out = new FileWriter(restartMeta);\n              // write out the current time.\n              out.write(Long.toString(Time.now() + restartBudget));\n              out.flush();\n              out.close();\n            } catch (IOException ioe) {\n              // The worst case is not recovering this RBW replica. \n              // Client will fall back to regular pipeline recovery.\n            }\n            try {\n              ((PacketResponder) responder.getRunnable()).\n                  sendOOBResponse(PipelineAck.getRestartOOBStatus());\n              // Even if the connection is closed after the ack packet is\n              // flushed, the client can react to the connection closure \n              // first. Insert a delay to lower the chance of client \n              // missing the OOB ack.\n              Thread.sleep(1000);\n            } catch (InterruptedException ie) {\n              // It is already going down. Ignore this.\n            } catch (IOException ioe) {\n              LOG.info(\"Error sending OOB Ack.\", ioe);\n            }\n          }\n          responder.interrupt();\n        }\n        IOUtils.closeStream(this);\n        cleanupBlock();\n      }\n      if (responder != null) {\n        try {\n          responder.interrupt();\n          // join() on the responder should timeout a bit earlier than the\n          // configured deadline. Otherwise, the join() on this thread will\n          // likely timeout as well.\n          long joinTimeout = datanode.getDnConf().getXceiverStopTimeout();\n          joinTimeout = joinTimeout > 1  ? joinTimeout*8/10 : joinTimeout;\n          responder.join(joinTimeout);\n          if (responder.isAlive()) {\n            String msg = \"Join on responder thread \" + responder\n                + \" timed out\";\n            LOG.warn(msg + \"\\n\" + StringUtils.getStackTrace(responder));\n            throw new IOException(msg);\n          }\n        } catch (InterruptedException e) {\n          responder.interrupt();\n          // do not throw if shutting down for restart.\n          if (!datanode.isRestarting()) {\n            throw new IOException(\"Interrupted receiveBlock\");\n          }\n        }\n        responder = null;\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.cleanupBlock": "  private void cleanupBlock() throws IOException {\n    if (isDatanode) {\n      datanode.data.unfinalizeBlock(block);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.toString": "    public String toString() {\n      return getClass().getSimpleName() + \"(seqno=\" + seqno\n        + \", lastPacketInBlock=\" + lastPacketInBlock\n        + \", offsetInBlock=\" + offsetInBlock\n        + \", ackEnqueueNanoTime=\" + ackEnqueueNanoTime\n        + \", ackStatus=\" + ackStatus\n        + \")\";\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.close": "    public void close() {\n      synchronized(ackQueue) {\n        while (isRunning() && ackQueue.size() != 0) {\n          try {\n            ackQueue.wait();\n          } catch (InterruptedException e) {\n            running = false;\n            Thread.currentThread().interrupt();\n          }\n        }\n        if(LOG.isDebugEnabled()) {\n          LOG.debug(myString + \": closing\");\n        }\n        running = false;\n        ackQueue.notifyAll();\n      }\n\n      synchronized(this) {\n        running = false;\n        notifyAll();\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock": "  public void writeBlock(final ExtendedBlock block,\n      final StorageType storageType, \n      final Token<BlockTokenIdentifier> blockToken,\n      final String clientname,\n      final DatanodeInfo[] targets,\n      final StorageType[] targetStorageTypes, \n      final DatanodeInfo srcDataNode,\n      final BlockConstructionStage stage,\n      final int pipelineSize,\n      final long minBytesRcvd,\n      final long maxBytesRcvd,\n      final long latestGenerationStamp,\n      DataChecksum requestedChecksum,\n      CachingStrategy cachingStrategy) throws IOException {\n    previousOpClientName = clientname;\n    updateCurrentThreadName(\"Receiving block \" + block);\n    final boolean isDatanode = clientname.length() == 0;\n    final boolean isClient = !isDatanode;\n    final boolean isTransfer = stage == BlockConstructionStage.TRANSFER_RBW\n        || stage == BlockConstructionStage.TRANSFER_FINALIZED;\n\n    // check single target for transfer-RBW/Finalized \n    if (isTransfer && targets.length > 0) {\n      throw new IOException(stage + \" does not support multiple targets \"\n          + Arrays.asList(targets));\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"opWriteBlock: stage=\" + stage + \", clientname=\" + clientname \n      \t\t+ \"\\n  block  =\" + block + \", newGs=\" + latestGenerationStamp\n      \t\t+ \", bytesRcvd=[\" + minBytesRcvd + \", \" + maxBytesRcvd + \"]\"\n          + \"\\n  targets=\" + Arrays.asList(targets)\n          + \"; pipelineSize=\" + pipelineSize + \", srcDataNode=\" + srcDataNode\n          );\n      LOG.debug(\"isDatanode=\" + isDatanode\n          + \", isClient=\" + isClient\n          + \", isTransfer=\" + isTransfer);\n      LOG.debug(\"writeBlock receive buf size \" + peer.getReceiveBufferSize() +\n                \" tcp no delay \" + peer.getTcpNoDelay());\n    }\n\n    // We later mutate block's generation stamp and length, but we need to\n    // forward the original version of the block to downstream mirrors, so\n    // make a copy here.\n    final ExtendedBlock originalBlock = new ExtendedBlock(block);\n    block.setNumBytes(dataXceiverServer.estimateBlockSize);\n    LOG.info(\"Receiving \" + block + \" src: \" + remoteAddress + \" dest: \"\n        + localAddress);\n\n    // reply to upstream datanode or client \n    final DataOutputStream replyOut = new DataOutputStream(\n        new BufferedOutputStream(\n            getOutputStream(),\n            HdfsConstants.SMALL_BUFFER_SIZE));\n    checkAccess(replyOut, isClient, block, blockToken,\n        Op.WRITE_BLOCK, BlockTokenSecretManager.AccessMode.WRITE);\n\n    DataOutputStream mirrorOut = null;  // stream to next target\n    DataInputStream mirrorIn = null;    // reply from next target\n    Socket mirrorSock = null;           // socket to next target\n    BlockReceiver blockReceiver = null; // responsible for data handling\n    String mirrorNode = null;           // the name:port of next target\n    String firstBadLink = \"\";           // first datanode that failed in connection setup\n    Status mirrorInStatus = SUCCESS;\n    final String storageUuid;\n    try {\n      if (isDatanode || \n          stage != BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        // open a block receiver\n        blockReceiver = new BlockReceiver(block, storageType, in,\n            peer.getRemoteAddressString(),\n            peer.getLocalAddressString(),\n            stage, latestGenerationStamp, minBytesRcvd, maxBytesRcvd,\n            clientname, srcDataNode, datanode, requestedChecksum,\n            cachingStrategy);\n        \n        storageUuid = blockReceiver.getStorageUuid();\n      } else {\n        storageUuid = datanode.data.recoverClose(\n            block, latestGenerationStamp, minBytesRcvd);\n      }\n\n      //\n      // Connect to downstream machine, if appropriate\n      //\n      if (targets.length > 0) {\n        InetSocketAddress mirrorTarget = null;\n        // Connect to backup machine\n        mirrorNode = targets[0].getXferAddr(connectToDnViaHostname);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Connecting to datanode \" + mirrorNode);\n        }\n        mirrorTarget = NetUtils.createSocketAddr(mirrorNode);\n        mirrorSock = datanode.newSocket();\n        try {\n          int timeoutValue = dnConf.socketTimeout\n              + (HdfsServerConstants.READ_TIMEOUT_EXTENSION * targets.length);\n          int writeTimeout = dnConf.socketWriteTimeout + \n                      (HdfsServerConstants.WRITE_TIMEOUT_EXTENSION * targets.length);\n          NetUtils.connect(mirrorSock, mirrorTarget, timeoutValue);\n          mirrorSock.setSoTimeout(timeoutValue);\n          mirrorSock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n          \n          OutputStream unbufMirrorOut = NetUtils.getOutputStream(mirrorSock,\n              writeTimeout);\n          InputStream unbufMirrorIn = NetUtils.getInputStream(mirrorSock);\n          DataEncryptionKeyFactory keyFactory =\n            datanode.getDataEncryptionKeyFactoryForBlock(block);\n          IOStreamPair saslStreams = datanode.saslClient.socketSend(mirrorSock,\n            unbufMirrorOut, unbufMirrorIn, keyFactory, blockToken, targets[0]);\n          unbufMirrorOut = saslStreams.out;\n          unbufMirrorIn = saslStreams.in;\n          mirrorOut = new DataOutputStream(new BufferedOutputStream(unbufMirrorOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          mirrorIn = new DataInputStream(unbufMirrorIn);\n\n          new Sender(mirrorOut).writeBlock(originalBlock, targetStorageTypes[0],\n              blockToken, clientname, targets, targetStorageTypes, srcDataNode,\n              stage, pipelineSize, minBytesRcvd, maxBytesRcvd,\n              latestGenerationStamp, requestedChecksum, cachingStrategy);\n\n          mirrorOut.flush();\n\n          // read connect ack (only for clients, not for replication req)\n          if (isClient) {\n            BlockOpResponseProto connectAck =\n              BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(mirrorIn));\n            mirrorInStatus = connectAck.getStatus();\n            firstBadLink = connectAck.getFirstBadLink();\n            if (LOG.isDebugEnabled() || mirrorInStatus != SUCCESS) {\n              LOG.info(\"Datanode \" + targets.length +\n                       \" got response for connect ack \" +\n                       \" from downstream datanode with firstbadlink as \" +\n                       firstBadLink);\n            }\n          }\n\n        } catch (IOException e) {\n          if (isClient) {\n            BlockOpResponseProto.newBuilder()\n              .setStatus(ERROR)\n               // NB: Unconditionally using the xfer addr w/o hostname\n              .setFirstBadLink(targets[0].getXferAddr())\n              .build()\n              .writeDelimitedTo(replyOut);\n            replyOut.flush();\n          }\n          IOUtils.closeStream(mirrorOut);\n          mirrorOut = null;\n          IOUtils.closeStream(mirrorIn);\n          mirrorIn = null;\n          IOUtils.closeSocket(mirrorSock);\n          mirrorSock = null;\n          if (isClient) {\n            LOG.error(datanode + \":Exception transfering block \" +\n                      block + \" to mirror \" + mirrorNode + \": \" + e);\n            throw e;\n          } else {\n            LOG.info(datanode + \":Exception transfering \" +\n                     block + \" to mirror \" + mirrorNode +\n                     \"- continuing without the mirror\", e);\n          }\n        }\n      }\n\n      // send connect-ack to source for clients and not transfer-RBW/Finalized\n      if (isClient && !isTransfer) {\n        if (LOG.isDebugEnabled() || mirrorInStatus != SUCCESS) {\n          LOG.info(\"Datanode \" + targets.length +\n                   \" forwarding connect ack to upstream firstbadlink is \" +\n                   firstBadLink);\n        }\n        BlockOpResponseProto.newBuilder()\n          .setStatus(mirrorInStatus)\n          .setFirstBadLink(firstBadLink)\n          .build()\n          .writeDelimitedTo(replyOut);\n        replyOut.flush();\n      }\n\n      // receive the block and mirror to the next target\n      if (blockReceiver != null) {\n        String mirrorAddr = (mirrorSock == null) ? null : mirrorNode;\n        blockReceiver.receiveBlock(mirrorOut, mirrorIn, replyOut,\n            mirrorAddr, null, targets);\n\n        // send close-ack for transfer-RBW/Finalized \n        if (isTransfer) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"TRANSFER: send close-ack\");\n          }\n          writeResponse(SUCCESS, null, replyOut);\n        }\n      }\n\n      // update its generation stamp\n      if (isClient && \n          stage == BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        block.setGenerationStamp(latestGenerationStamp);\n        block.setNumBytes(minBytesRcvd);\n      }\n      \n      // if this write is for a replication request or recovering\n      // a failed close for client, then confirm block. For other client-writes,\n      // the block is finalized in the PacketResponder.\n      if (isDatanode ||\n          stage == BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        datanode.closeBlock(block, DataNode.EMPTY_DEL_HINT, storageUuid);\n        LOG.info(\"Received \" + block + \" src: \" + remoteAddress + \" dest: \"\n            + localAddress + \" of size \" + block.getNumBytes());\n      }\n\n      \n    } catch (IOException ioe) {\n      LOG.info(\"opWriteBlock \" + block + \" received exception \" + ioe);\n      throw ioe;\n    } finally {\n      // close all opened streams\n      IOUtils.closeStream(mirrorOut);\n      IOUtils.closeStream(mirrorIn);\n      IOUtils.closeStream(replyOut);\n      IOUtils.closeSocket(mirrorSock);\n      IOUtils.closeStream(blockReceiver);\n    }\n\n    //update metrics\n    datanode.metrics.addWriteBlockOp(elapsed());\n    datanode.metrics.incrWritesFromClient(peer.isLocal());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.elapsed": "  private long elapsed() {\n    return now() - opStartTime;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeResponse": "  private static void writeResponse(Status status, String message, OutputStream out)\n  throws IOException {\n    BlockOpResponseProto.Builder response = BlockOpResponseProto.newBuilder()\n      .setStatus(status);\n    if (message != null) {\n      response.setMessage(message);\n    }\n    response.build().writeDelimitedTo(out);\n    out.flush();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.getOutputStream": "  private OutputStream getOutputStream() {\n    return socketOut;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess": "  private void checkAccess(OutputStream out, final boolean reply, \n      final ExtendedBlock blk,\n      final Token<BlockTokenIdentifier> t,\n      final Op op,\n      final BlockTokenSecretManager.AccessMode mode) throws IOException {\n    if (datanode.isBlockTokenEnabled) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Checking block access token for block '\" + blk.getBlockId()\n            + \"' with mode '\" + mode + \"'\");\n      }\n      try {\n        datanode.blockPoolTokenSecretManager.checkAccess(t, null, blk, mode);\n      } catch(InvalidToken e) {\n        try {\n          if (reply) {\n            BlockOpResponseProto.Builder resp = BlockOpResponseProto.newBuilder()\n              .setStatus(ERROR_ACCESS_TOKEN);\n            if (mode == BlockTokenSecretManager.AccessMode.WRITE) {\n              DatanodeRegistration dnR = \n                datanode.getDNRegistrationForBP(blk.getBlockPoolId());\n              // NB: Unconditionally using the xfer addr w/o hostname\n              resp.setFirstBadLink(dnR.getXferAddr());\n            }\n            resp.build().writeDelimitedTo(out);\n            out.flush();\n          }\n          LOG.warn(\"Block token verification failed: op=\" + op\n              + \", remoteAddress=\" + remoteAddress\n              + \", message=\" + e.getLocalizedMessage());\n          throw e;\n        } finally {\n          IOUtils.closeStream(out);\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.updateCurrentThreadName": "  private void updateCurrentThreadName(String status) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"DataXceiver for client \");\n    if (previousOpClientName != null) {\n      sb.append(previousOpClientName).append(\" at \");\n    }\n    sb.append(remoteAddress);\n    if (status != null) {\n      sb.append(\" [\").append(status).append(\"]\");\n    }\n    Thread.currentThread().setName(sb.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock": "  private void opWriteBlock(DataInputStream in) throws IOException {\n    final OpWriteBlockProto proto = OpWriteBlockProto.parseFrom(vintPrefixed(in));\n    final DatanodeInfo[] targets = PBHelper.convert(proto.getTargetsList());\n    writeBlock(PBHelper.convert(proto.getHeader().getBaseHeader().getBlock()),\n        PBHelper.convertStorageType(proto.getStorageType()),\n        PBHelper.convert(proto.getHeader().getBaseHeader().getToken()),\n        proto.getHeader().getClientName(),\n        targets,\n        PBHelper.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length),\n        PBHelper.convert(proto.getSource()),\n        fromProto(proto.getStage()),\n        proto.getPipelineSize(),\n        proto.getMinBytesRcvd(), proto.getMaxBytesRcvd(),\n        proto.getLatestGenerationStamp(),\n        fromProto(proto.getRequestedChecksum()),\n        (proto.hasCachingStrategy() ?\n            getCachingStrategy(proto.getCachingStrategy()) :\n          CachingStrategy.newDefaultStrategy()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.getCachingStrategy": "  static private CachingStrategy getCachingStrategy(CachingStrategyProto strategy) {\n    Boolean dropBehind = strategy.hasDropBehind() ?\n        strategy.getDropBehind() : null;\n    Long readahead = strategy.hasReadahead() ?\n        strategy.getReadahead() : null;\n    return new CachingStrategy(dropBehind, readahead);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp": "  protected final void processOp(Op op) throws IOException {\n    switch(op) {\n    case READ_BLOCK:\n      opReadBlock();\n      break;\n    case WRITE_BLOCK:\n      opWriteBlock(in);\n      break;\n    case REPLACE_BLOCK:\n      opReplaceBlock(in);\n      break;\n    case COPY_BLOCK:\n      opCopyBlock(in);\n      break;\n    case BLOCK_CHECKSUM:\n      opBlockChecksum(in);\n      break;\n    case TRANSFER_BLOCK:\n      opTransferBlock(in);\n      break;\n    case REQUEST_SHORT_CIRCUIT_FDS:\n      opRequestShortCircuitFds(in);\n      break;\n    case RELEASE_SHORT_CIRCUIT_FDS:\n      opReleaseShortCircuitFds(in);\n      break;\n    case REQUEST_SHORT_CIRCUIT_SHM:\n      opRequestShortCircuitShm(in);\n      break;\n    default:\n      throw new IOException(\"Unknown op \" + op + \" in data stream\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReplaceBlock": "  private void opReplaceBlock(DataInputStream in) throws IOException {\n    OpReplaceBlockProto proto = OpReplaceBlockProto.parseFrom(vintPrefixed(in));\n    replaceBlock(PBHelper.convert(proto.getHeader().getBlock()),\n        PBHelper.convertStorageType(proto.getStorageType()),\n        PBHelper.convert(proto.getHeader().getToken()),\n        proto.getDelHint(),\n        PBHelper.convert(proto.getSource()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitFds": "  private void opRequestShortCircuitFds(DataInputStream in) throws IOException {\n    final OpRequestShortCircuitAccessProto proto =\n      OpRequestShortCircuitAccessProto.parseFrom(vintPrefixed(in));\n    SlotId slotId = (proto.hasSlotId()) ? \n        PBHelper.convert(proto.getSlotId()) : null;\n    requestShortCircuitFds(PBHelper.convert(proto.getHeader().getBlock()),\n        PBHelper.convert(proto.getHeader().getToken()),\n        slotId, proto.getMaxVersion());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitShm": "  private void opRequestShortCircuitShm(DataInputStream in) throws IOException {\n    final ShortCircuitShmRequestProto proto =\n        ShortCircuitShmRequestProto.parseFrom(vintPrefixed(in));\n    requestShortCircuitShm(proto.getClientName());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opBlockChecksum": "  private void opBlockChecksum(DataInputStream in) throws IOException {\n    OpBlockChecksumProto proto = OpBlockChecksumProto.parseFrom(vintPrefixed(in));\n    \n    blockChecksum(PBHelper.convert(proto.getHeader().getBlock()),\n        PBHelper.convert(proto.getHeader().getToken()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReleaseShortCircuitFds": "  private void opReleaseShortCircuitFds(DataInputStream in)\n      throws IOException {\n    final ReleaseShortCircuitAccessRequestProto proto =\n      ReleaseShortCircuitAccessRequestProto.parseFrom(vintPrefixed(in));\n    releaseShortCircuitFds(PBHelper.convert(proto.getSlotId()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opTransferBlock": "  private void opTransferBlock(DataInputStream in) throws IOException {\n    final OpTransferBlockProto proto =\n      OpTransferBlockProto.parseFrom(vintPrefixed(in));\n    final DatanodeInfo[] targets = PBHelper.convert(proto.getTargetsList());\n    transferBlock(PBHelper.convert(proto.getHeader().getBaseHeader().getBlock()),\n        PBHelper.convert(proto.getHeader().getBaseHeader().getToken()),\n        proto.getHeader().getClientName(),\n        targets,\n        PBHelper.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opCopyBlock": "  private void opCopyBlock(DataInputStream in) throws IOException {\n    OpCopyBlockProto proto = OpCopyBlockProto.parseFrom(vintPrefixed(in));\n    copyBlock(PBHelper.convert(proto.getHeader().getBlock()),\n        PBHelper.convert(proto.getHeader().getToken()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock": "  private void opReadBlock() throws IOException {\n    OpReadBlockProto proto = OpReadBlockProto.parseFrom(vintPrefixed(in));\n    readBlock(PBHelper.convert(proto.getHeader().getBaseHeader().getBlock()),\n        PBHelper.convert(proto.getHeader().getBaseHeader().getToken()),\n        proto.getHeader().getClientName(),\n        proto.getOffset(),\n        proto.getLen(),\n        proto.getSendChecksums(),\n        (proto.hasCachingStrategy() ?\n            getCachingStrategy(proto.getCachingStrategy()) :\n          CachingStrategy.newDefaultStrategy()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.run": "  public void run() {\n    int opsProcessed = 0;\n    Op op = null;\n\n    try {\n      dataXceiverServer.addPeer(peer, Thread.currentThread());\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input = socketIn;\n      IOStreamPair saslStreams = datanode.saslServer.receive(peer, socketOut,\n        socketIn, datanode.getDatanodeId());\n      input = new BufferedInputStream(saslStreams.in,\n        HdfsConstants.SMALL_BUFFER_SIZE);\n      socketOut = saslStreams.out;\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed != 0) {\n            assert dnConf.socketKeepaliveTimeout > 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op = readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it's quite normal to get EOF here.\n          if (opsProcessed > 0 &&\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed != 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime = now();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer != null) &&\n          (!peer.isClosed() && dnConf.socketKeepaliveTimeout > 0));\n    } catch (Throwable t) {\n      String s = datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op == null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op == Op.WRITE_BLOCK && t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer != null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader.getHeaderSize": "  public static int getHeaderSize() {\n    return Short.SIZE/Byte.SIZE + DataChecksum.getChecksumHeaderSize();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.checkDiskError": "  public void checkDiskError() {\n    synchronized(checkDiskErrorMutex) {\n      checkDiskErrorFlag = true;\n      if(checkDiskErrorThread == null) {\n        startCheckDiskErrorThread();\n        checkDiskErrorThread.start();\n        LOG.info(\"Starting CheckDiskError Thread\");\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.startCheckDiskErrorThread": "  private void startCheckDiskErrorThread() {\n    checkDiskErrorThread = new Thread(new Runnable() {\n          @Override\n          public void run() {\n            while(shouldRun) {\n              boolean tempFlag ;\n              synchronized(checkDiskErrorMutex) {\n                tempFlag = checkDiskErrorFlag;\n                checkDiskErrorFlag = false;\n              }\n              if(tempFlag) {\n                try {\n                  data.checkDataDir();\n                } catch (DiskErrorException de) {\n                  handleDiskError(de.getMessage());\n                } catch (Exception e) {\n                  LOG.warn(\"Unexpected exception occurred while checking disk error  \" + e);\n                  checkDiskErrorThread = null;\n                  return;\n                }\n                synchronized(checkDiskErrorMutex) {\n                  lastDiskErrorCheck = Time.monotonicNow();\n                }\n              }\n              try {\n                Thread.sleep(checkDiskErrorInterval);\n              } catch (InterruptedException e) {\n                LOG.debug(\"InterruptedException in check disk error thread\", e);\n                checkDiskErrorThread = null;\n                return;\n              }\n            }\n          }\n    });\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDnConf": "  public DNConf getDnConf() {\n    return dnConf;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.isRestarting": "  boolean isRestarting() {\n    return shutdownForUpgrade;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.newSocket": "  protected Socket newSocket() throws IOException {\n    return (dnConf.socketWriteTimeout > 0) ? \n           SocketChannel.open().socket() : new Socket();                                   \n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.getStorageUuid": "  String getStorageUuid() {\n    return replicaInfo.getStorageUuid();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.closeBlock": "  void closeBlock(ExtendedBlock block, String delHint, String storageUuid) {\n    metrics.incrBlocksWritten();\n    BPOfferService bpos = blockPoolManager.get(block.getBlockPoolId());\n    if(bpos != null) {\n      bpos.notifyNamenodeReceivedBlock(block, delHint, storageUuid);\n    } else {\n      LOG.warn(\"Cannot find BPOfferService for reporting block received for bpid=\"\n          + block.getBlockPoolId());\n    }\n    if (blockScanner != null) {\n      blockScanner.addBlock(block);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.notifyNamenodeReceivedBlock": "  protected void notifyNamenodeReceivedBlock(\n      ExtendedBlock block, String delHint, String storageUuid) {\n    BPOfferService bpos = blockPoolManager.get(block.getBlockPoolId());\n    if(bpos != null) {\n      bpos.notifyNamenodeReceivedBlock(block, delHint, storageUuid);\n    } else {\n      LOG.error(\"Cannot find BPOfferService for reporting block received for bpid=\"\n          + block.getBlockPoolId());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDataEncryptionKeyFactoryForBlock": "  DataEncryptionKeyFactory getDataEncryptionKeyFactoryForBlock(\n      final ExtendedBlock block) {\n    return new DataEncryptionKeyFactory() {\n      @Override\n      public DataEncryptionKey newDataEncryptionKey() {\n        return dnConf.encryptDataTransfer ?\n          blockPoolTokenSecretManager.generateDataEncryptionKey(\n            block.getBlockPoolId()) : null;\n      }\n    };\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXceiverCount": "  public int getXceiverCount() {\n    return threadGroup == null ? 0 : threadGroup.activeCount();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDisplayName": "  public String getDisplayName() {\n    // NB: our DatanodeID may not be set yet\n    return hostName + \":\" + getXferPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXferPort": "  public int getXferPort() {\n    return streamingAddr.getPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dataXceiverServer.closePeer": "  synchronized void closePeer(Peer peer) {\n    peers.remove(peer);\n    IOUtils.cleanup(null, peer);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dataXceiverServer.addPeer": "  synchronized void addPeer(Peer peer, Thread t) throws IOException {\n    if (closed) {\n      throw new IOException(\"Server closed.\");\n    }\n    peers.put(peer, t);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDatanodeId": "  public DatanodeID getDatanodeId() {\n    return id;\n  }"
        },
        "bug_report": {
            "Title": "Add test for race condition between transferring block and appending block causes \"Unexpected checksum mismatch exception\" ",
            "Description": "We found some error log in the datanode. like this\n{noformat}\n2014-07-22 01:49:51,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Ex\nception for BP-2072804351-192.168.2.104-1406008383435:blk_1073741997_9248\njava.io.IOException: Terminating due to a checksum error.java.io.IOException: Unexpected checksum mismatch while writing BP-2072804351-192.168.2.104-1406008383435:blk_1073741997_9248 from /192.168.2.101:39495\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:536)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:703)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:575)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:115)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:68)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)\n        at java.lang.Thread.run(Thread.java:744)\n{noformat}\nWhile on the source datanode, the log says the block is transmitted.\n{noformat}\n2014-07-22 01:49:50,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Da\ntaTransfer: Transmitted BP-2072804351-192.168.2.104-1406008383435:blk_1073741997\n_9248 (numBytes=16188152) to /192.168.2.103:50010\n{noformat}\n\nWhen the destination datanode gets the checksum mismatch, it reports bad block to NameNode and NameNode marks the replica on the source datanode as corrupt. But actually, the replica on the source datanode is valid. Because the replica can pass the checksum verification.\n\nIn all, the replica on the source data is wrongly marked as corrupted."
        }
    },
    {
        "filename": "HDFS-5843.json",
        "creation_time": "2014-01-28T06:05:16.000+0000",
        "stack_trace": "```\njava.io.IOException: Fail to get block MD5 for BP-341493254-192.168.1.10-1390888724459:blk_1073741825_1001\n\tat org.apache.hadoop.hdfs.DFSClient.getFileChecksum(DFSClient.java:1965)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileChecksum(DFSClient.java:1771)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1186)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1194)\n\njava.lang.ArithmeticException: / by zero\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockChecksum(DataXceiver.java:658)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opBlockChecksum(Receiver.java:169)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:77)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)\n\tat java.lang.Thread.run(Thread.java:695)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.getFileChecksum": "  private static MD5MD5CRC32FileChecksum getFileChecksum(String src,\n      String clientName,\n      ClientProtocol namenode, SocketFactory socketFactory, int socketTimeout,\n      DataEncryptionKey encryptionKey, boolean connectToDnViaHostname)\n      throws IOException {\n    //get all block locations\n    LocatedBlocks blockLocations = callGetBlockLocations(namenode, src, 0, Long.MAX_VALUE);\n    if (null == blockLocations) {\n      throw new FileNotFoundException(\"File does not exist: \" + src);\n    }\n    List<LocatedBlock> locatedblocks = blockLocations.getLocatedBlocks();\n    final DataOutputBuffer md5out = new DataOutputBuffer();\n    int bytesPerCRC = -1;\n    DataChecksum.Type crcType = DataChecksum.Type.DEFAULT;\n    long crcPerBlock = 0;\n    boolean refetchBlocks = false;\n    int lastRetriedIndex = -1;\n\n    //get block checksum for each block\n    for(int i = 0; i < locatedblocks.size(); i++) {\n      if (refetchBlocks) {  // refetch to get fresh tokens\n        blockLocations = callGetBlockLocations(namenode, src, 0, Long.MAX_VALUE);\n        if (null == blockLocations) {\n          throw new FileNotFoundException(\"File does not exist: \" + src);\n        }\n        locatedblocks = blockLocations.getLocatedBlocks();\n        refetchBlocks = false;\n      }\n      LocatedBlock lb = locatedblocks.get(i);\n      final ExtendedBlock block = lb.getBlock();\n      final DatanodeInfo[] datanodes = lb.getLocations();\n      \n      //try each datanode location of the block\n      final int timeout = 3000 * datanodes.length + socketTimeout;\n      boolean done = false;\n      for(int j = 0; !done && j < datanodes.length; j++) {\n        DataOutputStream out = null;\n        DataInputStream in = null;\n        \n        try {\n          //connect to a datanode\n          IOStreamPair pair = connectToDN(socketFactory, connectToDnViaHostname,\n              encryptionKey, datanodes[j], timeout);\n          out = new DataOutputStream(new BufferedOutputStream(pair.out,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          in = new DataInputStream(pair.in);\n\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"write to \" + datanodes[j] + \": \"\n                + Op.BLOCK_CHECKSUM + \", block=\" + block);\n          }\n          // get block MD5\n          new Sender(out).blockChecksum(block, lb.getBlockToken());\n\n          final BlockOpResponseProto reply =\n            BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n\n          if (reply.getStatus() != Status.SUCCESS) {\n            if (reply.getStatus() == Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException();\n            } else {\n              throw new IOException(\"Bad response \" + reply + \" for block \"\n                  + block + \" from datanode \" + datanodes[j]);\n            }\n          }\n          \n          OpBlockChecksumResponseProto checksumData =\n            reply.getChecksumResponse();\n\n          //read byte-per-checksum\n          final int bpc = checksumData.getBytesPerCrc();\n          if (i == 0) { //first block\n            bytesPerCRC = bpc;\n          }\n          else if (bpc != bytesPerCRC) {\n            throw new IOException(\"Byte-per-checksum not matched: bpc=\" + bpc\n                + \" but bytesPerCRC=\" + bytesPerCRC);\n          }\n          \n          //read crc-per-block\n          final long cpb = checksumData.getCrcPerBlock();\n          if (locatedblocks.size() > 1 && i == 0) {\n            crcPerBlock = cpb;\n          }\n\n          //read md5\n          final MD5Hash md5 = new MD5Hash(\n              checksumData.getMd5().toByteArray());\n          md5.write(md5out);\n          \n          // read crc-type\n          final DataChecksum.Type ct;\n          if (checksumData.hasCrcType()) {\n            ct = PBHelper.convert(checksumData\n                .getCrcType());\n          } else {\n            LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                      \"inferring checksum by reading first byte\");\n            ct = inferChecksumTypeByReading(\n                clientName, socketFactory, socketTimeout, lb, datanodes[j],\n                encryptionKey, connectToDnViaHostname);\n          }\n\n          if (i == 0) { // first block\n            crcType = ct;\n          } else if (crcType != DataChecksum.Type.MIXED\n              && crcType != ct) {\n            // if crc types are mixed in a file\n            crcType = DataChecksum.Type.MIXED;\n          }\n\n          done = true;\n\n          if (LOG.isDebugEnabled()) {\n            if (i == 0) {\n              LOG.debug(\"set bytesPerCRC=\" + bytesPerCRC\n                  + \", crcPerBlock=\" + crcPerBlock);\n            }\n            LOG.debug(\"got reply from \" + datanodes[j] + \": md5=\" + md5);\n          }\n        } catch (InvalidBlockTokenException ibte) {\n          if (i > lastRetriedIndex) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                  + \"for file \" + src + \" for block \" + block\n                  + \" from datanode \" + datanodes[j]\n                  + \". Will retry the block once.\");\n            }\n            lastRetriedIndex = i;\n            done = true; // actually it's not done; but we'll retry\n            i--; // repeat at i-th block\n            refetchBlocks = true;\n            break;\n          }\n        } catch (IOException ie) {\n          LOG.warn(\"src=\" + src + \", datanodes[\"+j+\"]=\" + datanodes[j], ie);\n        } finally {\n          IOUtils.closeStream(in);\n          IOUtils.closeStream(out);\n        }\n      }\n\n      if (!done) {\n        throw new IOException(\"Fail to get block MD5 for \" + block);\n      }\n    }\n\n    //compute file MD5\n    final MD5Hash fileMD5 = MD5Hash.digest(md5out.getData()); \n    switch (crcType) {\n      case CRC32:\n        return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      case CRC32C:\n        return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      default:\n        // If there is no block allocated for the file,\n        // return one with the magic entry that matches what previous\n        // hdfs versions return.\n        if (locatedblocks.size() == 0) {\n          return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n        }\n\n        // we should never get here since the validity was checked\n        // when getCrcType() was called above.\n        return null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations": "  static LocatedBlocks callGetBlockLocations(ClientProtocol namenode,\n      String src, long start, long length) \n      throws IOException {\n    try {\n      return namenode.getBlockLocations(src, start, length);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     UnresolvedPathException.class);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.inferChecksumTypeByReading": "  private static Type inferChecksumTypeByReading(\n      String clientName, SocketFactory socketFactory, int socketTimeout,\n      LocatedBlock lb, DatanodeInfo dn,\n      DataEncryptionKey encryptionKey, boolean connectToDnViaHostname)\n      throws IOException {\n    IOStreamPair pair = connectToDN(socketFactory, connectToDnViaHostname,\n        encryptionKey, dn, socketTimeout);\n\n    try {\n      DataOutputStream out = new DataOutputStream(new BufferedOutputStream(pair.out,\n          HdfsConstants.SMALL_BUFFER_SIZE));\n      DataInputStream in = new DataInputStream(pair.in);\n  \n      new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n          0, 1, true, CachingStrategy.newDefaultStrategy());\n      final BlockOpResponseProto reply =\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n      \n      if (reply.getStatus() != Status.SUCCESS) {\n        if (reply.getStatus() == Status.ERROR_ACCESS_TOKEN) {\n          throw new InvalidBlockTokenException();\n        } else {\n          throw new IOException(\"Bad response \" + reply + \" trying to read \"\n              + lb.getBlock() + \" from datanode \" + dn);\n        }\n      }\n      \n      return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n    } finally {\n      IOUtils.cleanup(null, pair.in, pair.out);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.get": "    synchronized ClientMmapManager get(Configuration conf) {\n      if (refcnt++ == 0) {\n        mmapManager = ClientMmapManager.fromConf(conf);\n      } else {\n        String mismatches = mmapManager.verifyConfigurationMatches(conf);\n        if (!mismatches.isEmpty()) {\n          LOG.warn(\"The ClientMmapManager settings you specified \" +\n            \"have been ignored because another thread created the \" +\n            \"ClientMmapManager first.  \" + mismatches);\n        }\n      }\n      return mmapManager;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.checkOpen": "  void checkOpen() throws IOException {\n    if (!clientRunning) {\n      IOException result = new IOException(\"Filesystem closed\");\n      throw result;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.getDataEncryptionKey": "  public DataEncryptionKey getDataEncryptionKey()\n      throws IOException {\n    if (shouldEncryptData()) {\n      synchronized (this) {\n        if (encryptionKey == null ||\n            encryptionKey.expiryDate < Time.now()) {\n          LOG.debug(\"Getting new encryption token from NN\");\n          encryptionKey = namenode.getDataEncryptionKey();\n        }\n        return encryptionKey;\n      }\n    } else {\n      return null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks": "  public LocatedBlocks getLocatedBlocks(String src, long start, long length)\n      throws IOException {\n    return callGetBlockLocations(namenode, src, start, length);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.connectToDN": "  private static IOStreamPair connectToDN(\n      SocketFactory socketFactory, boolean connectToDnViaHostname,\n      DataEncryptionKey encryptionKey, DatanodeInfo dn, int timeout)\n      throws IOException\n  {\n    boolean success = false;\n    Socket sock = null;\n    try {\n      sock = socketFactory.createSocket();\n      String dnAddr = dn.getXferAddr(connectToDnViaHostname);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Connecting to datanode \" + dnAddr);\n      }\n      NetUtils.connect(sock, NetUtils.createSocketAddr(dnAddr), timeout);\n      sock.setSoTimeout(timeout);\n  \n      OutputStream unbufOut = NetUtils.getOutputStream(sock);\n      InputStream unbufIn = NetUtils.getInputStream(sock);\n      IOStreamPair ret;\n      if (encryptionKey != null) {\n        ret = DataTransferEncryptor.getEncryptedStreams(\n                unbufOut, unbufIn, encryptionKey);\n      } else {\n        ret = new IOStreamPair(unbufIn, unbufOut);        \n      }\n      success = true;\n      return ret;\n    } finally {\n      if (!success) {\n        IOUtils.closeSocket(sock);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.doCall": "      public Boolean doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.isFileClosed(getPathName(p));\n      }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs": "  public boolean mkdirs(Path f, FsPermission permission) throws IOException {\n    return mkdirsInternal(f, permission, true);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setOwner": "  public void setOwner(Path p, final String username, final String groupname\n      ) throws IOException {\n    if (username == null && groupname == null) {\n      throw new IOException(\"username == null && groupname == null\");\n    }\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(p);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.setOwner(getPathName(p), username, groupname);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        fs.setOwner(p, username, groupname);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getUri": "  public URI getUri() { return uri; }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal": "  private FileStatus[] listStatusInternal(Path p) throws IOException {\n    String src = getPathName(p);\n\n    // fetch the first batch of entries in the directory\n    DirectoryListing thisListing = dfs.listPaths(\n        src, HdfsFileStatus.EMPTY_NAME);\n\n    if (thisListing == null) { // the directory does not exist\n      throw new FileNotFoundException(\"File \" + p + \" does not exist.\");\n    }\n    \n    HdfsFileStatus[] partialListing = thisListing.getPartialListing();\n    if (!thisListing.hasMore()) { // got all entries of the directory\n      FileStatus[] stats = new FileStatus[partialListing.length];\n      for (int i = 0; i < partialListing.length; i++) {\n        stats[i] = partialListing[i].makeQualified(getUri(), p);\n      }\n      statistics.incrementReadOps(1);\n      return stats;\n    }\n\n    // The directory size is too big that it needs to fetch more\n    // estimate the total number of entries in the directory\n    int totalNumEntries =\n      partialListing.length + thisListing.getRemainingEntries();\n    ArrayList<FileStatus> listing =\n      new ArrayList<FileStatus>(totalNumEntries);\n    // add the first batch of entries to the array list\n    for (HdfsFileStatus fileStatus : partialListing) {\n      listing.add(fileStatus.makeQualified(getUri(), p));\n    }\n    statistics.incrementLargeReadOps(1);\n \n    // now fetch more entries\n    do {\n      thisListing = dfs.listPaths(src, thisListing.getLastName());\n \n      if (thisListing == null) { // the directory is deleted\n        throw new FileNotFoundException(\"File \" + p + \" does not exist.\");\n      }\n \n      partialListing = thisListing.getPartialListing();\n      for (HdfsFileStatus fileStatus : partialListing) {\n        listing.add(fileStatus.makeQualified(getUri(), p));\n      }\n      statistics.incrementLargeReadOps(1);\n    } while (thisListing.hasMore());\n \n    return listing.toArray(new FileStatus[listing.size()]);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.create": "  public FSDataOutputStream create(final Path f, final FsPermission permission,\n    final EnumSet<CreateFlag> cflags, final int bufferSize,\n    final short replication, final long blockSize, final Progressable progress,\n    final ChecksumOpt checksumOpt) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FSDataOutputStream>() {\n      @Override\n      public FSDataOutputStream doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return new HdfsDataOutputStream(dfs.create(getPathName(p), permission,\n            cflags, replication, blockSize, progress, bufferSize, checksumOpt),\n            statistics);\n      }\n      @Override\n      public FSDataOutputStream next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.create(p, permission, cflags, bufferSize,\n            replication, blockSize, progress, checksumOpt);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getSnapshotDiffReport": "  public SnapshotDiffReport getSnapshotDiffReport(final Path snapshotDir,\n      final String fromSnapshot, final String toSnapshot) throws IOException {\n    Path absF = fixRelativePart(snapshotDir);\n    return new FileSystemLinkResolver<SnapshotDiffReport>() {\n      @Override\n      public SnapshotDiffReport doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.getSnapshotDiffReport(getPathName(p), fromSnapshot,\n            toSnapshot);\n      }\n\n      @Override\n      public SnapshotDiffReport next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.getSnapshotDiffReport(p, fromSnapshot, toSnapshot);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + snapshotDir + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum": "  public FileChecksum getFileChecksum(Path f) throws IOException {\n    statistics.incrementReadOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FileChecksum>() {\n      @Override\n      public FileChecksum doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.getFileChecksum(getPathName(p));\n      }\n\n      @Override\n      public FileChecksum next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.getFileChecksum(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.recoverLease": "  public boolean recoverLease(final Path f) throws IOException {\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.recoverLease(getPathName(p));\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.recoverLease(p);\n        }\n        throw new UnsupportedOperationException(\"Cannot recoverLease through\" +\n            \" a symlink to a non-DistributedFileSystem: \" + f + \" -> \" + p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setPermission": "  public void setPermission(Path p, final FsPermission permission\n      ) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(p);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.setPermission(getPathName(p), permission);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        fs.setPermission(p, permission);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.rename": "  public void rename(Path src, Path dst, final Options.Rename... options)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    final Path absSrc = fixRelativePart(src);\n    final Path absDst = fixRelativePart(dst);\n    // Try the rename without resolving first\n    try {\n      dfs.rename(getPathName(absSrc), getPathName(absDst), options);\n    } catch (UnresolvedLinkException e) {\n      // Fully resolve the source\n      final Path source = getFileLinkStatus(absSrc).getPath();\n      // Keep trying to resolve the destination\n      new FileSystemLinkResolver<Void>() {\n        @Override\n        public Void doCall(final Path p)\n            throws IOException, UnresolvedLinkException {\n          dfs.rename(getPathName(source), getPathName(p), options);\n          return null;\n        }\n        @Override\n        public Void next(final FileSystem fs, final Path p)\n            throws IOException {\n          // Should just throw an error in FileSystem#checkPath\n          return doCall(p);\n        }\n      }.resolve(this, absDst);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.createSymlink": "  public void createSymlink(final Path target, final Path link,\n      final boolean createParent) throws AccessControlException,\n      FileAlreadyExistsException, FileNotFoundException,\n      ParentNotDirectoryException, UnsupportedFileSystemException, \n      IOException {\n    statistics.incrementWriteOps(1);\n    final Path absF = fixRelativePart(link);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException,\n          UnresolvedLinkException {\n        dfs.createSymlink(target.toString(), getPathName(p), createParent);\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException, UnresolvedLinkException {\n        fs.createSymlink(target, p, createParent);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.disallowSnapshot": "  public void disallowSnapshot(final Path path) throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.disallowSnapshot(getPathName(p));\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.disallowSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.delete": "  public boolean delete(Path f, final boolean recursive) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.delete(getPathName(p), recursive);\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.delete(p, recursive);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setReplication": "  public boolean setReplication(Path src, \n                                final short replication\n                               ) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(src);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.setReplication(getPathName(p), replication);\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.setReplication(p, replication);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.open": "  public FSDataInputStream open(Path f, final int bufferSize)\n      throws IOException {\n    statistics.incrementReadOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FSDataInputStream>() {\n      @Override\n      public FSDataInputStream doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return new HdfsDataInputStream(\n            dfs.open(getPathName(p), bufferSize, verifyChecksum));\n      }\n      @Override\n      public FSDataInputStream next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.open(p, bufferSize);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.append": "  public FSDataOutputStream append(Path f, final int bufferSize,\n      final Progressable progress) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FSDataOutputStream>() {\n      @Override\n      public FSDataOutputStream doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.append(getPathName(p), bufferSize, progress, statistics);\n      }\n      @Override\n      public FSDataOutputStream next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.append(p, bufferSize);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setTimes": "  public void setTimes(Path p, final long mtime, final long atime\n      ) throws IOException {\n    statistics.incrementWriteOps(1);\n    Path absF = fixRelativePart(p);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.setTimes(getPathName(p), mtime, atime);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        fs.setTimes(p, mtime, atime);\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.deleteSnapshot": "  public void deleteSnapshot(final Path snapshotDir, final String snapshotName)\n      throws IOException {\n    Path absF = fixRelativePart(snapshotDir);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.deleteSnapshot(getPathName(p), snapshotName);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.deleteSnapshot(p, snapshotName);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + snapshotDir + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.isFileClosed": "  public boolean isFileClosed(final Path src) throws IOException {\n    Path absF = fixRelativePart(src);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.isFileClosed(getPathName(p));\n      }\n\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.isFileClosed(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot call isFileClosed\"\n              + \" on a symlink to a non-DistributedFileSystem: \"\n              + src + \" -> \" + p);\n        }\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getContentSummary": "  public ContentSummary getContentSummary(Path f) throws IOException {\n    statistics.incrementReadOps(1);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<ContentSummary>() {\n      @Override\n      public ContentSummary doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return dfs.getContentSummary(getPathName(p));\n      }\n      @Override\n      public ContentSummary next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.getContentSummary(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.toString": "  public String toString() {\n    return \"DFS[\" + dfs + \"]\";\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.setQuota": "  public void setQuota(Path src, final long namespaceQuota,\n      final long diskspaceQuota) throws IOException {\n    Path absF = fixRelativePart(src);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.setQuota(getPathName(p), namespaceQuota, diskspaceQuota);\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        // setQuota is not defined in FileSystem, so we only can resolve\n        // within this DFS\n        return doCall(p);\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.renameSnapshot": "  public void renameSnapshot(final Path path, final String snapshotOldName,\n      final String snapshotNewName) throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.renameSnapshot(getPathName(p), snapshotOldName, snapshotNewName);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.renameSnapshot(p, snapshotOldName, snapshotNewName);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.allowSnapshot": "  public void allowSnapshot(final Path path) throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        dfs.allowSnapshot(getPathName(p));\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.allowSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.createSnapshot": "  public Path createSnapshot(final Path path, final String snapshotName) \n      throws IOException {\n    Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<Path>() {\n      @Override\n      public Path doCall(final Path p)\n          throws IOException, UnresolvedLinkException {\n        return new Path(dfs.createSnapshot(getPathName(p), snapshotName));\n      }\n\n      @Override\n      public Path next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.createSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n      }\n    }.resolve(this, absF);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getPathName": "  private String getPathName(Path file) {\n    checkPath(file);\n    String result = file.toUri().getPath();\n    if (!DFSUtil.isValidName(result)) {\n      throw new IllegalArgumentException(\"Pathname \" + result + \" from \" +\n                                         file+\" is not a valid DFS filename.\");\n    }\n    return result;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystemLinkResolver.resolve": "  public T resolve(final FileSystem filesys, final Path path)\n      throws IOException {\n    int count = 0;\n    T in = null;\n    Path p = path;\n    // Assumes path belongs to this FileSystem.\n    // Callers validate this by passing paths through FileSystem#checkPath\n    FileSystem fs = filesys;\n    for (boolean isLink = true; isLink;) {\n      try {\n        in = doCall(p);\n        isLink = false;\n      } catch (UnresolvedLinkException e) {\n        if (!filesys.resolveSymlinks) {\n          throw new IOException(\"Path \" + path + \" contains a symlink\"\n              + \" and symlink resolution is disabled (\"\n              + CommonConfigurationKeys.FS_CLIENT_RESOLVE_REMOTE_SYMLINKS_KEY\n              + \").\", e);\n        }\n        if (count++ > FsConstants.MAX_PATH_LINKS) {\n          throw new IOException(\"Possible cyclic loop while \" +\n                                \"following symbolic link \" + path);\n        }\n        // Resolve the first unresolved path component\n        p = FSLinkResolver.qualifySymlinkTarget(fs.getUri(), p,\n            filesys.resolveLink(p));\n        fs = FileSystem.getFSofPath(p, filesys.getConf());\n        // Have to call next if it's a new FS\n        if (!fs.equals(filesys)) {\n          return next(fs, p);\n        }\n        // Else, we keep resolving with this filesystem\n      }\n    }\n    // Successful call, path was fully resolved\n    return in;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystemLinkResolver.doCall": "  abstract public T doCall(final Path p) throws IOException,\n      UnresolvedLinkException;\n\n  /**\n   * Calls the abstract FileSystem call equivalent to the specialized subclass\n   * implementation in {@link #doCall(Path)}. This is used when retrying the",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystemLinkResolver.next": "  abstract public T next(final FileSystem fs, final Path p) throws IOException;\n\n  /**\n   * Attempt calling overridden {@link #doCall(Path)} method with",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockChecksum": "  public void blockChecksum(final ExtendedBlock block,\n      final Token<BlockTokenIdentifier> blockToken) throws IOException {\n    final DataOutputStream out = new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final LengthInputStream metadataIn = \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn = new DataInputStream(new BufferedInputStream(\n        metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header = BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum = header.getChecksum(); \n      final int bytesPerCRC = checksum.getBytesPerChecksum();\n      final long crcPerBlock = (metadataIn.getLength()\n          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n      \n      //compute block checksum\n      final MD5Hash md5 = MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block=\" + block + \", bytesPerCRC=\" + bytesPerCRC\n            + \", crcPerBlock=\" + crcPerBlock + \", md5=\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          .setCrcType(PBHelper.convert(checksum.getChecksumType()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.elapsed": "  private long elapsed() {\n    return now() - opStartTime;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.getOutputStream": "  private OutputStream getOutputStream() {\n    return socketOut;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess": "  private void checkAccess(OutputStream out, final boolean reply, \n      final ExtendedBlock blk,\n      final Token<BlockTokenIdentifier> t,\n      final Op op,\n      final BlockTokenSecretManager.AccessMode mode) throws IOException {\n    if (datanode.isBlockTokenEnabled) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Checking block access token for block '\" + blk.getBlockId()\n            + \"' with mode '\" + mode + \"'\");\n      }\n      try {\n        datanode.blockPoolTokenSecretManager.checkAccess(t, null, blk, mode);\n      } catch(InvalidToken e) {\n        try {\n          if (reply) {\n            BlockOpResponseProto.Builder resp = BlockOpResponseProto.newBuilder()\n              .setStatus(ERROR_ACCESS_TOKEN);\n            if (mode == BlockTokenSecretManager.AccessMode.WRITE) {\n              DatanodeRegistration dnR = \n                datanode.getDNRegistrationForBP(blk.getBlockPoolId());\n              // NB: Unconditionally using the xfer addr w/o hostname\n              resp.setFirstBadLink(dnR.getXferAddr());\n            }\n            resp.build().writeDelimitedTo(out);\n            out.flush();\n          }\n          LOG.warn(\"Block token verification failed: op=\" + op\n              + \", remoteAddress=\" + remoteAddress\n              + \", message=\" + e.getLocalizedMessage());\n          throw e;\n        } finally {\n          IOUtils.closeStream(out);\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.updateCurrentThreadName": "  private void updateCurrentThreadName(String status) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"DataXceiver for client \");\n    if (previousOpClientName != null) {\n      sb.append(previousOpClientName).append(\" at \");\n    }\n    sb.append(remoteAddress);\n    if (status != null) {\n      sb.append(\" [\").append(status).append(\"]\");\n    }\n    Thread.currentThread().setName(sb.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opBlockChecksum": "  private void opBlockChecksum(DataInputStream in) throws IOException {\n    OpBlockChecksumProto proto = OpBlockChecksumProto.parseFrom(vintPrefixed(in));\n    \n    blockChecksum(PBHelper.convert(proto.getHeader().getBlock()),\n        PBHelper.convert(proto.getHeader().getToken()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp": "  protected final void processOp(Op op) throws IOException {\n    switch(op) {\n    case READ_BLOCK:\n      opReadBlock();\n      break;\n    case WRITE_BLOCK:\n      opWriteBlock(in);\n      break;\n    case REPLACE_BLOCK:\n      opReplaceBlock(in);\n      break;\n    case COPY_BLOCK:\n      opCopyBlock(in);\n      break;\n    case BLOCK_CHECKSUM:\n      opBlockChecksum(in);\n      break;\n    case TRANSFER_BLOCK:\n      opTransferBlock(in);\n      break;\n    case REQUEST_SHORT_CIRCUIT_FDS:\n      opRequestShortCircuitFds(in);\n      break;\n    default:\n      throw new IOException(\"Unknown op \" + op + \" in data stream\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReplaceBlock": "  private void opReplaceBlock(DataInputStream in) throws IOException {\n    OpReplaceBlockProto proto = OpReplaceBlockProto.parseFrom(vintPrefixed(in));\n    replaceBlock(PBHelper.convert(proto.getHeader().getBlock()),\n        PBHelper.convert(proto.getHeader().getToken()),\n        proto.getDelHint(),\n        PBHelper.convert(proto.getSource()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitFds": "  private void opRequestShortCircuitFds(DataInputStream in) throws IOException {\n    final OpRequestShortCircuitAccessProto proto =\n      OpRequestShortCircuitAccessProto.parseFrom(vintPrefixed(in));\n    requestShortCircuitFds(PBHelper.convert(proto.getHeader().getBlock()),\n        PBHelper.convert(proto.getHeader().getToken()),\n        proto.getMaxVersion());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opTransferBlock": "  private void opTransferBlock(DataInputStream in) throws IOException {\n    final OpTransferBlockProto proto =\n      OpTransferBlockProto.parseFrom(vintPrefixed(in));\n    transferBlock(PBHelper.convert(proto.getHeader().getBaseHeader().getBlock()),\n        PBHelper.convert(proto.getHeader().getBaseHeader().getToken()),\n        proto.getHeader().getClientName(),\n        PBHelper.convert(proto.getTargetsList()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opCopyBlock": "  private void opCopyBlock(DataInputStream in) throws IOException {\n    OpCopyBlockProto proto = OpCopyBlockProto.parseFrom(vintPrefixed(in));\n    copyBlock(PBHelper.convert(proto.getHeader().getBlock()),\n        PBHelper.convert(proto.getHeader().getToken()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock": "  private void opReadBlock() throws IOException {\n    OpReadBlockProto proto = OpReadBlockProto.parseFrom(vintPrefixed(in));\n    readBlock(PBHelper.convert(proto.getHeader().getBaseHeader().getBlock()),\n        PBHelper.convert(proto.getHeader().getBaseHeader().getToken()),\n        proto.getHeader().getClientName(),\n        proto.getOffset(),\n        proto.getLen(),\n        proto.getSendChecksums(),\n        (proto.hasCachingStrategy() ?\n            getCachingStrategy(proto.getCachingStrategy()) :\n          CachingStrategy.newDefaultStrategy()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock": "  private void opWriteBlock(DataInputStream in) throws IOException {\n    final OpWriteBlockProto proto = OpWriteBlockProto.parseFrom(vintPrefixed(in));\n    writeBlock(PBHelper.convert(proto.getHeader().getBaseHeader().getBlock()),\n        PBHelper.convert(proto.getHeader().getBaseHeader().getToken()),\n        proto.getHeader().getClientName(),\n        PBHelper.convert(proto.getTargetsList()),\n        PBHelper.convert(proto.getSource()),\n        fromProto(proto.getStage()),\n        proto.getPipelineSize(),\n        proto.getMinBytesRcvd(), proto.getMaxBytesRcvd(),\n        proto.getLatestGenerationStamp(),\n        fromProto(proto.getRequestedChecksum()),\n        (proto.hasCachingStrategy() ?\n            getCachingStrategy(proto.getCachingStrategy()) :\n          CachingStrategy.newDefaultStrategy()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.run": "  public void run() {\n    int opsProcessed = 0;\n    Op op = null;\n\n    dataXceiverServer.addPeer(peer);\n    try {\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input = socketIn;\n      if ((!peer.hasSecureChannel()) && dnConf.encryptDataTransfer) {\n        IOStreamPair encryptedStreams = null;\n        try {\n          encryptedStreams = DataTransferEncryptor.getEncryptedStreams(socketOut,\n              socketIn, datanode.blockPoolTokenSecretManager,\n              dnConf.encryptionAlgorithm);\n        } catch (InvalidMagicNumberException imne) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n          return;\n        }\n        input = encryptedStreams.in;\n        socketOut = encryptedStreams.out;\n      }\n      input = new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed != 0) {\n            assert dnConf.socketKeepaliveTimeout > 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op = readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it's quite normal to get EOF here.\n          if (opsProcessed > 0 &&\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed != 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime = now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!peer.isClosed() && dnConf.socketKeepaliveTimeout > 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                ((op == null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      dataXceiverServer.closePeer(peer);\n      IOUtils.closeStream(in);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FSLinkResolver.qualifySymlinkTarget": "  public static Path qualifySymlinkTarget(final URI pathURI,\n      Path pathWithLink, Path target) {\n    // NB: makeQualified uses the target's scheme and authority, if\n    // specified, and the scheme and authority of pathURI, if not.\n    final URI targetUri = target.toUri();\n    final String scheme = targetUri.getScheme();\n    final String auth = targetUri.getAuthority();\n    return (scheme == null && auth == null) ? target.makeQualified(pathURI,\n        pathWithLink.getParent()) : target;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getUri": "  public abstract URI getUri();\n  \n  /**\n   * Return a canonicalized form of this FileSystem's URI.\n   * \n   * The default implementation simply calls {@link #canonicalizeUri(URI)}",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getFSofPath": "  protected static FileSystem getFSofPath(final Path absOrFqPath,\n      final Configuration conf)\n      throws UnsupportedFileSystemException, IOException {\n    absOrFqPath.checkNotSchemeWithRelative();\n    absOrFqPath.checkNotRelative();\n\n    // Uses the default file system if not fully qualified\n    return get(absOrFqPath.toUri(), conf);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.get": "    FileSystem get(URI uri, Configuration conf) throws IOException{\n      Key key = new Key(uri, conf);\n      return getInternal(uri, conf, key);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.equals": "      public boolean equals(Object obj) {\n        if (obj == this) {\n          return true;\n        }\n        if (obj != null && obj instanceof Key) {\n          Key that = (Key)obj;\n          return isEqual(this.scheme, that.scheme)\n                 && isEqual(this.authority, that.authority)\n                 && isEqual(this.ugi, that.ugi)\n                 && (this.unique == that.unique);\n        }\n        return false;        \n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.isEqual": "      static boolean isEqual(Object a, Object b) {\n        return a == b || (a != null && a.equals(b));        \n      }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader.getHeaderSize": "  public static int getHeaderSize() {\n    return Short.SIZE/Byte.SIZE + DataChecksum.getChecksumHeaderSize();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader.readHeader": "  private static BlockMetadataHeader readHeader(short version, DataInputStream in) \n                                   throws IOException {\n    DataChecksum checksum = DataChecksum.newDataChecksum(in);\n    return new BlockMetadataHeader(version, checksum);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader.getChecksum": "  public DataChecksum getChecksum() {\n    return checksum;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXceiverCount": "  public int getXceiverCount() {\n    return threadGroup == null ? 0 : threadGroup.activeCount();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDisplayName": "  public String getDisplayName() {\n    // NB: our DatanodeID may not be set yet\n    return hostName + \":\" + getXferPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXferPort": "  public int getXferPort() {\n    return streamingAddr.getPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dataXceiverServer.closePeer": "  synchronized void closePeer(Peer peer) {\n    peers.remove(peer);\n    IOUtils.cleanup(null, peer);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dataXceiverServer.addPeer": "  synchronized void addPeer(Peer peer) {\n    peers.add(peer);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDnConf": "  public DNConf getDnConf() {\n    return dnConf;\n  }"
        },
        "bug_report": {
            "Title": "DFSClient.getFileChecksum() throws IOException if checksum is disabled",
            "Description": "If a file is created with checksum disabled (using {{ChecksumOpt.disabled()}} for example), calling {{FileSystem.getFileChecksum()}} throws the following IOException:\n\n{noformat}\njava.io.IOException: Fail to get block MD5 for BP-341493254-192.168.1.10-1390888724459:blk_1073741825_1001\n\tat org.apache.hadoop.hdfs.DFSClient.getFileChecksum(DFSClient.java:1965)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileChecksum(DFSClient.java:1771)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1186)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1194)\n[...]\n{noformat}\n\nFrom the logs, the datanode is doing some wrong arithmetics because of the crcPerBlock:\n{noformat}\n2014-01-27 21:58:46,329 ERROR datanode.DataNode (DataXceiver.java:run(225)) - 127.0.0.1:52398:DataXceiver error processing BLOCK_CHECKSUM operation  src: /127.0.0.1:52407 dest: /127.0.0.1:52398\njava.lang.ArithmeticException: / by zero\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockChecksum(DataXceiver.java:658)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opBlockChecksum(Receiver.java:169)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:77)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)\n\tat java.lang.Thread.run(Thread.java:695)\n{noformat}"
        }
    },
    {
        "filename": "HDFS-8070.json",
        "creation_time": "2015-04-06T19:57:58.000+0000",
        "stack_trace": "```\njava.io.IOException: ERROR_INVALID: there is no shared memory segment registered with shmId a86ee34576d93c4964005d90b0d97c38\n\tat org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser.run(ShortCircuitCache.java:208)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\njava.nio.channels.ClosedChannelException\n\tat org.apache.hadoop.util.CloseableReferenceCount.reference(CloseableReferenceCount.java:57)\n\tat org.apache.hadoop.net.unix.DomainSocket.shutdown(DomainSocket.java:387)\n\tat org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager.shutdown(DfsClientShmManager.java:378)\n\tat org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser.run(ShortCircuitCache.java:223)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\njava.io.IOException: ERROR_INVALID: there is no shared memory segment registered with shmId a86ee34576d93c4964005d90b0d97c38\n\tat org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser.run(ShortCircuitCache.java:208)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\njava.io.EOFException\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.requestShortCircuitFds(DataXceiver.java:352)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitFds(Receiver.java:187)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:89)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)\n        at java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache.run": "    public void run() {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(ShortCircuitCache.this + \": about to release \" + slot);\n      }\n      final DfsClientShm shm = (DfsClientShm)slot.getShm();\n      final DomainSocket shmSock = shm.getPeer().getDomainSocket();\n      DomainSocket sock = null;\n      DataOutputStream out = null;\n      final String path = shmSock.getPath();\n      boolean success = false;\n      try {\n        sock = DomainSocket.connect(path);\n        out = new DataOutputStream(\n            new BufferedOutputStream(sock.getOutputStream()));\n        new Sender(out).releaseShortCircuitFds(slot.getSlotId());\n        DataInputStream in = new DataInputStream(sock.getInputStream());\n        ReleaseShortCircuitAccessResponseProto resp =\n            ReleaseShortCircuitAccessResponseProto.parseFrom(\n                PBHelper.vintPrefixed(in));\n        if (resp.getStatus() != Status.SUCCESS) {\n          String error = resp.hasError() ? resp.getError() : \"(unknown)\";\n          throw new IOException(resp.getStatus().toString() + \": \" + error);\n        }\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(ShortCircuitCache.this + \": released \" + slot);\n        }\n        success = true;\n      } catch (IOException e) {\n        LOG.error(ShortCircuitCache.this + \": failed to release \" +\n            \"short-circuit shared memory slot \" + slot + \" by sending \" +\n            \"ReleaseShortCircuitAccessRequestProto to \" + path +\n            \".  Closing shared memory segment.\", e);\n      } finally {\n        if (success) {\n          shmManager.freeSlot(slot);\n        } else {\n          shm.getEndpointShmManager().shutdown(shm);\n        }\n        IOUtils.cleanup(LOG, sock, out);\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache.toString": "  public String toString() {\n    return \"ShortCircuitCache(0x\" +\n        Integer.toHexString(System.identityHashCode(this)) + \")\";\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache.demoteOldEvictableMmaped": "  private int demoteOldEvictableMmaped(long now) {\n    int numDemoted = 0;\n    boolean needMoreSpace = false;\n    Long evictionTimeNs = Long.valueOf(0);\n\n    while (true) {\n      Entry<Long, ShortCircuitReplica> entry = \n          evictableMmapped.ceilingEntry(evictionTimeNs);\n      if (entry == null) break;\n      evictionTimeNs = entry.getKey();\n      long evictionTimeMs = \n          TimeUnit.MILLISECONDS.convert(evictionTimeNs, TimeUnit.NANOSECONDS);\n      if (evictionTimeMs + maxEvictableMmapedLifespanMs >= now) {\n        if (evictableMmapped.size() < maxEvictableMmapedSize) {\n          break;\n        }\n        needMoreSpace = true;\n      }\n      ShortCircuitReplica replica = entry.getValue();\n      if (LOG.isTraceEnabled()) {\n        String rationale = needMoreSpace ? \"because we need more space\" : \n            \"because it's too old\";\n        LOG.trace(\"demoteOldEvictable: demoting \" + replica + \": \" +\n            rationale + \": \" +\n            StringUtils.getStackTrace(Thread.currentThread()));\n      }\n      removeEvictable(replica, evictableMmapped);\n      munmap(replica);\n      insertEvictable(evictionTimeNs, replica, evictable);\n      numDemoted++;\n    }\n    return numDemoted;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache.freeSlot": "  public void freeSlot(Slot slot) {\n    Preconditions.checkState(shmManager != null);\n    slot.makeInvalid();\n    shmManager.freeSlot(slot);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache.purge": "  private void purge(ShortCircuitReplica replica) {\n    boolean removedFromInfoMap = false;\n    String evictionMapName = null;\n    Preconditions.checkArgument(!replica.purged);\n    replica.purged = true;\n    Waitable<ShortCircuitReplicaInfo> val = replicaInfoMap.get(replica.key);\n    if (val != null) {\n      ShortCircuitReplicaInfo info = val.getVal();\n      if ((info != null) && (info.getReplica() == replica)) {\n        replicaInfoMap.remove(replica.key);\n        removedFromInfoMap = true;\n      }\n    }\n    Long evictableTimeNs = replica.getEvictableTimeNs();\n    if (evictableTimeNs != null) {\n      evictionMapName = removeEvictable(replica);\n    }\n    if (LOG.isTraceEnabled()) {\n      StringBuilder builder = new StringBuilder();\n      builder.append(this).append(\": \").append(\": purged \").\n          append(replica).append(\" from the cache.\");\n      if (removedFromInfoMap) {\n        builder.append(\"  Removed from the replicaInfoMap.\");\n      }\n      if (evictionMapName != null) {\n        builder.append(\"  Removed from \").append(evictionMapName);\n      }\n      LOG.trace(builder.toString());\n    }\n    unref(replica);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.CloseableReferenceCount.reference": "  public void reference() throws ClosedChannelException {\n    int curBits = status.incrementAndGet();\n    if ((curBits & STATUS_CLOSED_MASK) != 0) {\n      status.decrementAndGet();\n      throw new ClosedChannelException();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.unix.DomainSocket.shutdown": "  public void shutdown() throws IOException {\n    refCount.reference();\n    boolean exc = true;\n    try {\n      shutdown0(fd);\n      exc = false;\n    } finally {\n      unreference(exc);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.unix.DomainSocket.unreference": "  private void unreference(boolean checkClosed) throws ClosedChannelException {\n    if (checkClosed) {\n      refCount.unreferenceCheckClosed();\n    } else {\n      refCount.unreference();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.unix.DomainSocket.shutdown0": "  private static native void shutdown0(int fd) throws IOException;\n\n  /**\n   * Close the Socket.\n   */\n  @Override\n  public void close() throws IOException {\n    // Set the closed bit on this DomainSocket\n    int count;\n    try {\n      count = refCount.setClosed();\n    } catch (ClosedChannelException e) {\n      // Someone else already closed the DomainSocket.\n      return;\n    }\n    // Wait for all references to go away\n    boolean didShutdown = false;\n    boolean interrupted = false;\n    while (count > 0) {\n      if (!didShutdown) {\n        try {\n          // Calling shutdown on the socket will interrupt blocking system\n          // calls like accept, write, and read that are going on in a\n          // different thread.\n          shutdown0(fd);\n        } catch (IOException e) {\n          LOG.error(\"shutdown error: \", e);\n        }\n        didShutdown = true;\n      }\n      try {\n        Thread.sleep(10);\n      } catch (InterruptedException e) {\n        interrupted = true;\n      }\n      count = refCount.getReferenceCount();\n    }\n\n    // At this point, nobody has a reference to the file descriptor, \n    // and nobody will be able to get one in the future either.\n    // We now call close(2) on the file descriptor.\n    // After this point, the file descriptor number will be reused by \n    // something else.  Although this DomainSocket object continues to hold \n    // the old file descriptor number (it's a final field), we never use it \n    // again because this DomainSocket is closed.\n    close0(fd);\n    if (interrupted) {\n      Thread.currentThread().interrupt();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager.shutdown": "    final void shutdown(DfsClientShm shm) {\n      try {\n        shm.getPeer().getDomainSocket().shutdown();\n      } catch (IOException e) {\n        LOG.warn(this + \": error shutting down shm: got IOException calling \" +\n            \"shutdown(SHUT_RDWR)\", e);\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.requestShortCircuitFds": "  public void requestShortCircuitFds(final ExtendedBlock blk,\n      final Token<BlockTokenIdentifier> token,\n      SlotId slotId, int maxVersion, boolean supportsReceiptVerification)\n        throws IOException {\n    updateCurrentThreadName(\"Passing file descriptors for block \" + blk);\n    BlockOpResponseProto.Builder bld = BlockOpResponseProto.newBuilder();\n    FileInputStream fis[] = null;\n    SlotId registeredSlotId = null;\n    boolean success = false;\n    try {\n      try {\n        if (peer.getDomainSocket() == null) {\n          throw new IOException(\"You cannot pass file descriptors over \" +\n              \"anything but a UNIX domain socket.\");\n        }\n        if (slotId != null) {\n          boolean isCached = datanode.data.\n              isCached(blk.getBlockPoolId(), blk.getBlockId());\n          datanode.shortCircuitRegistry.registerSlot(\n              ExtendedBlockId.fromExtendedBlock(blk), slotId, isCached);\n          registeredSlotId = slotId;\n        }\n        fis = datanode.requestShortCircuitFdsForRead(blk, token, maxVersion);\n        Preconditions.checkState(fis != null);\n        bld.setStatus(SUCCESS);\n        bld.setShortCircuitAccessVersion(DataNode.CURRENT_BLOCK_FORMAT_VERSION);\n      } catch (ShortCircuitFdsVersionException e) {\n        bld.setStatus(ERROR_UNSUPPORTED);\n        bld.setShortCircuitAccessVersion(DataNode.CURRENT_BLOCK_FORMAT_VERSION);\n        bld.setMessage(e.getMessage());\n      } catch (ShortCircuitFdsUnsupportedException e) {\n        bld.setStatus(ERROR_UNSUPPORTED);\n        bld.setMessage(e.getMessage());\n      } catch (InvalidToken e) {\n        bld.setStatus(ERROR_ACCESS_TOKEN);\n        bld.setMessage(e.getMessage());\n      } catch (IOException e) {\n        bld.setStatus(ERROR);\n        bld.setMessage(e.getMessage());\n      }\n      bld.build().writeDelimitedTo(socketOut);\n      if (fis != null) {\n        FileDescriptor fds[] = new FileDescriptor[fis.length];\n        for (int i = 0; i < fds.length; i++) {\n          fds[i] = fis[i].getFD();\n        }\n        byte buf[] = new byte[1];\n        if (supportsReceiptVerification) {\n          buf[0] = (byte)USE_RECEIPT_VERIFICATION.getNumber();\n        } else {\n          buf[0] = (byte)DO_NOT_USE_RECEIPT_VERIFICATION.getNumber();\n        }\n        DomainSocket sock = peer.getDomainSocket();\n        sock.sendFileDescriptors(fds, buf, 0, buf.length);\n        if (supportsReceiptVerification) {\n          LOG.trace(\"Reading receipt verification byte for \" + slotId);\n          int val = sock.getInputStream().read();\n          if (val < 0) {\n            throw new EOFException();\n          }\n        } else {\n          LOG.trace(\"Receipt verification is not enabled on the DataNode.  \" +\n                    \"Not verifying \" + slotId);\n        }\n        success = true;\n      }\n    } finally {\n      if ((!success) && (registeredSlotId != null)) {\n        LOG.info(\"Unregistering \" + registeredSlotId + \" because the \" +\n            \"requestShortCircuitFdsForRead operation failed.\");\n        datanode.shortCircuitRegistry.unregisterSlot(registeredSlotId);\n      }\n      if (ClientTraceLog.isInfoEnabled()) {\n        DatanodeRegistration dnR = datanode.getDNRegistrationForBP(blk\n            .getBlockPoolId());\n        BlockSender.ClientTraceLog.info(String.format(\n            \"src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS,\" +\n            \" blockid: %s, srvID: %s, success: %b\",\n            blk.getBlockId(), dnR.getDatanodeUuid(), success));\n      }\n      if (fis != null) {\n        IOUtils.cleanup(LOG, fis);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.updateCurrentThreadName": "  private void updateCurrentThreadName(String status) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"DataXceiver for client \");\n    if (previousOpClientName != null) {\n      sb.append(previousOpClientName).append(\" at \");\n    }\n    sb.append(remoteAddress);\n    if (status != null) {\n      sb.append(\" [\").append(status).append(\"]\");\n    }\n    Thread.currentThread().setName(sb.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitFds": "  private void opRequestShortCircuitFds(DataInputStream in) throws IOException {\n    final OpRequestShortCircuitAccessProto proto =\n      OpRequestShortCircuitAccessProto.parseFrom(vintPrefixed(in));\n    SlotId slotId = (proto.hasSlotId()) ? \n        PBHelper.convert(proto.getSlotId()) : null;\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      requestShortCircuitFds(PBHelper.convert(proto.getHeader().getBlock()),\n          PBHelper.convert(proto.getHeader().getToken()),\n          slotId, proto.getMaxVersion(), true);\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp": "  protected final void processOp(Op op) throws IOException {\n    switch(op) {\n    case READ_BLOCK:\n      opReadBlock();\n      break;\n    case WRITE_BLOCK:\n      opWriteBlock(in);\n      break;\n    case REPLACE_BLOCK:\n      opReplaceBlock(in);\n      break;\n    case COPY_BLOCK:\n      opCopyBlock(in);\n      break;\n    case BLOCK_CHECKSUM:\n      opBlockChecksum(in);\n      break;\n    case TRANSFER_BLOCK:\n      opTransferBlock(in);\n      break;\n    case REQUEST_SHORT_CIRCUIT_FDS:\n      opRequestShortCircuitFds(in);\n      break;\n    case RELEASE_SHORT_CIRCUIT_FDS:\n      opReleaseShortCircuitFds(in);\n      break;\n    case REQUEST_SHORT_CIRCUIT_SHM:\n      opRequestShortCircuitShm(in);\n      break;\n    default:\n      throw new IOException(\"Unknown op \" + op + \" in data stream\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReplaceBlock": "  private void opReplaceBlock(DataInputStream in) throws IOException {\n    OpReplaceBlockProto proto = OpReplaceBlockProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      replaceBlock(PBHelper.convert(proto.getHeader().getBlock()),\n          PBHelper.convertStorageType(proto.getStorageType()),\n          PBHelper.convert(proto.getHeader().getToken()),\n          proto.getDelHint(),\n          PBHelper.convert(proto.getSource()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitShm": "  private void opRequestShortCircuitShm(DataInputStream in) throws IOException {\n    final ShortCircuitShmRequestProto proto =\n        ShortCircuitShmRequestProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getTraceInfo(),\n        proto.getClass().getSimpleName());\n    try {\n      requestShortCircuitShm(proto.getClientName());\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opBlockChecksum": "  private void opBlockChecksum(DataInputStream in) throws IOException {\n    OpBlockChecksumProto proto = OpBlockChecksumProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n    blockChecksum(PBHelper.convert(proto.getHeader().getBlock()),\n        PBHelper.convert(proto.getHeader().getToken()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReleaseShortCircuitFds": "  private void opReleaseShortCircuitFds(DataInputStream in)\n      throws IOException {\n    final ReleaseShortCircuitAccessRequestProto proto =\n      ReleaseShortCircuitAccessRequestProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getTraceInfo(),\n        proto.getClass().getSimpleName());\n    try {\n      releaseShortCircuitFds(PBHelper.convert(proto.getSlotId()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opTransferBlock": "  private void opTransferBlock(DataInputStream in) throws IOException {\n    final OpTransferBlockProto proto =\n      OpTransferBlockProto.parseFrom(vintPrefixed(in));\n    final DatanodeInfo[] targets = PBHelper.convert(proto.getTargetsList());\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      transferBlock(PBHelper.convert(proto.getHeader().getBaseHeader().getBlock()),\n          PBHelper.convert(proto.getHeader().getBaseHeader().getToken()),\n          proto.getHeader().getClientName(),\n          targets,\n          PBHelper.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opCopyBlock": "  private void opCopyBlock(DataInputStream in) throws IOException {\n    OpCopyBlockProto proto = OpCopyBlockProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      copyBlock(PBHelper.convert(proto.getHeader().getBlock()),\n          PBHelper.convert(proto.getHeader().getToken()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock": "  private void opReadBlock() throws IOException {\n    OpReadBlockProto proto = OpReadBlockProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      readBlock(PBHelper.convert(proto.getHeader().getBaseHeader().getBlock()),\n        PBHelper.convert(proto.getHeader().getBaseHeader().getToken()),\n        proto.getHeader().getClientName(),\n        proto.getOffset(),\n        proto.getLen(),\n        proto.getSendChecksums(),\n        (proto.hasCachingStrategy() ?\n            getCachingStrategy(proto.getCachingStrategy()) :\n          CachingStrategy.newDefaultStrategy()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock": "  private void opWriteBlock(DataInputStream in) throws IOException {\n    final OpWriteBlockProto proto = OpWriteBlockProto.parseFrom(vintPrefixed(in));\n    final DatanodeInfo[] targets = PBHelper.convert(proto.getTargetsList());\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      writeBlock(PBHelper.convert(proto.getHeader().getBaseHeader().getBlock()),\n          PBHelper.convertStorageType(proto.getStorageType()),\n          PBHelper.convert(proto.getHeader().getBaseHeader().getToken()),\n          proto.getHeader().getClientName(),\n          targets,\n          PBHelper.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length),\n          PBHelper.convert(proto.getSource()),\n          fromProto(proto.getStage()),\n          proto.getPipelineSize(),\n          proto.getMinBytesRcvd(), proto.getMaxBytesRcvd(),\n          proto.getLatestGenerationStamp(),\n          fromProto(proto.getRequestedChecksum()),\n          (proto.hasCachingStrategy() ?\n              getCachingStrategy(proto.getCachingStrategy()) :\n            CachingStrategy.newDefaultStrategy()),\n          (proto.hasAllowLazyPersist() ? proto.getAllowLazyPersist() : false),\n          (proto.hasPinning() ? proto.getPinning(): false),\n          (PBHelper.convertBooleanList(proto.getTargetPinningsList())));\n    } finally {\n     if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.run": "  public void run() {\n    int opsProcessed = 0;\n    Op op = null;\n\n    try {\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input = socketIn;\n      try {\n        IOStreamPair saslStreams = datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input = new BufferedInputStream(saslStreams.in,\n          HdfsConstants.SMALL_BUFFER_SIZE);\n        socketOut = saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at \" + peer.getRemoteAddressString() + \n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\");\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed != 0) {\n            assert dnConf.socketKeepaliveTimeout > 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op = readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it's quite normal to get EOF here.\n          if (opsProcessed > 0 &&\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            incrDatanodeNetworkErrors();\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed != 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime = monotonicNow();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer != null) &&\n          (!peer.isClosed() && dnConf.socketKeepaliveTimeout > 0));\n    } catch (Throwable t) {\n      String s = datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op == null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op == Op.WRITE_BLOCK && t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op == Op.READ_BLOCK && t instanceof SocketTimeoutException) {\n        String s1 =\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 += \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer != null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.incrDatanodeNetworkErrors": "  private void incrDatanodeNetworkErrors() {\n    datanode.incrDatanodeNetworkErrors(remoteAddressWithoutPort);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.shortcircuit.DfsClientShm.getPeer": "  public DomainPeer getPeer() {\n    return peer;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.shortcircuit.DfsClientShm.getEndpointShmManager": "  public EndpointShmManager getEndpointShmManager() {\n    return manager;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDNRegistrationForBP": "  public DatanodeRegistration getDNRegistrationForBP(String bpid) \n  throws IOException {\n    BPOfferService bpos = blockPoolManager.get(bpid);\n    if(bpos==null || bpos.bpRegistration==null) {\n      throw new IOException(\"cannot find BPOfferService for bpid=\"+bpid);\n    }\n    return bpos.bpRegistration;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.requestShortCircuitFdsForRead": "  FileInputStream[] requestShortCircuitFdsForRead(final ExtendedBlock blk,\n      final Token<BlockTokenIdentifier> token, int maxVersion) \n          throws ShortCircuitFdsUnsupportedException,\n            ShortCircuitFdsVersionException, IOException {\n    if (fileDescriptorPassingDisabledReason != null) {\n      throw new ShortCircuitFdsUnsupportedException(\n          fileDescriptorPassingDisabledReason);\n    }\n    checkBlockToken(blk, token, BlockTokenSecretManager.AccessMode.READ);\n    int blkVersion = CURRENT_BLOCK_FORMAT_VERSION;\n    if (maxVersion < blkVersion) {\n      throw new ShortCircuitFdsVersionException(\"Your client is too old \" +\n        \"to read this block!  Its format version is \" + \n        blkVersion + \", but the highest format version you can read is \" +\n        maxVersion);\n    }\n    metrics.incrBlocksGetLocalPathInfo();\n    FileInputStream fis[] = new FileInputStream[2];\n    \n    try {\n      fis[0] = (FileInputStream)data.getBlockInputStream(blk, 0);\n      fis[1] = DatanodeUtil.getMetaDataInputStream(blk, data);\n    } catch (ClassCastException e) {\n      LOG.debug(\"requestShortCircuitFdsForRead failed\", e);\n      throw new ShortCircuitFdsUnsupportedException(\"This DataNode's \" +\n          \"FsDatasetSpi does not support short-circuit local reads\");\n    }\n    return fis;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.checkBlockToken": "  private void checkBlockToken(ExtendedBlock block, Token<BlockTokenIdentifier> token,\n      AccessMode accessMode) throws IOException {\n    if (isBlockTokenEnabled) {\n      BlockTokenIdentifier id = new BlockTokenIdentifier();\n      ByteArrayInputStream buf = new ByteArrayInputStream(token.getIdentifier());\n      DataInputStream in = new DataInputStream(buf);\n      id.readFields(in);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Got: \" + id.toString());\n      }\n      blockPoolTokenSecretManager.checkAccess(id, null, block, accessMode);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXceiverCount": "  public int getXceiverCount() {\n    return threadGroup == null ? 0 : threadGroup.activeCount();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDisplayName": "  public String getDisplayName() {\n    // NB: our DatanodeID may not be set yet\n    return hostName + \":\" + getXferPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXferPort": "  public int getXferPort() {\n    return streamingAddr.getPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dataXceiverServer.closePeer": "  synchronized void closePeer(Peer peer) {\n    peers.remove(peer);\n    peersXceiver.remove(peer);\n    IOUtils.cleanup(null, peer);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dataXceiverServer.addPeer": "  synchronized void addPeer(Peer peer, Thread t, DataXceiver xceiver)\n      throws IOException {\n    if (closed) {\n      throw new IOException(\"Server closed.\");\n    }\n    peers.put(peer, t);\n    peersXceiver.put(peer, xceiver);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDatanodeId": "  public DatanodeID getDatanodeId() {\n    return id;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXferAddress": "  public InetSocketAddress getXferAddress() {\n    return streamingAddr;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDnConf": "  public DNConf getDnConf() {\n    return dnConf;\n  }"
        },
        "bug_report": {
            "Title": "Pre-HDFS-7915 DFSClient cannot use short circuit on post-HDFS-7915 DataNode",
            "Description": "HDFS ShortCircuitShm layer keeps the task locked up during multi-threaded split-generation.\n\nI hit this immediately after I upgraded the data, so I wonder if the ShortCircuitShim wire protocol has trouble when 2.8.0 DN talks to a 2.7.0 Client?\n\n{code}\n2015-04-06 00:04:30,780 INFO [ORC_GET_SPLITS #3] orc.OrcInputFormat: ORC pushdown predicate: leaf-0 = (IS_NULL ss_sold_date_sk)\nexpr = (not leaf-0)\n2015-04-06 00:04:30,781 ERROR [ShortCircuitCache_SlotReleaser] shortcircuit.ShortCircuitCache: ShortCircuitCache(0x29e82045): failed to release short-circuit shared memory slot Slot(slotIdx=2, shm=DfsClientShm(a86ee34576d93c4964005d90b0d97c38)) by sending ReleaseShortCircuitAccessRequestProto to /grid/0/cluster/hdfs/dn_socket.  Closing shared memory segment.\njava.io.IOException: ERROR_INVALID: there is no shared memory segment registered with shmId a86ee34576d93c4964005d90b0d97c38\n\tat org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser.run(ShortCircuitCache.java:208)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n2015-04-06 00:04:30,781 INFO [ORC_GET_SPLITS #5] orc.OrcInputFormat: ORC pushdown predicate: leaf-0 = (IS_NULL ss_sold_date_sk)\nexpr = (not leaf-0)\n2015-04-06 00:04:30,781 WARN [ShortCircuitCache_SlotReleaser] shortcircuit.DfsClientShmManager: EndpointShmManager(172.19.128.60:50010, parent=ShortCircuitShmManager(5e763476)): error shutting down shm: got IOException calling shutdown(SHUT_RDWR)\njava.nio.channels.ClosedChannelException\n\tat org.apache.hadoop.util.CloseableReferenceCount.reference(CloseableReferenceCount.java:57)\n\tat org.apache.hadoop.net.unix.DomainSocket.shutdown(DomainSocket.java:387)\n\tat org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager.shutdown(DfsClientShmManager.java:378)\n\tat org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser.run(ShortCircuitCache.java:223)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n2015-04-06 00:04:30,783 INFO [ORC_GET_SPLITS #7] orc.OrcInputFormat: ORC pushdown predicate: leaf-0 = (IS_NULL cs_sold_date_sk)\nexpr = (not leaf-0)\n2015-04-06 00:04:30,785 ERROR [ShortCircuitCache_SlotReleaser] shortcircuit.ShortCircuitCache: ShortCircuitCache(0x29e82045): failed to release short-circuit shared memory slot Slot(slotIdx=4, shm=DfsClientShm(a86ee34576d93c4964005d90b0d97c38)) by sending ReleaseShortCircuitAccessRequestProto to /grid/0/cluster/hdfs/dn_socket.  Closing shared memory segment.\njava.io.IOException: ERROR_INVALID: there is no shared memory segment registered with shmId a86ee34576d93c4964005d90b0d97c38\n\tat org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser.run(ShortCircuitCache.java:208)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n{code}\n\nLooks like a double free-fd condition?\n\n{code}\n2015-04-02 18:58:47,653 [DataXceiver for client unix:/grid/0/cluster/hdfs/dn_socket [Passing file descriptors for block BP-942051088-172.18.0.41-1370508013893:blk_1076973408_1099515627985]] INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Unregistering SlotId(3bd7fd9aed791e95acfb5034e6617d83:0) because the requestShortCircuitFdsForRead operation failed.\n2015-04-02 18:58:47,653 [DataXceiver for client unix:/grid/0/cluster/hdfs/dn_socket [Passing file descriptors for block BP-942051088-<ip>-1370508013893:blk_1076973408_1099515627985]] INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: 1076973408, srvID: ba7b6f19-47e0-4b86-af50-23981649318c, success: false\n2015-04-02 18:58:47,654 [DataXceiver for client unix:/grid/0/cluster/hdfs/dn_socket [Passing file descriptors for block BP-942051088-172.18.0.41-1370508013893:blk_1076973408_1099515627985]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: cn060-10.l42scl.hortonworks.com:50010:DataXceiver error processing REQUEST_SHORT_CIRCUIT_FDS operation  src: unix:/grid/0/cluster/hdfs/dn_socket dst: <local>\njava.io.EOFException\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.requestShortCircuitFds(DataXceiver.java:352)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitFds(Receiver.java:187)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:89)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)\n        at java.lang.Thread.run(Thread.java:745)\n{code}\n\nInvestigating more, since the exact exception from the DataNode call is not logged."
        }
    },
    {
        "filename": "HDFS-1085.json",
        "creation_time": "2010-04-06T22:49:40.000+0000",
        "stack_trace": "```\njava.io.IOException: File size not matched: copied 193855488 bytes (184.9m) to tmpfile (=hdfs://omehost.com:8020/somepath/part-00032)\nbut expected 1710327403 bytes (1.6g) from hftp://someotherhost/somepath/part-00032\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.copy(DistCp.java:435)\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:543)\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:310)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)\n        at org.apache.hadoop.mapred.Child.main(Child.java:159)\n```",
        "source_code": {},
        "bug_report": {
            "Title": "hftp read  failing silently",
            "Description": "When performing a massive distcp through hftp, we saw many tasks fail with \n\n{quote}\n2010-04-06 17:56:43,005 INFO org.apache.hadoop.tools.DistCp: FAIL 2010/0/part-00032 : java.io.IOException: File size not matched: copied 193855488 bytes (184.9m) to tmpfile (=hdfs://omehost.com:8020/somepath/part-00032)\nbut expected 1710327403 bytes (1.6g) from hftp://someotherhost/somepath/part-00032\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.copy(DistCp.java:435)\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:543)\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:310)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)\n        at org.apache.hadoop.mapred.Child.main(Child.java:159)\n{quote}\n\nThis means that read itself didn't fail but the resulted file was somehow smaller.\n"
        }
    },
    {
        "filename": "HDFS-12339.json",
        "creation_time": "2017-08-23T03:16:49.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: Unregistration failure\nCaused by: java.net.SocketException: Socket is closed\nat java.net.DatagramSocket.send(DatagramSocket.java:641)\nat org.apache.hadoop.oncrpc.SimpleUdpClient.run(SimpleUdpClient.java:62)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.SimpleUdpClient.run": "  public void run() throws IOException {\n    InetAddress IPAddress = InetAddress.getByName(host);\n    byte[] sendData = request.getBytes();\n    byte[] receiveData = new byte[65535];\n    // Use the provided socket if there is one, else just make a new one.\n    DatagramSocket socket = this.clientSocket == null ?\n        new DatagramSocket() : this.clientSocket;\n\n    try {\n      DatagramPacket sendPacket = new DatagramPacket(sendData, sendData.length,\n          IPAddress, port);\n      socket.send(sendPacket);\n      socket.setSoTimeout(udpTimeoutMillis);\n      DatagramPacket receivePacket = new DatagramPacket(receiveData,\n          receiveData.length);\n      socket.receive(receivePacket);\n  \n      // Check reply status\n      XDR xdr = new XDR(Arrays.copyOfRange(receiveData, 0,\n          receivePacket.getLength()));\n      RpcReply reply = RpcReply.read(xdr);\n      if (reply.getState() != RpcReply.ReplyState.MSG_ACCEPTED) {\n        throw new IOException(\"Request failed: \" + reply.getState());\n      }\n    } finally {\n      // If the client socket was passed in to this UDP client, it's on the\n      // caller of this UDP client to close that socket.\n      if (this.clientSocket == null) {\n        socket.close();\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcReply.read": "  public static RpcReply read(XDR xdr) {\n    int xid = xdr.readInt();\n    final Type messageType = Type.fromValue(xdr.readInt());\n    Preconditions.checkState(messageType == RpcMessage.Type.RPC_REPLY);\n    \n    ReplyState stat = ReplyState.fromValue(xdr.readInt());\n    switch (stat) {\n    case MSG_ACCEPTED:\n      return RpcAcceptedReply.read(xid, stat, xdr);\n    case MSG_DENIED:\n      return RpcDeniedReply.read(xid, stat, xdr);\n    }\n    return null;\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcReply.fromValue": "    public static ReplyState fromValue(int value) {\n      return values()[value];\n    }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcReply.getState": "  public ReplyState getState() {\n    return replyState;\n  }"
        },
        "bug_report": {
            "Title": "NFS Gateway on Shutdown Gives Unregistration Failure. Does Not Unregister with rpcbind Portmapper",
            "Description": "\n\nWhen stopping NFS Gateway the following error is thrown in the NFS gateway role logs.\n\n2017-08-17 18:09:16,529 ERROR org.apache.hadoop.oncrpc.RpcProgram: Unregistration failure with localhost:2049, portmap entry: (PortmapMapping-100003:3:6:2049)\n\n2017-08-17 18:09:16,531 WARN org.apache.hadoop.util.ShutdownHookManager: ShutdownHook 'NfsShutdownHook' failed, java.lang.RuntimeException: Unregistration failure\njava.lang.RuntimeException: Unregistration failure\n..\nCaused by: java.net.SocketException: Socket is closed\nat java.net.DatagramSocket.send(DatagramSocket.java:641)\nat org.apache.hadoop.oncrpc.SimpleUdpClient.run(SimpleUdpClient.java:62)\n\nChecking rpcinfo -p : the following entry is still there:\n\" 100003 3 tcp 2049 nfs\"\n"
        }
    },
    {
        "filename": "HDFS-6520.json",
        "creation_time": "2014-06-12T02:32:29.000+0000",
        "stack_trace": "```\njava.io.IOException: Expected empty end-of-read packet! Header: PacketHeader with packetLen=66048 header data: offsetInBlock: 65536\nseqno: 1\nlastPacketInBlock: false\ndataLen: 65536\n\n        at org.apache.hadoop.hdfs.RemoteBlockReader2.readTrailingEmptyPacket(RemoteBlockReader2.java:259)\n        at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:220)\n        at org.apache.hadoop.hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:138)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlock(NamenodeFsck.java:649)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlocksToLostFound(NamenodeFsck.java:543)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:460)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:324)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.fsck(NamenodeFsck.java:233)\n        at org.apache.hadoop.hdfs.server.namenode.FsckServlet$1.run(FsckServlet.java:67)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.hdfs.server.namenode.FsckServlet.doGet(FsckServlet.java:58)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1192)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n\njava.lang.Exception: Could not copy block data for BP-654596295-10.37.7.84-1402466764642:blk_1073741825_1001\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlock(NamenodeFsck.java:664)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlocksToLostFound(NamenodeFsck.java:543)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:460)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:324)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.fsck(NamenodeFsck.java:233)\n        at org.apache.hadoop.hdfs.server.namenode.FsckServlet$1.run(FsckServlet.java:67)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.hdfs.server.namenode.FsckServlet.doGet(FsckServlet.java:58)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1192)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.RemoteBlockReader2.readTrailingEmptyPacket": "  private void readTrailingEmptyPacket() throws IOException {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Reading empty packet at end of read\");\n    }\n    \n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader trailer = packetReceiver.getHeader();\n    if (!trailer.isLastPacketInBlock() ||\n       trailer.getDataLen() != 0) {\n      throw new IOException(\"Expected empty end-of-read packet! Header: \" +\n                            trailer);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket": "  private void readNextPacket() throws IOException {\n    //Read packet headers.\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader curHeader = packetReceiver.getHeader();\n    curDataSlice = packetReceiver.getDataSlice();\n    assert curDataSlice.capacity() == curHeader.getDataLen();\n    \n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"DFSClient readNextPacket got header \" + curHeader);\n    }\n\n    // Sanity check the lengths\n    if (!curHeader.sanityCheck(lastSeqNo)) {\n         throw new IOException(\"BlockReader: error in packet header \" +\n                               curHeader);\n    }\n    \n    if (curHeader.getDataLen() > 0) {\n      int chunks = 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n      int checksumsLen = chunks * checksumSize;\n\n      assert packetReceiver.getChecksumSlice().capacity() == checksumsLen :\n        \"checksum slice capacity=\" + packetReceiver.getChecksumSlice().capacity() + \n          \" checksumsLen=\" + checksumsLen;\n      \n      lastSeqNo = curHeader.getSeqno();\n      if (verifyChecksum && curDataSlice.remaining() > 0) {\n        // N.B.: the checksum error offset reported here is actually\n        // relative to the start of the block, not the start of the file.\n        // This is slightly misleading, but preserves the behavior from\n        // the older BlockReader.\n        checksum.verifyChunkedSums(curDataSlice,\n            packetReceiver.getChecksumSlice(),\n            filename, curHeader.getOffsetInBlock());\n      }\n      bytesNeededToFinish -= curHeader.getDataLen();\n    }    \n    \n    // First packet will include some data prior to the first byte\n    // the user requested. Skip it.\n    if (curHeader.getOffsetInBlock() < startOffset) {\n      int newPos = (int) (startOffset - curHeader.getOffsetInBlock());\n      curDataSlice.position(newPos);\n    }\n\n    // If we've now satisfied the whole client read, read one last packet\n    // header, which should be empty\n    if (bytesNeededToFinish <= 0) {\n      readTrailingEmptyPacket();\n      if (verifyChecksum) {\n        sendReadResult(Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(Status.SUCCESS);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.RemoteBlockReader2.sendReadResult": "  void sendReadResult(Status statusCode) {\n    assert !sentStatusCode : \"already sent status code to \" + peer;\n    try {\n      writeReadResult(peer.getOutputStream(), statusCode);\n      sentStatusCode = true;\n    } catch (IOException e) {\n      // It's ok not to be able to send this. But something is probably wrong.\n      LOG.info(\"Could not send read status (\" + statusCode + \") to datanode \" +\n               peer.getRemoteAddressString() + \": \" + e.getMessage());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.RemoteBlockReader2.read": "  public int read(ByteBuffer buf) throws IOException {\n    if (curDataSlice == null || curDataSlice.remaining() == 0 && bytesNeededToFinish > 0) {\n      readNextPacket();\n    }\n    if (curDataSlice.remaining() == 0) {\n      // we're at EOF now\n      return -1;\n    }\n\n    int nRead = Math.min(curDataSlice.remaining(), buf.remaining());\n    ByteBuffer writeSlice = curDataSlice.duplicate();\n    writeSlice.limit(writeSlice.position() + nRead);\n    buf.put(writeSlice);\n    curDataSlice.position(writeSlice.position());\n\n    return nRead;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlock": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures = 0;\n    InetSocketAddress targetAddr = null;\n    TreeSet<DatanodeInfo> deadNodes = new TreeSet<DatanodeInfo>();\n    BlockReader blockReader = null; \n    ExtendedBlock block = lblock.getBlock(); \n\n    while (blockReader == null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode = bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr = NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures >= DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        String file = BlockReaderFactory.getFileName(targetAddr,\n            block.getBlockPoolId(), block.getBlockId());\n        blockReader = new BlockReaderFactory(dfs.getConf()).\n            setFileName(file).\n            setBlock(block).\n            setBlockToken(lblock.getBlockToken()).\n            setStartOffset(0).\n            setLength(-1).\n            setVerifyChecksum(true).\n            setClientName(\"fsck\").\n            setDatanodeInfo(chosenNode).\n            setInetSocketAddress(targetAddr).\n            setCachingStrategy(CachingStrategy.newDropBehind()).\n            setClientCacheContext(dfs.getClientContext()).\n            setConfiguration(namenode.conf).\n            setRemotePeerFactory(new RemotePeerFactory() {\n              @Override\n              public Peer newConnectedPeer(InetSocketAddress addr)\n                  throws IOException {\n                Peer peer = null;\n                Socket s = NetUtils.getDefaultSocketFactory(conf).createSocket();\n                try {\n                  s.connect(addr, HdfsServerConstants.READ_TIMEOUT);\n                  s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n                  peer = TcpPeerServer.peerFromSocketAndKey(s, namenode.getRpcServer().\n                        getDataEncryptionKey());\n                } finally {\n                  if (peer == null) {\n                    IOUtils.closeQuietly(s);\n                  }\n                }\n                return peer;\n              }\n            }).\n            build();\n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n      }\n    }\n    byte[] buf = new byte[1024];\n    int cnt = 0;\n    boolean success = true;\n    long bytesRead = 0;\n    try {\n      while ((cnt = blockReader.read(buf, 0, buf.length)) > 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead += cnt;\n      }\n      if ( bytesRead != block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success = false;\n    } finally {\n      blockReader.close();\n    }\n    if (!success) {\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.bestNode": "  private DatanodeInfo bestNode(DFSClient dfs, DatanodeInfo[] nodes,\n                                TreeSet<DatanodeInfo> deadNodes) throws IOException {\n    if ((nodes == null) ||\n        (nodes.length - deadNodes.size() < 1)) {\n      throw new IOException(\"No live nodes contain current block\");\n    }\n    DatanodeInfo chosenNode;\n    do {\n      chosenNode = nodes[DFSUtil.getRandom().nextInt(nodes.length)];\n    } while (deadNodes.contains(chosenNode));\n    return chosenNode;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlocksToLostFound": "  private void copyBlocksToLostFound(String parent, HdfsFileStatus file,\n        LocatedBlocks blocks) throws IOException {\n    final DFSClient dfs = new DFSClient(NameNode.getAddress(conf), conf);\n    final String fullName = file.getFullName(parent);\n    OutputStream fos = null;\n    try {\n      if (!lfInited) {\n        lostFoundInit(dfs);\n      }\n      if (!lfInitedOk) {\n        throw new IOException(\"failed to initialize lost+found\");\n      }\n      String target = lostFound + fullName;\n      if (hdfsPathExists(target)) {\n        LOG.warn(\"Fsck: can't copy the remains of \" + fullName + \" to \" +\n          \"lost+found, because \" + target + \" already exists.\");\n        return;\n      }\n      if (!namenode.getRpcServer().mkdirs(\n          target, file.getPermission(), true)) {\n        throw new IOException(\"failed to create directory \" + target);\n      }\n      // create chains\n      int chain = 0;\n      boolean copyError = false;\n      for (LocatedBlock lBlk : blocks.getLocatedBlocks()) {\n        LocatedBlock lblock = lBlk;\n        DatanodeInfo[] locs = lblock.getLocations();\n        if (locs == null || locs.length == 0) {\n          if (fos != null) {\n            fos.flush();\n            fos.close();\n            fos = null;\n          }\n          continue;\n        }\n        if (fos == null) {\n          fos = dfs.create(target + \"/\" + chain, true);\n          if (fos == null) {\n            throw new IOException(\"Failed to copy \" + fullName +\n                \" to /lost+found: could not store chain \" + chain);\n          }\n          chain++;\n        }\n        \n        // copy the block. It's a pity it's not abstracted from DFSInputStream ...\n        try {\n          copyBlock(dfs, lblock, fos);\n        } catch (Exception e) {\n          LOG.error(\"Fsck: could not copy block \" + lblock.getBlock() +\n              \" to \" + target, e);\n          fos.flush();\n          fos.close();\n          fos = null;\n          internalError = true;\n          copyError = true;\n        }\n      }\n      if (copyError) {\n        LOG.warn(\"Fsck: there were errors copying the remains of the \" +\n          \"corrupted file \" + fullName + \" to /lost+found\");\n      } else {\n        LOG.info(\"Fsck: copied the remains of the corrupted file \" + \n          fullName + \" to /lost+found\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"copyBlocksToLostFound: error processing \" + fullName, e);\n      internalError = true;\n    } finally {\n      if (fos != null) fos.close();\n      dfs.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.hdfsPathExists": "  boolean hdfsPathExists(String path)\n      throws AccessControlException, UnresolvedLinkException, IOException {\n    try {\n      HdfsFileStatus hfs = namenode.getRpcServer().getFileInfo(path);\n      return (hfs != null);\n    } catch (FileNotFoundException e) {\n      return false;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.lostFoundInit": "  private void lostFoundInit(DFSClient dfs) {\n    lfInited = true;\n    try {\n      String lfName = \"/lost+found\";\n      \n      final HdfsFileStatus lfStatus = dfs.getFileInfo(lfName);\n      if (lfStatus == null) { // not exists\n        lfInitedOk = dfs.mkdirs(lfName, null, true);\n        lostFound = lfName;\n      } else if (!lfStatus.isDir()) { // exists but not a directory\n        LOG.warn(\"Cannot use /lost+found : a regular file with this name exists.\");\n        lfInitedOk = false;\n      }  else { // exists and is a directory\n        lostFound = lfName;\n        lfInitedOk = true;\n      }\n    }  catch (Exception e) {\n      e.printStackTrace();\n      lfInitedOk = false;\n    }\n    if (lostFound == null) {\n      LOG.warn(\"Cannot initialize /lost+found .\");\n      lfInitedOk = false;\n      internalError = true;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check": "  void check(String parent, HdfsFileStatus file, Result res) throws IOException {\n    String path = file.getFullName(parent);\n    boolean isOpen = false;\n\n    if (file.isDir()) {\n      if (snapshottableDirs != null && snapshottableDirs.contains(path)) {\n        String snapshotPath = (path.endsWith(Path.SEPARATOR) ? path : path\n            + Path.SEPARATOR)\n            + HdfsConstants.DOT_SNAPSHOT_DIR;\n        HdfsFileStatus snapshotFileInfo = namenode.getRpcServer().getFileInfo(\n            snapshotPath);\n        check(snapshotPath, snapshotFileInfo, res);\n      }\n      byte[] lastReturnedName = HdfsFileStatus.EMPTY_NAME;\n      DirectoryListing thisListing;\n      if (showFiles) {\n        out.println(path + \" <dir>\");\n      }\n      res.totalDirs++;\n      do {\n        assert lastReturnedName != null;\n        thisListing = namenode.getRpcServer().getListing(\n            path, lastReturnedName, false);\n        if (thisListing == null) {\n          return;\n        }\n        HdfsFileStatus[] files = thisListing.getPartialListing();\n        for (int i = 0; i < files.length; i++) {\n          check(path, files[i], res);\n        }\n        lastReturnedName = thisListing.getLastName();\n      } while (thisListing.hasMore());\n      return;\n    }\n    if (file.isSymlink()) {\n      if (showFiles) {\n        out.println(path + \" <symlink>\");\n      }\n      res.totalSymlinks++;\n      return;\n    }\n    long fileLen = file.getLen();\n    // Get block locations without updating the file access time \n    // and without block access tokens\n    LocatedBlocks blocks;\n    try {\n      blocks = namenode.getNamesystem().getBlockLocations(path, 0,\n          fileLen, false, false, false);\n    } catch (FileNotFoundException fnfe) {\n      blocks = null;\n    }\n    if (blocks == null) { // the file is deleted\n      return;\n    }\n    isOpen = blocks.isUnderConstruction();\n    if (isOpen && !showOpenFiles) {\n      // We collect these stats about open files to report with default options\n      res.totalOpenFilesSize += fileLen;\n      res.totalOpenFilesBlocks += blocks.locatedBlockCount();\n      res.totalOpenFiles++;\n      return;\n    }\n    res.totalFiles++;\n    res.totalSize += fileLen;\n    res.totalBlocks += blocks.locatedBlockCount();\n    if (showOpenFiles && isOpen) {\n      out.print(path + \" \" + fileLen + \" bytes, \" +\n        blocks.locatedBlockCount() + \" block(s), OPENFORWRITE: \");\n    } else if (showFiles) {\n      out.print(path + \" \" + fileLen + \" bytes, \" +\n        blocks.locatedBlockCount() + \" block(s): \");\n    } else {\n      out.print('.');\n    }\n    if (res.totalFiles % 100 == 0) { out.println(); out.flush(); }\n    int missing = 0;\n    int corrupt = 0;\n    long missize = 0;\n    int underReplicatedPerFile = 0;\n    int misReplicatedPerFile = 0;\n    StringBuilder report = new StringBuilder();\n    int i = 0;\n    for (LocatedBlock lBlk : blocks.getLocatedBlocks()) {\n      ExtendedBlock block = lBlk.getBlock();\n      boolean isCorrupt = lBlk.isCorrupt();\n      String blkName = block.toString();\n      DatanodeInfo[] locs = lBlk.getLocations();\n      NumberReplicas numberReplicas = namenode.getNamesystem().getBlockManager().countNodes(block.getLocalBlock());\n      int liveReplicas = numberReplicas.liveReplicas();\n      res.totalReplicas += liveReplicas;\n      short targetFileReplication = file.getReplication();\n      res.numExpectedReplicas += targetFileReplication;\n      if (liveReplicas > targetFileReplication) {\n        res.excessiveReplicas += (liveReplicas - targetFileReplication);\n        res.numOverReplicatedBlocks += 1;\n      }\n      // Check if block is Corrupt\n      if (isCorrupt) {\n        corrupt++;\n        res.corruptBlocks++;\n        out.print(\"\\n\" + path + \": CORRUPT blockpool \" + block.getBlockPoolId() + \n            \" block \" + block.getBlockName()+\"\\n\");\n      }\n      if (liveReplicas >= minReplication)\n        res.numMinReplicatedBlocks++;\n      if (liveReplicas < targetFileReplication && liveReplicas > 0) {\n        res.missingReplicas += (targetFileReplication - liveReplicas);\n        res.numUnderReplicatedBlocks += 1;\n        underReplicatedPerFile++;\n        if (!showFiles) {\n          out.print(\"\\n\" + path + \": \");\n        }\n        out.println(\" Under replicated \" + block +\n                    \". Target Replicas is \" +\n                    targetFileReplication + \" but found \" +\n                    liveReplicas + \" replica(s).\");\n      }\n      // verify block placement policy\n      BlockPlacementStatus blockPlacementStatus = bpPolicy\n          .verifyBlockPlacement(path, lBlk, targetFileReplication);\n      if (!blockPlacementStatus.isPlacementPolicySatisfied()) {\n        res.numMisReplicatedBlocks++;\n        misReplicatedPerFile++;\n        if (!showFiles) {\n          if(underReplicatedPerFile == 0)\n            out.println();\n          out.print(path + \": \");\n        }\n        out.println(\" Replica placement policy is violated for \" + \n                    block + \". \" + blockPlacementStatus.getErrorDescription());\n      }\n      report.append(i + \". \" + blkName + \" len=\" + block.getNumBytes());\n      if (liveReplicas == 0) {\n        report.append(\" MISSING!\");\n        res.addMissing(block.toString(), block.getNumBytes());\n        missing++;\n        missize += block.getNumBytes();\n      } else {\n        report.append(\" repl=\" + liveReplicas);\n        if (showLocations || showRacks) {\n          StringBuilder sb = new StringBuilder(\"[\");\n          for (int j = 0; j < locs.length; j++) {\n            if (j > 0) { sb.append(\", \"); }\n            if (showRacks)\n              sb.append(NodeBase.getPath(locs[j]));\n            else\n              sb.append(locs[j]);\n          }\n          sb.append(']');\n          report.append(\" \" + sb.toString());\n        }\n      }\n      report.append('\\n');\n      i++;\n    }\n    if ((missing > 0) || (corrupt > 0)) {\n      if (!showFiles && (missing > 0)) {\n        out.print(\"\\n\" + path + \": MISSING \" + missing\n            + \" blocks of total size \" + missize + \" B.\");\n      }\n      res.corruptFiles++;\n      if (isOpen) {\n        LOG.info(\"Fsck: ignoring open file \" + path);\n      } else {\n        if (doMove) copyBlocksToLostFound(parent, file, blocks);\n        if (doDelete) deleteCorruptedFile(path);\n      }\n    }\n    if (showFiles) {\n      if (missing > 0) {\n        out.print(\" MISSING \" + missing + \" blocks of total size \" + missize + \" B\\n\");\n      }  else if (underReplicatedPerFile == 0 && misReplicatedPerFile == 0) {\n        out.print(\" OK\\n\");\n      }\n      if (showBlocks) {\n        out.print(report.toString() + \"\\n\");\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.deleteCorruptedFile": "  private void deleteCorruptedFile(String path) {\n    try {\n      namenode.getRpcServer().delete(path, true);\n      LOG.info(\"Fsck: deleted corrupt file \" + path);\n    } catch (Exception e) {\n      LOG.error(\"Fsck: error deleting corrupted file \" + path, e);\n      internalError = true;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.toString": "    public String toString() {\n      StringBuilder res = new StringBuilder();\n      res.append(\"Status: \").append((isHealthy() ? \"HEALTHY\" : \"CORRUPT\"))\n          .append(\"\\n Total size:\\t\").append(totalSize).append(\" B\");\n      if (totalOpenFilesSize != 0) {\n        res.append(\" (Total open files size: \").append(totalOpenFilesSize)\n            .append(\" B)\");\n      }\n      res.append(\"\\n Total dirs:\\t\").append(totalDirs).append(\n          \"\\n Total files:\\t\").append(totalFiles);\n      res.append(\"\\n Total symlinks:\\t\\t\").append(totalSymlinks);\n      if (totalOpenFiles != 0) {\n        res.append(\" (Files currently being written: \").append(totalOpenFiles)\n            .append(\")\");\n      }\n      res.append(\"\\n Total blocks (validated):\\t\").append(totalBlocks);\n      if (totalBlocks > 0) {\n        res.append(\" (avg. block size \").append((totalSize / totalBlocks))\n            .append(\" B)\");\n      }\n      if (totalOpenFilesBlocks != 0) {\n        res.append(\" (Total open file blocks (not validated): \").append(\n            totalOpenFilesBlocks).append(\")\");\n      }\n      if (corruptFiles > 0) {\n        res.append(\"\\n  ********************************\").append(\n            \"\\n  CORRUPT FILES:\\t\").append(corruptFiles);\n        if (missingSize > 0) {\n          res.append(\"\\n  MISSING BLOCKS:\\t\").append(missingIds.size()).append(\n              \"\\n  MISSING SIZE:\\t\\t\").append(missingSize).append(\" B\");\n        }\n        if (corruptBlocks > 0) {\n          res.append(\"\\n  CORRUPT BLOCKS: \\t\").append(corruptBlocks);\n        }\n        res.append(\"\\n  ********************************\");\n      }\n      res.append(\"\\n Minimally replicated blocks:\\t\").append(\n          numMinReplicatedBlocks);\n      if (totalBlocks > 0) {\n        res.append(\" (\").append(\n            ((float) (numMinReplicatedBlocks * 100) / (float) totalBlocks))\n            .append(\" %)\");\n      }\n      res.append(\"\\n Over-replicated blocks:\\t\")\n          .append(numOverReplicatedBlocks);\n      if (totalBlocks > 0) {\n        res.append(\" (\").append(\n            ((float) (numOverReplicatedBlocks * 100) / (float) totalBlocks))\n            .append(\" %)\");\n      }\n      res.append(\"\\n Under-replicated blocks:\\t\").append(\n          numUnderReplicatedBlocks);\n      if (totalBlocks > 0) {\n        res.append(\" (\").append(\n            ((float) (numUnderReplicatedBlocks * 100) / (float) totalBlocks))\n            .append(\" %)\");\n      }\n      res.append(\"\\n Mis-replicated blocks:\\t\\t\")\n          .append(numMisReplicatedBlocks);\n      if (totalBlocks > 0) {\n        res.append(\" (\").append(\n            ((float) (numMisReplicatedBlocks * 100) / (float) totalBlocks))\n            .append(\" %)\");\n      }\n      res.append(\"\\n Default replication factor:\\t\").append(replication)\n          .append(\"\\n Average block replication:\\t\").append(\n              getReplicationFactor()).append(\"\\n Corrupt blocks:\\t\\t\").append(\n              corruptBlocks).append(\"\\n Missing replicas:\\t\\t\").append(\n              missingReplicas);\n      if (totalReplicas > 0) {\n        res.append(\" (\").append(\n            ((float) (missingReplicas * 100) / (float) numExpectedReplicas)).append(\n            \" %)\");\n      }\n      return res.toString();\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.addMissing": "    void addMissing(String id, long size) {\n      missingIds.add(id);\n      missingSize += size;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.fsck": "  public void fsck() {\n    final long startTime = Time.now();\n    try {\n      String msg = \"FSCK started by \" + UserGroupInformation.getCurrentUser()\n          + \" from \" + remoteAddress + \" for path \" + path + \" at \" + new Date();\n      LOG.info(msg);\n      out.println(msg);\n      namenode.getNamesystem().logFsckEvent(path, remoteAddress);\n\n      if (snapshottableDirs != null) {\n        SnapshottableDirectoryStatus[] snapshotDirs = namenode.getRpcServer()\n            .getSnapshottableDirListing();\n        if (snapshotDirs != null) {\n          for (SnapshottableDirectoryStatus dir : snapshotDirs) {\n            snapshottableDirs.add(dir.getFullPath().toString());\n          }\n        }\n      }\n\n      final HdfsFileStatus file = namenode.getRpcServer().getFileInfo(path);\n      if (file != null) {\n\n        if (showCorruptFileBlocks) {\n          listCorruptFileBlocks();\n          return;\n        }\n        \n        Result res = new Result(conf);\n\n        check(path, file, res);\n\n        out.println(res);\n        out.println(\" Number of data-nodes:\\t\\t\" + totalDatanodes);\n        out.println(\" Number of racks:\\t\\t\" + networktopology.getNumOfRacks());\n\n        out.println(\"FSCK ended at \" + new Date() + \" in \"\n            + (Time.now() - startTime + \" milliseconds\"));\n\n        // If there were internal errors during the fsck operation, we want to\n        // return FAILURE_STATUS, even if those errors were not immediately\n        // fatal.  Otherwise many unit tests will pass even when there are bugs.\n        if (internalError) {\n          throw new IOException(\"fsck encountered internal errors!\");\n        }\n\n        // DFSck client scans for the string HEALTHY/CORRUPT to check the status\n        // of file system and return appropriate code. Changing the output\n        // string might break testcases. Also note this must be the last line \n        // of the report.\n        if (res.isHealthy()) {\n          out.print(\"\\n\\nThe filesystem under path '\" + path + \"' \" + HEALTHY_STATUS);\n        } else {\n          out.print(\"\\n\\nThe filesystem under path '\" + path + \"' \" + CORRUPT_STATUS);\n        }\n\n      } else {\n        out.print(\"\\n\\nPath '\" + path + \"' \" + NONEXISTENT_STATUS);\n      }\n    } catch (Exception e) {\n      String errMsg = \"Fsck on path '\" + path + \"' \" + FAILURE_STATUS;\n      LOG.warn(errMsg, e);\n      out.println(\"FSCK ended at \" + new Date() + \" in \"\n          + (Time.now() - startTime + \" milliseconds\"));\n      out.println(e.getMessage());\n      out.print(\"\\n\\n\" + errMsg);\n    } finally {\n      out.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.listCorruptFileBlocks": "  private void listCorruptFileBlocks() throws IOException {\n    Collection<FSNamesystem.CorruptFileBlockInfo> corruptFiles = namenode.\n      getNamesystem().listCorruptFileBlocks(path, currentCookie);\n    int numCorruptFiles = corruptFiles.size();\n    String filler;\n    if (numCorruptFiles > 0) {\n      filler = Integer.toString(numCorruptFiles);\n    } else if (currentCookie[0].equals(\"0\")) {\n      filler = \"no\";\n    } else {\n      filler = \"no more\";\n    }\n    out.println(\"Cookie:\\t\" + currentCookie[0]);\n    for (FSNamesystem.CorruptFileBlockInfo c : corruptFiles) {\n      out.println(c.toString());\n    }\n    out.println(\"\\n\\nThe filesystem under path '\" + path + \"' has \" + filler\n        + \" CORRUPT files\");\n    out.println();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.isHealthy": "    boolean isHealthy() {\n      return ((missingIds.size() == 0) && (corruptBlocks == 0));\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FsckServlet.run": "        public Object run() throws Exception {\n          NameNode nn = NameNodeHttpServer.getNameNodeFromContext(context);\n          \n          final FSNamesystem namesystem = nn.getNamesystem();\n          final BlockManager bm = namesystem.getBlockManager();\n          final int totalDatanodes = \n              namesystem.getNumberOfDatanodes(DatanodeReportType.LIVE); \n          new NamenodeFsck(conf, nn,\n              bm.getDatanodeManager().getNetworkTopology(), pmap, out,\n              totalDatanodes, bm.minReplication, remoteAddress).fsck();\n          \n          return null;\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FsckServlet.doGet": "  public void doGet(HttpServletRequest request, HttpServletResponse response\n      ) throws IOException {\n    @SuppressWarnings(\"unchecked\")\n    final Map<String,String[]> pmap = request.getParameterMap();\n    final PrintWriter out = response.getWriter();\n    final InetAddress remoteAddress = \n      InetAddress.getByName(request.getRemoteAddr());\n    final ServletContext context = getServletContext();    \n    final Configuration conf = NameNodeHttpServer.getConfFromContext(context);\n\n    final UserGroupInformation ugi = getUGI(request, conf);\n    try {\n      ugi.doAs(new PrivilegedExceptionAction<Object>() {\n        @Override\n        public Object run() throws Exception {\n          NameNode nn = NameNodeHttpServer.getNameNodeFromContext(context);\n          \n          final FSNamesystem namesystem = nn.getNamesystem();\n          final BlockManager bm = namesystem.getBlockManager();\n          final int totalDatanodes = \n              namesystem.getNumberOfDatanodes(DatanodeReportType.LIVE); \n          new NamenodeFsck(conf, nn,\n              bm.getDatanodeManager().getNetworkTopology(), pmap, out,\n              totalDatanodes, bm.minReplication, remoteAddress).fsck();\n          \n          return null;\n        }\n      });\n    } catch (InterruptedException e) {\n      response.sendError(400, e.getMessage());\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.doFilter": "    public void doFilter(ServletRequest request,\n                         ServletResponse response,\n                         FilterChain chain\n                         ) throws IOException, ServletException {\n      HttpServletRequestWrapper quoted =\n        new RequestQuoter((HttpServletRequest) request);\n      HttpServletResponse httpResponse = (HttpServletResponse) response;\n\n      String mime = inferMimeType(request);\n      if (mime == null) {\n        httpResponse.setContentType(\"text/plain; charset=utf-8\");\n      } else if (mime.startsWith(\"text/html\")) {\n        // HTML with unspecified encoding, we want to\n        // force HTML with utf-8 encoding\n        // This is to avoid the following security issue:\n        // http://openmya.hacker.jp/hasegawa/security/utf7cs.html\n        httpResponse.setContentType(\"text/html; charset=utf-8\");\n      } else if (mime.startsWith(\"application/xml\")) {\n        httpResponse.setContentType(\"text/xml; charset=utf-8\");\n      }\n      chain.doFilter(quoted, httpResponse);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.inferMimeType": "    private String inferMimeType(ServletRequest request) {\n      String path = ((HttpServletRequest)request).getRequestURI();\n      ContextHandler.SContext sContext = (ContextHandler.SContext)config.getServletContext();\n      MimeTypes mimes = sContext.getContextHandler().getMimeTypes();\n      Buffer mimeBuffer = mimes.getMimeByExtension(path);\n      return (mimeBuffer == null) ? null : mimeBuffer.toString();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.NoCacheFilter.doFilter": "  public void doFilter(ServletRequest req, ServletResponse res,\n                       FilterChain chain)\n    throws IOException, ServletException {\n    HttpServletResponse httpRes = (HttpServletResponse) res;\n    httpRes.setHeader(\"Cache-Control\", \"no-cache\");\n    long now = System.currentTimeMillis();\n    httpRes.addDateHeader(\"Expires\", now);\n    httpRes.addDateHeader(\"Date\", now);\n    httpRes.addHeader(\"Pragma\", \"no-cache\");\n    chain.doFilter(req, res);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.namenode.getRpcServer": "  public NamenodeProtocols getRpcServer() {\n    return rpcServer;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress": "  public static InetSocketAddress getAddress(URI filesystemURI) {\n    String authority = filesystemURI.getAuthority();\n    if (authority == null) {\n      throw new IllegalArgumentException(String.format(\n          \"Invalid URI for NameNode address (check %s): %s has no authority.\",\n          FileSystem.FS_DEFAULT_NAME_KEY, filesystemURI.toString()));\n    }\n    if (!HdfsConstants.HDFS_URI_SCHEME.equalsIgnoreCase(\n        filesystemURI.getScheme())) {\n      throw new IllegalArgumentException(String.format(\n          \"Invalid URI for NameNode address (check %s): %s is not of scheme '%s'.\",\n          FileSystem.FS_DEFAULT_NAME_KEY, filesystemURI.toString(),\n          HdfsConstants.HDFS_URI_SCHEME));\n    }\n    return getAddress(authority);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.format": "  private static boolean format(Configuration conf, boolean force,\n      boolean isInteractive) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    checkAllowFormat(conf);\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      InetSocketAddress socAddr = getAddress(conf);\n      SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n          DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, socAddr.getHostName());\n    }\n    \n    Collection<URI> nameDirsToFormat = FSNamesystem.getNamespaceDirs(conf);\n    List<URI> sharedDirs = FSNamesystem.getSharedEditsDirs(conf);\n    List<URI> dirsToPrompt = new ArrayList<URI>();\n    dirsToPrompt.addAll(nameDirsToFormat);\n    dirsToPrompt.addAll(sharedDirs);\n    List<URI> editDirsToFormat = \n                 FSNamesystem.getNamespaceEditsDirs(conf);\n\n    // if clusterID is not provided - see if you can find the current one\n    String clusterId = StartupOption.FORMAT.getClusterId();\n    if(clusterId == null || clusterId.equals(\"\")) {\n      //Generate a new cluster id\n      clusterId = NNStorage.newClusterID();\n    }\n    System.out.println(\"Formatting using clusterid: \" + clusterId);\n    \n    FSImage fsImage = new FSImage(conf, nameDirsToFormat, editDirsToFormat);\n    try {\n      FSNamesystem fsn = new FSNamesystem(conf, fsImage);\n      fsImage.getEditLog().initJournalsForWrite();\n\n      if (!fsImage.confirmFormat(force, isInteractive)) {\n        return true; // aborted\n      }\n\n      fsImage.format(fsn, clusterId);\n    } catch (IOException ioe) {\n      LOG.warn(\"Encountered exception during format: \", ioe);\n      fsImage.close();\n      throw ioe;\n    }\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.namenode.getNamesystem": "  public FSNamesystem getNamesystem() {\n    return namesystem;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNumberOfDatanodes": "  int getNumberOfDatanodes(DatanodeReportType type) {\n    readLock();\n    try {\n      return getBlockManager().getDatanodeManager().getDatanodeListForReport(\n          type).size(); \n    } finally {\n      readUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockManager": "  public BlockManager getBlockManager() {\n    return blockManager;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.readUnlock": "  public void readUnlock() {\n    this.fsLock.readLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.readLock": "  public void readLock() {\n    this.fsLock.readLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.getNameNodeFromContext": "  public static NameNode getNameNodeFromContext(ServletContext context) {\n    return (NameNode)context.getAttribute(NAMENODE_ATTRIBUTE_KEY);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getNamesystem": "  public FSNamesystem getNamesystem() {\n    return namesystem;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.getConfFromContext": "  static Configuration getConfFromContext(ServletContext context) {\n    return (Configuration)context.getAttribute(JspHelper.CURRENT_CONF);\n  }"
        },
        "bug_report": {
            "Title": "hdfs fsck -move passes invalid length value when creating BlockReader",
            "Description": "I met some error when I run fsck -move.\nMy steps are as the following:\n1. Set up a pseudo cluster\n2. Copy a file to hdfs\n3. Corrupt a block of the file\n4. Run fsck to check:\n{code}\nConnecting to namenode via http://localhost:50070\nFSCK started by hadoop (auth:SIMPLE) from /127.0.0.1 for path /user/hadoop at Wed Jun 11 15:58:38 CST 2014\n.\n/user/hadoop/fsck-test: CORRUPT blockpool BP-654596295-10.37.7.84-1402466764642 block blk_1073741825\n\n/user/hadoop/fsck-test: MISSING 1 blocks of total size 1048576 B.Status: CORRUPT\n Total size:    4104304 B\n Total dirs:    1\n Total files:   1\n Total symlinks:                0\n Total blocks (validated):      4 (avg. block size 1026076 B)\n  ********************************\n  CORRUPT FILES:        1\n  MISSING BLOCKS:       1\n  MISSING SIZE:         1048576 B\n  CORRUPT BLOCKS:       1\n  ********************************\n Minimally replicated blocks:   3 (75.0 %)\n Over-replicated blocks:        0 (0.0 %)\n Under-replicated blocks:       0 (0.0 %)\n Mis-replicated blocks:         0 (0.0 %)\n Default replication factor:    1\n Average block replication:     0.75\n Corrupt blocks:                1\n Missing replicas:              0 (0.0 %)\n Number of data-nodes:          1\n Number of racks:               1\nFSCK ended at Wed Jun 11 15:58:38 CST 2014 in 1 milliseconds\n\n\nThe filesystem under path '/user/hadoop' is CORRUPT\n{code}\n5. Run fsck -move to move the corrupted file to /lost+found and the error message in the namenode log:\n{code}\n2014-06-11 15:48:16,686 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: FSCK started by hadoop (auth:SIMPLE) from /127.0.0.1 for path /user/hadoop at Wed Jun 11 15:48:16 CST 2014\n2014-06-11 15:48:16,894 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 35 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 0 Number of syncs: 25 SyncTimes(ms): 73\n2014-06-11 15:48:16,991 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Error reading block\njava.io.IOException: Expected empty end-of-read packet! Header: PacketHeader with packetLen=66048 header data: offsetInBlock: 65536\nseqno: 1\nlastPacketInBlock: false\ndataLen: 65536\n\n        at org.apache.hadoop.hdfs.RemoteBlockReader2.readTrailingEmptyPacket(RemoteBlockReader2.java:259)\n        at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:220)\n        at org.apache.hadoop.hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:138)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlock(NamenodeFsck.java:649)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlocksToLostFound(NamenodeFsck.java:543)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:460)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:324)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.fsck(NamenodeFsck.java:233)\n        at org.apache.hadoop.hdfs.server.namenode.FsckServlet$1.run(FsckServlet.java:67)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.hdfs.server.namenode.FsckServlet.doGet(FsckServlet.java:58)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1192)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n2014-06-11 15:48:16,992 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Fsck: could not copy block BP-654596295-10.37.7.84-1402466764642:blk_1073741825_1001 to /lost+found/user/hadoop/fsck-test\njava.lang.Exception: Could not copy block data for BP-654596295-10.37.7.84-1402466764642:blk_1073741825_1001\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlock(NamenodeFsck.java:664)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlocksToLostFound(NamenodeFsck.java:543)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:460)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:324)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.fsck(NamenodeFsck.java:233)\n        at org.apache.hadoop.hdfs.server.namenode.FsckServlet$1.run(FsckServlet.java:67)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.hdfs.server.namenode.FsckServlet.doGet(FsckServlet.java:58)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1192)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n2014-06-11 15:48:16,994 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /lost+found/user/hadoop/fsck-test/0 is closed by DFSClient_NONMAPREDUCE_-774755866_14\n{code}"
        }
    },
    {
        "filename": "HDFS-10715.json",
        "creation_time": "2016-08-02T08:10:23.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.compareDataNode(AvailableSpaceBlockPlacementPolicy.java:95)\n        at org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.chooseDataNode(AvailableSpaceBlockPlacementPolicy.java:80)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:691)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:665)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseLocalRack(BlockPlacementPolicyDefault.java:572)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:457)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:367)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:242)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:114)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:130)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1606)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3315)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:679)\n        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:214)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:489)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.compareDataNode": "  protected int compareDataNode(final DatanodeDescriptor a,\n      final DatanodeDescriptor b) {\n    if (a.equals(b)\n        || Math.abs(a.getDfsUsedPercent() - b.getDfsUsedPercent()) < 5) {\n      return 0;\n    }\n    return a.getDfsUsedPercent() < b.getDfsUsedPercent() ? -1 : 1;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.chooseDataNode": "  protected DatanodeDescriptor chooseDataNode(final String scope,\n      final Collection<Node> excludedNode) {\n    DatanodeDescriptor a =\n        (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNode);\n    DatanodeDescriptor b =\n        (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNode);\n    int ret = compareDataNode(a, b);\n    if (ret == 0) {\n      return a;\n    } else if (ret < 0) {\n      return (RAND.nextInt(100) < balancedPreference) ? a : b;\n    } else {\n      return (RAND.nextInt(100) < balancedPreference) ? b : a;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom": "  protected DatanodeStorageInfo chooseRandom(int numOfReplicas,\n                            String scope,\n                            Set<Node> excludedNodes,\n                            long blocksize,\n                            int maxNodesPerRack,\n                            List<DatanodeStorageInfo> results,\n                            boolean avoidStaleNodes,\n                            EnumMap<StorageType, Integer> storageTypes)\n                            throws NotEnoughReplicasException {\n    StringBuilder builder = null;\n    if (LOG.isDebugEnabled()) {\n      builder = debugLoggingBuilder.get();\n      builder.setLength(0);\n      builder.append(\"[\");\n    }\n    boolean badTarget = false;\n    DatanodeStorageInfo firstChosen = null;\n    while (numOfReplicas > 0) {\n      DatanodeDescriptor chosenNode = chooseDataNode(scope, excludedNodes);\n      if (chosenNode == null) {\n        break;\n      }\n      Preconditions.checkState(excludedNodes.add(chosenNode), \"chosenNode \"\n          + chosenNode + \" is already in excludedNodes \" + excludedNodes);\n      if (LOG.isDebugEnabled()) {\n        builder.append(\"\\nNode \").append(NodeBase.getPath(chosenNode))\n            .append(\" [\");\n      }\n      DatanodeStorageInfo storage = null;\n      if (isGoodDatanode(chosenNode, maxNodesPerRack, considerLoad,\n          results, avoidStaleNodes)) {\n        for (Iterator<Map.Entry<StorageType, Integer>> iter = storageTypes\n            .entrySet().iterator(); iter.hasNext();) {\n          Map.Entry<StorageType, Integer> entry = iter.next();\n          storage = chooseStorage4Block(\n              chosenNode, blocksize, results, entry.getKey());\n          if (storage != null) {\n            numOfReplicas--;\n            if (firstChosen == null) {\n              firstChosen = storage;\n            }\n            // add node (subclasses may also add related nodes) to excludedNode\n            addToExcludedNodes(chosenNode, excludedNodes);\n            int num = entry.getValue();\n            if (num == 1) {\n              iter.remove();\n            } else {\n              entry.setValue(num - 1);\n            }\n            break;\n          }\n        }\n\n        if (LOG.isDebugEnabled()) {\n          builder.append(\"\\n]\");\n        }\n\n        // If no candidate storage was found on this DN then set badTarget.\n        badTarget = (storage == null);\n      }\n    }\n    if (numOfReplicas>0) {\n      String detail = enableDebugLogging;\n      if (LOG.isDebugEnabled()) {\n        if (badTarget && builder != null) {\n          detail = builder.toString();\n          builder.setLength(0);\n        } else {\n          detail = \"\";\n        }\n      }\n      throw new NotEnoughReplicasException(detail);\n    }\n    \n    return firstChosen;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseStorage4Block": "  DatanodeStorageInfo chooseStorage4Block(DatanodeDescriptor dnd,\n      long blockSize,\n      List<DatanodeStorageInfo> results,\n      StorageType storageType) {\n    DatanodeStorageInfo storage =\n        dnd.chooseStorage4Block(storageType, blockSize);\n    if (storage != null) {\n      results.add(storage);\n    } else {\n      logNodeIsNotChosen(dnd, \"no good storage to place the block \");\n    }\n    return storage;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.addToExcludedNodes": "  protected int addToExcludedNodes(DatanodeDescriptor localMachine,\n      Set<Node> excludedNodes) {\n    return excludedNodes.add(localMachine) ? 1 : 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.isGoodDatanode": "  boolean isGoodDatanode(DatanodeDescriptor node,\n                         int maxTargetPerRack, boolean considerLoad,\n                         List<DatanodeStorageInfo> results,\n                         boolean avoidStaleNodes) {\n    // check if the node is (being) decommissioned\n    if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n      logNodeIsNotChosen(node, \"the node is (being) decommissioned \");\n      return false;\n    }\n\n    if (avoidStaleNodes) {\n      if (node.isStale(this.staleInterval)) {\n        logNodeIsNotChosen(node, \"the node is stale \");\n        return false;\n      }\n    }\n\n    // check the communication traffic of the target machine\n    if (considerLoad) {\n      final double maxLoad = considerLoadFactor *\n          stats.getInServiceXceiverAverage();\n      final int nodeLoad = node.getXceiverCount();\n      if (nodeLoad > maxLoad) {\n        logNodeIsNotChosen(node, \"the node is too busy (load: \" + nodeLoad\n            + \" > \" + maxLoad + \") \");\n        return false;\n      }\n    }\n      \n    // check if the target rack has chosen too many nodes\n    String rackname = node.getNetworkLocation();\n    int counter=1;\n    for(DatanodeStorageInfo resultStorage : results) {\n      if (rackname.equals(\n          resultStorage.getDatanodeDescriptor().getNetworkLocation())) {\n        counter++;\n      }\n    }\n    if (counter > maxTargetPerRack) {\n      logNodeIsNotChosen(node, \"the rack has too many chosen nodes \");\n      return false;\n    }\n\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseDataNode": "  protected DatanodeDescriptor chooseDataNode(final String scope,\n      final Collection<Node> excludedNodes) {\n    return (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNodes);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseLocalRack": "  protected DatanodeStorageInfo chooseLocalRack(Node localMachine,\n                                                Set<Node> excludedNodes,\n                                                long blocksize,\n                                                int maxNodesPerRack,\n                                                List<DatanodeStorageInfo> results,\n                                                boolean avoidStaleNodes,\n                                                EnumMap<StorageType, Integer> storageTypes)\n      throws NotEnoughReplicasException {\n    // no local machine, so choose a random machine\n    if (localMachine == null) {\n      return chooseRandom(NodeBase.ROOT, excludedNodes, blocksize,\n          maxNodesPerRack, results, avoidStaleNodes, storageTypes);\n    }\n    final String localRack = localMachine.getNetworkLocation();\n      \n    try {\n      // choose one from the local rack\n      return chooseRandom(localRack, excludedNodes,\n          blocksize, maxNodesPerRack, results, avoidStaleNodes, storageTypes);\n    } catch (NotEnoughReplicasException e) {\n      // find the next replica and retry with its rack\n      for(DatanodeStorageInfo resultStorage : results) {\n        DatanodeDescriptor nextNode = resultStorage.getDatanodeDescriptor();\n        if (nextNode != localMachine) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Failed to choose from local rack (location = \" + localRack\n                + \"), retry with the rack of the next replica (location = \"\n                + nextNode.getNetworkLocation() + \")\", e);\n          }\n          return chooseFromNextRack(nextNode, excludedNodes, blocksize,\n              maxNodesPerRack, results, avoidStaleNodes, storageTypes);\n        }\n      }\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Failed to choose from local rack (location = \" + localRack\n            + \"); the second replica is not found, retry choosing ramdomly\", e);\n      }\n      //the second replica is not found, randomly choose one from the network\n      return chooseRandom(NodeBase.ROOT, excludedNodes, blocksize,\n          maxNodesPerRack, results, avoidStaleNodes, storageTypes);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseFromNextRack": "  private DatanodeStorageInfo chooseFromNextRack(Node next,\n      Set<Node> excludedNodes,\n      long blocksize,\n      int maxNodesPerRack,\n      List<DatanodeStorageInfo> results,\n      boolean avoidStaleNodes,\n      EnumMap<StorageType, Integer> storageTypes) throws NotEnoughReplicasException {\n    final String nextRack = next.getNetworkLocation();\n    try {\n      return chooseRandom(nextRack, excludedNodes, blocksize, maxNodesPerRack,\n          results, avoidStaleNodes, storageTypes);\n    } catch(NotEnoughReplicasException e) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Failed to choose from the next rack (location = \" + nextRack\n            + \"), retry choosing ramdomly\", e);\n      }\n      //otherwise randomly choose one from the network\n      return chooseRandom(NodeBase.ROOT, excludedNodes, blocksize,\n          maxNodesPerRack, results, avoidStaleNodes, storageTypes);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder": "  protected Node chooseTargetInOrder(int numOfReplicas, \n                                 Node writer,\n                                 final Set<Node> excludedNodes,\n                                 final long blocksize,\n                                 final int maxNodesPerRack,\n                                 final List<DatanodeStorageInfo> results,\n                                 final boolean avoidStaleNodes,\n                                 final boolean newBlock,\n                                 EnumMap<StorageType, Integer> storageTypes)\n                                 throws NotEnoughReplicasException {\n    final int numOfResults = results.size();\n    if (numOfResults == 0) {\n      writer = chooseLocalStorage(writer, excludedNodes, blocksize,\n          maxNodesPerRack, results, avoidStaleNodes, storageTypes, true)\n          .getDatanodeDescriptor();\n      if (--numOfReplicas == 0) {\n        return writer;\n      }\n    }\n    final DatanodeDescriptor dn0 = results.get(0).getDatanodeDescriptor();\n    if (numOfResults <= 1) {\n      chooseRemoteRack(1, dn0, excludedNodes, blocksize, maxNodesPerRack,\n          results, avoidStaleNodes, storageTypes);\n      if (--numOfReplicas == 0) {\n        return writer;\n      }\n    }\n    if (numOfResults <= 2) {\n      final DatanodeDescriptor dn1 = results.get(1).getDatanodeDescriptor();\n      if (clusterMap.isOnSameRack(dn0, dn1)) {\n        chooseRemoteRack(1, dn0, excludedNodes, blocksize, maxNodesPerRack,\n            results, avoidStaleNodes, storageTypes);\n      } else if (newBlock){\n        chooseLocalRack(dn1, excludedNodes, blocksize, maxNodesPerRack,\n            results, avoidStaleNodes, storageTypes);\n      } else {\n        chooseLocalRack(writer, excludedNodes, blocksize, maxNodesPerRack,\n            results, avoidStaleNodes, storageTypes);\n      }\n      if (--numOfReplicas == 0) {\n        return writer;\n      }\n    }\n    chooseRandom(numOfReplicas, NodeBase.ROOT, excludedNodes, blocksize,\n        maxNodesPerRack, results, avoidStaleNodes, storageTypes);\n    return writer;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack": "  protected void chooseRemoteRack(int numOfReplicas,\n                                DatanodeDescriptor localMachine,\n                                Set<Node> excludedNodes,\n                                long blocksize,\n                                int maxReplicasPerRack,\n                                List<DatanodeStorageInfo> results,\n                                boolean avoidStaleNodes,\n                                EnumMap<StorageType, Integer> storageTypes)\n                                    throws NotEnoughReplicasException {\n    int oldNumOfReplicas = results.size();\n    // randomly choose one node from remote racks\n    try {\n      chooseRandom(numOfReplicas, \"~\" + localMachine.getNetworkLocation(),\n          excludedNodes, blocksize, maxReplicasPerRack, results,\n          avoidStaleNodes, storageTypes);\n    } catch (NotEnoughReplicasException e) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Failed to choose remote rack (location = ~\"\n            + localMachine.getNetworkLocation() + \"), fallback to local rack\", e);\n      }\n      chooseRandom(numOfReplicas-(results.size()-oldNumOfReplicas),\n                   localMachine.getNetworkLocation(), excludedNodes, blocksize, \n                   maxReplicasPerRack, results, avoidStaleNodes, storageTypes);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseLocalStorage": "  protected DatanodeStorageInfo chooseLocalStorage(Node localMachine,\n      Set<Node> excludedNodes, long blocksize, int maxNodesPerRack,\n      List<DatanodeStorageInfo> results, boolean avoidStaleNodes,\n      EnumMap<StorageType, Integer> storageTypes, boolean fallbackToLocalRack)\n      throws NotEnoughReplicasException {\n    DatanodeStorageInfo localStorage = chooseLocalStorage(localMachine,\n        excludedNodes, blocksize, maxNodesPerRack, results,\n        avoidStaleNodes, storageTypes);\n    if (localStorage != null) {\n      return localStorage;\n    }\n\n    if (!fallbackToLocalRack) {\n      return null;\n    }\n    // try a node on local rack\n    return chooseLocalRack(localMachine, excludedNodes, blocksize,\n        maxNodesPerRack, results, avoidStaleNodes, storageTypes);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget": "  private Node chooseTarget(int numOfReplicas,\n                            Node writer,\n                            final Set<Node> excludedNodes,\n                            final long blocksize,\n                            final int maxNodesPerRack,\n                            final List<DatanodeStorageInfo> results,\n                            final boolean avoidStaleNodes,\n                            final BlockStoragePolicy storagePolicy,\n                            final EnumSet<StorageType> unavailableStorages,\n                            final boolean newBlock) {\n    if (numOfReplicas == 0 || clusterMap.getNumOfLeaves()==0) {\n      return (writer instanceof DatanodeDescriptor) ? writer : null;\n    }\n    final int numOfResults = results.size();\n    final int totalReplicasExpected = numOfReplicas + numOfResults;\n    if ((writer == null || !(writer instanceof DatanodeDescriptor)) && !newBlock) {\n      writer = results.get(0).getDatanodeDescriptor();\n    }\n\n    // Keep a copy of original excludedNodes\n    final Set<Node> oldExcludedNodes = new HashSet<>(excludedNodes);\n\n    // choose storage types; use fallbacks for unavailable storages\n    final List<StorageType> requiredStorageTypes = storagePolicy\n        .chooseStorageTypes((short) totalReplicasExpected,\n            DatanodeStorageInfo.toStorageTypes(results),\n            unavailableStorages, newBlock);\n    final EnumMap<StorageType, Integer> storageTypes =\n        getRequiredStorageTypes(requiredStorageTypes);\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"storageTypes=\" + storageTypes);\n    }\n\n    try {\n      if ((numOfReplicas = requiredStorageTypes.size()) == 0) {\n        throw new NotEnoughReplicasException(\n            \"All required storage types are unavailable: \"\n            + \" unavailableStorages=\" + unavailableStorages\n            + \", storagePolicy=\" + storagePolicy);\n      }\n      writer = chooseTargetInOrder(numOfReplicas, writer, excludedNodes, blocksize,\n          maxNodesPerRack, results, avoidStaleNodes, newBlock, storageTypes);\n    } catch (NotEnoughReplicasException e) {\n      final String message = \"Failed to place enough replicas, still in need of \"\n          + (totalReplicasExpected - results.size()) + \" to reach \"\n          + totalReplicasExpected\n          + \" (unavailableStorages=\" + unavailableStorages\n          + \", storagePolicy=\" + storagePolicy\n          + \", newBlock=\" + newBlock + \")\";\n\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(message, e);\n      } else {\n        LOG.warn(message + \" \" + e.getMessage());\n      }\n\n      if (avoidStaleNodes) {\n        // Retry chooseTarget again, this time not avoiding stale nodes.\n\n        // excludedNodes contains the initial excludedNodes and nodes that were\n        // not chosen because they were stale, decommissioned, etc.\n        // We need to additionally exclude the nodes that were added to the \n        // result list in the successful calls to choose*() above.\n        for (DatanodeStorageInfo resultStorage : results) {\n          addToExcludedNodes(resultStorage.getDatanodeDescriptor(), oldExcludedNodes);\n        }\n        // Set numOfReplicas, since it can get out of sync with the result list\n        // if the NotEnoughReplicasException was thrown in chooseRandom().\n        numOfReplicas = totalReplicasExpected - results.size();\n        return chooseTarget(numOfReplicas, writer, oldExcludedNodes, blocksize,\n            maxNodesPerRack, results, false, storagePolicy, unavailableStorages,\n            newBlock);\n      }\n\n      boolean retry = false;\n      // simply add all the remaining types into unavailableStorages and give\n      // another try. No best effort is guaranteed here.\n      for (StorageType type : storageTypes.keySet()) {\n        if (!unavailableStorages.contains(type)) {\n          unavailableStorages.add(type);\n          retry = true;\n        }\n      }\n      if (retry) {\n        for (DatanodeStorageInfo resultStorage : results) {\n          addToExcludedNodes(resultStorage.getDatanodeDescriptor(),\n              oldExcludedNodes);\n        }\n        numOfReplicas = totalReplicasExpected - results.size();\n        return chooseTarget(numOfReplicas, writer, oldExcludedNodes, blocksize,\n            maxNodesPerRack, results, false, storagePolicy, unavailableStorages,\n            newBlock);\n      }\n    }\n    return writer;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseFavouredNodes": "  protected void chooseFavouredNodes(String src, int numOfReplicas,\n      List<DatanodeDescriptor> favoredNodes,\n      Set<Node> favoriteAndExcludedNodes, long blocksize, int maxNodesPerRack,\n      List<DatanodeStorageInfo> results, boolean avoidStaleNodes,\n      EnumMap<StorageType, Integer> storageTypes)\n      throws NotEnoughReplicasException {\n    for (int i = 0; i < favoredNodes.size() && results.size() < numOfReplicas;\n        i++) {\n      DatanodeDescriptor favoredNode = favoredNodes.get(i);\n      // Choose a single node which is local to favoredNode.\n      // 'results' is updated within chooseLocalNode\n      final DatanodeStorageInfo target =\n          chooseLocalStorage(favoredNode, favoriteAndExcludedNodes, blocksize,\n            maxNodesPerRack, results, avoidStaleNodes, storageTypes, false);\n      if (target == null) {\n        LOG.warn(\"Could not find a target for file \" + src\n            + \" with favored node \" + favoredNode);\n        continue;\n      }\n      favoriteAndExcludedNodes.add(target.getDatanodeDescriptor());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.getPipeline": "  private DatanodeStorageInfo[] getPipeline(Node writer,\n      DatanodeStorageInfo[] storages) {\n    if (storages.length == 0) {\n      return storages;\n    }\n\n    synchronized(clusterMap) {\n      int index=0;\n      if (writer == null || !clusterMap.contains(writer)) {\n        writer = storages[0].getDatanodeDescriptor();\n      }\n      for(; index < storages.length; index++) {\n        DatanodeStorageInfo shortestStorage = storages[index];\n        int shortestDistance = clusterMap.getDistance(writer,\n            shortestStorage.getDatanodeDescriptor());\n        int shortestIndex = index;\n        for(int i = index + 1; i < storages.length; i++) {\n          int currentDistance = clusterMap.getDistance(writer,\n              storages[i].getDatanodeDescriptor());\n          if (shortestDistance>currentDistance) {\n            shortestDistance = currentDistance;\n            shortestStorage = storages[i];\n            shortestIndex = i;\n          }\n        }\n        //switch position index & shortestIndex\n        if (index != shortestIndex) {\n          storages[shortestIndex] = storages[index];\n          storages[index] = shortestStorage;\n        }\n        writer = shortestStorage.getDatanodeDescriptor();\n      }\n    }\n    return storages;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.getMaxNodesPerRack": "  protected int[] getMaxNodesPerRack(int numOfChosen, int numOfReplicas) {\n    int clusterSize = clusterMap.getNumOfLeaves();\n    int totalNumOfReplicas = numOfChosen + numOfReplicas;\n    if (totalNumOfReplicas > clusterSize) {\n      numOfReplicas -= (totalNumOfReplicas-clusterSize);\n      totalNumOfReplicas = clusterSize;\n    }\n    // No calculation needed when there is only one rack or picking one node.\n    int numOfRacks = clusterMap.getNumOfRacks();\n    if (numOfRacks == 1 || totalNumOfReplicas <= 1) {\n      return new int[] {numOfReplicas, totalNumOfReplicas};\n    }\n\n    int maxNodesPerRack = (totalNumOfReplicas-1)/numOfRacks + 2;\n    // At this point, there are more than one racks and more than one replicas\n    // to store. Avoid all replicas being in the same rack.\n    //\n    // maxNodesPerRack has the following properties at this stage.\n    //   1) maxNodesPerRack >= 2\n    //   2) (maxNodesPerRack-1) * numOfRacks > totalNumOfReplicas\n    //          when numOfRacks > 1\n    //\n    // Thus, the following adjustment will still result in a value that forces\n    // multi-rack allocation and gives enough number of total nodes.\n    if (maxNodesPerRack == totalNumOfReplicas) {\n      maxNodesPerRack--;\n    }\n    return new int[] {numOfReplicas, maxNodesPerRack};\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.getRequiredStorageTypes": "  private EnumMap<StorageType, Integer> getRequiredStorageTypes(\n      List<StorageType> types) {\n    EnumMap<StorageType, Integer> map = new EnumMap<>(StorageType.class);\n    for (StorageType type : types) {\n      if (!map.containsKey(type)) {\n        map.put(type, 1);\n      } else {\n        int num = map.get(type);\n        map.put(type, num + 1);\n      }\n    }\n    return map;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock": "  public DatanodeStorageInfo[] chooseTarget4NewBlock(final String src,\n      final int numOfReplicas, final Node client,\n      final Set<Node> excludedNodes,\n      final long blocksize,\n      final List<String> favoredNodes,\n      final byte storagePolicyID,\n      final boolean isStriped,\n      final EnumSet<AddBlockFlag> flags) throws IOException {\n    List<DatanodeDescriptor> favoredDatanodeDescriptors = \n        getDatanodeDescriptors(favoredNodes);\n    final BlockStoragePolicy storagePolicy = storagePolicySuite.getPolicy(storagePolicyID);\n    final BlockPlacementPolicy blockplacement = placementPolicies.getPolicy(isStriped);\n    final DatanodeStorageInfo[] targets = blockplacement.chooseTarget(src,\n        numOfReplicas, client, excludedNodes, blocksize, \n        favoredDatanodeDescriptors, storagePolicy, flags);\n    if (targets.length < minReplication) {\n      throw new IOException(\"File \" + src + \" could only be replicated to \"\n          + targets.length + \" nodes instead of minReplication (=\"\n          + minReplication + \").  There are \"\n          + getDatanodeManager().getNetworkTopology().getNumOfLeaves()\n          + \" datanode(s) running and \"\n          + (excludedNodes == null? \"no\": excludedNodes.size())\n          + \" node(s) are excluded in this operation.\");\n    }\n    return targets;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.getDatanodeManager": "  public DatanodeManager getDatanodeManager() {\n    return datanodeManager;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.getDatanodeDescriptors": "  List<DatanodeDescriptor> getDatanodeDescriptors(List<String> nodes) {\n    List<DatanodeDescriptor> datanodeDescriptors = null;\n    if (nodes != null) {\n      datanodeDescriptors = new ArrayList<DatanodeDescriptor>(nodes.size());\n      for (int i = 0; i < nodes.size(); i++) {\n        DatanodeDescriptor node = datanodeManager.getDatanodeDescriptor(nodes.get(i));\n        if (node != null) {\n          datanodeDescriptors.add(node);\n        }\n      }\n    }\n    return datanodeDescriptors;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock": "  LocatedBlock getAdditionalBlock(\n      String src, long fileId, String clientName, ExtendedBlock previous,\n      DatanodeInfo[] excludedNodes, String[] favoredNodes,\n      EnumSet<AddBlockFlag> flags) throws IOException {\n    NameNode.stateChangeLog.debug(\"BLOCK* getAdditionalBlock: {}  inodeId {}\" +\n        \" for {}\", src, fileId, clientName);\n\n    LocatedBlock[] onRetryBlock = new LocatedBlock[1];\n    FSDirWriteFileOp.ValidateAddBlockResult r;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.READ);\n    readLock();\n    try {\n      checkOperation(OperationCategory.READ);\n      r = FSDirWriteFileOp.validateAddBlock(this, pc, src, fileId, clientName,\n                                            previous, onRetryBlock);\n    } finally {\n      readUnlock();\n    }\n\n    if (r == null) {\n      assert onRetryBlock[0] != null : \"Retry block is null\";\n      // This is a retry. Just return the last block.\n      return onRetryBlock[0];\n    }\n\n    DatanodeStorageInfo[] targets = FSDirWriteFileOp.chooseTargetForNewBlock(\n        blockManager, src, excludedNodes, favoredNodes, flags, r);\n\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    LocatedBlock lb;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      lb = FSDirWriteFileOp.storeAllocatedBlock(\n          this, src, fileId, clientName, previous, targets);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    return lb;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEditLog": "  public FSEditLog getEditLog() {\n    return getFSImage().getEditLog();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock": "  public void writeUnlock() {\n    final boolean needReport = fsLock.getWriteHoldCount() == 1 &&\n        fsLock.isWriteLockedByCurrentThread();\n    final long writeLockInterval = monotonicNow() - writeLockHeldTimeStamp;\n\n    this.fsLock.writeLock().unlock();\n\n    if (needReport && writeLockInterval >= WRITELOCK_REPORTING_THRESHOLD) {\n      LOG.info(\"FSNamesystem write lock held for \" + writeLockInterval +\n          \" ms via\\n\" + StringUtils.getStackTrace(Thread.currentThread()));\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock": "  public void writeLock() {\n    this.fsLock.writeLock().lock();\n    if (fsLock.getWriteHoldCount() == 1) {\n      writeLockHeldTimeStamp = monotonicNow();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.readUnlock": "  public void readUnlock() {\n    this.fsLock.readLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation": "  public void checkOperation(OperationCategory op) throws StandbyException {\n    if (haContext != null) {\n      // null in some unit tests\n      haContext.checkOperation(op);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker": "  FSPermissionChecker getPermissionChecker()\n      throws AccessControlException {\n    return dir.getPermissionChecker();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.readLock": "  public void readLock() {\n    this.fsLock.readLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock": "  public LocatedBlock addBlock(String src, String clientName,\n      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n      String[] favoredNodes, EnumSet<AddBlockFlag> addBlockFlags)\n      throws IOException {\n    checkNNStartup();\n    LocatedBlock locatedBlock = namesystem.getAdditionalBlock(src, fileId,\n        clientName, previous, excludedNodes, favoredNodes, addBlockFlags);\n    if (locatedBlock != null) {\n      metrics.incrAddBlockOps();\n    }\n    return locatedBlock;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkNNStartup": "  private void checkNNStartup() throws IOException {\n    if (!this.nn.isStarted()) {\n      String message = NameNode.composeNotStartedMessage(this.nn.getRole());\n      throw new RetriableException(message);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock": "  public AddBlockResponseProto addBlock(RpcController controller,\n      AddBlockRequestProto req) throws ServiceException {\n    \n    try {\n      List<DatanodeInfoProto> excl = req.getExcludeNodesList();\n      List<String> favor = req.getFavoredNodesList();\n      EnumSet<AddBlockFlag> flags =\n          PBHelperClient.convertAddBlockFlags(req.getFlagsList());\n      LocatedBlock result = server.addBlock(\n          req.getSrc(),\n          req.getClientName(),\n          req.hasPrevious() ? PBHelperClient.convert(req.getPrevious()) : null,\n          (excl == null || excl.size() == 0) ? null : PBHelperClient.convert(excl\n              .toArray(new DatanodeInfoProto[excl.size()])), req.getFileId(),\n          (favor == null || favor.size() == 0) ? null : favor\n              .toArray(new String[favor.size()]),\n          flags);\n      return AddBlockResponseProto.newBuilder()\n          .setBlock(PBHelperClient.convertLocatedBlock(result)).build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcRequestWrapper request = (RpcRequestWrapper) writableRequest;\n        RequestHeaderProto rpcRequest = request.requestHeader;\n        String methodName = rpcRequest.getMethodName();\n        \n        \n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = prototype.newBuilderForType()\n            .mergeFrom(request.theRequestRead).build();\n        \n        Message result;\n        long startTime = Time.now();\n        int qTime = (int) (startTime - receiveTime);\n        Exception exception = null;\n        try {\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n        } catch (ServiceException e) {\n          exception = (Exception) e.getCause();\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          exception = e;\n          throw e;\n        } finally {\n          int processingTime = (int) (Time.now() - startTime);\n          if (LOG.isDebugEnabled()) {\n            String msg = \"Served: \" + methodName + \" queueTime= \" + qTime +\n                \" procesingTime= \" + processingTime;\n            if (exception != null) {\n              msg += \" exception= \" + exception.getClass().getSimpleName();\n            }\n            LOG.debug(msg);\n          }\n          String detailedMetricsName = (exception == null) ?\n              methodName :\n              exception.getClass().getSimpleName();\n          server.updateMetrics(detailedMetricsName, qTime, processingTime);\n        }\n        return new RpcResponseWrapper(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getProtocolImpl": "      private static ProtoClassProtoImpl getProtocolImpl(RPC.Server server,\n          String protoName, long clientVersion) throws RpcServerException {\n        ProtoNameVer pv = new ProtoNameVer(protoName, clientVersion);\n        ProtoClassProtoImpl impl = \n            server.getProtocolImplMap(RPC.RpcKind.RPC_PROTOCOL_BUFFER).get(pv);\n        if (impl == null) { // no match for Protocol AND Version\n          VerProtocolImpl highest = \n              server.getHighestSupportedProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER, \n                  protoName);\n          if (highest == null) {\n            throw new RpcNoSuchProtocolException(\n                \"Unknown protocol: \" + protoName);\n          }\n          // protocol supported but not the version that client wants\n          throw new RPC.VersionMismatch(protoName, clientVersion,\n              highest.version);\n        }\n        return impl;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.call": "    public Writable call(RPC.RpcKind rpcKind, String protocol,\n        Writable rpcRequest, long receiveTime) throws Exception {\n      return getRpcInvoker(rpcKind).call(this, protocol, rpcRequest,\n          receiveTime);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.toString": "    public String toString() {\n      return getHostAddress() + \":\" + remotePort; \n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private static void setupResponse(ByteArrayOutputStream responseBuf,\n                             Call call, RpcStatusProto status, RpcErrorCodeProto erCode,\n                             Writable rv, String errorClass, String error) \n  throws IOException {\n    responseBuf.reset();\n    DataOutputStream out = new DataOutputStream(responseBuf);\n    RpcResponseHeaderProto.Builder headerBuilder =  \n        RpcResponseHeaderProto.newBuilder();\n    headerBuilder.setClientId(ByteString.copyFrom(call.clientId));\n    headerBuilder.setCallId(call.callId);\n    headerBuilder.setRetryCount(call.retryCount);\n    headerBuilder.setStatus(status);\n    headerBuilder.setServerIpcVersionNum(CURRENT_VERSION);\n\n    if (status == RpcStatusProto.SUCCESS) {\n      RpcResponseHeaderProto header = headerBuilder.build();\n      final int headerLen = header.getSerializedSize();\n      int fullLength  = CodedOutputStream.computeRawVarint32Size(headerLen) +\n          headerLen;\n      try {\n        if (rv instanceof ProtobufRpcEngine.RpcWrapper) {\n          ProtobufRpcEngine.RpcWrapper resWrapper = \n              (ProtobufRpcEngine.RpcWrapper) rv;\n          fullLength += resWrapper.getLength();\n          out.writeInt(fullLength);\n          header.writeDelimitedTo(out);\n          rv.write(out);\n        } else { // Have to serialize to buffer to get len\n          final DataOutputBuffer buf = new DataOutputBuffer();\n          rv.write(buf);\n          byte[] data = buf.getData();\n          fullLength += buf.getLength();\n          out.writeInt(fullLength);\n          header.writeDelimitedTo(out);\n          out.write(data, 0, buf.getLength());\n        }\n      } catch (Throwable t) {\n        LOG.warn(\"Error serializing call response for call \" + call, t);\n        // Call back to same function - this is OK since the\n        // buffer is reset at the top, and since status is changed\n        // to ERROR it won't infinite loop.\n        setupResponse(responseBuf, call, RpcStatusProto.ERROR,\n            RpcErrorCodeProto.ERROR_SERIALIZING_RESPONSE,\n            null, t.getClass().getName(),\n            StringUtils.stringifyException(t));\n        return;\n      }\n    } else { // Rpc Failure\n      headerBuilder.setExceptionClassName(errorClass);\n      headerBuilder.setErrorMsg(error);\n      headerBuilder.setErrorDetail(erCode);\n      RpcResponseHeaderProto header = headerBuilder.build();\n      int headerLen = header.getSerializedSize();\n      final int fullLength  = \n          CodedOutputStream.computeRawVarint32Size(headerLen) + headerLen;\n      out.writeInt(fullLength);\n      header.writeDelimitedTo(out);\n    }\n    call.setResponse(ByteBuffer.wrap(responseBuf.toByteArray()));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.sendResponse": "    private void sendResponse(Call call) throws IOException {\n      responder.doRespond(call);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.remove": "    private boolean remove(Connection connection) {\n      boolean removed = connections.remove(connection);\n      if (removed) {\n        count.getAndDecrement();\n      }\n      return removed;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeAll": "    void closeAll() {\n      // use a copy of the connections to be absolutely sure the concurrent\n      // iterator doesn't miss a connection\n      for (Connection connection : toArray()) {\n        close(connection);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getRpcErrorCodeProto": "    public RpcErrorCodeProto getRpcErrorCodeProto() {\n      return errCode;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    boolean close(Connection connection) {\n      boolean exists = remove(connection);\n      if (exists) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(Thread.currentThread().getName() +\n              \": disconnecting client \" + connection +\n              \". Number of active connections: \"+ size());\n        }\n        // only close if actually removed to avoid double-closing due\n        // to possible races\n        connection.close();\n      }\n      return exists;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(RPC.RpcKind rpcKind, String protocol,\n      Writable param, long receiveTime) throws Exception;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param protocolName - the protocol\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  private void authorize(UserGroupInformation user, String protocolName,\n      InetAddress addr) throws AuthorizationException {\n    if (authorize) {\n      if (protocolName == null) {\n        throw new AuthorizationException(\"Null protocol not authorized\");\n      }\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(protocolName, getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         protocolName);\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isValid() && key.isWritable()) {\n                  doAsyncWrite(key);\n              }\n            } catch (IOException e) {\n              LOG.info(Thread.currentThread().getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = Time.now();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<Call> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<Call>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              Call call = (Call)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n          \n          for(Call call : calls) {\n            doPurge(call, now);\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.size": "    int size() {\n      return count.get();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeIdle": "    synchronized void closeIdle(boolean scanAll) {\n      long minLastContact = Time.now() - maxIdleTime;\n      // concurrent iterator might miss new connections added\n      // during the iteration, but that's ok because they won't\n      // be idle yet anyway and will be caught on next scan\n      int closed = 0;\n      for (Connection connection : connections) {\n        // stop if connections dropped below threshold unless scanning all\n        if (!scanAll && size() < idleScanThreshold) {\n          break;\n        }\n        // stop if not scanning all and max connections are closed\n        if (connection.isIdle() &&\n            connection.getLastContact() < minLastContact &&\n            close(connection) &&\n            !scanAll && (++closed == maxIdleToClose)) {\n          break;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.startIdleScan": "    void startIdleScan() {\n      scheduleIdleScanTask();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.logException": "  void logException(Log logger, Throwable e, Call call) {\n    if (exceptionsHandler.isSuppressedLog(e.getClass())) {\n      return; // Log nothing.\n    }\n\n    final String logMsg = Thread.currentThread().getName() + \", call \" + call;\n    if (exceptionsHandler.isTerseLog(e.getClass())) {\n      // Don't log the whole stack trace. Way too noisy!\n      logger.info(logMsg + \": \" + e);\n    } else if (e instanceof RuntimeException || e instanceof Error) {\n      // These exception types indicate something is probably wrong\n      // on the server side, as opposed to just a normal exceptional\n      // result.\n      logger.warn(logMsg, e);\n    } else {\n      logger.info(logMsg, e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.stopIdleScan": "    void stopIdleScan() {\n      idleScanTimer.cancel();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws InterruptedException, IOException,  OutOfMemoryError {\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        channel.socket().setKeepAlive(true);\n        \n        Reader reader = getReader();\n        Connection c = connectionManager.register(channel);\n        // If the connectionManager can't take it, close the connection.\n        if (c == null) {\n          if (channel.isOpen()) {\n            IOUtils.cleanup(null, channel);\n          }\n          continue;\n        }\n        key.attach(c);  // so closeCurrentConnection can get the object\n        reader.addConnection(c);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.scheduleIdleScanTask": "    private void scheduleIdleScanTask() {\n      if (!running) {\n        return;\n      }\n      TimerTask idleScanTask = new TimerTask(){\n        @Override\n        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }\n      };\n      idleScanTimer.schedule(idleScanTask, idleScanInterval);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause == null) {\n        throw new RuntimeException(\"PrivilegedActionException with no \" +\n                \"underlying cause. UGI [\" + this + \"]\" +\": \" + pae, pae);\n      } else if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.equals": "  public boolean equals(Object obj) {\n    // Sufficient to use super equality as datanodes are uniquely identified\n    // by DatanodeID\n    return (this == obj) || super.equals(obj);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.toStorageTypes": "  static Iterable<StorageType> toStorageTypes(\n      final Iterable<DatanodeStorageInfo> infos) {\n    return new Iterable<StorageType>() {\n        @Override\n        public Iterator<StorageType> iterator() {\n          return new Iterator<StorageType>() {\n            final Iterator<DatanodeStorageInfo> i = infos.iterator();\n            @Override\n            public boolean hasNext() {return i.hasNext();}\n            @Override\n            public StorageType next() {return i.next().getStorageType();}\n            @Override\n            public void remove() {\n              throw new UnsupportedOperationException();\n            }\n          };\n        }\n      };\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.next": "            public StorageType next() {return i.next().getStorageType();}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.iterator": "        public Iterator<StorageType> iterator() {\n          return new Iterator<StorageType>() {\n            final Iterator<DatanodeStorageInfo> i = infos.iterator();\n            @Override\n            public boolean hasNext() {return i.hasNext();}\n            @Override\n            public StorageType next() {return i.next().getStorageType();}\n            @Override\n            public void remove() {\n              throw new UnsupportedOperationException();\n            }\n          };\n        }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.getStorageType": "  public StorageType getStorageType() {\n    return storageType;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.hasNext": "            public boolean hasNext() {return i.hasNext();}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.getDatanodeDescriptor": "  public DatanodeDescriptor getDatanodeDescriptor() {\n    return dn;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.chooseTarget": "  DatanodeStorageInfo[] chooseTarget(String src,\n      int numOfReplicas, Node writer,\n      Set<Node> excludedNodes,\n      long blocksize,\n      List<DatanodeDescriptor> favoredNodes,\n      BlockStoragePolicy storagePolicy,\n      EnumSet<AddBlockFlag> flags) {\n    // This class does not provide the functionality of placing\n    // a block in favored datanodes. The implementations of this class\n    // are expected to provide this functionality\n\n    return chooseTarget(src, numOfReplicas, writer, \n        new ArrayList<DatanodeStorageInfo>(numOfReplicas), false,\n        excludedNodes, blocksize, storagePolicy, flags);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.storeAllocatedBlock": "  static LocatedBlock storeAllocatedBlock(FSNamesystem fsn, String src,\n      long fileId, String clientName, ExtendedBlock previous,\n      DatanodeStorageInfo[] targets) throws IOException {\n    long offset;\n    // Run the full analysis again, since things could have changed\n    // while chooseTarget() was executing.\n    LocatedBlock[] onRetryBlock = new LocatedBlock[1];\n    FileState fileState = analyzeFileState(fsn, src, fileId, clientName,\n                                           previous, onRetryBlock);\n    final INodeFile pendingFile = fileState.inode;\n    src = fileState.path;\n\n    if (onRetryBlock[0] != null) {\n      if (onRetryBlock[0].getLocations().length > 0) {\n        // This is a retry. Just return the last block if having locations.\n        return onRetryBlock[0];\n      } else {\n        // add new chosen targets to already allocated block and return\n        BlockInfo lastBlockInFile = pendingFile.getLastBlock();\n        lastBlockInFile.getUnderConstructionFeature().setExpectedLocations(\n            lastBlockInFile, targets, pendingFile.isStriped());\n        offset = pendingFile.computeFileSize();\n        return makeLocatedBlock(fsn, lastBlockInFile, targets, offset);\n      }\n    }\n\n    // commit the last block and complete it if it has minimum replicas\n    fsn.commitOrCompleteLastBlock(pendingFile, fileState.iip,\n                                  ExtendedBlock.getLocalBlock(previous));\n\n    // allocate new block, record block locations in INode.\n    final boolean isStriped = pendingFile.isStriped();\n    // allocate new block, record block locations in INode.\n    Block newBlock = fsn.createNewBlock(isStriped);\n    INodesInPath inodesInPath = INodesInPath.fromINode(pendingFile);\n    saveAllocatedBlock(fsn, src, inodesInPath, newBlock, targets, isStriped);\n\n    persistNewBlock(fsn, src, pendingFile);\n    offset = pendingFile.computeFileSize();\n\n    // Return located block\n    return makeLocatedBlock(fsn, fsn.getStoredBlock(newBlock), targets, offset);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.makeLocatedBlock": "  static LocatedBlock makeLocatedBlock(FSNamesystem fsn, BlockInfo blk,\n      DatanodeStorageInfo[] locs, long offset) throws IOException {\n    LocatedBlock lBlk = BlockManager.newLocatedBlock(\n        fsn.getExtendedBlock(new Block(blk)), blk, locs, offset);\n    fsn.getBlockManager().setBlockToken(lBlk,\n        BlockTokenIdentifier.AccessMode.WRITE);\n    return lBlk;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.persistNewBlock": "  private static void persistNewBlock(\n      FSNamesystem fsn, String path, INodeFile file) {\n    Preconditions.checkArgument(file.isUnderConstruction());\n    fsn.getEditLog().logAddBlock(path, file);\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"persistNewBlock: \"\n              + path + \" with new block \" + file.getLastBlock().toString()\n              + \", current total block count is \" + file.getBlocks().length);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.saveAllocatedBlock": "  private static void saveAllocatedBlock(FSNamesystem fsn, String src,\n      INodesInPath inodesInPath, Block newBlock, DatanodeStorageInfo[] targets,\n      boolean isStriped) throws IOException {\n    assert fsn.hasWriteLock();\n    BlockInfo b = addBlock(fsn.dir, src, inodesInPath, newBlock, targets,\n        isStriped);\n    logAllocatedBlock(src, b);\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState": "  private static FileState analyzeFileState(\n      FSNamesystem fsn, String src, long fileId, String clientName,\n      ExtendedBlock previous, LocatedBlock[] onRetryBlock)\n      throws IOException {\n    assert fsn.hasReadLock();\n\n    checkBlock(fsn, previous);\n    onRetryBlock[0] = null;\n    fsn.checkNameNodeSafeMode(\"Cannot add block to \" + src);\n\n    // have we exceeded the configured limit of fs objects.\n    fsn.checkFsObjectLimit();\n\n    Block previousBlock = ExtendedBlock.getLocalBlock(previous);\n    final INode inode;\n    final INodesInPath iip;\n    if (fileId == HdfsConstants.GRANDFATHER_INODE_ID) {\n      // Older clients may not have given us an inode ID to work with.\n      // In this case, we have to try to resolve the path and hope it\n      // hasn't changed or been deleted since the file was opened for write.\n      iip = fsn.dir.getINodesInPath4Write(src);\n      inode = iip.getLastINode();\n    } else {\n      // Newer clients pass the inode ID, so we can just get the inode\n      // directly.\n      inode = fsn.dir.getInode(fileId);\n      iip = INodesInPath.fromINode(inode);\n      if (inode != null) {\n        src = iip.getPath();\n      }\n    }\n    final INodeFile file = fsn.checkLease(src, clientName, inode, fileId);\n    BlockInfo lastBlockInFile = file.getLastBlock();\n    if (!Block.matchingIdAndGenStamp(previousBlock, lastBlockInFile)) {\n      // The block that the client claims is the current last block\n      // doesn't match up with what we think is the last block. There are\n      // four possibilities:\n      // 1) This is the first block allocation of an append() pipeline\n      //    which started appending exactly at or exceeding the block boundary.\n      //    In this case, the client isn't passed the previous block,\n      //    so it makes the allocateBlock() call with previous=null.\n      //    We can distinguish this since the last block of the file\n      //    will be exactly a full block.\n      // 2) This is a retry from a client that missed the response of a\n      //    prior getAdditionalBlock() call, perhaps because of a network\n      //    timeout, or because of an HA failover. In that case, we know\n      //    by the fact that the client is re-issuing the RPC that it\n      //    never began to write to the old block. Hence it is safe to\n      //    to return the existing block.\n      // 3) This is an entirely bogus request/bug -- we should error out\n      //    rather than potentially appending a new block with an empty\n      //    one in the middle, etc\n      // 4) This is a retry from a client that timed out while\n      //    the prior getAdditionalBlock() is still being processed,\n      //    currently working on chooseTarget().\n      //    There are no means to distinguish between the first and\n      //    the second attempts in Part I, because the first one hasn't\n      //    changed the namesystem state yet.\n      //    We run this analysis again in Part II where case 4 is impossible.\n\n      BlockInfo penultimateBlock = file.getPenultimateBlock();\n      if (previous == null &&\n          lastBlockInFile != null &&\n          lastBlockInFile.getNumBytes() >= file.getPreferredBlockSize() &&\n          lastBlockInFile.isComplete()) {\n        // Case 1\n        if (NameNode.stateChangeLog.isDebugEnabled()) {\n           NameNode.stateChangeLog.debug(\n               \"BLOCK* NameSystem.allocateBlock: handling block allocation\" +\n               \" writing to a file with a complete previous block: src=\" +\n               src + \" lastBlock=\" + lastBlockInFile);\n        }\n      } else if (Block.matchingIdAndGenStamp(penultimateBlock, previousBlock)) {\n        if (lastBlockInFile.getNumBytes() != 0) {\n          throw new IOException(\n              \"Request looked like a retry to allocate block \" +\n              lastBlockInFile + \" but it already contains \" +\n              lastBlockInFile.getNumBytes() + \" bytes\");\n        }\n\n        // Case 2\n        // Return the last block.\n        NameNode.stateChangeLog.info(\"BLOCK* allocateBlock: caught retry for \" +\n            \"allocation of a new block in \" + src + \". Returning previously\" +\n            \" allocated block \" + lastBlockInFile);\n        long offset = file.computeFileSize();\n        BlockUnderConstructionFeature uc =\n            lastBlockInFile.getUnderConstructionFeature();\n        onRetryBlock[0] = makeLocatedBlock(fsn, lastBlockInFile,\n            uc.getExpectedStorageLocations(), offset);\n        return new FileState(file, src, iip);\n      } else {\n        // Case 3\n        throw new IOException(\"Cannot allocate block in \" + src + \": \" +\n            \"passed 'previous' block \" + previous + \" does not match actual \" +\n            \"last block in file \" + lastBlockInFile);\n      }\n    }\n    return new FileState(file, src, iip);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock": "  static ValidateAddBlockResult validateAddBlock(\n      FSNamesystem fsn, FSPermissionChecker pc,\n      String src, long fileId, String clientName,\n      ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n    final long blockSize;\n    final short numTargets;\n    final byte storagePolicyID;\n    String clientMachine;\n    final boolean isStriped;\n\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    src = fsn.dir.resolvePath(pc, src, pathComponents);\n    FileState fileState = analyzeFileState(fsn, src, fileId, clientName,\n                                           previous, onRetryBlock);\n    if (onRetryBlock[0] != null && onRetryBlock[0].getLocations().length > 0) {\n      // This is a retry. No need to generate new locations.\n      // Use the last block if it has locations.\n      return null;\n    }\n\n    final INodeFile pendingFile = fileState.inode;\n    if (!fsn.checkFileProgress(src, pendingFile, false)) {\n      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n    }\n    if (pendingFile.getBlocks().length >= fsn.maxBlocksPerFile) {\n      throw new IOException(\"File has reached the limit on maximum number of\"\n          + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n          + \"): \" + pendingFile.getBlocks().length + \" >= \"\n          + fsn.maxBlocksPerFile);\n    }\n    blockSize = pendingFile.getPreferredBlockSize();\n    clientMachine = pendingFile.getFileUnderConstructionFeature()\n        .getClientMachine();\n    isStriped = pendingFile.isStriped();\n    ErasureCodingPolicy ecPolicy = null;\n    if (isStriped) {\n      ecPolicy = FSDirErasureCodingOp.getErasureCodingPolicy(fsn, src);\n      numTargets = (short) (ecPolicy.getSchema().getNumDataUnits()\n          + ecPolicy.getSchema().getNumParityUnits());\n    } else {\n      numTargets = pendingFile.getFileReplication();\n    }\n    storagePolicyID = pendingFile.getStoragePolicyID();\n    return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n                                      clientMachine, isStriped);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock": "  static DatanodeStorageInfo[] chooseTargetForNewBlock(\n      BlockManager bm, String src, DatanodeInfo[] excludedNodes,\n      String[] favoredNodes, EnumSet<AddBlockFlag> flags,\n      ValidateAddBlockResult r) throws IOException {\n    Node clientNode = bm.getDatanodeManager()\n        .getDatanodeByHost(r.clientMachine);\n    if (clientNode == null) {\n      clientNode = getClientNode(bm, r.clientMachine);\n    }\n\n    Set<Node> excludedNodesSet = null;\n    if (excludedNodes != null) {\n      excludedNodesSet = new HashSet<>(excludedNodes.length);\n      Collections.addAll(excludedNodesSet, excludedNodes);\n    }\n    List<String> favoredNodesList = (favoredNodes == null) ? null\n        : Arrays.asList(favoredNodes);\n    // choose targets for the new block to be allocated.\n    return bm.chooseTarget4NewBlock(src, r.numTargets, clientNode,\n                                    excludedNodesSet, r.blockSize,\n                                    favoredNodesList, r.storagePolicyID,\n                                    r.isStriped, flags);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.getClientNode": "  static Node getClientNode(BlockManager bm, String clientMachine) {\n    List<String> hosts = new ArrayList<>(1);\n    hosts.add(clientMachine);\n    List<String> rName = bm.getDatanodeManager()\n        .resolveNetworkLocation(hosts);\n    Node clientNode = null;\n    if (rName != null) {\n      // Able to resolve clientMachine mapping.\n      // Create a temp node to findout the rack local nodes\n      clientNode = new NodeBase(rName.get(0) + NodeBase.PATH_SEPARATOR_STR\n          + clientMachine);\n    }\n    return clientNode;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertAddBlockFlags": "  public static List<AddBlockFlagProto> convertAddBlockFlags(\n      EnumSet<AddBlockFlag> flags) {\n    List<AddBlockFlagProto> ret = new ArrayList<>();\n    for (AddBlockFlag flag : flags) {\n      AddBlockFlagProto abfp = AddBlockFlagProto.valueOf(flag.getMode());\n      if (abfp != null) {\n        ret.add(abfp);\n      }\n    }\n    return ret;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convert": "  static List<DatanodeInfosProto> convert(DatanodeInfo[][] targets) {\n    DatanodeInfosProto[] ret = new DatanodeInfosProto[targets.length];\n    for (int i = 0; i < targets.length; i++) {\n      ret[i] = DatanodeInfosProto.newBuilder()\n          .addAllDatanodes(convert(targets[i])).build();\n    }\n    return Arrays.asList(ret);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.addStorageTypes": "  private static void addStorageTypes(\n      HdfsProtos.StorageTypeQuotaInfosProto typeQuotaInfos,\n      QuotaUsage.Builder builder) {\n    for (HdfsProtos.StorageTypeQuotaInfoProto info :\n        typeQuotaInfos.getTypeQuotaInfoList()) {\n      StorageType type = convertStorageType(info.getType());\n      builder.typeConsumed(type, info.getConsumed());\n      builder.typeQuota(type, info.getQuota());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertAclEntry": "  public static List<AclEntry> convertAclEntry(List<AclEntryProto> aclSpec) {\n    ArrayList<AclEntry> r = Lists.newArrayListWithCapacity(aclSpec.size());\n    for (AclEntryProto e : aclSpec) {\n      AclEntry.Builder builder = new AclEntry.Builder();\n      builder.setType(convert(e.getType()));\n      builder.setScope(convert(e.getScope()));\n      builder.setPermission(convert(e.getPermissions()));\n      if (e.hasName()) {\n        builder.setName(e.getName());\n      }\n      r.add(builder.build());\n    }\n    return r;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertState": "  private static StorageState convertState(State state) {\n    switch(state) {\n    case READ_ONLY_SHARED:\n      return StorageState.READ_ONLY_SHARED;\n    case NORMAL:\n    default:\n      return StorageState.NORMAL;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertStorageType": "  public static StorageType convertStorageType(StorageTypeProto type) {\n    switch(type) {\n    case DISK:\n      return StorageType.DISK;\n    case SSD:\n      return StorageType.SSD;\n    case ARCHIVE:\n      return StorageType.ARCHIVE;\n    case RAM_DISK:\n      return StorageType.RAM_DISK;\n    default:\n      throw new IllegalStateException(\n          \"BUG: StorageTypeProto not found, type=\" + type);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertLocatedBlocks2": "  public static List<LocatedBlockProto> convertLocatedBlocks2(\n      List<LocatedBlock> lb) {\n    if (lb == null) return null;\n    final int len = lb.size();\n    List<LocatedBlockProto> result = new ArrayList<>(len);\n    for (LocatedBlock aLb : lb) {\n      result.add(convertLocatedBlock(aLb));\n    }\n    return result;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertLocatedBlock": "  public static LocatedBlock[] convertLocatedBlock(LocatedBlockProto[] lb) {\n    if (lb == null) return null;\n    return convertLocatedBlock(Arrays.asList(lb)).toArray(\n        new LocatedBlock[lb.length]);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertLocatedBlocks": "  public static List<LocatedBlock> convertLocatedBlocks(\n      List<LocatedBlockProto> lb) {\n    if (lb == null) return null;\n    final int len = lb.size();\n    List<LocatedBlock> result = new ArrayList<>(len);\n    for (LocatedBlockProto aLb : lb) {\n      result.add(convertLocatedBlockProto(aLb));\n    }\n    return result;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertXAttrs": "  public static List<XAttr> convertXAttrs(List<XAttrProto> xAttrSpec) {\n    ArrayList<XAttr> xAttrs = Lists.newArrayListWithCapacity(xAttrSpec.size());\n    for (XAttrProto a : xAttrSpec) {\n      XAttr.Builder builder = new XAttr.Builder();\n      builder.setNameSpace(convert(a.getNamespace()));\n      if (a.hasName()) {\n        builder.setName(a.getName());\n      }\n      if (a.hasValue()) {\n        builder.setValue(a.getValue().toByteArray());\n      }\n      xAttrs.add(builder.build());\n    }\n    return xAttrs;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.createTypeConvert": "  private static Event.CreateEvent.INodeType createTypeConvert(\n      InotifyProtos.INodeType type) {\n    switch (type) {\n    case I_TYPE_DIRECTORY:\n      return Event.CreateEvent.INodeType.DIRECTORY;\n    case I_TYPE_FILE:\n      return Event.CreateEvent.INodeType.FILE;\n    case I_TYPE_SYMLINK:\n      return Event.CreateEvent.INodeType.SYMLINK;\n    default:\n      return null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertAclEntryProto": "  public static List<AclEntryProto> convertAclEntryProto(\n      List<AclEntry> aclSpec) {\n    ArrayList<AclEntryProto> r = Lists.newArrayListWithCapacity(aclSpec.size());\n    for (AclEntry e : aclSpec) {\n      AclEntryProto.Builder builder = AclEntryProto.newBuilder();\n      builder.setType(convert(e.getType()));\n      builder.setScope(convert(e.getScope()));\n      builder.setPermissions(convert(e.getPermission()));\n      if (e.getName() != null) {\n        builder.setName(e.getName());\n      }\n      r.add(builder.build());\n    }\n    return r;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertRollingUpgradeStatus": "  public static RollingUpgradeStatusProto convertRollingUpgradeStatus(\n      RollingUpgradeStatus status) {\n    return RollingUpgradeStatusProto.newBuilder()\n        .setBlockPoolId(status.getBlockPoolId())\n        .setFinalized(status.isFinalized())\n        .build();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertLocatedBlockProto": "  public static LocatedBlock convertLocatedBlockProto(LocatedBlockProto proto) {\n    if (proto == null) return null;\n    List<DatanodeInfoProto> locs = proto.getLocsList();\n    DatanodeInfo[] targets = new DatanodeInfo[locs.size()];\n    for (int i = 0; i < locs.size(); i++) {\n      targets[i] = convert(locs.get(i));\n    }\n\n    final StorageType[] storageTypes = convertStorageTypes(\n        proto.getStorageTypesList(), locs.size());\n\n    final int storageIDsCount = proto.getStorageIDsCount();\n    final String[] storageIDs;\n    if (storageIDsCount == 0) {\n      storageIDs = null;\n    } else {\n      Preconditions.checkState(storageIDsCount == locs.size());\n      storageIDs = proto.getStorageIDsList()\n          .toArray(new String[storageIDsCount]);\n    }\n\n    byte[] indices = null;\n    if (proto.hasBlockIndices()) {\n      indices = proto.getBlockIndices().toByteArray();\n    }\n\n    // Set values from the isCached list, re-using references from loc\n    List<DatanodeInfo> cachedLocs = new ArrayList<>(locs.size());\n    List<Boolean> isCachedList = proto.getIsCachedList();\n    for (int i=0; i<isCachedList.size(); i++) {\n      if (isCachedList.get(i)) {\n        cachedLocs.add(targets[i]);\n      }\n    }\n\n    final LocatedBlock lb;\n    if (indices == null) {\n      lb = new LocatedBlock(PBHelperClient.convert(proto.getB()), targets,\n          storageIDs, storageTypes, proto.getOffset(), proto.getCorrupt(),\n          cachedLocs.toArray(new DatanodeInfo[cachedLocs.size()]));\n    } else {\n      lb = new LocatedStripedBlock(PBHelperClient.convert(proto.getB()),\n          targets, storageIDs, storageTypes, indices, proto.getOffset(),\n          proto.getCorrupt(),\n          cachedLocs.toArray(new DatanodeInfo[cachedLocs.size()]));\n      List<TokenProto> tokenProtos = proto.getBlockTokensList();\n      Token<BlockTokenIdentifier>[] blockTokens =\n          convertTokens(tokenProtos);\n      ((LocatedStripedBlock) lb).setBlockTokens(blockTokens);\n    }\n    lb.setBlockToken(convert(proto.getBlockToken()));\n\n    return lb;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertStorageTypes": "  public static StorageType[] convertStorageTypes(\n      List<StorageTypeProto> storageTypesList, int expectedSize) {\n    final StorageType[] storageTypes = new StorageType[expectedSize];\n    if (storageTypesList.size() != expectedSize) {\n      // missing storage types\n      Preconditions.checkState(storageTypesList.isEmpty());\n      Arrays.fill(storageTypes, StorageType.DEFAULT);\n    } else {\n      for (int i = 0; i < storageTypes.length; ++i) {\n        storageTypes[i] = convertStorageType(storageTypesList.get(i));\n      }\n    }\n    return storageTypes;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertErasureCodingPolicy": "  public static ErasureCodingPolicyProto convertErasureCodingPolicy(\n      ErasureCodingPolicy policy) {\n    ErasureCodingPolicyProto.Builder builder = ErasureCodingPolicyProto\n        .newBuilder()\n        .setName(policy.getName())\n        .setSchema(convertECSchema(policy.getSchema()))\n        .setCellSize(policy.getCellSize())\n        .setId(policy.getId());\n    return builder.build();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.castEnum": "  static <T extends Enum<T>, U extends Enum<U>> U castEnum(T from, U[] to) {\n    return to[from.ordinal()];\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.getByteString": "  public static ByteString getByteString(byte[] bytes) {\n    // return singleton to reduce object allocation\n    return (bytes.length == 0) ? ByteString.EMPTY : ByteString.copyFrom(bytes);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.metadataUpdateTypeConvert": "  private static Event.MetadataUpdateEvent.MetadataType metadataUpdateTypeConvert(\n      InotifyProtos.MetadataUpdateType type) {\n    switch (type) {\n    case META_TYPE_TIMES:\n      return Event.MetadataUpdateEvent.MetadataType.TIMES;\n    case META_TYPE_REPLICATION:\n      return Event.MetadataUpdateEvent.MetadataType.REPLICATION;\n    case META_TYPE_OWNER:\n      return Event.MetadataUpdateEvent.MetadataType.OWNER;\n    case META_TYPE_PERMS:\n      return Event.MetadataUpdateEvent.MetadataType.PERMS;\n    case META_TYPE_ACLS:\n      return Event.MetadataUpdateEvent.MetadataType.ACLS;\n    case META_TYPE_XATTRS:\n      return Event.MetadataUpdateEvent.MetadataType.XATTRS;\n    default:\n      return null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.getBuilder": "  private static HdfsProtos.StorageTypeQuotaInfosProto.Builder getBuilder(\n      QuotaUsage qu) {\n    HdfsProtos.StorageTypeQuotaInfosProto.Builder isb =\n            HdfsProtos.StorageTypeQuotaInfosProto.newBuilder();\n    for (StorageType t: StorageType.getTypesSupportingQuota()) {\n      HdfsProtos.StorageTypeQuotaInfoProto info =\n          HdfsProtos.StorageTypeQuotaInfoProto.newBuilder().\n              setType(convertStorageType(t)).\n              setConsumed(qu.getTypeConsumed(t)).\n              setQuota(qu.getTypeQuota(t)).\n              build();\n      isb.addTypeQuotaInfo(info);\n    }\n    return isb;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.updateMetrics": "  void updateMetrics(String name, int queueTime, int processingTime) {\n    rpcMetrics.addRpcQueueTime(queueTime);\n    rpcMetrics.addRpcProcessingTime(processingTime);\n    rpcDetailedMetrics.addProcessingTime(name, processingTime);\n    callQueue.addResponseTime(name, getPriorityLevel(), queueTime,\n        processingTime);\n\n    if (isLogSlowRPC()) {\n      logSlowRpcCalls(name, processingTime);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.logSlowRpcCalls": "  void logSlowRpcCalls(String methodName, int processingTime) {\n    final int deviation = 3;\n\n    // 1024 for minSampleSize just a guess -- not a number computed based on\n    // sample size analysis. It is chosen with the hope that this\n    // number is high enough to avoid spurious logging, yet useful\n    // in practice.\n    final int minSampleSize = 1024;\n    final double threeSigma = rpcMetrics.getProcessingMean() +\n        (rpcMetrics.getProcessingStdDev() * deviation);\n\n    if ((rpcMetrics.getProcessingSampleCount() > minSampleSize) &&\n        (processingTime > threeSigma)) {\n      if(LOG.isWarnEnabled()) {\n        String client = CurCall.get().connection.toString();\n        LOG.warn(\n            \"Slow RPC : \" + methodName + \" took \" + processingTime +\n                \" milliseconds to process from client \" + client);\n      }\n      rpcMetrics.incrSlowRpc();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.getPriorityLevel": "    public int getPriorityLevel() {\n      return this.priorityLevel;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.isLogSlowRPC": "  protected boolean isLogSlowRPC() {\n    return logSlowRPC;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcServerException.getRpcStatusProto": "  public RpcStatusProto getRpcStatusProto() {\n    return RpcStatusProto.ERROR;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.CallerContext.setCurrent": "  public static void setCurrent(CallerContext callerContext) {\n    CurrentCallerContextHolder.CALLER_CONTEXT.set(callerContext);\n  }"
        },
        "bug_report": {
            "Title": "NPE when applying AvailableSpaceBlockPlacementPolicy",
            "Description": "As HDFS-8131 introduced an AvailableSpaceBlockPlacementPolicy, but In some cases, it caused NPE. \n\nHere are my namenode daemon logs : \n\n2016-08-02 13:05:03,271 WARN org.apache.hadoop.ipc.Server: IPC Server handler 13 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.132.89.79:14001 Call#56 Retry#0\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.compareDataNode(AvailableSpaceBlockPlacementPolicy.java:95)\n        at org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.chooseDataNode(AvailableSpaceBlockPlacementPolicy.java:80)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:691)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:665)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseLocalRack(BlockPlacementPolicyDefault.java:572)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:457)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:367)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:242)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:114)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:130)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1606)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3315)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:679)\n        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:214)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:489)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)\n\nI reviewed the source code, and found the bug in method chooseDataNode. clusterMap.chooseRandom may return null, which cannot compare using equals a.equals(b) method.  \n\nThough this exception can be caught, and then retry another call. I think this bug should be fixed."
        }
    },
    {
        "filename": "HDFS-3332.json",
        "creation_time": "2012-04-27T04:42:38.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.protocol.DatanodeID.<init>(DatanodeID.java:66)\n\tat org.apache.hadoop.hdfs.protocol.DatanodeInfo.<init>(DatanodeInfo.java:87)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reportBadBlocks(BPServiceActor.java:238)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.reportBadBlocks(BPOfferService.java:187)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:559)\n\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.checkAndUpdate(FsDatasetImpl.java:1377)\n\tat org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.reconcile(DirectoryScanner.java:318)\n\tat org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.run(DirectoryScanner.java:284)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n\tat java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:619)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reportBadBlocks": "  void reportBadBlocks(ExtendedBlock block) {\n    DatanodeInfo[] dnArr = { new DatanodeInfo(bpRegistration) };\n    LocatedBlock[] blocks = { new LocatedBlock(block, dnArr) }; \n    \n    try {\n      bpNamenode.reportBadBlocks(blocks);  \n    } catch (IOException e){\n      /* One common reason is that NameNode could be in safe mode.\n       * Should we keep on retrying in that case?\n       */\n      LOG.warn(\"Failed to report bad block \" + block + \" to namenode : \"\n          + \" Exception\", e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPOfferService.reportBadBlocks": "  void reportBadBlocks(ExtendedBlock block) {\n    checkBlock(block);\n    for (BPServiceActor actor : bpServices) {\n      actor.reportBadBlocks(block);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPOfferService.checkBlock": "  private void checkBlock(ExtendedBlock block) {\n    Preconditions.checkArgument(block != null,\n        \"block is null\");\n    Preconditions.checkArgument(block.getBlockPoolId().equals(getBlockPoolId()),\n        \"block belongs to BP %s instead of BP %s\",\n        block.getBlockPoolId(), getBlockPoolId());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks": "  public void reportBadBlocks(ExtendedBlock block) throws IOException{\n    BPOfferService bpos = getBPOSForBlock(block);\n    bpos.reportBadBlocks(block);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getBPOSForBlock": "  private BPOfferService getBPOSForBlock(ExtendedBlock block)\n      throws IOException {\n    Preconditions.checkNotNull(block);\n    BPOfferService bpos = blockPoolManager.get(block.getBlockPoolId());\n    if (bpos == null) {\n      throw new IOException(\"cannot locate OfferService thread for bp=\"+\n          block.getBlockPoolId());\n    }\n    return bpos;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.checkAndUpdate": "  public void checkAndUpdate(String bpid, long blockId, File diskFile,\n      File diskMetaFile, FsVolumeSpi vol) {\n    Block corruptBlock = null;\n    ReplicaInfo memBlockInfo;\n    synchronized (this) {\n      memBlockInfo = volumeMap.get(bpid, blockId);\n      if (memBlockInfo != null && memBlockInfo.getState() != ReplicaState.FINALIZED) {\n        // Block is not finalized - ignore the difference\n        return;\n      }\n\n      final long diskGS = diskMetaFile != null && diskMetaFile.exists() ?\n          Block.getGenerationStamp(diskMetaFile.getName()) :\n            GenerationStamp.GRANDFATHER_GENERATION_STAMP;\n\n      if (diskFile == null || !diskFile.exists()) {\n        if (memBlockInfo == null) {\n          // Block file does not exist and block does not exist in memory\n          // If metadata file exists then delete it\n          if (diskMetaFile != null && diskMetaFile.exists()\n              && diskMetaFile.delete()) {\n            LOG.warn(\"Deleted a metadata file without a block \"\n                + diskMetaFile.getAbsolutePath());\n          }\n          return;\n        }\n        if (!memBlockInfo.getBlockFile().exists()) {\n          // Block is in memory and not on the disk\n          // Remove the block from volumeMap\n          volumeMap.remove(bpid, blockId);\n          final DataBlockScanner blockScanner = datanode.getBlockScanner();\n          if (blockScanner != null) {\n            blockScanner.deleteBlock(bpid, new Block(blockId));\n          }\n          LOG.warn(\"Removed block \" + blockId\n              + \" from memory with missing block file on the disk\");\n          // Finally remove the metadata file\n          if (diskMetaFile != null && diskMetaFile.exists()\n              && diskMetaFile.delete()) {\n            LOG.warn(\"Deleted a metadata file for the deleted block \"\n                + diskMetaFile.getAbsolutePath());\n          }\n        }\n        return;\n      }\n      /*\n       * Block file exists on the disk\n       */\n      if (memBlockInfo == null) {\n        // Block is missing in memory - add the block to volumeMap\n        ReplicaInfo diskBlockInfo = new FinalizedReplica(blockId, \n            diskFile.length(), diskGS, vol, diskFile.getParentFile());\n        volumeMap.add(bpid, diskBlockInfo);\n        final DataBlockScanner blockScanner = datanode.getBlockScanner();\n        if (blockScanner != null) {\n          blockScanner.addBlock(new ExtendedBlock(bpid, diskBlockInfo));\n        }\n        LOG.warn(\"Added missing block to memory \" + diskBlockInfo);\n        return;\n      }\n      /*\n       * Block exists in volumeMap and the block file exists on the disk\n       */\n      // Compare block files\n      File memFile = memBlockInfo.getBlockFile();\n      if (memFile.exists()) {\n        if (memFile.compareTo(diskFile) != 0) {\n          LOG.warn(\"Block file \" + memFile.getAbsolutePath()\n              + \" does not match file found by scan \"\n              + diskFile.getAbsolutePath());\n          // TODO: Should the diskFile be deleted?\n        }\n      } else {\n        // Block refers to a block file that does not exist.\n        // Update the block with the file found on the disk. Since the block\n        // file and metadata file are found as a pair on the disk, update\n        // the block based on the metadata file found on the disk\n        LOG.warn(\"Block file in volumeMap \"\n            + memFile.getAbsolutePath()\n            + \" does not exist. Updating it to the file found during scan \"\n            + diskFile.getAbsolutePath());\n        memBlockInfo.setDir(diskFile.getParentFile());\n        memFile = diskFile;\n\n        LOG.warn(\"Updating generation stamp for block \" + blockId\n            + \" from \" + memBlockInfo.getGenerationStamp() + \" to \" + diskGS);\n        memBlockInfo.setGenerationStamp(diskGS);\n      }\n\n      // Compare generation stamp\n      if (memBlockInfo.getGenerationStamp() != diskGS) {\n        File memMetaFile = FsDatasetUtil.getMetaFile(diskFile, \n            memBlockInfo.getGenerationStamp());\n        if (memMetaFile.exists()) {\n          if (memMetaFile.compareTo(diskMetaFile) != 0) {\n            LOG.warn(\"Metadata file in memory \"\n                + memMetaFile.getAbsolutePath()\n                + \" does not match file found by scan \"\n                + (diskMetaFile == null? null: diskMetaFile.getAbsolutePath()));\n          }\n        } else {\n          // Metadata file corresponding to block in memory is missing\n          // If metadata file found during the scan is on the same directory\n          // as the block file, then use the generation stamp from it\n          long gs = diskMetaFile != null && diskMetaFile.exists()\n              && diskMetaFile.getParent().equals(memFile.getParent()) ? diskGS\n              : GenerationStamp.GRANDFATHER_GENERATION_STAMP;\n\n          LOG.warn(\"Updating generation stamp for block \" + blockId\n              + \" from \" + memBlockInfo.getGenerationStamp() + \" to \" + gs);\n\n          memBlockInfo.setGenerationStamp(gs);\n        }\n      }\n\n      // Compare block size\n      if (memBlockInfo.getNumBytes() != memFile.length()) {\n        // Update the length based on the block file\n        corruptBlock = new Block(memBlockInfo);\n        LOG.warn(\"Updating size of block \" + blockId + \" from \"\n            + memBlockInfo.getNumBytes() + \" to \" + memFile.length());\n        memBlockInfo.setNumBytes(memFile.length());\n      }\n    }\n\n    // Send corrupt block report outside the lock\n    if (corruptBlock != null) {\n      LOG.warn(\"Reporting the block \" + corruptBlock\n          + \" as corrupt due to length mismatch\");\n      try {\n        datanode.reportBadBlocks(new ExtendedBlock(bpid, corruptBlock));  \n      } catch (IOException e) {\n        LOG.warn(\"Failed to repot bad block \" + corruptBlock, e);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getBlockFile": "  File getBlockFile(String bpid, Block b) throws IOException {\n    File f = validateBlockFile(bpid, b);\n    if(f == null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"b=\" + b + \", volumeMap=\" + volumeMap);\n      }\n      throw new IOException(\"Block \" + b + \" is not valid.\");\n    }\n    return f;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.reconcile": "  void reconcile() {\n    scan();\n    for (Entry<String, LinkedList<ScanInfo>> entry : diffs.entrySet()) {\n      String bpid = entry.getKey();\n      LinkedList<ScanInfo> diff = entry.getValue();\n      \n      for (ScanInfo info : diff) {\n        dataset.checkAndUpdate(bpid, info.getBlockId(), info.getBlockFile(),\n            info.getMetaFile(), info.getVolume());\n      }\n    }\n    if (!retainDiffs) clear();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.getMetaFile": "    File getMetaFile() {\n      return metaFile;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.getBlockId": "    long getBlockId() {\n      return blockId;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.getVolume": "    FsVolumeSpi getVolume() {\n      return volume;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.clear": "  private void clear() {\n    diffs.clear();\n    stats.clear();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.getBlockFile": "    File getBlockFile() {\n      return blockFile;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.scan": "  void scan() {\n    clear();\n    Map<String, ScanInfo[]> diskReport = getDiskReport();\n\n    // Hold FSDataset lock to prevent further changes to the block map\n    synchronized(dataset) {\n      for (Entry<String, ScanInfo[]> entry : diskReport.entrySet()) {\n        String bpid = entry.getKey();\n        ScanInfo[] blockpoolReport = entry.getValue();\n        \n        Stats statsRecord = new Stats(bpid);\n        stats.put(bpid, statsRecord);\n        LinkedList<ScanInfo> diffRecord = new LinkedList<ScanInfo>();\n        diffs.put(bpid, diffRecord);\n        \n        statsRecord.totalBlocks = blockpoolReport.length;\n        List<Block> bl = dataset.getFinalizedBlocks(bpid);\n        Block[] memReport = bl.toArray(new Block[bl.size()]);\n        Arrays.sort(memReport); // Sort based on blockId\n  \n        int d = 0; // index for blockpoolReport\n        int m = 0; // index for memReprot\n        while (m < memReport.length && d < blockpoolReport.length) {\n          Block memBlock = memReport[Math.min(m, memReport.length - 1)];\n          ScanInfo info = blockpoolReport[Math.min(\n              d, blockpoolReport.length - 1)];\n          if (info.getBlockId() < memBlock.getBlockId()) {\n            // Block is missing in memory\n            statsRecord.missingMemoryBlocks++;\n            addDifference(diffRecord, statsRecord, info);\n            d++;\n            continue;\n          }\n          if (info.getBlockId() > memBlock.getBlockId()) {\n            // Block is missing on the disk\n            addDifference(diffRecord, statsRecord, memBlock.getBlockId());\n            m++;\n            continue;\n          }\n          // Block file and/or metadata file exists on the disk\n          // Block exists in memory\n          if (info.getBlockFile() == null) {\n            // Block metadata file exits and block file is missing\n            addDifference(diffRecord, statsRecord, info);\n          } else if (info.getGenStamp() != memBlock.getGenerationStamp()\n              || info.getBlockFile().length() != memBlock.getNumBytes()) {\n            // Block metadata file is missing or has wrong generation stamp,\n            // or block file length is different than expected\n            statsRecord.mismatchBlocks++;\n            addDifference(diffRecord, statsRecord, info);\n          }\n          d++;\n          m++;\n        }\n        while (m < memReport.length) {\n          addDifference(diffRecord, statsRecord, memReport[m++].getBlockId());\n        }\n        while (d < blockpoolReport.length) {\n          statsRecord.missingMemoryBlocks++;\n          addDifference(diffRecord, statsRecord, blockpoolReport[d++]);\n        }\n        LOG.info(statsRecord.toString());\n      } //end for\n    } //end synchronized\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.run": "  public void run() {\n    try {\n      if (!shouldRun) {\n        //shutdown has been activated\n        LOG.warn(\"this cycle terminating immediately because 'shouldRun' has been deactivated\");\n        return;\n      }\n\n      String[] bpids = dataset.getBlockPoolList();\n      for(String bpid : bpids) {\n        UpgradeManagerDatanode um = \n          datanode.getUpgradeManagerDatanode(bpid);\n        if (um != null && !um.isUpgradeCompleted()) {\n          //If distributed upgrades underway, exit and wait for next cycle.\n          LOG.warn(\"this cycle terminating immediately because Distributed Upgrade is in process\");\n          return; \n        }\n      }\n      \n      //We're are okay to run - do it\n      reconcile();      \n      \n    } catch (Exception e) {\n      //Log and continue - allows Executor to run again next cycle\n      LOG.error(\"Exception during DirectoryScanner execution - will continue next cycle\", e);\n    } catch (Error er) {\n      //Non-recoverable error - re-throw after logging the problem\n      LOG.error(\"System Error during DirectoryScanner execution - permanently terminating periodic scanner\", er);\n      throw er;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil.getMetaFile": "  static File getMetaFile(File f, long gs) {\n    return new File(f.getParent(),\n        DatanodeUtil.getMetaName(f.getName(), gs));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getUpgradeManagerDatanode": "  UpgradeManagerDatanode getUpgradeManagerDatanode(String bpid) {\n    BPOfferService bpos = blockPoolManager.get(bpid);\n    if(bpos==null) {\n      return null;\n    }\n    return bpos.getUpgradeManager();\n  }"
        },
        "bug_report": {
            "Title": "NullPointerException in DN when directoryscanner is trying to report bad blocks",
            "Description": "There is 1 NN and 1 DN (NN is started with HA conf)\nI corrupted 1 block and found \n{code}\n2012-04-27 09:59:01,214 INFO  datanode.DataNode (BPServiceActor.java:blockReport(401)) - BlockReport of 2 blocks took 0 msec to generate and 5 msecs for RPC and NN processing\n2012-04-27 09:59:01,214 INFO  datanode.DataNode (BPServiceActor.java:blockReport(420)) - sent block report, processed command:org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@3b756db3\n2012-04-27 09:59:01,726 INFO  datanode.DirectoryScanner (DirectoryScanner.java:scan(390)) - BlockPool BP-2087868617-10.18.40.95-1335500488012 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:1\n2012-04-27 09:59:01,727 WARN  impl.FsDatasetImpl (FsDatasetImpl.java:checkAndUpdate(1366)) - Updating size of block -4466699320171028643 from 1024 to 1034\n2012-04-27 09:59:01,727 WARN  impl.FsDatasetImpl (FsDatasetImpl.java:checkAndUpdate(1374)) - Reporting the block blk_-4466699320171028643_1004 as corrupt due to length mismatch\n2012-04-27 09:59:01,728 DEBUG ipc.Client (Client.java:sendParam(807)) - IPC Client (1957050620) connection to /10.18.40.95:8020 from root sending #257\n2012-04-27 09:59:01,730 DEBUG ipc.Client (Client.java:receiveResponse(848)) - IPC Client (1957050620) connection to /10.18.40.95:8020 from root got value #257\n2012-04-27 09:59:01,730 DEBUG ipc.ProtobufRpcEngine (ProtobufRpcEngine.java:invoke(193)) - Call: reportBadBlocks 2\n2012-04-27 09:59:01,731 ERROR datanode.DirectoryScanner (DirectoryScanner.java:run(288)) - Exception during DirectoryScanner execution - will continue next cycle\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.protocol.DatanodeID.<init>(DatanodeID.java:66)\n\tat org.apache.hadoop.hdfs.protocol.DatanodeInfo.<init>(DatanodeInfo.java:87)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reportBadBlocks(BPServiceActor.java:238)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.reportBadBlocks(BPOfferService.java:187)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:559)\n\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.checkAndUpdate(FsDatasetImpl.java:1377)\n\tat org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.reconcile(DirectoryScanner.java:318)\n\tat org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.run(DirectoryScanner.java:284)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n\tat java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:619)\n{code}\n\nHere when Directory scanner is trying to report badblock we got a NPE."
        }
    },
    {
        "filename": "HDFS-6130.json",
        "creation_time": "2014-03-20T07:15:47.000+0000",
        "stack_trace": "```\nFATAL namenode.NameNode: Exception in namenode join\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.isReservedName(FSDirectory.java:2984)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.addToParent(FSImageFormatPBINode.java:205)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeDirectorySection(FSImageFormatPBINode.java:162)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.loadInternal(FSImageFormatProtobuf.java:243)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:168)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.load(FSImageFormat$LoaderDelegator.java:120)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:895)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:881)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:704)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:271)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:894)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:653)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits(NameNode.java:912)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1276)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1360)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.isReservedName": "  public static boolean isReservedName(String src) {\n    return src.startsWith(DOT_RESERVED_PATH_PREFIX);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode.addToParent": "    private void addToParent(INodeDirectory parent, INode child) {\n      if (parent == dir.rootDir && FSDirectory.isReservedName(child)) {\n        throw new HadoopIllegalArgumentException(\"File name \\\"\"\n            + child.getLocalName() + \"\\\" is reserved. Please \"\n            + \" change the name of the existing file or directory to another \"\n            + \"name before upgrading to this release.\");\n      }\n      // NOTE: This does not update space counts for parents\n      if (!parent.addChild(child)) {\n        return;\n      }\n      dir.cacheName(child);\n\n      if (child.isFile()) {\n        updateBlocksMap(child.asFile(), fsn.getBlockManager());\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode.updateBlocksMap": "    public static void updateBlocksMap(INodeFile file, BlockManager bm) {\n      // Add file->block mapping\n      final BlockInfo[] blocks = file.getBlocks();\n      if (blocks != null) {\n        for (int i = 0; i < blocks.length; i++) {\n          file.setBlock(i, bm.addBlockCollection(blocks[i], file));\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode.loadINodeDirectorySection": "    void loadINodeDirectorySection(InputStream in) throws IOException {\n      final List<INodeReference> refList = parent.getLoaderContext()\n          .getRefList();\n      while (true) {\n        INodeDirectorySection.DirEntry e = INodeDirectorySection.DirEntry\n            .parseDelimitedFrom(in);\n        // note that in is a LimitedInputStream\n        if (e == null) {\n          break;\n        }\n        INodeDirectory p = dir.getInode(e.getParent()).asDirectory();\n        for (long id : e.getChildrenList()) {\n          INode child = dir.getInode(id);\n          addToParent(p, child);\n        }\n        for (int refId : e.getRefChildrenList()) {\n          INodeReference ref = refList.get(refId);\n          addToParent(p, ref);\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf.loadInternal": "    private void loadInternal(RandomAccessFile raFile, FileInputStream fin)\n        throws IOException {\n      if (!FSImageUtil.checkFileFormat(raFile)) {\n        throw new IOException(\"Unrecognized file format\");\n      }\n      FileSummary summary = FSImageUtil.loadSummary(raFile);\n\n      FileChannel channel = fin.getChannel();\n\n      FSImageFormatPBINode.Loader inodeLoader = new FSImageFormatPBINode.Loader(\n          fsn, this);\n      FSImageFormatPBSnapshot.Loader snapshotLoader = new FSImageFormatPBSnapshot.Loader(\n          fsn, this);\n\n      ArrayList<FileSummary.Section> sections = Lists.newArrayList(summary\n          .getSectionsList());\n      Collections.sort(sections, new Comparator<FileSummary.Section>() {\n        @Override\n        public int compare(FileSummary.Section s1, FileSummary.Section s2) {\n          SectionName n1 = SectionName.fromString(s1.getName());\n          SectionName n2 = SectionName.fromString(s2.getName());\n          if (n1 == null) {\n            return n2 == null ? 0 : -1;\n          } else if (n2 == null) {\n            return -1;\n          } else {\n            return n1.ordinal() - n2.ordinal();\n          }\n        }\n      });\n\n      StartupProgress prog = NameNode.getStartupProgress();\n      /**\n       * beginStep() and the endStep() calls do not match the boundary of the\n       * sections. This is because that the current implementation only allows\n       * a particular step to be started for once.\n       */\n      Step currentStep = null;\n\n      for (FileSummary.Section s : sections) {\n        channel.position(s.getOffset());\n        InputStream in = new BufferedInputStream(new LimitInputStream(fin,\n            s.getLength()));\n\n        in = FSImageUtil.wrapInputStreamForCompression(conf,\n            summary.getCodec(), in);\n\n        String n = s.getName();\n\n        switch (SectionName.fromString(n)) {\n        case NS_INFO:\n          loadNameSystemSection(in);\n          break;\n        case STRING_TABLE:\n          loadStringTableSection(in);\n          break;\n        case INODE: {\n          currentStep = new Step(StepType.INODES);\n          prog.beginStep(Phase.LOADING_FSIMAGE, currentStep);\n          inodeLoader.loadINodeSection(in);\n        }\n          break;\n        case INODE_REFERENCE:\n          snapshotLoader.loadINodeReferenceSection(in);\n          break;\n        case INODE_DIR:\n          inodeLoader.loadINodeDirectorySection(in);\n          break;\n        case FILES_UNDERCONSTRUCTION:\n          inodeLoader.loadFilesUnderConstructionSection(in);\n          break;\n        case SNAPSHOT:\n          snapshotLoader.loadSnapshotSection(in);\n          break;\n        case SNAPSHOT_DIFF:\n          snapshotLoader.loadSnapshotDiffSection(in);\n          break;\n        case SECRET_MANAGER: {\n          prog.endStep(Phase.LOADING_FSIMAGE, currentStep);\n          Step step = new Step(StepType.DELEGATION_TOKENS);\n          prog.beginStep(Phase.LOADING_FSIMAGE, step);\n          loadSecretManagerSection(in);\n          prog.endStep(Phase.LOADING_FSIMAGE, step);\n        }\n          break;\n        case CACHE_MANAGER: {\n          Step step = new Step(StepType.CACHE_POOLS);\n          prog.beginStep(Phase.LOADING_FSIMAGE, step);\n          loadCacheManagerSection(in);\n          prog.endStep(Phase.LOADING_FSIMAGE, step);\n        }\n          break;\n        default:\n          LOG.warn(\"Unrecognized section \" + n);\n          break;\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf.loadStringTableSection": "    private void loadStringTableSection(InputStream in) throws IOException {\n      StringTableSection s = StringTableSection.parseDelimitedFrom(in);\n      ctx.stringTable = new String[s.getNumEntry() + 1];\n      for (int i = 0; i < s.getNumEntry(); ++i) {\n        StringTableSection.Entry e = StringTableSection.Entry\n            .parseDelimitedFrom(in);\n        ctx.stringTable[e.getId()] = e.getStr();\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf.loadSecretManagerSection": "    private void loadSecretManagerSection(InputStream in) throws IOException {\n      SecretManagerSection s = SecretManagerSection.parseDelimitedFrom(in);\n      int numKeys = s.getNumKeys(), numTokens = s.getNumTokens();\n      ArrayList<SecretManagerSection.DelegationKey> keys = Lists\n          .newArrayListWithCapacity(numKeys);\n      ArrayList<SecretManagerSection.PersistToken> tokens = Lists\n          .newArrayListWithCapacity(numTokens);\n\n      for (int i = 0; i < numKeys; ++i)\n        keys.add(SecretManagerSection.DelegationKey.parseDelimitedFrom(in));\n\n      for (int i = 0; i < numTokens; ++i)\n        tokens.add(SecretManagerSection.PersistToken.parseDelimitedFrom(in));\n\n      fsn.loadSecretManagerState(s, keys, tokens);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf.fromString": "    public static SectionName fromString(String name) {\n      for (SectionName n : values) {\n        if (n.name.equals(name))\n          return n;\n      }\n      return null;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf.loadCacheManagerSection": "    private void loadCacheManagerSection(InputStream in) throws IOException {\n      CacheManagerSection s = CacheManagerSection.parseDelimitedFrom(in);\n      ArrayList<CachePoolInfoProto> pools = Lists.newArrayListWithCapacity(s\n          .getNumPools());\n      ArrayList<CacheDirectiveInfoProto> directives = Lists\n          .newArrayListWithCapacity(s.getNumDirectives());\n      for (int i = 0; i < s.getNumPools(); ++i)\n        pools.add(CachePoolInfoProto.parseDelimitedFrom(in));\n      for (int i = 0; i < s.getNumDirectives(); ++i)\n        directives.add(CacheDirectiveInfoProto.parseDelimitedFrom(in));\n      fsn.getCacheManager().loadState(\n          new CacheManager.PersistState(s, pools, directives));\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf.loadNameSystemSection": "    private void loadNameSystemSection(InputStream in) throws IOException {\n      NameSystemSection s = NameSystemSection.parseDelimitedFrom(in);\n      fsn.setGenerationStampV1(s.getGenstampV1());\n      fsn.setGenerationStampV2(s.getGenstampV2());\n      fsn.setGenerationStampV1Limit(s.getGenstampV1Limit());\n      fsn.setLastAllocatedBlockId(s.getLastAllocatedBlockId());\n      imgTxId = s.getTransactionId();\n      if (s.hasRollingUpgradeStartTime()\n          && fsn.getFSImage().hasRollbackFSImage()) {\n        // we set the rollingUpgradeInfo only when we make sure we have the\n        // rollback image\n        fsn.setRollingUpgradeInfo(true, s.getRollingUpgradeStartTime());\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf.load": "    void load(File file) throws IOException {\n      long start = System.currentTimeMillis();\n      imgDigest = MD5FileUtils.computeMd5ForFile(file);\n      RandomAccessFile raFile = new RandomAccessFile(file, \"r\");\n      FileInputStream fin = new FileInputStream(file);\n      try {\n        loadInternal(raFile, fin);\n        long end = System.currentTimeMillis();\n        LOG.info(\"Loaded FSImage in \" + (end - start) / 1000 + \" seconds.\");\n      } finally {\n        fin.close();\n        raFile.close();\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage": "  private void loadFSImage(File curFile, MD5Hash expectedMd5,\n      FSNamesystem target, MetaRecoveryContext recovery) throws IOException {\n    // BlockPoolId is required when the FsImageLoader loads the rolling upgrade\n    // information. Make sure the ID is properly set.\n    target.setBlockPoolId(this.getBlockPoolID());\n\n    FSImageFormat.LoaderDelegator loader = FSImageFormat.newLoader(conf, target);\n    loader.load(curFile);\n\n    // Check that the image digest we loaded matches up with what\n    // we expected\n    MD5Hash readImageMd5 = loader.getLoadedImageMd5();\n    if (expectedMd5 != null &&\n        !expectedMd5.equals(readImageMd5)) {\n      throw new IOException(\"Image file \" + curFile +\n          \" is corrupt with MD5 checksum of \" + readImageMd5 +\n          \" but expecting \" + expectedMd5);\n    }\n\n    long txId = loader.getLoadedImageTxId();\n    LOG.info(\"Loaded image for txid \" + txId + \" from \" + curFile);\n    lastAppliedTxId = txId;\n    storage.setMostRecentCheckpointInfo(txId, curFile.lastModified());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getBlockPoolID": "  public String getBlockPoolID() {\n    return storage.getBlockPoolID();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.initEditLog": "  public void initEditLog(StartupOption startOpt) throws IOException {\n    Preconditions.checkState(getNamespaceID() != 0,\n        \"Must know namespace ID before initting edit log\");\n    String nameserviceId = DFSUtil.getNamenodeNameServiceId(conf);\n    if (!HAUtil.isHAEnabled(conf, nameserviceId)) {\n      // If this NN is not HA\n      editLog.initJournalsForWrite();\n      editLog.recoverUnclosedStreams();\n    } else if (HAUtil.isHAEnabled(conf, nameserviceId)\n        && (startOpt == StartupOption.UPGRADE\n            || RollingUpgradeStartupOption.ROLLBACK.matches(startOpt))) {\n      // This NN is HA, but we're doing an upgrade or a rollback of rolling\n      // upgrade so init the edit log for write.\n      editLog.initJournalsForWrite();\n      if (startOpt == StartupOption.UPGRADE) {\n        long sharedLogCTime = editLog.getSharedLogCTime();\n        if (this.storage.getCTime() < sharedLogCTime) {\n          throw new IOException(\"It looks like the shared log is already \" +\n              \"being upgraded but this NN has not been upgraded yet. You \" +\n              \"should restart this NameNode with the '\" +\n              StartupOption.BOOTSTRAPSTANDBY.getName() + \"' option to bring \" +\n              \"this NN in sync with the other.\");\n        }\n      }\n      editLog.recoverUnclosedStreams();\n    } else {\n      // This NN is HA and we're not doing an upgrade.\n      editLog.initSharedJournalsForRead();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getLayoutVersion": "  public int getLayoutVersion() {\n    return storage.getLayoutVersion();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.needsResaveBasedOnStaleCheckpoint": "  private boolean needsResaveBasedOnStaleCheckpoint(\n      File imageFile, long numEditsLoaded) {\n    final long checkpointPeriod = conf.getLong(\n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_KEY, \n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_DEFAULT);\n    final long checkpointTxnCount = conf.getLong(\n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_KEY, \n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_DEFAULT);\n    long checkpointAge = Time.now() - imageFile.lastModified();\n\n    return (checkpointAge > checkpointPeriod * 1000) ||\n           (numEditsLoaded > checkpointTxnCount);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.renameCheckpoint": "  void renameCheckpoint(NameNodeFile fromNnf, NameNodeFile toNnf)\n      throws IOException {\n    ArrayList<StorageDirectory> al = null;\n    FSImageTransactionalStorageInspector inspector =\n        new FSImageTransactionalStorageInspector(EnumSet.of(fromNnf));\n    storage.inspectStorageDirs(inspector);\n    for (FSImageFile image : inspector.getFoundImages()) {\n      try {\n        renameImageFileInDir(image.sd, fromNnf, toNnf, image.txId, true);\n      } catch (IOException ioe) {\n        LOG.warn(\"Unable to rename checkpoint in \" + image.sd, ioe);\n        if (al == null) {\n          al = Lists.newArrayList();\n        }\n        al.add(image.sd);\n      }\n    }\n    if(al != null) {\n      storage.reportErrorsOnDirectories(al);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.rollingRollback": "  private void rollingRollback(long discardSegmentTxId, long ckptId)\n      throws IOException {\n    // discard discard unnecessary editlog segments starting from the given id\n    this.editLog.discardSegments(discardSegmentTxId);\n    // rename the special checkpoint\n    renameCheckpoint(ckptId, NameNodeFile.IMAGE_ROLLBACK, NameNodeFile.IMAGE,\n        true);\n    // purge all the checkpoints after the marker\n    archivalManager.purgeCheckpoinsAfter(NameNodeFile.IMAGE, ckptId);\n    String nameserviceId = DFSUtil.getNamenodeNameServiceId(conf);\n    if (HAUtil.isHAEnabled(conf, nameserviceId)) {\n      // close the editlog since it is currently open for write\n      this.editLog.close();\n      // reopen the editlog for read\n      this.editLog.initSharedJournalsForRead();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile": "  void loadFSImageFile(FSNamesystem target, MetaRecoveryContext recovery,\n      FSImageFile imageFile) throws IOException {\n    LOG.debug(\"Planning to load image :\\n\" + imageFile);\n    StorageDirectory sdForProperties = imageFile.sd;\n    storage.readProperties(sdForProperties);\n\n    if (NameNodeLayoutVersion.supports(\n        LayoutVersion.Feature.TXID_BASED_LAYOUT, getLayoutVersion())) {\n      // For txid-based layout, we should have a .md5 file\n      // next to the image file\n      loadFSImage(imageFile.getFile(), target, recovery);\n    } else if (NameNodeLayoutVersion.supports(\n        LayoutVersion.Feature.FSIMAGE_CHECKSUM, getLayoutVersion())) {\n      // In 0.22, we have the checksum stored in the VERSION file.\n      String md5 = storage.getDeprecatedProperty(\n          NNStorage.DEPRECATED_MESSAGE_DIGEST_PROPERTY);\n      if (md5 == null) {\n        throw new InconsistentFSStateException(sdForProperties.getRoot(),\n            \"Message digest property \" +\n            NNStorage.DEPRECATED_MESSAGE_DIGEST_PROPERTY +\n            \" not set for storage directory \" + sdForProperties.getRoot());\n      }\n      loadFSImage(imageFile.getFile(), new MD5Hash(md5), target, recovery);\n    } else {\n      // We don't have any record of the md5sum\n      loadFSImage(imageFile.getFile(), null, target, recovery);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.isUpgradeFinalized": "  boolean isUpgradeFinalized() {\n    return isUpgradeFinalized;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits": "  private long loadEdits(Iterable<EditLogInputStream> editStreams,\n      FSNamesystem target, StartupOption startOpt, MetaRecoveryContext recovery)\n      throws IOException {\n    LOG.debug(\"About to load edits:\\n  \" + Joiner.on(\"\\n  \").join(editStreams));\n    StartupProgress prog = NameNode.getStartupProgress();\n    prog.beginPhase(Phase.LOADING_EDITS);\n    \n    long prevLastAppliedTxId = lastAppliedTxId;  \n    try {    \n      FSEditLogLoader loader = new FSEditLogLoader(target, lastAppliedTxId);\n      \n      // Load latest edits\n      for (EditLogInputStream editIn : editStreams) {\n        LOG.info(\"Reading \" + editIn + \" expecting start txid #\" +\n              (lastAppliedTxId + 1));\n        try {\n          loader.loadFSEdits(editIn, lastAppliedTxId + 1, startOpt, recovery);\n        } finally {\n          // Update lastAppliedTxId even in case of error, since some ops may\n          // have been successfully applied before the error.\n          lastAppliedTxId = loader.getLastAppliedTxId();\n        }\n        // If we are in recovery mode, we may have skipped over some txids.\n        if (editIn.getLastTxId() != HdfsConstants.INVALID_TXID) {\n          lastAppliedTxId = editIn.getLastTxId();\n        }\n      }\n    } finally {\n      FSEditLog.closeAllStreams(editStreams);\n      // update the counts\n      updateCountForQuota(target.dir.rootDir);\n    }\n    prog.endPhase(Phase.LOADING_EDITS);\n    return lastAppliedTxId - prevLastAppliedTxId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead": "  boolean recoverTransitionRead(StartupOption startOpt, FSNamesystem target,\n      MetaRecoveryContext recovery)\n      throws IOException {\n    assert startOpt != StartupOption.FORMAT : \n      \"NameNode formatting should be performed before reading the image\";\n    \n    Collection<URI> imageDirs = storage.getImageDirectories();\n    Collection<URI> editsDirs = editLog.getEditURIs();\n\n    // none of the data dirs exist\n    if((imageDirs.size() == 0 || editsDirs.size() == 0) \n                             && startOpt != StartupOption.IMPORT)  \n      throw new IOException(\n          \"All specified directories are not accessible or do not exist.\");\n    \n    // 1. For each data directory calculate its state and \n    // check whether all is consistent before transitioning.\n    Map<StorageDirectory, StorageState> dataDirStates = \n             new HashMap<StorageDirectory, StorageState>();\n    boolean isFormatted = recoverStorageDirs(startOpt, dataDirStates);\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Data dir states:\\n  \" +\n        Joiner.on(\"\\n  \").withKeyValueSeparator(\": \")\n        .join(dataDirStates));\n    }\n    \n    if (!isFormatted && startOpt != StartupOption.ROLLBACK \n                     && startOpt != StartupOption.IMPORT) {\n      throw new IOException(\"NameNode is not formatted.\");      \n    }\n\n\n    int layoutVersion = storage.getLayoutVersion();\n    if (layoutVersion < Storage.LAST_PRE_UPGRADE_LAYOUT_VERSION) {\n      NNStorage.checkVersionUpgradable(storage.getLayoutVersion());\n    }\n    if (startOpt != StartupOption.UPGRADE\n        && !RollingUpgradeStartupOption.STARTED.matches(startOpt)\n        && layoutVersion < Storage.LAST_PRE_UPGRADE_LAYOUT_VERSION\n        && layoutVersion != HdfsConstants.NAMENODE_LAYOUT_VERSION) {\n      throw new IOException(\n          \"\\nFile system image contains an old layout version \" \n          + storage.getLayoutVersion() + \".\\nAn upgrade to version \"\n          + HdfsConstants.NAMENODE_LAYOUT_VERSION + \" is required.\\n\"\n          + \"Please restart NameNode with the \\\"\"\n          + RollingUpgradeStartupOption.STARTED.getOptionString()\n          + \"\\\" option if a rolling upgraded is already started;\"\n          + \" or restart NameNode with the \\\"\"\n          + StartupOption.UPGRADE + \"\\\" to start a new upgrade.\");\n    }\n    \n    storage.processStartupOptionsForUpgrade(startOpt, layoutVersion);\n\n    // 2. Format unformatted dirs.\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      StorageState curState = dataDirStates.get(sd);\n      switch(curState) {\n      case NON_EXISTENT:\n        throw new IOException(StorageState.NON_EXISTENT + \n                              \" state cannot be here\");\n      case NOT_FORMATTED:\n        LOG.info(\"Storage directory \" + sd.getRoot() + \" is not formatted.\");\n        LOG.info(\"Formatting ...\");\n        sd.clearDirectory(); // create empty currrent dir\n        break;\n      default:\n        break;\n      }\n    }\n\n    // 3. Do transitions\n    switch(startOpt) {\n    case UPGRADE:\n      doUpgrade(target);\n      return false; // upgrade saved image already\n    case IMPORT:\n      doImportCheckpoint(target);\n      return false; // import checkpoint saved image already\n    case ROLLBACK:\n      throw new AssertionError(\"Rollback is now a standalone command, \" +\n          \"NameNode should not be starting with this option.\");\n    case REGULAR:\n    default:\n      // just load the image\n    }\n    \n    return loadFSImage(target, startOpt, recovery);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs": "  private boolean recoverStorageDirs(StartupOption startOpt,\n      Map<StorageDirectory, StorageState> dataDirStates) throws IOException {\n    boolean isFormatted = false;\n    // This loop needs to be over all storage dirs, even shared dirs, to make\n    // sure that we properly examine their state, but we make sure we don't\n    // mutate the shared dir below in the actual loop.\n    for (Iterator<StorageDirectory> it = \n                      storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      StorageState curState;\n      try {\n        curState = sd.analyzeStorage(startOpt, storage);\n        // sd is locked but not opened\n        switch(curState) {\n        case NON_EXISTENT:\n          // name-node fails if any of the configured storage dirs are missing\n          throw new InconsistentFSStateException(sd.getRoot(),\n                      \"storage directory does not exist or is not accessible.\");\n        case NOT_FORMATTED:\n          break;\n        case NORMAL:\n          break;\n        default:  // recovery is possible\n          sd.doRecover(curState);\n        }\n        if (curState != StorageState.NOT_FORMATTED \n            && startOpt != StartupOption.ROLLBACK) {\n          // read and verify consistency with other directories\n          storage.readProperties(sd);\n          isFormatted = true;\n        }\n        if (startOpt == StartupOption.IMPORT && isFormatted)\n          // import of a checkpoint is allowed only into empty image directories\n          throw new IOException(\"Cannot import image from a checkpoint. \" \n              + \" NameNode already contains an image in \" + sd.getRoot());\n      } catch (IOException ioe) {\n        sd.unlock();\n        throw ioe;\n      }\n      dataDirStates.put(sd,curState);\n    }\n    return isFormatted;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade": "  void doUpgrade(FSNamesystem target) throws IOException {\n    checkUpgrade(target);\n\n    // load the latest image\n\n    // Do upgrade for each directory\n    this.loadFSImage(target, StartupOption.UPGRADE, null);\n    target.checkRollingUpgrade(\"upgrade namenode\");\n    \n    long oldCTime = storage.getCTime();\n    storage.cTime = now();  // generate new cTime for the state\n    int oldLV = storage.getLayoutVersion();\n    storage.layoutVersion = HdfsConstants.NAMENODE_LAYOUT_VERSION;\n    \n    List<StorageDirectory> errorSDs =\n      Collections.synchronizedList(new ArrayList<StorageDirectory>());\n    assert !editLog.isSegmentOpen() : \"Edits log must not be open.\";\n    LOG.info(\"Starting upgrade of local storage directories.\"\n        + \"\\n   old LV = \" + oldLV\n        + \"; old CTime = \" + oldCTime\n        + \".\\n   new LV = \" + storage.getLayoutVersion()\n        + \"; new CTime = \" + storage.getCTime());\n    // Do upgrade for each directory\n    for (Iterator<StorageDirectory> it = storage.dirIterator(false); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      try {\n        NNUpgradeUtil.doPreUpgrade(sd);\n      } catch (Exception e) {\n        LOG.error(\"Failed to move aside pre-upgrade storage \" +\n            \"in image directory \" + sd.getRoot(), e);\n        errorSDs.add(sd);\n        continue;\n      }\n    }\n    if (target.isHaEnabled()) {\n      editLog.doPreUpgradeOfSharedLog();\n    }\n    storage.reportErrorsOnDirectories(errorSDs);\n    errorSDs.clear();\n\n    saveFSImageInAllDirs(target, editLog.getLastWrittenTxId());\n\n    for (Iterator<StorageDirectory> it = storage.dirIterator(false); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      try {\n        NNUpgradeUtil.doUpgrade(sd, storage);\n      } catch (IOException ioe) {\n        errorSDs.add(sd);\n        continue;\n      }\n    }\n    if (target.isHaEnabled()) {\n      editLog.doUpgradeOfSharedLog();\n    }\n    storage.reportErrorsOnDirectories(errorSDs);\n    \n    isUpgradeFinalized = false;\n    if (!storage.getRemovedStorageDirs().isEmpty()) {\n      // during upgrade, it's a fatal error to fail any storage directory\n      throw new IOException(\"Upgrade failed in \"\n          + storage.getRemovedStorageDirs().size()\n          + \" storage directory(ies), previously logged.\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.doImportCheckpoint": "  void doImportCheckpoint(FSNamesystem target) throws IOException {\n    Collection<URI> checkpointDirs =\n      FSImage.getCheckpointDirs(conf, null);\n    List<URI> checkpointEditsDirs =\n      FSImage.getCheckpointEditsDirs(conf, null);\n\n    if (checkpointDirs == null || checkpointDirs.isEmpty()) {\n      throw new IOException(\"Cannot import image from a checkpoint. \"\n                            + \"\\\"dfs.namenode.checkpoint.dir\\\" is not set.\" );\n    }\n    \n    if (checkpointEditsDirs == null || checkpointEditsDirs.isEmpty()) {\n      throw new IOException(\"Cannot import image from a checkpoint. \"\n                            + \"\\\"dfs.namenode.checkpoint.dir\\\" is not set.\" );\n    }\n\n    FSImage realImage = target.getFSImage();\n    FSImage ckptImage = new FSImage(conf, \n                                    checkpointDirs, checkpointEditsDirs);\n    target.dir.fsImage = ckptImage;\n    // load from the checkpoint dirs\n    try {\n      ckptImage.recoverTransitionRead(StartupOption.REGULAR, target, null);\n    } finally {\n      ckptImage.close();\n    }\n    // return back the real image\n    realImage.getStorage().setStorageInfo(ckptImage.getStorage());\n    realImage.getEditLog().setNextTxId(ckptImage.getEditLog().getLastWrittenTxId()+1);\n    realImage.initEditLog(StartupOption.IMPORT);\n\n    target.dir.fsImage = realImage;\n    realImage.getStorage().setBlockPoolID(ckptImage.getBlockPoolID());\n\n    // and save it but keep the same checkpointTime\n    saveNamespace(target);\n    getStorage().writeAll();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage": "  private void loadFSImage(StartupOption startOpt) throws IOException {\n    final FSImage fsImage = getFSImage();\n\n    // format before starting up if requested\n    if (startOpt == StartupOption.FORMAT) {\n      \n      fsImage.format(this, fsImage.getStorage().determineClusterId());// reuse current id\n\n      startOpt = StartupOption.REGULAR;\n    }\n    boolean success = false;\n    writeLock();\n    try {\n      // We shouldn't be calling saveNamespace if we've come up in standby state.\n      MetaRecoveryContext recovery = startOpt.createRecoveryContext();\n      final boolean staleImage\n          = fsImage.recoverTransitionRead(startOpt, this, recovery);\n      if (RollingUpgradeStartupOption.ROLLBACK.matches(startOpt)) {\n        rollingUpgradeInfo = null;\n      }\n      final boolean needToSave = staleImage && !haEnabled && !isRollingUpgrade(); \n      LOG.info(\"Need to save fs image? \" + needToSave\n          + \" (staleImage=\" + staleImage + \", haEnabled=\" + haEnabled\n          + \", isRollingUpgrade=\" + isRollingUpgrade() + \")\");\n      if (needToSave) {\n        fsImage.saveNamespace(this);\n      } else {\n        // No need to save, so mark the phase done.\n        StartupProgress prog = NameNode.getStartupProgress();\n        prog.beginPhase(Phase.SAVING_CHECKPOINT);\n        prog.endPhase(Phase.SAVING_CHECKPOINT);\n      }\n      // This will start a new log segment and write to the seen_txid file, so\n      // we shouldn't do it when coming up in standby state\n      if (!haEnabled || (haEnabled && startOpt == StartupOption.UPGRADE)) {\n        fsImage.openEditLogForWrite();\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        fsImage.close();\n      }\n      writeUnlock();\n    }\n    dir.imageLoadComplete();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.close": "  void close() {\n    fsRunning = false;\n    try {\n      stopCommonServices();\n      if (smmthread != null) smmthread.interrupt();\n    } finally {\n      // using finally to ensure we also wait for lease daemon\n      try {\n        stopActiveServices();\n        stopStandbyServices();\n        if (dir != null) {\n          dir.close();\n        }\n      } catch (IOException ie) {\n        LOG.error(\"Error closing FSDirectory\", ie);\n        IOUtils.cleanup(LOG, dir);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock": "  public void writeUnlock() {\n    this.fsLock.writeLock().unlock();\n    this.fsLock.longReadLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFSImage": "  public FSImage getFSImage() {\n    return dir.fsImage;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock": "  public void writeLock() {\n    this.fsLock.longReadLock().lock();\n    this.fsLock.writeLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.isRollingUpgrade": "  public boolean isRollingUpgrade() {\n    return rollingUpgradeInfo != null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveNamespace": "  void saveNamespace() throws AccessControlException, IOException {\n    checkOperation(OperationCategory.UNCHECKED);\n    checkSuperuserPrivilege();\n    \n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    boolean success = false;\n    readLock();\n    try {\n      checkOperation(OperationCategory.UNCHECKED);\n\n      if (!isInSafeMode()) {\n        throw new IOException(\"Safe mode should be turned ON \"\n            + \"in order to create namespace image.\");\n      }\n      getFSImage().saveNamespace(this);\n      success = true;\n    } finally {\n      readUnlock();\n      RetryCache.setState(cacheEntry, success);\n    }\n    LOG.info(\"New namespace image has been created\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk": "  static FSNamesystem loadFromDisk(Configuration conf) throws IOException {\n\n    checkConfiguration(conf);\n    FSImage fsImage = new FSImage(conf,\n        FSNamesystem.getNamespaceDirs(conf),\n        FSNamesystem.getNamespaceEditsDirs(conf));\n    FSNamesystem namesystem = new FSNamesystem(conf, fsImage, false);\n    StartupOption startOpt = NameNode.getStartupOption(conf);\n    if (startOpt == StartupOption.RECOVER) {\n      namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n    }\n\n    long loadStart = now();\n    try {\n      namesystem.loadFSImage(startOpt);\n    } catch (IOException ioe) {\n      LOG.warn(\"Encountered exception loading fsimage\", ioe);\n      fsImage.close();\n      throw ioe;\n    }\n    long timeTakenToLoadFSImage = now() - loadStart;\n    LOG.info(\"Finished loading FSImage in \" + timeTakenToLoadFSImage + \" msecs\");\n    NameNodeMetrics nnMetrics = NameNode.getNameNodeMetrics();\n    if (nnMetrics != null) {\n      nnMetrics.setFsImageLoadTime((int) timeTakenToLoadFSImage);\n    }\n    return namesystem;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNamespaceEditsDirs": "  public static List<URI> getNamespaceEditsDirs(Configuration conf,\n      boolean includeShared)\n      throws IOException {\n    // Use a LinkedHashSet so that order is maintained while we de-dup\n    // the entries.\n    LinkedHashSet<URI> editsDirs = new LinkedHashSet<URI>();\n    \n    if (includeShared) {\n      List<URI> sharedDirs = getSharedEditsDirs(conf);\n  \n      // Fail until multiple shared edits directories are supported (HDFS-2782)\n      if (sharedDirs.size() > 1) {\n        throw new IOException(\n            \"Multiple shared edits directories are not yet supported\");\n      }\n  \n      // First add the shared edits dirs. It's critical that the shared dirs\n      // are added first, since JournalSet syncs them in the order they are listed,\n      // and we need to make sure all edits are in place in the shared storage\n      // before they are replicated locally. See HDFS-2874.\n      for (URI dir : sharedDirs) {\n        if (!editsDirs.add(dir)) {\n          LOG.warn(\"Edits URI \" + dir + \" listed multiple times in \" + \n              DFS_NAMENODE_SHARED_EDITS_DIR_KEY + \". Ignoring duplicates.\");\n        }\n      }\n    }    \n    // Now add the non-shared dirs.\n    for (URI dir : getStorageDirs(conf, DFS_NAMENODE_EDITS_DIR_KEY)) {\n      if (!editsDirs.add(dir)) {\n        LOG.warn(\"Edits URI \" + dir + \" listed multiple times in \" + \n            DFS_NAMENODE_SHARED_EDITS_DIR_KEY + \" and \" +\n            DFS_NAMENODE_EDITS_DIR_KEY + \". Ignoring duplicates.\");\n      }\n    }\n\n    if (editsDirs.isEmpty()) {\n      // If this is the case, no edit dirs have been explicitly configured.\n      // Image dirs are to be used for edits too.\n      return Lists.newArrayList(getNamespaceDirs(conf));\n    } else {\n      return Lists.newArrayList(editsDirs);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkConfiguration": "  private static void checkConfiguration(Configuration conf)\n      throws IOException {\n\n    final Collection<URI> namespaceDirs =\n        FSNamesystem.getNamespaceDirs(conf);\n    final Collection<URI> editsDirs =\n        FSNamesystem.getNamespaceEditsDirs(conf);\n    final Collection<URI> requiredEditsDirs =\n        FSNamesystem.getRequiredNamespaceEditsDirs(conf);\n    final Collection<URI> sharedEditsDirs =\n        FSNamesystem.getSharedEditsDirs(conf);\n\n    for (URI u : requiredEditsDirs) {\n      if (u.toString().compareTo(\n              DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_DEFAULT) == 0) {\n        continue;\n      }\n\n      // Each required directory must also be in editsDirs or in\n      // sharedEditsDirs.\n      if (!editsDirs.contains(u) &&\n          !sharedEditsDirs.contains(u)) {\n        throw new IllegalArgumentException(\n            \"Required edits directory \" + u.toString() + \" not present in \" +\n            DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_KEY + \". \" +\n            DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_KEY + \"=\" +\n            editsDirs.toString() + \"; \" +\n            DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_REQUIRED_KEY + \"=\" +\n            requiredEditsDirs.toString() + \". \" +\n            DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY + \"=\" +\n            sharedEditsDirs.toString() + \".\");\n      }\n    }\n\n    if (namespaceDirs.size() == 1) {\n      LOG.warn(\"Only one image storage directory (\"\n          + DFS_NAMENODE_NAME_DIR_KEY + \") configured. Beware of dataloss\"\n          + \" due to lack of redundant storage directories!\");\n    }\n    if (editsDirs.size() == 1) {\n      LOG.warn(\"Only one namespace edits storage directory (\"\n          + DFS_NAMENODE_EDITS_DIR_KEY + \") configured. Beware of dataloss\"\n          + \" due to lack of redundant storage directories!\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setSafeMode": "  boolean setSafeMode(SafeModeAction action) throws IOException {\n    if (action != SafeModeAction.SAFEMODE_GET) {\n      checkSuperuserPrivilege();\n      switch(action) {\n      case SAFEMODE_LEAVE: // leave safe mode\n        leaveSafeMode();\n        break;\n      case SAFEMODE_ENTER: // enter safe mode\n        enterSafeMode(false);\n        break;\n      default:\n        LOG.error(\"Unexpected safe mode action\");\n      }\n    }\n    return isInSafeMode();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNamespaceDirs": "  public static Collection<URI> getNamespaceDirs(Configuration conf) {\n    return getStorageDirs(conf, DFS_NAMENODE_NAME_DIR_KEY);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits": "  private static boolean initializeSharedEdits(Configuration conf,\n      boolean force, boolean interactive) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    \n    if (conf.get(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY) == null) {\n      LOG.fatal(\"No shared edits directory configured for namespace \" +\n          nsId + \" namenode \" + namenodeId);\n      return false;\n    }\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      InetSocketAddress socAddr = getAddress(conf);\n      SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n          DFS_NAMENODE_USER_NAME_KEY, socAddr.getHostName());\n    }\n\n    NNStorage existingStorage = null;\n    FSImage sharedEditsImage = null;\n    try {\n      FSNamesystem fsns =\n          FSNamesystem.loadFromDisk(getConfigurationWithoutSharedEdits(conf));\n      \n      existingStorage = fsns.getFSImage().getStorage();\n      NamespaceInfo nsInfo = existingStorage.getNamespaceInfo();\n      \n      List<URI> sharedEditsDirs = FSNamesystem.getSharedEditsDirs(conf);\n      \n      sharedEditsImage = new FSImage(conf,\n          Lists.<URI>newArrayList(),\n          sharedEditsDirs);\n      sharedEditsImage.getEditLog().initJournalsForWrite();\n      \n      if (!sharedEditsImage.confirmFormat(force, interactive)) {\n        return true; // abort\n      }\n      \n      NNStorage newSharedStorage = sharedEditsImage.getStorage();\n      // Call Storage.format instead of FSImage.format here, since we don't\n      // actually want to save a checkpoint - just prime the dirs with\n      // the existing namespace info\n      newSharedStorage.format(nsInfo);\n      sharedEditsImage.getEditLog().formatNonFileJournals(nsInfo);\n\n      // Need to make sure the edit log segments are in good shape to initialize\n      // the shared edits dir.\n      fsns.getFSImage().getEditLog().close();\n      fsns.getFSImage().getEditLog().initJournalsForWrite();\n      fsns.getFSImage().getEditLog().recoverUnclosedStreams();\n\n      copyEditLogSegmentsToSharedDir(fsns, sharedEditsDirs, newSharedStorage,\n          conf);\n    } catch (IOException ioe) {\n      LOG.error(\"Could not initialize shared edits dir\", ioe);\n      return true; // aborted\n    } finally {\n      if (sharedEditsImage != null) {\n        try {\n          sharedEditsImage.close();\n        }  catch (IOException ioe) {\n          LOG.warn(\"Could not close sharedEditsImage\", ioe);\n        }\n      }\n      // Have to unlock storage explicitly for the case when we're running in a\n      // unit test, which runs in the same JVM as NNs.\n      if (existingStorage != null) {\n        try {\n          existingStorage.unlockAll();\n        } catch (IOException ioe) {\n          LOG.warn(\"Could not unlock storage directories\", ioe);\n          return true; // aborted\n        }\n      }\n    }\n    return false; // did not abort\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress": "  public static InetSocketAddress getAddress(URI filesystemURI) {\n    String authority = filesystemURI.getAuthority();\n    if (authority == null) {\n      throw new IllegalArgumentException(String.format(\n          \"Invalid URI for NameNode address (check %s): %s has no authority.\",\n          FileSystem.FS_DEFAULT_NAME_KEY, filesystemURI.toString()));\n    }\n    if (!HdfsConstants.HDFS_URI_SCHEME.equalsIgnoreCase(\n        filesystemURI.getScheme())) {\n      throw new IllegalArgumentException(String.format(\n          \"Invalid URI for NameNode address (check %s): %s is not of scheme '%s'.\",\n          FileSystem.FS_DEFAULT_NAME_KEY, filesystemURI.toString(),\n          HdfsConstants.HDFS_URI_SCHEME));\n    }\n    return getAddress(authority);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getFSImage": "  public FSImage getFSImage() {\n    return namesystem.dir.fsImage;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initializeGenericKeys": "  public static void initializeGenericKeys(Configuration conf,\n      String nameserviceId, String namenodeId) {\n    if ((nameserviceId != null && !nameserviceId.isEmpty()) || \n        (namenodeId != null && !namenodeId.isEmpty())) {\n      if (nameserviceId != null) {\n        conf.set(DFS_NAMESERVICE_ID, nameserviceId);\n      }\n      if (namenodeId != null) {\n        conf.set(DFS_HA_NAMENODE_ID_KEY, namenodeId);\n      }\n      \n      DFSUtil.setGenericConf(conf, nameserviceId, namenodeId,\n          NAMENODE_SPECIFIC_KEYS);\n      DFSUtil.setGenericConf(conf, nameserviceId, null,\n          NAMESERVICE_SPECIFIC_KEYS);\n    }\n    \n    // If the RPC address is set use it to (re-)configure the default FS\n    if (conf.get(DFS_NAMENODE_RPC_ADDRESS_KEY) != null) {\n      URI defaultUri = URI.create(HdfsConstants.HDFS_URI_SCHEME + \"://\"\n          + conf.get(DFS_NAMENODE_RPC_ADDRESS_KEY));\n      conf.set(FS_DEFAULT_NAME_KEY, defaultUri.toString());\n      LOG.debug(\"Setting \" + FS_DEFAULT_NAME_KEY + \" to \" + defaultUri.toString());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.isSecurityEnabled": "  public boolean isSecurityEnabled() {\n    return UserGroupInformation.isSecurityEnabled();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getConfigurationWithoutSharedEdits": "  private static Configuration getConfigurationWithoutSharedEdits(\n      Configuration conf)\n      throws IOException {\n    List<URI> editsDirs = FSNamesystem.getNamespaceEditsDirs(conf, false);\n    String editsDirsString = Joiner.on(\",\").join(editsDirs);\n\n    Configuration confWithoutShared = new Configuration(conf);\n    confWithoutShared.unset(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY);\n    confWithoutShared.setStrings(DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_KEY,\n        editsDirsString);\n    return confWithoutShared;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.format": "  private static boolean format(Configuration conf, boolean force,\n      boolean isInteractive) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    checkAllowFormat(conf);\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      InetSocketAddress socAddr = getAddress(conf);\n      SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n          DFS_NAMENODE_USER_NAME_KEY, socAddr.getHostName());\n    }\n    \n    Collection<URI> nameDirsToFormat = FSNamesystem.getNamespaceDirs(conf);\n    List<URI> sharedDirs = FSNamesystem.getSharedEditsDirs(conf);\n    List<URI> dirsToPrompt = new ArrayList<URI>();\n    dirsToPrompt.addAll(nameDirsToFormat);\n    dirsToPrompt.addAll(sharedDirs);\n    List<URI> editDirsToFormat = \n                 FSNamesystem.getNamespaceEditsDirs(conf);\n\n    // if clusterID is not provided - see if you can find the current one\n    String clusterId = StartupOption.FORMAT.getClusterId();\n    if(clusterId == null || clusterId.equals(\"\")) {\n      //Generate a new cluster id\n      clusterId = NNStorage.newClusterID();\n    }\n    System.out.println(\"Formatting using clusterid: \" + clusterId);\n    \n    FSImage fsImage = new FSImage(conf, nameDirsToFormat, editDirsToFormat);\n    try {\n      FSNamesystem fsn = new FSNamesystem(conf, fsImage);\n      fsImage.getEditLog().initJournalsForWrite();\n\n      if (!fsImage.confirmFormat(force, isInteractive)) {\n        return true; // aborted\n      }\n\n      fsImage.format(fsn, clusterId);\n    } catch (IOException ioe) {\n      LOG.warn(\"Encountered exception during format: \", ioe);\n      fsImage.close();\n      throw ioe;\n    }\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.copyEditLogSegmentsToSharedDir": "  private static void copyEditLogSegmentsToSharedDir(FSNamesystem fsns,\n      Collection<URI> sharedEditsDirs, NNStorage newSharedStorage,\n      Configuration conf) throws IOException {\n    Preconditions.checkArgument(!sharedEditsDirs.isEmpty(),\n        \"No shared edits specified\");\n    // Copy edit log segments into the new shared edits dir.\n    List<URI> sharedEditsUris = new ArrayList<URI>(sharedEditsDirs);\n    FSEditLog newSharedEditLog = new FSEditLog(conf, newSharedStorage,\n        sharedEditsUris);\n    newSharedEditLog.initJournalsForWrite();\n    newSharedEditLog.recoverUnclosedStreams();\n    \n    FSEditLog sourceEditLog = fsns.getFSImage().editLog;\n    \n    long fromTxId = fsns.getFSImage().getMostRecentCheckpointTxId();\n    \n    Collection<EditLogInputStream> streams = null;\n    try {\n      streams = sourceEditLog.selectInputStreams(fromTxId + 1, 0);\n\n      // Set the nextTxid to the CheckpointTxId+1\n      newSharedEditLog.setNextTxId(fromTxId + 1);\n\n      // Copy all edits after last CheckpointTxId to shared edits dir\n      for (EditLogInputStream stream : streams) {\n        LOG.debug(\"Beginning to copy stream \" + stream + \" to shared edits\");\n        FSEditLogOp op;\n        boolean segmentOpen = false;\n        while ((op = stream.readOp()) != null) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"copying op: \" + op);\n          }\n          if (!segmentOpen) {\n            newSharedEditLog.startLogSegment(op.txid, false);\n            segmentOpen = true;\n          }\n\n          newSharedEditLog.logEdit(op);\n\n          if (op.opCode == FSEditLogOpCodes.OP_END_LOG_SEGMENT) {\n            newSharedEditLog.logSync();\n            newSharedEditLog.endCurrentLogSegment(false);\n            LOG.debug(\"ending log segment because of END_LOG_SEGMENT op in \"\n                + stream);\n            segmentOpen = false;\n          }\n        }\n\n        if (segmentOpen) {\n          LOG.debug(\"ending log segment because of end of stream in \" + stream);\n          newSharedEditLog.logSync();\n          newSharedEditLog.endCurrentLogSegment(false);\n          segmentOpen = false;\n        }\n      }\n    } finally {\n      if (streams != null) {\n        FSEditLog.closeAllStreams(streams);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    LOG.info(\"createNameNode \" + Arrays.asList(argv));\n    if (conf == null)\n      conf = new HdfsConfiguration();\n    // Parse out some generic args into Configuration.\n    GenericOptionsParser hParser = new GenericOptionsParser(conf, argv);\n    argv = hParser.getRemainingArgs();\n    // Parse the rest, NN specific args.\n    StartupOption startOpt = parseArguments(argv);\n    if (startOpt == null) {\n      printUsage(System.err);\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted = format(conf, startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        terminate(0);\n        return null;\n      }\n      case FINALIZE: {\n        System.err.println(\"Use of the argument '\" + StartupOption.FINALIZE +\n            \"' is no longer supported. To finalize an upgrade, start the NN \" +\n            \" and then run `hdfs dfsadmin -finalizeUpgrade'\");\n        terminate(1);\n        return null; // avoid javac warning\n      }\n      case ROLLBACK: {\n        boolean aborted = doRollback(conf, true);\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] = Arrays.copyOfRange(argv, 1, argv.length);\n        int rc = BootstrapStandby.run(toolArgs, conf);\n        terminate(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted = initializeSharedEdits(conf,\n            startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role = startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      case RECOVER: {\n        NameNode.doRecovery(startOpt, conf);\n        return null;\n      }\n      default: {\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initialize": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) == null) {\n      String intervals = conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals != null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    if (NamenodeRole.NAMENODE == role) {\n      startHttpServer(conf);\n    }\n    loadNamesystem(conf);\n\n    rpcServer = createRpcServer(conf);\n    final String nsId = getNameServiceId(conf);\n    tokenServiceName = HAUtil.isHAEnabled(conf, nsId) ? nsId : NetUtils\n            .getHostPortString(rpcServer.getRpcAddress());\n    if (NamenodeRole.NAMENODE == role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n    \n    pauseMonitor = new JvmPauseMonitor(conf);\n    pauseMonitor.start();\n\n    startCommonServices(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.doRollback": "  public static boolean doRollback(Configuration conf,\n      boolean isConfirmationNeeded) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n\n    FSNamesystem nsys = new FSNamesystem(conf, new FSImage(conf));\n    System.err.print(\n        \"\\\"rollBack\\\" will remove the current state of the file system,\\n\"\n        + \"returning you to the state prior to initiating your recent.\\n\"\n        + \"upgrade. This action is permanent and cannot be undone. If you\\n\"\n        + \"are performing a rollback in an HA environment, you should be\\n\"\n        + \"certain that no NameNode process is running on any host.\");\n    if (isConfirmationNeeded) {\n      if (!confirmPrompt(\"Roll back file system state?\")) {\n        System.err.println(\"Rollback aborted.\");\n        return true;\n      }\n    }\n    nsys.dir.fsImage.doRollback(nsys);\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.printUsage": "  private static void printUsage(PrintStream out) {\n    out.println(USAGE + \"\\n\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.setStartupOption": "  private static void setStartupOption(Configuration conf, StartupOption opt) {\n    conf.set(DFS_NAMENODE_STARTUP_KEY, opt.name());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.run": "          public FileSystem run() throws IOException {\n            return FileSystem.get(conf);\n          }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.parseArguments": "  static StartupOption parseArguments(String args[]) {\n    int argsLen = (args == null) ? 0 : args.length;\n    StartupOption startOpt = StartupOption.REGULAR;\n    for(int i=0; i < argsLen; i++) {\n      String cmd = args[i];\n      if (StartupOption.FORMAT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.FORMAT;\n        for (i = i + 1; i < argsLen; i++) {\n          if (args[i].equalsIgnoreCase(StartupOption.CLUSTERID.getName())) {\n            i++;\n            if (i >= argsLen) {\n              // if no cluster id specified, return null\n              LOG.fatal(\"Must specify a valid cluster ID after the \"\n                  + StartupOption.CLUSTERID.getName() + \" flag\");\n              return null;\n            }\n            String clusterId = args[i];\n            // Make sure an id is specified and not another flag\n            if (clusterId.isEmpty() ||\n                clusterId.equalsIgnoreCase(StartupOption.FORCE.getName()) ||\n                clusterId.equalsIgnoreCase(\n                    StartupOption.NONINTERACTIVE.getName())) {\n              LOG.fatal(\"Must specify a valid cluster ID after the \"\n                  + StartupOption.CLUSTERID.getName() + \" flag\");\n              return null;\n            }\n            startOpt.setClusterId(clusterId);\n          }\n\n          if (args[i].equalsIgnoreCase(StartupOption.FORCE.getName())) {\n            startOpt.setForceFormat(true);\n          }\n\n          if (args[i].equalsIgnoreCase(StartupOption.NONINTERACTIVE.getName())) {\n            startOpt.setInteractiveFormat(false);\n          }\n        }\n      } else if (StartupOption.GENCLUSTERID.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.GENCLUSTERID;\n      } else if (StartupOption.REGULAR.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.REGULAR;\n      } else if (StartupOption.BACKUP.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.BACKUP;\n      } else if (StartupOption.CHECKPOINT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.CHECKPOINT;\n      } else if (StartupOption.UPGRADE.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.UPGRADE;\n        /* Can be followed by CLUSTERID with a required parameter or\n         * RENAMERESERVED with an optional parameter\n         */\n        while (i + 1 < argsLen) {\n          String flag = args[i + 1];\n          if (flag.equalsIgnoreCase(StartupOption.CLUSTERID.getName())) {\n            if (i + 2 < argsLen) {\n              i += 2;\n              startOpt.setClusterId(args[i]);\n            } else {\n              LOG.fatal(\"Must specify a valid cluster ID after the \"\n                  + StartupOption.CLUSTERID.getName() + \" flag\");\n              return null;\n            }\n          } else if (flag.equalsIgnoreCase(StartupOption.RENAMERESERVED\n              .getName())) {\n            if (i + 2 < argsLen) {\n              FSImageFormat.setRenameReservedPairs(args[i + 2]);\n              i += 2;\n            } else {\n              FSImageFormat.useDefaultRenameReservedPairs();\n              i += 1;\n            }\n          } else {\n            LOG.fatal(\"Unknown upgrade flag \" + flag);\n            return null;\n          }\n        }\n      } else if (StartupOption.ROLLINGUPGRADE.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.ROLLINGUPGRADE;\n        ++i;\n        startOpt.setRollingUpgradeStartupOption(args[i]);\n      } else if (StartupOption.ROLLBACK.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.ROLLBACK;\n      } else if (StartupOption.FINALIZE.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.FINALIZE;\n      } else if (StartupOption.IMPORT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.IMPORT;\n      } else if (StartupOption.BOOTSTRAPSTANDBY.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.BOOTSTRAPSTANDBY;\n        return startOpt;\n      } else if (StartupOption.INITIALIZESHAREDEDITS.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.INITIALIZESHAREDEDITS;\n        for (i = i + 1 ; i < argsLen; i++) {\n          if (StartupOption.NONINTERACTIVE.getName().equals(args[i])) {\n            startOpt.setInteractiveFormat(false);\n          } else if (StartupOption.FORCE.getName().equals(args[i])) {\n            startOpt.setForceFormat(true);\n          } else {\n            LOG.fatal(\"Invalid argument: \" + args[i]);\n            return null;\n          }\n        }\n        return startOpt;\n      } else if (StartupOption.RECOVER.getName().equalsIgnoreCase(cmd)) {\n        if (startOpt != StartupOption.REGULAR) {\n          throw new RuntimeException(\"Can't combine -recover with \" +\n              \"other startup options.\");\n        }\n        startOpt = StartupOption.RECOVER;\n        while (++i < argsLen) {\n          if (args[i].equalsIgnoreCase(\n                StartupOption.FORCE.getName())) {\n            startOpt.setForce(MetaRecoveryContext.FORCE_FIRST_CHOICE);\n          } else {\n            throw new RuntimeException(\"Error parsing recovery options: \" + \n              \"can't understand option \\\"\" + args[i] + \"\\\"\");\n          }\n        }\n      } else {\n        return null;\n      }\n    }\n    return startOpt;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.doRecovery": "  private static void doRecovery(StartupOption startOpt, Configuration conf)\n      throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    if (startOpt.getForce() < MetaRecoveryContext.FORCE_ALL) {\n      if (!confirmPrompt(\"You have selected Metadata Recovery mode.  \" +\n          \"This mode is intended to recover lost metadata on a corrupt \" +\n          \"filesystem.  Metadata recovery mode often permanently deletes \" +\n          \"data from your HDFS filesystem.  Please back up your edit log \" +\n          \"and fsimage before trying this!\\n\\n\" +\n          \"Are you ready to proceed? (Y/N)\\n\")) {\n        System.err.println(\"Recovery aborted at user request.\\n\");\n        return;\n      }\n    }\n    MetaRecoveryContext.LOG.info(\"starting recovery...\");\n    UserGroupInformation.setConfiguration(conf);\n    NameNode.initMetrics(conf, startOpt.toNodeRole());\n    FSNamesystem fsn = null;\n    try {\n      fsn = FSNamesystem.loadFromDisk(conf);\n      fsn.getFSImage().saveNamespace(fsn);\n      MetaRecoveryContext.LOG.info(\"RECOVERY COMPLETE\");\n    } catch (IOException e) {\n      MetaRecoveryContext.LOG.info(\"RECOVERY FAILED: caught exception\", e);\n      throw e;\n    } catch (RuntimeException e) {\n      MetaRecoveryContext.LOG.info(\"RECOVERY FAILED: caught exception\", e);\n      throw e;\n    } finally {\n      if (fsn != null)\n        fsn.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.main": "  public static void main(String argv[]) throws Exception {\n    if (DFSUtil.parseHelpArgument(argv, NameNode.USAGE, System.out, true)) {\n      System.exit(0);\n    }\n\n    try {\n      StringUtils.startupShutdownMessage(NameNode.class, argv, LOG);\n      NameNode namenode = createNameNode(argv, null);\n      if (namenode != null) {\n        namenode.join();\n      }\n    } catch (Throwable e) {\n      LOG.fatal(\"Exception in namenode join\", e);\n      terminate(1, e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.join": "  public void join() {\n    try {\n      rpcServer.join();\n    } catch (InterruptedException ie) {\n      LOG.info(\"Caught interrupted exception \", ie);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.getLocalName": "  public final String getLocalName() {\n    final byte[] name = getLocalNameBytes();\n    return name == null? null: DFSUtil.bytes2String(name);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.asFile": "  public INodeFile asFile() {\n    throw new IllegalStateException(\"Current inode is not a file: \"\n        + this.toDetailString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.toDetailString": "  public String toDetailString() {\n    return toString() + \"(\" + getObjectString() + \"), \" + getParentString();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.isFile": "  public boolean isFile() {\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageUtil.checkFileFormat": "  public static boolean checkFileFormat(RandomAccessFile file)\n      throws IOException {\n    if (file.length() < Loader.MINIMUM_FILE_LENGTH)\n      return false;\n\n    byte[] magic = new byte[MAGIC_HEADER.length];\n    file.readFully(magic);\n    if (!Arrays.equals(MAGIC_HEADER, magic))\n      return false;\n\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageUtil.loadSummary": "  public static FileSummary loadSummary(RandomAccessFile file)\n      throws IOException {\n    final int FILE_LENGTH_FIELD_SIZE = 4;\n    long fileLength = file.length();\n    file.seek(fileLength - FILE_LENGTH_FIELD_SIZE);\n    int summaryLength = file.readInt();\n\n    if (summaryLength <= 0) {\n      throw new IOException(\"Negative length of the file\");\n    }\n    file.seek(fileLength - FILE_LENGTH_FIELD_SIZE - summaryLength);\n\n    byte[] summaryBytes = new byte[summaryLength];\n    file.readFully(summaryBytes);\n\n    FileSummary summary = FileSummary\n        .parseDelimitedFrom(new ByteArrayInputStream(summaryBytes));\n    if (summary.getOndiskVersion() != FILE_VERSION) {\n      throw new IOException(\"Unsupported file version \"\n          + summary.getOndiskVersion());\n    }\n\n    if (!NameNodeLayoutVersion.supports(Feature.PROTOBUF_FORMAT,\n        summary.getLayoutVersion())) {\n      throw new IOException(\"Unsupported layout version \"\n          + summary.getLayoutVersion());\n    }\n    return summary;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode.loadFilesUnderConstructionSection": "    void loadFilesUnderConstructionSection(InputStream in) throws IOException {\n      while (true) {\n        FileUnderConstructionEntry entry = FileUnderConstructionEntry\n            .parseDelimitedFrom(in);\n        if (entry == null) {\n          break;\n        }\n        // update the lease manager\n        INodeFile file = dir.getInode(entry.getInodeId()).asFile();\n        FileUnderConstructionFeature uc = file.getFileUnderConstructionFeature();\n        Preconditions.checkState(uc != null); // file must be under-construction\n        fsn.leaseManager.addLease(uc.getClientName(), entry.getFullPath());\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getStartupProgress": "  public static StartupProgress getStartupProgress() {\n    return startupProgress;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageUtil.wrapInputStreamForCompression": "  public static InputStream wrapInputStreamForCompression(\n      Configuration conf, String codec, InputStream in) throws IOException {\n    if (codec.isEmpty())\n      return in;\n\n    FSImageCompression compression = FSImageCompression.createCompression(\n        conf, codec);\n    CompressionCodec imageCodec = compression.getImageCodec();\n    return imageCodec.createInputStream(in);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode.loadINodeSection": "    void loadINodeSection(InputStream in) throws IOException {\n      INodeSection s = INodeSection.parseDelimitedFrom(in);\n      fsn.resetLastInodeId(s.getLastInodeId());\n      LOG.info(\"Loading \" + s.getNumInodes() + \" INodes.\");\n      for (int i = 0; i < s.getNumInodes(); ++i) {\n        INodeSection.INode p = INodeSection.INode.parseDelimitedFrom(in);\n        if (p.getId() == INodeId.ROOT_INODE_ID) {\n          loadRootINode(p);\n        } else {\n          INode n = loadINode(p);\n          dir.addToInodeMap(n);\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode.loadINode": "    private INode loadINode(INodeSection.INode n) {\n      switch (n.getType()) {\n      case FILE:\n        return loadINodeFile(n);\n      case DIRECTORY:\n        return loadINodeDirectory(n, parent.getLoaderContext());\n      case SYMLINK:\n        return loadINodeSymlink(n);\n      default:\n        break;\n      }\n      return null;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode.loadRootINode": "    private void loadRootINode(INodeSection.INode p) {\n      INodeDirectory root = loadINodeDirectory(p, parent.getLoaderContext());\n      final Quota.Counts q = root.getQuotaCounts();\n      final long nsQuota = q.get(Quota.NAMESPACE);\n      final long dsQuota = q.get(Quota.DISKSPACE);\n      if (nsQuota != -1 || dsQuota != -1) {\n        dir.rootDir.getDirectoryWithQuotaFeature().setQuota(nsQuota, dsQuota);\n      }\n      dir.rootDir.cloneModificationTime(root);\n      dir.rootDir.clonePermissionStatus(root);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditLogStreams": "  static Iterable<EditLogInputStream> getEditLogStreams(NNStorage storage)\n      throws IOException {\n    FSImagePreTransactionalStorageInspector inspector \n      = new FSImagePreTransactionalStorageInspector();\n    storage.inspectStorageDirs(inspector);\n\n    List<EditLogInputStream> editStreams = new ArrayList<EditLogInputStream>();\n    for (File f : inspector.getLatestEditsFiles()) {\n      editStreams.add(new EditLogFileInputStream(f));\n    }\n    return editStreams;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getLatestEditsFiles": "  private List<File> getLatestEditsFiles() {\n    if (latestNameCheckpointTime > latestEditsCheckpointTime) {\n      // the image is already current, discard edits\n      LOG.debug(\n          \"Name checkpoint time is newer than edits, not loading edits.\");\n      return Collections.<File>emptyList();\n    }\n    \n    return getEditsInStorageDir(latestEditsSD);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.closeAllStreams": "  static void closeAllStreams(Iterable<EditLogInputStream> streams) {\n    for (EditLogInputStream s : streams) {\n      IOUtils.closeStream(s);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getLatestImages": "  abstract List<FSImageFile> getLatestImages() throws IOException;\n\n  /** \n   * Get the minimum tx id which should be loaded with this set of images.\n   */\n  abstract long getMaxSeenTxId();\n\n  /**\n   * @return true if the directories are in such a state that the image should be re-saved\n   * following the load\n   */\n  abstract boolean needToSave();\n\n  /**\n   * Record of an image that has been located and had its filename parsed.\n   */\n  static class FSImageFile {\n    final StorageDirectory sd;    \n    final long txId;\n    private final File file;\n    \n    FSImageFile(StorageDirectory sd, File file, long txId) {\n      assert txId >= 0 || txId == HdfsConstants.INVALID_TXID \n        : \"Invalid txid on \" + file +\": \" + txId;\n      \n      this.sd = sd;\n      this.txId = txId;\n      this.file = file;\n    } \n    \n    File getFile() {\n      return file;\n    }\n\n    public long getCheckpointTxId() {\n      return txId;\n    }\n    \n    @Override\n    public String toString() {\n      return String.format(\"FSImageFile(file=%s, cpktTxId=%019d)\", \n                           file.toString(), txId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion.supports": "  public static boolean supports(final LayoutFeature f, final int lv) {\n    return LayoutVersion.supports(FEATURES, f, lv);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.newLoader": "  public static LoaderDelegator newLoader(Configuration conf, FSNamesystem fsn) {\n    return new LoaderDelegator(conf, fsn);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.needToSave": "  abstract boolean needToSave();\n\n  /**\n   * Record of an image that has been located and had its filename parsed.\n   */\n  static class FSImageFile {\n    final StorageDirectory sd;    \n    final long txId;\n    private final File file;\n    \n    FSImageFile(StorageDirectory sd, File file, long txId) {\n      assert txId >= 0 || txId == HdfsConstants.INVALID_TXID \n        : \"Invalid txid on \" + file +\": \" + txId;\n      \n      this.sd = sd;\n      this.txId = txId;\n      this.file = file;\n    } \n    \n    File getFile() {\n      return file;\n    }\n\n    public long getCheckpointTxId() {\n      return txId;\n    }\n    \n    @Override\n    public String toString() {\n      return String.format(\"FSImageFile(file=%s, cpktTxId=%019d)\", \n                           file.toString(), txId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getMaxSeenTxId": "  abstract long getMaxSeenTxId();\n\n  /**\n   * @return true if the directories are in such a state that the image should be re-saved\n   * following the load\n   */\n  abstract boolean needToSave();\n\n  /**\n   * Record of an image that has been located and had its filename parsed.\n   */\n  static class FSImageFile {\n    final StorageDirectory sd;    \n    final long txId;\n    private final File file;\n    \n    FSImageFile(StorageDirectory sd, File file, long txId) {\n      assert txId >= 0 || txId == HdfsConstants.INVALID_TXID \n        : \"Invalid txid on \" + file +\": \" + txId;\n      \n      this.sd = sd;\n      this.txId = txId;\n      this.file = file;\n    } \n    \n    File getFile() {\n      return file;\n    }\n\n    public long getCheckpointTxId() {\n      return txId;\n    }\n    \n    @Override\n    public String toString() {\n      return String.format(\"FSImageFile(file=%s, cpktTxId=%019d)\", \n                           file.toString(), txId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.format": "  void format(FSNamesystem fsn, String clusterId) throws IOException {\n    long fileCount = fsn.getTotalFiles();\n    // Expect 1 file, which is the root inode\n    Preconditions.checkState(fileCount == 1,\n        \"FSImage.format should be called with an uninitialized namesystem, has \" +\n        fileCount + \" files\");\n    NamespaceInfo ns = NNStorage.newNamespaceInfo();\n    LOG.info(\"Allocated new BlockPoolId: \" + ns.getBlockPoolID());\n    ns.clusterID = clusterId;\n    \n    storage.format(ns);\n    editLog.formatNonFileJournals(ns);\n    saveFSImageInAllDirs(fsn, 0);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImageInAllDirs": "  private synchronized void saveFSImageInAllDirs(FSNamesystem source,\n      NameNodeFile nnf, long txid, Canceler canceler) throws IOException {\n    StartupProgress prog = NameNode.getStartupProgress();\n    prog.beginPhase(Phase.SAVING_CHECKPOINT);\n    if (storage.getNumStorageDirs(NameNodeDirType.IMAGE) == 0) {\n      throw new IOException(\"No image directories available!\");\n    }\n    if (canceler == null) {\n      canceler = new Canceler();\n    }\n    SaveNamespaceContext ctx = new SaveNamespaceContext(\n        source, txid, canceler);\n    \n    try {\n      List<Thread> saveThreads = new ArrayList<Thread>();\n      // save images into current\n      for (Iterator<StorageDirectory> it\n             = storage.dirIterator(NameNodeDirType.IMAGE); it.hasNext();) {\n        StorageDirectory sd = it.next();\n        FSImageSaver saver = new FSImageSaver(ctx, sd, nnf);\n        Thread saveThread = new Thread(saver, saver.toString());\n        saveThreads.add(saveThread);\n        saveThread.start();\n      }\n      waitForThreads(saveThreads);\n      saveThreads.clear();\n      storage.reportErrorsOnDirectories(ctx.getErrorSDs());\n  \n      if (storage.getNumStorageDirs(NameNodeDirType.IMAGE) == 0) {\n        throw new IOException(\n          \"Failed to save in any storage directories while saving namespace.\");\n      }\n      if (canceler.isCancelled()) {\n        deleteCancelledCheckpoint(txid);\n        ctx.checkCancelled(); // throws\n        assert false : \"should have thrown above!\";\n      }\n  \n      renameCheckpoint(txid, NameNodeFile.IMAGE_NEW, nnf, false);\n  \n      // Since we now have a new checkpoint, we can clean up some\n      // old edit logs and checkpoints.\n      purgeOldStorage(nnf);\n    } finally {\n      // Notify any threads waiting on the checkpoint to be canceled\n      // that it is complete.\n      ctx.markComplete();\n      ctx = null;\n    }\n    prog.endPhase(Phase.SAVING_CHECKPOINT);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getStorage": "  public NNStorage getStorage() {\n    return storage;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.openEditLogForWrite": "  void openEditLogForWrite() throws IOException {\n    assert editLog != null : \"editLog must be initialized\";\n    editLog.openForWrite();\n    storage.writeTransactionIdFileToStorage(editLog.getCurSegmentTxId());\n  };",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getStartupOption": "  static StartupOption getStartupOption(Configuration conf) {\n    return StartupOption.valueOf(conf.get(DFS_NAMENODE_STARTUP_KEY,\n                                          StartupOption.REGULAR.toString()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getNameNodeMetrics": "  public static NameNodeMetrics getNameNodeMetrics() {\n    return metrics;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getEditLog": "  public FSEditLog getEditLog() {\n    return editLog;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.confirmFormat": "  boolean confirmFormat(boolean force, boolean interactive) throws IOException {\n    List<FormatConfirmable> confirms = Lists.newArrayList();\n    for (StorageDirectory sd : storage.dirIterable(null)) {\n      confirms.add(sd);\n    }\n    \n    confirms.addAll(editLog.getFormatConfirmables());\n    return Storage.confirmFormat(confirms, force, interactive);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.getNamespaceInfo": "  public NamespaceInfo getNamespaceInfo() {\n    return new NamespaceInfo(\n        getNamespaceID(),\n        getClusterID(),\n        getBlockPoolID(),\n        getCTime());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.getBlockPoolID": "  public String getBlockPoolID() {\n    return blockpoolID;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.close": "  synchronized public void close() throws IOException {\n    if (editLog != null) { // 2NN doesn't have any edit log\n      getEditLog().close();\n    }\n    storage.close();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getSharedEditsDirs": "  public static List<URI> getSharedEditsDirs(Configuration conf) {\n    // don't use getStorageDirs here, because we want an empty default\n    // rather than the dir in /tmp\n    Collection<String> dirNames = conf.getTrimmedStringCollection(\n        DFS_NAMENODE_SHARED_EDITS_DIR_KEY);\n    return Util.stringCollectionAsURIs(dirNames);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.newClusterID": "  public static String newClusterID() {\n    return \"CID-\" + UUID.randomUUID().toString();\n  }"
        },
        "bug_report": {
            "Title": "NPE when upgrading namenode from fsimages older than -32",
            "Description": "I want upgrade an old cluster(0.20.2-cdh3u1) to trunk instance, \n\nI can upgrade successfully if I don't configurage HA, but if HA enabled,\nthere is NPE when I run ' hdfs namenode -initializeSharedEdits'\n\n{code}\n14/03/20 15:06:41 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n14/03/20 15:06:41 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n14/03/20 15:06:41 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n14/03/20 15:06:41 INFO util.GSet: VM type       = 64-bit\n14/03/20 15:06:41 INFO util.GSet: 0.029999999329447746% max memory 896 MB = 275.3 KB\n14/03/20 15:06:41 INFO util.GSet: capacity      = 2^15 = 32768 entries\n14/03/20 15:06:41 INFO namenode.AclConfigFlag: ACLs enabled? false\n14/03/20 15:06:41 INFO common.Storage: Lock on /data/hadoop/data1/dfs/name/in_use.lock acquired by nodename 7326@10-150-170-176\n14/03/20 15:06:42 INFO common.Storage: Lock on /data/hadoop/data2/dfs/name/in_use.lock acquired by nodename 7326@10-150-170-176\n14/03/20 15:06:42 INFO namenode.FSImage: No edit log streams selected.\n14/03/20 15:06:42 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.\n14/03/20 15:06:42 FATAL namenode.NameNode: Exception in namenode join\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.isReservedName(FSDirectory.java:2984)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.addToParent(FSImageFormatPBINode.java:205)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeDirectorySection(FSImageFormatPBINode.java:162)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.loadInternal(FSImageFormatProtobuf.java:243)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:168)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.load(FSImageFormat.java:120)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:895)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:881)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:704)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:271)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:894)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:653)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits(NameNode.java:912)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1276)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1360)\n14/03/20 15:06:42 INFO util.ExitUtil: Exiting with status 1\n14/03/20 15:06:42 INFO namenode.NameNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down NameNode at 10-150-170-176/10.150.170.176\n************************************************************/\n{code}\n"
        }
    },
    {
        "filename": "HDFS-2827.json",
        "creation_time": "2012-01-23T09:34:51.000+0000",
        "stack_trace": "```\njava.io.IOException: saveLeases found path /test1/est/abc.txt but no matching entry in namespace.[/test1/est/abc.txt]\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveFilesUnderConstruction(FSNamesystem.java:4336)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver.save(FSImageFormat.java:588)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImage(FSImage.java:761)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.run(FSImage.java:789)\n\tat java.lang.Thread.run(Unknown Source)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveFilesUnderConstruction": "  void saveFilesUnderConstruction(DataOutputStream out) throws IOException {\n    // This is run by an inferior thread of saveNamespace, which holds a read\n    // lock on our behalf. If we took the read lock here, we could block\n    // for fairness if a writer is waiting on the lock.\n    synchronized (leaseManager) {\n      out.writeInt(leaseManager.countPath()); // write the size\n\n      for (Lease lease : leaseManager.getSortedLeases()) {\n        for(String path : lease.getPaths()) {\n          // verify that path exists in namespace\n          INode node;\n          try {\n            node = dir.getFileINode(path);\n          } catch (UnresolvedLinkException e) {\n            throw new AssertionError(\"Lease files should reside on this FS\");\n          }\n          if (node == null) {\n            throw new IOException(\"saveLeases found path \" + path +\n                                  \" but no matching entry in namespace.\");\n          }\n          if (!node.isUnderConstruction()) {\n            throw new IOException(\"saveLeases found path \" + path +\n                                  \" but is not under construction.\");\n          }\n          INodeFileUnderConstruction cons = (INodeFileUnderConstruction) node;\n          FSImageSerialization.writeINodeUnderConstruction(out, cons, path);\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.save": "    void save(File newFile,\n              FSImageCompression compression)\n      throws IOException {\n      checkNotSaved();\n\n      final FSNamesystem sourceNamesystem = context.getSourceNamesystem();\n      FSDirectory fsDir = sourceNamesystem.dir;\n      long startTime = now();\n      //\n      // Write out data\n      //\n      MessageDigest digester = MD5Hash.getDigester();\n      FileOutputStream fout = new FileOutputStream(newFile);\n      DigestOutputStream fos = new DigestOutputStream(fout, digester);\n      DataOutputStream out = new DataOutputStream(fos);\n      try {\n        out.writeInt(HdfsConstants.LAYOUT_VERSION);\n        // We use the non-locked version of getNamespaceInfo here since\n        // the coordinating thread of saveNamespace already has read-locked\n        // the namespace for us. If we attempt to take another readlock\n        // from the actual saver thread, there's a potential of a\n        // fairness-related deadlock. See the comments on HDFS-2223.\n        out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n            .getNamespaceID());\n        out.writeLong(fsDir.rootDir.numItemsInTree());\n        out.writeLong(sourceNamesystem.getGenerationStamp());\n        out.writeLong(context.getTxId());\n\n        // write compression info and set up compressed stream\n        out = compression.writeHeaderAndWrapStream(fos);\n        LOG.info(\"Saving image file \" + newFile +\n                 \" using \" + compression);\n\n\n        byte[] byteStore = new byte[4*HdfsConstants.MAX_PATH_LENGTH];\n        ByteBuffer strbuf = ByteBuffer.wrap(byteStore);\n        // save the root\n        FSImageSerialization.saveINode2Image(fsDir.rootDir, out);\n        // save the rest of the nodes\n        saveImage(strbuf, fsDir.rootDir, out);\n        // save files under construction\n        sourceNamesystem.saveFilesUnderConstruction(out);\n        context.checkCancelled();\n        sourceNamesystem.saveSecretManagerState(out);\n        strbuf = null;\n        context.checkCancelled();\n        out.flush();\n        context.checkCancelled();\n        fout.getChannel().force(true);\n      } finally {\n        out.close();\n      }\n\n      saved = true;\n      // set md5 of the saved image\n      savedDigest = new MD5Hash(digester.digest());\n\n      LOG.info(\"Image file of size \" + newFile.length() + \" saved in \" \n          + (now() - startTime)/1000 + \" seconds.\");\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.saveImage": "    private void saveImage(ByteBuffer currentDirName,\n                                  INodeDirectory current,\n                                  DataOutputStream out) throws IOException {\n      context.checkCancelled();\n      List<INode> children = current.getChildrenRaw();\n      if (children == null || children.isEmpty())\n        return;\n      // print prefix (parent directory name)\n      int prefixLen = currentDirName.position();\n      if (prefixLen == 0) {  // root\n        out.writeShort(PATH_SEPARATOR.length);\n        out.write(PATH_SEPARATOR);\n      } else {  // non-root directories\n        out.writeShort(prefixLen);\n        out.write(currentDirName.array(), 0, prefixLen);\n      }\n      out.writeInt(children.size());\n      for(INode child : children) {\n        // print all children first\n        FSImageSerialization.saveINode2Image(child, out);\n      }\n      for(INode child : children) {\n        if(!child.isDirectory())\n          continue;\n        currentDirName.put(PATH_SEPARATOR).put(child.getLocalNameBytes());\n        saveImage(currentDirName, (INodeDirectory)child, out);\n        currentDirName.position(prefixLen);\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.checkNotSaved": "    private void checkNotSaved() {\n      if (saved) {\n        throw new IllegalStateException(\"FSImageSaver has already saved an image\");\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImage": "  void saveFSImage(SaveNamespaceContext context, StorageDirectory sd)\n      throws IOException {\n    long txid = context.getTxId();\n    File newFile = NNStorage.getStorageFile(sd, NameNodeFile.IMAGE_NEW, txid);\n    File dstFile = NNStorage.getStorageFile(sd, NameNodeFile.IMAGE, txid);\n    \n    FSImageFormat.Saver saver = new FSImageFormat.Saver(context);\n    FSImageCompression compression = FSImageCompression.createCompression(conf);\n    saver.save(newFile, compression);\n    \n    MD5FileUtils.saveMD5File(dstFile, saver.getSavedDigest());\n    storage.setMostRecentCheckpointTxId(txid);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.run": "    public void run() {\n      try {\n        saveFSImage(context, sd);\n      } catch (SaveNamespaceCancelledException snce) {\n        LOG.info(\"Cancelled image saving for \" + sd.getRoot() +\n            \": \" + snce.getMessage());\n        // don't report an error on the storage dir!\n      } catch (Throwable t) {\n        LOG.error(\"Unable to save image for \" + sd.getRoot(), t);\n        context.reportErrorOnStorageDirectory(sd);\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageSerialization.writeINodeUnderConstruction": "  static void writeINodeUnderConstruction(DataOutputStream out,\n                                           INodeFileUnderConstruction cons,\n                                           String path) \n                                           throws IOException {\n    writeString(path, out);\n    out.writeShort(cons.getReplication());\n    out.writeLong(cons.getModificationTime());\n    out.writeLong(cons.getPreferredBlockSize());\n    int nrBlocks = cons.getBlocks().length;\n    out.writeInt(nrBlocks);\n    for (int i = 0; i < nrBlocks; i++) {\n      cons.getBlocks()[i].write(out);\n    }\n    cons.getPermissionStatus().write(out);\n    writeString(cons.getClientName(), out);\n    writeString(cons.getClientMachine(), out);\n\n    out.writeInt(0); //  do not store locations of last block\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageSerialization.write": "    public void write(DataOutput out) throws IOException {\n      new DatanodeID(node).write(out);\n      out.writeLong(node.getCapacity());\n      out.writeLong(node.getRemaining());\n      out.writeLong(node.getLastUpdate());\n      out.writeInt(node.getXceiverCount());\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageSerialization.writeString": "  static void writeString(String str, DataOutputStream out) throws IOException {\n    DeprecatedUTF8 ustr = TL_DATA.get().U_STR;\n    ustr.set(str);\n    ustr.write(out);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageSerialization.writeLong": "  static void writeLong(long value, DataOutputStream out) throws IOException {\n    LongWritable uLong = TL_DATA.get().U_LONG;\n    uLong.set(value);\n    uLong.write(out);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageSerialization.writeShort": "  static void writeShort(short value, DataOutputStream out) throws IOException {\n    ShortWritable uShort = TL_DATA.get().U_SHORT;\n    uShort.set(value);\n    uShort.write(out);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.leaseManager.getSortedLeases": "  SortedSet<Lease> getSortedLeases() {return sortedLeases;}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.leaseManager.countPath": "  synchronized int countPath() {\n    int count = 0;\n    for(Lease lease : sortedLeases) {\n      count += lease.getPaths().size();\n    }\n    return count;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.leaseManager.getPaths": "    Collection<String> getPaths() {\n      return paths;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getGenerationStamp": "  long getGenerationStamp() {\n    return generationStamp.getStamp();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageCompression.writeHeaderAndWrapStream": "  DataOutputStream writeHeaderAndWrapStream(OutputStream os)\n  throws IOException {\n    DataOutputStream dos = new DataOutputStream(os);\n\n    dos.writeBoolean(imageCodec != null);\n\n    if (imageCodec != null) {\n      String codecClassName = imageCodec.getClass().getCanonicalName();\n      Text.writeString(dos, codecClassName);\n\n      return new DataOutputStream(imageCodec.createOutputStream(os));\n    } else {\n      // use a buffered output stream\n      return new DataOutputStream(new BufferedOutputStream(os));\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveSecretManagerState": "  void saveSecretManagerState(DataOutputStream out) throws IOException {\n    dtSecretManager.saveSecretManagerState(out);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageSerialization.saveINode2Image": "  static void saveINode2Image(INode node,\n                              DataOutputStream out) throws IOException {\n    byte[] name = node.getLocalNameBytes();\n    out.writeShort(name.length);\n    out.write(name);\n    FsPermission filePerm = TL_DATA.get().FILE_PERM;\n    if (node.isDirectory()) {\n      out.writeShort(0);  // replication\n      out.writeLong(node.getModificationTime());\n      out.writeLong(0);   // access time\n      out.writeLong(0);   // preferred block size\n      out.writeInt(-1);   // # of blocks\n      out.writeLong(node.getNsQuota());\n      out.writeLong(node.getDsQuota());\n      filePerm.fromShort(node.getFsPermissionShort());\n      PermissionStatus.write(out, node.getUserName(),\n                             node.getGroupName(),\n                             filePerm);\n    } else if (node.isLink()) {\n      out.writeShort(0);  // replication\n      out.writeLong(0);   // modification time\n      out.writeLong(0);   // access time\n      out.writeLong(0);   // preferred block size\n      out.writeInt(-2);   // # of blocks\n      Text.writeString(out, ((INodeSymlink)node).getLinkValue());\n      filePerm.fromShort(node.getFsPermissionShort());\n      PermissionStatus.write(out, node.getUserName(),\n                             node.getGroupName(),\n                             filePerm);      \n    } else {\n      INodeFile fileINode = (INodeFile)node;\n      out.writeShort(fileINode.getReplication());\n      out.writeLong(fileINode.getModificationTime());\n      out.writeLong(fileINode.getAccessTime());\n      out.writeLong(fileINode.getPreferredBlockSize());\n      Block[] blocks = fileINode.getBlocks();\n      out.writeInt(blocks.length);\n      for (Block blk : blocks)\n        blk.write(out);\n      filePerm.fromShort(fileINode.getFsPermissionShort());\n      PermissionStatus.write(out, fileINode.getUserName(),\n                             fileINode.getGroupName(),\n                             filePerm);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.unprotectedGetNamespaceInfo": "  NamespaceInfo unprotectedGetNamespaceInfo() {\n    return new NamespaceInfo(dir.fsImage.getStorage().getNamespaceID(),\n        getClusterId(), getBlockPoolId(),\n        dir.fsImage.getStorage().getCTime(),\n        upgradeManager.getUpgradeVersion());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockPoolId": "  public String getBlockPoolId() {\n    return blockPoolId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getClusterId": "  public String getClusterId() {\n    return dir.fsImage.getStorage().getClusterID();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageCompression.createCompression": "  private static FSImageCompression createCompression(Configuration conf,\n                                                      String codecClassName)\n    throws IOException {\n\n    CompressionCodecFactory factory = new CompressionCodecFactory(conf);\n    CompressionCodec codec = factory.getCodecByClassName(codecClassName);\n    if (codec == null) {\n      throw new IOException(\"Not a supported codec: \" + codecClassName);\n    }\n\n    return new FSImageCompression(codec);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageCompression.createNoopCompression": "  static FSImageCompression createNoopCompression() {\n    return new FSImageCompression();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.getSavedDigest": "    MD5Hash getSavedDigest() {\n      checkSaved();\n      return savedDigest;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.checkSaved": "    private void checkSaved() {\n      if (!saved) {\n        throw new IllegalStateException(\"FSImageSaver has not saved an image\");\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.getStorageFile": "  static File getStorageFile(StorageDirectory sd, NameNodeFile type) {\n    return new File(sd.getCurrentDir(), type.getName());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.format": "  public void format(String clusterId) throws IOException {\n    this.layoutVersion = HdfsConstants.LAYOUT_VERSION;\n    this.namespaceID = newNamespaceID();\n    this.clusterID = clusterId;\n    this.blockpoolID = newBlockPoolID();\n    this.cTime = 0L;\n    for (Iterator<StorageDirectory> it =\n                           dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      format(sd);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.getName": "    String getName() { return fileName; }"
        },
        "bug_report": {
            "Title": "Cannot save namespace after renaming a directory above a file with an open lease",
            "Description": "When i execute the following operations and wait for checkpoint to complete.\n\nfs.mkdirs(new Path(\"/test1\"));\nFSDataOutputStream create = fs.create(new Path(\"/test/abc.txt\")); //dont close\nfs.rename(new Path(\"/test/\"), new Path(\"/test1/\"));\n\nCheck-pointing is failing with the following exception.\n\n2012-01-23 15:03:14,204 ERROR namenode.FSImage (FSImage.java:run(795)) - Unable to save image for E:\\HDFS-1623\\hadoop-hdfs-project\\hadoop-hdfs\\build\\test\\data\\dfs\\name3\njava.io.IOException: saveLeases found path /test1/est/abc.txt but no matching entry in namespace.[/test1/est/abc.txt]\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveFilesUnderConstruction(FSNamesystem.java:4336)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver.save(FSImageFormat.java:588)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImage(FSImage.java:761)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.run(FSImage.java:789)\n\tat java.lang.Thread.run(Unknown Source)"
        }
    },
    {
        "filename": "HDFS-11056.json",
        "creation_time": "2016-10-25T23:00:33.000+0000",
        "stack_trace": "```\njava.io.IOException: No data exists for block BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182\n\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getBlockInputStream(FsDatasetImpl.java:773)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:400)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:581)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:150)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:102)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:289)\n\tat java.lang.Thread.run(Thread.java:745)\n\njava.io.IOException: No data exists for block BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182\n\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getBlockInputStream(FsDatasetImpl.java:773)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:400)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:581)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:150)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:102)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:289)\n\tat java.lang.Thread.run(Thread.java:745)\n\nException in thread \"Thread-144\" java.lang.RuntimeException: org.apache.hadoop.fs.ChecksumException: Checksum CRC32C not matched for file /tmp/bar.txt at position 0: expected=C893FEDE but computed=69322F90, algorithm=PureJavaCrc32C\n\tat org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1ReaderRunnable.run(BaseTestHttpFSWith.java:309)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.fs.ChecksumException: Checksum CRC32C not matched for file /tmp/bar.txt at position 0: expected=C893FEDE but computed=69322F90, algorithm=PureJavaCrc32C\n\tat org.apache.hadoop.util.DataChecksum.throwChecksumException(DataChecksum.java:407)\n\tat org.apache.hadoop.util.DataChecksum.verifyChunked(DataChecksum.java:351)\n\tat org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:311)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.readNextPacket(BlockReaderRemote.java:216)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.read(BlockReaderRemote.java:144)\n\tat org.apache.hadoop.hdfs.ByteArrayStrategy.readFromBlock(ReaderStrategy.java:119)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:704)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:765)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:814)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1ReaderRunnable.run(BaseTestHttpFSWith.java:302)\n\t... 1 more\n\njava.io.IOException: append: lastBlock=blk_1073741825_1185 of src=/tmp/bar.txt is not sufficiently replicated yet.\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:136)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2423)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:773)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:444)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:467)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:990)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:845)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:788)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1795)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2535)\n\nException in thread \"Thread-143\" java.lang.RuntimeException: java.io.IOException: HTTP status [500], exception [org.apache.hadoop.ipc.RemoteException], message [append: lastBlock=blk_1073741825_1185 of src=/tmp/bar.txt is not sufficiently replicated yet.] \n\tat org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1.run(BaseTestHttpFSWith.java:283)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: HTTP status [500], exception [org.apache.hadoop.ipc.RemoteException], message [append: lastBlock=blk_1073741825_1185 of src=/tmp/bar.txt is not sufficiently replicated yet.] \n\tat org.apache.hadoop.util.HttpExceptionUtils.validateResponse(HttpExceptionUtils.java:159)\n\tat org.apache.hadoop.fs.http.client.HttpFSFileSystem$HttpFSDataOutputStream.close(HttpFSFileSystem.java:470)\n\tat org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1.run(BaseTestHttpFSWith.java:279)```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getBlockInputStream": "  public InputStream getBlockInputStream(ExtendedBlock b,\n      long seekOffset) throws IOException {\n\n    ReplicaInfo info;\n    synchronized(this) {\n      info = volumeMap.get(b.getBlockPoolId(), b.getLocalBlock());\n    }\n\n    if (info != null && info.getVolume().isTransientStorage()) {\n      ramDiskReplicaTracker.touch(b.getBlockPoolId(), b.getBlockId());\n      datanode.getMetrics().incrRamDiskBlocksReadHits();\n    }\n\n    if(info != null && info.blockDataExists()) {\n      return info.getDataInputStream(seekOffset);\n    } else {\n      throw new IOException(\"No data exists for block \" + b);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getVolume": "  public FsVolumeImpl getVolume(final ExtendedBlock b) {\n    try (AutoCloseableLock lock = datasetLock.acquire()) {\n      final ReplicaInfo r =\n          volumeMap.get(b.getBlockPoolId(), b.getLocalBlock());\n      return r != null ? (FsVolumeImpl) r.getVolume() : null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getMetrics": "  public void getMetrics(MetricsCollector collector, boolean all) {\n    try {\n      DataNodeMetricHelper.getMetrics(collector, this, \"FSDatasetState\");\n    } catch (Exception e) {\n        LOG.warn(\"Exception thrown while metric collection. Exception : \"\n          + e.getMessage());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock": "  public void readBlock(final ExtendedBlock block,\n      final Token<BlockTokenIdentifier> blockToken,\n      final String clientName,\n      final long blockOffset,\n      final long length,\n      final boolean sendChecksum,\n      final CachingStrategy cachingStrategy) throws IOException {\n    previousOpClientName = clientName;\n    long read = 0;\n    updateCurrentThreadName(\"Sending block \" + block);\n    OutputStream baseStream = getOutputStream();\n    DataOutputStream out = getBufferedOutputStream();\n    checkAccess(out, true, block, blockToken,\n        Op.READ_BLOCK, BlockTokenIdentifier.AccessMode.READ);\n  \n    // send the block\n    BlockSender blockSender = null;\n    DatanodeRegistration dnR = \n      datanode.getDNRegistrationForBP(block.getBlockPoolId());\n    final String clientTraceFmt =\n      clientName.length() > 0 && ClientTraceLog.isInfoEnabled()\n        ? String.format(DN_CLIENTTRACE_FORMAT, localAddress, remoteAddress,\n            \"%d\", \"HDFS_READ\", clientName, \"%d\",\n            dnR.getDatanodeUuid(), block, \"%d\")\n        : dnR + \" Served block \" + block + \" to \" +\n            remoteAddress;\n\n    try {\n      try {\n        blockSender = new BlockSender(block, blockOffset, length,\n            true, false, sendChecksum, datanode, clientTraceFmt,\n            cachingStrategy);\n      } catch(IOException e) {\n        String msg = \"opReadBlock \" + block + \" received exception \" + e; \n        LOG.info(msg);\n        sendResponse(ERROR, msg);\n        throw e;\n      }\n      \n      // send op status\n      writeSuccessWithChecksumInfo(blockSender, new DataOutputStream(getOutputStream()));\n\n      long beginRead = Time.monotonicNow();\n      read = blockSender.sendBlock(out, baseStream, null); // send data\n      long duration = Time.monotonicNow() - beginRead;\n      if (blockSender.didSendEntireByteRange()) {\n        // If we sent the entire range, then we should expect the client\n        // to respond with a Status enum.\n        try {\n          ClientReadStatusProto stat = ClientReadStatusProto.parseFrom(\n              PBHelperClient.vintPrefixed(in));\n          if (!stat.hasStatus()) {\n            LOG.warn(\"Client \" + peer.getRemoteAddressString() +\n                \" did not send a valid status code after reading. \" +\n                \"Will close connection.\");\n            IOUtils.closeStream(out);\n          }\n        } catch (IOException ioe) {\n          LOG.debug(\"Error reading client status response. Will close connection.\", ioe);\n          IOUtils.closeStream(out);\n          incrDatanodeNetworkErrors();\n        }\n      } else {\n        IOUtils.closeStream(out);\n      }\n      datanode.metrics.incrBytesRead((int) read);\n      datanode.metrics.incrBlocksRead();\n      datanode.metrics.incrTotalReadTime(duration);\n    } catch ( SocketException ignored ) {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(dnR + \":Ignoring exception while serving \" + block + \" to \" +\n            remoteAddress, ignored);\n      }\n      // Its ok for remote side to close the connection anytime.\n      datanode.metrics.incrBlocksRead();\n      IOUtils.closeStream(out);\n    } catch ( IOException ioe ) {\n      /* What exactly should we do here?\n       * Earlier version shutdown() datanode if there is disk error.\n       */\n      if (!(ioe instanceof SocketTimeoutException)) {\n        LOG.warn(dnR + \":Got exception while serving \" + block + \" to \"\n          + remoteAddress, ioe);\n        incrDatanodeNetworkErrors();\n      }\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(blockSender);\n    }\n\n    //update metrics\n    datanode.metrics.addReadBlockOp(elapsed());\n    datanode.metrics.incrReadsFromClient(peer.isLocal(), read);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeSuccessWithChecksumInfo": "  private void writeSuccessWithChecksumInfo(BlockSender blockSender,\n      DataOutputStream out) throws IOException {\n\n    ReadOpChecksumInfoProto ckInfo = ReadOpChecksumInfoProto.newBuilder()\n      .setChecksum(DataTransferProtoUtil.toProto(blockSender.getChecksum()))\n      .setChunkOffset(blockSender.getOffset())\n      .build();\n      \n    BlockOpResponseProto response = BlockOpResponseProto.newBuilder()\n      .setStatus(SUCCESS)\n      .setReadOpChecksumInfo(ckInfo)\n      .build();\n    response.writeDelimitedTo(out);\n    out.flush();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.incrDatanodeNetworkErrors": "  private void incrDatanodeNetworkErrors() {\n    datanode.incrDatanodeNetworkErrors(remoteAddressWithoutPort);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBufferedOutputStream": "  DataOutputStream getBufferedOutputStream() {\n    return new DataOutputStream(\n        new BufferedOutputStream(getOutputStream(), smallBufferSize));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.elapsed": "  private long elapsed() {\n    return monotonicNow() - opStartTime;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.getOutputStream": "  private OutputStream getOutputStream() {\n    return socketOut;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess": "  private void checkAccess(OutputStream out, final boolean reply, \n      final ExtendedBlock blk,\n      final Token<BlockTokenIdentifier> t,\n      final Op op,\n      final BlockTokenIdentifier.AccessMode mode) throws IOException {\n    checkAndWaitForBP(blk);\n    if (datanode.isBlockTokenEnabled) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Checking block access token for block '\" + blk.getBlockId()\n            + \"' with mode '\" + mode + \"'\");\n      }\n      try {\n        datanode.blockPoolTokenSecretManager.checkAccess(t, null, blk, mode);\n      } catch(InvalidToken e) {\n        try {\n          if (reply) {\n            BlockOpResponseProto.Builder resp = BlockOpResponseProto.newBuilder()\n              .setStatus(ERROR_ACCESS_TOKEN);\n            if (mode == BlockTokenIdentifier.AccessMode.WRITE) {\n              DatanodeRegistration dnR = \n                datanode.getDNRegistrationForBP(blk.getBlockPoolId());\n              // NB: Unconditionally using the xfer addr w/o hostname\n              resp.setFirstBadLink(dnR.getXferAddr());\n            }\n            resp.build().writeDelimitedTo(out);\n            out.flush();\n          }\n          LOG.warn(\"Block token verification failed: op=\" + op\n              + \", remoteAddress=\" + remoteAddress\n              + \", message=\" + e.getLocalizedMessage());\n          throw e;\n        } finally {\n          IOUtils.closeStream(out);\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.sendResponse": "  private void sendResponse(Status status,\n      String message) throws IOException {\n    writeResponse(status, message, getOutputStream());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.updateCurrentThreadName": "  private void updateCurrentThreadName(String status) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"DataXceiver for client \");\n    if (previousOpClientName != null) {\n      sb.append(previousOpClientName).append(\" at \");\n    }\n    sb.append(remoteAddress);\n    if (status != null) {\n      sb.append(\" [\").append(status).append(\"]\");\n    }\n    Thread.currentThread().setName(sb.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock": "  private void opReadBlock() throws IOException {\n    OpReadBlockProto proto = OpReadBlockProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      readBlock(PBHelperClient.convert(proto.getHeader().getBaseHeader().getBlock()),\n        PBHelperClient.convert(proto.getHeader().getBaseHeader().getToken()),\n        proto.getHeader().getClientName(),\n        proto.getOffset(),\n        proto.getLen(),\n        proto.getSendChecksums(),\n        (proto.hasCachingStrategy() ?\n            getCachingStrategy(proto.getCachingStrategy()) :\n          CachingStrategy.newDefaultStrategy()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.continueTraceSpan": "  private TraceScope continueTraceSpan(BaseHeaderProto header,\n                                             String description) {\n    return continueTraceSpan(header.getTraceInfo(), description);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.getCachingStrategy": "  static private CachingStrategy getCachingStrategy(CachingStrategyProto strategy) {\n    Boolean dropBehind = strategy.hasDropBehind() ?\n        strategy.getDropBehind() : null;\n    Long readahead = strategy.hasReadahead() ?\n        strategy.getReadahead() : null;\n    return new CachingStrategy(dropBehind, readahead);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp": "  protected final void processOp(Op op) throws IOException {\n    switch(op) {\n    case READ_BLOCK:\n      opReadBlock();\n      break;\n    case WRITE_BLOCK:\n      opWriteBlock(in);\n      break;\n    case REPLACE_BLOCK:\n      opReplaceBlock(in);\n      break;\n    case COPY_BLOCK:\n      opCopyBlock(in);\n      break;\n    case BLOCK_CHECKSUM:\n      opBlockChecksum(in);\n      break;\n    case BLOCK_GROUP_CHECKSUM:\n      opStripedBlockChecksum(in);\n      break;\n    case TRANSFER_BLOCK:\n      opTransferBlock(in);\n      break;\n    case REQUEST_SHORT_CIRCUIT_FDS:\n      opRequestShortCircuitFds(in);\n      break;\n    case RELEASE_SHORT_CIRCUIT_FDS:\n      opReleaseShortCircuitFds(in);\n      break;\n    case REQUEST_SHORT_CIRCUIT_SHM:\n      opRequestShortCircuitShm(in);\n      break;\n    default:\n      throw new IOException(\"Unknown op \" + op + \" in data stream\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReplaceBlock": "  private void opReplaceBlock(DataInputStream in) throws IOException {\n    OpReplaceBlockProto proto = OpReplaceBlockProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      replaceBlock(PBHelperClient.convert(proto.getHeader().getBlock()),\n          PBHelperClient.convertStorageType(proto.getStorageType()),\n          PBHelperClient.convert(proto.getHeader().getToken()),\n          proto.getDelHint(),\n          PBHelperClient.convert(proto.getSource()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitFds": "  private void opRequestShortCircuitFds(DataInputStream in) throws IOException {\n    final OpRequestShortCircuitAccessProto proto =\n      OpRequestShortCircuitAccessProto.parseFrom(vintPrefixed(in));\n    SlotId slotId = (proto.hasSlotId()) ? \n        PBHelperClient.convert(proto.getSlotId()) : null;\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      requestShortCircuitFds(PBHelperClient.convert(proto.getHeader().getBlock()),\n          PBHelperClient.convert(proto.getHeader().getToken()),\n          slotId, proto.getMaxVersion(),\n          proto.getSupportsReceiptVerification());\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitShm": "  private void opRequestShortCircuitShm(DataInputStream in) throws IOException {\n    final ShortCircuitShmRequestProto proto =\n        ShortCircuitShmRequestProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getTraceInfo(),\n        proto.getClass().getSimpleName());\n    try {\n      requestShortCircuitShm(proto.getClientName());\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opBlockChecksum": "  private void opBlockChecksum(DataInputStream in) throws IOException {\n    OpBlockChecksumProto proto = OpBlockChecksumProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n    blockChecksum(PBHelperClient.convert(proto.getHeader().getBlock()),\n        PBHelperClient.convert(proto.getHeader().getToken()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReleaseShortCircuitFds": "  private void opReleaseShortCircuitFds(DataInputStream in)\n      throws IOException {\n    final ReleaseShortCircuitAccessRequestProto proto =\n      ReleaseShortCircuitAccessRequestProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getTraceInfo(),\n        proto.getClass().getSimpleName());\n    try {\n      releaseShortCircuitFds(PBHelperClient.convert(proto.getSlotId()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum": "  private void opStripedBlockChecksum(DataInputStream dis) throws IOException {\n    OpBlockGroupChecksumProto proto =\n        OpBlockGroupChecksumProto.parseFrom(vintPrefixed(dis));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    StripedBlockInfo stripedBlockInfo = new StripedBlockInfo(\n        PBHelperClient.convert(proto.getHeader().getBlock()),\n        PBHelperClient.convert(proto.getDatanodes()),\n        PBHelperClient.convertTokens(proto.getBlockTokensList()),\n        PBHelperClient.convertBlockIndices(proto.getBlockIndicesList()),\n        PBHelperClient.convertErasureCodingPolicy(proto.getEcPolicy())\n    );\n\n    try {\n      blockGroupChecksum(stripedBlockInfo,\n          PBHelperClient.convert(proto.getHeader().getToken()),\n          proto.getRequestedNumBytes());\n    } finally {\n      if (traceScope != null) {\n        traceScope.close();\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opTransferBlock": "  private void opTransferBlock(DataInputStream in) throws IOException {\n    final OpTransferBlockProto proto =\n      OpTransferBlockProto.parseFrom(vintPrefixed(in));\n    final DatanodeInfo[] targets = PBHelperClient.convert(proto.getTargetsList());\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      transferBlock(PBHelperClient.convert(proto.getHeader().getBaseHeader().getBlock()),\n          PBHelperClient.convert(proto.getHeader().getBaseHeader().getToken()),\n          proto.getHeader().getClientName(),\n          targets,\n          PBHelperClient.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opCopyBlock": "  private void opCopyBlock(DataInputStream in) throws IOException {\n    OpCopyBlockProto proto = OpCopyBlockProto.parseFrom(vintPrefixed(in));\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      copyBlock(PBHelperClient.convert(proto.getHeader().getBlock()),\n          PBHelperClient.convert(proto.getHeader().getToken()));\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock": "  private void opWriteBlock(DataInputStream in) throws IOException {\n    final OpWriteBlockProto proto = OpWriteBlockProto.parseFrom(vintPrefixed(in));\n    final DatanodeInfo[] targets = PBHelperClient.convert(proto.getTargetsList());\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      writeBlock(PBHelperClient.convert(proto.getHeader().getBaseHeader().getBlock()),\n          PBHelperClient.convertStorageType(proto.getStorageType()),\n          PBHelperClient.convert(proto.getHeader().getBaseHeader().getToken()),\n          proto.getHeader().getClientName(),\n          targets,\n          PBHelperClient.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length),\n          PBHelperClient.convert(proto.getSource()),\n          fromProto(proto.getStage()),\n          proto.getPipelineSize(),\n          proto.getMinBytesRcvd(), proto.getMaxBytesRcvd(),\n          proto.getLatestGenerationStamp(),\n          fromProto(proto.getRequestedChecksum()),\n          (proto.hasCachingStrategy() ?\n              getCachingStrategy(proto.getCachingStrategy()) :\n            CachingStrategy.newDefaultStrategy()),\n          (proto.hasAllowLazyPersist() ? proto.getAllowLazyPersist() : false),\n          (proto.hasPinning() ? proto.getPinning(): false),\n          (PBHelperClient.convertBooleanList(proto.getTargetPinningsList())));\n    } finally {\n     if (traceScope != null) traceScope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.run": "  public void run() {\n    int opsProcessed = 0;\n    Op op = null;\n\n    try {\n      synchronized(this) {\n        xceiver = Thread.currentThread();\n      }\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input = socketIn;\n      try {\n        IOStreamPair saslStreams = datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input = new BufferedInputStream(saslStreams.in,\n            smallBufferSize);\n        socketOut = saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at \" + peer.getRemoteAddressString() + \n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\");\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed != 0) {\n            assert dnConf.socketKeepaliveTimeout > 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op = readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (EOFException | ClosedChannelException e) {\n          // Since we optimistically expect the next op, it's quite normal to\n          // get EOF here.\n          LOG.debug(\"Cached {} closing after {} ops.  \" +\n              \"This message is usually benign.\", peer, opsProcessed);\n          break;\n        } catch (IOException err) {\n          incrDatanodeNetworkErrors();\n          throw err;\n        }\n\n        // restore normal timeout\n        if (opsProcessed != 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime = monotonicNow();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer != null) &&\n          (!peer.isClosed() && dnConf.socketKeepaliveTimeout > 0));\n    } catch (Throwable t) {\n      String s = datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op == null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op == Op.WRITE_BLOCK && t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op == Op.READ_BLOCK && t instanceof SocketTimeoutException) {\n        String s1 =\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 += \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else if (t instanceof InvalidToken) {\n        // The InvalidToken exception has already been logged in\n        // checkAccess() method and this is not a server error.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer != null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.DataChecksum.throwChecksumException": "  private static void throwChecksumException(Type type, Checksum algorithm,\n      String filename, long errPos, int expected, int computed)\n          throws ChecksumException {\n    throw new ChecksumException(\"Checksum \" + type\n        + \" not matched for file \" + filename + \" at position \"+ errPos\n        + String.format(\": expected=%X but computed=%X\", expected, computed)\n        + \", algorithm=\" + algorithm.getClass().getSimpleName(), errPos);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.DataChecksum.verifyChunked": "  static void verifyChunked(final Type type, final Checksum algorithm,\n      final byte[] data, final int dataOffset, final int dataLength,\n      final int bytesPerCrc, final byte[] crcs, final int crcsOffset,\n      final String filename, final long basePos) throws ChecksumException {\n    final int dataEnd = dataOffset + dataLength;\n    int i = dataOffset;\n    int j = crcsOffset;\n    for(final int n = dataEnd-bytesPerCrc+1; i < n; i += bytesPerCrc, j += 4) {\n      algorithm.reset();\n      algorithm.update(data, i, bytesPerCrc);\n      final int computed = (int)algorithm.getValue();\n      final int expected = ((crcs[j] << 24) + ((crcs[j + 1] << 24) >>> 8))\n          + (((crcs[j + 2] << 24) >>> 16) + ((crcs[j + 3] << 24) >>> 24));\n\n      if (computed != expected) {\n        final long errPos = basePos + i - dataOffset;\n        throwChecksumException(type, algorithm, filename, errPos, expected,\n            computed);\n      }\n    }\n    final int remainder = dataEnd - i;\n    if (remainder > 0) {\n      algorithm.reset();\n      algorithm.update(data, i, remainder);\n      final int computed = (int)algorithm.getValue();\n      final int expected = ((crcs[j] << 24) + ((crcs[j + 1] << 24) >>> 8))\n          + (((crcs[j + 2] << 24) >>> 16) + ((crcs[j + 3] << 24) >>> 24));\n\n      if (computed != expected) {\n        final long errPos = basePos + i - dataOffset;\n        throwChecksumException(type, algorithm, filename, errPos, expected,\n            computed);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.DataChecksum.update": "    public void update(int b) {}",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.DataChecksum.reset": "    public void reset() {}",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.DataChecksum.getValue": "    public long getValue() { return 0; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.DataChecksum.verifyChunkedSums": "  public void verifyChunkedSums(ByteBuffer data, ByteBuffer checksums,\n      String fileName, long basePos) throws ChecksumException {\n    if (type.size == 0) return;\n    \n    if (data.hasArray() && checksums.hasArray()) {\n      final int dataOffset = data.arrayOffset() + data.position();\n      final int crcsOffset = checksums.arrayOffset() + checksums.position();\n      verifyChunked(type, summer, data.array(), dataOffset, data.remaining(),\n          bytesPerChecksum, checksums.array(), crcsOffset, fileName, basePos);\n      return;\n    }\n    if (NativeCrc32.isAvailable() && data.isDirect()) {\n      NativeCrc32.verifyChunkedSums(bytesPerChecksum, type.id, checksums, data,\n          fileName, basePos);\n    } else {\n      verifyChunked(type, summer, data, bytesPerChecksum, checksums, fileName,\n          basePos);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.readNextPacket": "  private void readNextPacket() throws IOException {\n    //Read packet headers.\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader curHeader = packetReceiver.getHeader();\n    curDataSlice = packetReceiver.getDataSlice();\n    assert curDataSlice.capacity() == curHeader.getDataLen();\n\n    LOG.trace(\"DFSClient readNextPacket got header {}\", curHeader);\n\n    // Sanity check the lengths\n    if (!curHeader.sanityCheck(lastSeqNo)) {\n      throw new IOException(\"BlockReader: error in packet header \" +\n          curHeader);\n    }\n\n    if (curHeader.getDataLen() > 0) {\n      int chunks = 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n      int checksumsLen = chunks * checksumSize;\n\n      assert packetReceiver.getChecksumSlice().capacity() == checksumsLen :\n          \"checksum slice capacity=\" +\n              packetReceiver.getChecksumSlice().capacity() +\n              \" checksumsLen=\" + checksumsLen;\n\n      lastSeqNo = curHeader.getSeqno();\n      if (verifyChecksum && curDataSlice.remaining() > 0) {\n        // N.B.: the checksum error offset reported here is actually\n        // relative to the start of the block, not the start of the file.\n        // This is slightly misleading, but preserves the behavior from\n        // the older BlockReader.\n        checksum.verifyChunkedSums(curDataSlice,\n            packetReceiver.getChecksumSlice(),\n            filename, curHeader.getOffsetInBlock());\n      }\n      bytesNeededToFinish -= curHeader.getDataLen();\n    }\n\n    // First packet will include some data prior to the first byte\n    // the user requested. Skip it.\n    if (curHeader.getOffsetInBlock() < startOffset) {\n      int newPos = (int) (startOffset - curHeader.getOffsetInBlock());\n      curDataSlice.position(newPos);\n    }\n\n    // If we've now satisfied the whole client read, read one last packet\n    // header, which should be empty\n    if (bytesNeededToFinish <= 0) {\n      readTrailingEmptyPacket();\n      if (verifyChecksum) {\n        sendReadResult(Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(Status.SUCCESS);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.sendReadResult": "  void sendReadResult(Status statusCode) {\n    assert !sentStatusCode : \"already sent status code to \" + peer;\n    try {\n      writeReadResult(peer.getOutputStream(), statusCode);\n      sentStatusCode = true;\n    } catch (IOException e) {\n      // It's ok not to be able to send this. But something is probably wrong.\n      LOG.info(\"Could not send read status (\" + statusCode + \") to datanode \" +\n          peer.getRemoteAddressString() + \": \" + e.getMessage());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.readTrailingEmptyPacket": "  private void readTrailingEmptyPacket() throws IOException {\n    LOG.trace(\"Reading empty packet at end of read\");\n\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader trailer = packetReceiver.getHeader();\n    if (!trailer.isLastPacketInBlock() ||\n        trailer.getDataLen() != 0) {\n      throw new IOException(\"Expected empty end-of-read packet! Header: \" +\n          trailer);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.read": "  public synchronized int read(ByteBuffer buf) throws IOException {\n    if (curDataSlice == null ||\n        (curDataSlice.remaining() == 0 && bytesNeededToFinish > 0)) {\n      try (TraceScope ignored = tracer.newScope(\n          \"BlockReaderRemote2#readNextPacket(\" + blockId + \")\")) {\n        readNextPacket();\n      }\n    }\n    if (curDataSlice.remaining() == 0) {\n      // we're at EOF now\n      return -1;\n    }\n\n    int nRead = Math.min(curDataSlice.remaining(), buf.remaining());\n    ByteBuffer writeSlice = curDataSlice.duplicate();\n    writeSlice.limit(writeSlice.position() + nRead);\n    buf.put(writeSlice);\n    curDataSlice.position(writeSlice.position());\n\n    return nRead;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.ReaderStrategy.readFromBlock": "  public int readFromBlock(BlockReader blockReader,\n                           int length) throws IOException {\n    ByteBuffer tmpBuf = readBuf.duplicate();\n    tmpBuf.limit(tmpBuf.position() + length);\n    int nRead = blockReader.read(tmpBuf);\n    // Only when data are read, update the position\n    if (nRead > 0) {\n      readBuf.position(readBuf.position() + nRead);\n      updateReadStatistics(readStatistics, nRead, blockReader);\n      dfsClient.updateFileSystemReadStats(blockReader.getNetworkDistance(),\n          nRead);\n    }\n\n    return nRead;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.readBuffer": "  private synchronized int readBuffer(ReaderStrategy reader, int len,\n                                      CorruptedBlocks corruptedBlocks)\n      throws IOException {\n    IOException ioe;\n\n    /* we retry current node only once. So this is set to true only here.\n     * Intention is to handle one common case of an error that is not a\n     * failure on datanode or client : when DataNode closes the connection\n     * since client is idle. If there are other cases of \"non-errors\" then\n     * then a datanode might be retried by setting this to true again.\n     */\n    boolean retryCurrentNode = true;\n\n    while (true) {\n      // retry as many times as seekToNewSource allows.\n      try {\n        return reader.readFromBlock(blockReader, len);\n      } catch (ChecksumException ce) {\n        DFSClient.LOG.warn(\"Found Checksum error for \"\n            + getCurrentBlock() + \" from \" + currentNode\n            + \" at \" + ce.getPos());\n        ioe = ce;\n        retryCurrentNode = false;\n        // we want to remember which block replicas we have tried\n        corruptedBlocks.addCorruptedBlock(getCurrentBlock(), currentNode);\n      } catch (IOException e) {\n        if (!retryCurrentNode) {\n          DFSClient.LOG.warn(\"Exception while reading from \"\n              + getCurrentBlock() + \" of \" + src + \" from \"\n              + currentNode, e);\n        }\n        ioe = e;\n      }\n      boolean sourceFound;\n      if (retryCurrentNode) {\n        /* possibly retry the same node so that transient errors don't\n         * result in application level failures (e.g. Datanode could have\n         * closed the connection because the client is idle for too long).\n         */\n        sourceFound = seekToBlockSource(pos);\n      } else {\n        addToDeadNodes(currentNode);\n        sourceFound = seekToNewSource(pos);\n      }\n      if (!sourceFound) {\n        throw ioe;\n      }\n      retryCurrentNode = false;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.getCurrentBlock": "  synchronized public ExtendedBlock getCurrentBlock() {\n    if (currentLocatedBlock == null){\n      return null;\n    }\n    return currentLocatedBlock.getBlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.seekToNewSource": "  public synchronized boolean seekToNewSource(long targetPos)\n      throws IOException {\n    if (currentNode == null) {\n      return seekToBlockSource(targetPos);\n    }\n    boolean markedDead = deadNodes.containsKey(currentNode);\n    addToDeadNodes(currentNode);\n    DatanodeInfo oldNode = currentNode;\n    DatanodeInfo newNode = blockSeekTo(targetPos);\n    if (!markedDead) {\n      /* remove it from deadNodes. blockSeekTo could have cleared\n       * deadNodes and added currentNode again. Thats ok. */\n      deadNodes.remove(oldNode);\n    }\n    if (!oldNode.getDatanodeUuid().equals(newNode.getDatanodeUuid())) {\n      currentNode = newNode;\n      return true;\n    } else {\n      return false;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.addToDeadNodes": "  void addToDeadNodes(DatanodeInfo dnInfo) {\n    deadNodes.put(dnInfo, dnInfo);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource": "  private boolean seekToBlockSource(long targetPos)\n                                                 throws IOException {\n    currentNode = blockSeekTo(targetPos);\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.getPos": "  public synchronized long getPos() {\n    return pos;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy": "  protected synchronized int readWithStrategy(ReaderStrategy strategy)\n      throws IOException {\n    dfsClient.checkOpen();\n    if (closed.get()) {\n      throw new IOException(\"Stream closed\");\n    }\n\n    int len = strategy.getTargetLength();\n    CorruptedBlocks corruptedBlocks = new CorruptedBlocks();\n    failures = 0;\n    if (pos < getFileLength()) {\n      int retries = 2;\n      while (retries > 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos > blockEnd || currentNode == null) {\n            currentNode = blockSeekTo(pos);\n          }\n          int realLen = (int) Math.min(len, (blockEnd - pos + 1L));\n          synchronized(infoLock) {\n            if (locatedBlocks.isLastBlockComplete()) {\n              realLen = (int) Math.min(realLen,\n                  locatedBlocks.getFileLength() - pos);\n            }\n          }\n          int result = readBuffer(strategy, realLen, corruptedBlocks);\n\n          if (result >= 0) {\n            pos += result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;\n        } catch (IOException e) {\n          checkInterrupted(e);\n          if (retries == 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd = -1;\n          if (currentNode != null) {\n            addToDeadNodes(currentNode);\n          }\n          if (--retries == 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlocks,\n              currentLocatedBlock.getLocations().length, false);\n        }\n      }\n    }\n    return -1;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.reportCheckSumFailure": "  protected void reportCheckSumFailure(CorruptedBlocks corruptedBlocks,\n      int dataNodeCount, boolean isStriped) {\n\n    Map<ExtendedBlock, Set<DatanodeInfo>> corruptedBlockMap =\n        corruptedBlocks.getCorruptionMap();\n    if (corruptedBlockMap.isEmpty()) {\n      return;\n    }\n    List<LocatedBlock> reportList = new ArrayList<>(corruptedBlockMap.size());\n    for (Map.Entry<ExtendedBlock, Set<DatanodeInfo>> entry :\n        corruptedBlockMap.entrySet()) {\n      ExtendedBlock blk = entry.getKey();\n      Set<DatanodeInfo> dnSet = entry.getValue();\n      if (isStriped || ((dnSet.size() < dataNodeCount) && (dnSet.size() > 0))\n          || ((dataNodeCount == 1) && (dnSet.size() == dataNodeCount))) {\n        DatanodeInfo[] locs = new DatanodeInfo[dnSet.size()];\n        int i = 0;\n        for (DatanodeInfo dn:dnSet) {\n          locs[i++] = dn;\n        }\n        reportList.add(new LocatedBlock(blk, locs));\n      }\n    }\n    if (reportList.size() > 0) {\n      dfsClient.reportChecksumFailure(src,\n          reportList.toArray(new LocatedBlock[reportList.size()]));\n    }\n    corruptedBlockMap.clear();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.getFileLength": "  public long getFileLength() {\n    synchronized(infoLock) {\n      return locatedBlocks == null? 0:\n          locatedBlocks.getFileLength() + lastBlockBeingWrittenLength;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo": "  private synchronized DatanodeInfo blockSeekTo(long target)\n      throws IOException {\n    if (target >= getFileLength()) {\n      throw new IOException(\"Attempted to read past end of file\");\n    }\n\n    // Will be getting a new BlockReader.\n    closeCurrentBlockReaders();\n\n    //\n    // Connect to best DataNode for desired Block, with potential offset\n    //\n    DatanodeInfo chosenNode;\n    int refetchToken = 1; // only need to get a new access token once\n    int refetchEncryptionKey = 1; // only need to get a new encryption key once\n\n    boolean connectFailedOnce = false;\n\n    while (true) {\n      //\n      // Compute desired block\n      //\n      LocatedBlock targetBlock = getBlockAt(target);\n\n      // update current position\n      this.pos = target;\n      this.blockEnd = targetBlock.getStartOffset() +\n            targetBlock.getBlockSize() - 1;\n      this.currentLocatedBlock = targetBlock;\n\n      long offsetIntoBlock = target - targetBlock.getStartOffset();\n\n      DNAddrPair retval = chooseDataNode(targetBlock, null);\n      chosenNode = retval.info;\n      InetSocketAddress targetAddr = retval.addr;\n      StorageType storageType = retval.storageType;\n\n      try {\n        blockReader = getBlockReader(targetBlock, offsetIntoBlock,\n            targetBlock.getBlockSize() - offsetIntoBlock, targetAddr,\n            storageType, chosenNode);\n        if(connectFailedOnce) {\n          DFSClient.LOG.info(\"Successfully connected to \" + targetAddr +\n                             \" for \" + targetBlock.getBlock());\n        }\n        return chosenNode;\n      } catch (IOException ex) {\n        checkInterrupted(ex);\n        if (ex instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \" + targetAddr\n              + \" : \" + ex);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n        } else if (refetchToken > 0 && tokenRefetchNeeded(ex, targetAddr)) {\n          refetchToken--;\n          fetchBlockAt(target);\n        } else {\n          connectFailedOnce = true;\n          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \" for block\"\n              + \", add to deadNodes and continue. \" + ex, ex);\n          // Put chosen node into dead list, continue\n          addToDeadNodes(chosenNode);\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.checkInterrupted": "  private void checkInterrupted(IOException e) throws IOException {\n    if (Thread.currentThread().isInterrupted() &&\n        (e instanceof ClosedByInterruptException ||\n            e instanceof InterruptedIOException)) {\n      DFSClient.LOG.debug(\"The reading thread has been interrupted.\", e);\n      throw e;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.read": "  public synchronized ByteBuffer read(ByteBufferPool bufferPool,\n      int maxLength, EnumSet<ReadOption> opts)\n          throws IOException, UnsupportedOperationException {\n    if (maxLength == 0) {\n      return EMPTY_BUFFER;\n    } else if (maxLength < 0) {\n      throw new IllegalArgumentException(\"can't read a negative \" +\n          \"number of bytes.\");\n    }\n    if ((blockReader == null) || (blockEnd == -1)) {\n      if (pos >= getFileLength()) {\n        return null;\n      }\n      /*\n       * If we don't have a blockReader, or the one we have has no more bytes\n       * left to read, we call seekToBlockSource to get a new blockReader and\n       * recalculate blockEnd.  Note that we assume we're not at EOF here\n       * (we check this above).\n       */\n      if ((!seekToBlockSource(pos)) || (blockReader == null)) {\n        throw new IOException(\"failed to allocate new BlockReader \" +\n            \"at position \" + pos);\n      }\n    }\n    ByteBuffer buffer = null;\n    if (dfsClient.getConf().getShortCircuitConf().isShortCircuitMmapEnabled()) {\n      buffer = tryReadZeroCopy(maxLength, opts);\n    }\n    if (buffer != null) {\n      return buffer;\n    }\n    buffer = ByteBufferUtil.fallbackRead(this, bufferPool, maxLength);\n    if (buffer != null) {\n      getExtendedReadBuffers().put(buffer, bufferPool);\n    }\n    return buffer;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.tryReadZeroCopy": "  private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n      EnumSet<ReadOption> opts) throws IOException {\n    // Copy 'pos' and 'blockEnd' to local variables to make it easier for the\n    // JVM to optimize this function.\n    final long curPos = pos;\n    final long curEnd = blockEnd;\n    final long blockStartInFile = currentLocatedBlock.getStartOffset();\n    final long blockPos = curPos - blockStartInFile;\n\n    // Shorten this read if the end of the block is nearby.\n    long length63;\n    if ((curPos + maxLength) <= (curEnd + 1)) {\n      length63 = maxLength;\n    } else {\n      length63 = 1 + curEnd - curPos;\n      if (length63 <= 0) {\n        DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset {}\"\n                + \" of {}; {} bytes left in block. blockPos={}; curPos={};\"\n                + \"curEnd={}\",\n            curPos, src, length63, blockPos, curPos, curEnd);\n        return null;\n      }\n      DFSClient.LOG.debug(\"Reducing read length from {} to {} to avoid going \"\n              + \"more than one byte past the end of the block.  blockPos={}; \"\n              +\" curPos={}; curEnd={}\",\n          maxLength, length63, blockPos, curPos, curEnd);\n    }\n    // Make sure that don't go beyond 31-bit offsets in the MappedByteBuffer.\n    int length;\n    if (blockPos + length63 <= Integer.MAX_VALUE) {\n      length = (int)length63;\n    } else {\n      long length31 = Integer.MAX_VALUE - blockPos;\n      if (length31 <= 0) {\n        // Java ByteBuffers can't be longer than 2 GB, because they use\n        // 4-byte signed integers to represent capacity, etc.\n        // So we can't mmap the parts of the block higher than the 2 GB offset.\n        // FIXME: we could work around this with multiple memory maps.\n        // See HDFS-5101.\n        DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset {} \"\n            + \" of {}; 31-bit MappedByteBuffer limit exceeded.  blockPos={}, \"\n            + \"curEnd={}\", curPos, src, blockPos, curEnd);\n        return null;\n      }\n      length = (int)length31;\n      DFSClient.LOG.debug(\"Reducing read length from {} to {} to avoid 31-bit \"\n          + \"limit.  blockPos={}; curPos={}; curEnd={}\",\n          maxLength, length, blockPos, curPos, curEnd);\n    }\n    final ClientMmap clientMmap = blockReader.getClientMmap(opts);\n    if (clientMmap == null) {\n      DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset {} of\"\n          + \" {}; BlockReader#getClientMmap returned null.\", curPos, src);\n      return null;\n    }\n    boolean success = false;\n    ByteBuffer buffer;\n    try {\n      seek(curPos + length);\n      buffer = clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n      buffer.position((int)blockPos);\n      buffer.limit((int)(blockPos + length));\n      getExtendedReadBuffers().put(buffer, clientMmap);\n      readStatistics.addZeroCopyBytes(length);\n      DFSClient.LOG.debug(\"readZeroCopy read {} bytes from offset {} via the \"\n          + \"zero-copy read path.  blockEnd = {}\", length, curPos, blockEnd);\n      success = true;\n    } finally {\n      if (!success) {\n        IOUtils.closeQuietly(clientMmap);\n      }\n    }\n    return buffer;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.getExtendedReadBuffers": "  private synchronized IdentityHashStore<ByteBuffer, Object>\n        getExtendedReadBuffers() {\n    if (extendedReadBuffers == null) {\n      extendedReadBuffers = new IdentityHashStore<>(0);\n    }\n    return extendedReadBuffers;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.pread": "  private int pread(long position, ByteBuffer buffer)\n      throws IOException {\n    // sanity checks\n    dfsClient.checkOpen();\n    if (closed.get()) {\n      throw new IOException(\"Stream closed\");\n    }\n    failures = 0;\n    long filelen = getFileLength();\n    if ((position < 0) || (position >= filelen)) {\n      return -1;\n    }\n    int length = buffer.remaining();\n    int realLen = length;\n    if ((position + length) > filelen) {\n      realLen = (int)(filelen - position);\n    }\n\n    // determine the block and byte range within the block\n    // corresponding to position and realLen\n    List<LocatedBlock> blockRange = getBlockRange(position, realLen);\n    int remaining = realLen;\n    CorruptedBlocks corruptedBlocks = new CorruptedBlocks();\n    for (LocatedBlock blk : blockRange) {\n      long targetStart = position - blk.getStartOffset();\n      int bytesToRead = (int) Math.min(remaining,\n          blk.getBlockSize() - targetStart);\n      long targetEnd = targetStart + bytesToRead - 1;\n      try {\n        if (dfsClient.isHedgedReadsEnabled() && !blk.isStriped()) {\n          hedgedFetchBlockByteRange(blk, targetStart,\n              targetEnd, buffer, corruptedBlocks);\n        } else {\n          fetchBlockByteRange(blk, targetStart, targetEnd,\n              buffer, corruptedBlocks);\n        }\n      } finally {\n        // Check and report if any block replicas are corrupted.\n        // BlockMissingException may be caught if all block replicas are\n        // corrupted.\n        reportCheckSumFailure(corruptedBlocks, blk.getLocations().length,\n            false);\n      }\n\n      remaining -= bytesToRead;\n      position += bytesToRead;\n    }\n    assert remaining == 0 : \"Wrong number of bytes read.\";\n    return realLen;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile": "  static LastBlockWithStatus appendFile(final FSNamesystem fsn,\n      final String srcArg, final FSPermissionChecker pc, final String holder,\n      final String clientMachine, final boolean newBlock,\n      final boolean logRetryCache) throws IOException {\n    assert fsn.hasWriteLock();\n\n    final LocatedBlock lb;\n    final FSDirectory fsd = fsn.getFSDirectory();\n    final INodesInPath iip;\n    fsd.writeLock();\n    try {\n      iip = fsd.resolvePath(pc, srcArg, DirOp.WRITE);\n      // Verify that the destination does not exist as a directory already\n      final INode inode = iip.getLastINode();\n      final String path = iip.getPath();\n      if (inode != null && inode.isDirectory()) {\n        throw new FileAlreadyExistsException(\"Cannot append to directory \"\n            + path + \"; already exists as a directory.\");\n      }\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n      }\n\n      if (inode == null) {\n        throw new FileNotFoundException(\n            \"Failed to append to non-existent file \" + path + \" for client \"\n                + clientMachine);\n      }\n      final INodeFile file = INodeFile.valueOf(inode, path, true);\n\n      // not support appending file with striped blocks\n      if (file.isStriped()) {\n        throw new UnsupportedOperationException(\n            \"Cannot append to files with striped block \" + path);\n      }\n\n      BlockManager blockManager = fsd.getBlockManager();\n      final BlockStoragePolicy lpPolicy = blockManager\n          .getStoragePolicy(\"LAZY_PERSIST\");\n      if (lpPolicy != null && lpPolicy.getId() == file.getStoragePolicyID()) {\n        throw new UnsupportedOperationException(\n            \"Cannot append to lazy persist file \" + path);\n      }\n      // Opening an existing file for append - may need to recover lease.\n      fsn.recoverLeaseInternal(RecoverLeaseOp.APPEND_FILE, iip, path, holder,\n          clientMachine, false);\n\n      final BlockInfo lastBlock = file.getLastBlock();\n      // Check that the block has at least minimum replication.\n      if (lastBlock != null) {\n        if (lastBlock.getBlockUCState() == BlockUCState.COMMITTED) {\n          throw new RetriableException(\n              new NotReplicatedYetException(\"append: lastBlock=\"\n                  + lastBlock + \" of src=\" + path\n                  + \" is COMMITTED but not yet COMPLETE.\"));\n        } else if (lastBlock.isComplete()\n          && !blockManager.isSufficientlyReplicated(lastBlock)) {\n          throw new IOException(\"append: lastBlock=\" + lastBlock + \" of src=\"\n              + path + \" is not sufficiently replicated yet.\");\n        }\n      }\n      lb = prepareFileForAppend(fsn, iip, holder, clientMachine, newBlock,\n          true, logRetryCache);\n    } catch (IOException ie) {\n      NameNode.stateChangeLog\n          .warn(\"DIR* NameSystem.append: \" + ie.getMessage());\n      throw ie;\n    } finally {\n      fsd.writeUnlock();\n    }\n\n    HdfsFileStatus stat = FSDirStatAndListingOp.getFileInfo(fsd, iip);\n    if (lb != null) {\n      NameNode.stateChangeLog.debug(\n          \"DIR* NameSystem.appendFile: file {} for {} at {} block {} block\"\n              + \" size {}\", srcArg, holder, clientMachine, lb.getBlock(), lb\n              .getBlock().getNumBytes());\n    }\n    return new LastBlockWithStatus(lb, stat);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.prepareFileForAppend": "  static LocatedBlock prepareFileForAppend(final FSNamesystem fsn,\n      final INodesInPath iip, final String leaseHolder,\n      final String clientMachine, final boolean newBlock,\n      final boolean writeToEditLog, final boolean logRetryCache)\n      throws IOException {\n    assert fsn.hasWriteLock();\n\n    final INodeFile file = iip.getLastINode().asFile();\n    final QuotaCounts delta = verifyQuotaForUCBlock(fsn, file, iip);\n\n    file.recordModification(iip.getLatestSnapshotId());\n    file.toUnderConstruction(leaseHolder, clientMachine);\n\n    fsn.getLeaseManager().addLease(\n        file.getFileUnderConstructionFeature().getClientName(), file.getId());\n\n    LocatedBlock ret = null;\n    if (!newBlock) {\n      FSDirectory fsd = fsn.getFSDirectory();\n      ret = fsd.getBlockManager().convertLastBlockToUnderConstruction(file, 0);\n      if (ret != null && delta != null) {\n        Preconditions.checkState(delta.getStorageSpace() >= 0, \"appending to\"\n            + \" a block with size larger than the preferred block size\");\n        fsd.writeLock();\n        try {\n          fsd.updateCountNoQuotaCheck(iip, iip.length() - 1, delta);\n        } finally {\n          fsd.writeUnlock();\n        }\n      }\n    } else {\n      BlockInfo lastBlock = file.getLastBlock();\n      if (lastBlock != null) {\n        ExtendedBlock blk = new ExtendedBlock(fsn.getBlockPoolId(), lastBlock);\n        ret = new LocatedBlock(blk, new DatanodeInfo[0]);\n      }\n    }\n\n    if (writeToEditLog) {\n      final String path = iip.getPath();\n      if (NameNodeLayoutVersion.supports(Feature.APPEND_NEW_BLOCK,\n          fsn.getEffectiveLayoutVersion())) {\n        fsn.getEditLog().logAppendFile(path, file, newBlock, logRetryCache);\n      } else {\n        fsn.getEditLog().logOpenFile(path, file, false, logRetryCache);\n      }\n    }\n    return ret;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile": "  LastBlockWithStatus appendFile(String srcArg, String holder,\n      String clientMachine, EnumSet<CreateFlag> flag, boolean logRetryCache)\n      throws IOException {\n    boolean newBlock = flag.contains(CreateFlag.NEW_BLOCK);\n    if (newBlock) {\n      requireEffectiveLayoutVersionForFeature(Feature.APPEND_NEW_BLOCK);\n    }\n\n    NameNode.stateChangeLog.debug(\n        \"DIR* NameSystem.appendFile: src={}, holder={}, clientMachine={}\",\n        srcArg, holder, clientMachine);\n    try {\n      boolean skipSync = false;\n      LastBlockWithStatus lbs = null;\n      final FSPermissionChecker pc = getPermissionChecker();\n      checkOperation(OperationCategory.WRITE);\n      writeLock();\n      try {\n        checkOperation(OperationCategory.WRITE);\n        checkNameNodeSafeMode(\"Cannot append to file\" + srcArg);\n        lbs = FSDirAppendOp.appendFile(this, srcArg, pc, holder, clientMachine,\n            newBlock, logRetryCache);\n      } catch (StandbyException se) {\n        skipSync = true;\n        throw se;\n      } finally {\n        writeUnlock();\n        // There might be transactions logged while trying to recover the lease\n        // They need to be sync'ed even when an exception was thrown.\n        if (!skipSync) {\n          getEditLog().logSync();\n        }\n      }\n      logAuditEvent(true, \"append\", srcArg);\n      return lbs;\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"append\", srcArg);\n      throw e;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEditLog": "  public FSEditLog getEditLog() {\n    return getFSImage().getEditLog();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock": "  public void writeUnlock() {\n    this.fsLock.writeUnlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.requireEffectiveLayoutVersionForFeature": "  private void requireEffectiveLayoutVersionForFeature(Feature f)\n      throws HadoopIllegalArgumentException {\n    int lv = getEffectiveLayoutVersion();\n    if (!NameNodeLayoutVersion.supports(f, lv)) {\n      throw new HadoopIllegalArgumentException(String.format(\n          \"Feature %s unsupported at NameNode layout version %d.  If a \" +\n          \"rolling upgrade is in progress, then it must be finalized before \" +\n          \"using this feature.\", f, lv));\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock": "  public void writeLock() {\n    this.fsLock.writeLock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode": "  void checkNameNodeSafeMode(String errorMsg)\n      throws RetriableException, SafeModeException {\n    if (isInSafeMode()) {\n      SafeModeException se = newSafemodeException(errorMsg);\n      if (haEnabled && haContext != null\n          && haContext.getState().getServiceState() == HAServiceState.ACTIVE\n          && isInStartupSafeMode()) {\n        throw new RetriableException(se);\n      } else {\n        throw se;\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation": "  public void checkOperation(OperationCategory op) throws StandbyException {\n    if (haContext != null) {\n      // null in some unit tests\n      haContext.checkOperation(op);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker": "  FSPermissionChecker getPermissionChecker()\n      throws AccessControlException {\n    return dir.getPermissionChecker();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logAuditEvent": "    public void logAuditEvent(boolean succeeded, String userName,\n        InetAddress addr, String cmd, String src, String dst,\n        FileStatus status, UserGroupInformation ugi,\n        DelegationTokenSecretManager dtSecretManager) {\n      this.logAuditEvent(succeeded, userName, addr, cmd, src, dst, status,\n              null /*CallerContext*/, ugi, dtSecretManager);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append": "  public LastBlockWithStatus append(String src, String clientName,\n      EnumSetWritable<CreateFlag> flag) throws IOException {\n    checkNNStartup();\n    String clientMachine = getClientMachine();\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.append: file \"\n          +src+\" for \"+clientName+\" at \"+clientMachine);\n    }\n    namesystem.checkOperation(OperationCategory.WRITE);\n    CacheEntryWithPayload cacheEntry = RetryCache.waitForCompletion(retryCache,\n        null);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return (LastBlockWithStatus) cacheEntry.getPayload();\n    }\n\n    LastBlockWithStatus info = null;\n    boolean success = false;\n    try {\n      info = namesystem.appendFile(src, clientName, clientMachine, flag.get(),\n          cacheEntry != null);\n      success = true;\n    } finally {\n      RetryCache.setState(cacheEntry, success, info);\n    }\n    metrics.incrFilesAppended();\n    return info;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkNNStartup": "  private void checkNNStartup() throws IOException {\n    if (!this.nn.isStarted()) {\n      String message = NameNode.composeNotStartedMessage(this.nn.getRole());\n      throw new RetriableException(message);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getClientMachine": "  private static String getClientMachine() {\n    String clientMachine = Server.getRemoteAddress();\n    if (clientMachine == null) { //not a RPC client\n      clientMachine = \"\";\n    }\n    return clientMachine;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append": "  public AppendResponseProto append(RpcController controller,\n      AppendRequestProto req) throws ServiceException {\n    try {\n      EnumSetWritable<CreateFlag> flags = req.hasFlag() ?\n          PBHelperClient.convertCreateFlag(req.getFlag()) :\n          new EnumSetWritable<>(EnumSet.of(CreateFlag.APPEND));\n      LastBlockWithStatus result = server.append(req.getSrc(),\n          req.getClientName(), flags);\n      AppendResponseProto.Builder builder = AppendResponseProto.newBuilder();\n      if (result.getLastBlock() != null) {\n        builder.setBlock(PBHelperClient.convertLocatedBlock(\n            result.getLastBlock()));\n      }\n      if (result.getFileStatus() != null) {\n        builder.setStat(PBHelperClient.convert(result.getFileStatus()));\n      }\n      return builder.build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcProtobufRequest request = (RpcProtobufRequest) writableRequest;\n        RequestHeaderProto rpcRequest = request.getRequestHeader();\n        String methodName = rpcRequest.getMethodName();\n\n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = request.getValue(prototype);\n\n        Message result;\n        long startTime = Time.now();\n        int qTime = (int) (startTime - receiveTime);\n        Exception exception = null;\n        try {\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n        } catch (ServiceException e) {\n          exception = (Exception) e.getCause();\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          exception = e;\n          throw e;\n        } finally {\n          int processingTime = (int) (Time.now() - startTime);\n          if (LOG.isDebugEnabled()) {\n            String msg = \"Served: \" + methodName + \" queueTime= \" + qTime +\n                \" procesingTime= \" + processingTime;\n            if (exception != null) {\n              msg += \" exception= \" + exception.getClass().getSimpleName();\n            }\n            LOG.debug(msg);\n          }\n          String detailedMetricsName = (exception == null) ?\n              methodName :\n              exception.getClass().getSimpleName();\n          server.updateMetrics(detailedMetricsName, qTime, processingTime);\n        }\n        return RpcWritable.wrap(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getRequestHeader": "    RequestHeaderProto getRequestHeader() throws IOException {\n      if (getByteBuffer() != null && requestHeader == null) {\n        requestHeader = getValue(RequestHeaderProto.getDefaultInstance());\n      }\n      return requestHeader;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getProtocolImpl": "      private static ProtoClassProtoImpl getProtocolImpl(RPC.Server server,\n          String protoName, long clientVersion) throws RpcServerException {\n        ProtoNameVer pv = new ProtoNameVer(protoName, clientVersion);\n        ProtoClassProtoImpl impl = \n            server.getProtocolImplMap(RPC.RpcKind.RPC_PROTOCOL_BUFFER).get(pv);\n        if (impl == null) { // no match for Protocol AND Version\n          VerProtocolImpl highest = \n              server.getHighestSupportedProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER, \n                  protoName);\n          if (highest == null) {\n            throw new RpcNoSuchProtocolException(\n                \"Unknown protocol: \" + protoName);\n          }\n          // protocol supported but not the version that client wants\n          throw new RPC.VersionMismatch(protoName, clientVersion,\n              highest.version);\n        }\n        return impl;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.call": "    public Writable call(RPC.RpcKind rpcKind, String protocol,\n        Writable rpcRequest, long receiveTime) throws Exception {\n      return getRpcInvoker(rpcKind).call(this, protocol, rpcRequest,\n          receiveTime);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(RpcCall call,\n      RpcResponseHeaderProto header, Writable rv) throws IOException {\n    final byte[] response;\n    if (rv == null || (rv instanceof RpcWritable.ProtobufWrapper)) {\n      response = setupResponseForProtobuf(header, rv);\n    } else {\n      response = setupResponseForWritable(header, rv);\n    }\n    if (response.length > maxRespSize) {\n      LOG.warn(\"Large response size \" + response.length + \" for call \"\n          + call.toString());\n    }\n    call.setResponse(ByteBuffer.wrap(response));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getRpcErrorCodeProto": "    public RpcErrorCodeProto getRpcErrorCodeProto() {\n      return errCode;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.remove": "    private boolean remove(Connection connection) {\n      boolean removed = connections.remove(connection);\n      if (removed) {\n        count.getAndDecrement();\n      }\n      return removed;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeAll": "    void closeAll() {\n      // use a copy of the connections to be absolutely sure the concurrent\n      // iterator doesn't miss a connection\n      for (Connection connection : toArray()) {\n        close(connection);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(RPC.RpcKind rpcKind, String protocol,\n      Writable param, long receiveTime) throws Exception;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param protocolName - the protocol\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  private void authorize(UserGroupInformation user, String protocolName,\n      InetAddress addr) throws AuthorizationException {\n    if (authorize) {\n      if (protocolName == null) {\n        throw new AuthorizationException(\"Null protocol not authorized\");\n      }\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(protocolName, getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         protocolName);\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    boolean close(Connection connection) {\n      boolean exists = remove(connection);\n      if (exists) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(Thread.currentThread().getName() +\n              \": disconnecting client \" + connection +\n              \". Number of active connections: \"+ size());\n        }\n        // only close if actually removed to avoid double-closing due\n        // to possible races\n        connection.close();\n      }\n      return exists;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isWritable()) {\n                doAsyncWrite(key);\n              }\n            } catch (CancelledKeyException cke) {\n              // something else closed the connection, ex. reader or the\n              // listener doing an idle scan.  ignore it and let them clean\n              // up\n              RpcCall call = (RpcCall)key.attachment();\n              if (call != null) {\n                LOG.info(Thread.currentThread().getName() +\n                    \": connection aborted from \" + call.connection);\n              }\n            } catch (IOException e) {\n              LOG.info(Thread.currentThread().getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = Time.now();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<RpcCall> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<RpcCall>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              RpcCall call = (RpcCall)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n\n          for (RpcCall call : calls) {\n            doPurge(call, now);\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeIdle": "    synchronized void closeIdle(boolean scanAll) {\n      long minLastContact = Time.now() - maxIdleTime;\n      // concurrent iterator might miss new connections added\n      // during the iteration, but that's ok because they won't\n      // be idle yet anyway and will be caught on next scan\n      int closed = 0;\n      for (Connection connection : connections) {\n        // stop if connections dropped below threshold unless scanning all\n        if (!scanAll && size() < idleScanThreshold) {\n          break;\n        }\n        // stop if not scanning all and max connections are closed\n        if (connection.isIdle() &&\n            connection.getLastContact() < minLastContact &&\n            close(connection) &&\n            !scanAll && (++closed == maxIdleToClose)) {\n          break;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.startIdleScan": "    void startIdleScan() {\n      scheduleIdleScanTask();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.logException": "  void logException(Log logger, Throwable e, Call call) {\n    if (exceptionsHandler.isSuppressedLog(e.getClass())) {\n      return; // Log nothing.\n    }\n\n    final String logMsg = Thread.currentThread().getName() + \", call \" + call;\n    if (exceptionsHandler.isTerseLog(e.getClass())) {\n      // Don't log the whole stack trace. Way too noisy!\n      logger.info(logMsg + \": \" + e);\n    } else if (e instanceof RuntimeException || e instanceof Error) {\n      // These exception types indicate something is probably wrong\n      // on the server side, as opposed to just a normal exceptional\n      // result.\n      logger.warn(logMsg, e);\n    } else {\n      logger.info(logMsg, e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.sendResponse": "    private void sendResponse(RpcCall call) throws IOException {\n      responder.doRespond(call);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.stopIdleScan": "    void stopIdleScan() {\n      idleScanTimer.cancel();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws InterruptedException, IOException,  OutOfMemoryError {\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        channel.socket().setKeepAlive(true);\n        \n        Reader reader = getReader();\n        Connection c = connectionManager.register(channel);\n        // If the connectionManager can't take it, close the connection.\n        if (c == null) {\n          if (channel.isOpen()) {\n            IOUtils.cleanup(null, channel);\n          }\n          continue;\n        }\n        key.attach(c);  // so closeCurrentConnection can get the object\n        reader.addConnection(c);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.scheduleIdleScanTask": "    private void scheduleIdleScanTask() {\n      if (!running) {\n        return;\n      }\n      TimerTask idleScanTask = new TimerTask(){\n        @Override\n        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }\n      };\n      idleScanTimer.schedule(idleScanTask, idleScanInterval);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getRemoteUser": "    public UserGroupInformation getRemoteUser() {\n      return connection.user;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause == null) {\n        throw new RuntimeException(\"PrivilegedActionException with no \" +\n                \"underlying cause. UGI [\" + this + \"]\" +\": \" + pae, pae);\n      } else if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.HttpExceptionUtils.validateResponse": "  public static void validateResponse(HttpURLConnection conn,\n      int expectedStatus) throws IOException {\n    if (conn.getResponseCode() != expectedStatus) {\n      Exception toThrow;\n      InputStream es = null;\n      try {\n        es = conn.getErrorStream();\n        ObjectMapper mapper = new ObjectMapper();\n        Map json = mapper.readValue(es, Map.class);\n        json = (Map) json.get(ERROR_JSON);\n        String exClass = (String) json.get(ERROR_CLASSNAME_JSON);\n        String exMsg = (String) json.get(ERROR_MESSAGE_JSON);\n        if (exClass != null) {\n          try {\n            ClassLoader cl = HttpExceptionUtils.class.getClassLoader();\n            Class klass = cl.loadClass(exClass);\n            Constructor constr = klass.getConstructor(String.class);\n            toThrow = (Exception) constr.newInstance(exMsg);\n          } catch (Exception ex) {\n            toThrow = new IOException(String.format(\n                \"HTTP status [%d], exception [%s], message [%s] \",\n                conn.getResponseCode(), exClass, exMsg));\n          }\n        } else {\n          String msg = (exMsg != null) ? exMsg : conn.getResponseMessage();\n          toThrow = new IOException(String.format(\n              \"HTTP status [%d], message [%s]\", conn.getResponseCode(), msg));\n        }\n      } catch (Exception ex) {\n        toThrow = new IOException(String.format(\n            \"HTTP status [%d], message [%s]\", conn.getResponseCode(),\n            conn.getResponseMessage()));\n      } finally {\n        if (es != null) {\n          try {\n            es.close();\n          } catch (IOException ex) {\n            //ignore\n          }\n        }\n      }\n      throwEx(toThrow);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.HttpExceptionUtils.throwEx": "  private static void throwEx(Throwable ex) {\n    HttpExceptionUtils.<RuntimeException>throwException(ex);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ramDiskReplicaTracker.touch": "  abstract void touch(final String bpid, final long blockId);\n\n  /**\n   * Get the next replica to write to persistent storage.\n   */\n  abstract RamDiskReplica dequeueNextReplicaToPersist();\n\n  /**\n   * Invoked if a replica that was previously dequeued for persistence\n   * could not be successfully persisted. Add it back so it can be retried\n   * later.\n   */\n  abstract void reenqueueReplicaNotPersisted(\n      final RamDiskReplica ramDiskReplica);\n\n  /**\n   * Invoked when the Lazy persist operation is started by the DataNode.\n   * @param checkpointVolume\n   */\n  abstract void recordStartLazyPersist(\n      final String bpid, final long blockId, FsVolumeImpl checkpointVolume);\n\n  /**\n   * Invoked when the Lazy persist operation is complete.\n   *\n   * @param savedFiles The saved meta and block files, in that order.\n   */\n  abstract void recordEndLazyPersist(\n      final String bpid, final long blockId, final File[] savedFiles);\n\n  /**\n   * Return a candidate replica to remove from RAM Disk. The exact replica\n   * to be returned may depend on the eviction scheme utilized.\n   *\n   * @return\n   */\n  abstract RamDiskReplica getNextCandidateForEviction();\n\n  /**\n   * Return the number of replicas pending persistence to disk.\n   */\n  abstract int numReplicasNotPersisted();\n\n  /**\n   * Discard all state we are tracking for the given replica.\n   */\n  abstract void discardReplica(\n      final String bpid, final long blockId,\n      boolean deleteSavedCopies);\n\n  /**\n   * Return RamDiskReplica info given block pool id and block id\n   * Return null if it does not exist in RamDisk\n   */\n  abstract RamDiskReplica getReplica(\n    final String bpid, final long blockId);\n}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    final TraceScope scope = datanode.getTracer().\n        newScope(\"sendBlock_\" + block.getBlockId());\n    try {\n      return doSendBlock(out, baseStream, throttler);\n    } finally {\n      scope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock": "  private long doSendBlock(DataOutputStream out, OutputStream baseStream,\n        DataTransferThrottler throttler) throws IOException {\n    if (out == null) {\n      throw new IOException( \"out stream is null\" );\n    }\n    initialOffset = offset;\n    long totalRead = 0;\n    OutputStream streamForSendChunks = out;\n    \n    lastCacheDropOffset = initialOffset;\n\n    if (isLongRead() && blockInFd != null) {\n      // Advise that this file descriptor will be accessed sequentially.\n      NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n          block.getBlockName(), blockInFd, 0, 0, POSIX_FADV_SEQUENTIAL);\n    }\n    \n    // Trigger readahead of beginning of file if configured.\n    manageOsCache();\n\n    final long startTime = ClientTraceLog.isDebugEnabled() ? System.nanoTime() : 0;\n    try {\n      int maxChunksPerPacket;\n      int pktBufSize = PacketHeader.PKT_MAX_HEADER_LEN;\n      boolean transferTo = transferToAllowed && !verifyChecksum\n          && baseStream instanceof SocketOutputStream\n          && blockIn instanceof FileInputStream;\n      if (transferTo) {\n        FileChannel fileChannel = ((FileInputStream)blockIn).getChannel();\n        blockInPosition = fileChannel.position();\n        streamForSendChunks = baseStream;\n        maxChunksPerPacket = numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n        \n        // Smaller packet size to only hold checksum when doing transferTo\n        pktBufSize += checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket = Math.max(1,\n            numberOfChunks(IO_FILE_BUFFER_SIZE));\n        // Packet size includes both checksum and data\n        pktBufSize += (chunkSize + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf = ByteBuffer.allocate(pktBufSize);\n\n      while (endOffset > offset && !Thread.currentThread().isInterrupted()) {\n        manageOsCache();\n        long len = sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n            transferTo, throttler);\n        offset += len;\n        totalRead += len + (numberOfChunks(len) * checksumSize);\n        seqno++;\n      }\n      // If this thread was interrupted, then it did not send the full block.\n      if (!Thread.currentThread().isInterrupted()) {\n        try {\n          // send an empty packet to mark the end of the block\n          sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n              throttler);\n          out.flush();\n        } catch (IOException e) { //socket error\n          throw ioeToSocketException(e);\n        }\n\n        sentEntireByteRange = true;\n      }\n    } finally {\n      if ((clientTraceFmt != null) && ClientTraceLog.isDebugEnabled()) {\n        final long endTime = System.nanoTime();\n        ClientTraceLog.debug(String.format(clientTraceFmt, totalRead,\n            initialOffset, endTime - startTime));\n      }\n      close();\n    }\n    return totalRead;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.close": "  public void close() throws IOException {\n    if (blockInFd != null &&\n        ((dropCacheBehindAllReads) ||\n         (dropCacheBehindLargeReads && isLongRead()))) {\n      try {\n        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n            block.getBlockName(), blockInFd, lastCacheDropOffset,\n            offset - lastCacheDropOffset, POSIX_FADV_DONTNEED);\n      } catch (Exception e) {\n        LOG.warn(\"Unable to drop cache on file close\", e);\n      }\n    }\n    if (curReadahead != null) {\n      curReadahead.cancel();\n    }\n    \n    IOException ioe = null;\n    if(checksumIn!=null) {\n      try {\n        checksumIn.close(); // close checksum file\n      } catch (IOException e) {\n        ioe = e;\n      }\n      checksumIn = null;\n    }   \n    if(blockIn!=null) {\n      try {\n        blockIn.close(); // close data file\n      } catch (IOException e) {\n        ioe = e;\n      }\n      blockIn = null;\n      blockInFd = null;\n    }\n    if (volumeRef != null) {\n      IOUtils.cleanup(null, volumeRef);\n      volumeRef = null;\n    }\n    // throw IOException if there is any\n    if(ioe!= null) {\n      throw ioe;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDNRegistrationForBP": "  public DatanodeRegistration getDNRegistrationForBP(String bpid) \n  throws IOException {\n    DataNodeFaultInjector.get().noRegistration();\n    BPOfferService bpos = blockPoolManager.get(bpid);\n    if(bpos==null || bpos.bpRegistration==null) {\n      throw new IOException(\"cannot find BPOfferService for bpid=\"+bpid);\n    }\n    return bpos.bpRegistration;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.didSendEntireByteRange": "  boolean didSendEntireByteRange() {\n    return sentEntireByteRange;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXceiverCount": "  public int getXceiverCount() {\n    return threadGroup == null ? 0 : threadGroup.activeCount();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDisplayName": "  public String getDisplayName() {\n    // NB: our DatanodeID may not be set yet\n    return hostName + \":\" + getXferPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXferPort": "  public int getXferPort() {\n    return streamingAddr.getPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dataXceiverServer.closePeer": "  synchronized void closePeer(Peer peer) {\n    peers.remove(peer);\n    peersXceiver.remove(peer);\n    datanode.metrics.decrDataNodeActiveXceiversCount();\n    IOUtils.cleanup(null, peer);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.dataXceiverServer.addPeer": "  synchronized void addPeer(Peer peer, Thread t, DataXceiver xceiver)\n      throws IOException {\n    if (closed) {\n      throw new IOException(\"Server closed.\");\n    }\n    peers.put(peer, t);\n    peersXceiver.put(peer, xceiver);\n    datanode.metrics.incrDataNodeActiveXceiversCount();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDatanodeId": "  public DatanodeID getDatanodeId() {\n    return id;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXferAddress": "  public InetSocketAddress getXferAddress() {\n    return streamingAddr;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDnConf": "  public DNConf getDnConf() {\n    return dnConf;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.NativeCrc32.isAvailable": "  public static boolean isAvailable() {\n    if (System.getProperty(\"os.arch\").toLowerCase().startsWith(\"sparc\")) {\n      return false;\n    } else {\n      return NativeCodeLoader.isNativeCodeLoaded();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.blockReader.getNetworkDistance": "  int getNetworkDistance();\n}",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.dfsClient.updateFileSystemReadStats": "  void updateFileSystemReadStats(int distance, int nRead) {\n    if (stats != null) {\n      stats.incrementBytesRead(nRead);\n      stats.incrementBytesReadByDistance(distance, nRead);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.blockReader.read": "  int read(byte[] buf, int off, int len) throws IOException;\n\n  /**\n   * Skip the given number of bytes\n   */\n  long skip(long n) throws IOException;\n\n  /**\n   * Returns an estimate of the number of bytes that can be read\n   * (or skipped over) from this input stream without performing\n   * network I/O.\n   * This may return more than what is actually present in the block.\n   */\n  int available();\n\n  /**\n   * Close the block reader.\n   *\n   * @throws IOException\n   */\n  @Override // java.io.Closeable\n  void close() throws IOException;\n\n  /**\n   * Read exactly the given amount of data, throwing an exception\n   * if EOF is reached before that amount\n   */\n  void readFully(byte[] buf, int readOffset, int amtToRead) throws IOException;\n\n  /**\n   * Similar to {@link #readFully(byte[], int, int)} except that it will",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.dfsClient.checkOpen": "  void checkOpen() throws IOException {\n    if (!clientRunning) {\n      throw new IOException(\"Filesystem closed\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.dfsClient.getConf": "  public DfsClientConf getConf() {\n    return dfsClientConf;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.dfsClient.addRetLenToReaderScope": "  void addRetLenToReaderScope(TraceScope scope, int retLen) {\n    scope.addKVAnnotation(\"retLen\", Integer.toString(retLen));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.dfsClient.toString": "  public String toString() {\n    return getClass().getSimpleName() + \"[clientName=\" + clientName\n        + \", ugi=\" + ugi + \"]\";\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.dfsClient.newReaderTraceScope": "  TraceScope newReaderTraceScope(String description, String path, long pos,\n      int reqLen) {\n    TraceScope scope = newPathTraceScope(description, path);\n    scope.addKVAnnotation(\"pos\", Long.toString(pos));\n    scope.addKVAnnotation(\"reqLen\", Integer.toString(reqLen));\n    return scope;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.dfsClient.newPathTraceScope": "  TraceScope newPathTraceScope(String description, String path) {\n    TraceScope scope = tracer.newScope(description);\n    if (path != null) {\n      scope.addKVAnnotation(\"path\", path);\n    }\n    return scope;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.isStriped": "  public boolean isStriped() {\n    return HeaderFormat.isStriped(header);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPathAccess": "  void checkPathAccess(FSPermissionChecker pc, INodesInPath iip,\n      FsAction access) throws AccessControlException {\n    checkPermission(pc, iip, false, null, null, access, null);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission": "  void checkPermission(FSPermissionChecker pc, INodesInPath iip,\n      boolean doCheckOwner, FsAction ancestorAccess, FsAction parentAccess,\n      FsAction access, FsAction subAccess, boolean ignoreEmptyDir)\n      throws AccessControlException {\n    if (!pc.isSuperUser()) {\n      readLock();\n      try {\n        pc.checkPermission(iip, doCheckOwner, ancestorAccess,\n            parentAccess, access, subAccess, ignoreEmptyDir);\n      } finally {\n        readUnlock();\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo": "  static HdfsFileStatus getFileInfo(FSDirectory fsd, INodesInPath iip)\n    throws IOException {\n    fsd.readLock();\n    try {\n      HdfsFileStatus status = null;\n      if (FSDirectory.isExactReservedName(iip.getPathComponents())) {\n        status = FSDirectory.DOT_RESERVED_STATUS;\n      } else if (iip.isDotSnapshotDir()) {\n        if (fsd.getINode4DotSnapshot(iip) != null) {\n          status = FSDirectory.DOT_SNAPSHOT_DIR_STATUS;\n        }\n      } else {\n        status = getFileInfo(fsd, iip, true);\n      }\n      return status;\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getStoragePolicyID": "  private static byte getStoragePolicyID(byte inodePolicy, byte parentPolicy) {\n    return inodePolicy != HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED\n        ? inodePolicy : parentPolicy;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.createFileStatus": "  private static HdfsFileStatus createFileStatus(long length, boolean isdir,\n      int replication, long blocksize, long mtime,\n      long atime, FsPermission permission, String owner, String group,\n      byte[] symlink, byte[] path, long fileId, int childrenNum,\n      FileEncryptionInfo feInfo, byte storagePolicy,\n      ErasureCodingPolicy ecPolicy, LocatedBlocks locations) {\n    if (locations == null) {\n      return new HdfsFileStatus(length, isdir, replication, blocksize,\n          mtime, atime, permission, owner, group, symlink, path, fileId,\n          childrenNum, feInfo, storagePolicy, ecPolicy);\n    } else {\n      return new HdfsLocatedFileStatus(length, isdir, replication, blocksize,\n          mtime, atime, permission, owner, group, symlink, path, fileId,\n          locations, childrenNum, feInfo, storagePolicy, ecPolicy);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.isPermissionEnabled": "  boolean isPermissionEnabled() {\n    return isPermissionEnabled;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getBlockManager": "  BlockManager getBlockManager() {\n    return getFSNamesystem().getBlockManager();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFSNamesystem": "  FSNamesystem getFSNamesystem() {\n    return namesystem;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getPath": "  public String getPath(int pos) {\n    return DFSUtil.byteArray2PathString(path, 0, pos + 1); // it's a length...\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf": "  public static INodeFile valueOf(INode inode, String path, boolean acceptNull)\n      throws FileNotFoundException {\n    if (inode == null) {\n      if (acceptNull) {\n        return null;\n      } else {\n        throw new FileNotFoundException(\"File does not exist: \" + path);\n      }\n    }\n    if (!inode.isFile()) {\n      throw new FileNotFoundException(\"Path is not a file: \" + path);\n    }\n    return inode.asFile();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.asFile": "  public final INodeFile asFile() {\n    return this;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.isFile": "  public final boolean isFile() {\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.getStoragePolicyID": "  public byte getStoragePolicyID() {\n    byte id = getLocalStoragePolicyID();\n    if (id == BLOCK_STORAGE_POLICY_ID_UNSPECIFIED) {\n      id = this.getParent() != null ?\n          this.getParent().getStoragePolicyID() : id;\n    }\n\n    // For Striped EC files, we support only suitable policies. Current\n    // supported policies are HOT, COLD, ALL_SSD.\n    // If the file was set with any other policies, then we just treat policy as\n    // BLOCK_STORAGE_POLICY_ID_UNSPECIFIED.\n    if (isStriped() && id != BLOCK_STORAGE_POLICY_ID_UNSPECIFIED\n        && !ErasureCodingPolicyManager\n            .checkStoragePolicySuitableForECStripedMode(id)) {\n      id = HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"The current effective storage policy id : \" + id\n            + \" is not suitable for striped mode EC file : \" + getName()\n            + \". So, just returning unspecified storage policy id\");\n      }\n    }\n\n    return id;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.getName": "  public String getName() {\n    // Get the full path name of this inode.\n    return getFullPathName();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.getLocalStoragePolicyID": "  public byte getLocalStoragePolicyID() {\n    return HeaderFormat.getStoragePolicyID(header);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.writeUnlock": "  void writeUnlock() {\n    this.dirLock.writeLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.writeLock": "  void writeLock() {\n    this.dirLock.writeLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.getLastBlock": "  public BlockInfo getLastBlock() {\n    return blocks.length == 0 ? null: blocks[blocks.length-1];\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath": "  static String resolvePath(String src,\n      FSDirectory fsd) throws FileNotFoundException {\n    byte[][] pathComponents = INode.getPathComponents(src);\n    pathComponents = resolveComponents(pathComponents, fsd);\n    return DFSUtil.byteArray2PathString(pathComponents);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolveComponents": "  static byte[][] resolveComponents(byte[][] pathComponents,\n      FSDirectory fsd) throws FileNotFoundException {\n    final int nComponents = pathComponents.length;\n    if (nComponents < 3 || !isReservedName(pathComponents)) {\n      /* This is not a /.reserved/ path so do nothing. */\n    } else if (Arrays.equals(DOT_INODES, pathComponents[2])) {\n      /* It's a /.reserved/.inodes path. */\n      if (nComponents > 3) {\n        pathComponents = resolveDotInodesPath(pathComponents, fsd);\n      }\n    } else if (Arrays.equals(RAW, pathComponents[2])) {\n      /* It's /.reserved/raw so strip off the /.reserved/raw prefix. */\n      if (nComponents == 3) {\n        pathComponents = new byte[][]{INodeDirectory.ROOT_NAME};\n      } else {\n        if (nComponents == 4\n            && Arrays.equals(DOT_RESERVED, pathComponents[3])) {\n          /* It's /.reserved/raw/.reserved so don't strip */\n        } else {\n          pathComponents = constructRemainingPath(\n              new byte[][]{INodeDirectory.ROOT_NAME}, pathComponents, 3);\n        }\n      }\n    }\n    return pathComponents;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getInode": "  public INode getInode(long id) {\n    readLock();\n    try {\n      return inodeMap.get(id);\n    } finally {\n      readUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getPathComponents": "  public static byte[][] getPathComponents(INode inode) {\n    List<byte[]> components = new ArrayList<byte[]>();\n    components.add(0, inode.getLocalNameBytes());\n    while(inode.getParent() != null) {\n      components.add(0, inode.getParent().getLocalNameBytes());\n      inode = inode.getParent();\n    }\n    return components.toArray(new byte[components.size()][]);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse": "  void checkTraverse(FSPermissionChecker pc, INodesInPath iip,\n      DirOp dirOp) throws AccessControlException, UnresolvedPathException,\n          ParentNotDirectoryException {\n    final boolean resolveLink;\n    switch (dirOp) {\n      case READ_LINK:\n      case WRITE_LINK:\n      case CREATE_LINK:\n        resolveLink = false;\n        break;\n      default:\n        resolveLink = true;\n        break;\n    }\n    checkTraverse(pc, iip, resolveLink);\n    boolean allowSnapshot = (dirOp == DirOp.READ || dirOp == DirOp.READ_LINK);\n    if (!allowSnapshot && iip.isSnapshot()) {\n      throw new SnapshotAccessControlException(\n          \"Modification on a read-only snapshot is disallowed\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.isReservedRawName": "  static boolean isReservedRawName(byte[][] components) {\n    return (components.length > 2) &&\n           isReservedName(components) &&\n           Arrays.equals(RAW, components[2]);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getLastINode": "  public INode getLastINode() {\n    return getINode(-1);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getINode": "  public INode getINode(int i) {\n    if (inodes == null || inodes.length == 0) {\n      throw new NoSuchElementException(\"inodes is null or empty\");\n    }\n    int index = i >= 0 ? i : inodes.length + i;\n    if (index < inodes.length && index >= 0) {\n      return inodes[index];\n    } else {\n      throw new NoSuchElementException(\"inodes.length == \" + inodes.length);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.isDirectory": "  public boolean isDirectory() {\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertCreateFlag": "  public static EnumSetWritable<CreateFlag> convertCreateFlag(int flag) {\n    EnumSet<CreateFlag> result =\n        EnumSet.noneOf(CreateFlag.class);\n    if ((flag & CreateFlagProto.APPEND_VALUE) == CreateFlagProto.APPEND_VALUE) {\n      result.add(CreateFlag.APPEND);\n    }\n    if ((flag & CreateFlagProto.CREATE_VALUE) == CreateFlagProto.CREATE_VALUE) {\n      result.add(CreateFlag.CREATE);\n    }\n    if ((flag & CreateFlagProto.OVERWRITE_VALUE)\n        == CreateFlagProto.OVERWRITE_VALUE) {\n      result.add(CreateFlag.OVERWRITE);\n    }\n    if ((flag & CreateFlagProto.LAZY_PERSIST_VALUE)\n        == CreateFlagProto.LAZY_PERSIST_VALUE) {\n      result.add(CreateFlag.LAZY_PERSIST);\n    }\n    if ((flag & CreateFlagProto.NEW_BLOCK_VALUE)\n        == CreateFlagProto.NEW_BLOCK_VALUE) {\n      result.add(CreateFlag.NEW_BLOCK);\n    }\n    return new EnumSetWritable<>(result, CreateFlag.class);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertLocatedBlock": "  public static LocatedBlock[] convertLocatedBlock(LocatedBlockProto[] lb) {\n    if (lb == null) return null;\n    return convertLocatedBlock(Arrays.asList(lb)).toArray(\n        new LocatedBlock[lb.length]);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convert": "  static List<DatanodeInfosProto> convert(DatanodeInfo[][] targets) {\n    DatanodeInfosProto[] ret = new DatanodeInfosProto[targets.length];\n    for (int i = 0; i < targets.length; i++) {\n      ret[i] = DatanodeInfosProto.newBuilder()\n          .addAllDatanodes(convert(targets[i])).build();\n    }\n    return Arrays.asList(ret);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertLocatedBlockProto": "  public static LocatedBlock convertLocatedBlockProto(LocatedBlockProto proto) {\n    if (proto == null) return null;\n    List<DatanodeInfoProto> locs = proto.getLocsList();\n    DatanodeInfo[] targets = new DatanodeInfo[locs.size()];\n    for (int i = 0; i < locs.size(); i++) {\n      targets[i] = convert(locs.get(i));\n    }\n\n    final StorageType[] storageTypes = convertStorageTypes(\n        proto.getStorageTypesList(), locs.size());\n\n    final int storageIDsCount = proto.getStorageIDsCount();\n    final String[] storageIDs;\n    if (storageIDsCount == 0) {\n      storageIDs = null;\n    } else {\n      Preconditions.checkState(storageIDsCount == locs.size());\n      storageIDs = proto.getStorageIDsList()\n          .toArray(new String[storageIDsCount]);\n    }\n\n    byte[] indices = null;\n    if (proto.hasBlockIndices()) {\n      indices = proto.getBlockIndices().toByteArray();\n    }\n\n    // Set values from the isCached list, re-using references from loc\n    List<DatanodeInfo> cachedLocs = new ArrayList<>(locs.size());\n    List<Boolean> isCachedList = proto.getIsCachedList();\n    for (int i=0; i<isCachedList.size(); i++) {\n      if (isCachedList.get(i)) {\n        cachedLocs.add(targets[i]);\n      }\n    }\n\n    final LocatedBlock lb;\n    if (indices == null) {\n      lb = new LocatedBlock(PBHelperClient.convert(proto.getB()), targets,\n          storageIDs, storageTypes, proto.getOffset(), proto.getCorrupt(),\n          cachedLocs.toArray(new DatanodeInfo[cachedLocs.size()]));\n    } else {\n      lb = new LocatedStripedBlock(PBHelperClient.convert(proto.getB()),\n          targets, storageIDs, storageTypes, indices, proto.getOffset(),\n          proto.getCorrupt(),\n          cachedLocs.toArray(new DatanodeInfo[cachedLocs.size()]));\n      List<TokenProto> tokenProtos = proto.getBlockTokensList();\n      Token<BlockTokenIdentifier>[] blockTokens =\n          convertTokens(tokenProtos);\n      ((LocatedStripedBlock) lb).setBlockTokens(blockTokens);\n    }\n    lb.setBlockToken(convert(proto.getBlockToken()));\n\n    return lb;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.addStorageTypes": "  private static void addStorageTypes(\n      HdfsProtos.StorageTypeQuotaInfosProto typeQuotaInfos,\n      QuotaUsage.Builder builder) {\n    for (HdfsProtos.StorageTypeQuotaInfoProto info :\n        typeQuotaInfos.getTypeQuotaInfoList()) {\n      StorageType type = convertStorageType(info.getType());\n      builder.typeConsumed(type, info.getConsumed());\n      builder.typeQuota(type, info.getQuota());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertStorageType": "  public static StorageType convertStorageType(StorageTypeProto type) {\n    switch(type) {\n    case DISK:\n      return StorageType.DISK;\n    case SSD:\n      return StorageType.SSD;\n    case ARCHIVE:\n      return StorageType.ARCHIVE;\n    case RAM_DISK:\n      return StorageType.RAM_DISK;\n    default:\n      throw new IllegalStateException(\n          \"BUG: StorageTypeProto not found, type=\" + type);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelperClient.getByteString": "  public static ByteString getByteString(byte[] bytes) {\n    // return singleton to reduce object allocation\n    return (bytes.length == 0) ? ByteString.EMPTY : ByteString.copyFrom(bytes);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcWritable.wrap": "    public static Buffer wrap(ByteBuffer bb) {\n      return new Buffer(bb);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.updateMetrics": "  void updateMetrics(String name, int queueTime, int processingTime) {\n    rpcMetrics.addRpcQueueTime(queueTime);\n    rpcMetrics.addRpcProcessingTime(processingTime);\n    rpcDetailedMetrics.addProcessingTime(name, processingTime);\n    callQueue.addResponseTime(name, getPriorityLevel(), queueTime,\n        processingTime);\n\n    if (isLogSlowRPC()) {\n      logSlowRpcCalls(name, processingTime);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.logSlowRpcCalls": "  void logSlowRpcCalls(String methodName, int processingTime) {\n    final int deviation = 3;\n\n    // 1024 for minSampleSize just a guess -- not a number computed based on\n    // sample size analysis. It is chosen with the hope that this\n    // number is high enough to avoid spurious logging, yet useful\n    // in practice.\n    final int minSampleSize = 1024;\n    final double threeSigma = rpcMetrics.getProcessingMean() +\n        (rpcMetrics.getProcessingStdDev() * deviation);\n\n    if ((rpcMetrics.getProcessingSampleCount() > minSampleSize) &&\n        (processingTime > threeSigma)) {\n      if(LOG.isWarnEnabled()) {\n        String client = CurCall.get().toString();\n        LOG.warn(\n            \"Slow RPC : \" + methodName + \" took \" + processingTime +\n                \" milliseconds to process from client \" + client);\n      }\n      rpcMetrics.incrSlowRpc();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.getPriorityLevel": "    public int getPriorityLevel() {\n      return this.priorityLevel;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.isLogSlowRPC": "  protected boolean isLogSlowRPC() {\n    return logSlowRPC;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcServerException.getRpcStatusProto": "  public RpcStatusProto getRpcStatusProto() {\n    return RpcStatusProto.ERROR;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.CallerContext.setCurrent": "  public static void setCurrent(CallerContext callerContext) {\n    CurrentCallerContextHolder.CALLER_CONTEXT.set(callerContext);\n  }"
        },
        "bug_report": {
            "Title": "Concurrent append and read operations lead to checksum error",
            "Description": "If there are two clients, one of them open-append-close a file continuously, while the other open-read-close the same file continuously, the reader eventually gets a checksum error in the data read.\n\nOn my local Mac, it takes a few minutes to produce the error. This happens to httpfs clients, but there's no reason not believe this happens to any append clients.\n\nI have a unit test that demonstrates the checksum error. Will attach later.\n\nRelevant log:\n{quote}\n2016-10-25 15:34:45,153 INFO  audit - allowed=true\tugi=weichiu (auth:SIMPLE)\tip=/127.0.0.1\tcmd=open\tsrc=/tmp/bar.txt\tdst=null\tperm=null\tproto=rpc\n2016-10-25 15:34:45,155 INFO  DataNode - Receiving BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182 src: /127.0.0.1:51130 dest: /127.0.0.1:50131\n2016-10-25 15:34:45,155 INFO  FsDatasetImpl - Appending to FinalizedReplica, blk_1073741825_1182, FINALIZED\n  getNumBytes()     = 182\n  getBytesOnDisk()  = 182\n  getVisibleLength()= 182\n  getVolume()       = /Users/weichiu/sandbox/hadoop/hadoop-hdfs-project/hadoop-hdfs-httpfs/target/test-dir/dfs/data/data1\n  getBlockURI()     = file:/Users/weichiu/sandbox/hadoop/hadoop-hdfs-project/hadoop-hdfs-httpfs/target/test-dir/dfs/data/data1/current/BP-837130339-172.16.1.88-1477434851452/current/finalized/subdir0/subdir0/blk_1073741825\n2016-10-25 15:34:45,167 INFO  DataNode - opReadBlock BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182 received exception java.io.IOException: No data exists for block BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182\n2016-10-25 15:34:45,167 WARN  DataNode - DatanodeRegistration(127.0.0.1:50131, datanodeUuid=41c96335-5e4b-4950-ac22-3d21b353abb8, infoPort=50133, infoSecurePort=0, ipcPort=50134, storageInfo=lv=-57;cid=testClusterID;nsid=1472068852;c=1477434851452):Got exception while serving BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182 to /127.0.0.1:51121\njava.io.IOException: No data exists for block BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182\n\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getBlockInputStream(FsDatasetImpl.java:773)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:400)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:581)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:150)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:102)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:289)\n\tat java.lang.Thread.run(Thread.java:745)\n2016-10-25 15:34:45,168 INFO  FSNamesystem - updatePipeline(blk_1073741825_1182, newGS=1183, newLength=182, newNodes=[127.0.0.1:50131], client=DFSClient_NONMAPREDUCE_-1743096965_197)\n2016-10-25 15:34:45,168 ERROR DataNode - 127.0.0.1:50131:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:51121 dst: /127.0.0.1:50131\njava.io.IOException: No data exists for block BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182\n\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getBlockInputStream(FsDatasetImpl.java:773)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:400)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:581)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:150)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:102)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:289)\n\tat java.lang.Thread.run(Thread.java:745)\n2016-10-25 15:34:45,168 INFO  FSNamesystem - updatePipeline(blk_1073741825_1182 => blk_1073741825_1183) success\n2016-10-25 15:34:45,170 WARN  DFSClient - Found Checksum error for BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182 from DatanodeInfoWithStorage[127.0.0.1:50131,DS-a1878418-4f7f-4fc9-b3f7-d7ed780b5373,DISK] at 0\n2016-10-25 15:34:45,170 WARN  DFSClient - No live nodes contain block BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182 after checking nodes = [DatanodeInfoWithStorage[127.0.0.1:50131,DS-a1878418-4f7f-4fc9-b3f7-d7ed780b5373,DISK]], ignoredNodes = null\n2016-10-25 15:34:45,170 INFO  DFSClient - Could not obtain BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:50131,DS-a1878418-4f7f-4fc9-b3f7-d7ed780b5373,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:50131,DS-a1878418-4f7f-4fc9-b3f7-d7ed780b5373,DISK]. Will get new block locations from namenode and retry...\n2016-10-25 15:34:45,170 WARN  DFSClient - DFS chooseDataNode: got # 1 IOException, will wait for 981.8085941094539 msec.\n2016-10-25 15:34:45,171 INFO  clienttrace - src: /127.0.0.1:51130, dest: /127.0.0.1:50131, bytes: 183, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1743096965_197, offset: 0, srvID: 41c96335-5e4b-4950-ac22-3d21b353abb8, blockid: BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1183, duration: 2175363\n2016-10-25 15:34:45,171 INFO  DataNode - PacketResponder: BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1183, type=LAST_IN_PIPELINE terminating\n2016-10-25 15:34:45,172 INFO  FSNamesystem - BLOCK* blk_1073741825_1183 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /tmp/bar.txt\n2016-10-25 15:34:45,576 INFO  StateChange - DIR* completeFile: /tmp/bar.txt is closed by DFSClient_NONMAPREDUCE_-1743096965_197\n2016-10-25 15:34:45,577 INFO  httpfsaudit - [/tmp/bar.txt]\n2016-10-25 15:34:45,579 INFO  AppendTestUtil - seed=-3144873070946578911, size=1\n2016-10-25 15:34:45,590 INFO  audit - allowed=true\tugi=weichiu (auth:PROXY) via weichiu (auth:SIMPLE)\tip=/127.0.0.1\tcmd=append\tsrc=/tmp/bar.txt\tdst=null\tperm=null\tproto=rpc\n2016-10-25 15:34:45,593 INFO  DataNode - Receiving BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1183 src: /127.0.0.1:51132 dest: /127.0.0.1:50131\n2016-10-25 15:34:45,593 INFO  FsDatasetImpl - Appending to FinalizedReplica, blk_1073741825_1183, FINALIZED\n  getNumBytes()     = 183\n  getBytesOnDisk()  = 183\n  getVisibleLength()= 183\n  getVolume()       = /Users/weichiu/sandbox/hadoop/hadoop-hdfs-project/hadoop-hdfs-httpfs/target/test-dir/dfs/data/data1\n  getBlockURI()     = file:/Users/weichiu/sandbox/hadoop/hadoop-hdfs-project/hadoop-hdfs-httpfs/target/test-dir/dfs/data/data1/current/BP-837130339-172.16.1.88-1477434851452/current/finalized/subdir0/subdir0/blk_1073741825\n2016-10-25 15:34:45,603 INFO  FSNamesystem - updatePipeline(blk_1073741825_1183, newGS=1184, newLength=183, newNodes=[127.0.0.1:50131], client=DFSClient_NONMAPREDUCE_-1743096965_197)\n2016-10-25 15:34:45,603 INFO  FSNamesystem - updatePipeline(blk_1073741825_1183 => blk_1073741825_1184) success\n2016-10-25 15:34:45,605 INFO  clienttrace - src: /127.0.0.1:51132, dest: /127.0.0.1:50131, bytes: 184, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1743096965_197, offset: 0, srvID: 41c96335-5e4b-4950-ac22-3d21b353abb8, blockid: BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1184, duration: 1377229\n2016-10-25 15:34:45,605 INFO  DataNode - PacketResponder: BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1184, type=LAST_IN_PIPELINE terminating\n2016-10-25 15:34:45,606 INFO  FSNamesystem - BLOCK* blk_1073741825_1184 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /tmp/bar.txt\n2016-10-25 15:34:46,009 INFO  StateChange - DIR* completeFile: /tmp/bar.txt is closed by DFSClient_NONMAPREDUCE_-1743096965_197\n2016-10-25 15:34:46,010 INFO  httpfsaudit - [/tmp/bar.txt]\n2016-10-25 15:34:46,012 INFO  AppendTestUtil - seed=-263001291976323720, size=1\n2016-10-25 15:34:46,022 INFO  audit - allowed=true\tugi=weichiu (auth:PROXY) via weichiu (auth:SIMPLE)\tip=/127.0.0.1\tcmd=append\tsrc=/tmp/bar.txt\tdst=null\tperm=null\tproto=rpc\n2016-10-25 15:34:46,024 INFO  DataNode - Receiving BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1184 src: /127.0.0.1:51133 dest: /127.0.0.1:50131\n2016-10-25 15:34:46,024 INFO  FsDatasetImpl - Appending to FinalizedReplica, blk_1073741825_1184, FINALIZED\n  getNumBytes()     = 184\n  getBytesOnDisk()  = 184\n  getVisibleLength()= 184\n  getVolume()       = /Users/weichiu/sandbox/hadoop/hadoop-hdfs-project/hadoop-hdfs-httpfs/target/test-dir/dfs/data/data1\n  getBlockURI()     = file:/Users/weichiu/sandbox/hadoop/hadoop-hdfs-project/hadoop-hdfs-httpfs/target/test-dir/dfs/data/data1/current/BP-837130339-172.16.1.88-1477434851452/current/finalized/subdir0/subdir0/blk_1073741825\n2016-10-25 15:34:46,032 INFO  FSNamesystem - updatePipeline(blk_1073741825_1184, newGS=1185, newLength=184, newNodes=[127.0.0.1:50131], client=DFSClient_NONMAPREDUCE_-1743096965_197)\n2016-10-25 15:34:46,032 INFO  FSNamesystem - updatePipeline(blk_1073741825_1184 => blk_1073741825_1185) success\n2016-10-25 15:34:46,033 INFO  clienttrace - src: /127.0.0.1:51133, dest: /127.0.0.1:50131, bytes: 185, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1743096965_197, offset: 0, srvID: 41c96335-5e4b-4950-ac22-3d21b353abb8, blockid: BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1185, duration: 1112564\n2016-10-25 15:34:46,033 INFO  DataNode - PacketResponder: BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1185, type=LAST_IN_PIPELINE terminating\n2016-10-25 15:34:46,033 INFO  FSNamesystem - BLOCK* blk_1073741825_1185 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /tmp/bar.txt\n2016-10-25 15:34:46,156 INFO  audit - allowed=true\tugi=weichiu (auth:SIMPLE)\tip=/127.0.0.1\tcmd=open\tsrc=/tmp/bar.txt\tdst=null\tperm=null\tproto=rpc\n2016-10-25 15:34:46,158 INFO  StateChange - *DIR* reportBadBlocks for block: BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182 on datanode: 127.0.0.1:50131\nException in thread \"Thread-144\" java.lang.RuntimeException: org.apache.hadoop.fs.ChecksumException: Checksum CRC32C not matched for file /tmp/bar.txt at position 0: expected=C893FEDE but computed=69322F90, algorithm=PureJavaCrc32C\n\tat org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1ReaderRunnable.run(BaseTestHttpFSWith.java:309)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.fs.ChecksumException: Checksum CRC32C not matched for file /tmp/bar.txt at position 0: expected=C893FEDE but computed=69322F90, algorithm=PureJavaCrc32C\n\tat org.apache.hadoop.util.DataChecksum.throwChecksumException(DataChecksum.java:407)\n\tat org.apache.hadoop.util.DataChecksum.verifyChunked(DataChecksum.java:351)\n\tat org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:311)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.readNextPacket(BlockReaderRemote.java:216)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.read(BlockReaderRemote.java:144)\n\tat org.apache.hadoop.hdfs.ByteArrayStrategy.readFromBlock(ReaderStrategy.java:119)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:704)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:765)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:814)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1ReaderRunnable.run(BaseTestHttpFSWith.java:302)\n\t... 1 more\n2016-10-25 15:34:46,437 INFO  StateChange - DIR* completeFile: /tmp/bar.txt is closed by DFSClient_NONMAPREDUCE_-1743096965_197\n2016-10-25 15:34:46,437 INFO  httpfsaudit - [/tmp/bar.txt]\n2016-10-25 15:34:46,440 INFO  AppendTestUtil - seed=8756761565208093670, size=1\n2016-10-25 15:34:46,450 WARN  StateChange - DIR* NameSystem.append: append: lastBlock=blk_1073741825_1185 of src=/tmp/bar.txt is not sufficiently replicated yet.\n2016-10-25 15:34:46,450 INFO  Server - IPC Server handler 7 on 50130, call Call#25082 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.append from 127.0.0.1:50147\njava.io.IOException: append: lastBlock=blk_1073741825_1185 of src=/tmp/bar.txt is not sufficiently replicated yet.\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:136)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2423)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:773)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:444)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:467)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:990)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:845)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:788)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1795)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2535)\n\nException in thread \"Thread-143\" java.lang.RuntimeException: java.io.IOException: HTTP status [500], exception [org.apache.hadoop.ipc.RemoteException], message [append: lastBlock=blk_1073741825_1185 of src=/tmp/bar.txt is not sufficiently replicated yet.] \n\tat org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1.run(BaseTestHttpFSWith.java:283)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: HTTP status [500], exception [org.apache.hadoop.ipc.RemoteException], message [append: lastBlock=blk_1073741825_1185 of src=/tmp/bar.txt is not sufficiently replicated yet.] \n\tat org.apache.hadoop.util.HttpExceptionUtils.validateResponse(HttpExceptionUtils.java:159)\n\tat org.apache.hadoop.fs.http.client.HttpFSFileSystem$HttpFSDataOutputStream.close(HttpFSFileSystem.java:470)\n\tat org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1.run(BaseTestHttpFSWith.java:279)\n\t... 1 more\n\norg.apache.hadoop.fs.ChecksumException: Checksum CRC32C not matched for file /tmp/bar.txt at position 0: expected=C893FEDE but computed=69322F90, algorithm=PureJavaCrc32C\n\n\tat org.apache.hadoop.util.DataChecksum.throwChecksumException(DataChecksum.java:407)\n\tat org.apache.hadoop.util.DataChecksum.verifyChunked(DataChecksum.java:351)\n\tat org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:311)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.readNextPacket(BlockReaderRemote.java:216)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.read(BlockReaderRemote.java:144)\n\tat org.apache.hadoop.hdfs.ByteArrayStrategy.readFromBlock(ReaderStrategy.java:119)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:704)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:765)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:814)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1ReaderRunnable.run(BaseTestHttpFSWith.java:302)\n\tat java.lang.Thread.run(Thread.java:745)\n{quote}\n\n"
        }
    },
    {
        "filename": "HDFS-6825.json",
        "creation_time": "2014-08-06T01:31:34.000+0000",
        "stack_trace": "```\njava.io.FileNotFoundException: Path not found: /solr/hierarchy/core_node1/data/tlog/tlog.xyz\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateSpaceConsumed(FSDirectory.java:1807)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitOrCompleteLastBlock(FSNamesystem.java:3975)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.closeFileCommitBlocks(FSNamesystem.java:4178)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:4146)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization(NameNodeRpcServer.java:662)\n        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolServerSideTranslatorPB.java:270)\n        at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:28073)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateSpaceConsumed": "  void updateSpaceConsumed(String path, long nsDelta, long dsDelta)\n      throws QuotaExceededException, FileNotFoundException,\n          UnresolvedLinkException, SnapshotAccessControlException {\n    writeLock();\n    try {\n      final INodesInPath iip = getINodesInPath4Write(path, false);\n      if (iip.getLastINode() == null) {\n        throw new FileNotFoundException(\"Path not found: \" + path);\n      }\n      updateCount(iip, nsDelta, dsDelta, true);\n    } finally {\n      writeUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.writeUnlock": "  void writeUnlock() {\n    this.dirLock.writeLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.writeLock": "  void writeLock() {\n    this.dirLock.writeLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount": "  private void updateCount(INodesInPath iip, int numOfINodes, \n                           long nsDelta, long dsDelta, boolean checkQuota)\n                           throws QuotaExceededException {\n    assert hasWriteLock();\n    if (!namesystem.isImageLoaded()) {\n      //still initializing. do not check or update quotas.\n      return;\n    }\n    final INode[] inodes = iip.getINodes();\n    if (numOfINodes > inodes.length) {\n      numOfINodes = inodes.length;\n    }\n    if (checkQuota && !skipQuotaCheck) {\n      verifyQuota(inodes, numOfINodes, nsDelta, dsDelta, null);\n    }\n    unprotectedUpdateCount(iip, numOfINodes, nsDelta, dsDelta);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getINodesInPath4Write": "  private INodesInPath getINodesInPath4Write(String src, boolean resolveLink)\n          throws UnresolvedLinkException, SnapshotAccessControlException {\n    final byte[][] components = INode.getPathComponents(src);\n    INodesInPath inodesInPath = INodesInPath.resolve(rootDir, components,\n            components.length, resolveLink);\n    if (inodesInPath.isSnapshot()) {\n      throw new SnapshotAccessControlException(\n              \"Modification on a read-only snapshot is disallowed\");\n    }\n    return inodesInPath;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitOrCompleteLastBlock": "  private void commitOrCompleteLastBlock(final INodeFile fileINode,\n      final Block commitBlock) throws IOException {\n    assert hasWriteLock();\n    Preconditions.checkArgument(fileINode.isUnderConstruction());\n    if (!blockManager.commitOrCompleteLastBlock(fileINode, commitBlock)) {\n      return;\n    }\n\n    // Adjust disk space consumption if required\n    final long diff = fileINode.getPreferredBlockSize() - commitBlock.getNumBytes();    \n    if (diff > 0) {\n      try {\n        String path = fileINode.getFullPathName();\n        dir.updateSpaceConsumed(path, 0, -diff*fileINode.getFileReplication());\n      } catch (IOException e) {\n        LOG.warn(\"Unexpected exception while updating disk space.\", e);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.hasWriteLock": "  public boolean hasWriteLock() {\n    return this.fsLock.isWriteLockedByCurrentThread();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPreferredBlockSize": "  long getPreferredBlockSize(String filename) \n      throws IOException, UnresolvedLinkException {\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.READ);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(filename);\n    readLock();\n    try {\n      checkOperation(OperationCategory.READ);\n      filename = FSDirectory.resolvePath(filename, pathComponents, dir);\n      if (isPermissionEnabled) {\n        checkTraverse(pc, filename);\n      }\n      return dir.getPreferredBlockSize(filename);\n    } finally {\n      readUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.closeFileCommitBlocks": "  String closeFileCommitBlocks(INodeFile pendingFile, BlockInfo storedBlock)\n      throws IOException {\n    String src = pendingFile.getFullPathName();\n\n    // commit the last block and complete it if it has minimum replicas\n    commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n    //remove lease, close file\n    finalizeINodeFileUnderConstruction(src, pendingFile,\n        Snapshot.findLatestSnapshot(pendingFile, Snapshot.CURRENT_STATE_ID));\n\n    return src;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.finalizeINodeFileUnderConstruction": "  private void finalizeINodeFileUnderConstruction(String src,\n      INodeFile pendingFile, int latestSnapshot) throws IOException,\n      UnresolvedLinkException {\n    assert hasWriteLock();\n\n    FileUnderConstructionFeature uc = pendingFile.getFileUnderConstructionFeature();\n    Preconditions.checkArgument(uc != null);\n    leaseManager.removeLease(uc.getClientName(), src);\n    \n    pendingFile = pendingFile.recordModification(latestSnapshot);\n\n    // The file is no longer pending.\n    // Create permanent INode, update blocks. No need to replace the inode here\n    // since we just remove the uc feature from pendingFile\n    final INodeFile newFile = pendingFile.toCompleteFile(now());\n\n    waitForLoadingFSImage();\n    // close file and persist block allocations for this file\n    closeFile(src, newFile);\n\n    blockManager.checkReplication(newFile);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock=\" + lastblock\n             + \", newgenerationstamp=\" + newgenerationstamp\n             + \", newlength=\" + newlength\n             + \", newtargets=\" + Arrays.asList(newtargets)\n             + \", closeFile=\" + closeFile\n             + \", deleteBlock=\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src = \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock = getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock == null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (=\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (=\" + lastblock + \") not found\");\n        }\n      }\n      INodeFile iFile = ((INode)storedBlock.getBlockCollection()).asFile();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (=\" + lastblock\n                    + \") since the file (=\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      long recoveryId =\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId != newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      if (deleteblock) {\n        Block blockToDel = ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove = iFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        ArrayList<DatanodeDescriptor> trimmedTargets =\n            new ArrayList<DatanodeDescriptor>(newtargets.length);\n        ArrayList<String> trimmedStorages =\n            new ArrayList<String>(newtargets.length);\n        if (newtargets.length > 0) {\n          for (int i = 0; i < newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode =\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode != null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (=\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) && !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i = 0; i < trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo =\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo != null) {\n              storageInfo.addBlock(storedBlock);\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos =\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n      }\n\n      if (closeFile) {\n        src = closeFileCommitBlocks(iFile, storedBlock);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src = iFile.getFullPathName();\n        persistBlocks(src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock=\" + lastblock\n          + \", file=\" + src\n          + \", newgenerationstamp=\" + newgenerationstamp\n          + \", newlength=\" + newlength\n          + \", newtargets=\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.persistBlocks": "  private void persistBlocks(String path, INodeFile file,\n                             boolean logRetryCache) {\n    assert hasWriteLock();\n    Preconditions.checkArgument(file.isUnderConstruction());\n    getEditLog().logUpdateBlocks(path, file, logRetryCache);\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"persistBlocks: \" + path\n              + \" with \" + file.getBlocks().length + \" blocks is persisted to\" +\n              \" the file system\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock": "  public void writeUnlock() {\n    this.fsLock.writeLock().unlock();\n    this.fsLock.longReadLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.waitForLoadingFSImage": "  void waitForLoadingFSImage() {\n    if (!imageLoaded) {\n      writeLock();\n      try {\n        while (!imageLoaded) {\n          try {\n            cond.await(5000, TimeUnit.MILLISECONDS);\n          } catch (InterruptedException ignored) {\n          }\n        }\n      } finally {\n        writeUnlock();\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getStoredBlock": "  BlockInfo getStoredBlock(Block block) {\n    return blockManager.getStoredBlock(block);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEditLog": "  public FSEditLog getEditLog() {\n    return getFSImage().getEditLog();\n  }    ",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode": "  private void checkNameNodeSafeMode(String errorMsg)\n      throws RetriableException, SafeModeException {\n    if (isInSafeMode()) {\n      SafeModeException se = new SafeModeException(errorMsg, safeMode);\n      if (haEnabled && haContext != null\n          && haContext.getState().getServiceState() == HAServiceState.ACTIVE\n          && shouldRetrySafeMode(this.safeMode)) {\n        throw new RetriableException(se);\n      } else {\n        throw se;\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock": "  public void writeLock() {\n    this.fsLock.longReadLock().lock();\n    this.fsLock.writeLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation": "  public void checkOperation(OperationCategory op) throws StandbyException {\n    if (haContext != null) {\n      // null in some unit tests\n      haContext.checkOperation(op);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization": "  public void commitBlockSynchronization(ExtendedBlock block,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException {\n    namesystem.commitBlockSynchronization(block, newgenerationstamp,\n        newlength, closeFile, deleteblock, newtargets, newtargetstorages);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.commitBlockSynchronization": "  public CommitBlockSynchronizationResponseProto commitBlockSynchronization(\n      RpcController controller, CommitBlockSynchronizationRequestProto request)\n      throws ServiceException {\n    List<DatanodeIDProto> dnprotos = request.getNewTaragetsList();\n    DatanodeID[] dns = new DatanodeID[dnprotos.size()];\n    for (int i = 0; i < dnprotos.size(); i++) {\n      dns[i] = PBHelper.convert(dnprotos.get(i));\n    }\n    final List<String> sidprotos = request.getNewTargetStoragesList();\n    final String[] storageIDs = sidprotos.toArray(new String[sidprotos.size()]);\n    try {\n      impl.commitBlockSynchronization(PBHelper.convert(request.getBlock()),\n          request.getNewGenStamp(), request.getNewLength(),\n          request.getCloseFile(), request.getDeleteBlock(), dns, storageIDs);\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n    return VOID_COMMIT_BLOCK_SYNCHRONIZATION_RESPONSE_PROTO;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcRequestWrapper request = (RpcRequestWrapper) writableRequest;\n        RequestHeaderProto rpcRequest = request.requestHeader;\n        String methodName = rpcRequest.getMethodName();\n        \n        \n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = prototype.newBuilderForType()\n            .mergeFrom(request.theRequestRead).build();\n        \n        Message result;\n        long startTime = Time.now();\n        int qTime = (int) (startTime - receiveTime);\n        Exception exception = null;\n        try {\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n        } catch (ServiceException e) {\n          exception = (Exception) e.getCause();\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          exception = e;\n          throw e;\n        } finally {\n          int processingTime = (int) (Time.now() - startTime);\n          if (LOG.isDebugEnabled()) {\n            String msg = \"Served: \" + methodName + \" queueTime= \" + qTime +\n                \" procesingTime= \" + processingTime;\n            if (exception != null) {\n              msg += \" exception= \" + exception.getClass().getSimpleName();\n            }\n            LOG.debug(msg);\n          }\n          String detailedMetricsName = (exception == null) ?\n              methodName :\n              exception.getClass().getSimpleName();\n          server.rpcMetrics.addRpcQueueTime(qTime);\n          server.rpcMetrics.addRpcProcessingTime(processingTime);\n          server.rpcDetailedMetrics.addProcessingTime(detailedMetricsName,\n              processingTime);\n        }\n        return new RpcResponseWrapper(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getProtocolImpl": "      private static ProtoClassProtoImpl getProtocolImpl(RPC.Server server,\n          String protoName, long clientVersion) throws RpcServerException {\n        ProtoNameVer pv = new ProtoNameVer(protoName, clientVersion);\n        ProtoClassProtoImpl impl = \n            server.getProtocolImplMap(RPC.RpcKind.RPC_PROTOCOL_BUFFER).get(pv);\n        if (impl == null) { // no match for Protocol AND Version\n          VerProtocolImpl highest = \n              server.getHighestSupportedProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER, \n                  protoName);\n          if (highest == null) {\n            throw new RpcNoSuchProtocolException(\n                \"Unknown protocol: \" + protoName);\n          }\n          // protocol supported but not the version that client wants\n          throw new RPC.VersionMismatch(protoName, clientVersion,\n              highest.version);\n        }\n        return impl;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.call": "    public Writable call(RPC.RpcKind rpcKind, String protocol,\n        Writable rpcRequest, long receiveTime) throws Exception {\n      return getRpcInvoker(rpcKind).call(this, protocol, rpcRequest,\n          receiveTime);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.toString": "    public String toString() {\n      return getHostAddress() + \":\" + remotePort; \n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(ByteArrayOutputStream responseBuf,\n                             Call call, RpcStatusProto status, RpcErrorCodeProto erCode,\n                             Writable rv, String errorClass, String error) \n  throws IOException {\n    responseBuf.reset();\n    DataOutputStream out = new DataOutputStream(responseBuf);\n    RpcResponseHeaderProto.Builder headerBuilder =  \n        RpcResponseHeaderProto.newBuilder();\n    headerBuilder.setClientId(ByteString.copyFrom(call.clientId));\n    headerBuilder.setCallId(call.callId);\n    headerBuilder.setRetryCount(call.retryCount);\n    headerBuilder.setStatus(status);\n    headerBuilder.setServerIpcVersionNum(CURRENT_VERSION);\n\n    if (status == RpcStatusProto.SUCCESS) {\n      RpcResponseHeaderProto header = headerBuilder.build();\n      final int headerLen = header.getSerializedSize();\n      int fullLength  = CodedOutputStream.computeRawVarint32Size(headerLen) +\n          headerLen;\n      try {\n        if (rv instanceof ProtobufRpcEngine.RpcWrapper) {\n          ProtobufRpcEngine.RpcWrapper resWrapper = \n              (ProtobufRpcEngine.RpcWrapper) rv;\n          fullLength += resWrapper.getLength();\n          out.writeInt(fullLength);\n          header.writeDelimitedTo(out);\n          rv.write(out);\n        } else { // Have to serialize to buffer to get len\n          final DataOutputBuffer buf = new DataOutputBuffer();\n          rv.write(buf);\n          byte[] data = buf.getData();\n          fullLength += buf.getLength();\n          out.writeInt(fullLength);\n          header.writeDelimitedTo(out);\n          out.write(data, 0, buf.getLength());\n        }\n      } catch (Throwable t) {\n        LOG.warn(\"Error serializing call response for call \" + call, t);\n        // Call back to same function - this is OK since the\n        // buffer is reset at the top, and since status is changed\n        // to ERROR it won't infinite loop.\n        setupResponse(responseBuf, call, RpcStatusProto.ERROR,\n            RpcErrorCodeProto.ERROR_SERIALIZING_RESPONSE,\n            null, t.getClass().getName(),\n            StringUtils.stringifyException(t));\n        return;\n      }\n    } else { // Rpc Failure\n      headerBuilder.setExceptionClassName(errorClass);\n      headerBuilder.setErrorMsg(error);\n      headerBuilder.setErrorDetail(erCode);\n      RpcResponseHeaderProto header = headerBuilder.build();\n      int headerLen = header.getSerializedSize();\n      final int fullLength  = \n          CodedOutputStream.computeRawVarint32Size(headerLen) + headerLen;\n      out.writeInt(fullLength);\n      header.writeDelimitedTo(out);\n    }\n    if (call.connection.useWrap) {\n      wrapWithSasl(responseBuf, call);\n    }\n    call.setResponse(ByteBuffer.wrap(responseBuf.toByteArray()));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.remove": "    private boolean remove(Connection connection) {\n      boolean removed = connections.remove(connection);\n      if (removed) {\n        count.getAndDecrement();\n      }\n      return removed;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeAll": "    void closeAll() {\n      // use a copy of the connections to be absolutely sure the concurrent\n      // iterator doesn't miss a connection\n      for (Connection connection : toArray()) {\n        close(connection);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.isTerse": "    boolean isTerse(Class<?> t) {\n      return terseExceptions.contains(t.toString());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    boolean close(Connection connection) {\n      boolean exists = remove(connection);\n      if (exists) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(Thread.currentThread().getName() +\n              \": disconnecting client \" + connection +\n              \". Number of active connections: \"+ size());\n        }\n        // only close if actually removed to avoid double-closing due\n        // to possible races\n        connection.close();\n      }\n      return exists;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(RPC.RpcKind rpcKind, String protocol,\n      Writable param, long receiveTime) throws Exception;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param protocolName - the protocol\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  private void authorize(UserGroupInformation user, String protocolName,\n      InetAddress addr) throws AuthorizationException {\n    if (authorize) {\n      if (protocolName == null) {\n        throw new AuthorizationException(\"Null protocol not authorized\");\n      }\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(protocolName, getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         protocolName);\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isValid() && key.isWritable()) {\n                  doAsyncWrite(key);\n              }\n            } catch (IOException e) {\n              LOG.info(Thread.currentThread().getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = Time.now();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<Call> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<Call>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              Call call = (Call)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n          \n          for(Call call : calls) {\n            doPurge(call, now);\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getRpcErrorCodeProto": "    public RpcErrorCodeProto getRpcErrorCodeProto() {\n      return errCode;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.size": "    int size() {\n      return count.get();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeIdle": "    synchronized void closeIdle(boolean scanAll) {\n      long minLastContact = Time.now() - maxIdleTime;\n      // concurrent iterator might miss new connections added\n      // during the iteration, but that's ok because they won't\n      // be idle yet anyway and will be caught on next scan\n      int closed = 0;\n      for (Connection connection : connections) {\n        // stop if connections dropped below threshold unless scanning all\n        if (!scanAll && size() < idleScanThreshold) {\n          break;\n        }\n        // stop if not scanning all and max connections are closed\n        if (connection.isIdle() &&\n            connection.getLastContact() < minLastContact &&\n            close(connection) &&\n            !scanAll && (++closed == maxIdleToClose)) {\n          break;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.startIdleScan": "    void startIdleScan() {\n      scheduleIdleScanTask();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRespond": "    void doRespond(Call call) throws IOException {\n      synchronized (call.connection.responseQueue) {\n        call.connection.responseQueue.addLast(call);\n        if (call.connection.responseQueue.size() == 1) {\n          processResponse(call.connection.responseQueue, true);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.stopIdleScan": "    void stopIdleScan() {\n      idleScanTimer.cancel();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws InterruptedException, IOException,  OutOfMemoryError {\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        channel.socket().setKeepAlive(true);\n        \n        Reader reader = getReader();\n        Connection c = connectionManager.register(channel);\n        key.attach(c);  // so closeCurrentConnection can get the object\n        reader.addConnection(c);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.scheduleIdleScanTask": "    private void scheduleIdleScanTask() {\n      if (!running) {\n        return;\n      }\n      TimerTask idleScanTask = new TimerTask(){\n        @Override\n        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }\n      };\n      idleScanTimer.schedule(idleScanTask, idleScanInterval);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getLastINode": "  public INode getLastINode() {\n    return inodes[inodes.length - 1];\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.isUnderConstruction": "  public boolean isUnderConstruction() {\n    return getFileUnderConstructionFeature() != null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.getFileUnderConstructionFeature": "  public final FileUnderConstructionFeature getFileUnderConstructionFeature() {\n    return getFeature(FileUnderConstructionFeature.class);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.setLastBlock": "  public BlockInfoUnderConstruction setLastBlock(BlockInfo lastBlock,\n      DatanodeStorageInfo[] locations) throws IOException {\n    Preconditions.checkState(isUnderConstruction(),\n        \"file is no longer under construction\");\n\n    if (numBlocks() == 0) {\n      throw new IOException(\"Failed to set last block: File is empty.\");\n    }\n    BlockInfoUnderConstruction ucBlock =\n      lastBlock.convertToBlockUnderConstruction(\n          BlockUCState.UNDER_CONSTRUCTION, locations);\n    ucBlock.setBlockCollection(this);\n    setBlock(numBlocks() - 1, ucBlock);\n    return ucBlock;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.numBlocks": "  public int numBlocks() {\n    return blocks == null ? 0 : blocks.length;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.setBlock": "  public void setBlock(int index, BlockInfo blk) {\n    this.blocks[index] = blk;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.removeLastBlock": "  boolean removeLastBlock(Block oldblock) {\n    Preconditions.checkState(isUnderConstruction(),\n        \"file is no longer under construction\");\n    if (blocks == null || blocks.length == 0) {\n      return false;\n    }\n    int size_1 = blocks.length - 1;\n    if (!blocks[size_1].equals(oldblock)) {\n      return false;\n    }\n\n    //copy to a new list\n    BlockInfo[] newlist = new BlockInfo[size_1];\n    System.arraycopy(blocks, 0, newlist, 0, size_1);\n    setBlocks(newlist);\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.setBlocks": "  public void setBlocks(BlockInfo[] blocks) {\n    this.blocks = blocks;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convert": "  public static ShmId convert(ShortCircuitShmIdProto shmId) {\n    return new ShmId(shmId.getHi(), shmId.getLo());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertAclEntry": "  public static List<AclEntry> convertAclEntry(List<AclEntryProto> aclSpec) {\n    ArrayList<AclEntry> r = Lists.newArrayListWithCapacity(aclSpec.size());\n    for (AclEntryProto e : aclSpec) {\n      AclEntry.Builder builder = new AclEntry.Builder();\n      builder.setType(convert(e.getType()));\n      builder.setScope(convert(e.getScope()));\n      builder.setPermission(convert(e.getPermissions()));\n      if (e.hasName()) {\n        builder.setName(e.getName());\n      }\n      r.add(builder.build());\n    }\n    return r;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertLocatedBlock": "  public static List<LocatedBlock> convertLocatedBlock(\n      List<LocatedBlockProto> lb) {\n    if (lb == null) return null;\n    final int len = lb.size();\n    List<LocatedBlock> result = \n        new ArrayList<LocatedBlock>(len);\n    for (int i = 0; i < len; ++i) {\n      result.add(PBHelper.convert(lb.get(i)));\n    }\n    return result;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertXAttrs": "  public static List<XAttr> convertXAttrs(List<XAttrProto> xAttrSpec) {\n    ArrayList<XAttr> xAttrs = Lists.newArrayListWithCapacity(xAttrSpec.size());\n    for (XAttrProto a : xAttrSpec) {\n      XAttr.Builder builder = new XAttr.Builder();\n      builder.setNameSpace(convert(a.getNamespace()));\n      if (a.hasName()) {\n        builder.setName(a.getName());\n      }\n      if (a.hasValue()) {\n        builder.setValue(a.getValue().toByteArray());\n      }\n      xAttrs.add(builder.build());\n    }\n    return xAttrs;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertLocatedBlock2": "  public static List<LocatedBlockProto> convertLocatedBlock2(List<LocatedBlock> lb) {\n    if (lb == null) return null;\n    final int len = lb.size();\n    List<LocatedBlockProto> result = new ArrayList<LocatedBlockProto>(len);\n    for (int i = 0; i < len; ++i) {\n      result.add(PBHelper.convert(lb.get(i)));\n    }\n    return result;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertAclEntryProto": "  public static List<AclEntryProto> convertAclEntryProto(\n      List<AclEntry> aclSpec) {\n    ArrayList<AclEntryProto> r = Lists.newArrayListWithCapacity(aclSpec.size());\n    for (AclEntry e : aclSpec) {\n      AclEntryProto.Builder builder = AclEntryProto.newBuilder();\n      builder.setType(convert(e.getType()));\n      builder.setScope(convert(e.getScope()));\n      builder.setPermissions(convert(e.getPermission()));\n      if (e.getName() != null) {\n        builder.setName(e.getName());\n      }\n      r.add(builder.build());\n    }\n    return r;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertState": "  private static State convertState(StorageState state) {\n    switch(state) {\n    case READ_ONLY_SHARED:\n      return DatanodeStorage.State.READ_ONLY_SHARED;\n    case NORMAL:\n    default:\n      return DatanodeStorage.State.NORMAL;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertRollingUpgradeStatus": "  public static RollingUpgradeStatusProto convertRollingUpgradeStatus(\n      RollingUpgradeStatus status) {\n    return RollingUpgradeStatusProto.newBuilder()\n        .setBlockPoolId(status.getBlockPoolId())\n        .build();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertStorageType": "  public static StorageType convertStorageType(StorageTypeProto type) {\n    switch(type) {\n      case DISK:\n        return StorageType.DISK;\n      case SSD:\n        return StorageType.SSD;\n      default:\n        throw new IllegalStateException(\n            \"BUG: StorageTypeProto not found, type=\" + type);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertStorageTypes": "  public static StorageType[] convertStorageTypes(\n      List<StorageTypeProto> storageTypesList, int expectedSize) {\n    final StorageType[] storageTypes = new StorageType[expectedSize];\n    if (storageTypesList.size() != expectedSize) { // missing storage types\n      Preconditions.checkState(storageTypesList.isEmpty());\n      Arrays.fill(storageTypes, StorageType.DEFAULT);\n    } else {\n      for (int i = 0; i < storageTypes.length; ++i) {\n        storageTypes[i] = convertStorageType(storageTypesList.get(i));\n      }\n    }\n    return storageTypes;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.castEnum": "  private static <T extends Enum<T>, U extends Enum<U>> U castEnum(T from, U[] to) {\n    return to[from.ordinal()];\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertBlockKeys": "  public static BlockKey[] convertBlockKeys(List<BlockKeyProto> list) {\n    BlockKey[] ret = new BlockKey[list.size()];\n    int i = 0;\n    for (BlockKeyProto k : list) {\n      ret[i++] = convert(k);\n    }\n    return ret;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcServerException.getRpcStatusProto": "  public RpcStatusProto getRpcStatusProto() {\n    return RpcStatusProto.ERROR;\n  }"
        },
        "bug_report": {
            "Title": "Edit log corruption due to delayed block removal",
            "Description": "Observed the following stack:\n{code}\n2014-08-04 23:49:44,133 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(lastblock=BP-.., newgenerationstamp=..., newlength=..., newtargets=..., closeFile=true, deleteBlock=false)\n2014-08-04 23:49:44,133 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Unexpected exception while updating disk space. \njava.io.FileNotFoundException: Path not found: /solr/hierarchy/core_node1/data/tlog/tlog.xyz\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateSpaceConsumed(FSDirectory.java:1807)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitOrCompleteLastBlock(FSNamesystem.java:3975)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.closeFileCommitBlocks(FSNamesystem.java:4178)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:4146)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization(NameNodeRpcServer.java:662)\n        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolServerSideTranslatorPB.java:270)\n        at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:28073)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n{code}\n\nFound this is what happened:\n\n- client created file /solr/hierarchy/core_node1/data/tlog/tlog.xyz\n- client tried to append to this file, but the lease expired, so lease recovery is started, thus the append failed\n- the file get deleted, however, there are still pending blocks of this file not deleted\n- then commitBlockSynchronization() method is called (see stack above), an InodeFile is created out of the pending block, not aware of that the file was deleted already\n- FileNotExistException was thrown by FSDirectory.updateSpaceConsumed, but swallowed by commitOrCompleteLastBlock\n- closeFileCommitBlocks continue to call finalizeINodeFileUnderConstruction and wrote CloseOp to the edit log\n"
        }
    },
    {
        "filename": "HDFS-5710.json",
        "creation_time": "2014-01-01T04:06:03.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFullPathName(FSDirectory.java:1871)\n\tat org.apache.hadoop.hdfs.server.namenode.INode.getFullPathName(INode.java:482)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.getName(INodeFile.java:316)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.chooseTarget(BlockPlacementPolicy.java:118)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1259)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1167)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3158)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3112)\n\tat java.lang.Thread.run(Thread.java:724)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFullPathName": "  static String getFullPathName(INode inode) {\n    INode[] inodes = getFullPathINodes(inode);\n    return getFullPathName(inodes, inodes.length - 1);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFullPathINodes": "  private static INode[] getFullPathINodes(INode inode) {\n    return getRelativePathINodes(inode, null);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.getName": "  public String getName() {\n    // Get the full path name of this inode.\n    return getFullPathName();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.chooseTarget": "  DatanodeStorageInfo[] chooseTarget(String src,\n      int numOfReplicas, Node writer,\n      Set<Node> excludedNodes,\n      long blocksize,\n      List<DatanodeDescriptor> favoredNodes,\n      StorageType storageType) {\n    // This class does not provide the functionality of placing\n    // a block in favored datanodes. The implementations of this class\n    // are expected to provide this functionality\n\n    return chooseTarget(src, numOfReplicas, writer, \n        new ArrayList<DatanodeStorageInfo>(numOfReplicas), false,\n        excludedNodes, blocksize, storageType);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks": "  int computeReplicationWorkForBlocks(List<List<Block>> blocksToReplicate) {\n    int requiredReplication, numEffectiveReplicas;\n    List<DatanodeDescriptor> containingNodes;\n    DatanodeDescriptor srcNode;\n    BlockCollection bc = null;\n    int additionalReplRequired;\n\n    int scheduledWork = 0;\n    List<ReplicationWork> work = new LinkedList<ReplicationWork>();\n\n    namesystem.writeLock();\n    try {\n      synchronized (neededReplications) {\n        for (int priority = 0; priority < blocksToReplicate.size(); priority++) {\n          for (Block block : blocksToReplicate.get(priority)) {\n            // block should belong to a file\n            bc = blocksMap.getBlockCollection(block);\n            // abandoned block or block reopened for append\n            if(bc == null || bc.isUnderConstruction()) {\n              neededReplications.remove(block, priority); // remove from neededReplications\n              continue;\n            }\n\n            requiredReplication = bc.getBlockReplication();\n\n            // get a source data-node\n            containingNodes = new ArrayList<DatanodeDescriptor>();\n            List<DatanodeStorageInfo> liveReplicaNodes = new ArrayList<DatanodeStorageInfo>();\n            NumberReplicas numReplicas = new NumberReplicas();\n            srcNode = chooseSourceDatanode(\n                block, containingNodes, liveReplicaNodes, numReplicas,\n                priority);\n            if(srcNode == null) { // block can not be replicated from any node\n              LOG.debug(\"Block \" + block + \" cannot be repl from any node\");\n              continue;\n            }\n\n            assert liveReplicaNodes.size() == numReplicas.liveReplicas();\n            // do not schedule more if enough replicas is already pending\n            numEffectiveReplicas = numReplicas.liveReplicas() +\n                                    pendingReplications.getNumReplicas(block);\n      \n            if (numEffectiveReplicas >= requiredReplication) {\n              if ( (pendingReplications.getNumReplicas(block) > 0) ||\n                   (blockHasEnoughRacks(block)) ) {\n                neededReplications.remove(block, priority); // remove from neededReplications\n                blockLog.info(\"BLOCK* Removing \" + block\n                    + \" from neededReplications as it has enough replicas\");\n                continue;\n              }\n            }\n\n            if (numReplicas.liveReplicas() < requiredReplication) {\n              additionalReplRequired = requiredReplication\n                  - numEffectiveReplicas;\n            } else {\n              additionalReplRequired = 1; // Needed on a new rack\n            }\n            work.add(new ReplicationWork(block, bc, srcNode,\n                containingNodes, liveReplicaNodes, additionalReplRequired,\n                priority));\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    final Set<Node> excludedNodes = new HashSet<Node>();\n    for(ReplicationWork rw : work){\n      // Exclude all of the containing nodes from being targets.\n      // This list includes decommissioning or corrupt nodes.\n      excludedNodes.clear();\n      for (DatanodeDescriptor dn : rw.containingNodes) {\n        excludedNodes.add(dn);\n      }\n\n      // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n      // It is costly to extract the filename for which chooseTargets is called,\n      // so for now we pass in the block collection itself.\n      rw.chooseTargets(blockplacement, excludedNodes);\n    }\n\n    namesystem.writeLock();\n    try {\n      for(ReplicationWork rw : work){\n        final DatanodeStorageInfo[] targets = rw.targets;\n        if(targets == null || targets.length == 0){\n          rw.targets = null;\n          continue;\n        }\n\n        synchronized (neededReplications) {\n          Block block = rw.block;\n          int priority = rw.priority;\n          // Recheck since global lock was released\n          // block should belong to a file\n          bc = blocksMap.getBlockCollection(block);\n          // abandoned block or block reopened for append\n          if(bc == null || bc.isUnderConstruction()) {\n            neededReplications.remove(block, priority); // remove from neededReplications\n            rw.targets = null;\n            continue;\n          }\n          requiredReplication = bc.getBlockReplication();\n\n          // do not schedule more if enough replicas is already pending\n          NumberReplicas numReplicas = countNodes(block);\n          numEffectiveReplicas = numReplicas.liveReplicas() +\n            pendingReplications.getNumReplicas(block);\n\n          if (numEffectiveReplicas >= requiredReplication) {\n            if ( (pendingReplications.getNumReplicas(block) > 0) ||\n                 (blockHasEnoughRacks(block)) ) {\n              neededReplications.remove(block, priority); // remove from neededReplications\n              rw.targets = null;\n              blockLog.info(\"BLOCK* Removing \" + block\n                  + \" from neededReplications as it has enough replicas\");\n              continue;\n            }\n          }\n\n          if ( (numReplicas.liveReplicas() >= requiredReplication) &&\n               (!blockHasEnoughRacks(block)) ) {\n            if (rw.srcNode.getNetworkLocation().equals(\n                targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n              //No use continuing, unless a new rack in this case\n              continue;\n            }\n          }\n\n          // Add block to the to be replicated list\n          rw.srcNode.addBlockToBeReplicated(block, targets);\n          scheduledWork++;\n          DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n          // Move the block-replication into a \"pending\" state.\n          // The reason we use 'pending' is so we can retry\n          // replications that fail after an appropriate amount of time.\n          pendingReplications.increment(block,\n              DatanodeStorageInfo.toDatanodeDescriptors(targets));\n          if(blockLog.isDebugEnabled()) {\n            blockLog.debug(\n                \"BLOCK* block \" + block\n                + \" is moved from neededReplications to pendingReplications\");\n          }\n\n          // remove from neededReplications\n          if(numEffectiveReplicas + targets.length >= requiredReplication) {\n            neededReplications.remove(block, priority); // remove from neededReplications\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    if (blockLog.isInfoEnabled()) {\n      // log which blocks have been scheduled for replication\n      for(ReplicationWork rw : work){\n        DatanodeStorageInfo[] targets = rw.targets;\n        if (targets != null && targets.length != 0) {\n          StringBuilder targetList = new StringBuilder(\"datanode(s)\");\n          for (int k = 0; k < targets.length; k++) {\n            targetList.append(' ');\n            targetList.append(targets[k].getDatanodeDescriptor());\n          }\n          blockLog.info(\"BLOCK* ask \" + rw.srcNode\n              + \" to replicate \" + rw.block + \" to \" + targetList);\n        }\n      }\n    }\n    if(blockLog.isDebugEnabled()) {\n        blockLog.debug(\n          \"BLOCK* neededReplications = \" + neededReplications.size()\n          + \" pendingReplications = \" + pendingReplications.size());\n    }\n\n    return scheduledWork;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.getBlockCollection": "  public BlockCollection getBlockCollection(Block b) {\n    return blocksMap.getBlockCollection(b);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.countNodes": "  public NumberReplicas countNodes(Block b) {\n    int decommissioned = 0;\n    int live = 0;\n    int corrupt = 0;\n    int excess = 0;\n    int stale = 0;\n    Collection<DatanodeDescriptor> nodesCorrupt = corruptReplicas.getNodes(b);\n    for(DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n      final DatanodeDescriptor node = storage.getDatanodeDescriptor();\n      if ((nodesCorrupt != null) && (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        decommissioned++;\n      } else {\n        LightWeightLinkedSet<Block> blocksExcess = excessReplicateMap.get(node\n            .getDatanodeUuid());\n        if (blocksExcess != null && blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n      if (storage.areBlockContentsStale()) {\n        stale++;\n      }\n    }\n    return new NumberReplicas(live, decommissioned, corrupt, excess, stale);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTargets": "    private void chooseTargets(BlockPlacementPolicy blockplacement,\n        Set<Node> excludedNodes) {\n      targets = blockplacement.chooseTarget(bc.getName(),\n          additionalReplRequired, srcNode, liveReplicaStorages, false,\n          excludedNodes, block.getNumBytes(), StorageType.DEFAULT);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseSourceDatanode": "   DatanodeDescriptor chooseSourceDatanode(Block block,\n       List<DatanodeDescriptor> containingNodes,\n       List<DatanodeStorageInfo>  nodesContainingLiveReplicas,\n       NumberReplicas numReplicas,\n       int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    DatanodeDescriptor srcNode = null;\n    int live = 0;\n    int decommissioned = 0;\n    int corrupt = 0;\n    int excess = 0;\n    \n    Collection<DatanodeDescriptor> nodesCorrupt = corruptReplicas.getNodes(block);\n    for(DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node = storage.getDatanodeDescriptor();\n      LightWeightLinkedSet<Block> excessBlocks =\n        excessReplicateMap.get(node.getDatanodeUuid());\n      if ((nodesCorrupt != null) && (nodesCorrupt.contains(node)))\n        corrupt++;\n      else if (node.isDecommissionInProgress() || node.isDecommissioned())\n        decommissioned++;\n      else if (excessBlocks != null && excessBlocks.contains(block)) {\n        excess++;\n      } else {\n        nodesContainingLiveReplicas.add(storage);\n        live++;\n      }\n      containingNodes.add(node);\n      // Check if this replica is corrupt\n      // If so, do not select the node as src node\n      if ((nodesCorrupt != null) && nodesCorrupt.contains(node))\n        continue;\n      if(priority != UnderReplicatedBlocks.QUEUE_HIGHEST_PRIORITY\n          && node.getNumberOfBlocksToBeReplicated() >= maxReplicationStreams)\n      {\n        continue; // already reached replication limit\n      }\n      if (node.getNumberOfBlocksToBeReplicated() >= replicationStreamsHardLimit)\n      {\n        continue;\n      }\n      // the block must not be scheduled for removal on srcNode\n      if(excessBlocks != null && excessBlocks.contains(block))\n        continue;\n      // never use already decommissioned nodes\n      if(node.isDecommissioned())\n        continue;\n      // we prefer nodes that are in DECOMMISSION_INPROGRESS state\n      if(node.isDecommissionInProgress() || srcNode == null) {\n        srcNode = node;\n        continue;\n      }\n      if(srcNode.isDecommissionInProgress())\n        continue;\n      // switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if(DFSUtil.getRandom().nextBoolean())\n        srcNode = node;\n    }\n    if(numReplicas != null)\n      numReplicas.initialize(live, decommissioned, corrupt, excess, 0);\n    return srcNode;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.blockHasEnoughRacks": "  boolean blockHasEnoughRacks(Block b) {\n    if (!this.shouldCheckForEnoughRacks) {\n      return true;\n    }\n    boolean enoughRacks = false;;\n    Collection<DatanodeDescriptor> corruptNodes = \n                                  corruptReplicas.getNodes(b);\n    int numExpectedReplicas = getReplication(b);\n    String rackName = null;\n    for(DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n      final DatanodeDescriptor cur = storage.getDatanodeDescriptor();\n      if (!cur.isDecommissionInProgress() && !cur.isDecommissioned()) {\n        if ((corruptNodes == null ) || !corruptNodes.contains(cur)) {\n          if (numExpectedReplicas == 1 ||\n              (numExpectedReplicas > 1 &&\n                  !datanodeManager.hasClusterEverBeenMultiRack())) {\n            enoughRacks = true;\n            break;\n          }\n          String rackNameNew = cur.getNetworkLocation();\n          if (rackName == null) {\n            rackName = rackNameNew;\n          } else if (!rackName.equals(rackNameNew)) {\n            enoughRacks = true;\n            break;\n          }\n        }\n      }\n    }\n    return enoughRacks;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork": "  int computeReplicationWork(int blocksToProcess) {\n    List<List<Block>> blocksToReplicate = null;\n    namesystem.writeLock();\n    try {\n      // Choose the blocks to be replicated\n      blocksToReplicate = neededReplications\n          .chooseUnderReplicatedBlocks(blocksToProcess);\n    } finally {\n      namesystem.writeUnlock();\n    }\n    return computeReplicationWorkForBlocks(blocksToReplicate);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork": "  int computeDatanodeWork() {\n    // Blocks should not be replicated or removed if in safe mode.\n    // It's OK to check safe mode here w/o holding lock, in the worst\n    // case extra replications will be scheduled, and these will get\n    // fixed up later.\n    if (namesystem.isInSafeMode()) {\n      return 0;\n    }\n\n    final int numlive = heartbeatManager.getLiveDatanodeCount();\n    final int blocksToProcess = numlive\n        * this.blocksReplWorkMultiplier;\n    final int nodesToProcess = (int) Math.ceil(numlive\n        * this.blocksInvalidateWorkPct);\n\n    int workFound = this.computeReplicationWork(blocksToProcess);\n\n    // Update counters\n    namesystem.writeLock();\n    try {\n      this.updateState();\n      this.scheduledReplicationBlocksCount = workFound;\n    } finally {\n      namesystem.writeUnlock();\n    }\n    workFound += this.computeInvalidateWork(nodesToProcess);\n    return workFound;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.updateState": "  void updateState() {\n    pendingReplicationBlocksCount = pendingReplications.size();\n    underReplicatedBlocksCount = neededReplications.size();\n    corruptReplicaBlocksCount = corruptReplicas.size();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeInvalidateWork": "  int computeInvalidateWork(int nodesToProcess) {\n    final List<String> nodes = invalidateBlocks.getStorageIDs();\n    Collections.shuffle(nodes);\n\n    nodesToProcess = Math.min(nodes.size(), nodesToProcess);\n\n    int blockCnt = 0;\n    for(int nodeCnt = 0; nodeCnt < nodesToProcess; nodeCnt++ ) {\n      blockCnt += invalidateWorkForOneNode(nodes.get(nodeCnt));\n    }\n    return blockCnt;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.run": "    public void run() {\n      while (namesystem.isRunning()) {\n        try {\n          computeDatanodeWork();\n          processPendingReplications();\n          Thread.sleep(replicationRecheckInterval);\n        } catch (Throwable t) {\n          if (!namesystem.isRunning()) {\n            LOG.info(\"Stopping ReplicationMonitor.\");\n            if (!(t instanceof InterruptedException)) {\n              LOG.info(\"ReplicationMonitor received an exception\"\n                  + \" while shutting down.\", t);\n            }\n            break;\n          } else if (!checkNSRunning && t instanceof InterruptedException) {\n            LOG.info(\"Stopping ReplicationMonitor for testing.\");\n            break;\n          }\n          LOG.fatal(\"ReplicationMonitor thread received Runtime exception. \", t);\n          terminate(1, t);\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processPendingReplications": "  private void processPendingReplications() {\n    Block[] timedOutItems = pendingReplications.getTimedOutBlocks();\n    if (timedOutItems != null) {\n      namesystem.writeLock();\n      try {\n        for (int i = 0; i < timedOutItems.length; i++) {\n          NumberReplicas num = countNodes(timedOutItems[i]);\n          if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                 num.liveReplicas())) {\n            neededReplications.add(timedOutItems[i],\n                                   num.liveReplicas(),\n                                   num.decommissionedReplicas(),\n                                   getReplication(timedOutItems[i]));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.toDatanodeDescriptors": "  static DatanodeDescriptor[] toDatanodeDescriptors(\n      DatanodeStorageInfo[] storages) {\n    DatanodeDescriptor[] datanodes = new DatanodeDescriptor[storages.length];\n    for (int i = 0; i < storages.length; ++i) {\n      datanodes[i] = storages[i].getDatanodeDescriptor();\n    }\n    return datanodes;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.getDatanodeDescriptor": "  public DatanodeDescriptor getDatanodeDescriptor() {\n    return dn;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockCollection.getBlockReplication": "  public short getBlockReplication();\n\n  /**\n   * Get the name of the collection.\n   */\n  public String getName();\n\n  /**\n   * Set the block at the given index.\n   */\n  public void setBlock(int index, BlockInfo blk);\n\n  /**\n   * Convert the last block of the collection to an under-construction block\n   * and set the locations.\n   */\n  public BlockInfoUnderConstruction setLastBlock(BlockInfo lastBlock,\n      DatanodeStorageInfo[] targets) throws IOException;\n\n  /**\n   * @return whether the block collection is under construction.\n   */\n  public boolean isUnderConstruction();\n}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.incrementBlocksScheduled": "  public static void incrementBlocksScheduled(DatanodeStorageInfo... storages) {\n    for (DatanodeStorageInfo s : storages) {\n      s.getDatanodeDescriptor().incrementBlocksScheduled();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockCollection.isUnderConstruction": "  public boolean isUnderConstruction();\n}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas.liveReplicas": "  public int liveReplicas() {\n    return liveReplicas;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.heartbeatManager.getLiveDatanodeCount": "  synchronized int getLiveDatanodeCount() {\n    return datanodes.size();\n  }"
        },
        "bug_report": {
            "Title": "FSDirectory#getFullPathName should check inodes against null",
            "Description": "From https://builds.apache.org/job/hbase-0.96-hadoop2/166/testReport/junit/org.apache.hadoop.hbase.mapreduce/TestTableInputFormatScan1/org_apache_hadoop_hbase_mapreduce_TestTableInputFormatScan1/ :\n{code}\n2014-01-01 00:10:15,571 INFO  [IPC Server handler 2 on 50198] blockmanagement.BlockManager(1009): BLOCK* addToInvalidates: blk_1073741967_1143 127.0.0.1:40188 127.0.0.1:46149 127.0.0.1:41496 \n2014-01-01 00:10:16,559 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@93935b] namenode.FSDirectory(1854): Could not get full path. Corresponding file might have deleted already.\n2014-01-01 00:10:16,560 FATAL [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@93935b] blockmanagement.BlockManager$ReplicationMonitor(3127): ReplicationMonitor thread received Runtime exception. \njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFullPathName(FSDirectory.java:1871)\n\tat org.apache.hadoop.hdfs.server.namenode.INode.getFullPathName(INode.java:482)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.getName(INodeFile.java:316)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.chooseTarget(BlockPlacementPolicy.java:118)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1259)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1167)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3158)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3112)\n\tat java.lang.Thread.run(Thread.java:724)\n{code}\nLooks like getRelativePathINodes() returned null but getFullPathName() didn't check inodes against null, leading to NPE."
        }
    },
    {
        "filename": "HDFS-3555.json",
        "creation_time": "2012-06-20T17:36:48.000+0000",
        "stack_trace": "```\njava.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.10.120.67:50010 remote=/10.10.120.67:59282]\nat org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:246)\nat org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:159)\nat org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:198)\nat org.apache.hadoop.hdfs.server.datanode.BlockSender.sendChunks(BlockSender.java:397)\nat org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:493)\nat org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:267)\nat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:163)\n```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.waitForIO": "  void waitForIO(int ops) throws IOException {\n    \n    if (selector.select(channel, ops, timeout) == 0) {\n      throw new SocketTimeoutException(timeoutExceptionString(channel, timeout,\n                                                              ops)); \n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.timeoutExceptionString": "  private static String timeoutExceptionString(SelectableChannel channel,\n                                               long timeout, int ops) {\n    \n    String waitingFor;\n    switch(ops) {\n    \n    case SelectionKey.OP_READ :\n      waitingFor = \"read\"; break;\n      \n    case SelectionKey.OP_WRITE :\n      waitingFor = \"write\"; break;      \n      \n    case SelectionKey.OP_CONNECT :\n      waitingFor = \"connect\"; break;\n      \n    default :\n      waitingFor = \"\" + ops;  \n    }\n    \n    return timeout + \" millis timeout while \" +\n           \"waiting for channel to be ready for \" + \n           waitingFor + \". ch : \" + channel;    \n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.select": "    int select(SelectableChannel channel, int ops, long timeout) \n                                                   throws IOException {\n     \n      SelectorInfo info = get(channel);\n      \n      SelectionKey key = null;\n      int ret = 0;\n      \n      try {\n        while (true) {\n          long start = (timeout == 0) ? 0 : System.currentTimeMillis();\n\n          key = channel.register(info.selector, ops);\n          ret = info.selector.select(timeout);\n          \n          if (ret != 0) {\n            return ret;\n          }\n          \n          /* Sometimes select() returns 0 much before timeout for \n           * unknown reasons. So select again if required.\n           */\n          if (timeout > 0) {\n            timeout -= System.currentTimeMillis() - start;\n            if (timeout <= 0) {\n              return 0;\n            }\n          }\n          \n          if (Thread.currentThread().isInterrupted()) {\n            throw new InterruptedIOException(\"Interruped while waiting for \" +\n                                             \"IO on channel \" + channel +\n                                             \". \" + timeout + \n                                             \" millis timeout left.\");\n          }\n        }\n      } finally {\n        if (key != null) {\n          key.cancel();\n        }\n        \n        //clear the canceled key.\n        try {\n          info.selector.selectNow();\n        } catch (IOException e) {\n          LOG.info(\"Unexpected Exception while clearing selector : \", e);\n          // don't put the selector back.\n          info.close();\n          return ret; \n        }\n        \n        release(info);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketOutputStream.waitForWritable": "  public void waitForWritable() throws IOException {\n    writer.waitForIO(SelectionKey.OP_WRITE);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketOutputStream.transferToFully": "  public void transferToFully(FileChannel fileCh, long position, int count) \n                              throws IOException {\n    \n    while (count > 0) {\n      /* \n       * Ideally we should wait after transferTo returns 0. But because of\n       * a bug in JRE on Linux (http://bugs.sun.com/view_bug.do?bug_id=5103988),\n       * which throws an exception instead of returning 0, we wait for the\n       * channel to be writable before writing to it. If you ever see \n       * IOException with message \"Resource temporarily unavailable\" \n       * thrown here, please let us know.\n       * \n       * Once we move to JAVA SE 7, wait should be moved to correct place.\n       */\n      waitForWritable();\n      int nTransfered = (int) fileCh.transferTo(position, count, getChannel());\n      \n      if (nTransfered == 0) {\n        //check if end of file is reached.\n        if (position >= fileCh.size()) {\n          throw new EOFException(\"EOF Reached. file size is \" + fileCh.size() + \n                                 \" and \" + count + \" more bytes left to be \" +\n                                 \"transfered.\");\n        }\n        //otherwise assume the socket is full.\n        //waitForWritable(); // see comment above.\n      } else if (nTransfered < 0) {\n        throw new IOException(\"Unexpected return of \" + nTransfered + \n                              \" from transferTo()\");\n      } else {\n        position += nTransfered;\n        count -= nTransfered;\n      }\n    }\n  }  ",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketOutputStream.getChannel": "  public WritableByteChannel getChannel() {\n    return writer.channel; \n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    if (out == null) {\n      throw new IOException( \"out stream is null\" );\n    }\n    initialOffset = offset;\n    long totalRead = 0;\n    OutputStream streamForSendChunks = out;\n    \n    lastCacheDropOffset = initialOffset;\n\n    if (isLongRead() && blockInFd != null) {\n      // Advise that this file descriptor will be accessed sequentially.\n      NativeIO.posixFadviseIfPossible(blockInFd, 0, 0, NativeIO.POSIX_FADV_SEQUENTIAL);\n    }\n    \n    // Trigger readahead of beginning of file if configured.\n    manageOsCache();\n\n    final long startTime = ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n    try {\n      int maxChunksPerPacket;\n      int pktSize = PacketHeader.PKT_HEADER_LEN;\n      boolean transferTo = transferToAllowed && !verifyChecksum\n          && baseStream instanceof SocketOutputStream\n          && blockIn instanceof FileInputStream;\n      if (transferTo) {\n        FileChannel fileChannel = ((FileInputStream)blockIn).getChannel();\n        blockInPosition = fileChannel.position();\n        streamForSendChunks = baseStream;\n        maxChunksPerPacket = numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n        \n        // Smaller packet size to only hold checksum when doing transferTo\n        pktSize += checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket = Math.max(1,\n            numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n        // Packet size includes both checksum and data\n        pktSize += (chunkSize + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf = ByteBuffer.allocate(pktSize);\n\n      while (endOffset > offset) {\n        manageOsCache();\n        long len = sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n            transferTo, throttler);\n        offset += len;\n        totalRead += len + (numberOfChunks(len) * checksumSize);\n        seqno++;\n      }\n      try {\n        // send an empty packet to mark the end of the block\n        sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n            throttler);\n        out.flush();\n      } catch (IOException e) { //socket error\n        throw ioeToSocketException(e);\n      }\n\n      sentEntireByteRange = true;\n    } finally {\n      if (clientTraceFmt != null) {\n        final long endTime = System.nanoTime();\n        ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n            initialOffset, endTime - startTime));\n      }\n      close();\n    }\n    return totalRead;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.numberOfChunks": "  private int numberOfChunks(long datalen) {\n    return (int) ((datalen + chunkSize - 1)/chunkSize);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.isLongRead": "  private boolean isLongRead() {\n    return (endOffset - offset) > LONG_READ_THRESHOLD_BYTES;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.ioeToSocketException": "  private static IOException ioeToSocketException(IOException ioe) {\n    if (ioe.getClass().equals(IOException.class)) {\n      // \"se\" could be a new class in stead of SocketException.\n      IOException se = new SocketException(\"Original Exception : \" + ioe);\n      se.initCause(ioe);\n      /* Change the stacktrace so that original trace is not truncated\n       * when printed.*/ \n      se.setStackTrace(ioe.getStackTrace());\n      return se;\n    }\n    // otherwise just return the same exception.\n    return ioe;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket": "  private int sendPacket(ByteBuffer pkt, int maxChunks, OutputStream out,\n      boolean transferTo, DataTransferThrottler throttler) throws IOException {\n    int dataLen = (int) Math.min(endOffset - offset,\n                             (chunkSize * (long) maxChunks));\n    \n    int numChunks = numberOfChunks(dataLen); // Number of chunks be sent in the packet\n    int checksumDataLen = numChunks * checksumSize;\n    int packetLen = dataLen + checksumDataLen + 4;\n    boolean lastDataPacket = offset + dataLen == endOffset && dataLen > 0;\n\n    writePacketHeader(pkt, dataLen, packetLen);\n\n    int checksumOff = pkt.position();\n    byte[] buf = pkt.array();\n    \n    if (checksumSize > 0 && checksumIn != null) {\n      readChecksum(buf, checksumOff, checksumDataLen);\n\n      // write in progress that we need to use to get last checksum\n      if (lastDataPacket && lastChunkChecksum != null) {\n        int start = checksumOff + checksumDataLen - checksumSize;\n        byte[] updatedChecksum = lastChunkChecksum.getChecksum();\n        \n        if (updatedChecksum != null) {\n          System.arraycopy(updatedChecksum, 0, buf, start, checksumSize);\n        }\n      }\n    }\n    \n    int dataOff = checksumOff + checksumDataLen;\n    if (!transferTo) { // normal transfer\n      IOUtils.readFully(blockIn, buf, dataOff, dataLen);\n\n      if (verifyChecksum) {\n        verifyChecksum(buf, dataOff, dataLen, numChunks, checksumOff);\n      }\n    }\n    \n    try {\n      if (transferTo) {\n        SocketOutputStream sockOut = (SocketOutputStream)out;\n        sockOut.write(buf, 0, dataOff); // First write checksum\n        \n        // no need to flush. since we know out is not a buffered stream. \n        sockOut.transferToFully(((FileInputStream)blockIn).getChannel(), \n                                blockInPosition, dataLen);\n        blockInPosition += dataLen;\n      } else { \n        // normal transfer\n        out.write(buf, 0, dataOff + dataLen);\n      }\n    } catch (IOException e) {\n      /* Exception while writing to the client. Connection closure from\n       * the other end is mostly the case and we do not care much about\n       * it. But other things can go wrong, especially in transferTo(),\n       * which we do not want to ignore.\n       *\n       * The message parsing below should not be considered as a good\n       * coding example. NEVER do it to drive a program logic. NEVER.\n       * It was done here because the NIO throws an IOException for EPIPE.\n       */\n      String ioem = e.getMessage();\n      if (!ioem.startsWith(\"Broken pipe\") && !ioem.startsWith(\"Connection reset\")) {\n        LOG.error(\"BlockSender.sendChunks() exception: \", e);\n      }\n      throw ioeToSocketException(e);\n    }\n\n    if (throttler != null) { // rebalancing so throttle\n      throttler.throttle(packetLen);\n    }\n\n    return dataLen;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.manageOsCache": "  private void manageOsCache() throws IOException {\n    if (!isLongRead() || blockInFd == null) {\n      // don't manage cache manually for short-reads, like\n      // HBase random read workloads.\n      return;\n    }\n\n    // Perform readahead if necessary\n    if (readaheadLength > 0 && readaheadPool != null) {\n      curReadahead = readaheadPool.readaheadStream(\n          clientTraceFmt, blockInFd,\n          offset, readaheadLength, Long.MAX_VALUE,\n          curReadahead);\n    }\n\n    // Drop what we've just read from cache, since we aren't\n    // likely to need it again\n    long nextCacheDropOffset = lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n    if (shouldDropCacheBehindRead &&\n        offset >= nextCacheDropOffset) {\n      long dropLength = offset - lastCacheDropOffset;\n      if (dropLength >= 1024) {\n        NativeIO.posixFadviseIfPossible(blockInFd,\n            lastCacheDropOffset, dropLength,\n            NativeIO.POSIX_FADV_DONTNEED);\n      }\n      lastCacheDropOffset += CACHE_DROP_INTERVAL_BYTES;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.close": "  public void close() throws IOException {\n    if (blockInFd != null && shouldDropCacheBehindRead && isLongRead()) {\n      // drop the last few MB of the file from cache\n      try {\n        NativeIO.posixFadviseIfPossible(\n            blockInFd, lastCacheDropOffset, offset - lastCacheDropOffset,\n            NativeIO.POSIX_FADV_DONTNEED);\n      } catch (Exception e) {\n        LOG.warn(\"Unable to drop cache on file close\", e);\n      }\n    }\n    if (curReadahead != null) {\n      curReadahead.cancel();\n    }\n    \n    IOException ioe = null;\n    if(checksumIn!=null) {\n      try {\n        checksumIn.close(); // close checksum file\n      } catch (IOException e) {\n        ioe = e;\n      }\n      checksumIn = null;\n    }   \n    if(blockIn!=null) {\n      try {\n        blockIn.close(); // close data file\n      } catch (IOException e) {\n        ioe = e;\n      }\n      blockIn = null;\n      blockInFd = null;\n    }\n    // throw IOException if there is any\n    if(ioe!= null) {\n      throw ioe;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock": "  public void readBlock(final ExtendedBlock block,\n      final Token<BlockTokenIdentifier> blockToken,\n      final String clientName,\n      final long blockOffset,\n      final long length) throws IOException {\n    previousOpClientName = clientName;\n\n    OutputStream baseStream = NetUtils.getOutputStream(s, \n        dnConf.socketWriteTimeout);\n    DataOutputStream out = new DataOutputStream(new BufferedOutputStream(\n        baseStream, HdfsConstants.SMALL_BUFFER_SIZE));\n    checkAccess(out, true, block, blockToken,\n        Op.READ_BLOCK, BlockTokenSecretManager.AccessMode.READ);\n  \n    // send the block\n    BlockSender blockSender = null;\n    DatanodeRegistration dnR = \n      datanode.getDNRegistrationForBP(block.getBlockPoolId());\n    final String clientTraceFmt =\n      clientName.length() > 0 && ClientTraceLog.isInfoEnabled()\n        ? String.format(DN_CLIENTTRACE_FORMAT, localAddress, remoteAddress,\n            \"%d\", \"HDFS_READ\", clientName, \"%d\",\n            dnR.getStorageID(), block, \"%d\")\n        : dnR + \" Served block \" + block + \" to \" +\n            remoteAddress;\n\n    updateCurrentThreadName(\"Sending block \" + block);\n    try {\n      try {\n        blockSender = new BlockSender(block, blockOffset, length,\n            true, false, datanode, clientTraceFmt);\n      } catch(IOException e) {\n        String msg = \"opReadBlock \" + block + \" received exception \" + e; \n        LOG.info(msg);\n        sendResponse(s, ERROR, msg, dnConf.socketWriteTimeout);\n        throw e;\n      }\n      \n      // send op status\n      writeSuccessWithChecksumInfo(blockSender,\n          getStreamWithTimeout(s, dnConf.socketWriteTimeout));\n\n      long read = blockSender.sendBlock(out, baseStream, null); // send data\n\n      if (blockSender.didSendEntireByteRange()) {\n        // If we sent the entire range, then we should expect the client\n        // to respond with a Status enum.\n        try {\n          ClientReadStatusProto stat = ClientReadStatusProto.parseFrom(\n              HdfsProtoUtil.vintPrefixed(in));\n          if (!stat.hasStatus()) {\n            LOG.warn(\"Client \" + s.getInetAddress() + \" did not send a valid status \" +\n                     \"code after reading. Will close connection.\");\n            IOUtils.closeStream(out);\n          }\n        } catch (IOException ioe) {\n          LOG.debug(\"Error reading client status response. Will close connection.\", ioe);\n          IOUtils.closeStream(out);\n        }\n      } else {\n        IOUtils.closeStream(out);\n      }\n      datanode.metrics.incrBytesRead((int) read);\n      datanode.metrics.incrBlocksRead();\n    } catch ( SocketException ignored ) {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(dnR + \":Ignoring exception while serving \" + block + \" to \" +\n            remoteAddress, ignored);\n      }\n      // Its ok for remote side to close the connection anytime.\n      datanode.metrics.incrBlocksRead();\n      IOUtils.closeStream(out);\n    } catch ( IOException ioe ) {\n      /* What exactly should we do here?\n       * Earlier version shutdown() datanode if there is disk error.\n       */\n      LOG.warn(dnR + \":Got exception while serving \" + block + \" to \"\n          + remoteAddress, ioe);\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(blockSender);\n    }\n\n    //update metrics\n    datanode.metrics.addReadBlockOp(elapsed());\n    datanode.metrics.incrReadsFromClient(isLocal);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeSuccessWithChecksumInfo": "  private void writeSuccessWithChecksumInfo(BlockSender blockSender,\n      DataOutputStream out) throws IOException {\n\n    ReadOpChecksumInfoProto ckInfo = ReadOpChecksumInfoProto.newBuilder()\n      .setChecksum(DataTransferProtoUtil.toProto(blockSender.getChecksum()))\n      .setChunkOffset(blockSender.getOffset())\n      .build();\n      \n    BlockOpResponseProto response = BlockOpResponseProto.newBuilder()\n      .setStatus(SUCCESS)\n      .setReadOpChecksumInfo(ckInfo)\n      .build();\n    response.writeDelimitedTo(out);\n    out.flush();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.elapsed": "  private long elapsed() {\n    return now() - opStartTime;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess": "  private void checkAccess(DataOutputStream out, final boolean reply, \n      final ExtendedBlock blk,\n      final Token<BlockTokenIdentifier> t,\n      final Op op,\n      final BlockTokenSecretManager.AccessMode mode) throws IOException {\n    if (datanode.isBlockTokenEnabled) {\n      try {\n        datanode.blockPoolTokenSecretManager.checkAccess(t, null, blk, mode);\n      } catch(InvalidToken e) {\n        try {\n          if (reply) {\n            if (out == null) {\n              out = new DataOutputStream(\n                  NetUtils.getOutputStream(s, dnConf.socketWriteTimeout));\n            }\n            \n            BlockOpResponseProto.Builder resp = BlockOpResponseProto.newBuilder()\n              .setStatus(ERROR_ACCESS_TOKEN);\n            if (mode == BlockTokenSecretManager.AccessMode.WRITE) {\n              DatanodeRegistration dnR = \n                datanode.getDNRegistrationForBP(blk.getBlockPoolId());\n              resp.setFirstBadLink(dnR.getXferAddr());\n            }\n            resp.build().writeDelimitedTo(out);\n            out.flush();\n          }\n          LOG.warn(\"Block token verification failed: op=\" + op\n              + \", remoteAddress=\" + remoteAddress\n              + \", message=\" + e.getLocalizedMessage());\n          throw e;\n        } finally {\n          IOUtils.closeStream(out);\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.sendResponse": "  private static void sendResponse(Socket s, Status status, String message,\n      long timeout) throws IOException {\n    DataOutputStream reply = getStreamWithTimeout(s, timeout);\n    \n    writeResponse(status, message, reply);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.getStreamWithTimeout": "  private static DataOutputStream getStreamWithTimeout(Socket s, long timeout)\n      throws IOException {\n    return new DataOutputStream(NetUtils.getOutputStream(s, timeout));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.updateCurrentThreadName": "  private void updateCurrentThreadName(String status) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"DataXceiver for client \");\n    if (previousOpClientName != null) {\n      sb.append(previousOpClientName).append(\" at \");\n    }\n    sb.append(remoteAddress);\n    if (status != null) {\n      sb.append(\" [\").append(status).append(\"]\");\n    }\n    Thread.currentThread().setName(sb.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.run": "  public void run() {\n    int opsProcessed = 0;\n    Op op = null;\n    dataXceiverServer.childSockets.add(s);\n    try {\n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed != 0) {\n            assert dnConf.socketKeepaliveTimeout > 0;\n            socketInputWrapper.setTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            socketInputWrapper.setTimeout(dnConf.socketTimeout);\n          }\n          op = readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it's quite normal to get EOF here.\n          if (opsProcessed > 0 &&\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed != 0) {\n          s.setSoTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime = now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!s.isClosed() && dnConf.socketKeepaliveTimeout > 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                ((op == null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDNRegistrationForBP": "  DatanodeRegistration getDNRegistrationForBP(String bpid) \n  throws IOException {\n    BPOfferService bpos = blockPoolManager.get(bpid);\n    if(bpos==null || bpos.bpRegistration==null) {\n      throw new IOException(\"cannot find BPOfferService for bpid=\"+bpid);\n    }\n    return bpos.bpRegistration;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.didSendEntireByteRange": "  boolean didSendEntireByteRange() {\n    return sentEntireByteRange;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDisplayName": "  public String getDisplayName() {\n    // NB: our DatanodeID may not be set yet\n    return hostName + \":\" + getXferPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXferPort": "  int getXferPort() {\n    return streamingAddr.getPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXceiverCount": "  public int getXceiverCount() {\n    return threadGroup == null ? 0 : threadGroup.activeCount();\n  }"
        },
        "bug_report": {
            "Title": "idle client socket triggers DN ERROR log (should be INFO or DEBUG)",
            "Description": "Datanode service is logging java.net.SocketTimeoutException at ERROR level.\nThis message indicates that the datanode is not able to send data to the client because the client has stopped reading. This message is not really a cause for alarm and should be INFO level.\n\n2012-06-18 17:47:13 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode DatanodeRegistration(x.x.x.x:50010, storageID=DS-196671195-10.10.120.67-50010-1334328338972, infoPort=50075, ipcPort=50020):DataXceiver\njava.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.10.120.67:50010 remote=/10.10.120.67:59282]\nat org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:246)\nat org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:159)\nat org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:198)\nat org.apache.hadoop.hdfs.server.datanode.BlockSender.sendChunks(BlockSender.java:397)\nat org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:493)\nat org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:267)\nat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:163)"
        }
    },
    {
        "filename": "HDFS-10962.json",
        "creation_time": "2016-10-05T04:31:45.000+0000",
        "stack_trace": "```\norg.mockito.exceptions.verification.WantedButNotInvoked: \nWanted but not invoked:\nnamenodeProtocols.getStats();\n-> at org.apache.hadoop.hdfs.server.namenode.ha.TestRequestHedgingProxyProvider.testHedgingWhenOneFails(TestRequestHedgingProxyProvider.java:78)\nActually, there were zero interactions with this mock.\n\n\tat org.apache.hadoop.hdfs.server.namenode.ha.TestRequestHedgingProxyProvider.testHedgingWhenOneFails(TestRequestHedgingProxyProvider.java:78)\n```",
        "source_code": {},
        "bug_report": {
            "Title": "TestRequestHedgingProxyProvider is flaky",
            "Description": "This test fails occasionally with an error like this:\n\n{noformat}\norg.apache.hadoop.hdfs.server.namenode.ha.TestRequestHedgingProxyProvider.testHedgingWhenOneFails\n\n\nError Message\n\nWanted but not invoked:\nnamenodeProtocols.getStats();\n-> at org.apache.hadoop.hdfs.server.namenode.ha.TestRequestHedgingProxyProvider.testHedgingWhenOneFails(TestRequestHedgingProxyProvider.java:78)\nActually, there were zero interactions with this mock.\nStacktrace\n\norg.mockito.exceptions.verification.WantedButNotInvoked: \nWanted but not invoked:\nnamenodeProtocols.getStats();\n-> at org.apache.hadoop.hdfs.server.namenode.ha.TestRequestHedgingProxyProvider.testHedgingWhenOneFails(TestRequestHedgingProxyProvider.java:78)\nActually, there were zero interactions with this mock.\n\n\tat org.apache.hadoop.hdfs.server.namenode.ha.TestRequestHedgingProxyProvider.testHedgingWhenOneFails(TestRequestHedgingProxyProvider.java:78)\nStandard Output\n\n2016-09-26 15:26:02,234 WARN  hdfs.DFSUtil (DFSUtil.java:getAddressesForNameserviceId(689)) - Namenode for mycluster-26780990 remains unresolved for ID nn1.  Check your hdfs-site.xml file to ensure namenodes are configured properly.\n2016-09-26 15:26:02,340 WARN  hdfs.DFSUtil (DFSUtil.java:getAddressesForNameserviceId(689)) - Namenode for mycluster-26780990 remains unresolved for ID nn2.  Check your hdfs-site.xml file to ensure namenodes are configured properly.\n{noformat}"
        }
    },
    {
        "filename": "HDFS-12363.json",
        "creation_time": "2017-08-28T05:34:21.000+0000",
        "stack_trace": "```\nERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Thread received Runtime exception.\njava.lang.NullPointerException\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.scanAndCompactStorages(BlockManager.java:3897)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.run(BlockManager.java:3852)\nat java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.scanAndCompactStorages": "    private void scanAndCompactStorages() throws InterruptedException {\n      ArrayList<String> datanodesAndStorages = new ArrayList<>();\n      for (DatanodeDescriptor node\n          : datanodeManager.getDatanodeListForReport(DatanodeReportType.ALL)) {\n        for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n          try {\n            namesystem.readLock();\n            double ratio = storage.treeSetFillRatio();\n            if (ratio < storageInfoDefragmentRatio) {\n              datanodesAndStorages.add(node.getDatanodeUuid());\n              datanodesAndStorages.add(storage.getStorageID());\n            }\n            LOG.info(\"StorageInfo TreeSet fill ratio {} : {}{}\",\n                     storage.getStorageID(), ratio,\n                     (ratio < storageInfoDefragmentRatio)\n                     ? \" (queued for defragmentation)\" : \"\");\n          } finally {\n            namesystem.readUnlock();\n          }\n        }\n      }\n      if (!datanodesAndStorages.isEmpty()) {\n        for (int i = 0; i < datanodesAndStorages.size(); i += 2) {\n          namesystem.writeLock();\n          try {\n            DatanodeStorageInfo storage = datanodeManager.\n                getDatanode(datanodesAndStorages.get(i)).\n                getStorageInfo(datanodesAndStorages.get(i + 1));\n            if (storage != null) {\n              boolean aborted =\n                  !storage.treeSetCompact(storageInfoDefragmentTimeout);\n              if (aborted) {\n                // Compaction timed out, reset iterator to continue with\n                // the same storage next iteration.\n                i -= 2;\n              }\n              LOG.info(\"StorageInfo TreeSet defragmented {} : {}{}\",\n                       storage.getStorageID(), storage.treeSetFillRatio(),\n                       aborted ? \" (aborted)\" : \"\");\n            }\n          } finally {\n            namesystem.writeUnlock();\n          }\n          // Wait between each iteration\n          Thread.sleep(1000);\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.run": "    public void run() {\n      try {\n        processQueue();\n      } catch (Throwable t) {\n        ExitUtil.terminate(1,\n            getName() + \" encountered fatal exception: \" + t);\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processQueue": "    private void processQueue() {\n      while (namesystem.isRunning()) {\n        NameNodeMetrics metrics = NameNode.getNameNodeMetrics();\n        try {\n          Runnable action = queue.take();\n          // batch as many operations in the write lock until the queue\n          // runs dry, or the max lock hold is reached.\n          int processed = 0;\n          namesystem.writeLock();\n          metrics.setBlockOpsQueued(queue.size() + 1);\n          try {\n            long start = Time.monotonicNow();\n            do {\n              processed++;\n              action.run();\n              if (Time.monotonicNow() - start > MAX_LOCK_HOLD_MS) {\n                break;\n              }\n              action = queue.poll();\n            } while (action != null);\n          } finally {\n            namesystem.writeUnlock();\n            metrics.addBlockOpsBatched(processed - 1);\n          }\n        } catch (InterruptedException e) {\n          // ignore unless thread was specifically interrupted.\n          if (Thread.interrupted()) {\n            break;\n          }\n        }\n      }\n      queue.clear();\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.isPopulatingReplQueues": "  public boolean isPopulatingReplQueues() {\n    if (!shouldPopulateReplQueues()) {\n      return false;\n    }\n    return initializedReplQueues;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.rescanPostponedMisreplicatedBlocks": "  void rescanPostponedMisreplicatedBlocks() {\n    if (getPostponedMisreplicatedBlocksCount() == 0) {\n      return;\n    }\n    namesystem.writeLock();\n    long startTime = Time.monotonicNow();\n    long startSize = postponedMisreplicatedBlocks.size();\n    try {\n      Iterator<Block> it = postponedMisreplicatedBlocks.iterator();\n      for (int i=0; i < blocksPerPostpondedRescan && it.hasNext(); i++) {\n        Block b = it.next();\n        it.remove();\n\n        BlockInfo bi = getStoredBlock(b);\n        if (bi == null) {\n          LOG.debug(\"BLOCK* rescanPostponedMisreplicatedBlocks: \" +\n              \"Postponed mis-replicated block {} no longer found \" +\n              \"in block map.\", b);\n          continue;\n        }\n        MisReplicationResult res = processMisReplicatedBlock(bi);\n        LOG.debug(\"BLOCK* rescanPostponedMisreplicatedBlocks: \" +\n            \"Re-scanned block {}, result is {}\", b, res);\n        if (res == MisReplicationResult.POSTPONE) {\n          rescannedMisreplicatedBlocks.add(b);\n        }\n      }\n    } finally {\n      postponedMisreplicatedBlocks.addAll(rescannedMisreplicatedBlocks);\n      rescannedMisreplicatedBlocks.clear();\n      long endSize = postponedMisreplicatedBlocks.size();\n      namesystem.writeUnlock();\n      LOG.info(\"Rescan of postponedMisreplicatedBlocks completed in {}\" +\n          \" msecs. {} blocks are left. {} blocks were removed.\",\n          (Time.monotonicNow() - startTime), endSize, (startSize - endSize));\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processPendingReconstructions": "  private void processPendingReconstructions() {\n    BlockInfo[] timedOutItems = pendingReconstruction.getTimedOutBlocks();\n    if (timedOutItems != null) {\n      namesystem.writeLock();\n      try {\n        for (int i = 0; i < timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we're working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi = blocksMap.getStoredBlock(timedOutItems[i]);\n          if (bi == null) {\n            continue;\n          }\n          NumberReplicas num = countNodes(timedOutItems[i]);\n          if (isNeededReconstruction(bi, num)) {\n            neededReconstruction.add(bi, num.liveReplicas(),\n                num.readOnlyReplicas(), num.outOfServiceReplicas(),\n                getExpectedRedundancyNum(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync": "  private void processMisReplicatesAsync() throws InterruptedException {\n    long nrInvalid = 0, nrOverReplicated = 0;\n    long nrUnderReplicated = 0, nrPostponed = 0, nrUnderConstruction = 0;\n    long startTimeMisReplicatedScan = Time.monotonicNow();\n    Iterator<BlockInfo> blocksItr = blocksMap.getBlocks().iterator();\n    long totalBlocks = blocksMap.size();\n    reconstructionQueuesInitProgress = 0;\n    long totalProcessed = 0;\n    long sleepDuration =\n        Math.max(1, Math.min(numBlocksPerIteration/1000, 10000));\n\n    while (namesystem.isRunning() && !Thread.currentThread().isInterrupted()) {\n      int processed = 0;\n      namesystem.writeLockInterruptibly();\n      try {\n        while (processed < numBlocksPerIteration && blocksItr.hasNext()) {\n          BlockInfo block = blocksItr.next();\n          MisReplicationResult res = processMisReplicatedBlock(block);\n          switch (res) {\n          case UNDER_REPLICATED:\n            LOG.trace(\"under replicated block {}: {}\", block, res);\n            nrUnderReplicated++;\n            break;\n          case OVER_REPLICATED:\n            LOG.trace(\"over replicated block {}: {}\", block, res);\n            nrOverReplicated++;\n            break;\n          case INVALID:\n            LOG.trace(\"invalid block {}: {}\", block, res);\n            nrInvalid++;\n            break;\n          case POSTPONE:\n            LOG.trace(\"postpone block {}: {}\", block, res);\n            nrPostponed++;\n            postponeBlock(block);\n            break;\n          case UNDER_CONSTRUCTION:\n            LOG.trace(\"under construction block {}: {}\", block, res);\n            nrUnderConstruction++;\n            break;\n          case OK:\n            break;\n          default:\n            throw new AssertionError(\"Invalid enum value: \" + res);\n          }\n          processed++;\n        }\n        totalProcessed += processed;\n        // there is a possibility that if any of the blocks deleted/added during\n        // initialisation, then progress might be different.\n        reconstructionQueuesInitProgress = Math.min((double) totalProcessed\n            / totalBlocks, 1.0);\n\n        if (!blocksItr.hasNext()) {\n          LOG.info(\"Total number of blocks            = {}\", blocksMap.size());\n          LOG.info(\"Number of invalid blocks          = {}\", nrInvalid);\n          LOG.info(\"Number of under-replicated blocks = {}\", nrUnderReplicated);\n          LOG.info(\"Number of  over-replicated blocks = {}{}\", nrOverReplicated,\n              ((nrPostponed > 0) ? (\" (\" + nrPostponed + \" postponed)\") : \"\"));\n          LOG.info(\"Number of blocks being written    = {}\",\n                   nrUnderConstruction);\n          NameNode.stateChangeLog\n              .info(\"STATE* Replication Queue initialization \"\n                  + \"scan for invalid, over- and under-replicated blocks \"\n                  + \"completed in \"\n                  + (Time.monotonicNow() - startTimeMisReplicatedScan)\n                  + \" msec\");\n          break;\n        }\n      } finally {\n        namesystem.writeUnlock();\n        // Make sure it is out of the write lock for sufficiently long time.\n        Thread.sleep(sleepDuration);\n      }\n    }\n    if (Thread.currentThread().isInterrupted()) {\n      LOG.info(\"Interrupted while processing replication queues.\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork": "  int computeDatanodeWork() {\n    // Blocks should not be replicated or removed if in safe mode.\n    // It's OK to check safe mode here w/o holding lock, in the worst\n    // case extra replications will be scheduled, and these will get\n    // fixed up later.\n    if (namesystem.isInSafeMode()) {\n      return 0;\n    }\n\n    final int numlive = heartbeatManager.getLiveDatanodeCount();\n    final int blocksToProcess = numlive\n        * this.blocksReplWorkMultiplier;\n    final int nodesToProcess = (int) Math.ceil(numlive\n        * this.blocksInvalidateWorkPct);\n\n    int workFound = this.computeBlockReconstructionWork(blocksToProcess);\n\n    // Update counters\n    namesystem.writeLock();\n    try {\n      this.updateState();\n      this.scheduledReplicationBlocksCount = workFound;\n    } finally {\n      namesystem.writeUnlock();\n    }\n    workFound += this.computeInvalidateWork(nodesToProcess);\n    return workFound;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.getStorageInfos": "  public DatanodeStorageInfo[] getStorageInfos() {\n    synchronized (storageMap) {\n      final Collection<DatanodeStorageInfo> storages = storageMap.values();\n      return storages.toArray(new DatanodeStorageInfo[storages.size()]);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.size": "    synchronized int size() {return blockq.size();}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.treeSetCompact": "  public boolean treeSetCompact(long timeout) {\n    return blocks.compact(timeout);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.datanodeManager.getDatanode": "  public DatanodeDescriptor getDatanode(DatanodeID nodeID)\n      throws UnregisteredNodeException {\n    final DatanodeDescriptor node = getDatanode(nodeID.getDatanodeUuid());\n    if (node == null) \n      return null;\n    if (!node.getXferAddr().equals(nodeID.getXferAddr())) {\n      final UnregisteredNodeException e = new UnregisteredNodeException(\n          nodeID, node);\n      NameNode.stateChangeLog.error(\"BLOCK* NameSystem.getDatanode: \"\n                                    + e.getLocalizedMessage());\n      throw e;\n    }\n    return node;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.getStorageID": "  public String getStorageID() {\n    return storageID;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.treeSetFillRatio": "  public double treeSetFillRatio() {\n    return blocks.fillRatio();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.datanodeManager.getDatanodeListForReport": "  public List<DatanodeDescriptor> getDatanodeListForReport(\n      final DatanodeReportType type) {\n    final boolean listLiveNodes =\n        type == DatanodeReportType.ALL ||\n        type == DatanodeReportType.LIVE;\n    final boolean listDeadNodes =\n        type == DatanodeReportType.ALL ||\n        type == DatanodeReportType.DEAD;\n    final boolean listDecommissioningNodes =\n        type == DatanodeReportType.ALL ||\n        type == DatanodeReportType.DECOMMISSIONING;\n    final boolean listEnteringMaintenanceNodes =\n        type == DatanodeReportType.ALL ||\n        type == DatanodeReportType.ENTERING_MAINTENANCE;\n    final boolean listInMaintenanceNodes =\n        type == DatanodeReportType.ALL ||\n        type == DatanodeReportType.IN_MAINTENANCE;\n\n    ArrayList<DatanodeDescriptor> nodes;\n    final HostSet foundNodes = new HostSet();\n    final Iterable<InetSocketAddress> includedNodes =\n        hostConfigManager.getIncludes();\n\n    synchronized(this) {\n      nodes = new ArrayList<>(datanodeMap.size());\n      for (DatanodeDescriptor dn : datanodeMap.values()) {\n        final boolean isDead = isDatanodeDead(dn);\n        final boolean isDecommissioning = dn.isDecommissionInProgress();\n        final boolean isEnteringMaintenance = dn.isEnteringMaintenance();\n        final boolean isInMaintenance = dn.isInMaintenance();\n\n        if (((listLiveNodes && !isDead) ||\n            (listDeadNodes && isDead) ||\n            (listDecommissioningNodes && isDecommissioning) ||\n            (listEnteringMaintenanceNodes && isEnteringMaintenance) ||\n            (listInMaintenanceNodes && isInMaintenance)) &&\n            hostConfigManager.isIncluded(dn)) {\n          nodes.add(dn);\n        }\n\n        foundNodes.add(dn.getResolvedAddress());\n      }\n    }\n    Collections.sort(nodes);\n\n    if (listDeadNodes) {\n      for (InetSocketAddress addr : includedNodes) {\n        if (foundNodes.matchedBy(addr)) {\n          continue;\n        }\n        // The remaining nodes are ones that are referenced by the hosts\n        // files but that we do not know about, ie that we have never\n        // head from. Eg. an entry that is no longer part of the cluster\n        // or a bogus entry was given in the hosts files\n        //\n        // If the host file entry specified the xferPort, we use that.\n        // Otherwise, we guess that it is the default xfer port.\n        // We can't ask the DataNode what it had configured, because it's\n        // dead.\n        DatanodeDescriptor dn = new DatanodeDescriptor(new DatanodeID(addr\n                .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                addr.getPort() == 0 ? defaultXferPort : addr.getPort(),\n                defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n        setDatanodeDead(dn);\n        if (hostConfigManager.isExcluded(dn)) {\n          dn.setDecommissioned();\n        }\n        nodes.add(dn);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getDatanodeListForReport with \" +\n          \"includedNodes = \" + hostConfigManager.getIncludes() +\n          \", excludedNodes = \" + hostConfigManager.getExcludes() +\n          \", foundNodes = \" + foundNodes +\n          \", nodes = \" + nodes);\n    }\n    return nodes;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.datanodeManager.isDatanodeDead": "  boolean isDatanodeDead(DatanodeDescriptor node) {\n    return (node.getLastUpdateMonotonic() <\n            (monotonicNow() - heartbeatExpireInterval));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.datanodeManager.setDatanodeDead": "  private void setDatanodeDead(DatanodeDescriptor node) {\n    node.setLastUpdate(0);\n    node.setLastUpdateMonotonic(0);\n  }"
        },
        "bug_report": {
            "Title": "Possible NPE in BlockManager$StorageInfoDefragmenter#scanAndCompactStorages",
            "Description": "Saw NN going down with NPE below:\n\n{noformat}\nERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Thread received Runtime exception.\njava.lang.NullPointerException\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.scanAndCompactStorages(BlockManager.java:3897)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.run(BlockManager.java:3852)\nat java.lang.Thread.run(Thread.java:745)\n2017-08-21 22:14:05,303 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1\n2017-08-21 22:14:05,313 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: \n{noformat}\n\nIn that version, {{BlockManager}} code is:\n{code}\n3896  try {\n3897           DatanodeStorageInfo storage = datanodeManager.\n3898                 getDatanode(datanodesAndStorages.get(i)).\n3899                getStorageInfo(datanodesAndStorages.get(i + 1));\n3900            if (storage != null) {\n{code}"
        }
    },
    {
        "filename": "HDFS-7916.json",
        "creation_time": "2015-03-11T10:40:00.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hdfs.server.datanode.BPServiceActorActionException: Failed to report bad block BP-1384821822-10.224.54.68-1422634566395:blk_1079544278_5812006 to namenode:\n        at org.apache.hadoop.hdfs.server.datanode.ReportBadBlockAction.reportTo(ReportBadBlockAction.java:63)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processQueueMessages(BPServiceActor.java:1020)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:762)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:856)\n        at java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.ReportBadBlockAction.reportTo": "  public void reportTo(DatanodeProtocolClientSideTranslatorPB bpNamenode, \n    DatanodeRegistration bpRegistration) throws BPServiceActorActionException {\n    if (bpRegistration == null) {\n      return;\n    }\n    DatanodeInfo[] dnArr = { new DatanodeInfo(bpRegistration) };\n    String[] uuids = { storageUuid };\n    StorageType[] types = { storageType };\n    LocatedBlock[] locatedBlock = { new LocatedBlock(block,\n        dnArr, uuids, types) };\n\n    try {\n      bpNamenode.reportBadBlocks(locatedBlock);  \n    } catch (IOException e){\n      throw new BPServiceActorActionException(\"Failed to report bad block \"\n          + block + \" to namenode: \");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processQueueMessages": "  private void processQueueMessages() {\n    LinkedList<BPServiceActorAction> duplicateQueue;\n    synchronized (bpThreadQueue) {\n      duplicateQueue = new LinkedList<BPServiceActorAction>(bpThreadQueue);\n      bpThreadQueue.clear();\n    }\n    while (!duplicateQueue.isEmpty()) {\n      BPServiceActorAction actionItem = duplicateQueue.remove();\n      try {\n        actionItem.reportTo(bpNamenode, bpRegistration);\n      } catch (BPServiceActorActionException baae) {\n        LOG.warn(baae.getMessage() + nnAddr , baae);\n        // Adding it back to the queue if not present\n        bpThreadEnqueue(actionItem);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.bpThreadEnqueue": "  public void bpThreadEnqueue(BPServiceActorAction action) {\n    synchronized (bpThreadQueue) {\n      if (!bpThreadQueue.contains(action)) {\n        bpThreadQueue.add(action);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService": "  private void offerService() throws Exception {\n    LOG.info(\"For namenode \" + nnAddr + \" using\"\n        + \" DELETEREPORT_INTERVAL of \" + dnConf.deleteReportInterval + \" msec \"\n        + \" BLOCKREPORT_INTERVAL of \" + dnConf.blockReportInterval + \"msec\"\n        + \" CACHEREPORT_INTERVAL of \" + dnConf.cacheReportInterval + \"msec\"\n        + \" Initial delay: \" + dnConf.initialBlockReportDelay + \"msec\"\n        + \"; heartBeatInterval=\" + dnConf.heartBeatInterval);\n\n    //\n    // Now loop for a long time....\n    //\n    while (shouldRun()) {\n      try {\n        final long startTime = now();\n\n        //\n        // Every so often, send heartbeat or block-report\n        //\n        if (startTime - lastHeartbeat >= dnConf.heartBeatInterval) {\n          //\n          // All heartbeat messages include following info:\n          // -- Datanode name\n          // -- data transfer port\n          // -- Total capacity\n          // -- Bytes remaining\n          //\n          lastHeartbeat = startTime;\n          if (!dn.areHeartbeatsDisabledForTests()) {\n            HeartbeatResponse resp = sendHeartBeat();\n            assert resp != null;\n            dn.getMetrics().addHeartbeat(now() - startTime);\n\n            // If the state of this NN has changed (eg STANDBY->ACTIVE)\n            // then let the BPOfferService update itself.\n            //\n            // Important that this happens before processCommand below,\n            // since the first heartbeat to a new active might have commands\n            // that we should actually process.\n            bpos.updateActorStatesFromHeartbeat(\n                this, resp.getNameNodeHaState());\n            state = resp.getNameNodeHaState().getState();\n\n            if (state == HAServiceState.ACTIVE) {\n              handleRollingUpgradeStatus(resp);\n            }\n\n            long startProcessCommands = now();\n            if (!processCommand(resp.getCommands()))\n              continue;\n            long endProcessCommands = now();\n            if (endProcessCommands - startProcessCommands > 2000) {\n              LOG.info(\"Took \" + (endProcessCommands - startProcessCommands)\n                  + \"ms to process \" + resp.getCommands().length\n                  + \" commands from NN\");\n            }\n          }\n        }\n        if (sendImmediateIBR ||\n            (startTime - lastDeletedReport > dnConf.deleteReportInterval)) {\n          reportReceivedDeletedBlocks();\n          lastDeletedReport = startTime;\n        }\n\n        List<DatanodeCommand> cmds = blockReport();\n        processCommand(cmds == null ? null : cmds.toArray(new DatanodeCommand[cmds.size()]));\n\n        DatanodeCommand cmd = cacheReport();\n        processCommand(new DatanodeCommand[]{ cmd });\n\n        //\n        // There is no work to do;  sleep until hearbeat timer elapses, \n        // or work arrives, and then iterate again.\n        //\n        long waitTime = dnConf.heartBeatInterval - \n        (Time.now() - lastHeartbeat);\n        synchronized(pendingIncrementalBRperStorage) {\n          if (waitTime > 0 && !sendImmediateIBR) {\n            try {\n              pendingIncrementalBRperStorage.wait(waitTime);\n            } catch (InterruptedException ie) {\n              LOG.warn(\"BPOfferService for \" + this + \" interrupted\");\n            }\n          }\n        } // synchronized\n      } catch(RemoteException re) {\n        String reClass = re.getClassName();\n        if (UnregisteredNodeException.class.getName().equals(reClass) ||\n            DisallowedDatanodeException.class.getName().equals(reClass) ||\n            IncorrectVersionException.class.getName().equals(reClass)) {\n          LOG.warn(this + \" is shutting down\", re);\n          shouldServiceRun = false;\n          return;\n        }\n        LOG.warn(\"RemoteException in offerService\", re);\n        try {\n          long sleepTime = Math.min(1000, dnConf.heartBeatInterval);\n          Thread.sleep(sleepTime);\n        } catch (InterruptedException ie) {\n          Thread.currentThread().interrupt();\n        }\n      } catch (IOException e) {\n        LOG.warn(\"IOException in offerService\", e);\n      }\n      processQueueMessages();\n    } // while (shouldRun())\n  } // offerService",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reportReceivedDeletedBlocks": "  private void reportReceivedDeletedBlocks() throws IOException {\n\n    // Generate a list of the pending reports for each storage under the lock\n    ArrayList<StorageReceivedDeletedBlocks> reports =\n        new ArrayList<StorageReceivedDeletedBlocks>(pendingIncrementalBRperStorage.size());\n    synchronized (pendingIncrementalBRperStorage) {\n      for (Map.Entry<DatanodeStorage, PerStoragePendingIncrementalBR> entry :\n           pendingIncrementalBRperStorage.entrySet()) {\n        final DatanodeStorage storage = entry.getKey();\n        final PerStoragePendingIncrementalBR perStorageMap = entry.getValue();\n\n        if (perStorageMap.getBlockInfoCount() > 0) {\n          // Send newly-received and deleted blockids to namenode\n          ReceivedDeletedBlockInfo[] rdbi = perStorageMap.dequeueBlockInfos();\n          reports.add(new StorageReceivedDeletedBlocks(storage, rdbi));\n        }\n      }\n      sendImmediateIBR = false;\n    }\n\n    if (reports.size() == 0) {\n      // Nothing new to report.\n      return;\n    }\n\n    // Send incremental block reports to the Namenode outside the lock\n    boolean success = false;\n    try {\n      bpNamenode.blockReceivedAndDeleted(bpRegistration,\n          bpos.getBlockPoolId(),\n          reports.toArray(new StorageReceivedDeletedBlocks[reports.size()]));\n      success = true;\n    } finally {\n      if (!success) {\n        synchronized (pendingIncrementalBRperStorage) {\n          for (StorageReceivedDeletedBlocks report : reports) {\n            // If we didn't succeed in sending the report, put all of the\n            // blocks back onto our queue, but only in the case where we\n            // didn't put something newer in the meantime.\n            PerStoragePendingIncrementalBR perStorageMap =\n                pendingIncrementalBRperStorage.get(report.getStorage());\n            perStorageMap.putMissingBlockInfos(report.getBlocks());\n            sendImmediateIBR = true;\n          }\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.shouldRun": "  private boolean shouldRun() {\n    return shouldServiceRun && dn.shouldRun();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand": "  boolean processCommand(DatanodeCommand[] cmds) {\n    if (cmds != null) {\n      for (DatanodeCommand cmd : cmds) {\n        try {\n          if (bpos.processCommandFromActor(cmd, this) == false) {\n            return false;\n          }\n        } catch (IOException ioe) {\n          LOG.warn(\"Error processing datanode Command\", ioe);\n        }\n      }\n    }\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.handleRollingUpgradeStatus": "  private void handleRollingUpgradeStatus(HeartbeatResponse resp) throws IOException {\n    RollingUpgradeStatus rollingUpgradeStatus = resp.getRollingUpdateStatus();\n    if (rollingUpgradeStatus != null &&\n        rollingUpgradeStatus.getBlockPoolId().compareTo(bpos.getBlockPoolId()) != 0) {\n      // Can this ever occur?\n      LOG.error(\"Invalid BlockPoolId \" +\n          rollingUpgradeStatus.getBlockPoolId() +\n          \" in HeartbeatResponse. Expected \" +\n          bpos.getBlockPoolId());\n    } else {\n      bpos.signalRollingUpgrade(rollingUpgradeStatus != null);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.blockReport": "  List<DatanodeCommand> blockReport() throws IOException {\n    // send block report if timer has expired.\n    final long startTime = now();\n    if (startTime - lastBlockReport <= dnConf.blockReportInterval) {\n      return null;\n    }\n\n    final ArrayList<DatanodeCommand> cmds = new ArrayList<DatanodeCommand>();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    reportReceivedDeletedBlocks();\n    lastDeletedReport = startTime;\n\n    long brCreateStartTime = now();\n    Map<DatanodeStorage, BlockListAsLongs> perVolumeBlockLists =\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i = 0;\n    int totalBlockCount = 0;\n    StorageBlockReport reports[] =\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry<DatanodeStorage, BlockListAsLongs> kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList = kvPair.getValue();\n      reports[i++] = new StorageBlockReport(\n          kvPair.getKey(), blockList.getBlockListAsLongs());\n      totalBlockCount += blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent = 0;\n    int numRPCs = 0;\n    boolean success = false;\n    long brSendStartTime = now();\n    try {\n      if (totalBlockCount < dnConf.blockReportSplitThreshold) {\n        // Below split threshold, send all reports in a single message.\n        DatanodeCommand cmd = bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), reports);\n        numRPCs = 1;\n        numReportsSent = reports.length;\n        if (cmd != null) {\n          cmds.add(cmd);\n        }\n      } else {\n        // Send one block report per message.\n        for (StorageBlockReport report : reports) {\n          StorageBlockReport singleReport[] = { report };\n          DatanodeCommand cmd = bpNamenode.blockReport(\n              bpRegistration, bpos.getBlockPoolId(), singleReport);\n          numReportsSent++;\n          numRPCs++;\n          if (cmd != null) {\n            cmds.add(cmd);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost = now() - brSendStartTime;\n      long brCreateCost = brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      final int nCmds = cmds.size();\n      LOG.info((success ? \"S\" : \"Uns\") +\n          \"uccessfully sent \" + numReportsSent +\n          \" of \" + reports.length +\n          \" blockreports for \" + totalBlockCount +\n          \" total blocks using \" + numRPCs +\n          \" RPCs. This took \" + brCreateCost +\n          \" msec to generate and \" + brSendCost +\n          \" msecs for RPC and NN processing.\" +\n          \" Got back \" +\n          ((nCmds == 0) ? \"no commands\" :\n              ((nCmds == 1) ? \"one command: \" + cmds.get(0) :\n                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n          \".\");\n    }\n    scheduleNextBlockReport(startTime);\n    return cmds.size() == 0 ? null : cmds;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat": "  HeartbeatResponse sendHeartBeat() throws IOException {\n    StorageReport[] reports =\n        dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat with \" + reports.length +\n                \" storage reports from service actor: \" + this);\n    }\n    \n    VolumeFailureSummary volumeFailureSummary = dn.getFSDataset()\n        .getVolumeFailureSummary();\n    int numFailedVolumes = volumeFailureSummary != null ?\n        volumeFailureSummary.getFailedStorageLocations().length : 0;\n    return bpNamenode.sendHeartbeat(bpRegistration,\n        reports,\n        dn.getFSDataset().getCacheCapacity(),\n        dn.getFSDataset().getCacheUsed(),\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        numFailedVolumes,\n        volumeFailureSummary);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cacheReport": "  DatanodeCommand cacheReport() throws IOException {\n    // If caching is disabled, do not send a cache report\n    if (dn.getFSDataset().getCacheCapacity() == 0) {\n      return null;\n    }\n    // send cache report if timer has expired.\n    DatanodeCommand cmd = null;\n    final long startTime = Time.monotonicNow();\n    if (startTime - lastCacheReport > dnConf.cacheReportInterval) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Sending cacheReport from service actor: \" + this);\n      }\n      lastCacheReport = startTime;\n\n      String bpid = bpos.getBlockPoolId();\n      List<Long> blockIds = dn.getFSDataset().getCacheReport(bpid);\n      long createTime = Time.monotonicNow();\n\n      cmd = bpNamenode.cacheReport(bpRegistration, bpid, blockIds);\n      long sendTime = Time.monotonicNow();\n      long createCost = createTime - startTime;\n      long sendCost = sendTime - createTime;\n      dn.getMetrics().addCacheReport(sendCost);\n      LOG.debug(\"CacheReport of \" + blockIds.size()\n          + \" block(s) took \" + createCost + \" msec to generate and \"\n          + sendCost + \" msecs for RPC and NN processing\");\n    }\n    return cmd;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run": "  public void run() {\n    LOG.info(this + \" starting to offer service\");\n\n    try {\n      while (true) {\n        // init stuff\n        try {\n          // setup storage\n          connectToNNAndHandshake();\n          break;\n        } catch (IOException ioe) {\n          // Initial handshake, storage recovery or registration failed\n          runningState = RunningState.INIT_FAILED;\n          if (shouldRetryInit()) {\n            // Retry until all namenode's of BPOS failed initialization\n            LOG.error(\"Initialization failed for \" + this + \" \"\n                + ioe.getLocalizedMessage());\n            sleepAndLogInterrupts(5000, \"initializing\");\n          } else {\n            runningState = RunningState.FAILED;\n            LOG.fatal(\"Initialization failed for \" + this + \". Exiting. \", ioe);\n            return;\n          }\n        }\n      }\n\n      runningState = RunningState.RUNNING;\n\n      while (shouldRun()) {\n        try {\n          offerService();\n        } catch (Exception ex) {\n          LOG.error(\"Exception in BPOfferService for \" + this, ex);\n          sleepAndLogInterrupts(5000, \"offering service\");\n        }\n      }\n      runningState = RunningState.EXITED;\n    } catch (Throwable ex) {\n      LOG.warn(\"Unexpected exception in block pool \" + this, ex);\n      runningState = RunningState.FAILED;\n    } finally {\n      LOG.warn(\"Ending block pool service for: \" + this);\n      cleanUp();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp": "  private synchronized void cleanUp() {\n    \n    shouldServiceRun = false;\n    IOUtils.cleanup(LOG, bpNamenode);\n    bpos.shutdownActor(this);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sleepAndLogInterrupts": "  private void sleepAndLogInterrupts(int millis,\n      String stateString) {\n    try {\n      Thread.sleep(millis);\n    } catch (InterruptedException ie) {\n      LOG.info(\"BPOfferService \" + this + \" interrupted while \" + stateString);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake": "  private void connectToNNAndHandshake() throws IOException {\n    // get NN proxy\n    bpNamenode = dn.connectToNN(nnAddr);\n\n    // First phase of the handshake with NN - get the namespace\n    // info.\n    NamespaceInfo nsInfo = retrieveNamespaceInfo();\n    \n    // Verify that this matches the other NN in this HA pair.\n    // This also initializes our block pool in the DN if we are\n    // the first NN connection for this BP.\n    bpos.verifyAndSetNamespaceInfo(nsInfo);\n    \n    // Second phase of the handshake with the NN.\n    register();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.shouldRetryInit": "  private boolean shouldRetryInit() {\n    return shouldRun() && bpos.shouldRetryInit();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActorAction.reportTo": "  public void reportTo(DatanodeProtocolClientSideTranslatorPB bpNamenode,\n    DatanodeRegistration bpRegistration) throws BPServiceActorActionException;\n}"
        },
        "bug_report": {
            "Title": "'reportBadBlocks' from datanodes to standby Node BPServiceActor goes for infinite loop",
            "Description": "if any badblock found, then BPSA for StandbyNode will go for infinite times to report it.\n\n{noformat}2015-03-11 19:43:41,528 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to report bad block BP-1384821822-10.224.54.68-1422634566395:blk_1079544278_5812006 to namenode: stobdtserver3/10.224.54.70:18010\norg.apache.hadoop.hdfs.server.datanode.BPServiceActorActionException: Failed to report bad block BP-1384821822-10.224.54.68-1422634566395:blk_1079544278_5812006 to namenode:\n        at org.apache.hadoop.hdfs.server.datanode.ReportBadBlockAction.reportTo(ReportBadBlockAction.java:63)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processQueueMessages(BPServiceActor.java:1020)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:762)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:856)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\n"
        }
    },
    {
        "filename": "HDFS-9549.json",
        "creation_time": "2015-12-11T16:25:08.000+0000",
        "stack_trace": "```\njava.lang.AssertionError: Pending cached list of 127.0.0.1:54134 is not empty, [{blockId=1073741841, replication=1, mark=true}]\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.checkPendingCachedEmpty(TestCacheDirectives.java:1479)\n\tat org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testExceedsCapacity(TestCacheDirectives.java:1502)\n```",
        "source_code": {},
        "bug_report": {
            "Title": "TestCacheDirectives#testExceedsCapacity is flaky",
            "Description": "I have observed that this test (TestCacheDirectives.testExceedsCapacity) fails quite frequently in Jenkins (trunk, trunk-Java8)  \n\nError Message\n\nPending cached list of 127.0.0.1:54134 is not empty, [{blockId=1073741841, replication=1, mark=true}]\n\nStacktrace\n\njava.lang.AssertionError: Pending cached list of 127.0.0.1:54134 is not empty, [{blockId=1073741841, replication=1, mark=true}]\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.checkPendingCachedEmpty(TestCacheDirectives.java:1479)\n\tat org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testExceedsCapacity(TestCacheDirectives.java:1502)\n\n\n\n"
        }
    },
    {
        "filename": "HDFS-11164.json",
        "creation_time": "2016-11-22T05:58:03.000+0000",
        "stack_trace": "```\njava.io.IOException: Got error, status=ERROR, status message opReplaceBlock BP-1772076264-10.252.146.200-1479792322960:blk_1073741825_1001 received exception java.io.IOException: Got error, status=ERROR, status message Not able to copy block 1073741825 to /127.0.0.1:19826 because it's pinned , copy block BP-1772076264-10.252.146.200-1479792322960:blk_1073741825_1001 from /127.0.0.1:19501, reportedBlock move is failed\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:118)\n\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.receiveResponse(Dispatcher.java:417)\n\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.dispatch(Dispatcher.java:358)\n\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.access$5(Dispatcher.java:322)\n\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$1.run(Dispatcher.java:1075)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus": "  public static void checkBlockOpStatus(\n          BlockOpResponseProto response,\n          String logInfo) throws IOException {\n    if (response.getStatus() != Status.SUCCESS) {\n      if (response.getStatus() == Status.ERROR_ACCESS_TOKEN) {\n        throw new InvalidBlockTokenException(\n          \"Got access token error\"\n          + \", status message \" + response.getMessage()\n          + \", \" + logInfo\n        );\n      } else {\n        throw new IOException(\n          \"Got error\"\n          + \", status=\" + response.getStatus().name()\n          + \", status message \" + response.getMessage()\n          + \", \" + logInfo\n        );\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.receiveResponse": "    private void receiveResponse(DataInputStream in) throws IOException {\n      long startTime = Time.monotonicNow();\n      BlockOpResponseProto response =\n          BlockOpResponseProto.parseFrom(vintPrefixed(in));\n      while (response.getStatus() == Status.IN_PROGRESS) {\n        // read intermediate responses\n        response = BlockOpResponseProto.parseFrom(vintPrefixed(in));\n        // Stop waiting for slow block moves. Even if it stops waiting,\n        // the actual move may continue.\n        if (stopWaitingForResponse(startTime)) {\n          throw new IOException(\"Block move timed out\");\n        }\n      }\n      String logInfo = \"reportedBlock move is failed\";\n      DataTransferProtoUtil.checkBlockOpStatus(response, logInfo);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.stopWaitingForResponse": "    private boolean stopWaitingForResponse(long startTime) {\n      return source.isIterationOver() ||\n          (blockMoveTimeout > 0 &&\n          (Time.monotonicNow() - startTime > blockMoveTimeout));\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatch": "    private void dispatch() {\n      LOG.info(\"Start moving \" + this);\n      assert !(reportedBlock instanceof DBlockStriped);\n\n      Socket sock = new Socket();\n      DataOutputStream out = null;\n      DataInputStream in = null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().\n                getXferAddr(Dispatcher.this.connectToDnViaHostname)),\n                HdfsConstants.READ_TIMEOUT);\n\n        // Set read timeout so that it doesn't hang forever against\n        // unresponsive nodes. Datanode normally sends IN_PROGRESS response\n        // twice within the client read timeout period (every 30 seconds by\n        // default). Here, we make it give up after 5 minutes of no response.\n        sock.setSoTimeout(HdfsConstants.READ_TIMEOUT * 5);\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut = sock.getOutputStream();\n        InputStream unbufIn = sock.getInputStream();\n        ExtendedBlock eb = new ExtendedBlock(nnc.getBlockpoolID(),\n            reportedBlock.getBlock());\n        final KeyManager km = nnc.getKeyManager(); \n        Token<BlockTokenIdentifier> accessToken = km.getAccessToken(eb);\n        IOStreamPair saslStreams = saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut = saslStreams.out;\n        unbufIn = saslStreams.in;\n        out = new DataOutputStream(new BufferedOutputStream(unbufOut,\n            ioFileBufferSize));\n        in = new DataInputStream(new BufferedInputStream(unbufIn,\n            ioFileBufferSize));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n        target.getDDatanode().setHasSuccess();\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this, e);\n        target.getDDatanode().setHasFailure();\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.activateDelay": "    synchronized private void activateDelay(long delta) {\n      delayUntil = Time.monotonicNow() + delta;\n      LOG.info(this + \" activateDelay \" + delta/1000.0 + \" seconds\");\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.getBytesMoved": "  long getBytesMoved() {\n    return nnc.getBytesMoved().get();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.removePendingBlock": "    synchronized boolean removePendingBlock(PendingMove pendingBlock) {\n      return pendings.remove(pendingBlock);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.setHasSuccess": "    void setHasSuccess() {\n      this.hasSuccess = true;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.getDatanodeInfo": "    public DatanodeInfo getDatanodeInfo() {\n      return datanode;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.getDDatanode": "      private DDatanode getDDatanode() {\n        return DDatanode.this;\n      }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.setHasFailure": "    void setHasFailure() {\n      this.hasFailure = true;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.sendRequest": "    private void sendRequest(DataOutputStream out, ExtendedBlock eb,\n        Token<BlockTokenIdentifier> accessToken) throws IOException {\n      new Sender(out).replaceBlock(eb, target.storageType, accessToken,\n          source.getDatanodeInfo().getDatanodeUuid(), proxySource.datanode);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.getNumBytes": "    public long getNumBytes(StorageGroup storage) {\n      return getInternalBlock(storage).getNumBytes();\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.reset": "  void reset(Configuration conf) {\n    cluster = NetworkTopology.getInstance(conf);\n    storageGroupMap.clear();\n    sources.clear();\n\n    moverThreadAllocator.reset();\n    for(StorageGroup t : targets) {\n      t.getDDatanode().shutdownMoveExecutor();\n    }\n    targets.clear();\n    globalBlocks.removeAllButRetain(movedBlocks);\n    movedBlocks.cleanup();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.run": "        public void run() {\n          s.dispatchBlocks();\n        }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchBlocks": "    private void dispatchBlocks() {\n      this.blocksToReceive = 2 * getScheduledSize();\n      long previousMoveTimestamp = Time.monotonicNow();\n      while (getScheduledSize() > 0 && !isIterationOver()\n          && (!srcBlocks.isEmpty() || blocksToReceive > 0)) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(this + \" blocksToReceive=\" + blocksToReceive\n              + \", scheduledSize=\" + getScheduledSize()\n              + \", srcBlocks#=\" + srcBlocks.size());\n        }\n        final PendingMove p = chooseNextMove();\n        if (p != null) {\n          // Reset previous move timestamp\n          previousMoveTimestamp = Time.monotonicNow();\n          executePendingMove(p);\n          continue;\n        }\n\n        // Since we cannot schedule any block to move,\n        // remove any moved blocks from the source block list and\n        removeMovedBlocks(); // filter already moved blocks\n        // check if we should fetch more blocks from the namenode\n        if (shouldFetchMoreBlocks()) {\n          // fetch new blocks\n          try {\n            final long received = getBlockList();\n            if (received == 0) {\n              return;\n            }\n            blocksToReceive -= received;\n            continue;\n          } catch (IOException e) {\n            LOG.warn(\"Exception while getting reportedBlock list\", e);\n            return;\n          }\n        } else {\n          // jump out of while-loop after the configured timeout.\n          long noMoveInterval = Time.monotonicNow() - previousMoveTimestamp;\n          if (noMoveInterval > maxNoMoveInterval) {\n            LOG.info(\"Failed to find a pending move for \"  + noMoveInterval\n                + \" ms.  Skipping \" + this);\n            resetScheduledSize();\n          }\n        }\n\n        // Now we can not schedule any block to move and there are\n        // no new blocks added to the source block list, so we wait.\n        try {\n          synchronized (Dispatcher.this) {\n            Dispatcher.this.wait(1000); // wait for targets/sources to be idle\n          }\n          // Didn't find a possible move in this iteration of the while loop,\n          // adding a small delay before choosing next move again.\n          Thread.sleep(100);\n        } catch (InterruptedException ignored) {\n        }\n      }\n\n      if (isIterationOver()) {\n        LOG.info(\"The maximum iteration time (\" + MAX_ITERATION_TIME/1000\n            + \" seconds) has been reached. Stopping \" + this);\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.KeyManager.getAccessToken": "  public Token<BlockTokenIdentifier> getAccessToken(ExtendedBlock eb\n      ) throws IOException {\n    if (!isBlockTokenEnabled) {\n      return BlockTokenSecretManager.DUMMY_TOKEN;\n    } else {\n      if (!shouldRun) {\n        throw new IOException(\n            \"Cannot get access token since BlockKeyUpdater is not running\");\n      }\n      return blockTokenSecretManager.generateToken(null, eb,\n          EnumSet.of(BlockTokenIdentifier.AccessMode.REPLACE, BlockTokenIdentifier.AccessMode.COPY));\n    }\n  }"
        },
        "bug_report": {
            "Title": "Mover should avoid unnecessary retries if the block is pinned",
            "Description": "When mover is trying to move a pinned block to another datanode, it will internally hits the following IOException and mark the block movement as {{failure}}. Since the Mover has {{dfs.mover.retry.max.attempts}} configs, it will continue moving this block until it reaches {{retryMaxAttempts}}. If the block movement failure(s) are only due to block pinning, then retry is unnecessary. The idea of this jira is to avoid retry attempts of pinned blocks as they won't be able to move to a different node. \n\n{code}\n2016-11-22 10:56:10,537 WARN org.apache.hadoop.hdfs.server.balancer.Dispatcher: Failed to move blk_1073741825_1001 with size=52 from 127.0.0.1:19501:DISK to 127.0.0.1:19758:ARCHIVE through 127.0.0.1:19501\njava.io.IOException: Got error, status=ERROR, status message opReplaceBlock BP-1772076264-10.252.146.200-1479792322960:blk_1073741825_1001 received exception java.io.IOException: Got error, status=ERROR, status message Not able to copy block 1073741825 to /127.0.0.1:19826 because it's pinned , copy block BP-1772076264-10.252.146.200-1479792322960:blk_1073741825_1001 from /127.0.0.1:19501, reportedBlock move is failed\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:118)\n\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.receiveResponse(Dispatcher.java:417)\n\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.dispatch(Dispatcher.java:358)\n\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.access$5(Dispatcher.java:322)\n\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$1.run(Dispatcher.java:1075)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n{code}"
        }
    },
    {
        "filename": "HDFS-5291.json",
        "creation_time": "2013-10-02T23:52:02.000+0000",
        "stack_trace": "```\njava.lang.InterruptedException: sleep interrupted\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:334)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:279)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:296)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:356)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1463)\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:454)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTail\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doWork": "    private void doWork() {\n      while (shouldRun) {\n        try {\n          // There's no point in triggering a log roll if the Standby hasn't\n          // read any more transactions since the last time a roll was\n          // triggered. \n          if (tooLongSinceLastLoad() &&\n              lastRollTriggerTxId < lastLoadedTxnId) {\n            triggerActiveLogRoll();\n          }\n          /**\n           * Check again in case someone calls {@link EditLogTailer#stop} while\n           * we're triggering an edit log roll, since ipc.Client catches and\n           * ignores {@link InterruptedException} in a few places. This fixes\n           * the bug described in HDFS-2823.\n           */\n          if (!shouldRun) {\n            break;\n          }\n          doTailEdits();\n        } catch (EditLogInputException elie) {\n          LOG.warn(\"Error while reading edits from disk. Will try again.\", elie);\n        } catch (InterruptedException ie) {\n          // interrupter should have already set shouldRun to false\n          continue;\n        } catch (Throwable t) {\n          LOG.fatal(\"Unknown error encountered while tailing edits. \" +\n              \"Shutting down standby NN.\", t);\n          terminate(1, t);\n        }\n\n        try {\n          Thread.sleep(sleepTimeMs);\n        } catch (InterruptedException e) {\n          LOG.warn(\"Edit log tailer interrupted\", e);\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits": "  void doTailEdits() throws IOException, InterruptedException {\n    // Write lock needs to be interruptible here because the \n    // transitionToActive RPC takes the write lock before calling\n    // tailer.stop() -- so if we're not interruptible, it will\n    // deadlock.\n    namesystem.writeLockInterruptibly();\n    try {\n      FSImage image = namesystem.getFSImage();\n\n      long lastTxnId = image.getLastAppliedTxId();\n      \n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"lastTxnId: \" + lastTxnId);\n      }\n      Collection<EditLogInputStream> streams;\n      try {\n        streams = editLog.selectInputStreams(lastTxnId + 1, 0, null, false);\n      } catch (IOException ioe) {\n        // This is acceptable. If we try to tail edits in the middle of an edits\n        // log roll, i.e. the last one has been finalized but the new inprogress\n        // edits file hasn't been started yet.\n        LOG.warn(\"Edits tailer failed to find any streams. Will try again \" +\n            \"later.\", ioe);\n        return;\n      }\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"edit streams to load from: \" + streams.size());\n      }\n      \n      // Once we have streams to load, errors encountered are legitimate cause\n      // for concern, so we don't catch them here. Simple errors reading from\n      // disk are ignored.\n      long editsLoaded = 0;\n      try {\n        editsLoaded = image.loadEdits(streams, namesystem, null);\n      } catch (EditLogInputException elie) {\n        editsLoaded = elie.getNumEditsLoaded();\n        throw elie;\n      } finally {\n        if (editsLoaded > 0 || LOG.isDebugEnabled()) {\n          LOG.info(String.format(\"Loaded %d edits starting from txid %d \",\n              editsLoaded, lastTxnId));\n        }\n      }\n\n      if (editsLoaded > 0) {\n        lastLoadTimestamp = now();\n      }\n      lastLoadedTxnId = image.getLastAppliedTxId();\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.tooLongSinceLastLoad": "  private boolean tooLongSinceLastLoad() {\n    return logRollPeriodMs >= 0 && \n      (now() - lastLoadTimestamp) > logRollPeriodMs ;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll": "  private void triggerActiveLogRoll() {\n    LOG.info(\"Triggering log roll on remote NameNode \" + activeAddr);\n    try {\n      getActiveNodeProxy().rollEditLog();\n      lastRollTriggerTxId = lastLoadedTxnId;\n    } catch (IOException ioe) {\n      LOG.warn(\"Unable to trigger a roll of the active NN\", ioe);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.run": "          public Object run() {\n            doWork();\n            return null;\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      LOG.error(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal": "  public static <T> T doAsLoginUserOrFatal(PrivilegedAction<T> action) { \n    if (UserGroupInformation.isSecurityEnabled()) {\n      UserGroupInformation ugi = null;\n      try { \n        ugi = UserGroupInformation.getLoginUser();\n      } catch (IOException e) {\n        LOG.fatal(\"Exception while getting login user\", e);\n        e.printStackTrace();\n        Runtime.getRuntime().exit(-1);\n      }\n      return ugi.doAs(action);\n    } else {\n      return action.run();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getLoginUser": "  static UserGroupInformation getLoginUser() throws IOException {\n    if (loginUser == null) {\n      ensureInitialized();\n      try {\n        Subject subject = new Subject();\n        LoginContext login =\n            newLoginContext(authenticationMethod.getLoginAppName(), \n                            subject, new HadoopConfiguration());\n        login.login();\n        UserGroupInformation realUser = new UserGroupInformation(subject);\n        realUser.setLogin(login);\n        realUser.setAuthenticationMethod(authenticationMethod);\n        realUser = new UserGroupInformation(login.getSubject());\n        // If the HADOOP_PROXY_USER environment variable or property\n        // is specified, create a proxy user as the logged in user.\n        String proxyUser = System.getenv(HADOOP_PROXY_USER);\n        if (proxyUser == null) {\n          proxyUser = System.getProperty(HADOOP_PROXY_USER);\n        }\n        loginUser = proxyUser == null ? realUser : createProxyUser(proxyUser, realUser);\n\n        String fileLocation = System.getenv(HADOOP_TOKEN_FILE_LOCATION);\n        if (fileLocation != null) {\n          // Load the token storage file and put all of the tokens into the\n          // user. Don't use the FileSystem API for reading since it has a lock\n          // cycle (HADOOP-9212).\n          Credentials cred = Credentials.readTokenStorageFile(\n              new File(fileLocation), conf);\n          loginUser.addCredentials(cred);\n        }\n        loginUser.spawnAutoRenewalThreadForUserCreds();\n      } catch (LoginException le) {\n        LOG.debug(\"failure to login\", le);\n        throw new IOException(\"failure to login\", le);\n      }\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"UGI loginUser:\"+loginUser);\n      }\n    }\n    return loginUser;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.addCredentials": "  public synchronized void addCredentials(Credentials credentials) {\n    getCredentialsInternal().addAll(credentials);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.setLogin": "  private void setLogin(LoginContext login) {\n    user.setLogin(login);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getSubject": "  protected Subject getSubject() {\n    return subject;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.login": "    public boolean login() throws LoginException {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"hadoop login\");\n      }\n      return true;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.newLoginContext": "  private static LoginContext\n  newLoginContext(String appName, Subject subject,\n    javax.security.auth.login.Configuration loginConf)\n      throws LoginException {\n    // Temporarily switch the thread's ContextClassLoader to match this\n    // class's classloader, so that we can properly load HadoopLoginModule\n    // from the JAAS libraries.\n    Thread t = Thread.currentThread();\n    ClassLoader oldCCL = t.getContextClassLoader();\n    t.setContextClassLoader(HadoopLoginModule.class.getClassLoader());\n    try {\n      return new LoginContext(appName, subject, null, loginConf);\n    } finally {\n      t.setContextClassLoader(oldCCL);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.spawnAutoRenewalThreadForUserCreds": "  private void spawnAutoRenewalThreadForUserCreds() {\n    if (isSecurityEnabled()) {\n      //spawn thread only if we have kerb credentials\n      if (user.getAuthenticationMethod() == AuthenticationMethod.KERBEROS &&\n          !isKeytab) {\n        Thread t = new Thread(new Runnable() {\n          \n          @Override\n          public void run() {\n            String cmd = conf.get(\"hadoop.kerberos.kinit.command\",\n                                  \"kinit\");\n            KerberosTicket tgt = getTGT();\n            if (tgt == null) {\n              return;\n            }\n            long nextRefresh = getRefreshTime(tgt);\n            while (true) {\n              try {\n                long now = Time.now();\n                if(LOG.isDebugEnabled()) {\n                  LOG.debug(\"Current time is \" + now);\n                  LOG.debug(\"Next refresh is \" + nextRefresh);\n                }\n                if (now < nextRefresh) {\n                  Thread.sleep(nextRefresh - now);\n                }\n                Shell.execCommand(cmd, \"-R\");\n                if(LOG.isDebugEnabled()) {\n                  LOG.debug(\"renewed ticket\");\n                }\n                reloginFromTicketCache();\n                tgt = getTGT();\n                if (tgt == null) {\n                  LOG.warn(\"No TGT after renewal. Aborting renew thread for \" +\n                           getUserName());\n                  return;\n                }\n                nextRefresh = Math.max(getRefreshTime(tgt),\n                                       now + kerberosMinSecondsBeforeRelogin);\n              } catch (InterruptedException ie) {\n                LOG.warn(\"Terminating renewal thread\");\n                return;\n              } catch (IOException ie) {\n                LOG.warn(\"Exception encountered while running the\" +\n                    \" renewal command. Aborting renew thread. \" + ie);\n                return;\n              }\n            }\n          }\n        });\n        t.setDaemon(true);\n        t.setName(\"TGT Renewer for \" + getUserName());\n        t.start();\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.setAuthenticationMethod": "  public void setAuthenticationMethod(AuthMethod authMethod) {\n    user.setAuthenticationMethod(AuthenticationMethod.valueOf(authMethod));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getLoginAppName": "    String getLoginAppName() {\n      if (loginAppName == null) {\n        throw new UnsupportedOperationException(\n            this + \" login authentication is not supported\");\n      }\n      return loginAppName;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.ensureInitialized": "  private static void ensureInitialized() {\n    if (conf == null) {\n      synchronized(UserGroupInformation.class) {\n        if (conf == null) { // someone might have beat us\n          initialize(new Configuration(), false);\n        }\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.createProxyUser": "  public static UserGroupInformation createProxyUser(String user,\n      UserGroupInformation realUser) {\n    if (user == null || user.isEmpty()) {\n      throw new IllegalArgumentException(\"Null user\");\n    }\n    if (realUser == null) {\n      throw new IllegalArgumentException(\"Null real user\");\n    }\n    Subject subject = new Subject();\n    Set<Principal> principals = subject.getPrincipals();\n    principals.add(new User(user));\n    principals.add(new RealUser(realUser));\n    UserGroupInformation result =new UserGroupInformation(subject);\n    result.setAuthenticationMethod(AuthenticationMethod.PROXY);\n    return result;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled": "  public static boolean isSecurityEnabled() {\n    return !isAuthenticationMethodEnabled(AuthenticationMethod.SIMPLE);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.isAuthenticationMethodEnabled": "  private static boolean isAuthenticationMethodEnabled(AuthenticationMethod method) {\n    ensureInitialized();\n    return (authenticationMethod == method);\n  }"
        },
        "bug_report": {
            "Title": "Clients need to retry when Active NN is in SafeMode",
            "Description": "In our test, we saw NN immediately went into safemode after transitioning to active state. This can cause HBase region server to timeout and kill itself. We should allow clients to retry when HA is enabled and ANN is in SafeMode.\n\n============================================\nSome log snippets:\n\nstandby state to active transition\n{code}\n2013-10-02 00:13:49,482 INFO  ipc.Server (Server.java:run(2068)) - IPC Server handler 69 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from IP:33911 Call#1483 Retry#1: error: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\n2013-10-02 00:13:49,689 INFO  ipc.Server (Server.java:saslProcess(1342)) - Auth successful for nn/hostname@EXAMPLE.COM (auth:SIMPLE)\n2013-10-02 00:13:49,696 INFO  authorize.ServiceAuthorizationManager (ServiceAuthorizationManager.java:authorize(111)) - Authorization successful for nn/hostname@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ha.HAServiceProtocol\n2013-10-02 00:13:49,700 INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1013)) - Stopping services started for standby state\n2013-10-02 00:13:49,701 WARN  ha.EditLogTailer (EditLogTailer.java:doWork(336)) - Edit log tailer interrupted\njava.lang.InterruptedException: sleep interrupted\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:334)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:279)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:296)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:356)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1463)\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:454)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTail\n2013-10-02 00:13:49,704 INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(885)) - Starting services required for active state\n2013-10-02 00:13:49,719 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnfinalizedSegments(419)) - Starting recovery process for unclosed journal segments...\n2013-10-02 00:13:49,755 INFO  ipc.Server (Server.java:saslProcess(1342)) - Auth successful for hbase/hostname@EXAMPLE.COM (auth:SIMPLE)\n2013-10-02 00:13:49,761 INFO  authorize.ServiceAuthorizationManager (ServiceAuthorizationManager.java:authorize(111)) - Authorization successful for hbase/hostname@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol\n2013-10-02 00:13:49,839 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnfinalizedSegments(421)) - Successfully started new epoch 85\n2013-10-02 00:13:49,839 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnclosedSegment(249)) - Beginning recovery of unclosed segment starting at txid 887112\n2013-10-02 00:13:49,874 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnclosedSegment(258)) - Recovery prepare phase complete. Responses:\nIP:8485: segmentState { startTxId: 887112 endTxId: 887531 isInProgress: true } lastWriterEpoch: 84 lastCommittedTxId: 887530\n172.18.145.97:8485: segmentState { startTxId: 887112 endTxId: 887531 isInProgress: true } lastWriterEpoch: 84 lastCommittedTxId: 887530\n2013-10-02 00:13:49,875 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recover\n{code}\n\n\nAnd then we get into safemode\n\n{code}\nConstruction[IP:1019|RBW]]} size 0\n2013-10-02 00:13:50,277 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2237)) - BLOCK* addStoredBlock: blockMap updated: IP:1019 is added to blk_IP157{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[IP:1019|RBW], ReplicaUnderConstruction[172.18.145.96:1019|RBW], ReplicaUnde\nrConstruction[IP:1019|RBW]]} size 0\n2013-10-02 00:13:50,279 INFO  hdfs.StateChange (FSNamesystem.java:reportStatus(4703)) - STATE* Safe mode ON.\nThe reported blocks 1071 needs additional 5 blocks to reach the threshold 1.0000 of total blocks 1075.\nSafe mode will be turned off automatically\n2013-10-02 00:13:50,279 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2237)) - BLOCK* addStoredBlock: blockMap updated: IP:1019 is added to blk_IP158{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[172.18.145.99:1019|RBW], ReplicaUnderConstruction[172.18.145.97:1019|RBW], ReplicaUnderConstruction[IP:1019|RBW]]} size 0\n2013-10-02 00:13:50,280 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2237)) - BLOCK* addStoredBlock: blockMap updated: 172.18.145.99:1019 is added to blk_IP158{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[172.18.145.99:1019|RBW], ReplicaUnderConstruction[172.18.145.97:1019|RBW], ReplicaUnderConstruction[IP:1019|RBW]]} size 0\n2013-10-02 00:13:50,281 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2237)) - BLOCK* addStoredBlock: blockMap updated: 172.18.145.97:1019 is added to blk_IP158{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[172.18.145.99:1019|RBW], ReplicaUnderConstruction[172.18.145.97:1019|RBW], ReplicaUnderConstruction[IP:1019|RBW]]} size 0\n{code}"
        }
    },
    {
        "filename": "HDFS-12836.json",
        "creation_time": "2017-11-17T20:26:32.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$PrematureEOFException: got premature end-of-file at txid 86; expected file to go up to 85\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:197)\n        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:189)\n        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:205)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:158)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:882)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:863)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:293)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:427)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:380)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:397)\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:481)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:393)\n\norg.apache.hadoop.hdfs.server.namenode.EditLogInputException: Error replaying edit log at offset 1048576.  Expected transaction ID was 87\nRecent opcode offsets: 1048576\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:218)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:158)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:882)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:863)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:293)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:427)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:380)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:397)\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:481)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:393)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp": "  protected FSEditLogOp nextOp() throws IOException {\n    while (true) {\n      switch (state) {\n      case SKIP_UNTIL:\n       try {\n          if (prevTxId != HdfsServerConstants.INVALID_TXID) {\n            LOG.info(\"Fast-forwarding stream '\" + streams[curIdx].getName() +\n                \"' to transaction ID \" + (prevTxId + 1));\n            streams[curIdx].skipUntil(prevTxId + 1);\n          }\n        } catch (IOException e) {\n          prevException = e;\n          state = State.STREAM_FAILED;\n        }\n        state = State.OK;\n        break;\n      case OK:\n        try {\n          FSEditLogOp op = streams[curIdx].readOp();\n          if (op == null) {\n            state = State.EOF;\n            if (streams[curIdx].getLastTxId() == prevTxId) {\n              return null;\n            } else {\n              throw new PrematureEOFException(\"got premature end-of-file \" +\n                  \"at txid \" + prevTxId + \"; expected file to go up to \" +\n                  streams[curIdx].getLastTxId());\n            }\n          }\n          prevTxId = op.getTransactionId();\n          return op;\n        } catch (IOException e) {\n          prevException = e;\n          state = State.STREAM_FAILED;\n        }\n        break;\n      case STREAM_FAILED:\n        if (curIdx + 1 == streams.length) {\n          throw prevException;\n        }\n        long oldLast = streams[curIdx].getLastTxId();\n        long newLast = streams[curIdx + 1].getLastTxId();\n        if (newLast < oldLast) {\n          throw new IOException(\"We encountered an error reading \" +\n              streams[curIdx].getName() + \".  During automatic edit log \" +\n              \"failover, we noticed that all of the remaining edit log \" +\n              \"streams are shorter than the current one!  The best \" +\n              \"remaining edit log ends at transaction \" +\n              newLast + \", but we thought we could read up to transaction \" +\n              oldLast + \".  If you continue, metadata will be lost forever!\");\n        }\n        LOG.error(\"Got error reading edit log input stream \" +\n          streams[curIdx].getName() + \"; failing over to edit log \" +\n          streams[curIdx + 1].getName(), prevException);\n        curIdx++;\n        state = State.SKIP_UNTIL;\n        break;\n      case STREAM_FAILED_RESYNC:\n        if (curIdx + 1 == streams.length) {\n          if (prevException instanceof PrematureEOFException) {\n            // bypass early EOF check\n            state = State.EOF;\n          } else {\n            streams[curIdx].resync();\n            state = State.SKIP_UNTIL;\n          }\n        } else {\n          LOG.error(\"failing over to edit log \" +\n              streams[curIdx + 1].getName());\n          curIdx++;\n          state = State.SKIP_UNTIL;\n        }\n        break;\n      case EOF:\n        return null;\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.getLastTxId": "  public long getLastTxId() {\n    return streams[curIdx].getLastTxId();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.getName": "  public String getName() {\n    StringBuilder bld = new StringBuilder();\n    String prefix = \"\";\n    for (EditLogInputStream elis : streams) {\n      bld.append(prefix);\n      bld.append(elis.getName());\n      prefix = \", \";\n    }\n    return bld.toString();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp": "  public FSEditLogOp readOp() throws IOException {\n    FSEditLogOp ret;\n    if (cachedOp != null) {\n      ret = cachedOp;\n      cachedOp = null;\n      return ret;\n    }\n    return nextOp();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.nextOp": "  protected abstract FSEditLogOp nextOp() throws IOException;\n\n  /**\n   * Go through the next operation from the stream storage.\n   * @return the txid of the next operation.\n   */\n  protected long scanNextOp() throws IOException {\n    FSEditLogOp next = readOp();\n    return next != null ? next.txid : HdfsServerConstants.INVALID_TXID;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords": "  long loadEditRecords(EditLogInputStream in, boolean closeOnExit,\n      long expectedStartingTxId, StartupOption startOpt,\n      MetaRecoveryContext recovery) throws IOException {\n    FSDirectory fsDir = fsNamesys.dir;\n\n    EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts =\n      new EnumMap<FSEditLogOpCodes, Holder<Integer>>(FSEditLogOpCodes.class);\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Acquiring write lock to replay edit log\");\n    }\n\n    fsNamesys.writeLock();\n    fsDir.writeLock();\n\n    long recentOpcodeOffsets[] = new long[4];\n    Arrays.fill(recentOpcodeOffsets, -1);\n    \n    long expectedTxId = expectedStartingTxId;\n    long numEdits = 0;\n    long lastTxId = in.getLastTxId();\n    long numTxns = (lastTxId - expectedStartingTxId) + 1;\n    StartupProgress prog = NameNode.getStartupProgress();\n    Step step = createStartupProgressStep(in);\n    prog.setTotal(Phase.LOADING_EDITS, step, numTxns);\n    Counter counter = prog.getCounter(Phase.LOADING_EDITS, step);\n    long lastLogTime = monotonicNow();\n    long lastInodeId = fsNamesys.dir.getLastInodeId();\n    \n    try {\n      while (true) {\n        try {\n          FSEditLogOp op;\n          try {\n            op = in.readOp();\n            if (op == null) {\n              break;\n            }\n          } catch (Throwable e) {\n            // Handle a problem with our input\n            check203UpgradeFailure(in.getVersion(true), e);\n            String errorMessage =\n              formatEditLogReplayError(in, recentOpcodeOffsets, expectedTxId);\n            FSImage.LOG.error(errorMessage, e);\n            if (recovery == null) {\n               // We will only try to skip over problematic opcodes when in\n               // recovery mode.\n              throw new EditLogInputException(errorMessage, e, numEdits);\n            }\n            MetaRecoveryContext.editLogLoaderPrompt(\n                \"We failed to read txId \" + expectedTxId,\n                recovery, \"skipping the bad section in the log\");\n            in.resync();\n            continue;\n          }\n          recentOpcodeOffsets[(int)(numEdits % recentOpcodeOffsets.length)] =\n            in.getPosition();\n          if (op.hasTransactionId()) {\n            if (op.getTransactionId() > expectedTxId) { \n              MetaRecoveryContext.editLogLoaderPrompt(\"There appears \" +\n                  \"to be a gap in the edit log.  We expected txid \" +\n                  expectedTxId + \", but got txid \" +\n                  op.getTransactionId() + \".\", recovery, \"ignoring missing \" +\n                  \" transaction IDs\");\n            } else if (op.getTransactionId() < expectedTxId) { \n              MetaRecoveryContext.editLogLoaderPrompt(\"There appears \" +\n                  \"to be an out-of-order edit in the edit log.  We \" +\n                  \"expected txid \" + expectedTxId + \", but got txid \" +\n                  op.getTransactionId() + \".\", recovery,\n                  \"skipping the out-of-order edit\");\n              continue;\n            }\n          }\n          try {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"op=\" + op + \", startOpt=\" + startOpt\n                  + \", numEdits=\" + numEdits + \", totalEdits=\" + totalEdits);\n            }\n            long inodeId = applyEditLogOp(op, fsDir, startOpt,\n                in.getVersion(true), lastInodeId);\n            if (lastInodeId < inodeId) {\n              lastInodeId = inodeId;\n            }\n          } catch (RollingUpgradeOp.RollbackException e) {\n            throw e;\n          } catch (Throwable e) {\n            LOG.error(\"Encountered exception on operation \" + op, e);\n            if (recovery == null) {\n              throw e instanceof IOException? (IOException)e: new IOException(e);\n            }\n\n            MetaRecoveryContext.editLogLoaderPrompt(\"Failed to \" +\n             \"apply edit log operation \" + op + \": error \" +\n             e.getMessage(), recovery, \"applying edits\");\n          }\n          // Now that the operation has been successfully decoded and\n          // applied, update our bookkeeping.\n          incrOpCount(op.opCode, opCounts, step, counter);\n          if (op.hasTransactionId()) {\n            lastAppliedTxId = op.getTransactionId();\n            expectedTxId = lastAppliedTxId + 1;\n          } else {\n            expectedTxId = lastAppliedTxId = expectedStartingTxId;\n          }\n          // log progress\n          if (op.hasTransactionId()) {\n            long now = monotonicNow();\n            if (now - lastLogTime > REPLAY_TRANSACTION_LOG_INTERVAL) {\n              long deltaTxId = lastAppliedTxId - expectedStartingTxId + 1;\n              int percent = Math.round((float) deltaTxId / numTxns * 100);\n              LOG.info(\"replaying edit log: \" + deltaTxId + \"/\" + numTxns\n                  + \" transactions completed. (\" + percent + \"%)\");\n              lastLogTime = now;\n            }\n          }\n          numEdits++;\n          totalEdits++;\n        } catch (RollingUpgradeOp.RollbackException e) {\n          LOG.info(\"Stopped at OP_START_ROLLING_UPGRADE for rollback.\");\n          break;\n        } catch (MetaRecoveryContext.RequestStopException e) {\n          MetaRecoveryContext.LOG.warn(\"Stopped reading edit log at \" +\n              in.getPosition() + \"/\"  + in.length());\n          break;\n        }\n      }\n    } finally {\n      fsNamesys.dir.resetLastInodeId(lastInodeId);\n      if(closeOnExit) {\n        in.close();\n      }\n      fsDir.writeUnlock();\n      fsNamesys.writeUnlock(\"loadEditRecords\");\n\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"replaying edit log finished\");\n      }\n\n      if (FSImage.LOG.isDebugEnabled()) {\n        dumpOpCounts(opCounts);\n      }\n    }\n    return numEdits;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.createStartupProgressStep": "  private static Step createStartupProgressStep(EditLogInputStream edits)\n      throws IOException {\n    long length = edits.length();\n    String name = edits.getCurrentStreamName();\n    return length != -1 ? new Step(name, length) : new Step(name);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.formatEditLogReplayError": "  private static String formatEditLogReplayError(EditLogInputStream in,\n      long recentOpcodeOffsets[], long txid) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"Error replaying edit log at offset \" + in.getPosition());\n    sb.append(\".  Expected transaction ID was \").append(txid);\n    if (recentOpcodeOffsets[0] != -1) {\n      Arrays.sort(recentOpcodeOffsets);\n      sb.append(\"\\nRecent opcode offsets:\");\n      for (long offset : recentOpcodeOffsets) {\n        if (offset != -1) {\n          sb.append(' ').append(offset);\n        }\n      }\n    }\n    return sb.toString();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.check203UpgradeFailure": "  private void check203UpgradeFailure(int logVersion, Throwable e)\n      throws IOException {\n    // 0.20.203 version version has conflicting opcodes with the later releases.\n    // The editlog must be emptied by restarting the namenode, before proceeding\n    // with the upgrade.\n    if (Storage.is203LayoutVersion(logVersion)\n        && logVersion != HdfsServerConstants.NAMENODE_LAYOUT_VERSION) {\n      String msg = \"During upgrade failed to load the editlog version \"\n          + logVersion + \" from release 0.20.203. Please go back to the old \"\n          + \" release and restart the namenode. This empties the editlog \"\n          + \" and saves the namespace. Resume the upgrade after this step.\";\n      throw new IOException(msg, e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp": "  private long applyEditLogOp(FSEditLogOp op, FSDirectory fsDir,\n      StartupOption startOpt, int logVersion, long lastInodeId) throws IOException {\n    long inodeId = HdfsConstants.GRANDFATHER_INODE_ID;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"replaying edit log: \" + op);\n    }\n    final boolean toAddRetryCache = fsNamesys.hasRetryCache() && op.hasRpcIds();\n\n    switch (op.opCode) {\n    case OP_ADD: {\n      AddCloseOp addCloseOp = (AddCloseOp)op;\n      final String path =\n          renameReservedPathsOnUpgrade(addCloseOp.path, logVersion);\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + path +\n            \" numblocks : \" + addCloseOp.blocks.length +\n            \" clientHolder \" + addCloseOp.clientName +\n            \" clientMachine \" + addCloseOp.clientMachine);\n      }\n      // There are 3 cases here:\n      // 1. OP_ADD to create a new file\n      // 2. OP_ADD to update file blocks\n      // 3. OP_ADD to open file for append (old append)\n\n      // See if the file already exists (persistBlocks call)\n      INodesInPath iip = fsDir.getINodesInPath(path, DirOp.WRITE);\n      INodeFile oldFile = INodeFile.valueOf(iip.getLastINode(), path, true);\n      if (oldFile != null && addCloseOp.overwrite) {\n        // This is OP_ADD with overwrite\n        FSDirDeleteOp.deleteForEditLog(fsDir, iip, addCloseOp.mtime);\n        iip = INodesInPath.replace(iip, iip.length() - 1, null);\n        oldFile = null;\n      }\n      INodeFile newFile = oldFile;\n      if (oldFile == null) { // this is OP_ADD on a new file (case 1)\n        // versions > 0 support per file replication\n        // get name and replication\n        final short replication = fsNamesys.getBlockManager()\n            .adjustReplication(addCloseOp.replication);\n        assert addCloseOp.blocks.length == 0;\n\n        // add to the file tree\n        inodeId = getAndUpdateLastInodeId(addCloseOp.inodeId, logVersion, lastInodeId);\n        newFile = FSDirWriteFileOp.addFileForEditLog(fsDir, inodeId,\n            iip.getExistingINodes(), iip.getLastLocalName(),\n            addCloseOp.permissions, addCloseOp.aclEntries,\n            addCloseOp.xAttrs, replication, addCloseOp.mtime,\n            addCloseOp.atime, addCloseOp.blockSize, true,\n            addCloseOp.clientName, addCloseOp.clientMachine,\n            addCloseOp.storagePolicyId);\n        assert newFile != null;\n        iip = INodesInPath.replace(iip, iip.length() - 1, newFile);\n        fsNamesys.leaseManager.addLease(addCloseOp.clientName, newFile.getId());\n\n        // add the op into retry cache if necessary\n        if (toAddRetryCache) {\n          HdfsFileStatus stat =\n              FSDirStatAndListingOp.createFileStatusForEditLog(fsDir, iip);\n          fsNamesys.addCacheEntryWithPayload(addCloseOp.rpcClientId,\n              addCloseOp.rpcCallId, stat);\n        }\n      } else { // This is OP_ADD on an existing file (old append)\n        if (!oldFile.isUnderConstruction()) {\n          // This is case 3: a call to append() on an already-closed file.\n          if (FSNamesystem.LOG.isDebugEnabled()) {\n            FSNamesystem.LOG.debug(\"Reopening an already-closed file \" +\n                \"for append\");\n          }\n          LocatedBlock lb = FSDirAppendOp.prepareFileForAppend(fsNamesys, iip,\n              addCloseOp.clientName, addCloseOp.clientMachine, false, false,\n              false);\n          // add the op into retry cache if necessary\n          if (toAddRetryCache) {\n            HdfsFileStatus stat =\n                FSDirStatAndListingOp.createFileStatusForEditLog(fsDir, iip);\n            fsNamesys.addCacheEntryWithPayload(addCloseOp.rpcClientId,\n                addCloseOp.rpcCallId, new LastBlockWithStatus(lb, stat));\n          }\n        }\n      }\n      // Fall-through for case 2.\n      // Regardless of whether it's a new file or an updated file,\n      // update the block list.\n      \n      // Update the salient file attributes.\n      newFile.setAccessTime(addCloseOp.atime, Snapshot.CURRENT_STATE_ID, false);\n      newFile.setModificationTime(addCloseOp.mtime, Snapshot.CURRENT_STATE_ID);\n      ErasureCodingPolicy ecPolicy =\n          FSDirErasureCodingOp.unprotectedGetErasureCodingPolicy(\n              fsDir.getFSNamesystem(), iip);\n      updateBlocks(fsDir, addCloseOp, iip, newFile, ecPolicy);\n      break;\n    }\n    case OP_CLOSE: {\n      AddCloseOp addCloseOp = (AddCloseOp)op;\n      final String path =\n          renameReservedPathsOnUpgrade(addCloseOp.path, logVersion);\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + path +\n            \" numblocks : \" + addCloseOp.blocks.length +\n            \" clientHolder \" + addCloseOp.clientName +\n            \" clientMachine \" + addCloseOp.clientMachine);\n      }\n\n      final INodesInPath iip = fsDir.getINodesInPath(path, DirOp.READ);\n      final INodeFile file = INodeFile.valueOf(iip.getLastINode(), path);\n\n      // Update the salient file attributes.\n      file.setAccessTime(addCloseOp.atime, Snapshot.CURRENT_STATE_ID, false);\n      file.setModificationTime(addCloseOp.mtime, Snapshot.CURRENT_STATE_ID);\n      ErasureCodingPolicy ecPolicy =\n          FSDirErasureCodingOp.unprotectedGetErasureCodingPolicy(\n              fsDir.getFSNamesystem(), iip);\n      updateBlocks(fsDir, addCloseOp, iip, file, ecPolicy);\n\n      // Now close the file\n      if (!file.isUnderConstruction() &&\n          logVersion <= LayoutVersion.BUGFIX_HDFS_2991_VERSION) {\n        // There was a bug (HDFS-2991) in hadoop < 0.23.1 where OP_CLOSE\n        // could show up twice in a row. But after that version, this\n        // should be fixed, so we should treat it as an error.\n        throw new IOException(\n            \"File is not under construction: \" + path);\n      }\n      // One might expect that you could use removeLease(holder, path) here,\n      // but OP_CLOSE doesn't serialize the holder. So, remove the inode.\n      if (file.isUnderConstruction()) {\n        fsNamesys.getLeaseManager().removeLease(file.getId());\n        file.toCompleteFile(file.getModificationTime(), 0,\n            fsNamesys.getBlockManager().getMinReplication());\n      }\n      break;\n    }\n    case OP_APPEND: {\n      AppendOp appendOp = (AppendOp) op;\n      final String path = renameReservedPathsOnUpgrade(appendOp.path,\n          logVersion);\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + path +\n            \" clientName \" + appendOp.clientName +\n            \" clientMachine \" + appendOp.clientMachine +\n            \" newBlock \" + appendOp.newBlock);\n      }\n      INodesInPath iip = fsDir.getINodesInPath(path, DirOp.WRITE);\n      INodeFile file = INodeFile.valueOf(iip.getLastINode(), path);\n      if (!file.isUnderConstruction()) {\n        LocatedBlock lb = FSDirAppendOp.prepareFileForAppend(fsNamesys, iip,\n            appendOp.clientName, appendOp.clientMachine, appendOp.newBlock,\n            false, false);\n        // add the op into retry cache if necessary\n        if (toAddRetryCache) {\n          HdfsFileStatus stat =\n              FSDirStatAndListingOp.createFileStatusForEditLog(fsDir, iip);\n          fsNamesys.addCacheEntryWithPayload(appendOp.rpcClientId,\n              appendOp.rpcCallId, new LastBlockWithStatus(lb, stat));\n        }\n      }\n      break;\n    }\n    case OP_UPDATE_BLOCKS: {\n      UpdateBlocksOp updateOp = (UpdateBlocksOp)op;\n      final String path =\n          renameReservedPathsOnUpgrade(updateOp.path, logVersion);\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + path +\n            \" numblocks : \" + updateOp.blocks.length);\n      }\n      INodesInPath iip = fsDir.getINodesInPath(path, DirOp.READ);\n      INodeFile oldFile = INodeFile.valueOf(iip.getLastINode(), path);\n      // Update in-memory data structures\n      ErasureCodingPolicy ecPolicy =\n          FSDirErasureCodingOp.unprotectedGetErasureCodingPolicy(\n              fsDir.getFSNamesystem(), iip);\n      updateBlocks(fsDir, updateOp, iip, oldFile, ecPolicy);\n\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(updateOp.rpcClientId, updateOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_ADD_BLOCK: {\n      AddBlockOp addBlockOp = (AddBlockOp) op;\n      String path = renameReservedPathsOnUpgrade(addBlockOp.getPath(), logVersion);\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + path +\n            \" new block id : \" + addBlockOp.getLastBlock().getBlockId());\n      }\n      INodesInPath iip = fsDir.getINodesInPath(path, DirOp.READ);\n      INodeFile oldFile = INodeFile.valueOf(iip.getLastINode(), path);\n      // add the new block to the INodeFile\n      ErasureCodingPolicy ecPolicy =\n          FSDirErasureCodingOp.unprotectedGetErasureCodingPolicy(\n              fsDir.getFSNamesystem(), iip);\n      addNewBlock(addBlockOp, oldFile, ecPolicy);\n      break;\n    }\n    case OP_SET_REPLICATION: {\n      SetReplicationOp setReplicationOp = (SetReplicationOp)op;\n      String src = renameReservedPathsOnUpgrade(\n          setReplicationOp.path, logVersion);\n      INodesInPath iip = fsDir.getINodesInPath(src, DirOp.WRITE);\n      short replication = fsNamesys.getBlockManager().adjustReplication(\n          setReplicationOp.replication);\n      FSDirAttrOp.unprotectedSetReplication(fsDir, iip, replication);\n      break;\n    }\n    case OP_CONCAT_DELETE: {\n      ConcatDeleteOp concatDeleteOp = (ConcatDeleteOp)op;\n      String trg = renameReservedPathsOnUpgrade(concatDeleteOp.trg, logVersion);\n      String[] srcs = new String[concatDeleteOp.srcs.length];\n      for (int i=0; i<srcs.length; i++) {\n        srcs[i] =\n            renameReservedPathsOnUpgrade(concatDeleteOp.srcs[i], logVersion);\n      }\n      INodesInPath targetIIP = fsDir.getINodesInPath(trg, DirOp.WRITE);\n      INodeFile[] srcFiles = new INodeFile[srcs.length];\n      for (int i = 0; i < srcs.length; i++) {\n        INodesInPath srcIIP = fsDir.getINodesInPath(srcs[i], DirOp.WRITE);\n        srcFiles[i] = srcIIP.getLastINode().asFile();\n      }\n      FSDirConcatOp.unprotectedConcat(fsDir, targetIIP, srcFiles,\n          concatDeleteOp.timestamp);\n      \n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(concatDeleteOp.rpcClientId,\n            concatDeleteOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_RENAME_OLD: {\n      RenameOldOp renameOp = (RenameOldOp)op;\n      final String src = renameReservedPathsOnUpgrade(renameOp.src, logVersion);\n      final String dst = renameReservedPathsOnUpgrade(renameOp.dst, logVersion);\n      FSDirRenameOp.renameForEditLog(fsDir, src, dst, renameOp.timestamp);\n      \n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(renameOp.rpcClientId, renameOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_DELETE: {\n      DeleteOp deleteOp = (DeleteOp)op;\n      final String src = renameReservedPathsOnUpgrade(\n          deleteOp.path, logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(src, DirOp.WRITE_LINK);\n      FSDirDeleteOp.deleteForEditLog(fsDir, iip, deleteOp.timestamp);\n\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(deleteOp.rpcClientId, deleteOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_MKDIR: {\n      MkdirOp mkdirOp = (MkdirOp)op;\n      inodeId = getAndUpdateLastInodeId(mkdirOp.inodeId, logVersion,\n          lastInodeId);\n      FSDirMkdirOp.mkdirForEditLog(fsDir, inodeId,\n          renameReservedPathsOnUpgrade(mkdirOp.path, logVersion),\n          mkdirOp.permissions, mkdirOp.aclEntries, mkdirOp.timestamp);\n      break;\n    }\n    case OP_SET_GENSTAMP_V1: {\n      SetGenstampV1Op setGenstampV1Op = (SetGenstampV1Op)op;\n      blockManager.getBlockIdManager().setLegacyGenerationStamp(\n          setGenstampV1Op.genStampV1);\n      break;\n    }\n    case OP_SET_PERMISSIONS: {\n      SetPermissionsOp setPermissionsOp = (SetPermissionsOp)op;\n      final String src =\n          renameReservedPathsOnUpgrade(setPermissionsOp.src, logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(src, DirOp.WRITE);\n      FSDirAttrOp.unprotectedSetPermission(fsDir, iip,\n          setPermissionsOp.permissions);\n      break;\n    }\n    case OP_SET_OWNER: {\n      SetOwnerOp setOwnerOp = (SetOwnerOp)op;\n      final String src = renameReservedPathsOnUpgrade(\n          setOwnerOp.src, logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(src, DirOp.WRITE);\n      FSDirAttrOp.unprotectedSetOwner(fsDir, iip,\n          setOwnerOp.username, setOwnerOp.groupname);\n      break;\n    }\n    case OP_SET_NS_QUOTA: {\n      SetNSQuotaOp setNSQuotaOp = (SetNSQuotaOp)op;\n      final String src = renameReservedPathsOnUpgrade(\n          setNSQuotaOp.src, logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(src, DirOp.WRITE);\n      FSDirAttrOp.unprotectedSetQuota(fsDir, iip,\n          setNSQuotaOp.nsQuota, HdfsConstants.QUOTA_DONT_SET, null);\n      break;\n    }\n    case OP_CLEAR_NS_QUOTA: {\n      ClearNSQuotaOp clearNSQuotaOp = (ClearNSQuotaOp)op;\n      final String src = renameReservedPathsOnUpgrade(\n          clearNSQuotaOp.src, logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(src, DirOp.WRITE);\n      FSDirAttrOp.unprotectedSetQuota(fsDir, iip,\n          HdfsConstants.QUOTA_RESET, HdfsConstants.QUOTA_DONT_SET, null);\n      break;\n    }\n    case OP_SET_QUOTA: {\n      SetQuotaOp setQuotaOp = (SetQuotaOp) op;\n      final String src = renameReservedPathsOnUpgrade(\n          setQuotaOp.src, logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(src, DirOp.WRITE);\n      FSDirAttrOp.unprotectedSetQuota(fsDir, iip,\n          setQuotaOp.nsQuota, setQuotaOp.dsQuota, null);\n      break;\n    }\n    case OP_SET_QUOTA_BY_STORAGETYPE: {\n      FSEditLogOp.SetQuotaByStorageTypeOp setQuotaByStorageTypeOp =\n          (FSEditLogOp.SetQuotaByStorageTypeOp) op;\n      final String src = renameReservedPathsOnUpgrade(\n          setQuotaByStorageTypeOp.src, logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(src, DirOp.WRITE);\n      FSDirAttrOp.unprotectedSetQuota(fsDir, iip,\n          HdfsConstants.QUOTA_DONT_SET, setQuotaByStorageTypeOp.dsQuota,\n          setQuotaByStorageTypeOp.type);\n      break;\n    }\n    case OP_TIMES: {\n      TimesOp timesOp = (TimesOp)op;\n      final String src = renameReservedPathsOnUpgrade(\n          timesOp.path, logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(src, DirOp.WRITE);\n      FSDirAttrOp.unprotectedSetTimes(fsDir, iip,\n          timesOp.mtime, timesOp.atime, true);\n      break;\n    }\n    case OP_SYMLINK: {\n      if (!FileSystem.areSymlinksEnabled()) {\n        throw new IOException(\"Symlinks not supported - please remove symlink before upgrading to this version of HDFS\");\n      }\n      SymlinkOp symlinkOp = (SymlinkOp)op;\n      inodeId = getAndUpdateLastInodeId(symlinkOp.inodeId, logVersion,\n          lastInodeId);\n      final String path = renameReservedPathsOnUpgrade(symlinkOp.path,\n          logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(path, DirOp.WRITE_LINK);\n      FSDirSymlinkOp.unprotectedAddSymlink(fsDir, iip.getExistingINodes(),\n          iip.getLastLocalName(), inodeId, symlinkOp.value, symlinkOp.mtime,\n          symlinkOp.atime, symlinkOp.permissionStatus);\n      \n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(symlinkOp.rpcClientId, symlinkOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_RENAME: {\n      RenameOp renameOp = (RenameOp)op;\n      FSDirRenameOp.renameForEditLog(fsDir,\n          renameReservedPathsOnUpgrade(renameOp.src, logVersion),\n          renameReservedPathsOnUpgrade(renameOp.dst, logVersion),\n          renameOp.timestamp, renameOp.options);\n      \n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(renameOp.rpcClientId, renameOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_GET_DELEGATION_TOKEN: {\n      GetDelegationTokenOp getDelegationTokenOp\n        = (GetDelegationTokenOp)op;\n\n      fsNamesys.getDelegationTokenSecretManager()\n        .addPersistedDelegationToken(getDelegationTokenOp.token,\n                                     getDelegationTokenOp.expiryTime);\n      break;\n    }\n    case OP_RENEW_DELEGATION_TOKEN: {\n      RenewDelegationTokenOp renewDelegationTokenOp\n        = (RenewDelegationTokenOp)op;\n      fsNamesys.getDelegationTokenSecretManager()\n        .updatePersistedTokenRenewal(renewDelegationTokenOp.token,\n                                     renewDelegationTokenOp.expiryTime);\n      break;\n    }\n    case OP_CANCEL_DELEGATION_TOKEN: {\n      CancelDelegationTokenOp cancelDelegationTokenOp\n        = (CancelDelegationTokenOp)op;\n      fsNamesys.getDelegationTokenSecretManager()\n          .updatePersistedTokenCancellation(\n              cancelDelegationTokenOp.token);\n      break;\n    }\n    case OP_UPDATE_MASTER_KEY: {\n      UpdateMasterKeyOp updateMasterKeyOp = (UpdateMasterKeyOp)op;\n      fsNamesys.getDelegationTokenSecretManager()\n        .updatePersistedMasterKey(updateMasterKeyOp.key);\n      break;\n    }\n    case OP_REASSIGN_LEASE: {\n      ReassignLeaseOp reassignLeaseOp = (ReassignLeaseOp)op;\n\n      Lease lease = fsNamesys.leaseManager.getLease(\n          reassignLeaseOp.leaseHolder);\n      final String path =\n          renameReservedPathsOnUpgrade(reassignLeaseOp.path, logVersion);\n      INodeFile pendingFile = fsDir.getINode(path, DirOp.READ).asFile();\n      Preconditions.checkState(pendingFile.isUnderConstruction());\n      fsNamesys.reassignLeaseInternal(lease, reassignLeaseOp.newHolder,\n              pendingFile);\n      break;\n    }\n    case OP_START_LOG_SEGMENT:\n    case OP_END_LOG_SEGMENT: {\n      // no data in here currently.\n      break;\n    }\n    case OP_CREATE_SNAPSHOT: {\n      CreateSnapshotOp createSnapshotOp = (CreateSnapshotOp) op;\n      final String snapshotRoot =\n          renameReservedPathsOnUpgrade(createSnapshotOp.snapshotRoot,\n              logVersion);\n      INodesInPath iip = fsDir.getINodesInPath(snapshotRoot, DirOp.WRITE);\n      String path = fsNamesys.getSnapshotManager().createSnapshot(\n          fsDir.getFSNamesystem().getLeaseManager(),\n          iip, snapshotRoot, createSnapshotOp.snapshotName);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntryWithPayload(createSnapshotOp.rpcClientId,\n            createSnapshotOp.rpcCallId, path);\n      }\n      break;\n    }\n    case OP_DELETE_SNAPSHOT: {\n      DeleteSnapshotOp deleteSnapshotOp = (DeleteSnapshotOp) op;\n      BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n      List<INode> removedINodes = new ChunkedArrayList<INode>();\n      final String snapshotRoot =\n          renameReservedPathsOnUpgrade(deleteSnapshotOp.snapshotRoot,\n              logVersion);\n      INodesInPath iip = fsDir.getINodesInPath(snapshotRoot, DirOp.WRITE);\n      fsNamesys.getSnapshotManager().deleteSnapshot(iip,\n          deleteSnapshotOp.snapshotName,\n          new INode.ReclaimContext(fsNamesys.dir.getBlockStoragePolicySuite(),\n              collectedBlocks, removedINodes, null));\n      fsNamesys.getBlockManager().removeBlocksAndUpdateSafemodeTotal(\n          collectedBlocks);\n      collectedBlocks.clear();\n      fsNamesys.dir.removeFromInodeMap(removedINodes);\n      removedINodes.clear();\n\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(deleteSnapshotOp.rpcClientId,\n            deleteSnapshotOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_RENAME_SNAPSHOT: {\n      RenameSnapshotOp renameSnapshotOp = (RenameSnapshotOp) op;\n      final String snapshotRoot =\n          renameReservedPathsOnUpgrade(renameSnapshotOp.snapshotRoot,\n              logVersion);\n      INodesInPath iip = fsDir.getINodesInPath(snapshotRoot, DirOp.WRITE);\n      fsNamesys.getSnapshotManager().renameSnapshot(iip,\n          snapshotRoot, renameSnapshotOp.snapshotOldName,\n          renameSnapshotOp.snapshotNewName);\n      \n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(renameSnapshotOp.rpcClientId,\n            renameSnapshotOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_ALLOW_SNAPSHOT: {\n      AllowSnapshotOp allowSnapshotOp = (AllowSnapshotOp) op;\n      final String snapshotRoot =\n          renameReservedPathsOnUpgrade(allowSnapshotOp.snapshotRoot, logVersion);\n      fsNamesys.getSnapshotManager().setSnapshottable(\n          snapshotRoot, false);\n      break;\n    }\n    case OP_DISALLOW_SNAPSHOT: {\n      DisallowSnapshotOp disallowSnapshotOp = (DisallowSnapshotOp) op;\n      final String snapshotRoot =\n          renameReservedPathsOnUpgrade(disallowSnapshotOp.snapshotRoot,\n              logVersion);\n      fsNamesys.getSnapshotManager().resetSnapshottable(\n          snapshotRoot);\n      break;\n    }\n    case OP_SET_GENSTAMP_V2: {\n      SetGenstampV2Op setGenstampV2Op = (SetGenstampV2Op) op;\n      blockManager.getBlockIdManager().setGenerationStamp(\n          setGenstampV2Op.genStampV2);\n      break;\n    }\n    case OP_ALLOCATE_BLOCK_ID: {\n      AllocateBlockIdOp allocateBlockIdOp = (AllocateBlockIdOp) op;\n      if (BlockIdManager.isStripedBlockID(allocateBlockIdOp.blockId)) {\n        // ALLOCATE_BLOCK_ID is added for sequential block id, thus if the id\n        // is negative, it must belong to striped blocks\n        blockManager.getBlockIdManager().setLastAllocatedStripedBlockId(\n            allocateBlockIdOp.blockId);\n      } else {\n        blockManager.getBlockIdManager().setLastAllocatedContiguousBlockId(\n            allocateBlockIdOp.blockId);\n      }\n      break;\n    }\n    case OP_ROLLING_UPGRADE_START: {\n      if (startOpt == StartupOption.ROLLINGUPGRADE) {\n        final RollingUpgradeStartupOption rollingUpgradeOpt\n            = startOpt.getRollingUpgradeStartupOption(); \n        if (rollingUpgradeOpt == RollingUpgradeStartupOption.ROLLBACK) {\n          throw new RollingUpgradeOp.RollbackException();\n        }\n      }\n      // start rolling upgrade\n      final long startTime = ((RollingUpgradeOp) op).getTime();\n      fsNamesys.startRollingUpgradeInternal(startTime);\n      fsNamesys.triggerRollbackCheckpoint();\n      break;\n    }\n    case OP_ROLLING_UPGRADE_FINALIZE: {\n      final long finalizeTime = ((RollingUpgradeOp) op).getTime();\n      if (fsNamesys.isRollingUpgrade()) {\n        // Only do it when NN is actually doing rolling upgrade.\n        // We can get FINALIZE without corresponding START, if NN is restarted\n        // before this op is consumed and a new checkpoint is created.\n        fsNamesys.finalizeRollingUpgradeInternal(finalizeTime);\n      }\n      fsNamesys.getFSImage().updateStorageVersion();\n      fsNamesys.getFSImage().renameCheckpoint(NameNodeFile.IMAGE_ROLLBACK,\n          NameNodeFile.IMAGE);\n      break;\n    }\n    case OP_ADD_CACHE_DIRECTIVE: {\n      AddCacheDirectiveInfoOp addOp = (AddCacheDirectiveInfoOp) op;\n      CacheDirectiveInfo result = fsNamesys.\n          getCacheManager().addDirectiveFromEditLog(addOp.directive);\n      if (toAddRetryCache) {\n        Long id = result.getId();\n        fsNamesys.addCacheEntryWithPayload(op.rpcClientId, op.rpcCallId, id);\n      }\n      break;\n    }\n    case OP_MODIFY_CACHE_DIRECTIVE: {\n      ModifyCacheDirectiveInfoOp modifyOp =\n          (ModifyCacheDirectiveInfoOp) op;\n      fsNamesys.getCacheManager().modifyDirectiveFromEditLog(\n          modifyOp.directive);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(op.rpcClientId, op.rpcCallId);\n      }\n      break;\n    }\n    case OP_REMOVE_CACHE_DIRECTIVE: {\n      RemoveCacheDirectiveInfoOp removeOp =\n          (RemoveCacheDirectiveInfoOp) op;\n      fsNamesys.getCacheManager().removeDirective(removeOp.id, null);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(op.rpcClientId, op.rpcCallId);\n      }\n      break;\n    }\n    case OP_ADD_CACHE_POOL: {\n      AddCachePoolOp addOp = (AddCachePoolOp) op;\n      fsNamesys.getCacheManager().addCachePool(addOp.info);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(op.rpcClientId, op.rpcCallId);\n      }\n      break;\n    }\n    case OP_MODIFY_CACHE_POOL: {\n      ModifyCachePoolOp modifyOp = (ModifyCachePoolOp) op;\n      fsNamesys.getCacheManager().modifyCachePool(modifyOp.info);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(op.rpcClientId, op.rpcCallId);\n      }\n      break;\n    }\n    case OP_REMOVE_CACHE_POOL: {\n      RemoveCachePoolOp removeOp = (RemoveCachePoolOp) op;\n      fsNamesys.getCacheManager().removeCachePool(removeOp.poolName);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(op.rpcClientId, op.rpcCallId);\n      }\n      break;\n    }\n    case OP_SET_ACL: {\n      SetAclOp setAclOp = (SetAclOp) op;\n      INodesInPath iip = fsDir.getINodesInPath(setAclOp.src, DirOp.WRITE);\n      FSDirAclOp.unprotectedSetAcl(fsDir, iip, setAclOp.aclEntries, true);\n      break;\n    }\n    case OP_SET_XATTR: {\n      SetXAttrOp setXAttrOp = (SetXAttrOp) op;\n      INodesInPath iip = fsDir.getINodesInPath(setXAttrOp.src, DirOp.WRITE);\n      FSDirXAttrOp.unprotectedSetXAttrs(fsDir, iip,\n                                        setXAttrOp.xAttrs,\n                                        EnumSet.of(XAttrSetFlag.CREATE,\n                                                   XAttrSetFlag.REPLACE));\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(setXAttrOp.rpcClientId, setXAttrOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_REMOVE_XATTR: {\n      RemoveXAttrOp removeXAttrOp = (RemoveXAttrOp) op;\n      INodesInPath iip = fsDir.getINodesInPath(removeXAttrOp.src, DirOp.WRITE);\n      FSDirXAttrOp.unprotectedRemoveXAttrs(fsDir, iip,\n                                           removeXAttrOp.xAttrs);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(removeXAttrOp.rpcClientId,\n            removeXAttrOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_TRUNCATE: {\n      TruncateOp truncateOp = (TruncateOp) op;\n      INodesInPath iip = fsDir.getINodesInPath(truncateOp.src, DirOp.WRITE);\n      FSDirTruncateOp.unprotectedTruncate(fsNamesys, iip,\n          truncateOp.clientName, truncateOp.clientMachine,\n          truncateOp.newLength, truncateOp.timestamp, truncateOp.truncateBlock);\n      break;\n    }\n    case OP_SET_STORAGE_POLICY: {\n      SetStoragePolicyOp setStoragePolicyOp = (SetStoragePolicyOp) op;\n      final String path = renameReservedPathsOnUpgrade(setStoragePolicyOp.path,\n          logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(path, DirOp.WRITE);\n      FSDirAttrOp.unprotectedSetStoragePolicy(\n          fsDir, fsNamesys.getBlockManager(), iip,\n          setStoragePolicyOp.policyId);\n      break;\n    }\n    case OP_ADD_ERASURE_CODING_POLICY:\n      AddErasureCodingPolicyOp addOp = (AddErasureCodingPolicyOp) op;\n      fsNamesys.getErasureCodingPolicyManager().addPolicy(\n          addOp.getEcPolicy());\n\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntryWithPayload(op.rpcClientId, op.rpcCallId,\n            addOp.getEcPolicy());\n      }\n      break;\n    case OP_ENABLE_ERASURE_CODING_POLICY:\n      EnableErasureCodingPolicyOp enableOp = (EnableErasureCodingPolicyOp) op;\n      fsNamesys.getErasureCodingPolicyManager().enablePolicy(\n          enableOp.getEcPolicy());\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(op.rpcClientId, op.rpcCallId);\n      }\n      break;\n    case OP_DISABLE_ERASURE_CODING_POLICY:\n      DisableErasureCodingPolicyOp disableOp =\n          (DisableErasureCodingPolicyOp) op;\n      fsNamesys.getErasureCodingPolicyManager().disablePolicy(\n          disableOp.getEcPolicy());\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(op.rpcClientId, op.rpcCallId);\n      }\n      break;\n    case OP_REMOVE_ERASURE_CODING_POLICY:\n      RemoveErasureCodingPolicyOp removeOp = (RemoveErasureCodingPolicyOp) op;\n      fsNamesys.getErasureCodingPolicyManager().removePolicy(\n          removeOp.getEcPolicy());\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(op.rpcClientId, op.rpcCallId);\n      }\n      break;\n    default:\n      throw new IOException(\"Invalid operation read \" + op.opCode);\n    }\n    return inodeId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.incrOpCount": "  private void incrOpCount(FSEditLogOpCodes opCode,\n      EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts, Step step,\n      Counter counter) {\n    Holder<Integer> holder = opCounts.get(opCode);\n    if (holder == null) {\n      holder = new Holder<Integer>(1);\n      opCounts.put(opCode, holder);\n    } else {\n      holder.held++;\n    }\n    counter.increment();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.dumpOpCounts": "  private static void dumpOpCounts(\n      EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"Summary of operations loaded from edit log:\\n  \");\n    Joiner.on(\"\\n  \").withKeyValueSeparator(\"=\").appendTo(sb, opCounts);\n    FSImage.LOG.debug(sb.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits": "  long loadFSEdits(EditLogInputStream edits, long expectedStartingTxId,\n      StartupOption startOpt, MetaRecoveryContext recovery) throws IOException {\n    StartupProgress prog = NameNode.getStartupProgress();\n    Step step = createStartupProgressStep(edits);\n    prog.beginStep(Phase.LOADING_EDITS, step);\n    fsNamesys.writeLock();\n    try {\n      long startTime = monotonicNow();\n      FSImage.LOG.info(\"Start loading edits file \" + edits.getName());\n      long numEdits = loadEditRecords(edits, false, expectedStartingTxId,\n          startOpt, recovery);\n      FSImage.LOG.info(\"Edits file \" + edits.getName() \n          + \" of size \" + edits.length() + \" edits # \" + numEdits \n          + \" loaded in \" + (monotonicNow()-startTime)/1000 + \" seconds\");\n      return numEdits;\n    } finally {\n      edits.close();\n      fsNamesys.writeUnlock(\"loadFSEdits\");\n      prog.endStep(Phase.LOADING_EDITS, step);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits": "  private long loadEdits(Iterable<EditLogInputStream> editStreams,\n      FSNamesystem target, StartupOption startOpt, MetaRecoveryContext recovery)\n      throws IOException {\n    LOG.debug(\"About to load edits:\\n  \" + Joiner.on(\"\\n  \").join(editStreams));\n    StartupProgress prog = NameNode.getStartupProgress();\n    prog.beginPhase(Phase.LOADING_EDITS);\n    \n    long prevLastAppliedTxId = lastAppliedTxId;  \n    try {    \n      FSEditLogLoader loader = new FSEditLogLoader(target, lastAppliedTxId);\n      \n      // Load latest edits\n      for (EditLogInputStream editIn : editStreams) {\n        LOG.info(\"Reading \" + editIn + \" expecting start txid #\" +\n              (lastAppliedTxId + 1));\n        try {\n          loader.loadFSEdits(editIn, lastAppliedTxId + 1, startOpt, recovery);\n        } finally {\n          // Update lastAppliedTxId even in case of error, since some ops may\n          // have been successfully applied before the error.\n          lastAppliedTxId = loader.getLastAppliedTxId();\n        }\n        // If we are in recovery mode, we may have skipped over some txids.\n        if (editIn.getLastTxId() != HdfsServerConstants.INVALID_TXID) {\n          lastAppliedTxId = editIn.getLastTxId();\n        }\n      }\n    } finally {\n      FSEditLog.closeAllStreams(editStreams);\n    }\n    prog.endPhase(Phase.LOADING_EDITS);\n    return lastAppliedTxId - prevLastAppliedTxId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getLastAppliedTxId": "  public synchronized long getLastAppliedTxId() {\n    return lastAppliedTxId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits": "  void doTailEdits() throws IOException, InterruptedException {\n    // Write lock needs to be interruptible here because the \n    // transitionToActive RPC takes the write lock before calling\n    // tailer.stop() -- so if we're not interruptible, it will\n    // deadlock.\n    namesystem.writeLockInterruptibly();\n    try {\n      FSImage image = namesystem.getFSImage();\n\n      long lastTxnId = image.getLastAppliedTxId();\n      \n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"lastTxnId: \" + lastTxnId);\n      }\n      Collection<EditLogInputStream> streams;\n      try {\n        streams = editLog.selectInputStreams(lastTxnId + 1, 0,\n            null, inProgressOk, true);\n      } catch (IOException ioe) {\n        // This is acceptable. If we try to tail edits in the middle of an edits\n        // log roll, i.e. the last one has been finalized but the new inprogress\n        // edits file hasn't been started yet.\n        LOG.warn(\"Edits tailer failed to find any streams. Will try again \" +\n            \"later.\", ioe);\n        return;\n      }\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"edit streams to load from: \" + streams.size());\n      }\n      \n      // Once we have streams to load, errors encountered are legitimate cause\n      // for concern, so we don't catch them here. Simple errors reading from\n      // disk are ignored.\n      long editsLoaded = 0;\n      try {\n        editsLoaded = image.loadEdits(streams, namesystem);\n      } catch (EditLogInputException elie) {\n        editsLoaded = elie.getNumEditsLoaded();\n        throw elie;\n      } finally {\n        if (editsLoaded > 0 || LOG.isDebugEnabled()) {\n          LOG.debug(String.format(\"Loaded %d edits starting from txid %d \",\n              editsLoaded, lastTxnId));\n        }\n      }\n\n      if (editsLoaded > 0) {\n        lastLoadTimeMs = monotonicNow();\n      }\n      lastLoadedTxnId = image.getLastAppliedTxId();\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doWork": "    protected abstract T doWork() throws IOException;\n\n    public T call() throws IOException {\n      // reset the loop count on success\n      nnLoopCount = 0;\n      while ((cachedActiveProxy = getActiveNodeProxy()) != null) {\n        try {\n          T ret = doWork();\n          return ret;\n        } catch (RemoteException e) {\n          Throwable cause = e.unwrapRemoteException(StandbyException.class);\n          // if its not a standby exception, then we need to re-throw it, something bad has happened\n          if (cause == e) {\n            throw e;\n          } else {\n            // it is a standby exception, so we try the other NN\n            LOG.warn(\"Failed to reach remote node: \" + currentNN\n                + \", retrying with remaining remote NNs\");\n            cachedActiveProxy = null;\n            // this NN isn't responding to requests, try the next one\n            nnLoopCount++;\n          }\n        }\n      }\n      throw new IOException(\"Cannot find any valid remote NN to service request!\");\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.tooLongSinceLastLoad": "  private boolean tooLongSinceLastLoad() {\n    return logRollPeriodMs >= 0 && \n      (monotonicNow() - lastLoadTimeMs) > logRollPeriodMs ;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll": "  void triggerActiveLogRoll() {\n    LOG.info(\"Triggering log roll on remote NameNode\");\n    Future<Void> future = null;\n    try {\n      future = rollEditsRpcExecutor.submit(getNameNodeProxy());\n      future.get(rollEditsTimeoutMs, TimeUnit.MILLISECONDS);\n      lastRollTriggerTxId = lastLoadedTxnId;\n    } catch (ExecutionException e) {\n      Throwable cause = e.getCause();\n      if (cause instanceof RemoteException) {\n        IOException ioe = ((RemoteException) cause).unwrapRemoteException();\n        if (ioe instanceof StandbyException) {\n          LOG.info(\"Skipping log roll. Remote node is not in Active state: \" +\n              ioe.getMessage().split(\"\\n\")[0]);\n          return;\n        }\n      }\n      LOG.warn(\"Unable to trigger a roll of the active NN\", e);\n    } catch (TimeoutException e) {\n      if (future != null) {\n        future.cancel(true);\n      }\n      LOG.warn(String.format(\n          \"Unable to finish rolling edits in %d ms\", rollEditsTimeoutMs));\n    } catch (InterruptedException e) {\n      LOG.warn(\"Unable to trigger a roll of the active NN\", e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.run": "          public Object run() {\n            doWork();\n            return null;\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal": "  public static <T> T doAsLoginUserOrFatal(PrivilegedAction<T> action) { \n    if (UserGroupInformation.isSecurityEnabled()) {\n      UserGroupInformation ugi = null;\n      try { \n        ugi = UserGroupInformation.getLoginUser();\n      } catch (IOException e) {\n        LOG.error(\"Exception while getting login user\", e);\n        e.printStackTrace();\n        Runtime.getRuntime().exit(-1);\n      }\n      return ugi.doAs(action);\n    } else {\n      return action.run();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.getTransactionId": "  public long getTransactionId() {\n    Preconditions.checkState(txid != HdfsServerConstants.INVALID_TXID);\n    return txid;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.editLogLoaderPrompt": "  public static void editLogLoaderPrompt(String prompt,\n        MetaRecoveryContext recovery, String contStr)\n        throws IOException, RequestStopException\n  {\n    if (recovery == null) {\n      throw new IOException(prompt);\n    }\n    LOG.error(prompt);\n    String answer = recovery.ask(\"\\nEnter 'c' to continue, \" + contStr + \"\\n\" +\n      \"Enter 's' to stop reading the edit log here, abandoning any later \" +\n        \"edits\\n\" +\n      \"Enter 'q' to quit without saving\\n\" +\n      \"Enter 'a' to always select the first choice in the future \" +\n      \"without prompting. \" + \n      \"(c/s/q/a)\\n\", \"c\", \"s\", \"q\", \"a\");\n    if (answer.equals(\"c\")) {\n      LOG.info(\"Continuing\");\n      return;\n    } else if (answer.equals(\"s\")) {\n      throw new RequestStopException(\"user requested stop\");\n    } else if (answer.equals(\"q\")) {\n      recovery.quit();\n    } else {\n      recovery.setForce(FORCE_FIRST_CHOICE);\n      return;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.quit": "  public void quit() {\n    LOG.error(\"Exiting on user request.\");\n    System.exit(0);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.ask": "  public String ask(String prompt, String firstChoice, String... choices) \n      throws IOException {\n    while (true) {\n      System.err.print(prompt);\n      if (force > FORCE_NONE) {\n        System.out.println(\"automatically choosing \" + firstChoice);\n        return firstChoice;\n      }\n      StringBuilder responseBuilder = new StringBuilder();\n      while (true) {\n        int c = System.in.read();\n        if (c == -1 || c == '\\r' || c == '\\n') {\n          break;\n        }\n        responseBuilder.append((char)c);\n      }\n      String response = responseBuilder.toString();\n      if (response.equalsIgnoreCase(firstChoice))\n        return firstChoice;\n      for (String c : choices) {\n        if (response.equalsIgnoreCase(c)) {\n          return c;\n        }\n      }\n      System.err.print(\"I'm sorry, I cannot understand your response.\\n\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.setForce": "  public void setForce(int force) {\n    this.force = force;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getStartupProgress": "  public static StartupProgress getStartupProgress() {\n    return startupProgress;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.hasTransactionId": "  public boolean hasTransactionId() {\n    return (txid != HdfsServerConstants.INVALID_TXID);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.writeUnlock": "  void writeUnlock() {\n    this.dirLock.writeLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.writeLock": "  void writeLock() {\n    this.dirLock.writeLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.closeAllStreams": "  static void closeAllStreams(Iterable<EditLogInputStream> streams) {\n    for (EditLogInputStream s : streams) {\n      IOUtils.closeStream(s);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getLoginUser": "  static UserGroupInformation getLoginUser() throws IOException {\n    if (loginUser == null) {\n      loginUserFromSubject(null);\n    }\n    return loginUser;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject": "  static void loginUserFromSubject(Subject subject) throws IOException {\n    ensureInitialized();\n    boolean externalSubject = false;\n    try {\n      if (subject == null) {\n        subject = new Subject();\n      } else {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Treat subject external: \" + treatSubjectExternal\n              + \". When true, assuming keytab is managed extenally since \"\n              + \" logged in from subject\");\n        }\n        externalSubject = treatSubjectExternal;\n      }\n      LoginContext login =\n          newLoginContext(authenticationMethod.getLoginAppName(), \n                          subject, new HadoopConfiguration());\n      login.login();\n\n      UserGroupInformation realUser =\n          new UserGroupInformation(subject, externalSubject);\n      realUser.setLogin(login);\n      realUser.setAuthenticationMethod(authenticationMethod);\n      // If the HADOOP_PROXY_USER environment variable or property\n      // is specified, create a proxy user as the logged in user.\n      String proxyUser = System.getenv(HADOOP_PROXY_USER);\n      if (proxyUser == null) {\n        proxyUser = System.getProperty(HADOOP_PROXY_USER);\n      }\n      loginUser = proxyUser == null ? realUser : createProxyUser(proxyUser, realUser);\n\n      String tokenFileLocation = System.getProperty(HADOOP_TOKEN_FILES);\n      if (tokenFileLocation == null) {\n        tokenFileLocation = conf.get(HADOOP_TOKEN_FILES);\n      }\n      if (tokenFileLocation != null) {\n        for (String tokenFileName:\n             StringUtils.getTrimmedStrings(tokenFileLocation)) {\n          if (tokenFileName.length() > 0) {\n            File tokenFile = new File(tokenFileName);\n            if (tokenFile.exists() && tokenFile.isFile()) {\n              Credentials cred = Credentials.readTokenStorageFile(\n                  tokenFile, conf);\n              loginUser.addCredentials(cred);\n            } else {\n              LOG.info(\"tokenFile(\"+tokenFileName+\") does not exist\");\n            }\n          }\n        }\n      }\n\n      String fileLocation = System.getenv(HADOOP_TOKEN_FILE_LOCATION);\n      if (fileLocation != null) {\n        // Load the token storage file and put all of the tokens into the\n        // user. Don't use the FileSystem API for reading since it has a lock\n        // cycle (HADOOP-9212).\n        File source = new File(fileLocation);\n        LOG.debug(\"Reading credentials from location set in {}: {}\",\n            HADOOP_TOKEN_FILE_LOCATION,\n            source.getCanonicalPath());\n        if (!source.isFile()) {\n          throw new FileNotFoundException(\"Source file \"\n              + source.getCanonicalPath() + \" from \"\n              + HADOOP_TOKEN_FILE_LOCATION\n              + \" not found\");\n        }\n        Credentials cred = Credentials.readTokenStorageFile(\n            source, conf);\n        LOG.debug(\"Loaded {} tokens\", cred.numberOfTokens());\n        loginUser.addCredentials(cred);\n      }\n      loginUser.spawnAutoRenewalThreadForUserCreds();\n    } catch (LoginException le) {\n      LOG.debug(\"failure to login\", le);\n      throw new KerberosAuthException(FAILURE_TO_LOGIN, le);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"UGI loginUser:\"+loginUser);\n    } \n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled": "  public static boolean isSecurityEnabled() {\n    return !isAuthenticationMethodEnabled(AuthenticationMethod.SIMPLE);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.isAuthenticationMethodEnabled": "  private static boolean isAuthenticationMethodEnabled(AuthenticationMethod method) {\n    ensureInitialized();\n    return (authenticationMethod == method);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause == null) {\n        throw new RuntimeException(\"PrivilegedActionException with no \" +\n                \"underlying cause. UGI [\" + this + \"]\" +\": \" + pae, pae);\n      } else if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }"
        },
        "bug_report": {
            "Title": "startTxId could be greater than endTxId when tailing in-progress edit log",
            "Description": "When {{dfs.ha.tail-edits.in-progress}} is true, edit log tailer will also tail those in progress edit log segments. However, in the following code:\r\n\r\n{code}\r\n        if (onlyDurableTxns && inProgressOk) {\r\n          endTxId = Math.min(endTxId, committedTxnId);\r\n        }\r\n\r\n        EditLogInputStream elis = EditLogFileInputStream.fromUrl(\r\n            connectionFactory, url, remoteLog.getStartTxId(),\r\n            endTxId, remoteLog.isInProgress());\r\n{code}\r\n\r\nit is possible that {{remoteLog.getStartTxId()}} could be greater than {{endTxId}}, and therefore will cause the following error:\r\n\r\n{code}\r\n2017-11-17 19:55:41,165 ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: Error replaying edit log at offset 1048576.  Expected transaction ID was 87\r\nRecent opcode offsets: 1048576\r\norg.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$PrematureEOFException: got premature end-of-file at txid 86; expected file to go up to 85\r\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:197)\r\n        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)\r\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:189)\r\n        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:205)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:158)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:882)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:863)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:293)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:427)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:380)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:397)\r\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:481)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:393)\r\n2017-11-17 19:55:41,165 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Error while reading edits from disk. Will try again.\r\norg.apache.hadoop.hdfs.server.namenode.EditLogInputException: Error replaying edit log at offset 1048576.  Expected transaction ID was 87\r\nRecent opcode offsets: 1048576\r\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:218)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:158)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:882)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:863)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:293)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:427)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:380)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:397)\r\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:481)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:393)\r\nCaused by: org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$PrematureEOFException: got premature end-of-file at txid 86; expected file to go up to 85\r\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:197)\r\n        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)\r\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:189)\r\n        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:205)\r\n        ... 9 more\r\n{code}"
        }
    },
    {
        "filename": "HDFS-8113.json",
        "creation_time": "2015-04-09T11:29:31.000+0000",
        "stack_trace": "```\norg.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo.(BlockInfo.java:80)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt.(BlockManager.java:1696)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.checkReplicaCorrupt(BlockManager.java:2185)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReportedBlock(BlockManager.java:2047)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.reportDiff(BlockManager.java:1950)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:1823)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:1750)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.blockReport(NameNodeRpcServer.java:1069)\nat org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.blockReport(DatanodeProtocolServerSideTranslatorPB.java:152)\nat org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:26382)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1623)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.checkReplicaCorrupt": "  private BlockToMarkCorrupt checkReplicaCorrupt(\n      Block reported, ReplicaState reportedState, \n      BlockInfoContiguous storedBlock, BlockUCState ucState,\n      DatanodeDescriptor dn) {\n    switch(reportedState) {\n    case FINALIZED:\n      switch(ucState) {\n      case COMPLETE:\n      case COMMITTED:\n        if (storedBlock.getGenerationStamp() != reported.getGenerationStamp()) {\n          final long reportedGS = reported.getGenerationStamp();\n          return new BlockToMarkCorrupt(storedBlock, reportedGS,\n              \"block is \" + ucState + \" and reported genstamp \" + reportedGS\n              + \" does not match genstamp in block map \"\n              + storedBlock.getGenerationStamp(), Reason.GENSTAMP_MISMATCH);\n        } else if (storedBlock.getNumBytes() != reported.getNumBytes()) {\n          return new BlockToMarkCorrupt(storedBlock,\n              \"block is \" + ucState + \" and reported length \" +\n              reported.getNumBytes() + \" does not match \" +\n              \"length in block map \" + storedBlock.getNumBytes(),\n              Reason.SIZE_MISMATCH);\n        } else {\n          return null; // not corrupt\n        }\n      case UNDER_CONSTRUCTION:\n        if (storedBlock.getGenerationStamp() > reported.getGenerationStamp()) {\n          final long reportedGS = reported.getGenerationStamp();\n          return new BlockToMarkCorrupt(storedBlock, reportedGS, \"block is \"\n              + ucState + \" and reported state \" + reportedState\n              + \", But reported genstamp \" + reportedGS\n              + \" does not match genstamp in block map \"\n              + storedBlock.getGenerationStamp(), Reason.GENSTAMP_MISMATCH);\n        }\n        return null;\n      default:\n        return null;\n      }\n    case RBW:\n    case RWR:\n      if (!storedBlock.isComplete()) {\n        return null; // not corrupt\n      } else if (storedBlock.getGenerationStamp() != reported.getGenerationStamp()) {\n        final long reportedGS = reported.getGenerationStamp();\n        return new BlockToMarkCorrupt(storedBlock, reportedGS,\n            \"reported \" + reportedState + \" replica with genstamp \" + reportedGS\n            + \" does not match COMPLETE block's genstamp in block map \"\n            + storedBlock.getGenerationStamp(), Reason.GENSTAMP_MISMATCH);\n      } else { // COMPLETE block, same genstamp\n        if (reportedState == ReplicaState.RBW) {\n          // If it's a RBW report for a COMPLETE block, it may just be that\n          // the block report got a little bit delayed after the pipeline\n          // closed. So, ignore this report, assuming we will get a\n          // FINALIZED replica later. See HDFS-2791\n          LOG.info(\"Received an RBW replica for \" + storedBlock +\n              \" on \" + dn + \": ignoring it, since it is \" +\n              \"complete with the same genstamp\");\n          return null;\n        } else {\n          return new BlockToMarkCorrupt(storedBlock,\n              \"reported replica has invalid state \" + reportedState,\n              Reason.INVALID_STATE);\n        }\n      }\n    case RUR:       // should not be reported\n    case TEMPORARY: // should not be reported\n    default:\n      String msg = \"Unexpected replica state \" + reportedState\n      + \" for block: \" + storedBlock + \n      \" on \" + dn + \" size \" + storedBlock.getNumBytes();\n      // log here at WARN level since this is really a broken HDFS invariant\n      LOG.warn(msg);\n      return new BlockToMarkCorrupt(storedBlock, msg, Reason.INVALID_STATE);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReportedBlock": "  private BlockInfoContiguous processReportedBlock(\n      final DatanodeStorageInfo storageInfo,\n      final Block block, final ReplicaState reportedState, \n      final Collection<BlockInfoContiguous> toAdd,\n      final Collection<Block> toInvalidate, \n      final Collection<BlockToMarkCorrupt> toCorrupt,\n      final Collection<StatefulBlockInfo> toUC) {\n    \n    DatanodeDescriptor dn = storageInfo.getDatanodeDescriptor();\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Reported block \" + block\n          + \" on \" + dn + \" size \" + block.getNumBytes()\n          + \" replicaState = \" + reportedState);\n    }\n  \n    if (shouldPostponeBlocksFromFuture &&\n        namesystem.isGenStampInFuture(block)) {\n      queueReportedBlock(storageInfo, block, reportedState,\n          QUEUE_REASON_FUTURE_GENSTAMP);\n      return null;\n    }\n    \n    // find block by blockId\n    BlockInfoContiguous storedBlock = blocksMap.getStoredBlock(block);\n    if(storedBlock == null) {\n      // If blocksMap does not contain reported block id,\n      // the replica should be removed from the data-node.\n      toInvalidate.add(new Block(block));\n      return null;\n    }\n    BlockUCState ucState = storedBlock.getBlockUCState();\n    \n    // Block is on the NN\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"In memory blockUCState = \" + ucState);\n    }\n\n    // Ignore replicas already scheduled to be removed from the DN\n    if(invalidateBlocks.contains(dn, block)) {\n      /*\n       * TODO: following assertion is incorrect, see HDFS-2668 assert\n       * storedBlock.findDatanode(dn) < 0 : \"Block \" + block +\n       * \" in recentInvalidatesSet should not appear in DN \" + dn;\n       */\n      return storedBlock;\n    }\n\n    BlockToMarkCorrupt c = checkReplicaCorrupt(\n        block, reportedState, storedBlock, ucState, dn);\n    if (c != null) {\n      if (shouldPostponeBlocksFromFuture) {\n        // If the block is an out-of-date generation stamp or state,\n        // but we're the standby, we shouldn't treat it as corrupt,\n        // but instead just queue it for later processing.\n        // TODO: Pretty confident this should be s/storedBlock/block below,\n        // since we should be postponing the info of the reported block, not\n        // the stored block. See HDFS-6289 for more context.\n        queueReportedBlock(storageInfo, storedBlock, reportedState,\n            QUEUE_REASON_CORRUPT_STATE);\n      } else {\n        toCorrupt.add(c);\n      }\n      return storedBlock;\n    }\n\n    if (isBlockUnderConstruction(storedBlock, ucState, reportedState)) {\n      toUC.add(new StatefulBlockInfo(\n          (BlockInfoContiguousUnderConstruction) storedBlock,\n          new Block(block), reportedState));\n      return storedBlock;\n    }\n\n    // Add replica if appropriate. If the replica was previously corrupt\n    // but now okay, it might need to be updated.\n    if (reportedState == ReplicaState.FINALIZED\n        && (storedBlock.findStorageInfo(storageInfo) == -1 ||\n            corruptReplicas.isReplicaCorrupt(storedBlock, dn))) {\n      toAdd.add(storedBlock);\n    }\n    return storedBlock;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.queueReportedBlock": "  private void queueReportedBlock(DatanodeStorageInfo storageInfo, Block block,\n      ReplicaState reportedState, String reason) {\n    assert shouldPostponeBlocksFromFuture;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Queueing reported block \" + block +\n          \" in state \" + reportedState + \n          \" from datanode \" + storageInfo.getDatanodeDescriptor() +\n          \" for later processing because \" + reason + \".\");\n    }\n    pendingDNMessages.enqueueReportedBlock(storageInfo, block, reportedState);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.isBlockUnderConstruction": "  private boolean isBlockUnderConstruction(BlockInfoContiguous storedBlock,\n      BlockUCState ucState, ReplicaState reportedState) {\n    switch(reportedState) {\n    case FINALIZED:\n      switch(ucState) {\n      case UNDER_CONSTRUCTION:\n      case UNDER_RECOVERY:\n        return true;\n      default:\n        return false;\n      }\n    case RBW:\n    case RWR:\n      return (!storedBlock.isComplete());\n    case RUR:       // should not be reported                                                                                             \n    case TEMPORARY: // should not be reported                                                                                             \n    default:\n      return false;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.getStoredBlock": "  public BlockInfoContiguous getStoredBlock(Block block) {\n    return blocksMap.getStoredBlock(block);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.reportDiff": "  private void reportDiff(DatanodeStorageInfo storageInfo, \n      BlockListAsLongs newReport, \n      Collection<BlockInfoContiguous> toAdd,              // add to DatanodeDescriptor\n      Collection<Block> toRemove,           // remove from DatanodeDescriptor\n      Collection<Block> toInvalidate,       // should be removed from DN\n      Collection<BlockToMarkCorrupt> toCorrupt, // add to corrupt replicas list\n      Collection<StatefulBlockInfo> toUC) { // add to under-construction list\n\n    // place a delimiter in the list which separates blocks \n    // that have been reported from those that have not\n    BlockInfoContiguous delimiter = new BlockInfoContiguous(new Block(), (short) 1);\n    AddBlockResult result = storageInfo.addBlock(delimiter);\n    assert result == AddBlockResult.ADDED \n        : \"Delimiting block cannot be present in the node\";\n    int headIndex = 0; //currently the delimiter is in the head of the list\n    int curIndex;\n\n    if (newReport == null) {\n      newReport = BlockListAsLongs.EMPTY;\n    }\n    // scan the report and process newly reported blocks\n    for (BlockReportReplica iblk : newReport) {\n      ReplicaState iState = iblk.getState();\n      BlockInfoContiguous storedBlock = processReportedBlock(storageInfo,\n          iblk, iState, toAdd, toInvalidate, toCorrupt, toUC);\n\n      // move block to the head of the list\n      if (storedBlock != null &&\n          (curIndex = storedBlock.findStorageInfo(storageInfo)) >= 0) {\n        headIndex = storageInfo.moveBlockToHead(storedBlock, curIndex, headIndex);\n      }\n    }\n\n    // collect blocks that have not been reported\n    // all of them are next to the delimiter\n    Iterator<BlockInfoContiguous> it =\n        storageInfo.new BlockIterator(delimiter.getNext(0));\n    while(it.hasNext())\n      toRemove.add(it.next());\n    storageInfo.removeBlock(delimiter);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.removeBlock": "  public void removeBlock(Block block) {\n    assert namesystem.hasWriteLock();\n    // No need to ACK blocks that are being removed entirely\n    // from the namespace, since the removal of the associated\n    // file already removes them from the block map below.\n    block.setNumBytes(BlockCommand.NO_ACK);\n    addToInvalidates(block);\n    removeBlockFromMap(block);\n    // Remove the block from pendingReplications and neededReplications\n    pendingReplications.remove(block);\n    neededReplications.remove(block, UnderReplicatedBlocks.LEVEL);\n    if (postponedMisreplicatedBlocks.remove(block)) {\n      postponedMisreplicatedBlocksCount.decrementAndGet();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.addBlock": "  void addBlock(DatanodeStorageInfo storageInfo, Block block, String delHint)\n      throws IOException {\n    DatanodeDescriptor node = storageInfo.getDatanodeDescriptor();\n    // Decrement number of blocks scheduled to this datanode.\n    // for a retry request (of DatanodeProtocol#blockReceivedAndDeleted with \n    // RECEIVED_BLOCK), we currently also decrease the approximate number. \n    node.decrementBlocksScheduled(storageInfo.getStorageType());\n\n    // get the deletion hint node\n    DatanodeDescriptor delHintNode = null;\n    if (delHint != null && delHint.length() != 0) {\n      delHintNode = datanodeManager.getDatanode(delHint);\n      if (delHintNode == null) {\n        blockLog.warn(\"BLOCK* blockReceived: {} is expected to be removed \" +\n            \"from an unrecorded node {}\", block, delHint);\n      }\n    }\n\n    //\n    // Modify the blocks->datanode map and node's map.\n    //\n    pendingReplications.decrement(block, node);\n    processAndHandleReportedBlock(storageInfo, block, ReplicaState.FINALIZED,\n        delHintNode);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport": "  private Collection<Block> processReport(\n      final DatanodeStorageInfo storageInfo,\n      final BlockListAsLongs report) throws IOException {\n    // Normal case:\n    // Modify the (block-->datanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection<BlockInfoContiguous> toAdd = new LinkedList<BlockInfoContiguous>();\n    Collection<Block> toRemove = new TreeSet<Block>();\n    Collection<Block> toInvalidate = new LinkedList<Block>();\n    Collection<BlockToMarkCorrupt> toCorrupt = new LinkedList<BlockToMarkCorrupt>();\n    Collection<StatefulBlockInfo> toUC = new LinkedList<StatefulBlockInfo>();\n    reportDiff(storageInfo, report,\n        toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n   \n    DatanodeDescriptor node = storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (Block b : toRemove) {\n      removeStoredBlock(b, node);\n    }\n    int numBlocksLogged = 0;\n    for (BlockInfoContiguous b : toAdd) {\n      addStoredBlock(b, storageInfo, null, numBlocksLogged < maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged > maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n          \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n\n    return toInvalidate;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processFirstBlockReport": "  private void processFirstBlockReport(\n      final DatanodeStorageInfo storageInfo,\n      final BlockListAsLongs report) throws IOException {\n    if (report == null) return;\n    assert (namesystem.hasWriteLock());\n    assert (storageInfo.numBlocks() == 0);\n\n    for (BlockReportReplica iblk : report) {\n      ReplicaState reportedState = iblk.getState();\n      \n      if (shouldPostponeBlocksFromFuture &&\n          namesystem.isGenStampInFuture(iblk)) {\n        queueReportedBlock(storageInfo, iblk, reportedState,\n            QUEUE_REASON_FUTURE_GENSTAMP);\n        continue;\n      }\n      \n      BlockInfoContiguous storedBlock = blocksMap.getStoredBlock(iblk);\n      // If block does not belong to any file, we are done.\n      if (storedBlock == null) continue;\n      \n      // If block is corrupt, mark it and continue to next block.\n      BlockUCState ucState = storedBlock.getBlockUCState();\n      BlockToMarkCorrupt c = checkReplicaCorrupt(\n          iblk, reportedState, storedBlock, ucState,\n          storageInfo.getDatanodeDescriptor());\n      if (c != null) {\n        if (shouldPostponeBlocksFromFuture) {\n          // In the Standby, we may receive a block report for a file that we\n          // just have an out-of-date gen-stamp or state for, for example.\n          queueReportedBlock(storageInfo, iblk, reportedState,\n              QUEUE_REASON_CORRUPT_STATE);\n        } else {\n          markBlockAsCorrupt(c, storageInfo, storageInfo.getDatanodeDescriptor());\n        }\n        continue;\n      }\n      \n      // If block is under construction, add this replica to its list\n      if (isBlockUnderConstruction(storedBlock, ucState, reportedState)) {\n        ((BlockInfoContiguousUnderConstruction)storedBlock)\n            .addReplicaIfNotPresent(storageInfo, iblk, reportedState);\n        // OpenFileBlocks only inside snapshots also will be added to safemode\n        // threshold. So we need to update such blocks to safemode\n        // refer HDFS-5283\n        BlockInfoContiguousUnderConstruction blockUC =\n            (BlockInfoContiguousUnderConstruction) storedBlock;\n        if (namesystem.isInSnapshot(blockUC)) {\n          int numOfReplicas = blockUC.getNumExpectedLocations();\n          namesystem.incrementSafeBlockCount(numOfReplicas);\n        }\n        //and fall through to next clause\n      }      \n      //add replica if appropriate\n      if (reportedState == ReplicaState.FINALIZED) {\n        addStoredBlockImmediate(storedBlock, storageInfo);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.removeZombieReplicas": "  private void removeZombieReplicas(BlockReportContext context,\n      DatanodeStorageInfo zombie) {\n    LOG.warn(\"processReport 0x{}: removing zombie storage {}, which no \" +\n             \"longer exists on the DataNode.\",\n              Long.toHexString(context.getReportId()), zombie.getStorageID());\n    assert(namesystem.hasWriteLock());\n    Iterator<BlockInfoContiguous> iter = zombie.getBlockIterator();\n    int prevBlocks = zombie.numBlocks();\n    while (iter.hasNext()) {\n      BlockInfoContiguous block = iter.next();\n      // We assume that a block can be on only one storage in a DataNode.\n      // That's why we pass in the DatanodeDescriptor rather than the\n      // DatanodeStorageInfo.\n      // TODO: remove this assumption in case we want to put a block on\n      // more than one storage on a datanode (and because it's a difficult\n      // assumption to really enforce)\n      removeStoredBlock(block, zombie.getDatanodeDescriptor());\n      invalidateBlocks.remove(zombie.getDatanodeDescriptor(), block);\n    }\n    assert(zombie.numBlocks() == 0);\n    LOG.warn(\"processReport 0x{}: removed {} replicas from storage {}, \" +\n            \"which no longer exists on the DataNode.\",\n            Long.toHexString(context.getReportId()), prevBlocks,\n            zombie.getStorageID());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.removeStoredBlock": "  public void removeStoredBlock(Block block, DatanodeDescriptor node) {\n    blockLog.debug(\"BLOCK* removeStoredBlock: {} from {}\", block, node);\n    assert (namesystem.hasWriteLock());\n    {\n      if (!blocksMap.removeNode(block, node)) {\n        blockLog.debug(\"BLOCK* removeStoredBlock: {} has already been\" +\n            \" removed from node {}\", block, node);\n        return;\n      }\n\n      //\n      // It's possible that the block was removed because of a datanode\n      // failure. If the block is still valid, check if replication is\n      // necessary. In that case, put block on a possibly-will-\n      // be-replicated list.\n      //\n      BlockCollection bc = blocksMap.getBlockCollection(block);\n      if (bc != null) {\n        namesystem.decrementSafeBlockCount(block);\n        updateNeededReplications(block, -1, 0);\n      }\n\n      //\n      // We've removed a block from a node, so it's definitely no longer\n      // in \"excess\" there.\n      //\n      LightWeightLinkedSet<Block> excessBlocks = excessReplicateMap.get(node\n          .getDatanodeUuid());\n      if (excessBlocks != null) {\n        if (excessBlocks.remove(block)) {\n          excessBlocksCount.decrementAndGet();\n          blockLog.debug(\"BLOCK* removeStoredBlock: {} is removed from \" +\n              \"excessBlocks\", block);\n          if (excessBlocks.size() == 0) {\n            excessReplicateMap.remove(node.getDatanodeUuid());\n          }\n        }\n      }\n\n      // Remove the replica from corruptReplicas\n      corruptReplicas.removeFromCorruptReplicasMap(block, node);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.markBlockAsCorrupt": "  private void markBlockAsCorrupt(BlockToMarkCorrupt b,\n      DatanodeStorageInfo storageInfo,\n      DatanodeDescriptor node) throws IOException {\n\n    BlockCollection bc = b.corrupted.getBlockCollection();\n    if (bc == null) {\n      blockLog.info(\"BLOCK markBlockAsCorrupt: {} cannot be marked as\" +\n          \" corrupt as it does not belong to any file\", b);\n      addToInvalidates(b.corrupted, node);\n      return;\n    } \n\n    // Add replica to the data-node if it is not already there\n    if (storageInfo != null) {\n      storageInfo.addBlock(b.stored);\n    }\n\n    // Add this replica to corruptReplicas Map\n    corruptReplicas.addToCorruptReplicasMap(b.corrupted, node, b.reason,\n        b.reasonCode);\n\n    NumberReplicas numberOfReplicas = countNodes(b.stored);\n    boolean hasEnoughLiveReplicas = numberOfReplicas.liveReplicas() >= bc\n        .getBlockReplication();\n    boolean minReplicationSatisfied =\n        numberOfReplicas.liveReplicas() >= minReplication;\n    boolean hasMoreCorruptReplicas = minReplicationSatisfied &&\n        (numberOfReplicas.liveReplicas() + numberOfReplicas.corruptReplicas()) >\n        bc.getBlockReplication();\n    boolean corruptedDuringWrite = minReplicationSatisfied &&\n        (b.stored.getGenerationStamp() > b.corrupted.getGenerationStamp());\n    // case 1: have enough number of live replicas\n    // case 2: corrupted replicas + live replicas > Replication factor\n    // case 3: Block is marked corrupt due to failure while writing. In this\n    //         case genstamp will be different than that of valid block.\n    // In all these cases we can delete the replica.\n    // In case of 3, rbw block will be deleted and valid block can be replicated\n    if (hasEnoughLiveReplicas || hasMoreCorruptReplicas\n        || corruptedDuringWrite) {\n      // the block is over-replicated so invalidate the replicas immediately\n      invalidateBlock(b, node);\n    } else if (namesystem.isPopulatingReplQueues()) {\n      // add the block to neededReplication\n      updateNeededReplications(b.stored, -1, 0);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.addStoredBlockUnderConstruction": "  void addStoredBlockUnderConstruction(StatefulBlockInfo ucBlock,\n      DatanodeStorageInfo storageInfo) throws IOException {\n    BlockInfoContiguousUnderConstruction block = ucBlock.storedBlock;\n    block.addReplicaIfNotPresent(\n        storageInfo, ucBlock.reportedBlock, ucBlock.reportedState);\n\n    if (ucBlock.reportedState == ReplicaState.FINALIZED &&\n        !block.findDatanode(storageInfo.getDatanodeDescriptor())) {\n      addStoredBlock(block, storageInfo, null, true);\n    }\n  } ",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.addToInvalidates": "  private void addToInvalidates(Block b) {\n    if (!namesystem.isPopulatingReplQueues()) {\n      return;\n    }\n    StringBuilder datanodes = new StringBuilder();\n    for(DatanodeStorageInfo storage : blocksMap.getStorages(b, State.NORMAL)) {\n      final DatanodeDescriptor node = storage.getDatanodeDescriptor();\n      invalidateBlocks.add(b, node, false);\n      datanodes.append(node).append(\" \");\n    }\n    if (datanodes.length() != 0) {\n      blockLog.info(\"BLOCK* addToInvalidates: {} {}\", b, datanodes.toString());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.addStoredBlock": "  private Block addStoredBlock(final BlockInfoContiguous block,\n                               DatanodeStorageInfo storageInfo,\n                               DatanodeDescriptor delNodeHint,\n                               boolean logEveryBlock)\n  throws IOException {\n    assert block != null && namesystem.hasWriteLock();\n    BlockInfoContiguous storedBlock;\n    DatanodeDescriptor node = storageInfo.getDatanodeDescriptor();\n    if (block instanceof BlockInfoContiguousUnderConstruction) {\n      //refresh our copy in case the block got completed in another thread\n      storedBlock = blocksMap.getStoredBlock(block);\n    } else {\n      storedBlock = block;\n    }\n    if (storedBlock == null || storedBlock.getBlockCollection() == null) {\n      // If this block does not belong to anyfile, then we are done.\n      blockLog.info(\"BLOCK* addStoredBlock: {} on {} size {} but it does not\" +\n          \" belong to any file\", block, node, block.getNumBytes());\n\n      // we could add this block to invalidate set of this datanode.\n      // it will happen in next block report otherwise.\n      return block;\n    }\n    BlockCollection bc = storedBlock.getBlockCollection();\n    assert bc != null : \"Block must belong to a file\";\n\n    // add block to the datanode\n    AddBlockResult result = storageInfo.addBlock(storedBlock);\n\n    int curReplicaDelta;\n    if (result == AddBlockResult.ADDED) {\n      curReplicaDelta = 1;\n      if (logEveryBlock) {\n        logAddStoredBlock(storedBlock, node);\n      }\n    } else if (result == AddBlockResult.REPLACED) {\n      curReplicaDelta = 0;\n      blockLog.warn(\"BLOCK* addStoredBlock: block {} moved to storageType \" +\n          \"{} on node {}\", storedBlock, storageInfo.getStorageType(), node);\n    } else {\n      // if the same block is added again and the replica was corrupt\n      // previously because of a wrong gen stamp, remove it from the\n      // corrupt block list.\n      corruptReplicas.removeFromCorruptReplicasMap(block, node,\n          Reason.GENSTAMP_MISMATCH);\n      curReplicaDelta = 0;\n      blockLog.warn(\"BLOCK* addStoredBlock: Redundant addStoredBlock request\"\n              + \" received for {} on node {} size {}\", storedBlock, node,\n          storedBlock.getNumBytes());\n    }\n\n    // Now check for completion of blocks and safe block count\n    NumberReplicas num = countNodes(storedBlock);\n    int numLiveReplicas = num.liveReplicas();\n    int numCurrentReplica = numLiveReplicas\n      + pendingReplications.getNumReplicas(storedBlock);\n\n    if(storedBlock.getBlockUCState() == BlockUCState.COMMITTED &&\n        numLiveReplicas >= minReplication) {\n      storedBlock = completeBlock(bc, storedBlock, false);\n    } else if (storedBlock.isComplete() && result == AddBlockResult.ADDED) {\n      // check whether safe replication is reached for the block\n      // only complete blocks are counted towards that\n      // Is no-op if not in safe mode.\n      // In the case that the block just became complete above, completeBlock()\n      // handles the safe block count maintenance.\n      namesystem.incrementSafeBlockCount(numCurrentReplica);\n    }\n    \n    // if file is under construction, then done for now\n    if (bc.isUnderConstruction()) {\n      return storedBlock;\n    }\n\n    // do not try to handle over/under-replicated blocks during first safe mode\n    if (!namesystem.isPopulatingReplQueues()) {\n      return storedBlock;\n    }\n\n    // handle underReplication/overReplication\n    short fileReplication = bc.getBlockReplication();\n    if (!isNeededReplication(storedBlock, fileReplication, numCurrentReplica)) {\n      neededReplications.remove(storedBlock, numCurrentReplica,\n          num.decommissionedReplicas(), fileReplication);\n    } else {\n      updateNeededReplications(storedBlock, curReplicaDelta, 0);\n    }\n    if (numCurrentReplica > fileReplication) {\n      processOverReplicatedBlock(storedBlock, fileReplication, node, delNodeHint);\n    }\n    // If the file replication has reached desired value\n    // we can remove any corrupt replicas the block may have\n    int corruptReplicasCount = corruptReplicas.numCorruptReplicas(storedBlock);\n    int numCorruptNodes = num.corruptReplicas();\n    if (numCorruptNodes != corruptReplicasCount) {\n      LOG.warn(\"Inconsistent number of corrupt replicas for \" +\n          storedBlock + \"blockMap has \" + numCorruptNodes + \n          \" but corrupt replicas map has \" + corruptReplicasCount);\n    }\n    if ((corruptReplicasCount > 0) && (numLiveReplicas >= fileReplication))\n      invalidateCorruptReplicas(storedBlock);\n    return storedBlock;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.blockReport": "  public DatanodeCommand blockReport(DatanodeRegistration nodeReg,\n        String poolId, StorageBlockReport[] reports,\n        BlockReportContext context) throws IOException {\n    checkNNStartup();\n    verifyRequest(nodeReg);\n    if(blockStateChangeLog.isDebugEnabled()) {\n      blockStateChangeLog.debug(\"*BLOCK* NameNode.blockReport: \"\n           + \"from \" + nodeReg + \", reports.length=\" + reports.length);\n    }\n    final BlockManager bm = namesystem.getBlockManager(); \n    boolean noStaleStorages = false;\n    for (int r = 0; r < reports.length; r++) {\n      final BlockListAsLongs blocks = reports[r].getBlocks();\n      //\n      // BlockManager.processReport accumulates information of prior calls\n      // for the same node and storage, so the value returned by the last\n      // call of this loop is the final updated value for noStaleStorage.\n      //\n      noStaleStorages = bm.processReport(nodeReg, reports[r].getStorage(),\n          blocks, context, (r == reports.length - 1));\n      metrics.incrStorageBlockReportOps();\n    }\n\n    if (nn.getFSImage().isUpgradeFinalized() &&\n        !namesystem.isRollingUpgrade() &&\n        !nn.isStandbyState() &&\n        noStaleStorages) {\n      return new FinalizeCommand(poolId);\n    }\n\n    return null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlocks": "  public BlocksWithLocations getBlocks(DatanodeInfo datanode, long size)\n  throws IOException {\n    if(size <= 0) {\n      throw new IllegalArgumentException(\n        \"Unexpected not positive size: \"+size);\n    }\n    checkNNStartup();\n    namesystem.checkSuperuserPrivilege();\n    return namesystem.getBlockManager().getBlocks(datanode, size); \n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.verifyRequest": "  private void verifyRequest(NodeRegistration nodeReg) throws IOException {\n    // verify registration ID\n    final String id = nodeReg.getRegistrationID();\n    final String expectedID = namesystem.getRegistrationID();\n    if (!expectedID.equals(id)) {\n      LOG.warn(\"Registration IDs mismatched: the \"\n          + nodeReg.getClass().getSimpleName() + \" ID is \" + id\n          + \" but the expected ID is \" + expectedID);\n       throw new UnregisteredNodeException(nodeReg);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkNNStartup": "  private void checkNNStartup() throws IOException {\n    if (!this.nn.isStarted()) {\n      throw new IOException(this.nn.getRole() + \" still not started\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.blockReport": "  public BlockReportResponseProto blockReport(RpcController controller,\n      BlockReportRequestProto request) throws ServiceException {\n    DatanodeCommand cmd = null;\n    StorageBlockReport[] report = \n        new StorageBlockReport[request.getReportsCount()];\n    \n    int index = 0;\n    for (StorageBlockReportProto s : request.getReportsList()) {\n      final BlockListAsLongs blocks;\n      if (s.hasNumberOfBlocks()) { // new style buffer based reports\n        int num = (int)s.getNumberOfBlocks();\n        Preconditions.checkState(s.getBlocksCount() == 0,\n            \"cannot send both blocks list and buffers\");\n        blocks = BlockListAsLongs.decodeBuffers(num, s.getBlocksBuffersList());\n      } else {\n        blocks = BlockListAsLongs.decodeLongs(s.getBlocksList());\n      }\n      report[index++] = new StorageBlockReport(PBHelper.convert(s.getStorage()),\n          blocks);\n    }\n    try {\n      cmd = impl.blockReport(PBHelper.convert(request.getRegistration()),\n          request.getBlockPoolId(), report,\n          request.hasContext() ?\n              PBHelper.convert(request.getContext()) : null);\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n    BlockReportResponseProto.Builder builder = \n        BlockReportResponseProto.newBuilder();\n    if (cmd != null) {\n      builder.setCmd(PBHelper.convert(cmd));\n    }\n    return builder.build();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcRequestWrapper request = (RpcRequestWrapper) writableRequest;\n        RequestHeaderProto rpcRequest = request.requestHeader;\n        String methodName = rpcRequest.getMethodName();\n        \n        \n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = prototype.newBuilderForType()\n            .mergeFrom(request.theRequestRead).build();\n        \n        Message result;\n        long startTime = Time.now();\n        int qTime = (int) (startTime - receiveTime);\n        Exception exception = null;\n        try {\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n        } catch (ServiceException e) {\n          exception = (Exception) e.getCause();\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          exception = e;\n          throw e;\n        } finally {\n          int processingTime = (int) (Time.now() - startTime);\n          if (LOG.isDebugEnabled()) {\n            String msg = \"Served: \" + methodName + \" queueTime= \" + qTime +\n                \" procesingTime= \" + processingTime;\n            if (exception != null) {\n              msg += \" exception= \" + exception.getClass().getSimpleName();\n            }\n            LOG.debug(msg);\n          }\n          String detailedMetricsName = (exception == null) ?\n              methodName :\n              exception.getClass().getSimpleName();\n          server.rpcMetrics.addRpcQueueTime(qTime);\n          server.rpcMetrics.addRpcProcessingTime(processingTime);\n          server.rpcDetailedMetrics.addProcessingTime(detailedMetricsName,\n              processingTime);\n        }\n        return new RpcResponseWrapper(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getProtocolImpl": "      private static ProtoClassProtoImpl getProtocolImpl(RPC.Server server,\n          String protoName, long clientVersion) throws RpcServerException {\n        ProtoNameVer pv = new ProtoNameVer(protoName, clientVersion);\n        ProtoClassProtoImpl impl = \n            server.getProtocolImplMap(RPC.RpcKind.RPC_PROTOCOL_BUFFER).get(pv);\n        if (impl == null) { // no match for Protocol AND Version\n          VerProtocolImpl highest = \n              server.getHighestSupportedProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER, \n                  protoName);\n          if (highest == null) {\n            throw new RpcNoSuchProtocolException(\n                \"Unknown protocol: \" + protoName);\n          }\n          // protocol supported but not the version that client wants\n          throw new RPC.VersionMismatch(protoName, clientVersion,\n              highest.version);\n        }\n        return impl;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.call": "    public Writable call(RPC.RpcKind rpcKind, String protocol,\n        Writable rpcRequest, long receiveTime) throws Exception {\n      return getRpcInvoker(rpcKind).call(this, protocol, rpcRequest,\n          receiveTime);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.toString": "    public String toString() {\n      return getHostAddress() + \":\" + remotePort; \n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(ByteArrayOutputStream responseBuf,\n                             Call call, RpcStatusProto status, RpcErrorCodeProto erCode,\n                             Writable rv, String errorClass, String error) \n  throws IOException {\n    responseBuf.reset();\n    DataOutputStream out = new DataOutputStream(responseBuf);\n    RpcResponseHeaderProto.Builder headerBuilder =  \n        RpcResponseHeaderProto.newBuilder();\n    headerBuilder.setClientId(ByteString.copyFrom(call.clientId));\n    headerBuilder.setCallId(call.callId);\n    headerBuilder.setRetryCount(call.retryCount);\n    headerBuilder.setStatus(status);\n    headerBuilder.setServerIpcVersionNum(CURRENT_VERSION);\n\n    if (status == RpcStatusProto.SUCCESS) {\n      RpcResponseHeaderProto header = headerBuilder.build();\n      final int headerLen = header.getSerializedSize();\n      int fullLength  = CodedOutputStream.computeRawVarint32Size(headerLen) +\n          headerLen;\n      try {\n        if (rv instanceof ProtobufRpcEngine.RpcWrapper) {\n          ProtobufRpcEngine.RpcWrapper resWrapper = \n              (ProtobufRpcEngine.RpcWrapper) rv;\n          fullLength += resWrapper.getLength();\n          out.writeInt(fullLength);\n          header.writeDelimitedTo(out);\n          rv.write(out);\n        } else { // Have to serialize to buffer to get len\n          final DataOutputBuffer buf = new DataOutputBuffer();\n          rv.write(buf);\n          byte[] data = buf.getData();\n          fullLength += buf.getLength();\n          out.writeInt(fullLength);\n          header.writeDelimitedTo(out);\n          out.write(data, 0, buf.getLength());\n        }\n      } catch (Throwable t) {\n        LOG.warn(\"Error serializing call response for call \" + call, t);\n        // Call back to same function - this is OK since the\n        // buffer is reset at the top, and since status is changed\n        // to ERROR it won't infinite loop.\n        setupResponse(responseBuf, call, RpcStatusProto.ERROR,\n            RpcErrorCodeProto.ERROR_SERIALIZING_RESPONSE,\n            null, t.getClass().getName(),\n            StringUtils.stringifyException(t));\n        return;\n      }\n    } else { // Rpc Failure\n      headerBuilder.setExceptionClassName(errorClass);\n      headerBuilder.setErrorMsg(error);\n      headerBuilder.setErrorDetail(erCode);\n      RpcResponseHeaderProto header = headerBuilder.build();\n      int headerLen = header.getSerializedSize();\n      final int fullLength  = \n          CodedOutputStream.computeRawVarint32Size(headerLen) + headerLen;\n      out.writeInt(fullLength);\n      header.writeDelimitedTo(out);\n    }\n    if (call.connection.useWrap) {\n      wrapWithSasl(responseBuf, call);\n    }\n    call.setResponse(ByteBuffer.wrap(responseBuf.toByteArray()));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.remove": "    private boolean remove(Connection connection) {\n      boolean removed = connections.remove(connection);\n      if (removed) {\n        count.getAndDecrement();\n      }\n      return removed;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeAll": "    void closeAll() {\n      // use a copy of the connections to be absolutely sure the concurrent\n      // iterator doesn't miss a connection\n      for (Connection connection : toArray()) {\n        close(connection);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.isTerse": "    boolean isTerse(Class<?> t) {\n      return terseExceptions.contains(t.toString());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    boolean close(Connection connection) {\n      boolean exists = remove(connection);\n      if (exists) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(Thread.currentThread().getName() +\n              \": disconnecting client \" + connection +\n              \". Number of active connections: \"+ size());\n        }\n        // only close if actually removed to avoid double-closing due\n        // to possible races\n        connection.close();\n      }\n      return exists;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(RPC.RpcKind rpcKind, String protocol,\n      Writable param, long receiveTime) throws Exception;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param protocolName - the protocol\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  private void authorize(UserGroupInformation user, String protocolName,\n      InetAddress addr) throws AuthorizationException {\n    if (authorize) {\n      if (protocolName == null) {\n        throw new AuthorizationException(\"Null protocol not authorized\");\n      }\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(protocolName, getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         protocolName);\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isValid() && key.isWritable()) {\n                  doAsyncWrite(key);\n              }\n            } catch (IOException e) {\n              LOG.info(Thread.currentThread().getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = Time.now();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<Call> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<Call>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              Call call = (Call)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n          \n          for(Call call : calls) {\n            doPurge(call, now);\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getRpcErrorCodeProto": "    public RpcErrorCodeProto getRpcErrorCodeProto() {\n      return errCode;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.size": "    int size() {\n      return count.get();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeIdle": "    synchronized void closeIdle(boolean scanAll) {\n      long minLastContact = Time.now() - maxIdleTime;\n      // concurrent iterator might miss new connections added\n      // during the iteration, but that's ok because they won't\n      // be idle yet anyway and will be caught on next scan\n      int closed = 0;\n      for (Connection connection : connections) {\n        // stop if connections dropped below threshold unless scanning all\n        if (!scanAll && size() < idleScanThreshold) {\n          break;\n        }\n        // stop if not scanning all and max connections are closed\n        if (connection.isIdle() &&\n            connection.getLastContact() < minLastContact &&\n            close(connection) &&\n            !scanAll && (++closed == maxIdleToClose)) {\n          break;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.startIdleScan": "    void startIdleScan() {\n      scheduleIdleScanTask();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRespond": "    void doRespond(Call call) throws IOException {\n      synchronized (call.connection.responseQueue) {\n        call.connection.responseQueue.addLast(call);\n        if (call.connection.responseQueue.size() == 1) {\n          processResponse(call.connection.responseQueue, true);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.stopIdleScan": "    void stopIdleScan() {\n      idleScanTimer.cancel();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws InterruptedException, IOException,  OutOfMemoryError {\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        channel.socket().setKeepAlive(true);\n        \n        Reader reader = getReader();\n        Connection c = connectionManager.register(channel);\n        // If the connectionManager can't take it, close the connection.\n        if (c == null) {\n          if (channel.isOpen()) {\n            IOUtils.cleanup(null, channel);\n          }\n          continue;\n        }\n        key.attach(c);  // so closeCurrentConnection can get the object\n        reader.addConnection(c);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.scheduleIdleScanTask": "    private void scheduleIdleScanTask() {\n      if (!running) {\n        return;\n      }\n      TimerTask idleScanTask = new TimerTask(){\n        @Override\n        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }\n      };\n      idleScanTimer.schedule(idleScanTask, idleScanInterval);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous.isComplete": "  public boolean isComplete() {\n    return getBlockUCState().equals(BlockUCState.COMPLETE);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous.getBlockUCState": "  public BlockUCState getBlockUCState() {\n    return BlockUCState.COMPLETE;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous.equals": "  public boolean equals(Object obj) {\n    // Sufficient to rely on super's implementation\n    return (this == obj) || super.equals(obj);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.invalidateBlocks.contains": "  synchronized boolean contains(final DatanodeInfo dn, final Block block) {\n    final LightWeightHashSet<Block> s = node2blocks.get(dn);\n    if (s == null) {\n      return false; // no invalidate blocks for this storage ID\n    }\n    Block blockInSet = s.getElement(block);\n    return blockInSet != null &&\n        block.getGenerationStamp() == blockInSet.getGenerationStamp();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.getDatanodeDescriptor": "  public DatanodeDescriptor getDatanodeDescriptor() {\n    return dn;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous.findStorageInfo": "  int findStorageInfo(DatanodeStorageInfo storageInfo) {\n    int len = getCapacity();\n    for(int idx = 0; idx < len; idx++) {\n      DatanodeStorageInfo cur = getStorageInfo(idx);\n      if (cur == storageInfo) {\n        return idx;\n      }\n      if (cur == null) {\n        break;\n      }\n    }\n    return -1;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous.getStorageInfo": "  DatanodeStorageInfo getStorageInfo(int index) {\n    assert this.triplets != null : \"BlockInfo is not initialized\";\n    assert index >= 0 && index*3 < triplets.length : \"Index is out of bound\";\n    return (DatanodeStorageInfo)triplets[index*3];\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous.getCapacity": "  public int getCapacity() {\n    assert this.triplets != null : \"BlockInfo is not initialized\";\n    assert triplets.length % 3 == 0 : \"Malformed BlockInfo\";\n    return triplets.length / 3;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous.getNext": "  public LightWeightGSet.LinkedElement getNext() {\n    return nextLinkedElement;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.moveBlockToHead": "  int moveBlockToHead(BlockInfoContiguous b, int curIndex, int headIndex) {\n    blockList = b.moveBlockToHead(blockList, this, curIndex, headIndex);\n    return curIndex;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.updateStorage": "  DatanodeStorageInfo updateStorage(DatanodeStorage s) {\n    synchronized (storageMap) {\n      DatanodeStorageInfo storage = storageMap.get(s.getStorageID());\n      if (storage == null) {\n        LOG.info(\"Adding new storage ID \" + s.getStorageID() +\n                 \" for DN \" + getXferAddr());\n        storage = new DatanodeStorageInfo(this, s);\n        storageMap.put(s.getStorageID(), storage);\n      } else if (storage.getState() != s.getState() ||\n                 storage.getStorageType() != s.getStorageType()) {\n        // For backwards compatibility, make sure that the type and\n        // state are updated. Some reports from older datanodes do\n        // not include these fields so we may have assumed defaults.\n        storage.updateFromStorage(s);\n        storageMap.put(storage.getStorageID(), storage);\n      }\n      return storage;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.removeZombieStorages": "  List<DatanodeStorageInfo> removeZombieStorages() {\n    List<DatanodeStorageInfo> zombies = null;\n    synchronized (storageMap) {\n      Iterator<Map.Entry<String, DatanodeStorageInfo>> iter =\n          storageMap.entrySet().iterator();\n      while (iter.hasNext()) {\n        Map.Entry<String, DatanodeStorageInfo> entry = iter.next();\n        DatanodeStorageInfo storageInfo = entry.getValue();\n        if (storageInfo.getLastBlockReportId() != curBlockReportId) {\n          LOG.info(storageInfo.getStorageID() + \" had lastBlockReportId 0x\" +\n              Long.toHexString(storageInfo.getLastBlockReportId()) +\n              \", but curBlockReportId = 0x\" +\n              Long.toHexString(curBlockReportId));\n          iter.remove();\n          if (zombies == null) {\n            zombies = new LinkedList<DatanodeStorageInfo>();\n          }\n          zombies.add(storageInfo);\n        }\n        storageInfo.setLastBlockReportId(0);\n      }\n    }\n    return zombies == null ? EMPTY_STORAGE_INFO_LIST : zombies;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.hasNext": "    public boolean hasNext() {\n      update();\n      return !iterators.isEmpty() && iterators.get(index).hasNext();\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.remove": "    public void remove() {\n      throw new UnsupportedOperationException(\"Remove unsupported.\");\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.next": "    public BlockInfoContiguous next() {\n      update();\n      return iterators.get(index).next();\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.getBlockReportCount": "  int getBlockReportCount() {\n    return blockReportCount;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.hasStaleStorages": "  boolean hasStaleStorages() {\n    synchronized (storageMap) {\n      for (DatanodeStorageInfo storage : storageMap.values()) {\n        if (storage.areBlockContentsStale()) {\n          return true;\n        }\n      }\n      return false;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.getStorageInfo": "  public DatanodeStorageInfo getStorageInfo(String storageID) {\n    synchronized (storageMap) {\n      return storageMap.get(storageID);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.receivedBlockReport": "  void receivedBlockReport() {\n    if (heartbeatedSinceFailover) {\n      blockContentsStale = false;\n    }\n    blockReportCount++;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.updateBlockReportContext": "  public int updateBlockReportContext(BlockReportContext context) {\n    if (curBlockReportId != context.getReportId()) {\n      curBlockReportId = context.getReportId();\n      curBlockReportRpcsSeen = new BitSet(context.getTotalRpcs());\n    }\n    curBlockReportRpcsSeen.set(context.getCurRpc());\n    return curBlockReportRpcsSeen.cardinality();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.set": "    synchronized void set(int underRep,\n        int onlyRep, int underConstruction) {\n      if (isDecommissionInProgress() == false) {\n        return;\n      }\n      underReplicatedBlocks = underRep;\n      decommissionOnlyReplicas = onlyRep;\n      underReplicatedInOpenFiles = underConstruction;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.numBlocks": "  int numBlocks() {\n    return numBlocks;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.setLastBlockReportId": "  void setLastBlockReportId(long lastBlockReportId) {\n    this.lastBlockReportId = lastBlockReportId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.datanodeManager.getDatanode": "  public DatanodeDescriptor getDatanode(DatanodeID nodeID\n      ) throws UnregisteredNodeException {\n    final DatanodeDescriptor node = getDatanode(nodeID.getDatanodeUuid());\n    if (node == null) \n      return null;\n    if (!node.getXferAddr().equals(nodeID.getXferAddr())) {\n      final UnregisteredNodeException e = new UnregisteredNodeException(\n          nodeID, node);\n      NameNode.stateChangeLog.error(\"BLOCK* NameSystem.getDatanode: \"\n                                    + e.getLocalizedMessage());\n      throw e;\n    }\n    return node;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.clearBlockReportContext": "  public void clearBlockReportContext() {\n    curBlockReportId = 0;\n    curBlockReportRpcsSeen = null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convert": "  public static BlockReportContextProto convert(BlockReportContext context) {\n    return BlockReportContextProto.newBuilder().\n        setTotalRpcs(context.getTotalRpcs()).\n        setCurRpc(context.getCurRpc()).\n        setId(context.getReportId()).\n        build();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertAclEntry": "  public static List<AclEntry> convertAclEntry(List<AclEntryProto> aclSpec) {\n    ArrayList<AclEntry> r = Lists.newArrayListWithCapacity(aclSpec.size());\n    for (AclEntryProto e : aclSpec) {\n      AclEntry.Builder builder = new AclEntry.Builder();\n      builder.setType(convert(e.getType()));\n      builder.setScope(convert(e.getScope()));\n      builder.setPermission(convert(e.getPermissions()));\n      if (e.hasName()) {\n        builder.setName(e.getName());\n      }\n      r.add(builder.build());\n    }\n    return r;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertLocatedBlock": "  public static List<LocatedBlock> convertLocatedBlock(\n      List<LocatedBlockProto> lb) {\n    if (lb == null) return null;\n    final int len = lb.size();\n    List<LocatedBlock> result = \n        new ArrayList<LocatedBlock>(len);\n    for (int i = 0; i < len; ++i) {\n      result.add(PBHelper.convert(lb.get(i)));\n    }\n    return result;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertStorageType": "  public static StorageType convertStorageType(StorageTypeProto type) {\n    switch(type) {\n      case DISK:\n        return StorageType.DISK;\n      case SSD:\n        return StorageType.SSD;\n      case ARCHIVE:\n        return StorageType.ARCHIVE;\n      case RAM_DISK:\n        return StorageType.RAM_DISK;\n      default:\n        throw new IllegalStateException(\n            \"BUG: StorageTypeProto not found, type=\" + type);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertXAttrs": "  public static List<XAttr> convertXAttrs(List<XAttrProto> xAttrSpec) {\n    ArrayList<XAttr> xAttrs = Lists.newArrayListWithCapacity(xAttrSpec.size());\n    for (XAttrProto a : xAttrSpec) {\n      XAttr.Builder builder = new XAttr.Builder();\n      builder.setNameSpace(convert(a.getNamespace()));\n      if (a.hasName()) {\n        builder.setName(a.getName());\n      }\n      if (a.hasValue()) {\n        builder.setValue(a.getValue().toByteArray());\n      }\n      xAttrs.add(builder.build());\n    }\n    return xAttrs;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertLocatedBlock2": "  public static List<LocatedBlockProto> convertLocatedBlock2(List<LocatedBlock> lb) {\n    if (lb == null) return null;\n    final int len = lb.size();\n    List<LocatedBlockProto> result = new ArrayList<LocatedBlockProto>(len);\n    for (int i = 0; i < len; ++i) {\n      result.add(PBHelper.convert(lb.get(i)));\n    }\n    return result;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.createTypeConvert": "  private static InotifyProtos.INodeType createTypeConvert(Event.CreateEvent.INodeType\n      type) {\n    switch (type) {\n    case DIRECTORY:\n      return InotifyProtos.INodeType.I_TYPE_DIRECTORY;\n    case FILE:\n      return InotifyProtos.INodeType.I_TYPE_FILE;\n    case SYMLINK:\n      return InotifyProtos.INodeType.I_TYPE_SYMLINK;\n    default:\n      return null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertAclEntryProto": "  public static List<AclEntryProto> convertAclEntryProto(\n      List<AclEntry> aclSpec) {\n    ArrayList<AclEntryProto> r = Lists.newArrayListWithCapacity(aclSpec.size());\n    for (AclEntry e : aclSpec) {\n      AclEntryProto.Builder builder = AclEntryProto.newBuilder();\n      builder.setType(convert(e.getType()));\n      builder.setScope(convert(e.getScope()));\n      builder.setPermissions(convert(e.getPermission()));\n      if (e.getName() != null) {\n        builder.setName(e.getName());\n      }\n      r.add(builder.build());\n    }\n    return r;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertState": "  private static State convertState(StorageState state) {\n    switch(state) {\n    case READ_ONLY_SHARED:\n      return DatanodeStorage.State.READ_ONLY_SHARED;\n    case NORMAL:\n    default:\n      return DatanodeStorage.State.NORMAL;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertRollingUpgradeStatus": "  public static RollingUpgradeStatusProto convertRollingUpgradeStatus(\n      RollingUpgradeStatus status) {\n    return RollingUpgradeStatusProto.newBuilder()\n        .setBlockPoolId(status.getBlockPoolId())\n        .setFinalized(status.isFinalized())\n        .build();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertStorageTypes": "  public static StorageType[] convertStorageTypes(\n      List<StorageTypeProto> storageTypesList, int expectedSize) {\n    final StorageType[] storageTypes = new StorageType[expectedSize];\n    if (storageTypesList.size() != expectedSize) { // missing storage types\n      Preconditions.checkState(storageTypesList.isEmpty());\n      Arrays.fill(storageTypes, StorageType.DEFAULT);\n    } else {\n      for (int i = 0; i < storageTypes.length; ++i) {\n        storageTypes[i] = convertStorageType(storageTypesList.get(i));\n      }\n    }\n    return storageTypes;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.castEnum": "  private static <T extends Enum<T>, U extends Enum<U>> U castEnum(T from, U[] to) {\n    return to[from.ordinal()];\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.getByteString": "  public static ByteString getByteString(byte[] bytes) {\n    return ByteString.copyFrom(bytes);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertBlockKeys": "  public static BlockKey[] convertBlockKeys(List<BlockKeyProto> list) {\n    BlockKey[] ret = new BlockKey[list.size()];\n    int i = 0;\n    for (BlockKeyProto k : list) {\n      ret[i++] = convert(k);\n    }\n    return ret;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.metadataUpdateTypeConvert": "  private static Event.MetadataUpdateEvent.MetadataType metadataUpdateTypeConvert(\n      InotifyProtos.MetadataUpdateType type) {\n    switch (type) {\n    case META_TYPE_TIMES:\n      return Event.MetadataUpdateEvent.MetadataType.TIMES;\n    case META_TYPE_REPLICATION:\n      return Event.MetadataUpdateEvent.MetadataType.REPLICATION;\n    case META_TYPE_OWNER:\n      return Event.MetadataUpdateEvent.MetadataType.OWNER;\n    case META_TYPE_PERMS:\n      return Event.MetadataUpdateEvent.MetadataType.PERMS;\n    case META_TYPE_ACLS:\n      return Event.MetadataUpdateEvent.MetadataType.ACLS;\n    case META_TYPE_XATTRS:\n      return Event.MetadataUpdateEvent.MetadataType.XATTRS;\n    default:\n      return null;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcServerException.getRpcStatusProto": "  public RpcStatusProto getRpcStatusProto() {\n    return RpcStatusProto.ERROR;\n  }"
        },
        "bug_report": {
            "Title": "Add check for null BlockCollection pointers in BlockInfoContiguous structures",
            "Description": "The following copy constructor can throw NullPointerException if {{bc}} is null.\n{code}\n  protected BlockInfoContiguous(BlockInfoContiguous from) {\n    this(from, from.bc.getBlockReplication());\n    this.bc = from.bc;\n  }\n{code}\n\nWe have observed that some DataNodes keeps failing doing block reports with NameNode. The stacktrace is as follows. Though we are not using the latest version, the problem still exists.\n{quote}\n2015-03-08 19:28:13,442 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: RemoteException in offerService\norg.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo.(BlockInfo.java:80)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt.(BlockManager.java:1696)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.checkReplicaCorrupt(BlockManager.java:2185)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReportedBlock(BlockManager.java:2047)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.reportDiff(BlockManager.java:1950)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:1823)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:1750)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.blockReport(NameNodeRpcServer.java:1069)\nat org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.blockReport(DatanodeProtocolServerSideTranslatorPB.java:152)\nat org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:26382)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1623)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)\n{quote}"
        }
    },
    {
        "filename": "HDFS-10512.json",
        "creation_time": "2016-06-09T14:34:04.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:1018)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler.handle(VolumeScanner.java:287)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.scanBlock(VolumeScanner.java:443)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:547)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:621)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks": "  public void reportBadBlocks(ExtendedBlock block) throws IOException{\n    BPOfferService bpos = getBPOSForBlock(block);\n    FsVolumeSpi volume = getFSDataset().getVolume(block);\n    bpos.reportBadBlocks(\n        block, volume.getStorageID(), volume.getStorageType());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getFSDataset": "  public FsDatasetSpi<?> getFSDataset() {\n    return data;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getBPOSForBlock": "  private BPOfferService getBPOSForBlock(ExtendedBlock block)\n      throws IOException {\n    Preconditions.checkNotNull(block);\n    BPOfferService bpos = blockPoolManager.get(block.getBlockPoolId());\n    if (bpos == null) {\n      throw new IOException(\"cannot locate OfferService thread for bp=\"+\n          block.getBlockPoolId());\n    }\n    return bpos;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.VolumeScanner.handle": "    public void handle(ExtendedBlock block, IOException e) {\n      FsVolumeSpi volume = scanner.volume;\n      if (e == null) {\n        LOG.trace(\"Successfully scanned {} on {}\", block, volume.getBasePath());\n        return;\n      }\n      // If the block does not exist anymore, then it's not an error.\n      if (!volume.getDataset().contains(block)) {\n        LOG.debug(\"Volume {}: block {} is no longer in the dataset.\",\n            volume.getBasePath(), block);\n        return;\n      }\n      // If the block exists, the exception may due to a race with write:\n      // The BlockSender got an old block path in rbw. BlockReceiver removed\n      // the rbw block from rbw to finalized but BlockSender tried to open the\n      // file before BlockReceiver updated the VolumeMap. The state of the\n      // block can be changed again now, so ignore this error here. If there\n      // is a block really deleted by mistake, DirectoryScan should catch it.\n      if (e instanceof FileNotFoundException ) {\n        LOG.info(\"Volume {}: verification failed for {} because of \" +\n                \"FileNotFoundException.  This may be due to a race with write.\",\n            volume.getBasePath(), block);\n        return;\n      }\n      LOG.warn(\"Reporting bad {} on {}\", block, volume.getBasePath());\n      try {\n        scanner.datanode.reportBadBlocks(block);\n      } catch (IOException ie) {\n        // This is bad, but not bad enough to shut down the scanner.\n        LOG.warn(\"Cannot report bad \" + block.getBlockId(), e);\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.VolumeScanner.scanBlock": "  private long scanBlock(ExtendedBlock cblock, long bytesPerSec) {\n    // 'cblock' has a valid blockId and block pool id, but we don't yet know the\n    // genstamp the block is supposed to have.  Ask the FsDatasetImpl for this\n    // information.\n    ExtendedBlock block = null;\n    try {\n      Block b = volume.getDataset().getStoredBlock(\n          cblock.getBlockPoolId(), cblock.getBlockId());\n      if (b == null) {\n        LOG.info(\"Replica {} was not found in the VolumeMap for volume {}\",\n            cblock, volume.getBasePath());\n      } else {\n        block = new ExtendedBlock(cblock.getBlockPoolId(), b);\n      }\n    } catch (FileNotFoundException e) {\n      LOG.info(\"FileNotFoundException while finding block {} on volume {}\",\n          cblock, volume.getBasePath());\n    } catch (IOException e) {\n      LOG.warn(\"I/O error while finding block {} on volume {}\",\n            cblock, volume.getBasePath());\n    }\n    if (block == null) {\n      return -1; // block not found.\n    }\n    BlockSender blockSender = null;\n    try {\n      blockSender = new BlockSender(block, 0, -1,\n          false, true, true, datanode, null,\n          CachingStrategy.newDropBehind());\n      throttler.setBandwidth(bytesPerSec);\n      long bytesRead = blockSender.sendBlock(nullStream, null, throttler);\n      resultHandler.handle(block, null);\n      return bytesRead;\n    } catch (IOException e) {\n      resultHandler.handle(block, e);\n    } finally {\n      IOUtils.cleanup(null, blockSender);\n    }\n    return -1;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop": "  private long runLoop(ExtendedBlock suspectBlock) {\n    long bytesScanned = -1;\n    boolean scanError = false;\n    ExtendedBlock block = null;\n    try {\n      long monotonicMs = Time.monotonicNow();\n      expireOldScannedBytesRecords(monotonicMs);\n\n      if (!calculateShouldScan(volume.getStorageID(), conf.targetBytesPerSec,\n          scannedBytesSum, startMinute, curMinute)) {\n        // If neededBytesPerSec is too low, then wait few seconds for some old\n        // scannedBytes records to expire.\n        return 30000L;\n      }\n\n      // Find a usable block pool to scan.\n      if (suspectBlock != null) {\n        block = suspectBlock;\n      } else {\n        if ((curBlockIter == null) || curBlockIter.atEnd()) {\n          long timeout = findNextUsableBlockIter();\n          if (timeout > 0) {\n            LOG.trace(\"{}: no block pools are ready to scan yet.  Waiting \" +\n                \"{} ms.\", this, timeout);\n            synchronized (stats) {\n              stats.nextBlockPoolScanStartMs = Time.monotonicNow() + timeout;\n            }\n            return timeout;\n          }\n          synchronized (stats) {\n            stats.scansSinceRestart++;\n            stats.blocksScannedInCurrentPeriod = 0;\n            stats.nextBlockPoolScanStartMs = -1;\n          }\n          return 0L;\n        }\n        try {\n          block = curBlockIter.nextBlock();\n        } catch (IOException e) {\n          // There was an error listing the next block in the volume.  This is a\n          // serious issue.\n          LOG.warn(\"{}: nextBlock error on {}\", this, curBlockIter);\n          // On the next loop iteration, curBlockIter#eof will be set to true, and\n          // we will pick a different block iterator.\n          return 0L;\n        }\n        if (block == null) {\n          // The BlockIterator is at EOF.\n          LOG.info(\"{}: finished scanning block pool {}\",\n              this, curBlockIter.getBlockPoolId());\n          saveBlockIterator(curBlockIter);\n          return 0;\n        }\n      }\n      if (curBlockIter != null) {\n        long saveDelta = monotonicMs - curBlockIter.getLastSavedMs();\n        if (saveDelta >= conf.cursorSaveMs) {\n          LOG.debug(\"{}: saving block iterator {} after {} ms.\",\n              this, curBlockIter, saveDelta);\n          saveBlockIterator(curBlockIter);\n        }\n      }\n      bytesScanned = scanBlock(block, conf.targetBytesPerSec);\n      if (bytesScanned >= 0) {\n        scannedBytesSum += bytesScanned;\n        scannedBytes[(int)(curMinute % MINUTES_PER_HOUR)] += bytesScanned;\n      } else {\n        scanError = true;\n      }\n      return 0L;\n    } finally {\n      synchronized (stats) {\n        stats.bytesScannedInPastHour = scannedBytesSum;\n        if (bytesScanned > 0) {\n          stats.blocksScannedInCurrentPeriod++;\n          stats.blocksScannedSinceRestart++;\n        }\n        if (scanError) {\n          stats.scanErrorsSinceRestart++;\n        }\n        if (block != null) {\n          stats.lastBlockScanned = block;\n        }\n        if (curBlockIter == null) {\n          stats.eof = true;\n          stats.blockPoolPeriodEndsMs = -1;\n        } else {\n          stats.eof = curBlockIter.atEnd();\n          stats.blockPoolPeriodEndsMs =\n              curBlockIter.getIterStartMs() + conf.scanPeriodMs;\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.VolumeScanner.saveBlockIterator": "  private void saveBlockIterator(BlockIterator iter) {\n    try {\n      iter.save();\n    } catch (IOException e) {\n      LOG.warn(\"{}: error saving {}.\", this, iter, e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.VolumeScanner.findNextUsableBlockIter": "  private synchronized long findNextUsableBlockIter() {\n    int numBlockIters = blockIters.size();\n    if (numBlockIters == 0) {\n      LOG.debug(\"{}: no block pools are registered.\", this);\n      return Long.MAX_VALUE;\n    }\n    int curIdx;\n    if (curBlockIter == null) {\n      curIdx = 0;\n    } else {\n      curIdx = blockIters.indexOf(curBlockIter);\n      Preconditions.checkState(curIdx >= 0);\n    }\n    // Note that this has to be wall-clock time, not monotonic time.  This is\n    // because the time saved in the cursor file is a wall-clock time.  We do\n    // not want to save a monotonic time in the cursor file, because it resets\n    // every time the machine reboots (on most platforms).\n    long nowMs = Time.now();\n    long minTimeoutMs = Long.MAX_VALUE;\n    for (int i = 0; i < numBlockIters; i++) {\n      int idx = (curIdx + i + 1) % numBlockIters;\n      BlockIterator iter = blockIters.get(idx);\n      if (!iter.atEnd()) {\n        LOG.info(\"Now scanning bpid {} on volume {}\",\n            iter.getBlockPoolId(), volume.getBasePath());\n        curBlockIter = iter;\n        return 0L;\n      }\n      long iterStartMs = iter.getIterStartMs();\n      long waitMs = (iterStartMs + conf.scanPeriodMs) - nowMs;\n      if (waitMs <= 0) {\n        iter.rewind();\n        LOG.info(\"Now rescanning bpid {} on volume {}, after more than \" +\n            \"{} hour(s)\", iter.getBlockPoolId(), volume.getBasePath(),\n            TimeUnit.HOURS.convert(conf.scanPeriodMs, TimeUnit.MILLISECONDS));\n        curBlockIter = iter;\n        return 0L;\n      }\n      minTimeoutMs = Math.min(minTimeoutMs, waitMs);\n    }\n    LOG.info(\"{}: no suitable block pools found to scan.  Waiting {} ms.\",\n        this, minTimeoutMs);\n    return minTimeoutMs;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.VolumeScanner.calculateShouldScan": "  static boolean calculateShouldScan(String storageId, long targetBytesPerSec,\n                   long scannedBytesSum, long startMinute, long curMinute) {\n    long runMinutes = curMinute - startMinute;\n    long effectiveBytesPerSec;\n    if (runMinutes <= 0) {\n      // avoid division by zero\n      effectiveBytesPerSec = scannedBytesSum;\n    } else {\n      if (runMinutes > MINUTES_PER_HOUR) {\n        // we only keep an hour's worth of rate information\n        runMinutes = MINUTES_PER_HOUR;\n      }\n      effectiveBytesPerSec = scannedBytesSum /\n          (SECONDS_PER_MINUTE * runMinutes);\n    }\n\n    boolean shouldScan = effectiveBytesPerSec <= targetBytesPerSec;\n    LOG.trace(\"{}: calculateShouldScan: effectiveBytesPerSec = {}, and \" +\n        \"targetBytesPerSec = {}.  startMinute = {}, curMinute = {}, \" +\n        \"shouldScan = {}\",\n        storageId, effectiveBytesPerSec, targetBytesPerSec,\n        startMinute, curMinute, shouldScan);\n    return shouldScan;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.VolumeScanner.expireOldScannedBytesRecords": "  private void expireOldScannedBytesRecords(long monotonicMs) {\n    long newMinute =\n        TimeUnit.MINUTES.convert(monotonicMs, TimeUnit.MILLISECONDS);\n    if (curMinute == newMinute) {\n      return;\n    }\n    // If a minute or more has gone past since we last updated the scannedBytes\n    // array, zero out the slots corresponding to those minutes.\n    for (long m = curMinute + 1; m <= newMinute; m++) {\n      int slotIdx = (int)(m % MINUTES_PER_HOUR);\n      LOG.trace(\"{}: updateScannedBytes is zeroing out slotIdx {}.  \" +\n              \"curMinute = {}; newMinute = {}\", this, slotIdx,\n              curMinute, newMinute);\n      scannedBytesSum -= scannedBytes[slotIdx];\n      scannedBytes[slotIdx] = 0;\n    }\n    curMinute = newMinute;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run": "  public void run() {\n    // Record the minute on which the scanner started.\n    this.startMinute =\n        TimeUnit.MINUTES.convert(Time.monotonicNow(), TimeUnit.MILLISECONDS);\n    this.curMinute = startMinute;\n    try {\n      LOG.trace(\"{}: thread starting.\", this);\n      resultHandler.setup(this);\n      try {\n        long timeout = 0;\n        while (true) {\n          ExtendedBlock suspectBlock = null;\n          // Take the lock to check if we should stop, and access the\n          // suspect block list.\n          synchronized (this) {\n            if (stopping) {\n              break;\n            }\n            if (timeout > 0) {\n              wait(timeout);\n              if (stopping) {\n                break;\n              }\n            }\n            suspectBlock = popNextSuspectBlock();\n          }\n          timeout = runLoop(suspectBlock);\n        }\n      } catch (InterruptedException e) {\n        // We are exiting because of an InterruptedException,\n        // probably sent by VolumeScanner#shutdown.\n        LOG.trace(\"{} exiting because of InterruptedException.\", this);\n      } catch (Throwable e) {\n        LOG.error(\"{} exiting because of exception \", this, e);\n      }\n      LOG.info(\"{} exiting.\", this);\n      // Save the current position of all block iterators and close them.\n      for (BlockIterator iter : blockIters) {\n        saveBlockIterator(iter);\n        IOUtils.cleanup(null, iter);\n      }\n    } finally {\n      // When the VolumeScanner exits, release the reference we were holding\n      // on the volume.  This will allow the volume to be removed later.\n      IOUtils.cleanup(null, ref);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.VolumeScanner.popNextSuspectBlock": "  private synchronized ExtendedBlock popNextSuspectBlock() {\n    Iterator<ExtendedBlock> iter = suspectBlocks.iterator();\n    if (!iter.hasNext()) {\n      return null;\n    }\n    ExtendedBlock block = iter.next();\n    iter.remove();\n    return block;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.VolumeScanner.setup": "    public void setup(VolumeScanner scanner) {\n      LOG.trace(\"Starting VolumeScanner {}\",\n          scanner.volume.getBasePath());\n      this.scanner = scanner;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.StorageLocation.getStorageType": "  public StorageType getStorageType() {\n    return this.storageType;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    final TraceScope scope = datanode.getTracer().\n        newScope(\"sendBlock_\" + block.getBlockId());\n    try {\n      return doSendBlock(out, baseStream, throttler);\n    } finally {\n      scope.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock": "  private long doSendBlock(DataOutputStream out, OutputStream baseStream,\n        DataTransferThrottler throttler) throws IOException {\n    if (out == null) {\n      throw new IOException( \"out stream is null\" );\n    }\n    initialOffset = offset;\n    long totalRead = 0;\n    OutputStream streamForSendChunks = out;\n    \n    lastCacheDropOffset = initialOffset;\n\n    if (isLongRead() && blockInFd != null) {\n      // Advise that this file descriptor will be accessed sequentially.\n      NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n          block.getBlockName(), blockInFd, 0, 0, POSIX_FADV_SEQUENTIAL);\n    }\n    \n    // Trigger readahead of beginning of file if configured.\n    manageOsCache();\n\n    final long startTime = ClientTraceLog.isDebugEnabled() ? System.nanoTime() : 0;\n    try {\n      int maxChunksPerPacket;\n      int pktBufSize = PacketHeader.PKT_MAX_HEADER_LEN;\n      boolean transferTo = transferToAllowed && !verifyChecksum\n          && baseStream instanceof SocketOutputStream\n          && blockIn instanceof FileInputStream;\n      if (transferTo) {\n        FileChannel fileChannel = ((FileInputStream)blockIn).getChannel();\n        blockInPosition = fileChannel.position();\n        streamForSendChunks = baseStream;\n        maxChunksPerPacket = numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n        \n        // Smaller packet size to only hold checksum when doing transferTo\n        pktBufSize += checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket = Math.max(1,\n            numberOfChunks(IO_FILE_BUFFER_SIZE));\n        // Packet size includes both checksum and data\n        pktBufSize += (chunkSize + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf = ByteBuffer.allocate(pktBufSize);\n\n      while (endOffset > offset && !Thread.currentThread().isInterrupted()) {\n        manageOsCache();\n        long len = sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n            transferTo, throttler);\n        offset += len;\n        totalRead += len + (numberOfChunks(len) * checksumSize);\n        seqno++;\n      }\n      // If this thread was interrupted, then it did not send the full block.\n      if (!Thread.currentThread().isInterrupted()) {\n        try {\n          // send an empty packet to mark the end of the block\n          sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n              throttler);\n          out.flush();\n        } catch (IOException e) { //socket error\n          throw ioeToSocketException(e);\n        }\n\n        sentEntireByteRange = true;\n      }\n    } finally {\n      if ((clientTraceFmt != null) && ClientTraceLog.isDebugEnabled()) {\n        final long endTime = System.nanoTime();\n        ClientTraceLog.debug(String.format(clientTraceFmt, totalRead,\n            initialOffset, endTime - startTime));\n      }\n      close();\n    }\n    return totalRead;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.close": "  public void close() throws IOException {\n    if (blockInFd != null &&\n        ((dropCacheBehindAllReads) ||\n         (dropCacheBehindLargeReads && isLongRead()))) {\n      try {\n        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n            block.getBlockName(), blockInFd, lastCacheDropOffset,\n            offset - lastCacheDropOffset, POSIX_FADV_DONTNEED);\n      } catch (Exception e) {\n        LOG.warn(\"Unable to drop cache on file close\", e);\n      }\n    }\n    if (curReadahead != null) {\n      curReadahead.cancel();\n    }\n    \n    IOException ioe = null;\n    if(checksumIn!=null) {\n      try {\n        checksumIn.close(); // close checksum file\n      } catch (IOException e) {\n        ioe = e;\n      }\n      checksumIn = null;\n    }   \n    if(blockIn!=null) {\n      try {\n        blockIn.close(); // close data file\n      } catch (IOException e) {\n        ioe = e;\n      }\n      blockIn = null;\n      blockInFd = null;\n    }\n    if (volumeRef != null) {\n      IOUtils.cleanup(null, volumeRef);\n      volumeRef = null;\n    }\n    // throw IOException if there is any\n    if(ioe!= null) {\n      throw ioe;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.server.datanode.CachingStrategy.newDropBehind": "  public static CachingStrategy newDropBehind() {\n    return new CachingStrategy(true, null);\n  }"
        },
        "bug_report": {
            "Title": "VolumeScanner may terminate due to NPE in DataNode.reportBadBlocks",
            "Description": "VolumeScanner may terminate due to unexpected NullPointerException thrown in {{DataNode.reportBadBlocks()}}. This is different from HDFS-8850/HDFS-9190\n\nI observed this bug in a production CDH 5.5.1 cluster and the same bug still persist in upstream trunk.\n\n{noformat}\n2016-04-07 20:30:53,830 WARN org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Reporting bad BP-1800173197-10.204.68.5-1444425156296:blk_1170134484_96468685 on /dfs/dn\n2016-04-07 20:30:53,831 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/dfs/dn, DS-89b72832-2a8c-48f3-8235-48e6c5eb5ab3) exiting because of exception\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:1018)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler.handle(VolumeScanner.java:287)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.scanBlock(VolumeScanner.java:443)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:547)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:621)\n2016-04-07 20:30:53,832 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/dfs/dn, DS-89b72832-2a8c-48f3-8235-48e6c5eb5ab3) exiting.\n{noformat}\n\nI think the NPE comes from the volume variable in the following code snippet. Somehow the volume scanner know the volume, but the datanode can not lookup the volume using the block.\n{code}\npublic void reportBadBlocks(ExtendedBlock block) throws IOException{\n    BPOfferService bpos = getBPOSForBlock(block);\n    FsVolumeSpi volume = getFSDataset().getVolume(block);\n    bpos.reportBadBlocks(\n        block, volume.getStorageID(), volume.getStorageType());\n  }\n{code}"
        }
    },
    {
        "filename": "HDFS-10729.json",
        "creation_time": "2016-08-08T02:36:05.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFileEncryptionInfo(FSDirectory.java:2914)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.createFileStatus(FSDirectory.java:2469)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:375)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:230)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:139)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:829)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:810)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:232)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:331)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:284)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:301)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:360)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1651)\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:410)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:297)\n\norg.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException: The directory item limit of [path] is exceeded: limit=1048576 items=1049332\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:2060)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addChild(FSDirectory.java:2112)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:2081)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1900)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedAddFile(FSDirectory.java:368)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:365)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:230)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:139)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:829)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:810)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:232)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$1.run(EditLogTailer.java:188)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$1.run(EditLogTailer.java:182)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n        at org.apache.hadoop.security.SecurityUtil.doAsUser(SecurityUtil.java:445)\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUser(SecurityUtil.java:426)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.catchupDuringFailover(EditLogTailer.java:182)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1205)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1762)\n        at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)\n        at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)\n        at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1635)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1351)\n        at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)\n        at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:4460)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp": "  private long applyEditLogOp(FSEditLogOp op, FSDirectory fsDir,\n      StartupOption startOpt, int logVersion, long lastInodeId) throws IOException {\n    long inodeId = HdfsConstants.GRANDFATHER_INODE_ID;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"replaying edit log: \" + op);\n    }\n    final boolean toAddRetryCache = fsNamesys.hasRetryCache() && op.hasRpcIds();\n\n    switch (op.opCode) {\n    case OP_ADD: {\n      AddCloseOp addCloseOp = (AddCloseOp)op;\n      final String path =\n          renameReservedPathsOnUpgrade(addCloseOp.path, logVersion);\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + path +\n            \" numblocks : \" + addCloseOp.blocks.length +\n            \" clientHolder \" + addCloseOp.clientName +\n            \" clientMachine \" + addCloseOp.clientMachine);\n      }\n      // There are 3 cases here:\n      // 1. OP_ADD to create a new file\n      // 2. OP_ADD to update file blocks\n      // 3. OP_ADD to open file for append (old append)\n\n      // See if the file already exists (persistBlocks call)\n      INodesInPath iip = fsDir.getINodesInPath(path, true);\n      INodeFile oldFile = INodeFile.valueOf(iip.getLastINode(), path, true);\n      if (oldFile != null && addCloseOp.overwrite) {\n        // This is OP_ADD with overwrite\n        FSDirDeleteOp.deleteForEditLog(fsDir, path, addCloseOp.mtime);\n        iip = INodesInPath.replace(iip, iip.length() - 1, null);\n        oldFile = null;\n      }\n      INodeFile newFile = oldFile;\n      if (oldFile == null) { // this is OP_ADD on a new file (case 1)\n        // versions > 0 support per file replication\n        // get name and replication\n        final short replication = fsNamesys.getBlockManager()\n            .adjustReplication(addCloseOp.replication);\n        assert addCloseOp.blocks.length == 0;\n\n        // add to the file tree\n        inodeId = getAndUpdateLastInodeId(addCloseOp.inodeId, logVersion, lastInodeId);\n        newFile = FSDirWriteFileOp.addFileForEditLog(fsDir, inodeId,\n            iip.getExistingINodes(), iip.getLastLocalName(),\n            addCloseOp.permissions, addCloseOp.aclEntries,\n            addCloseOp.xAttrs, replication, addCloseOp.mtime,\n            addCloseOp.atime, addCloseOp.blockSize, true,\n            addCloseOp.clientName, addCloseOp.clientMachine,\n            addCloseOp.storagePolicyId);\n        assert newFile != null;\n        iip = INodesInPath.replace(iip, iip.length() - 1, newFile);\n        fsNamesys.leaseManager.addLease(addCloseOp.clientName, newFile.getId());\n\n        // add the op into retry cache if necessary\n        if (toAddRetryCache) {\n          HdfsFileStatus stat = FSDirStatAndListingOp.createFileStatusForEditLog(\n              fsNamesys.dir, path, HdfsFileStatus.EMPTY_NAME,\n              HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n              false, iip);\n          fsNamesys.addCacheEntryWithPayload(addCloseOp.rpcClientId,\n              addCloseOp.rpcCallId, stat);\n        }\n      } else { // This is OP_ADD on an existing file (old append)\n        if (!oldFile.isUnderConstruction()) {\n          // This is case 3: a call to append() on an already-closed file.\n          if (FSNamesystem.LOG.isDebugEnabled()) {\n            FSNamesystem.LOG.debug(\"Reopening an already-closed file \" +\n                \"for append\");\n          }\n          LocatedBlock lb = FSDirAppendOp.prepareFileForAppend(fsNamesys, iip,\n              addCloseOp.clientName, addCloseOp.clientMachine, false, false,\n              false);\n          // add the op into retry cache if necessary\n          if (toAddRetryCache) {\n            HdfsFileStatus stat = FSDirStatAndListingOp.createFileStatusForEditLog(\n                fsNamesys.dir, path, HdfsFileStatus.EMPTY_NAME,\n                HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n                Snapshot.CURRENT_STATE_ID, false, iip);\n            fsNamesys.addCacheEntryWithPayload(addCloseOp.rpcClientId,\n                addCloseOp.rpcCallId, new LastBlockWithStatus(lb, stat));\n          }\n        }\n      }\n      // Fall-through for case 2.\n      // Regardless of whether it's a new file or an updated file,\n      // update the block list.\n      \n      // Update the salient file attributes.\n      newFile.setAccessTime(addCloseOp.atime, Snapshot.CURRENT_STATE_ID);\n      newFile.setModificationTime(addCloseOp.mtime, Snapshot.CURRENT_STATE_ID);\n      ErasureCodingPolicy ecPolicy = FSDirErasureCodingOp.getErasureCodingPolicy(\n          fsDir.getFSNamesystem(), iip);\n      updateBlocks(fsDir, addCloseOp, iip, newFile, ecPolicy);\n      break;\n    }\n    case OP_CLOSE: {\n      AddCloseOp addCloseOp = (AddCloseOp)op;\n      final String path =\n          renameReservedPathsOnUpgrade(addCloseOp.path, logVersion);\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + path +\n            \" numblocks : \" + addCloseOp.blocks.length +\n            \" clientHolder \" + addCloseOp.clientName +\n            \" clientMachine \" + addCloseOp.clientMachine);\n      }\n\n      final INodesInPath iip = fsDir.getINodesInPath(path, true);\n      final INodeFile file = INodeFile.valueOf(iip.getLastINode(), path);\n\n      // Update the salient file attributes.\n      file.setAccessTime(addCloseOp.atime, Snapshot.CURRENT_STATE_ID);\n      file.setModificationTime(addCloseOp.mtime, Snapshot.CURRENT_STATE_ID);\n      ErasureCodingPolicy ecPolicy = FSDirErasureCodingOp.getErasureCodingPolicy(\n          fsDir.getFSNamesystem(), iip);\n      updateBlocks(fsDir, addCloseOp, iip, file, ecPolicy);\n\n      // Now close the file\n      if (!file.isUnderConstruction() &&\n          logVersion <= LayoutVersion.BUGFIX_HDFS_2991_VERSION) {\n        // There was a bug (HDFS-2991) in hadoop < 0.23.1 where OP_CLOSE\n        // could show up twice in a row. But after that version, this\n        // should be fixed, so we should treat it as an error.\n        throw new IOException(\n            \"File is not under construction: \" + path);\n      }\n      // One might expect that you could use removeLease(holder, path) here,\n      // but OP_CLOSE doesn't serialize the holder. So, remove the inode.\n      if (file.isUnderConstruction()) {\n        fsNamesys.getLeaseManager().removeLease(file.getId());\n        file.toCompleteFile(file.getModificationTime(), 0,\n            fsNamesys.getBlockManager().getMinReplication());\n      }\n      break;\n    }\n    case OP_APPEND: {\n      AppendOp appendOp = (AppendOp) op;\n      final String path = renameReservedPathsOnUpgrade(appendOp.path,\n          logVersion);\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + path +\n            \" clientName \" + appendOp.clientName +\n            \" clientMachine \" + appendOp.clientMachine +\n            \" newBlock \" + appendOp.newBlock);\n      }\n      INodesInPath iip = fsDir.getINodesInPath4Write(path);\n      INodeFile file = INodeFile.valueOf(iip.getLastINode(), path);\n      if (!file.isUnderConstruction()) {\n        LocatedBlock lb = FSDirAppendOp.prepareFileForAppend(fsNamesys, iip,\n            appendOp.clientName, appendOp.clientMachine, appendOp.newBlock,\n            false, false);\n        // add the op into retry cache if necessary\n        if (toAddRetryCache) {\n          HdfsFileStatus stat = FSDirStatAndListingOp.createFileStatusForEditLog(\n              fsNamesys.dir, path, HdfsFileStatus.EMPTY_NAME,\n              HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n              Snapshot.CURRENT_STATE_ID, false, iip);\n          fsNamesys.addCacheEntryWithPayload(appendOp.rpcClientId,\n              appendOp.rpcCallId, new LastBlockWithStatus(lb, stat));\n        }\n      }\n      break;\n    }\n    case OP_UPDATE_BLOCKS: {\n      UpdateBlocksOp updateOp = (UpdateBlocksOp)op;\n      final String path =\n          renameReservedPathsOnUpgrade(updateOp.path, logVersion);\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + path +\n            \" numblocks : \" + updateOp.blocks.length);\n      }\n      INodesInPath iip = fsDir.getINodesInPath(path, true);\n      INodeFile oldFile = INodeFile.valueOf(iip.getLastINode(), path);\n      // Update in-memory data structures\n      ErasureCodingPolicy ecPolicy = FSDirErasureCodingOp.getErasureCodingPolicy(\n          fsDir.getFSNamesystem(), iip);\n      updateBlocks(fsDir, updateOp, iip, oldFile, ecPolicy);\n\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(updateOp.rpcClientId, updateOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_ADD_BLOCK: {\n      AddBlockOp addBlockOp = (AddBlockOp) op;\n      String path = renameReservedPathsOnUpgrade(addBlockOp.getPath(), logVersion);\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + path +\n            \" new block id : \" + addBlockOp.getLastBlock().getBlockId());\n      }\n      INodesInPath iip = fsDir.getINodesInPath(path, true);\n      INodeFile oldFile = INodeFile.valueOf(iip.getLastINode(), path);\n      // add the new block to the INodeFile\n      ErasureCodingPolicy ecPolicy = FSDirErasureCodingOp.getErasureCodingPolicy(\n          fsDir.getFSNamesystem(), iip);\n      addNewBlock(addBlockOp, oldFile, ecPolicy);\n      break;\n    }\n    case OP_SET_REPLICATION: {\n      SetReplicationOp setReplicationOp = (SetReplicationOp)op;\n      short replication = fsNamesys.getBlockManager().adjustReplication(\n          setReplicationOp.replication);\n      FSDirAttrOp.unprotectedSetReplication(fsDir, renameReservedPathsOnUpgrade(\n          setReplicationOp.path, logVersion), replication);\n      break;\n    }\n    case OP_CONCAT_DELETE: {\n      ConcatDeleteOp concatDeleteOp = (ConcatDeleteOp)op;\n      String trg = renameReservedPathsOnUpgrade(concatDeleteOp.trg, logVersion);\n      String[] srcs = new String[concatDeleteOp.srcs.length];\n      for (int i=0; i<srcs.length; i++) {\n        srcs[i] =\n            renameReservedPathsOnUpgrade(concatDeleteOp.srcs[i], logVersion);\n      }\n      INodesInPath targetIIP = fsDir.getINodesInPath4Write(trg);\n      INodeFile[] srcFiles = new INodeFile[srcs.length];\n      for (int i = 0; i < srcs.length; i++) {\n        INodesInPath srcIIP = fsDir.getINodesInPath4Write(srcs[i]);\n        srcFiles[i] = srcIIP.getLastINode().asFile();\n      }\n      FSDirConcatOp.unprotectedConcat(fsDir, targetIIP, srcFiles,\n          concatDeleteOp.timestamp);\n      \n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(concatDeleteOp.rpcClientId,\n            concatDeleteOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_RENAME_OLD: {\n      RenameOldOp renameOp = (RenameOldOp)op;\n      final String src = renameReservedPathsOnUpgrade(renameOp.src, logVersion);\n      final String dst = renameReservedPathsOnUpgrade(renameOp.dst, logVersion);\n      FSDirRenameOp.renameForEditLog(fsDir, src, dst, renameOp.timestamp);\n      \n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(renameOp.rpcClientId, renameOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_DELETE: {\n      DeleteOp deleteOp = (DeleteOp)op;\n      FSDirDeleteOp.deleteForEditLog(\n          fsDir, renameReservedPathsOnUpgrade(deleteOp.path, logVersion),\n          deleteOp.timestamp);\n      \n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(deleteOp.rpcClientId, deleteOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_MKDIR: {\n      MkdirOp mkdirOp = (MkdirOp)op;\n      inodeId = getAndUpdateLastInodeId(mkdirOp.inodeId, logVersion,\n          lastInodeId);\n      FSDirMkdirOp.mkdirForEditLog(fsDir, inodeId,\n          renameReservedPathsOnUpgrade(mkdirOp.path, logVersion),\n          mkdirOp.permissions, mkdirOp.aclEntries, mkdirOp.timestamp);\n      break;\n    }\n    case OP_SET_GENSTAMP_V1: {\n      SetGenstampV1Op setGenstampV1Op = (SetGenstampV1Op)op;\n      blockManager.getBlockIdManager().setLegacyGenerationStamp(\n          setGenstampV1Op.genStampV1);\n      break;\n    }\n    case OP_SET_PERMISSIONS: {\n      SetPermissionsOp setPermissionsOp = (SetPermissionsOp)op;\n      FSDirAttrOp.unprotectedSetPermission(fsDir, renameReservedPathsOnUpgrade(\n          setPermissionsOp.src, logVersion), setPermissionsOp.permissions);\n      break;\n    }\n    case OP_SET_OWNER: {\n      SetOwnerOp setOwnerOp = (SetOwnerOp)op;\n      FSDirAttrOp.unprotectedSetOwner(\n          fsDir, renameReservedPathsOnUpgrade(setOwnerOp.src, logVersion),\n          setOwnerOp.username, setOwnerOp.groupname);\n      break;\n    }\n    case OP_SET_NS_QUOTA: {\n      SetNSQuotaOp setNSQuotaOp = (SetNSQuotaOp)op;\n      FSDirAttrOp.unprotectedSetQuota(\n          fsDir, renameReservedPathsOnUpgrade(setNSQuotaOp.src, logVersion),\n          setNSQuotaOp.nsQuota, HdfsConstants.QUOTA_DONT_SET, null);\n      break;\n    }\n    case OP_CLEAR_NS_QUOTA: {\n      ClearNSQuotaOp clearNSQuotaOp = (ClearNSQuotaOp)op;\n      FSDirAttrOp.unprotectedSetQuota(\n          fsDir, renameReservedPathsOnUpgrade(clearNSQuotaOp.src, logVersion),\n          HdfsConstants.QUOTA_RESET, HdfsConstants.QUOTA_DONT_SET, null);\n      break;\n    }\n\n    case OP_SET_QUOTA:\n      SetQuotaOp setQuotaOp = (SetQuotaOp) op;\n      FSDirAttrOp.unprotectedSetQuota(fsDir,\n          renameReservedPathsOnUpgrade(setQuotaOp.src, logVersion),\n          setQuotaOp.nsQuota, setQuotaOp.dsQuota, null);\n      break;\n\n    case OP_SET_QUOTA_BY_STORAGETYPE:\n        FSEditLogOp.SetQuotaByStorageTypeOp setQuotaByStorageTypeOp =\n          (FSEditLogOp.SetQuotaByStorageTypeOp) op;\n        FSDirAttrOp.unprotectedSetQuota(fsDir,\n          renameReservedPathsOnUpgrade(setQuotaByStorageTypeOp.src, logVersion),\n          HdfsConstants.QUOTA_DONT_SET, setQuotaByStorageTypeOp.dsQuota,\n          setQuotaByStorageTypeOp.type);\n        break;\n\n    case OP_TIMES: {\n      TimesOp timesOp = (TimesOp)op;\n      FSDirAttrOp.unprotectedSetTimes(\n          fsDir, renameReservedPathsOnUpgrade(timesOp.path, logVersion),\n          timesOp.mtime, timesOp.atime, true);\n      break;\n    }\n    case OP_SYMLINK: {\n      if (!FileSystem.areSymlinksEnabled()) {\n        throw new IOException(\"Symlinks not supported - please remove symlink before upgrading to this version of HDFS\");\n      }\n      SymlinkOp symlinkOp = (SymlinkOp)op;\n      inodeId = getAndUpdateLastInodeId(symlinkOp.inodeId, logVersion,\n          lastInodeId);\n      final String path = renameReservedPathsOnUpgrade(symlinkOp.path,\n          logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(path, false);\n      FSDirSymlinkOp.unprotectedAddSymlink(fsDir, iip.getExistingINodes(),\n          iip.getLastLocalName(), inodeId, symlinkOp.value, symlinkOp.mtime,\n          symlinkOp.atime, symlinkOp.permissionStatus);\n      \n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(symlinkOp.rpcClientId, symlinkOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_RENAME: {\n      RenameOp renameOp = (RenameOp)op;\n      FSDirRenameOp.renameForEditLog(fsDir,\n          renameReservedPathsOnUpgrade(renameOp.src, logVersion),\n          renameReservedPathsOnUpgrade(renameOp.dst, logVersion),\n          renameOp.timestamp, renameOp.options);\n      \n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(renameOp.rpcClientId, renameOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_GET_DELEGATION_TOKEN: {\n      GetDelegationTokenOp getDelegationTokenOp\n        = (GetDelegationTokenOp)op;\n\n      fsNamesys.getDelegationTokenSecretManager()\n        .addPersistedDelegationToken(getDelegationTokenOp.token,\n                                     getDelegationTokenOp.expiryTime);\n      break;\n    }\n    case OP_RENEW_DELEGATION_TOKEN: {\n      RenewDelegationTokenOp renewDelegationTokenOp\n        = (RenewDelegationTokenOp)op;\n      fsNamesys.getDelegationTokenSecretManager()\n        .updatePersistedTokenRenewal(renewDelegationTokenOp.token,\n                                     renewDelegationTokenOp.expiryTime);\n      break;\n    }\n    case OP_CANCEL_DELEGATION_TOKEN: {\n      CancelDelegationTokenOp cancelDelegationTokenOp\n        = (CancelDelegationTokenOp)op;\n      fsNamesys.getDelegationTokenSecretManager()\n          .updatePersistedTokenCancellation(\n              cancelDelegationTokenOp.token);\n      break;\n    }\n    case OP_UPDATE_MASTER_KEY: {\n      UpdateMasterKeyOp updateMasterKeyOp = (UpdateMasterKeyOp)op;\n      fsNamesys.getDelegationTokenSecretManager()\n        .updatePersistedMasterKey(updateMasterKeyOp.key);\n      break;\n    }\n    case OP_REASSIGN_LEASE: {\n      ReassignLeaseOp reassignLeaseOp = (ReassignLeaseOp)op;\n\n      Lease lease = fsNamesys.leaseManager.getLease(\n          reassignLeaseOp.leaseHolder);\n      final String path =\n          renameReservedPathsOnUpgrade(reassignLeaseOp.path, logVersion);\n      INodeFile pendingFile = fsDir.getINode(path).asFile();\n      Preconditions.checkState(pendingFile.isUnderConstruction());\n      fsNamesys.reassignLeaseInternal(lease, reassignLeaseOp.newHolder,\n              pendingFile);\n      break;\n    }\n    case OP_START_LOG_SEGMENT:\n    case OP_END_LOG_SEGMENT: {\n      // no data in here currently.\n      break;\n    }\n    case OP_CREATE_SNAPSHOT: {\n      CreateSnapshotOp createSnapshotOp = (CreateSnapshotOp) op;\n      final String snapshotRoot =\n          renameReservedPathsOnUpgrade(createSnapshotOp.snapshotRoot,\n              logVersion);\n      INodesInPath iip = fsDir.getINodesInPath4Write(snapshotRoot);\n      String path = fsNamesys.getSnapshotManager().createSnapshot(iip,\n          snapshotRoot, createSnapshotOp.snapshotName);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntryWithPayload(createSnapshotOp.rpcClientId,\n            createSnapshotOp.rpcCallId, path);\n      }\n      break;\n    }\n    case OP_DELETE_SNAPSHOT: {\n      DeleteSnapshotOp deleteSnapshotOp = (DeleteSnapshotOp) op;\n      BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n      List<INode> removedINodes = new ChunkedArrayList<INode>();\n      final String snapshotRoot =\n          renameReservedPathsOnUpgrade(deleteSnapshotOp.snapshotRoot,\n              logVersion);\n      INodesInPath iip = fsDir.getINodesInPath4Write(snapshotRoot);\n      fsNamesys.getSnapshotManager().deleteSnapshot(iip,\n          deleteSnapshotOp.snapshotName,\n          new INode.ReclaimContext(fsNamesys.dir.getBlockStoragePolicySuite(),\n              collectedBlocks, removedINodes, null));\n      fsNamesys.getBlockManager().removeBlocksAndUpdateSafemodeTotal(\n          collectedBlocks);\n      collectedBlocks.clear();\n      fsNamesys.dir.removeFromInodeMap(removedINodes);\n      removedINodes.clear();\n\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(deleteSnapshotOp.rpcClientId,\n            deleteSnapshotOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_RENAME_SNAPSHOT: {\n      RenameSnapshotOp renameSnapshotOp = (RenameSnapshotOp) op;\n      final String snapshotRoot =\n          renameReservedPathsOnUpgrade(renameSnapshotOp.snapshotRoot,\n              logVersion);\n      INodesInPath iip = fsDir.getINodesInPath4Write(snapshotRoot);\n      fsNamesys.getSnapshotManager().renameSnapshot(iip,\n          snapshotRoot, renameSnapshotOp.snapshotOldName,\n          renameSnapshotOp.snapshotNewName);\n      \n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(renameSnapshotOp.rpcClientId,\n            renameSnapshotOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_ALLOW_SNAPSHOT: {\n      AllowSnapshotOp allowSnapshotOp = (AllowSnapshotOp) op;\n      final String snapshotRoot =\n          renameReservedPathsOnUpgrade(allowSnapshotOp.snapshotRoot, logVersion);\n      fsNamesys.getSnapshotManager().setSnapshottable(\n          snapshotRoot, false);\n      break;\n    }\n    case OP_DISALLOW_SNAPSHOT: {\n      DisallowSnapshotOp disallowSnapshotOp = (DisallowSnapshotOp) op;\n      final String snapshotRoot =\n          renameReservedPathsOnUpgrade(disallowSnapshotOp.snapshotRoot,\n              logVersion);\n      fsNamesys.getSnapshotManager().resetSnapshottable(\n          snapshotRoot);\n      break;\n    }\n    case OP_SET_GENSTAMP_V2: {\n      SetGenstampV2Op setGenstampV2Op = (SetGenstampV2Op) op;\n      blockManager.getBlockIdManager().setGenerationStamp(\n          setGenstampV2Op.genStampV2);\n      break;\n    }\n    case OP_ALLOCATE_BLOCK_ID: {\n      AllocateBlockIdOp allocateBlockIdOp = (AllocateBlockIdOp) op;\n      if (BlockIdManager.isStripedBlockID(allocateBlockIdOp.blockId)) {\n        // ALLOCATE_BLOCK_ID is added for sequential block id, thus if the id\n        // is negative, it must belong to striped blocks\n        blockManager.getBlockIdManager().setLastAllocatedStripedBlockId(\n            allocateBlockIdOp.blockId);\n      } else {\n        blockManager.getBlockIdManager().setLastAllocatedContiguousBlockId(\n            allocateBlockIdOp.blockId);\n      }\n      break;\n    }\n    case OP_ROLLING_UPGRADE_START: {\n      if (startOpt == StartupOption.ROLLINGUPGRADE) {\n        final RollingUpgradeStartupOption rollingUpgradeOpt\n            = startOpt.getRollingUpgradeStartupOption(); \n        if (rollingUpgradeOpt == RollingUpgradeStartupOption.ROLLBACK) {\n          throw new RollingUpgradeOp.RollbackException();\n        }\n      }\n      // start rolling upgrade\n      final long startTime = ((RollingUpgradeOp) op).getTime();\n      fsNamesys.startRollingUpgradeInternal(startTime);\n      fsNamesys.triggerRollbackCheckpoint();\n      break;\n    }\n    case OP_ROLLING_UPGRADE_FINALIZE: {\n      final long finalizeTime = ((RollingUpgradeOp) op).getTime();\n      if (fsNamesys.isRollingUpgrade()) {\n        // Only do it when NN is actually doing rolling upgrade.\n        // We can get FINALIZE without corresponding START, if NN is restarted\n        // before this op is consumed and a new checkpoint is created.\n        fsNamesys.finalizeRollingUpgradeInternal(finalizeTime);\n      }\n      fsNamesys.getFSImage().updateStorageVersion();\n      fsNamesys.getFSImage().renameCheckpoint(NameNodeFile.IMAGE_ROLLBACK,\n          NameNodeFile.IMAGE);\n      break;\n    }\n    case OP_ADD_CACHE_DIRECTIVE: {\n      AddCacheDirectiveInfoOp addOp = (AddCacheDirectiveInfoOp) op;\n      CacheDirectiveInfo result = fsNamesys.\n          getCacheManager().addDirectiveFromEditLog(addOp.directive);\n      if (toAddRetryCache) {\n        Long id = result.getId();\n        fsNamesys.addCacheEntryWithPayload(op.rpcClientId, op.rpcCallId, id);\n      }\n      break;\n    }\n    case OP_MODIFY_CACHE_DIRECTIVE: {\n      ModifyCacheDirectiveInfoOp modifyOp =\n          (ModifyCacheDirectiveInfoOp) op;\n      fsNamesys.getCacheManager().modifyDirectiveFromEditLog(\n          modifyOp.directive);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(op.rpcClientId, op.rpcCallId);\n      }\n      break;\n    }\n    case OP_REMOVE_CACHE_DIRECTIVE: {\n      RemoveCacheDirectiveInfoOp removeOp =\n          (RemoveCacheDirectiveInfoOp) op;\n      fsNamesys.getCacheManager().removeDirective(removeOp.id, null);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(op.rpcClientId, op.rpcCallId);\n      }\n      break;\n    }\n    case OP_ADD_CACHE_POOL: {\n      AddCachePoolOp addOp = (AddCachePoolOp) op;\n      fsNamesys.getCacheManager().addCachePool(addOp.info);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(op.rpcClientId, op.rpcCallId);\n      }\n      break;\n    }\n    case OP_MODIFY_CACHE_POOL: {\n      ModifyCachePoolOp modifyOp = (ModifyCachePoolOp) op;\n      fsNamesys.getCacheManager().modifyCachePool(modifyOp.info);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(op.rpcClientId, op.rpcCallId);\n      }\n      break;\n    }\n    case OP_REMOVE_CACHE_POOL: {\n      RemoveCachePoolOp removeOp = (RemoveCachePoolOp) op;\n      fsNamesys.getCacheManager().removeCachePool(removeOp.poolName);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(op.rpcClientId, op.rpcCallId);\n      }\n      break;\n    }\n    case OP_SET_ACL: {\n      SetAclOp setAclOp = (SetAclOp) op;\n      FSDirAclOp.unprotectedSetAcl(fsDir, setAclOp.src, setAclOp.aclEntries,\n          true);\n      break;\n    }\n    case OP_SET_XATTR: {\n      SetXAttrOp setXAttrOp = (SetXAttrOp) op;\n      FSDirXAttrOp.unprotectedSetXAttrs(fsDir, setXAttrOp.src,\n                                        setXAttrOp.xAttrs,\n                                        EnumSet.of(XAttrSetFlag.CREATE,\n                                                   XAttrSetFlag.REPLACE));\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(setXAttrOp.rpcClientId, setXAttrOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_REMOVE_XATTR: {\n      RemoveXAttrOp removeXAttrOp = (RemoveXAttrOp) op;\n      FSDirXAttrOp.unprotectedRemoveXAttrs(fsDir, removeXAttrOp.src,\n                                           removeXAttrOp.xAttrs);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(removeXAttrOp.rpcClientId,\n            removeXAttrOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_TRUNCATE: {\n      TruncateOp truncateOp = (TruncateOp) op;\n      FSDirTruncateOp.unprotectedTruncate(fsNamesys, truncateOp.src,\n          truncateOp.clientName, truncateOp.clientMachine,\n          truncateOp.newLength, truncateOp.timestamp, truncateOp.truncateBlock);\n      break;\n    }\n    case OP_SET_STORAGE_POLICY: {\n      SetStoragePolicyOp setStoragePolicyOp = (SetStoragePolicyOp) op;\n      final String path = renameReservedPathsOnUpgrade(setStoragePolicyOp.path,\n          logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath4Write(path);\n      FSDirAttrOp.unprotectedSetStoragePolicy(\n          fsDir, fsNamesys.getBlockManager(), iip,\n          setStoragePolicyOp.policyId);\n      break;\n    }\n    default:\n      throw new IOException(\"Invalid operation read \" + op.opCode);\n    }\n    return inodeId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.getAndUpdateLastInodeId": "  private long getAndUpdateLastInodeId(long inodeIdFromOp, int logVersion,\n      long lastInodeId) throws IOException {\n    long inodeId = inodeIdFromOp;\n\n    if (inodeId == HdfsConstants.GRANDFATHER_INODE_ID) {\n      if (NameNodeLayoutVersion.supports(\n          LayoutVersion.Feature.ADD_INODE_ID, logVersion)) {\n        throw new IOException(\"The layout version \" + logVersion\n            + \" supports inodeId but gave bogus inodeId\");\n      }\n      inodeId = fsNamesys.dir.allocateNewInodeId();\n    } else {\n      // need to reset lastInodeId. fsnamesys gets lastInodeId firstly from\n      // fsimage but editlog captures more recent inodeId allocations\n      if (inodeId > lastInodeId) {\n        fsNamesys.dir.resetLastInodeId(inodeId);\n      }\n    }\n    return inodeId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.addNewBlock": "  private void addNewBlock(AddBlockOp op, INodeFile file,\n      ErasureCodingPolicy ecPolicy) throws IOException {\n    BlockInfo[] oldBlocks = file.getBlocks();\n    Block pBlock = op.getPenultimateBlock();\n    Block newBlock= op.getLastBlock();\n    \n    if (pBlock != null) { // the penultimate block is not null\n      assert oldBlocks != null && oldBlocks.length > 0;\n      // compare pBlock with the last block of oldBlocks\n      BlockInfo oldLastBlock = oldBlocks[oldBlocks.length - 1];\n      if (oldLastBlock.getBlockId() != pBlock.getBlockId()\n          || oldLastBlock.getGenerationStamp() != pBlock.getGenerationStamp()) {\n        throw new IOException(\n            \"Mismatched block IDs or generation stamps for the old last block of file \"\n                + op.getPath() + \", the old last block is \" + oldLastBlock\n                + \", and the block read from editlog is \" + pBlock);\n      }\n      \n      oldLastBlock.setNumBytes(pBlock.getNumBytes());\n      if (!oldLastBlock.isComplete()) {\n        fsNamesys.getBlockManager().forceCompleteBlock(oldLastBlock);\n        fsNamesys.getBlockManager().processQueuedMessagesForBlock(pBlock);\n      }\n    } else { // the penultimate block is null\n      Preconditions.checkState(oldBlocks == null || oldBlocks.length == 0);\n    }\n    // add the new block\n    final BlockInfo newBlockInfo;\n    boolean isStriped = ecPolicy != null;\n    if (isStriped) {\n      newBlockInfo = new BlockInfoStriped(newBlock, ecPolicy);\n    } else {\n      newBlockInfo = new BlockInfoContiguous(newBlock,\n          file.getPreferredBlockReplication());\n    }\n    newBlockInfo.convertToBlockUnderConstruction(\n        BlockUCState.UNDER_CONSTRUCTION, null);\n    fsNamesys.getBlockManager().addBlockCollectionWithCheck(newBlockInfo, file);\n    file.addBlock(newBlockInfo);\n    fsNamesys.getBlockManager().processQueuedMessagesForBlock(newBlock);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.updateBlocks": "  private void updateBlocks(FSDirectory fsDir, BlockListUpdatingOp op,\n      INodesInPath iip, INodeFile file, ErasureCodingPolicy ecPolicy)\n      throws IOException {\n    // Update its block list\n    BlockInfo[] oldBlocks = file.getBlocks();\n    Block[] newBlocks = op.getBlocks();\n    String path = op.getPath();\n    \n    // Are we only updating the last block's gen stamp.\n    boolean isGenStampUpdate = oldBlocks.length == newBlocks.length;\n    \n    // First, update blocks in common\n    for (int i = 0; i < oldBlocks.length && i < newBlocks.length; i++) {\n      BlockInfo oldBlock = oldBlocks[i];\n      Block newBlock = newBlocks[i];\n      \n      boolean isLastBlock = i == newBlocks.length - 1;\n      if (oldBlock.getBlockId() != newBlock.getBlockId() ||\n          (oldBlock.getGenerationStamp() != newBlock.getGenerationStamp() && \n              !(isGenStampUpdate && isLastBlock))) {\n        throw new IOException(\"Mismatched block IDs or generation stamps, \" +\n            \"attempting to replace block \" + oldBlock + \" with \" + newBlock +\n            \" as block # \" + i + \"/\" + newBlocks.length + \" of \" +\n            path);\n      }\n      \n      oldBlock.setNumBytes(newBlock.getNumBytes());\n      boolean changeMade =\n        oldBlock.getGenerationStamp() != newBlock.getGenerationStamp();\n      oldBlock.setGenerationStamp(newBlock.getGenerationStamp());\n      \n      if (!oldBlock.isComplete() &&\n          (!isLastBlock || op.shouldCompleteLastBlock())) {\n        changeMade = true;\n        fsNamesys.getBlockManager().forceCompleteBlock(oldBlock);\n      }\n      if (changeMade) {\n        // The state or gen-stamp of the block has changed. So, we may be\n        // able to process some messages from datanodes that we previously\n        // were unable to process.\n        fsNamesys.getBlockManager().processQueuedMessagesForBlock(newBlock);\n      }\n    }\n    \n    if (newBlocks.length < oldBlocks.length) {\n      // We're removing a block from the file, e.g. abandonBlock(...)\n      if (!file.isUnderConstruction()) {\n        throw new IOException(\"Trying to remove a block from file \" +\n            path + \" which is not under construction.\");\n      }\n      if (newBlocks.length != oldBlocks.length - 1) {\n        throw new IOException(\"Trying to remove more than one block from file \"\n            + path);\n      }\n      Block oldBlock = oldBlocks[oldBlocks.length - 1];\n      boolean removed = FSDirWriteFileOp.unprotectedRemoveBlock(\n          fsDir, path, iip, file, oldBlock);\n      if (!removed && !(op instanceof UpdateBlocksOp)) {\n        throw new IOException(\"Trying to delete non-existant block \" + oldBlock);\n      }\n    } else if (newBlocks.length > oldBlocks.length) {\n      final boolean isStriped = ecPolicy != null;\n      // We're adding blocks\n      for (int i = oldBlocks.length; i < newBlocks.length; i++) {\n        Block newBlock = newBlocks[i];\n        final BlockInfo newBI;\n        if (!op.shouldCompleteLastBlock()) {\n          // TODO: shouldn't this only be true for the last block?\n          // what about an old-version fsync() where fsync isn't called\n          // until several blocks in?\n          if (isStriped) {\n            newBI = new BlockInfoStriped(newBlock, ecPolicy);\n          } else {\n            newBI = new BlockInfoContiguous(newBlock,\n                file.getPreferredBlockReplication());\n          }\n          newBI.convertToBlockUnderConstruction(\n              BlockUCState.UNDER_CONSTRUCTION, null);\n        } else {\n          // OP_CLOSE should add finalized blocks. This code path\n          // is only executed when loading edits written by prior\n          // versions of Hadoop. Current versions always log\n          // OP_ADD operations as each block is allocated.\n          if (isStriped) {\n            newBI = new BlockInfoStriped(newBlock,\n                ErasureCodingPolicyManager.getSystemDefaultPolicy());\n          } else {\n            newBI = new BlockInfoContiguous(newBlock,\n                file.getFileReplication());\n          }\n        }\n        fsNamesys.getBlockManager().addBlockCollectionWithCheck(newBI, file);\n        file.addBlock(newBI);\n        fsNamesys.getBlockManager().processQueuedMessagesForBlock(newBlock);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords": "  long loadEditRecords(EditLogInputStream in, boolean closeOnExit,\n      long expectedStartingTxId, StartupOption startOpt,\n      MetaRecoveryContext recovery) throws IOException {\n    FSDirectory fsDir = fsNamesys.dir;\n\n    EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts =\n      new EnumMap<FSEditLogOpCodes, Holder<Integer>>(FSEditLogOpCodes.class);\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Acquiring write lock to replay edit log\");\n    }\n\n    fsNamesys.writeLock();\n    fsDir.writeLock();\n\n    long recentOpcodeOffsets[] = new long[4];\n    Arrays.fill(recentOpcodeOffsets, -1);\n    \n    long expectedTxId = expectedStartingTxId;\n    long numEdits = 0;\n    long lastTxId = in.getLastTxId();\n    long numTxns = (lastTxId - expectedStartingTxId) + 1;\n    StartupProgress prog = NameNode.getStartupProgress();\n    Step step = createStartupProgressStep(in);\n    prog.setTotal(Phase.LOADING_EDITS, step, numTxns);\n    Counter counter = prog.getCounter(Phase.LOADING_EDITS, step);\n    long lastLogTime = monotonicNow();\n    long lastInodeId = fsNamesys.dir.getLastInodeId();\n    \n    try {\n      while (true) {\n        try {\n          FSEditLogOp op;\n          try {\n            op = in.readOp();\n            if (op == null) {\n              break;\n            }\n          } catch (Throwable e) {\n            // Handle a problem with our input\n            check203UpgradeFailure(in.getVersion(true), e);\n            String errorMessage =\n              formatEditLogReplayError(in, recentOpcodeOffsets, expectedTxId);\n            FSImage.LOG.error(errorMessage, e);\n            if (recovery == null) {\n               // We will only try to skip over problematic opcodes when in\n               // recovery mode.\n              throw new EditLogInputException(errorMessage, e, numEdits);\n            }\n            MetaRecoveryContext.editLogLoaderPrompt(\n                \"We failed to read txId \" + expectedTxId,\n                recovery, \"skipping the bad section in the log\");\n            in.resync();\n            continue;\n          }\n          recentOpcodeOffsets[(int)(numEdits % recentOpcodeOffsets.length)] =\n            in.getPosition();\n          if (op.hasTransactionId()) {\n            if (op.getTransactionId() > expectedTxId) { \n              MetaRecoveryContext.editLogLoaderPrompt(\"There appears \" +\n                  \"to be a gap in the edit log.  We expected txid \" +\n                  expectedTxId + \", but got txid \" +\n                  op.getTransactionId() + \".\", recovery, \"ignoring missing \" +\n                  \" transaction IDs\");\n            } else if (op.getTransactionId() < expectedTxId) { \n              MetaRecoveryContext.editLogLoaderPrompt(\"There appears \" +\n                  \"to be an out-of-order edit in the edit log.  We \" +\n                  \"expected txid \" + expectedTxId + \", but got txid \" +\n                  op.getTransactionId() + \".\", recovery,\n                  \"skipping the out-of-order edit\");\n              continue;\n            }\n          }\n          try {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"op=\" + op + \", startOpt=\" + startOpt\n                  + \", numEdits=\" + numEdits + \", totalEdits=\" + totalEdits);\n            }\n            long inodeId = applyEditLogOp(op, fsDir, startOpt,\n                in.getVersion(true), lastInodeId);\n            if (lastInodeId < inodeId) {\n              lastInodeId = inodeId;\n            }\n          } catch (RollingUpgradeOp.RollbackException e) {\n            throw e;\n          } catch (Throwable e) {\n            LOG.error(\"Encountered exception on operation \" + op, e);\n            if (recovery == null) {\n              throw e instanceof IOException? (IOException)e: new IOException(e);\n            }\n\n            MetaRecoveryContext.editLogLoaderPrompt(\"Failed to \" +\n             \"apply edit log operation \" + op + \": error \" +\n             e.getMessage(), recovery, \"applying edits\");\n          }\n          // Now that the operation has been successfully decoded and\n          // applied, update our bookkeeping.\n          incrOpCount(op.opCode, opCounts, step, counter);\n          if (op.hasTransactionId()) {\n            lastAppliedTxId = op.getTransactionId();\n            expectedTxId = lastAppliedTxId + 1;\n          } else {\n            expectedTxId = lastAppliedTxId = expectedStartingTxId;\n          }\n          // log progress\n          if (op.hasTransactionId()) {\n            long now = monotonicNow();\n            if (now - lastLogTime > REPLAY_TRANSACTION_LOG_INTERVAL) {\n              long deltaTxId = lastAppliedTxId - expectedStartingTxId + 1;\n              int percent = Math.round((float) deltaTxId / numTxns * 100);\n              LOG.info(\"replaying edit log: \" + deltaTxId + \"/\" + numTxns\n                  + \" transactions completed. (\" + percent + \"%)\");\n              lastLogTime = now;\n            }\n          }\n          numEdits++;\n          totalEdits++;\n        } catch (RollingUpgradeOp.RollbackException e) {\n          LOG.info(\"Stopped at OP_START_ROLLING_UPGRADE for rollback.\");\n          break;\n        } catch (MetaRecoveryContext.RequestStopException e) {\n          MetaRecoveryContext.LOG.warn(\"Stopped reading edit log at \" +\n              in.getPosition() + \"/\"  + in.length());\n          break;\n        }\n      }\n    } finally {\n      fsNamesys.dir.resetLastInodeId(lastInodeId);\n      if(closeOnExit) {\n        in.close();\n      }\n      fsDir.writeUnlock();\n      fsNamesys.writeUnlock();\n\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"replaying edit log finished\");\n      }\n\n      if (FSImage.LOG.isDebugEnabled()) {\n        dumpOpCounts(opCounts);\n      }\n    }\n    return numEdits;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.createStartupProgressStep": "  private static Step createStartupProgressStep(EditLogInputStream edits)\n      throws IOException {\n    long length = edits.length();\n    String name = edits.getCurrentStreamName();\n    return length != -1 ? new Step(name, length) : new Step(name);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.formatEditLogReplayError": "  private static String formatEditLogReplayError(EditLogInputStream in,\n      long recentOpcodeOffsets[], long txid) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"Error replaying edit log at offset \" + in.getPosition());\n    sb.append(\".  Expected transaction ID was \").append(txid);\n    if (recentOpcodeOffsets[0] != -1) {\n      Arrays.sort(recentOpcodeOffsets);\n      sb.append(\"\\nRecent opcode offsets:\");\n      for (long offset : recentOpcodeOffsets) {\n        if (offset != -1) {\n          sb.append(' ').append(offset);\n        }\n      }\n    }\n    return sb.toString();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.check203UpgradeFailure": "  private void check203UpgradeFailure(int logVersion, Throwable e)\n      throws IOException {\n    // 0.20.203 version version has conflicting opcodes with the later releases.\n    // The editlog must be emptied by restarting the namenode, before proceeding\n    // with the upgrade.\n    if (Storage.is203LayoutVersion(logVersion)\n        && logVersion != HdfsServerConstants.NAMENODE_LAYOUT_VERSION) {\n      String msg = \"During upgrade failed to load the editlog version \"\n          + logVersion + \" from release 0.20.203. Please go back to the old \"\n          + \" release and restart the namenode. This empties the editlog \"\n          + \" and saves the namespace. Resume the upgrade after this step.\";\n      throw new IOException(msg, e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.incrOpCount": "  private void incrOpCount(FSEditLogOpCodes opCode,\n      EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts, Step step,\n      Counter counter) {\n    Holder<Integer> holder = opCounts.get(opCode);\n    if (holder == null) {\n      holder = new Holder<Integer>(1);\n      opCounts.put(opCode, holder);\n    } else {\n      holder.held++;\n    }\n    counter.increment();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.dumpOpCounts": "  private static void dumpOpCounts(\n      EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"Summary of operations loaded from edit log:\\n  \");\n    Joiner.on(\"\\n  \").withKeyValueSeparator(\"=\").appendTo(sb, opCounts);\n    FSImage.LOG.debug(sb.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits": "  long loadFSEdits(EditLogInputStream edits, long expectedStartingTxId,\n      StartupOption startOpt, MetaRecoveryContext recovery) throws IOException {\n    StartupProgress prog = NameNode.getStartupProgress();\n    Step step = createStartupProgressStep(edits);\n    prog.beginStep(Phase.LOADING_EDITS, step);\n    fsNamesys.writeLock();\n    try {\n      long startTime = monotonicNow();\n      FSImage.LOG.info(\"Start loading edits file \" + edits.getName());\n      long numEdits = loadEditRecords(edits, false, expectedStartingTxId,\n          startOpt, recovery);\n      FSImage.LOG.info(\"Edits file \" + edits.getName() \n          + \" of size \" + edits.length() + \" edits # \" + numEdits \n          + \" loaded in \" + (monotonicNow()-startTime)/1000 + \" seconds\");\n      return numEdits;\n    } finally {\n      edits.close();\n      fsNamesys.writeUnlock();\n      prog.endStep(Phase.LOADING_EDITS, step);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits": "  private long loadEdits(Iterable<EditLogInputStream> editStreams,\n      FSNamesystem target, StartupOption startOpt, MetaRecoveryContext recovery)\n      throws IOException {\n    LOG.debug(\"About to load edits:\\n  \" + Joiner.on(\"\\n  \").join(editStreams));\n    StartupProgress prog = NameNode.getStartupProgress();\n    prog.beginPhase(Phase.LOADING_EDITS);\n    \n    long prevLastAppliedTxId = lastAppliedTxId;  \n    try {    \n      FSEditLogLoader loader = new FSEditLogLoader(target, lastAppliedTxId);\n      \n      // Load latest edits\n      for (EditLogInputStream editIn : editStreams) {\n        LOG.info(\"Reading \" + editIn + \" expecting start txid #\" +\n              (lastAppliedTxId + 1));\n        try {\n          loader.loadFSEdits(editIn, lastAppliedTxId + 1, startOpt, recovery);\n        } finally {\n          // Update lastAppliedTxId even in case of error, since some ops may\n          // have been successfully applied before the error.\n          lastAppliedTxId = loader.getLastAppliedTxId();\n        }\n        // If we are in recovery mode, we may have skipped over some txids.\n        if (editIn.getLastTxId() != HdfsServerConstants.INVALID_TXID) {\n          lastAppliedTxId = editIn.getLastTxId();\n        }\n      }\n    } finally {\n      FSEditLog.closeAllStreams(editStreams);\n    }\n    prog.endPhase(Phase.LOADING_EDITS);\n    return lastAppliedTxId - prevLastAppliedTxId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getLastAppliedTxId": "  public synchronized long getLastAppliedTxId() {\n    return lastAppliedTxId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits": "  void doTailEdits() throws IOException, InterruptedException {\n    // Write lock needs to be interruptible here because the \n    // transitionToActive RPC takes the write lock before calling\n    // tailer.stop() -- so if we're not interruptible, it will\n    // deadlock.\n    namesystem.writeLockInterruptibly();\n    try {\n      FSImage image = namesystem.getFSImage();\n\n      long lastTxnId = image.getLastAppliedTxId();\n      \n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"lastTxnId: \" + lastTxnId);\n      }\n      Collection<EditLogInputStream> streams;\n      try {\n        streams = editLog.selectInputStreams(lastTxnId + 1, 0,\n            null, inProgressOk, true);\n      } catch (IOException ioe) {\n        // This is acceptable. If we try to tail edits in the middle of an edits\n        // log roll, i.e. the last one has been finalized but the new inprogress\n        // edits file hasn't been started yet.\n        LOG.warn(\"Edits tailer failed to find any streams. Will try again \" +\n            \"later.\", ioe);\n        return;\n      }\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"edit streams to load from: \" + streams.size());\n      }\n      \n      // Once we have streams to load, errors encountered are legitimate cause\n      // for concern, so we don't catch them here. Simple errors reading from\n      // disk are ignored.\n      long editsLoaded = 0;\n      try {\n        editsLoaded = image.loadEdits(streams, namesystem);\n      } catch (EditLogInputException elie) {\n        editsLoaded = elie.getNumEditsLoaded();\n        throw elie;\n      } finally {\n        if (editsLoaded > 0 || LOG.isDebugEnabled()) {\n          LOG.debug(String.format(\"Loaded %d edits starting from txid %d \",\n              editsLoaded, lastTxnId));\n        }\n      }\n\n      if (editsLoaded > 0) {\n        lastLoadTimeMs = monotonicNow();\n      }\n      lastLoadedTxnId = image.getLastAppliedTxId();\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doWork": "    protected abstract T doWork() throws IOException;\n\n    public T call() throws IOException {\n      // reset the loop count on success\n      nnLoopCount = 0;\n      while ((cachedActiveProxy = getActiveNodeProxy()) != null) {\n        try {\n          T ret = doWork();\n          return ret;\n        } catch (RemoteException e) {\n          Throwable cause = e.unwrapRemoteException(StandbyException.class);\n          // if its not a standby exception, then we need to re-throw it, something bad has happened\n          if (cause == e) {\n            throw e;\n          } else {\n            // it is a standby exception, so we try the other NN\n            LOG.warn(\"Failed to reach remote node: \" + currentNN\n                + \", retrying with remaining remote NNs\");\n            cachedActiveProxy = null;\n            // this NN isn't responding to requests, try the next one\n            nnLoopCount++;\n          }\n        }\n      }\n      throw new IOException(\"Cannot find any valid remote NN to service request!\");\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.tooLongSinceLastLoad": "  private boolean tooLongSinceLastLoad() {\n    return logRollPeriodMs >= 0 && \n      (monotonicNow() - lastLoadTimeMs) > logRollPeriodMs ;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll": "  void triggerActiveLogRoll() {\n    LOG.info(\"Triggering log roll on remote NameNode\");\n    Future<Void> future = null;\n    try {\n      future = rollEditsRpcExecutor.submit(getNameNodeProxy());\n      future.get(rollEditsTimeoutMs, TimeUnit.MILLISECONDS);\n      lastRollTriggerTxId = lastLoadedTxnId;\n    } catch (ExecutionException e) {\n      Throwable cause = e.getCause();\n      if (cause instanceof RemoteException) {\n        IOException ioe = ((RemoteException) cause).unwrapRemoteException();\n        if (ioe instanceof StandbyException) {\n          LOG.info(\"Skipping log roll. Remote node is not in Active state: \" +\n              ioe.getMessage().split(\"\\n\")[0]);\n          return;\n        }\n      }\n      LOG.warn(\"Unable to trigger a roll of the active NN\", e);\n    } catch (TimeoutException e) {\n      if (future != null) {\n        future.cancel(true);\n      }\n      LOG.warn(String.format(\n          \"Unable to finish rolling edits in %d ms\", rollEditsTimeoutMs));\n    } catch (InterruptedException e) {\n      LOG.warn(\"Unable to trigger a roll of the active NN\", e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.run": "          public Object run() {\n            doWork();\n            return null;\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause == null) {\n        throw new RuntimeException(\"PrivilegedActionException with no \" +\n                \"underlying cause. UGI [\" + this + \"]\" +\": \" + pae, pae);\n      } else if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal": "  public static <T> T doAsLoginUserOrFatal(PrivilegedAction<T> action) { \n    if (UserGroupInformation.isSecurityEnabled()) {\n      UserGroupInformation ugi = null;\n      try { \n        ugi = UserGroupInformation.getLoginUser();\n      } catch (IOException e) {\n        LOG.fatal(\"Exception while getting login user\", e);\n        e.printStackTrace();\n        Runtime.getRuntime().exit(-1);\n      }\n      return ugi.doAs(action);\n    } else {\n      return action.run();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems": "  void verifyMaxDirItems(INodeDirectory parent, String parentPath)\n      throws MaxDirectoryItemsExceededException {\n    final int count = parent.getChildrenList(CURRENT_STATE_ID).size();\n    if (count >= maxDirItems) {\n      final MaxDirectoryItemsExceededException e\n          = new MaxDirectoryItemsExceededException(maxDirItems, count);\n      if (namesystem.isImageLoaded()) {\n        e.setPathName(parentPath);\n        throw e;\n      } else {\n        // Do not throw if edits log is still being processed\n        NameNode.LOG.error(\"FSDirectory.verifyMaxDirItems: \"\n            + e.getLocalizedMessage());\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode": "  public INodesInPath addLastINode(INodesInPath existing, INode inode,\n      boolean checkQuota) throws QuotaExceededException {\n    assert existing.getLastINode() != null &&\n        existing.getLastINode().isDirectory();\n\n    final int pos = existing.length();\n    // Disallow creation of /.reserved. This may be created when loading\n    // editlog/fsimage during upgrade since /.reserved was a valid name in older\n    // release. This may also be called when a user tries to create a file\n    // or directory /.reserved.\n    if (pos == 1 && existing.getINode(0) == rootDir && isReservedName(inode)) {\n      throw new HadoopIllegalArgumentException(\n          \"File name \\\"\" + inode.getLocalName() + \"\\\" is reserved and cannot \"\n              + \"be created. If this is during upgrade change the name of the \"\n              + \"existing file or directory to another name before upgrading \"\n              + \"to the new release.\");\n    }\n    final INodeDirectory parent = existing.getINode(pos - 1).asDirectory();\n    // The filesystem limits are not really quotas, so this check may appear\n    // odd. It's because a rename operation deletes the src, tries to add\n    // to the dest, if that fails, re-adds the src from whence it came.\n    // The rename code disables the quota when it's restoring to the\n    // original location because a quota violation would cause the the item\n    // to go \"poof\".  The fs limits must be bypassed for the same reason.\n    if (checkQuota) {\n      final String parentPath = existing.getPath();\n      verifyMaxComponentLength(inode.getLocalNameBytes(), parentPath);\n      verifyMaxDirItems(parent, parentPath);\n    }\n    // always verify inode name\n    verifyINodeName(inode.getLocalNameBytes());\n\n    final QuotaCounts counts = inode.computeQuotaUsage(getBlockStoragePolicySuite());\n    updateCount(existing, pos, counts, checkQuota);\n\n    boolean isRename = (inode.getParent() != null);\n    boolean added;\n    try {\n      added = parent.addChild(inode, true, existing.getLatestSnapshotId());\n    } catch (QuotaExceededException e) {\n      updateCountNoQuotaCheck(existing, pos, counts.negation());\n      throw e;\n    }\n    if (!added) {\n      updateCountNoQuotaCheck(existing, pos, counts.negation());\n      return null;\n    } else {\n      if (!isRename) {\n        AclStorage.copyINodeDefaultAcl(inode);\n      }\n      addToInodeMap(inode);\n    }\n    return INodesInPath.append(existing, inode, inode.getLocalNameBytes());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCountNoQuotaCheck": "   void updateCountNoQuotaCheck(INodesInPath inodesInPath,\n      int numOfINodes, QuotaCounts counts) {\n    assert hasWriteLock();\n    try {\n      updateCount(inodesInPath, numOfINodes, counts, false);\n    } catch (QuotaExceededException e) {\n      NameNode.LOG.error(\"BUG: unexpected exception \", e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.isReservedName": "  public static boolean isReservedName(String src) {\n    return src.startsWith(DOT_RESERVED_PATH_PREFIX + Path.SEPARATOR);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount": "  void updateCount(INodesInPath iip, int numOfINodes,\n                    QuotaCounts counts, boolean checkQuota)\n                    throws QuotaExceededException {\n    assert hasWriteLock();\n    if (!namesystem.isImageLoaded()) {\n      //still initializing. do not check or update quotas.\n      return;\n    }\n    if (numOfINodes > iip.length()) {\n      numOfINodes = iip.length();\n    }\n    if (checkQuota && !skipQuotaCheck) {\n      verifyQuota(iip, numOfINodes, counts, null);\n    }\n    unprotectedUpdateCount(iip, numOfINodes, counts);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyINodeName": "  void verifyINodeName(byte[] childName) throws HadoopIllegalArgumentException {\n    if (Arrays.equals(HdfsServerConstants.DOT_SNAPSHOT_DIR_BYTES, childName)) {\n      String s = \"\\\"\" + HdfsConstants.DOT_SNAPSHOT_DIR + \"\\\" is a reserved name.\";\n      if (!namesystem.isImageLoaded()) {\n        s += \"  Please rename it before upgrade.\";\n      }\n      throw new HadoopIllegalArgumentException(s);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getINode": "  public INode getINode(String src) throws UnresolvedLinkException {\n    return getINode(src, true);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.addToInodeMap": "  public final void addToInodeMap(INode inode) {\n    if (inode instanceof INodeWithAdditionalFields) {\n      inodeMap.put(inode);\n      if (!inode.isSymlink()) {\n        final XAttrFeature xaf = inode.getXAttrFeature();\n        addEncryptionZone((INodeWithAdditionalFields) inode, xaf);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getBlockStoragePolicySuite": "  public BlockStoragePolicySuite getBlockStoragePolicySuite() {\n    return getBlockManager().getStoragePolicySuite();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength": "  void verifyMaxComponentLength(byte[] childName, String parentPath)\n      throws PathComponentTooLongException {\n    if (maxComponentLength == 0) {\n      return;\n    }\n\n    final int length = childName.length;\n    if (length > maxComponentLength) {\n      final PathComponentTooLongException e = new PathComponentTooLongException(\n          maxComponentLength, length, parentPath,\n          DFSUtil.bytes2String(childName));\n      if (namesystem.isImageLoaded()) {\n        throw e;\n      } else {\n        // Do not throw if edits log is still being processed\n        NameNode.LOG.error(\"ERROR in FSDirectory.verifyINodeName\", e);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode": "  INodesInPath addINode(INodesInPath existing, INode child)\n      throws QuotaExceededException, UnresolvedLinkException {\n    cacheName(child);\n    writeLock();\n    try {\n      return addLastINode(existing, child, true);\n    } finally {\n      writeUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.writeLock": "  void writeLock() {\n    this.dirLock.writeLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.cacheName": "  void cacheName(INode inode) {\n    // Name is cached only for files\n    if (!inode.isFile()) {\n      return;\n    }\n    ByteArray name = new ByteArray(inode.getLocalNameBytes());\n    name = nameCache.put(name);\n    if (name != null) {\n      inode.setLocalName(name.getBytes());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.writeUnlock": "  void writeUnlock() {\n    this.dirLock.writeLock().unlock();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SecurityUtil.doAsUser": "  private static <T> T doAsUser(UserGroupInformation ugi,\n      PrivilegedExceptionAction<T> action) throws IOException {\n    try {\n      return ugi.doAs(action);\n    } catch (InterruptedException ie) {\n      throw new IOException(ie);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SecurityUtil.doAsLoginUser": "  public static <T> T doAsLoginUser(PrivilegedExceptionAction<T> action)\n      throws IOException {\n    return doAsUser(UserGroupInformation.getLoginUser(), action);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.catchupDuringFailover": "  public void catchupDuringFailover() throws IOException {\n    Preconditions.checkState(tailerThread == null ||\n        !tailerThread.isAlive(),\n        \"Tailer thread should not be running once failover starts\");\n    // Important to do tailing as the login user, in case the shared\n    // edits storage is implemented by a JournalManager that depends\n    // on security credentials to access the logs (eg QuorumJournalManager).\n    SecurityUtil.doAsLoginUser(new PrivilegedExceptionAction<Void>() {\n      @Override\n      public Void run() throws Exception {\n        try {\n          // It is already under the full name system lock and the checkpointer\n          // thread is already stopped. No need to acqure any other lock.\n          doTailEdits();\n        } catch (InterruptedException e) {\n          throw new IOException(e);\n        }\n        return null;\n      }\n    });\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices": "  void startActiveServices() throws IOException {\n    startingActiveService = true;\n    LOG.info(\"Starting services required for active state\");\n    writeLock();\n    try {\n      FSEditLog editLog = getFSImage().getEditLog();\n      \n      if (!editLog.isOpenForWrite()) {\n        // During startup, we're already open for write during initialization.\n        editLog.initJournalsForWrite();\n        // May need to recover\n        editLog.recoverUnclosedStreams();\n        \n        LOG.info(\"Catching up to latest edits from old active before \" +\n            \"taking over writer role in edits logs\");\n        editLogTailer.catchupDuringFailover();\n        \n        blockManager.setPostponeBlocksFromFuture(false);\n        blockManager.getDatanodeManager().markAllDatanodesStale();\n        blockManager.clearQueues();\n        blockManager.processAllPendingDNMessages();\n\n        // Only need to re-process the queue, If not in SafeMode.\n        if (!isInSafeMode()) {\n          LOG.info(\"Reprocessing replication and invalidation queues\");\n          blockManager.initializeReplQueues();\n        }\n\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"NameNode metadata after re-processing \" +\n              \"replication and invalidation queues during failover:\\n\" +\n              metaSaveAsString());\n        }\n\n        long nextTxId = getFSImage().getLastAppliedTxId() + 1;\n        LOG.info(\"Will take over writing edit logs at txnid \" + \n            nextTxId);\n        editLog.setNextTxId(nextTxId);\n\n        getFSImage().editLog.openForWrite(getEffectiveLayoutVersion());\n      }\n\n      // Initialize the quota.\n      dir.updateCountForQuota();\n      // Enable quota checks.\n      dir.enableQuotaChecks();\n      if (haEnabled) {\n        // Renew all of the leases before becoming active.\n        // This is because, while we were in standby mode,\n        // the leases weren't getting renewed on this NN.\n        // Give them all a fresh start here.\n        leaseManager.renewAllLeases();\n      }\n      leaseManager.startMonitor();\n      startSecretManagerIfNecessary();\n\n      //ResourceMonitor required only at ActiveNN. See HDFS-2914\n      this.nnrmthread = new Daemon(new NameNodeResourceMonitor());\n      nnrmthread.start();\n\n      nnEditLogRoller = new Daemon(new NameNodeEditLogRoller(\n          editLogRollerThreshold, editLogRollerInterval));\n      nnEditLogRoller.start();\n\n      if (lazyPersistFileScrubIntervalSec > 0) {\n        lazyPersistFileScrubber = new Daemon(new LazyPersistFileScrubber(\n            lazyPersistFileScrubIntervalSec));\n        lazyPersistFileScrubber.start();\n      } else {\n        LOG.warn(\"Lazy persist file scrubber is disabled,\"\n            + \" configured scrub interval is zero.\");\n      }\n\n      cacheManager.startMonitorThread();\n      blockManager.getDatanodeManager().setShouldSendCachingCommands(true);\n      if (provider != null) {\n        edekCacheLoader = Executors.newSingleThreadExecutor(\n            new ThreadFactoryBuilder().setDaemon(true)\n                .setNameFormat(\"Warm Up EDEK Cache Thread #%d\")\n                .build());\n        FSDirEncryptionZoneOp.warmUpEdekCache(edekCacheLoader, dir,\n            edekCacheLoaderDelay, edekCacheLoaderInterval);\n      }\n    } finally {\n      startingActiveService = false;\n      blockManager.checkSafeMode();\n      writeUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.metaSaveAsString": "  private String metaSaveAsString() {\n    StringWriter sw = new StringWriter();\n    PrintWriter pw = new PrintWriter(sw);\n    metaSave(pw);\n    pw.flush();\n    return sw.toString();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEditLog": "  public FSEditLog getEditLog() {\n    return getFSImage().getEditLog();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEffectiveLayoutVersion": "  static int getEffectiveLayoutVersion(boolean isRollingUpgrade, int storageLV,\n      int minCompatLV, int currentLV) {\n    if (isRollingUpgrade) {\n      if (storageLV <= minCompatLV) {\n        // The prior layout version satisfies the minimum compatible layout\n        // version of the current software.  Keep reporting the prior layout\n        // as the effective one.  Downgrade is possible.\n        return storageLV;\n      }\n    }\n    // The current software cannot satisfy the layout version of the prior\n    // software.  Proceed with using the current layout version.\n    return currentLV;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock": "  public void writeUnlock() {\n    final boolean needReport = fsLock.getWriteHoldCount() == 1 &&\n        fsLock.isWriteLockedByCurrentThread();\n    final long writeLockInterval = monotonicNow() - writeLockHeldTimeStamp;\n\n    this.fsLock.writeLock().unlock();\n\n    if (needReport && writeLockInterval >= WRITELOCK_REPORTING_THRESHOLD) {\n      LOG.info(\"FSNamesystem write lock held for \" + writeLockInterval +\n          \" ms via\\n\" + StringUtils.getStackTrace(Thread.currentThread()));\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.isInSafeMode": "  public boolean isInSafeMode() {\n    return isInManualOrResourceLowSafeMode() || blockManager.isInSafeMode();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFSImage": "  public FSImage getFSImage() {\n    return fsImage;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startSecretManagerIfNecessary": "  public void startSecretManagerIfNecessary() {\n    assert hasWriteLock() : \"Starting secret manager needs write lock\";\n    boolean shouldRun = shouldUseDelegationTokens() &&\n      !isInSafeMode() && getEditLog().isOpenForWrite();\n    boolean running = dtSecretManager.isRunning();\n    if (shouldRun && !running) {\n      startSecretManager();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock": "  public void writeLock() {\n    this.fsLock.writeLock().lock();\n    if (fsLock.getWriteHoldCount() == 1) {\n      writeLockHeldTimeStamp = monotonicNow();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.startActiveServices": "    public void startActiveServices() throws IOException {\n      try {\n        namesystem.startActiveServices();\n        startTrashEmptier(getConf());\n      } catch (Throwable t) {\n        doImmediateShutdown(t);\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.startTrashEmptier": "  private void startTrashEmptier(final Configuration conf) throws IOException {\n    long trashInterval =\n        conf.getLong(FS_TRASH_INTERVAL_KEY, FS_TRASH_INTERVAL_DEFAULT);\n    if (trashInterval == 0) {\n      return;\n    } else if (trashInterval < 0) {\n      throw new IOException(\"Cannot start trash emptier with negative interval.\"\n          + \" Set \" + FS_TRASH_INTERVAL_KEY + \" to a positive value.\");\n    }\n    \n    // This may be called from the transitionToActive code path, in which\n    // case the current user is the administrator, not the NN. The trash\n    // emptier needs to run as the NN. See HDFS-3972.\n    FileSystem fs = SecurityUtil.doAsLoginUser(\n        new PrivilegedExceptionAction<FileSystem>() {\n          @Override\n          public FileSystem run() throws IOException {\n            return FileSystem.get(conf);\n          }\n        });\n    this.emptier = new Thread(new Trash(fs, conf).getEmptier(), \"Trash Emptier\");\n    this.emptier.setDaemon(true);\n    this.emptier.start();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.doImmediateShutdown": "  protected synchronized void doImmediateShutdown(Throwable t)\n      throws ExitException {\n    String message = \"Error encountered requiring NN shutdown. \" +\n        \"Shutting down immediately.\";\n    try {\n      LOG.error(message, t);\n    } catch (Throwable ignored) {\n      // This is unlikely to happen, but there's nothing we can do if it does.\n    }\n    terminate(1, t);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState": "  public void enterState(HAContext context) throws ServiceFailedException {\n    try {\n      context.startActiveServices();\n    } catch (IOException e) {\n      throw new ServiceFailedException(\"Failed to start active services\", e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal": "  protected final void setStateInternal(final HAContext context, final HAState s)\n      throws ServiceFailedException {\n    prepareToExitState(context);\n    s.prepareToEnterState(context);\n    context.writeLock();\n    try {\n      exitState(context);\n      context.setState(s);\n      s.enterState(context);\n      s.updateLastHATransitionTime();\n    } finally {\n      context.writeUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.exitState": "  public abstract void exitState(final HAContext context)\n      throws ServiceFailedException;\n\n  /**\n   * Move from the existing state to a new state\n   * @param context HA context\n   * @param s new state\n   * @throws ServiceFailedException on failure to transition to new state.\n   */\n  public void setState(HAContext context, HAState s) throws ServiceFailedException {\n    if (this == s) { // Aleady in the new state\n      return;\n    }\n    throw new ServiceFailedException(\"Transtion from state \" + this + \" to \"\n        + s + \" is not allowed.\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.setState": "  public void setState(HAContext context, HAState s) throws ServiceFailedException {\n    if (this == s) { // Aleady in the new state\n      return;\n    }\n    throw new ServiceFailedException(\"Transtion from state \" + this + \" to \"\n        + s + \" is not allowed.\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.updateLastHATransitionTime": "  private void updateLastHATransitionTime() {\n    lastHATransitionTime = Time.now();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.prepareToExitState": "  public void prepareToExitState(final HAContext context)\n      throws ServiceFailedException {}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.enterState": "  public abstract void enterState(final HAContext context)\n      throws ServiceFailedException;\n\n  /**\n   * Method to be overridden by subclasses to prepare to exit a state.\n   * This method is called <em>without</em> the context being locked.\n   * This is used by the standby state to cancel any checkpoints\n   * that are going on. It can also be used to check any preconditions\n   * for the state transition.\n   * \n   * This method should not make any destructuve changes to the state\n   * (eg stopping threads) since {@link #prepareToEnterState(HAContext)}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.prepareToEnterState": "  public void prepareToEnterState(final HAContext context)\n      throws ServiceFailedException {}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState": "  public void setState(HAContext context, HAState s) throws ServiceFailedException {\n    if (s == NameNode.ACTIVE_STATE) {\n      setStateInternal(context, s);\n      return;\n    }\n    super.setState(context, s);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive": "  synchronized void transitionToActive() \n      throws ServiceFailedException, AccessControlException {\n    namesystem.checkSuperuserPrivilege();\n    if (!haEnabled) {\n      throw new ServiceFailedException(\"HA for namenode is not enabled\");\n    }\n    state.setState(haContext, ACTIVE_STATE);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.setState": "    public void setState(HAState s) {\n      state = s;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive": "  public synchronized void transitionToActive(StateChangeRequestInfo req) \n      throws ServiceFailedException, AccessControlException, IOException {\n    checkNNStartup();\n    nn.checkHaStateChange(req);\n    nn.transitionToActive();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkNNStartup": "  private void checkNNStartup() throws IOException {\n    if (!this.nn.isStarted()) {\n      String message = NameNode.composeNotStartedMessage(this.nn.getRole());\n      throw new RetriableException(message);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive": "  public TransitionToActiveResponseProto transitionToActive(\n      RpcController controller, TransitionToActiveRequestProto request)\n      throws ServiceException {\n    try {\n      server.transitionToActive(convert(request.getReqInfo()));\n      return TRANSITION_TO_ACTIVE_RESP;\n    } catch(IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.convert": "  private StateChangeRequestInfo convert(HAStateChangeRequestInfoProto proto) {\n    RequestSource src;\n    switch (proto.getReqSource()) {\n    case REQUEST_BY_USER:\n      src = RequestSource.REQUEST_BY_USER;\n      break;\n    case REQUEST_BY_USER_FORCED:\n      src = RequestSource.REQUEST_BY_USER_FORCED;\n      break;\n    case REQUEST_BY_ZKFC:\n      src = RequestSource.REQUEST_BY_ZKFC;\n      break;\n    default:\n      LOG.warn(\"Unknown request source: \" + proto.getReqSource());\n      src = null;\n    }\n    \n    return new StateChangeRequestInfo(src);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcWritable.Buffer request = (RpcWritable.Buffer) writableRequest;\n        RequestHeaderProto rpcRequest =\n            request.getValue(RequestHeaderProto.getDefaultInstance());\n        String methodName = rpcRequest.getMethodName();\n\n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = request.getValue(prototype);\n\n        Message result;\n        long startTime = Time.now();\n        int qTime = (int) (startTime - receiveTime);\n        Exception exception = null;\n        try {\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n        } catch (ServiceException e) {\n          exception = (Exception) e.getCause();\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          exception = e;\n          throw e;\n        } finally {\n          int processingTime = (int) (Time.now() - startTime);\n          if (LOG.isDebugEnabled()) {\n            String msg = \"Served: \" + methodName + \" queueTime= \" + qTime +\n                \" procesingTime= \" + processingTime;\n            if (exception != null) {\n              msg += \" exception= \" + exception.getClass().getSimpleName();\n            }\n            LOG.debug(msg);\n          }\n          String detailedMetricsName = (exception == null) ?\n              methodName :\n              exception.getClass().getSimpleName();\n          server.updateMetrics(detailedMetricsName, qTime, processingTime);\n        }\n        return RpcWritable.wrap(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getProtocolImpl": "      private static ProtoClassProtoImpl getProtocolImpl(RPC.Server server,\n          String protoName, long clientVersion) throws RpcServerException {\n        ProtoNameVer pv = new ProtoNameVer(protoName, clientVersion);\n        ProtoClassProtoImpl impl = \n            server.getProtocolImplMap(RPC.RpcKind.RPC_PROTOCOL_BUFFER).get(pv);\n        if (impl == null) { // no match for Protocol AND Version\n          VerProtocolImpl highest = \n              server.getHighestSupportedProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER, \n                  protoName);\n          if (highest == null) {\n            throw new RpcNoSuchProtocolException(\n                \"Unknown protocol: \" + protoName);\n          }\n          // protocol supported but not the version that client wants\n          throw new RPC.VersionMismatch(protoName, clientVersion,\n              highest.version);\n        }\n        return impl;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.call": "    public Writable call(RPC.RpcKind rpcKind, String protocol,\n        Writable rpcRequest, long receiveTime) throws Exception {\n      return getRpcInvoker(rpcKind).call(this, protocol, rpcRequest,\n          receiveTime);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(Call call,\n      RpcResponseHeaderProto header, Writable rv) throws IOException {\n    ResponseBuffer buf = responseBuffer.get().reset();\n    try {\n      RpcWritable.wrap(header).writeTo(buf);\n      if (rv != null) {\n        RpcWritable.wrap(rv).writeTo(buf);\n      }\n      call.setResponse(ByteBuffer.wrap(buf.toByteArray()));\n    } finally {\n      // Discard a large buf and reset it back to smaller size\n      // to free up heap.\n      if (buf.capacity() > maxRespSize) {\n        LOG.warn(\"Large response size \" + buf.size() + \" for call \"\n            + call.toString());\n        buf.setCapacity(INITIAL_RESP_BUF_SIZE);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.sendResponse": "    private void sendResponse(Call call) throws IOException {\n      responder.doRespond(call);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.remove": "    private boolean remove(Connection connection) {\n      boolean removed = connections.remove(connection);\n      if (removed) {\n        count.getAndDecrement();\n      }\n      return removed;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeAll": "    void closeAll() {\n      // use a copy of the connections to be absolutely sure the concurrent\n      // iterator doesn't miss a connection\n      for (Connection connection : toArray()) {\n        close(connection);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getRpcErrorCodeProto": "    public RpcErrorCodeProto getRpcErrorCodeProto() {\n      return errCode;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    boolean close(Connection connection) {\n      boolean exists = remove(connection);\n      if (exists) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(Thread.currentThread().getName() +\n              \": disconnecting client \" + connection +\n              \". Number of active connections: \"+ size());\n        }\n        // only close if actually removed to avoid double-closing due\n        // to possible races\n        connection.close();\n      }\n      return exists;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(RPC.RpcKind rpcKind, String protocol,\n      Writable param, long receiveTime) throws Exception;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param protocolName - the protocol\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  private void authorize(UserGroupInformation user, String protocolName,\n      InetAddress addr) throws AuthorizationException {\n    if (authorize) {\n      if (protocolName == null) {\n        throw new AuthorizationException(\"Null protocol not authorized\");\n      }\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(protocolName, getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         protocolName);\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isValid() && key.isWritable()) {\n                  doAsyncWrite(key);\n              }\n            } catch (IOException e) {\n              LOG.info(Thread.currentThread().getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = Time.now();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<Call> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<Call>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              Call call = (Call)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n          \n          for(Call call : calls) {\n            doPurge(call, now);\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeIdle": "    synchronized void closeIdle(boolean scanAll) {\n      long minLastContact = Time.now() - maxIdleTime;\n      // concurrent iterator might miss new connections added\n      // during the iteration, but that's ok because they won't\n      // be idle yet anyway and will be caught on next scan\n      int closed = 0;\n      for (Connection connection : connections) {\n        // stop if connections dropped below threshold unless scanning all\n        if (!scanAll && size() < idleScanThreshold) {\n          break;\n        }\n        // stop if not scanning all and max connections are closed\n        if (connection.isIdle() &&\n            connection.getLastContact() < minLastContact &&\n            close(connection) &&\n            !scanAll && (++closed == maxIdleToClose)) {\n          break;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.startIdleScan": "    void startIdleScan() {\n      scheduleIdleScanTask();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.logException": "  void logException(Log logger, Throwable e, Call call) {\n    if (exceptionsHandler.isSuppressedLog(e.getClass())) {\n      return; // Log nothing.\n    }\n\n    final String logMsg = Thread.currentThread().getName() + \", call \" + call;\n    if (exceptionsHandler.isTerseLog(e.getClass())) {\n      // Don't log the whole stack trace. Way too noisy!\n      logger.info(logMsg + \": \" + e);\n    } else if (e instanceof RuntimeException || e instanceof Error) {\n      // These exception types indicate something is probably wrong\n      // on the server side, as opposed to just a normal exceptional\n      // result.\n      logger.warn(logMsg, e);\n    } else {\n      logger.info(logMsg, e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.stopIdleScan": "    void stopIdleScan() {\n      idleScanTimer.cancel();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws InterruptedException, IOException,  OutOfMemoryError {\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        channel.socket().setKeepAlive(true);\n        \n        Reader reader = getReader();\n        Connection c = connectionManager.register(channel);\n        // If the connectionManager can't take it, close the connection.\n        if (c == null) {\n          if (channel.isOpen()) {\n            IOUtils.cleanup(null, channel);\n          }\n          continue;\n        }\n        key.attach(c);  // so closeCurrentConnection can get the object\n        reader.addConnection(c);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.scheduleIdleScanTask": "    private void scheduleIdleScanTask() {\n      if (!running) {\n        return;\n      }\n      TimerTask idleScanTask = new TimerTask(){\n        @Override\n        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }\n      };\n      idleScanTimer.schedule(idleScanTask, idleScanInterval);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirConcatOp.unprotectedConcat": "  static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,\n      INodeFile[] srcList, long timestamp) throws IOException {\n    assert fsd.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"\n          + targetIIP.getPath());\n    }\n\n    final INodeFile trgInode = targetIIP.getLastINode().asFile();\n    QuotaCounts deltas = computeQuotaDeltas(fsd, trgInode, srcList);\n    verifyQuota(fsd, targetIIP, deltas);\n\n    // the target file can be included in a snapshot\n    trgInode.recordModification(targetIIP.getLatestSnapshotId());\n    INodeDirectory trgParent = targetIIP.getINode(-2).asDirectory();\n    trgInode.concatBlocks(srcList, fsd.getBlockManager());\n\n    // since we are in the same dir - we can use same parent to remove files\n    int count = 0;\n    for (INodeFile nodeToRemove : srcList) {\n      if(nodeToRemove != null) {\n        nodeToRemove.clearBlocks();\n        nodeToRemove.getParent().removeChild(nodeToRemove);\n        fsd.getINodeMap().remove(nodeToRemove);\n        count++;\n      }\n    }\n\n    trgInode.setModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    trgParent.updateModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    // update quota on the parent directory with deltas\n    FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1, deltas);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirConcatOp.verifyQuota": "  private static void verifyQuota(FSDirectory fsd, INodesInPath targetIIP,\n      QuotaCounts deltas) throws QuotaExceededException {\n    if (!fsd.getFSNamesystem().isImageLoaded() || fsd.shouldSkipQuotaChecks()) {\n      // Do not check quota if editlog is still being processed\n      return;\n    }\n    FSDirectory.verifyQuota(targetIIP, targetIIP.length() - 1, deltas, null);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirConcatOp.computeQuotaDeltas": "  private static QuotaCounts computeQuotaDeltas(FSDirectory fsd,\n      INodeFile target, INodeFile[] srcList) {\n    QuotaCounts deltas = new QuotaCounts.Builder().build();\n    final short targetRepl = target.getPreferredBlockReplication();\n    for (INodeFile src : srcList) {\n      short srcRepl = src.getFileReplication();\n      long fileSize = src.computeFileSize();\n      if (targetRepl != srcRepl) {\n        deltas.addStorageSpace(fileSize * (targetRepl - srcRepl));\n        BlockStoragePolicy bsp =\n            fsd.getBlockStoragePolicySuite().getPolicy(src.getStoragePolicyID());\n        if (bsp != null) {\n          List<StorageType> srcTypeChosen = bsp.chooseStorageTypes(srcRepl);\n          for (StorageType t : srcTypeChosen) {\n            if (t.supportTypeQuota()) {\n              deltas.addTypeSpace(t, -fileSize);\n            }\n          }\n          List<StorageType> targetTypeChosen = bsp.chooseStorageTypes(targetRepl);\n          for (StorageType t : targetTypeChosen) {\n            if (t.supportTypeQuota()) {\n              deltas.addTypeSpace(t, fileSize);\n            }\n          }\n        }\n      }\n    }\n    return deltas;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFSNamesystem": "  FSNamesystem getFSNamesystem() {\n    return namesystem;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.isUnderConstruction": "  public boolean isUnderConstruction() {\n    return getFileUnderConstructionFeature() != null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.getFileUnderConstructionFeature": "  public final FileUnderConstructionFeature getFileUnderConstructionFeature() {\n    return getFeature(FileUnderConstructionFeature.class);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetQuota": "  static INodeDirectory unprotectedSetQuota(\n      FSDirectory fsd, String src, long nsQuota, long ssQuota, StorageType type)\n      throws FileNotFoundException, PathIsNotDirectoryException,\n      QuotaExceededException, UnresolvedLinkException,\n      SnapshotAccessControlException, UnsupportedActionException {\n    assert fsd.hasWriteLock();\n    // sanity check\n    if ((nsQuota < 0 && nsQuota != HdfsConstants.QUOTA_DONT_SET &&\n         nsQuota != HdfsConstants.QUOTA_RESET) ||\n        (ssQuota < 0 && ssQuota != HdfsConstants.QUOTA_DONT_SET &&\n          ssQuota != HdfsConstants.QUOTA_RESET)) {\n      throw new IllegalArgumentException(\"Illegal value for nsQuota or \" +\n                                         \"ssQuota : \" + nsQuota + \" and \" +\n                                         ssQuota);\n    }\n    // sanity check for quota by storage type\n    if ((type != null) && (!fsd.isQuotaByStorageTypeEnabled() ||\n        nsQuota != HdfsConstants.QUOTA_DONT_SET)) {\n      throw new UnsupportedActionException(\n          \"Failed to set quota by storage type because either\" +\n          DFS_QUOTA_BY_STORAGETYPE_ENABLED_KEY + \" is set to \" +\n          fsd.isQuotaByStorageTypeEnabled() + \" or nsQuota value is illegal \" +\n          nsQuota);\n    }\n\n    String srcs = FSDirectory.normalizePath(src);\n    final INodesInPath iip = fsd.getINodesInPath4Write(srcs, true);\n    INodeDirectory dirNode = INodeDirectory.valueOf(iip.getLastINode(), srcs);\n    if (dirNode.isRoot() && nsQuota == HdfsConstants.QUOTA_RESET) {\n      throw new IllegalArgumentException(\"Cannot clear namespace quota on root.\");\n    } else { // a directory inode\n      final QuotaCounts oldQuota = dirNode.getQuotaCounts();\n      final long oldNsQuota = oldQuota.getNameSpace();\n      final long oldSsQuota = oldQuota.getStorageSpace();\n\n      if (nsQuota == HdfsConstants.QUOTA_DONT_SET) {\n        nsQuota = oldNsQuota;\n      }\n      if (ssQuota == HdfsConstants.QUOTA_DONT_SET) {\n        ssQuota = oldSsQuota;\n      }\n\n      // unchanged space/namespace quota\n      if (type == null && oldNsQuota == nsQuota && oldSsQuota == ssQuota) {\n        return null;\n      }\n\n      // unchanged type quota\n      if (type != null) {\n          EnumCounters<StorageType> oldTypeQuotas = oldQuota.getTypeSpaces();\n          if (oldTypeQuotas != null && oldTypeQuotas.get(type) == ssQuota) {\n              return null;\n          }\n      }\n\n      final int latest = iip.getLatestSnapshotId();\n      dirNode.recordModification(latest);\n      dirNode.setQuota(fsd.getBlockStoragePolicySuite(), nsQuota, ssQuota, type);\n      return dirNode;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setQuota": "  static void setQuota(FSDirectory fsd, String src, long nsQuota, long ssQuota,\n      StorageType type) throws IOException {\n    if (fsd.isPermissionEnabled()) {\n      FSPermissionChecker pc = fsd.getPermissionChecker();\n      pc.checkSuperuserPrivilege();\n    }\n\n    fsd.writeLock();\n    try {\n      INodeDirectory changed = unprotectedSetQuota(fsd, src, nsQuota, ssQuota, type);\n      if (changed != null) {\n        final QuotaCounts q = changed.getQuotaCounts();\n        if (type == null) {\n          fsd.getEditLog().logSetQuota(src, q.getNameSpace(), q.getStorageSpace());\n        } else {\n          fsd.getEditLog().logSetQuotaByStorageType(\n              src, q.getTypeSpaces().get(type), type);\n        }\n      }\n    } finally {\n      fsd.writeUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.deleteForEditLog": "  static void deleteForEditLog(FSDirectory fsd, String src, long mtime)\n      throws IOException {\n    assert fsd.hasWriteLock();\n    FSNamesystem fsn = fsd.getFSNamesystem();\n    BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n    List<INode> removedINodes = new ChunkedArrayList<>();\n    List<Long> removedUCFiles = new ChunkedArrayList<>();\n\n    final INodesInPath iip = fsd.getINodesInPath4Write(\n        FSDirectory.normalizePath(src), false);\n    if (!deleteAllowed(iip, src)) {\n      return;\n    }\n    List<INodeDirectory> snapshottableDirs = new ArrayList<>();\n    FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n    boolean filesRemoved = unprotectedDelete(fsd, iip,\n        new ReclaimContext(fsd.getBlockStoragePolicySuite(),\n            collectedBlocks, removedINodes, removedUCFiles),\n        mtime);\n    fsn.removeSnapshottableDirs(snapshottableDirs);\n\n    if (filesRemoved) {\n      fsn.removeLeasesAndINodes(removedUCFiles, removedINodes, false);\n      fsn.getBlockManager().removeBlocksAndUpdateSafemodeTotal(collectedBlocks);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.unprotectedDelete": "  private static boolean unprotectedDelete(FSDirectory fsd, INodesInPath iip,\n      ReclaimContext reclaimContext, long mtime) {\n    assert fsd.hasWriteLock();\n\n    // check if target node exists\n    INode targetNode = iip.getLastINode();\n    if (targetNode == null) {\n      return false;\n    }\n\n    // record modification\n    final int latestSnapshot = iip.getLatestSnapshotId();\n    targetNode.recordModification(latestSnapshot);\n\n    // Remove the node from the namespace\n    long removed = fsd.removeLastINode(iip);\n    if (removed == -1) {\n      return false;\n    }\n\n    // set the parent's modification time\n    final INodeDirectory parent = targetNode.getParent();\n    parent.updateModificationTime(mtime, latestSnapshot);\n\n    // collect block and update quota\n    if (!targetNode.isInLatestSnapshot(latestSnapshot)) {\n      targetNode.destroyAndCollectBlocks(reclaimContext);\n    } else {\n      targetNode.cleanSubtree(reclaimContext, CURRENT_STATE_ID, latestSnapshot);\n    }\n\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.unprotectedDelete: \"\n          + iip.getPath() + \" is removed\");\n    }\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.deleteAllowed": "  private static boolean deleteAllowed(final INodesInPath iip,\n      final String src) {\n    if (iip.length() < 1 || iip.getLastINode() == null) {\n      if (NameNode.stateChangeLog.isDebugEnabled()) {\n        NameNode.stateChangeLog.debug(\n            \"DIR* FSDirectory.unprotectedDelete: failed to remove \"\n                + src + \" because it does not exist\");\n      }\n      return false;\n    } else if (iip.length() == 1) { // src is the root\n      NameNode.stateChangeLog.warn(\n          \"DIR* FSDirectory.unprotectedDelete: failed to remove \" + src +\n              \" because the root is not allowed to be deleted\");\n      return false;\n    }\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getINodesInPath4Write": "  INodesInPath getINodesInPath4Write(String src, boolean resolveLink)\n          throws UnresolvedLinkException, SnapshotAccessControlException {\n    final byte[][] components = INode.getPathComponents(src);\n    INodesInPath inodesInPath = INodesInPath.resolve(rootDir, components,\n        resolveLink);\n    if (inodesInPath.isSnapshot()) {\n      throw new SnapshotAccessControlException(\n              \"Modification on a read-only snapshot is disallowed\");\n    }\n    return inodesInPath;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getPathComponents": "  public static byte[][] getPathComponents(INode inode) {\n    List<byte[]> components = new ArrayList<byte[]>();\n    components.add(0, inode.getLocalNameBytes());\n    while(inode.getParent() != null) {\n      components.add(0, inode.getParent().getLocalNameBytes());\n      inode = inode.getParent();\n    }\n    return components.toArray(new byte[components.size()][]);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirForEditLog": "  static void mkdirForEditLog(FSDirectory fsd, long inodeId, String src,\n      PermissionStatus permissions, List<AclEntry> aclEntries, long timestamp)\n      throws QuotaExceededException, UnresolvedLinkException, AclException,\n      FileAlreadyExistsException {\n    assert fsd.hasWriteLock();\n    INodesInPath iip = fsd.getINodesInPath(src, false);\n    final byte[] localName = iip.getLastLocalName();\n    final INodesInPath existing = iip.getParentINodesInPath();\n    Preconditions.checkState(existing.getLastINode() != null);\n    unprotectedMkdir(fsd, inodeId, existing, localName, permissions, aclEntries,\n        timestamp);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.unprotectedMkdir": "  private static INodesInPath unprotectedMkdir(FSDirectory fsd, long inodeId,\n      INodesInPath parent, byte[] name, PermissionStatus permission,\n      List<AclEntry> aclEntries, long timestamp)\n      throws QuotaExceededException, AclException, FileAlreadyExistsException {\n    assert fsd.hasWriteLock();\n    assert parent.getLastINode() != null;\n    if (!parent.getLastINode().isDirectory()) {\n      throw new FileAlreadyExistsException(\"Parent path is not a directory: \" +\n          parent.getPath() + \" \" + DFSUtil.bytes2String(name));\n    }\n    final INodeDirectory dir = new INodeDirectory(inodeId, name, permission,\n        timestamp);\n\n    INodesInPath iip = fsd.addLastINode(parent, dir, true);\n    if (iip != null && aclEntries != null) {\n      AclStorage.updateINodeAcl(dir, aclEntries, Snapshot.CURRENT_STATE_ID);\n    }\n    return iip;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirXAttrOp.unprotectedSetXAttrs": "  static INode unprotectedSetXAttrs(\n      FSDirectory fsd, final String src, final List<XAttr> xAttrs,\n      final EnumSet<XAttrSetFlag> flag)\n      throws IOException {\n    assert fsd.hasWriteLock();\n    INodesInPath iip = fsd.getINodesInPath4Write(FSDirectory.normalizePath(src),\n        true);\n    INode inode = FSDirectory.resolveLastINode(iip);\n    int snapshotId = iip.getLatestSnapshotId();\n    List<XAttr> existingXAttrs = XAttrStorage.readINodeXAttrs(inode);\n    List<XAttr> newXAttrs = setINodeXAttrs(fsd, existingXAttrs, xAttrs, flag);\n    final boolean isFile = inode.isFile();\n\n    for (XAttr xattr : newXAttrs) {\n      final String xaName = XAttrHelper.getPrefixedName(xattr);\n\n      /*\n       * If we're adding the encryption zone xattr, then add src to the list\n       * of encryption zones.\n       */\n      if (CRYPTO_XATTR_ENCRYPTION_ZONE.equals(xaName)) {\n        final HdfsProtos.ZoneEncryptionInfoProto ezProto =\n            HdfsProtos.ZoneEncryptionInfoProto.parseFrom(xattr.getValue());\n        fsd.ezManager.addEncryptionZone(inode.getId(),\n            PBHelperClient.convert(ezProto.getSuite()),\n            PBHelperClient.convert(ezProto.getCryptoProtocolVersion()),\n            ezProto.getKeyName());\n      }\n\n      if (!isFile && SECURITY_XATTR_UNREADABLE_BY_SUPERUSER.equals(xaName)) {\n        throw new IOException(\"Can only set '\" +\n            SECURITY_XATTR_UNREADABLE_BY_SUPERUSER + \"' on a file.\");\n      }\n    }\n\n    XAttrStorage.updateINodeXAttrs(inode, newXAttrs, snapshotId);\n    return inode;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirXAttrOp.setINodeXAttrs": "  static List<XAttr> setINodeXAttrs(\n      FSDirectory fsd, final List<XAttr> existingXAttrs,\n      final List<XAttr> toSet, final EnumSet<XAttrSetFlag> flag)\n      throws IOException {\n    // Check for duplicate XAttrs in toSet\n    // We need to use a custom comparator, so using a HashSet is not suitable\n    for (int i = 0; i < toSet.size(); i++) {\n      for (int j = i + 1; j < toSet.size(); j++) {\n        if (toSet.get(i).equalsIgnoreValue(toSet.get(j))) {\n          throw new IOException(\"Cannot specify the same XAttr to be set \" +\n              \"more than once\");\n        }\n      }\n    }\n\n    // Count the current number of user-visible XAttrs for limit checking\n    int userVisibleXAttrsNum = 0; // Number of user visible xAttrs\n\n    // The XAttr list is copied to an exactly-sized array when it's stored,\n    // so there's no need to size it precisely here.\n    int newSize = (existingXAttrs != null) ? existingXAttrs.size() : 0;\n    newSize += toSet.size();\n    List<XAttr> xAttrs = Lists.newArrayListWithCapacity(newSize);\n\n    // Check if the XAttr already exists to validate with the provided flag\n    for (XAttr xAttr: toSet) {\n      boolean exist = false;\n      if (existingXAttrs != null) {\n        for (XAttr a : existingXAttrs) {\n          if (a.equalsIgnoreValue(xAttr)) {\n            exist = true;\n            break;\n          }\n        }\n      }\n      XAttrSetFlag.validate(xAttr.getName(), exist, flag);\n      // add the new XAttr since it passed validation\n      xAttrs.add(xAttr);\n      if (isUserVisible(xAttr)) {\n        userVisibleXAttrsNum++;\n      }\n    }\n\n    // Add the existing xattrs back in, if they weren't already set\n    if (existingXAttrs != null) {\n      for (XAttr existing : existingXAttrs) {\n        boolean alreadySet = false;\n        for (XAttr set : toSet) {\n          if (set.equalsIgnoreValue(existing)) {\n            alreadySet = true;\n            break;\n          }\n        }\n        if (!alreadySet) {\n          xAttrs.add(existing);\n          if (isUserVisible(existing)) {\n            userVisibleXAttrsNum++;\n          }\n        }\n      }\n    }\n\n    if (userVisibleXAttrsNum > fsd.getInodeXAttrsLimit()) {\n      throw new IOException(\"Cannot add additional XAttr to inode, \"\n          + \"would exceed limit of \" + fsd.getInodeXAttrsLimit());\n    }\n\n    return xAttrs;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetStoragePolicy": "  static void unprotectedSetStoragePolicy(FSDirectory fsd, BlockManager bm,\n      INodesInPath iip, final byte policyId)\n      throws IOException {\n    assert fsd.hasWriteLock();\n    final INode inode = iip.getLastINode();\n    if (inode == null) {\n      throw new FileNotFoundException(\"File/Directory does not exist: \"\n          + iip.getPath());\n    }\n    final int snapshotId = iip.getLatestSnapshotId();\n    if (inode.isFile()) {\n      if (policyId != HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED) {\n        BlockStoragePolicy newPolicy = bm.getStoragePolicy(policyId);\n        if (newPolicy.isCopyOnCreateFile()) {\n          throw new HadoopIllegalArgumentException(\"Policy \" + newPolicy\n              + \" cannot be set after file creation.\");\n        }\n      }\n\n      BlockStoragePolicy currentPolicy =\n          bm.getStoragePolicy(inode.getLocalStoragePolicyID());\n\n      if (currentPolicy != null && currentPolicy.isCopyOnCreateFile()) {\n        throw new HadoopIllegalArgumentException(\n            \"Existing policy \" + currentPolicy.getName() +\n                \" cannot be changed after file creation.\");\n      }\n      inode.asFile().setStoragePolicyID(policyId, snapshotId);\n    } else if (inode.isDirectory()) {\n      setDirStoragePolicy(fsd, inode.asDirectory(), policyId,\n          snapshotId);\n    } else {\n      throw new FileNotFoundException(iip.getPath()\n          + \" is not a file or directory\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.getStoragePolicy": "  static BlockStoragePolicy getStoragePolicy(FSDirectory fsd, BlockManager bm,\n      String path) throws IOException {\n    FSPermissionChecker pc = fsd.getPermissionChecker();\n    byte[][] pathComponents = FSDirectory\n        .getPathComponentsForReservedPath(path);\n    fsd.readLock();\n    try {\n      path = fsd.resolvePath(pc, path, pathComponents);\n      final INodesInPath iip = fsd.getINodesInPath(path, false);\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.READ);\n      }\n      INode inode = iip.getLastINode();\n      if (inode == null) {\n        throw new FileNotFoundException(\"File/Directory does not exist: \"\n            + iip.getPath());\n      }\n      return bm.getStoragePolicy(inode.getStoragePolicyID());\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setDirStoragePolicy": "  private static void setDirStoragePolicy(\n      FSDirectory fsd, INodeDirectory inode, byte policyId,\n      int latestSnapshotId) throws IOException {\n    List<XAttr> existingXAttrs = XAttrStorage.readINodeXAttrs(inode);\n    XAttr xAttr = BlockStoragePolicySuite.buildXAttr(policyId);\n    List<XAttr> newXAttrs = null;\n    if (policyId == HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED) {\n      List<XAttr> toRemove = Lists.newArrayList();\n      toRemove.add(xAttr);\n      List<XAttr> removed = Lists.newArrayList();\n      newXAttrs = FSDirXAttrOp.filterINodeXAttrs(existingXAttrs, toRemove,\n          removed);\n    } else {\n      newXAttrs = FSDirXAttrOp.setINodeXAttrs(fsd, existingXAttrs,\n          Arrays.asList(xAttr),\n          EnumSet.of(XAttrSetFlag.CREATE, XAttrSetFlag.REPLACE));\n    }\n    XAttrStorage.updateINodeXAttrs(inode, newXAttrs, latestSnapshotId);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAclOp.unprotectedSetAcl": "  static List<AclEntry> unprotectedSetAcl(\n      FSDirectory fsd, String src, List<AclEntry> aclSpec, boolean fromEdits)\n      throws IOException {\n    assert fsd.hasWriteLock();\n    final INodesInPath iip = fsd.getINodesInPath4Write(\n        FSDirectory.normalizePath(src), true);\n\n    // ACL removal is logged to edits as OP_SET_ACL with an empty list.\n    if (aclSpec.isEmpty()) {\n      unprotectedRemoveAcl(fsd, iip);\n      return AclFeature.EMPTY_ENTRY_LIST;\n    }\n\n    INode inode = FSDirectory.resolveLastINode(iip);\n    int snapshotId = iip.getLatestSnapshotId();\n    List<AclEntry> newAcl = aclSpec;\n    if (!fromEdits) {\n      List<AclEntry> existingAcl = AclStorage.readINodeLogicalAcl(inode);\n      newAcl = AclTransformation.replaceAclEntries(existingAcl, aclSpec);\n    }\n    AclStorage.updateINodeAcl(inode, newAcl, snapshotId);\n    return newAcl;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAclOp.unprotectedRemoveAcl": "  private static void unprotectedRemoveAcl(FSDirectory fsd, INodesInPath iip)\n      throws IOException {\n    assert fsd.hasWriteLock();\n    INode inode = FSDirectory.resolveLastINode(iip);\n    int snapshotId = iip.getLatestSnapshotId();\n    AclFeature f = inode.getAclFeature();\n    if (f == null) {\n      return;\n    }\n\n    FsPermission perm = inode.getFsPermission();\n    List<AclEntry> featureEntries = AclStorage.getEntriesFromAclFeature(f);\n    if (featureEntries.get(0).getScope() == AclEntryScope.ACCESS) {\n      // Restore group permissions from the feature's entry to permission\n      // bits, overwriting the mask, which is not part of a minimal ACL.\n      AclEntry groupEntryKey = new AclEntry.Builder()\n          .setScope(AclEntryScope.ACCESS).setType(AclEntryType.GROUP).build();\n      int groupEntryIndex = Collections.binarySearch(\n          featureEntries, groupEntryKey,\n          AclTransformation.ACL_ENTRY_COMPARATOR);\n      assert groupEntryIndex >= 0;\n      FsAction groupPerm = featureEntries.get(groupEntryIndex).getPermission();\n      FsPermission newPerm = new FsPermission(perm.getUserAction(), groupPerm,\n          perm.getOtherAction(), perm.getStickyBit());\n      inode.setPermission(newPerm, snapshotId);\n    }\n\n    inode.removeAclFeature(snapshotId);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetPermission": "  static void unprotectedSetPermission(\n      FSDirectory fsd, String src, FsPermission permissions)\n      throws FileNotFoundException, UnresolvedLinkException,\n             QuotaExceededException, SnapshotAccessControlException {\n    assert fsd.hasWriteLock();\n    final INodesInPath inodesInPath = fsd.getINodesInPath4Write(src, true);\n    final INode inode = inodesInPath.getLastINode();\n    if (inode == null) {\n      throw new FileNotFoundException(\"File does not exist: \" + src);\n    }\n    int snapshotId = inodesInPath.getLatestSnapshotId();\n    inode.setPermission(permissions, snapshotId);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setPermission": "  static HdfsFileStatus setPermission(\n      FSDirectory fsd, final String srcArg, FsPermission permission)\n      throws IOException {\n    String src = srcArg;\n    if (FSDirectory.isExactReservedName(src)) {\n      throw new InvalidPathException(src);\n    }\n    FSPermissionChecker pc = fsd.getPermissionChecker();\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    INodesInPath iip;\n    fsd.writeLock();\n    try {\n      src = fsd.resolvePath(pc, src, pathComponents);\n      iip = fsd.getINodesInPath4Write(src);\n      fsd.checkOwner(pc, iip);\n      unprotectedSetPermission(fsd, src, permission);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsd.getEditLog().logSetPermissions(src, permission);\n    return fsd.getAuditFileInfo(iip);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp.unprotectedTruncate": "  private static boolean unprotectedTruncate(FSNamesystem fsn,\n      INodesInPath iip, long newLength, BlocksMapUpdateInfo collectedBlocks,\n      long mtime, QuotaCounts delta) throws IOException {\n    assert fsn.hasWriteLock();\n\n    INodeFile file = iip.getLastINode().asFile();\n    int latestSnapshot = iip.getLatestSnapshotId();\n    file.recordModification(latestSnapshot, true);\n\n    verifyQuotaForTruncate(fsn, iip, file, newLength, delta);\n\n    Set<BlockInfo> toRetain = file.getSnapshotBlocksToRetain(latestSnapshot);\n    long remainingLength = file.collectBlocksBeyondMax(newLength,\n        collectedBlocks, toRetain);\n    file.setModificationTime(mtime);\n    // return whether on a block boundary\n    return (remainingLength - newLength) == 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp.verifyQuotaForTruncate": "  private static void verifyQuotaForTruncate(FSNamesystem fsn,\n      INodesInPath iip, INodeFile file, long newLength, QuotaCounts delta)\n      throws QuotaExceededException {\n    FSDirectory fsd = fsn.getFSDirectory();\n    if (!fsn.isImageLoaded() || fsd.shouldSkipQuotaChecks()) {\n      // Do not check quota if edit log is still being processed\n      return;\n    }\n    final BlockStoragePolicy policy = fsd.getBlockStoragePolicySuite()\n        .getPolicy(file.getStoragePolicyID());\n    file.computeQuotaDeltaForTruncate(newLength, policy, delta);\n    fsd.readLock();\n    try {\n      FSDirectory.verifyQuota(iip, iip.length() - 1, delta, null);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp.prepareFileForTruncate": "  static Block prepareFileForTruncate(FSNamesystem fsn, INodesInPath iip,\n      String leaseHolder, String clientMachine, long lastBlockDelta,\n      Block newBlock) throws IOException {\n    assert fsn.hasWriteLock();\n\n    INodeFile file = iip.getLastINode().asFile();\n    assert !file.isStriped();\n    file.recordModification(iip.getLatestSnapshotId());\n    file.toUnderConstruction(leaseHolder, clientMachine);\n    assert file.isUnderConstruction() : \"inode should be under construction.\";\n    fsn.getLeaseManager().addLease(\n        file.getFileUnderConstructionFeature().getClientName(), file.getId());\n    boolean shouldRecoverNow = (newBlock == null);\n    BlockInfo oldBlock = file.getLastBlock();\n\n    boolean shouldCopyOnTruncate = shouldCopyOnTruncate(fsn, file, oldBlock);\n    if (newBlock == null) {\n      newBlock = (shouldCopyOnTruncate) ? fsn.createNewBlock(false)\n          : new Block(oldBlock.getBlockId(), oldBlock.getNumBytes(),\n          fsn.nextGenerationStamp(fsn.getBlockManager().isLegacyBlock(\n              oldBlock)));\n    }\n\n    final BlockInfo truncatedBlockUC;\n    BlockManager blockManager = fsn.getFSDirectory().getBlockManager();\n    if (shouldCopyOnTruncate) {\n      // Add new truncateBlock into blocksMap and\n      // use oldBlock as a source for copy-on-truncate recovery\n      truncatedBlockUC = new BlockInfoContiguous(newBlock,\n          file.getPreferredBlockReplication());\n      truncatedBlockUC.convertToBlockUnderConstruction(\n          BlockUCState.UNDER_CONSTRUCTION, blockManager.getStorages(oldBlock));\n      truncatedBlockUC.setNumBytes(oldBlock.getNumBytes() - lastBlockDelta);\n      truncatedBlockUC.getUnderConstructionFeature().setTruncateBlock(oldBlock);\n      file.setLastBlock(truncatedBlockUC);\n      blockManager.addBlockCollection(truncatedBlockUC, file);\n\n      NameNode.stateChangeLog.debug(\n          \"BLOCK* prepareFileForTruncate: Scheduling copy-on-truncate to new\"\n              + \" size {}  new block {} old block {}\",\n          truncatedBlockUC.getNumBytes(), newBlock, oldBlock);\n    } else {\n      // Use new generation stamp for in-place truncate recovery\n      blockManager.convertLastBlockToUnderConstruction(file, lastBlockDelta);\n      oldBlock = file.getLastBlock();\n      assert !oldBlock.isComplete() : \"oldBlock should be under construction\";\n      BlockUnderConstructionFeature uc = oldBlock.getUnderConstructionFeature();\n      uc.setTruncateBlock(new Block(oldBlock));\n      uc.getTruncateBlock().setNumBytes(oldBlock.getNumBytes() - lastBlockDelta);\n      uc.getTruncateBlock().setGenerationStamp(newBlock.getGenerationStamp());\n      truncatedBlockUC = oldBlock;\n\n      NameNode.stateChangeLog.debug(\"BLOCK* prepareFileForTruncate: \" +\n          \"{} Scheduling in-place block truncate to new size {}\",\n          uc, uc.getTruncateBlock().getNumBytes());\n    }\n    if (shouldRecoverNow) {\n      truncatedBlockUC.getUnderConstructionFeature().initializeBlockRecovery(\n          truncatedBlockUC, newBlock.getGenerationStamp());\n    }\n\n    return newBlock;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getINodesInPath": "  public INodesInPath getINodesInPath(String path, boolean resolveLink)\n      throws UnresolvedLinkException {\n    final byte[][] components = INode.getPathComponents(path);\n    return INodesInPath.resolve(rootDir, components, resolveLink);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.createFileStatusForEditLog": "  static HdfsFileStatus createFileStatusForEditLog(\n      FSDirectory fsd, String fullPath, byte[] path,\n      byte storagePolicy, int snapshot, boolean isRawPath,\n      INodesInPath iip) throws IOException {\n    INodeAttributes nodeAttrs = getINodeAttributes(\n        fsd, fullPath, path, iip.getLastINode(), snapshot);\n    return createFileStatus(fsd, path, nodeAttrs, storagePolicy,\n                            snapshot, isRawPath, iip);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getINodeAttributes": "  private static INodeAttributes getINodeAttributes(\n      FSDirectory fsd, String fullPath, byte[] path, INode node, int snapshot) {\n    return fsd.getAttributes(fullPath, path, node, snapshot);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.createFileStatus": "  static HdfsFileStatus createFileStatus(\n      FSDirectory fsd, byte[] path,\n      INodeAttributes nodeAttrs, byte storagePolicy, int snapshot,\n      boolean isRawPath, INodesInPath iip) throws IOException {\n    long size = 0;     // length is zero for directories\n    short replication = 0;\n    long blocksize = 0;\n    final boolean isEncrypted;\n    final INode node = iip.getLastINode();\n\n    final FileEncryptionInfo feInfo = isRawPath ? null : FSDirEncryptionZoneOp\n        .getFileEncryptionInfo(fsd, node, snapshot, iip);\n\n    final ErasureCodingPolicy ecPolicy = FSDirErasureCodingOp\n        .getErasureCodingPolicy(fsd.getFSNamesystem(), iip);\n\n    if (node.isFile()) {\n      final INodeFile fileNode = node.asFile();\n      size = fileNode.computeFileSize(snapshot);\n      replication = fileNode.getFileReplication(snapshot);\n      blocksize = fileNode.getPreferredBlockSize();\n      isEncrypted = (feInfo != null)\n          || (isRawPath && FSDirEncryptionZoneOp.isInAnEZ(fsd, iip));\n    } else {\n      isEncrypted = FSDirEncryptionZoneOp.isInAnEZ(fsd, iip);\n    }\n\n    int childrenNum = node.isDirectory() ?\n        node.asDirectory().getChildrenNum(snapshot) : 0;\n\n    return new HdfsFileStatus(\n        size,\n        node.isDirectory(),\n        replication,\n        blocksize,\n        node.getModificationTime(snapshot),\n        node.getAccessTime(snapshot),\n        getPermissionForFileStatus(nodeAttrs, isEncrypted),\n        nodeAttrs.getUserName(),\n        nodeAttrs.getGroupName(),\n        node.isSymlink() ? node.asSymlink().getSymlink() : null,\n        path,\n        node.getId(),\n        childrenNum,\n        feInfo,\n        storagePolicy,\n        ecPolicy);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.hasRpcIds": "  public boolean hasRpcIds() {\n    return rpcClientId != RpcConstants.DUMMY_CLIENT_ID\n        && rpcCallId != RpcConstants.INVALID_CALL_ID;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.prepareFileForAppend": "  static LocatedBlock prepareFileForAppend(final FSNamesystem fsn,\n      final INodesInPath iip, final String leaseHolder,\n      final String clientMachine, final boolean newBlock,\n      final boolean writeToEditLog, final boolean logRetryCache)\n      throws IOException {\n    assert fsn.hasWriteLock();\n\n    final INodeFile file = iip.getLastINode().asFile();\n    final QuotaCounts delta = verifyQuotaForUCBlock(fsn, file, iip);\n\n    file.recordModification(iip.getLatestSnapshotId());\n    file.toUnderConstruction(leaseHolder, clientMachine);\n\n    fsn.getLeaseManager().addLease(\n        file.getFileUnderConstructionFeature().getClientName(), file.getId());\n\n    LocatedBlock ret = null;\n    if (!newBlock) {\n      FSDirectory fsd = fsn.getFSDirectory();\n      ret = fsd.getBlockManager().convertLastBlockToUnderConstruction(file, 0);\n      if (ret != null && delta != null) {\n        Preconditions.checkState(delta.getStorageSpace() >= 0, \"appending to\"\n            + \" a block with size larger than the preferred block size\");\n        fsd.writeLock();\n        try {\n          fsd.updateCountNoQuotaCheck(iip, iip.length() - 1, delta);\n        } finally {\n          fsd.writeUnlock();\n        }\n      }\n    } else {\n      BlockInfo lastBlock = file.getLastBlock();\n      if (lastBlock != null) {\n        ExtendedBlock blk = new ExtendedBlock(fsn.getBlockPoolId(), lastBlock);\n        ret = new LocatedBlock(blk, new DatanodeInfo[0]);\n      }\n    }\n\n    if (writeToEditLog) {\n      final String path = iip.getPath();\n      if (NameNodeLayoutVersion.supports(Feature.APPEND_NEW_BLOCK,\n          fsn.getEffectiveLayoutVersion())) {\n        fsn.getEditLog().logAppendFile(path, file, newBlock, logRetryCache);\n      } else {\n        fsn.getEditLog().logOpenFile(path, file, false, logRetryCache);\n      }\n    }\n    return ret;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.verifyQuotaForUCBlock": "  private static QuotaCounts verifyQuotaForUCBlock(FSNamesystem fsn,\n      INodeFile file, INodesInPath iip) throws QuotaExceededException {\n    FSDirectory fsd = fsn.getFSDirectory();\n    if (!fsn.isImageLoaded() || fsd.shouldSkipQuotaChecks()) {\n      // Do not check quota if editlog is still being processed\n      return null;\n    }\n    if (file.getLastBlock() != null) {\n      final QuotaCounts delta = computeQuotaDeltaForUCBlock(fsn, file);\n      fsd.readLock();\n      try {\n        FSDirectory.verifyQuota(iip, iip.length() - 1, delta, null);\n        return delta;\n      } finally {\n        fsd.readUnlock();\n      }\n    }\n    return null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.toCompleteFile": "  void toCompleteFile(long mtime, int numCommittedAllowed, short minReplication) {\n    final FileUnderConstructionFeature uc = getFileUnderConstructionFeature();\n    Preconditions.checkNotNull(uc, \"File %s is not under construction\", this);\n    assertAllBlocksComplete(numCommittedAllowed, minReplication);\n    removeFeature(uc);\n    setModificationTime(mtime);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.assertAllBlocksComplete": "  private void assertAllBlocksComplete(int numCommittedAllowed,\n      short minReplication) {\n    if (blocks == null) {\n      return;\n    }\n    for (int i = 0; i < blocks.length; i++) {\n      final String err = checkBlockComplete(blocks, i, numCommittedAllowed,\n          minReplication);\n      Preconditions.checkState(err == null,\n          \"Unexpected block state: %s, file=%s (%s), blocks=%s (i=%s)\",\n          err, this, getClass().getSimpleName(), Arrays.asList(blocks), i);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirErasureCodingOp.getErasureCodingPolicy": "  static ErasureCodingPolicy getErasureCodingPolicy(final FSNamesystem fsn,\n      final INodesInPath iip) throws IOException {\n    assert fsn.hasReadLock();\n\n    return getErasureCodingPolicyForPath(fsn, iip);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirErasureCodingOp.getINodesInPath": "  private static INodesInPath getINodesInPath(final FSNamesystem fsn,\n      final String srcArg) throws IOException {\n    String src = srcArg;\n    final byte[][] pathComponents = FSDirectory\n        .getPathComponentsForReservedPath(src);\n    final FSDirectory fsd = fsn.getFSDirectory();\n    final FSPermissionChecker pc = fsn.getPermissionChecker();\n    src = fsd.resolvePath(pc, src, pathComponents);\n    INodesInPath iip = fsd.getINodesInPath(src, true);\n    if (fsn.isPermissionEnabled()) {\n      fsn.getFSDirectory().checkPathAccess(pc, iip, FsAction.READ);\n    }\n    return iip;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirErasureCodingOp.getErasureCodingPolicyForPath": "  private static ErasureCodingPolicy getErasureCodingPolicyForPath(FSNamesystem fsn,\n      INodesInPath iip) throws IOException {\n    Preconditions.checkNotNull(iip, \"INodes cannot be null\");\n    FSDirectory fsd = fsn.getFSDirectory();\n    fsd.readLock();\n    try {\n      List<INode> inodes = iip.getReadOnlyINodes();\n      for (int i = inodes.size() - 1; i >= 0; i--) {\n        final INode inode = inodes.get(i);\n        if (inode == null) {\n          continue;\n        }\n        if (inode.isFile()) {\n          byte id = inode.asFile().getErasureCodingPolicyID();\n          return id < 0 ? null : fsd.getFSNamesystem().\n              getErasureCodingPolicyManager().getPolicyByID(id);\n        }\n        // We don't allow setting EC policies on paths with a symlink. Thus\n        // if a symlink is encountered, the dir shouldn't have EC policy.\n        // TODO: properly support symlinks\n        if (inode.isSymlink()) {\n          return null;\n        }\n        final XAttrFeature xaf = inode.getXAttrFeature();\n        if (xaf != null) {\n          XAttr xattr = xaf.getXAttr(XATTR_ERASURECODING_POLICY);\n          if (xattr != null) {\n            ByteArrayInputStream bIn = new ByteArrayInputStream(xattr.getValue());\n            DataInputStream dIn = new DataInputStream(bIn);\n            String ecPolicyName = WritableUtils.readString(dIn);\n            return fsd.getFSNamesystem().getErasureCodingPolicyManager().\n                getPolicyByName(ecPolicyName);\n          }\n        }\n      }\n    } finally {\n      fsd.readUnlock();\n    }\n    return null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirSymlinkOp.unprotectedAddSymlink": "  static INodeSymlink unprotectedAddSymlink(FSDirectory fsd, INodesInPath iip,\n      byte[] localName, long id, String target, long mtime, long atime,\n      PermissionStatus perm)\n      throws UnresolvedLinkException, QuotaExceededException {\n    assert fsd.hasWriteLock();\n    final INodeSymlink symlink = new INodeSymlink(id, null, perm, mtime, atime,\n        target);\n    symlink.setLocalName(localName);\n    return fsd.addINode(iip, symlink) != null ? symlink : null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirXAttrOp.unprotectedRemoveXAttrs": "  static List<XAttr> unprotectedRemoveXAttrs(\n      FSDirectory fsd, final String src, final List<XAttr> toRemove)\n      throws IOException {\n    assert fsd.hasWriteLock();\n    INodesInPath iip = fsd.getINodesInPath4Write(\n        FSDirectory.normalizePath(src), true);\n    INode inode = FSDirectory.resolveLastINode(iip);\n    int snapshotId = iip.getLatestSnapshotId();\n    List<XAttr> existingXAttrs = XAttrStorage.readINodeXAttrs(inode);\n    List<XAttr> removedXAttrs = Lists.newArrayListWithCapacity(toRemove.size());\n    List<XAttr> newXAttrs = filterINodeXAttrs(existingXAttrs, toRemove,\n                                              removedXAttrs);\n    if (existingXAttrs.size() != newXAttrs.size()) {\n      XAttrStorage.updateINodeXAttrs(inode, newXAttrs, snapshotId);\n      return removedXAttrs;\n    }\n    return null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirXAttrOp.filterINodeXAttrs": "  static List<XAttr> filterINodeXAttrs(\n      final List<XAttr> existingXAttrs, final List<XAttr> toFilter,\n      final List<XAttr> filtered)\n    throws AccessControlException {\n    if (existingXAttrs == null || existingXAttrs.isEmpty() ||\n        toFilter == null || toFilter.isEmpty()) {\n      return existingXAttrs;\n    }\n\n    // Populate a new list with XAttrs that pass the filter\n    List<XAttr> newXAttrs =\n        Lists.newArrayListWithCapacity(existingXAttrs.size());\n    for (XAttr a : existingXAttrs) {\n      boolean add = true;\n      for (ListIterator<XAttr> it = toFilter.listIterator(); it.hasNext()\n          ;) {\n        XAttr filter = it.next();\n        Preconditions.checkArgument(\n            !KEYID_XATTR.equalsIgnoreValue(filter),\n            \"The encryption zone xattr should never be deleted.\");\n        if (UNREADABLE_BY_SUPERUSER_XATTR.equalsIgnoreValue(filter)) {\n          throw new AccessControlException(\"The xattr '\" +\n              SECURITY_XATTR_UNREADABLE_BY_SUPERUSER + \"' can not be deleted.\");\n        }\n        if (a.equalsIgnoreValue(filter)) {\n          add = false;\n          it.remove();\n          filtered.add(filter);\n          break;\n        }\n      }\n      if (add) {\n        newXAttrs.add(a);\n      }\n    }\n\n    return newXAttrs;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf": "  public static INodeFile valueOf(INode inode, String path, boolean acceptNull)\n      throws FileNotFoundException {\n    if (inode == null) {\n      if (acceptNull) {\n        return null;\n      } else {\n        throw new FileNotFoundException(\"File does not exist: \" + path);\n      }\n    }\n    if (!inode.isFile()) {\n      throw new FileNotFoundException(\"Path is not a file: \" + path);\n    }\n    return inode.asFile();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.asFile": "  public final INodeFile asFile() {\n    return this;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.isFile": "  public final boolean isFile() {\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.length": "  public int length() {\n    return inodes.length;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog": "  static INodeFile addFileForEditLog(\n      FSDirectory fsd, long id, INodesInPath existing, byte[] localName,\n      PermissionStatus permissions, List<AclEntry> aclEntries,\n      List<XAttr> xAttrs, short replication, long modificationTime, long atime,\n      long preferredBlockSize, boolean underConstruction, String clientName,\n      String clientMachine, byte storagePolicyId) {\n    final INodeFile newNode;\n    Preconditions.checkNotNull(existing);\n    assert fsd.hasWriteLock();\n    try {\n      // check if the file has an EC policy\n      ErasureCodingPolicy ecPolicy = FSDirErasureCodingOp.\n          getErasureCodingPolicy(fsd.getFSNamesystem(), existing);\n      if (ecPolicy != null) {\n        replication = ecPolicy.getId();\n      }\n      if (underConstruction) {\n        newNode = newINodeFile(id, permissions, modificationTime,\n            modificationTime, replication, preferredBlockSize, storagePolicyId,\n            ecPolicy != null);\n        newNode.toUnderConstruction(clientName, clientMachine);\n      } else {\n        newNode = newINodeFile(id, permissions, modificationTime, atime,\n            replication, preferredBlockSize, storagePolicyId, ecPolicy != null);\n      }\n      newNode.setLocalName(localName);\n      INodesInPath iip = fsd.addINode(existing, newNode);\n      if (iip != null) {\n        if (aclEntries != null) {\n          AclStorage.updateINodeAcl(newNode, aclEntries, CURRENT_STATE_ID);\n        }\n        if (xAttrs != null) {\n          XAttrStorage.updateINodeXAttrs(newNode, xAttrs, CURRENT_STATE_ID);\n        }\n        return newNode;\n      }\n    } catch (IOException e) {\n      if(NameNode.stateChangeLog.isDebugEnabled()) {\n        NameNode.stateChangeLog.debug(\n            \"DIR* FSDirectory.unprotectedAddFile: exception when add \"\n                + existing.getPath() + \" to the file system\", e);\n      }\n    }\n    return null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.newINodeFile": "  private static INodeFile newINodeFile(long id, PermissionStatus permissions,\n      long mtime, long atime, short replication, long preferredBlockSize,\n      boolean isStriped) {\n    return newINodeFile(id, permissions, mtime, atime, replication,\n        preferredBlockSize, (byte)0, isStriped);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetOwner": "  static void unprotectedSetOwner(\n      FSDirectory fsd, String src, String username, String groupname)\n      throws FileNotFoundException, UnresolvedLinkException,\n      QuotaExceededException, SnapshotAccessControlException {\n    assert fsd.hasWriteLock();\n    final INodesInPath inodesInPath = fsd.getINodesInPath4Write(src, true);\n    INode inode = inodesInPath.getLastINode();\n    if (inode == null) {\n      throw new FileNotFoundException(\"File does not exist: \" + src);\n    }\n    if (username != null) {\n      inode = inode.setUser(username, inodesInPath.getLatestSnapshotId());\n    }\n    if (groupname != null) {\n      inode.setGroup(groupname, inodesInPath.getLatestSnapshotId());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getLastINode": "  public INode getLastINode() {\n    return getINode(-1);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getINode": "  public INode getINode(int i) {\n    if (inodes == null || inodes.length == 0) {\n      throw new NoSuchElementException(\"inodes is null or empty\");\n    }\n    int index = i >= 0 ? i : inodes.length + i;\n    if (index < inodes.length && index >= 0) {\n      return inodes[index];\n    } else {\n      throw new NoSuchElementException(\"inodes.length == \" + inodes.length);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getLastLocalName": "  byte[] getLastLocalName() {\n    return path[path.length - 1];\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetTimes": "  private static boolean unprotectedSetTimes(\n      FSDirectory fsd, INode inode, long mtime, long atime, boolean force,\n      int latest) throws QuotaExceededException {\n    assert fsd.hasWriteLock();\n    boolean status = false;\n    if (mtime != -1) {\n      inode = inode.setModificationTime(mtime, latest);\n      status = true;\n    }\n    if (atime != -1) {\n      long inodeTime = inode.getAccessTime();\n\n      // if the last access time update was within the last precision interval, then\n      // no need to store access time\n      if (atime <= inodeTime + fsd.getAccessTimePrecision() && !force) {\n        status =  false;\n      } else {\n        inode.setAccessTime(atime, latest);\n        status = true;\n      }\n    }\n    return status;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getExistingINodes": "  public INodesInPath getExistingINodes() {\n    Preconditions.checkState(!isSnapshot());\n    int i = 0;\n    for (; i < inodes.length; i++) {\n      if (inodes[i] == null) {\n        break;\n      }\n    }\n    INode[] existing = new INode[i];\n    byte[][] existingPath = new byte[i][];\n    System.arraycopy(inodes, 0, existing, 0, i);\n    System.arraycopy(path, 0, existingPath, 0, i);\n    return new INodesInPath(existing, existingPath, false, snapshotId);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.isSnapshot": "  boolean isSnapshot() {\n    return this.isSnapshot;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.replace": "  public static INodesInPath replace(INodesInPath iip, int pos, INode inode) {\n    Preconditions.checkArgument(iip.length() > 0 && pos > 0 // no for root\n        && pos < iip.length());\n    if (iip.getINode(pos) == null) {\n      Preconditions.checkState(iip.getINode(pos - 1) != null);\n    }\n    INode[] inodes = new INode[iip.inodes.length];\n    System.arraycopy(iip.inodes, 0, inodes, 0, inodes.length);\n    inodes[pos] = inode;\n    return new INodesInPath(inodes, iip.path, iip.isSnapshot, iip.snapshotId);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetReplication": "  static BlockInfo[] unprotectedSetReplication(\n      FSDirectory fsd, String src, short replication)\n      throws QuotaExceededException, UnresolvedLinkException,\n      SnapshotAccessControlException, UnsupportedActionException {\n    assert fsd.hasWriteLock();\n\n    final BlockManager bm = fsd.getBlockManager();\n    final INodesInPath iip = fsd.getINodesInPath4Write(src, true);\n    final INode inode = iip.getLastINode();\n    if (inode == null || !inode.isFile() || inode.asFile().isStriped()) {\n      // TODO we do not support replication on stripe layout files yet\n      return null;\n    }\n\n    INodeFile file = inode.asFile();\n    // Make sure the directory has sufficient quotas\n    short oldBR = file.getPreferredBlockReplication();\n\n    // Ensure the quota does not exceed\n    if (oldBR < replication) {\n      long size = file.computeFileSize(true, true);\n      fsd.updateCount(iip, 0L, size, oldBR, replication, true);\n    }\n\n    file.setFileReplication(replication, iip.getLatestSnapshotId());\n    short targetReplication = (short) Math.max(\n        replication, file.getPreferredBlockReplication());\n\n    for (BlockInfo b : file.getBlocks()) {\n      if (oldBR == targetReplication) {\n        continue;\n      }\n      if (oldBR > replication) {\n        fsd.updateCount(iip, 0L, b.getNumBytes(), oldBR, targetReplication,\n                        true);\n      }\n      bm.setReplication(oldBR, targetReplication, b);\n    }\n\n    if (oldBR != -1) {\n      if (oldBR > targetReplication) {\n        FSDirectory.LOG.info(\"Decreasing replication from {} to {} for {}\",\n                             oldBR, targetReplication, src);\n      } else {\n        FSDirectory.LOG.info(\"Increasing replication from {} to {} for {}\",\n                             oldBR, targetReplication, src);\n      }\n    }\n    return file.getBlocks();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setReplication": "  static boolean setReplication(\n      FSDirectory fsd, BlockManager bm, String src, final short replication)\n      throws IOException {\n    bm.verifyReplication(src, replication, null);\n    final boolean isFile;\n    FSPermissionChecker pc = fsd.getPermissionChecker();\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    fsd.writeLock();\n    try {\n      src = fsd.resolvePath(pc, src, pathComponents);\n      final INodesInPath iip = fsd.getINodesInPath4Write(src);\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n      }\n\n      final BlockInfo[] blocks = unprotectedSetReplication(fsd, src,\n                                                           replication);\n      isFile = blocks != null;\n      if (isFile) {\n        fsd.getEditLog().logSetReplication(src, replication);\n      }\n    } finally {\n      fsd.writeUnlock();\n    }\n    return isFile;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog": "  static void renameForEditLog(\n      FSDirectory fsd, String src, String dst, long timestamp,\n      Options.Rename... options)\n      throws IOException {\n    BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n    final INodesInPath srcIIP = fsd.getINodesInPath4Write(src, false);\n    final INodesInPath dstIIP = fsd.getINodesInPath4Write(dst, false);\n    unprotectedRenameTo(fsd, src, dst, srcIIP, dstIIP, timestamp,\n        collectedBlocks, options);\n    if (!collectedBlocks.getToDeleteList().isEmpty()) {\n      fsd.getFSNamesystem().getBlockManager()\n          .removeBlocksAndUpdateSafemodeTotal(collectedBlocks);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo": "  static boolean unprotectedRenameTo(FSDirectory fsd, String src, String dst,\n      final INodesInPath srcIIP, final INodesInPath dstIIP, long timestamp,\n      BlocksMapUpdateInfo collectedBlocks, Options.Rename... options)\n      throws IOException {\n    assert fsd.hasWriteLock();\n    boolean overwrite = options != null\n        && Arrays.asList(options).contains(Options.Rename.OVERWRITE);\n\n    final String error;\n    final INode srcInode = srcIIP.getLastINode();\n    validateRenameSource(srcIIP);\n\n    // validate the destination\n    if (dst.equals(src)) {\n      throw new FileAlreadyExistsException(\"The source \" + src +\n          \" and destination \" + dst + \" are the same\");\n    }\n    validateDestination(src, dst, srcInode);\n\n    if (dstIIP.length() == 1) {\n      error = \"rename destination cannot be the root\";\n      NameNode.stateChangeLog.warn(\"DIR* FSDirectory.unprotectedRenameTo: \" +\n          error);\n      throw new IOException(error);\n    }\n\n    BlockStoragePolicySuite bsps = fsd.getBlockStoragePolicySuite();\n    fsd.ezManager.checkMoveValidity(srcIIP, dstIIP, src);\n    final INode dstInode = dstIIP.getLastINode();\n    List<INodeDirectory> snapshottableDirs = new ArrayList<>();\n    if (dstInode != null) { // Destination exists\n      validateOverwrite(src, dst, overwrite, srcInode, dstInode);\n      FSDirSnapshotOp.checkSnapshot(dstInode, snapshottableDirs);\n    }\n\n    INode dstParent = dstIIP.getINode(-2);\n    if (dstParent == null) {\n      error = \"rename destination parent \" + dst + \" not found.\";\n      NameNode.stateChangeLog.warn(\"DIR* FSDirectory.unprotectedRenameTo: \" +\n          error);\n      throw new FileNotFoundException(error);\n    }\n    if (!dstParent.isDirectory()) {\n      error = \"rename destination parent \" + dst + \" is a file.\";\n      NameNode.stateChangeLog.warn(\"DIR* FSDirectory.unprotectedRenameTo: \" +\n          error);\n      throw new ParentNotDirectoryException(error);\n    }\n\n    // Ensure dst has quota to accommodate rename\n    verifyFsLimitsForRename(fsd, srcIIP, dstIIP);\n    verifyQuotaForRename(fsd, srcIIP, dstIIP);\n\n    RenameOperation tx = new RenameOperation(fsd, src, dst, srcIIP, dstIIP);\n\n    boolean undoRemoveSrc = true;\n    tx.removeSrc();\n\n    boolean undoRemoveDst = false;\n    long removedNum = 0;\n    try {\n      if (dstInode != null) { // dst exists, remove it\n        removedNum = tx.removeDst();\n        if (removedNum != -1) {\n          undoRemoveDst = true;\n        }\n      }\n\n      // add src as dst to complete rename\n      if (tx.addSourceToDestination()) {\n        undoRemoveSrc = false;\n        if (NameNode.stateChangeLog.isDebugEnabled()) {\n          NameNode.stateChangeLog.debug(\"DIR* FSDirectory.unprotectedRenameTo: \"\n              + src + \" is renamed to \" + dst);\n        }\n\n        tx.updateMtimeAndLease(timestamp);\n\n        // Collect the blocks and remove the lease for previous dst\n        boolean filesDeleted = false;\n        if (undoRemoveDst) {\n          undoRemoveDst = false;\n          if (removedNum > 0) {\n            filesDeleted = tx.cleanDst(bsps, collectedBlocks);\n          }\n        }\n\n        if (snapshottableDirs.size() > 0) {\n          // There are snapshottable directories (without snapshots) to be\n          // deleted. Need to update the SnapshotManager.\n          fsd.getFSNamesystem().removeSnapshottableDirs(snapshottableDirs);\n        }\n\n        tx.updateQuotasInSourceTree(bsps);\n        return filesDeleted;\n      }\n    } finally {\n      if (undoRemoveSrc) {\n        tx.restoreSource();\n      }\n      if (undoRemoveDst) { // Rename failed - restore dst\n        tx.restoreDst(bsps);\n      }\n    }\n    NameNode.stateChangeLog.warn(\"DIR* FSDirectory.unprotectedRenameTo: \" +\n        \"failed to rename \" + src + \" to \" + dst);\n    throw new IOException(\"rename from \" + src + \" to \" + dst + \" failed.\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.editLogLoaderPrompt": "  public static void editLogLoaderPrompt(String prompt,\n        MetaRecoveryContext recovery, String contStr)\n        throws IOException, RequestStopException\n  {\n    if (recovery == null) {\n      throw new IOException(prompt);\n    }\n    LOG.error(prompt);\n    String answer = recovery.ask(\"\\nEnter 'c' to continue, \" + contStr + \"\\n\" +\n      \"Enter 's' to stop reading the edit log here, abandoning any later \" +\n        \"edits\\n\" +\n      \"Enter 'q' to quit without saving\\n\" +\n      \"Enter 'a' to always select the first choice in the future \" +\n      \"without prompting. \" + \n      \"(c/s/q/a)\\n\", \"c\", \"s\", \"q\", \"a\");\n    if (answer.equals(\"c\")) {\n      LOG.info(\"Continuing\");\n      return;\n    } else if (answer.equals(\"s\")) {\n      throw new RequestStopException(\"user requested stop\");\n    } else if (answer.equals(\"q\")) {\n      recovery.quit();\n    } else {\n      recovery.setForce(FORCE_FIRST_CHOICE);\n      return;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.quit": "  public void quit() {\n    LOG.error(\"Exiting on user request.\");\n    System.exit(0);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.ask": "  public String ask(String prompt, String firstChoice, String... choices) \n      throws IOException {\n    while (true) {\n      System.err.print(prompt);\n      if (force > FORCE_NONE) {\n        System.out.println(\"automatically choosing \" + firstChoice);\n        return firstChoice;\n      }\n      StringBuilder responseBuilder = new StringBuilder();\n      while (true) {\n        int c = System.in.read();\n        if (c == -1 || c == '\\r' || c == '\\n') {\n          break;\n        }\n        responseBuilder.append((char)c);\n      }\n      String response = responseBuilder.toString();\n      if (response.equalsIgnoreCase(firstChoice))\n        return firstChoice;\n      for (String c : choices) {\n        if (response.equalsIgnoreCase(c)) {\n          return c;\n        }\n      }\n      System.err.print(\"I'm sorry, I cannot understand your response.\\n\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.setForce": "  public void setForce(int force) {\n    this.force = force;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getStartupProgress": "  public static StartupProgress getStartupProgress() {\n    return startupProgress;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.hasTransactionId": "  public boolean hasTransactionId() {\n    return (txid != HdfsServerConstants.INVALID_TXID);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.getTransactionId": "  public long getTransactionId() {\n    Preconditions.checkState(txid != HdfsServerConstants.INVALID_TXID);\n    return txid;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.closeAllStreams": "  static void closeAllStreams(Iterable<EditLogInputStream> streams) {\n    for (EditLogInputStream s : streams) {\n      IOUtils.closeStream(s);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.getLoginUser": "  static UserGroupInformation getLoginUser() throws IOException {\n    if (loginUser == null) {\n      loginUserFromSubject(null);\n    }\n    return loginUser;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject": "  static void loginUserFromSubject(Subject subject) throws IOException {\n    ensureInitialized();\n    try {\n      if (subject == null) {\n        subject = new Subject();\n      }\n      LoginContext login =\n          newLoginContext(authenticationMethod.getLoginAppName(), \n                          subject, new HadoopConfiguration());\n      login.login();\n      UserGroupInformation realUser = new UserGroupInformation(subject);\n      realUser.setLogin(login);\n      realUser.setAuthenticationMethod(authenticationMethod);\n      realUser = new UserGroupInformation(login.getSubject());\n      // If the HADOOP_PROXY_USER environment variable or property\n      // is specified, create a proxy user as the logged in user.\n      String proxyUser = System.getenv(HADOOP_PROXY_USER);\n      if (proxyUser == null) {\n        proxyUser = System.getProperty(HADOOP_PROXY_USER);\n      }\n      loginUser = proxyUser == null ? realUser : createProxyUser(proxyUser, realUser);\n\n      String tokenFileLocation = System.getProperty(HADOOP_TOKEN_FILES);\n      if (tokenFileLocation == null) {\n        tokenFileLocation = conf.get(HADOOP_TOKEN_FILES);\n      }\n      if (tokenFileLocation != null) {\n        for (String tokenFileName:\n             StringUtils.getTrimmedStrings(tokenFileLocation)) {\n          if (tokenFileName.length() > 0) {\n            File tokenFile = new File(tokenFileName);\n            if (tokenFile.exists() && tokenFile.isFile()) {\n              Credentials cred = Credentials.readTokenStorageFile(\n                  tokenFile, conf);\n              loginUser.addCredentials(cred);\n            } else {\n              LOG.info(\"tokenFile(\"+tokenFileName+\") does not exist\");\n            }\n          }\n        }\n      }\n\n      String fileLocation = System.getenv(HADOOP_TOKEN_FILE_LOCATION);\n      if (fileLocation != null) {\n        // Load the token storage file and put all of the tokens into the\n        // user. Don't use the FileSystem API for reading since it has a lock\n        // cycle (HADOOP-9212).\n        File source = new File(fileLocation);\n        LOG.debug(\"Reading credentials from location set in {}: {}\",\n            HADOOP_TOKEN_FILE_LOCATION,\n            source.getCanonicalPath());\n        if (!source.isFile()) {\n          throw new FileNotFoundException(\"Source file \"\n              + source.getCanonicalPath() + \" from \"\n              + HADOOP_TOKEN_FILE_LOCATION\n              + \" not found\");\n        }\n        Credentials cred = Credentials.readTokenStorageFile(\n            source, conf);\n        LOG.debug(\"Loaded {} tokens\", cred.numberOfTokens());\n        loginUser.addCredentials(cred);\n      }\n      loginUser.spawnAutoRenewalThreadForUserCreds();\n    } catch (LoginException le) {\n      LOG.debug(\"failure to login\", le);\n      throw new IOException(\"failure to login: \" + le, le);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"UGI loginUser:\"+loginUser);\n    } \n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled": "  public static boolean isSecurityEnabled() {\n    return !isAuthenticationMethodEnabled(AuthenticationMethod.SIMPLE);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.isAuthenticationMethodEnabled": "  private static boolean isAuthenticationMethodEnabled(AuthenticationMethod method) {\n    ensureInitialized();\n    return (authenticationMethod == method);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeDirectory.getChildrenList": "  public ReadOnlyList<INode> getChildrenList(final int snapshotId) {\n    DirectoryWithSnapshotFeature sf;\n    if (snapshotId == Snapshot.CURRENT_STATE_ID\n        || (sf = this.getDirectoryWithSnapshotFeature()) == null) {\n      return getCurrentChildrenList();\n    }\n    return sf.getChildrenList(this, snapshotId);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeDirectory.getCurrentChildrenList": "  private ReadOnlyList<INode> getCurrentChildrenList() {\n    return children == null ? ReadOnlyList.Util.<INode> emptyList()\n        : ReadOnlyList.Util.asReadOnlyList(children);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeDirectory.getDirectoryWithSnapshotFeature": "  public final DirectoryWithSnapshotFeature getDirectoryWithSnapshotFeature() {\n    return getFeature(DirectoryWithSnapshotFeature.class);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.append": "  public static INodesInPath append(INodesInPath iip, INode child,\n      byte[] childName) {\n    Preconditions.checkArgument(iip.length() > 0);\n    Preconditions.checkArgument(iip.getLastINode() != null && iip\n        .getLastINode().isDirectory());\n    INode[] inodes = new INode[iip.length() + 1];\n    System.arraycopy(iip.inodes, 0, inodes, 0, inodes.length - 1);\n    inodes[inodes.length - 1] = child;\n    byte[][] path = new byte[iip.path.length + 1][];\n    System.arraycopy(iip.path, 0, path, 0, path.length - 1);\n    path[path.length - 1] = childName;\n    return new INodesInPath(inodes, path, iip.isSnapshot, iip.snapshotId);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.getParent": "  public final INodeDirectory getParent() {\n    return parent == null? null\n        : parent.isReference()? getParentReference().getParent(): parent.asDirectory();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.asDirectory": "  public INodeDirectory asDirectory() {\n    throw new IllegalStateException(\"Current inode is not a directory: \"\n        + this.toDetailString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.getParentReference": "  public INodeReference getParentReference() {\n    return parent == null || !parent.isReference()? null: (INodeReference)parent;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.isReference": "  public boolean isReference() {\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.QuotaCounts.negation": "  public QuotaCounts negation() {\n    QuotaCounts ret = new QuotaCounts.Builder().quotaCount(this).build();\n    ret.nsSsCounts.negation();\n    ret.tsCounts.negation();\n    return ret;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.QuotaCounts.build": "    public QuotaCounts build() {\n      return new QuotaCounts(this);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.QuotaCounts.quotaCount": "    public Builder quotaCount(QuotaCounts that) {\n      this.nsSsCounts.set(that.nsSsCounts);\n      this.tsCounts.set(that.tsCounts);\n      return this;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.AclStorage.copyINodeDefaultAcl": "  public static void copyINodeDefaultAcl(INode child) {\n    INodeDirectory parent = child.getParent();\n    AclFeature parentAclFeature = parent.getAclFeature();\n    if (parentAclFeature == null || !(child.isFile() || child.isDirectory())) {\n      return;\n    }\n\n    // Split parent's entries into access vs. default.\n    List<AclEntry> featureEntries = getEntriesFromAclFeature(parent\n        .getAclFeature());\n    ScopedAclEntries scopedEntries = new ScopedAclEntries(featureEntries);\n    List<AclEntry> parentDefaultEntries = scopedEntries.getDefaultEntries();\n\n    // The parent may have an access ACL but no default ACL.  If so, exit.\n    if (parentDefaultEntries.isEmpty()) {\n      return;\n    }\n\n    // Pre-allocate list size for access entries to copy from parent.\n    List<AclEntry> accessEntries = Lists.newArrayListWithCapacity(\n      parentDefaultEntries.size());\n\n    FsPermission childPerm = child.getFsPermission();\n\n    // Copy each default ACL entry from parent to new child's access ACL.\n    boolean parentDefaultIsMinimal = AclUtil.isMinimalAcl(parentDefaultEntries);\n    for (AclEntry entry: parentDefaultEntries) {\n      AclEntryType type = entry.getType();\n      String name = entry.getName();\n      AclEntry.Builder builder = new AclEntry.Builder()\n        .setScope(AclEntryScope.ACCESS)\n        .setType(type)\n        .setName(name);\n\n      // The child's initial permission bits are treated as the mode parameter,\n      // which can filter copied permission values for owner, mask and other.\n      final FsAction permission;\n      if (type == AclEntryType.USER && name == null) {\n        permission = entry.getPermission().and(childPerm.getUserAction());\n      } else if (type == AclEntryType.GROUP && parentDefaultIsMinimal) {\n        // This only happens if the default ACL is a minimal ACL: exactly 3\n        // entries corresponding to owner, group and other.  In this case,\n        // filter the group permissions.\n        permission = entry.getPermission().and(childPerm.getGroupAction());\n      } else if (type == AclEntryType.MASK) {\n        // Group bits from mode parameter filter permission of mask entry.\n        permission = entry.getPermission().and(childPerm.getGroupAction());\n      } else if (type == AclEntryType.OTHER) {\n        permission = entry.getPermission().and(childPerm.getOtherAction());\n      } else {\n        permission = entry.getPermission();\n      }\n\n      builder.setPermission(permission);\n      accessEntries.add(builder.build());\n    }\n\n    // A new directory also receives a copy of the parent's default ACL.\n    List<AclEntry> defaultEntries = child.isDirectory() ? parentDefaultEntries :\n      Collections.<AclEntry>emptyList();\n\n    final FsPermission newPerm;\n    if (!AclUtil.isMinimalAcl(accessEntries) || !defaultEntries.isEmpty()) {\n      // Save the new ACL to the child.\n      child.addAclFeature(createAclFeature(accessEntries, defaultEntries));\n      newPerm = createFsPermissionForExtendedAcl(accessEntries, childPerm);\n    } else {\n      // The child is receiving a minimal ACL.\n      newPerm = createFsPermissionForMinimalAcl(accessEntries, childPerm);\n    }\n\n    child.setPermission(newPerm);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.AclStorage.getEntriesFromAclFeature": "  static ImmutableList<AclEntry> getEntriesFromAclFeature(AclFeature aclFeature) {\n    if (aclFeature == null) {\n      return ImmutableList.<AclEntry> of();\n    }\n    ImmutableList.Builder<AclEntry> b = new ImmutableList.Builder<AclEntry>();\n    for (int pos = 0, entry; pos < aclFeature.getEntriesSize(); pos++) {\n      entry = aclFeature.getEntryAt(pos);\n      b.add(AclEntryStatusFormat.toAclEntry(entry));\n    }\n    return b.build();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.AclStorage.createAclFeature": "  private static AclFeature createAclFeature(List<AclEntry> accessEntries,\n      List<AclEntry> defaultEntries) {\n    // Pre-allocate list size for the explicit entries stored in the feature,\n    // which is all entries minus the 3 entries implicitly stored in the\n    // permission bits.\n    List<AclEntry> featureEntries = Lists.newArrayListWithCapacity(\n      (accessEntries.size() - 3) + defaultEntries.size());\n\n    // For the access ACL, the feature only needs to hold the named user and\n    // group entries.  For a correctly sorted ACL, these will be in a\n    // predictable range.\n    if (!AclUtil.isMinimalAcl(accessEntries)) {\n      featureEntries.addAll(\n        accessEntries.subList(1, accessEntries.size() - 2));\n    }\n\n    // Add all default entries to the feature.\n    featureEntries.addAll(defaultEntries);\n    return new AclFeature(AclEntryStatusFormat.toInt(featureEntries));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.AclStorage.addAclFeature": "  public static AclFeature addAclFeature(AclFeature aclFeature) {\n    return UNIQUE_ACL_FEATURES.put(aclFeature);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.AclStorage.createFsPermissionForExtendedAcl": "  private static FsPermission createFsPermissionForExtendedAcl(\n      List<AclEntry> accessEntries, FsPermission existingPerm) {\n    return new FsPermission(accessEntries.get(0).getPermission(),\n      accessEntries.get(accessEntries.size() - 2).getPermission(),\n      accessEntries.get(accessEntries.size() - 1).getPermission(),\n      existingPerm.getStickyBit());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.AclStorage.createFsPermissionForMinimalAcl": "  private static FsPermission createFsPermissionForMinimalAcl(\n      List<AclEntry> accessEntries, FsPermission existingPerm) {\n    return new FsPermission(accessEntries.get(0).getPermission(),\n      accessEntries.get(1).getPermission(),\n      accessEntries.get(2).getPermission(),\n      existingPerm.getStickyBit());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeDirectory.addChild": "  private void addChild(final INode node, final int insertionPoint) {\n    if (children == null) {\n      children = new ArrayList<>(DEFAULT_FILES_PER_DIRECTORY);\n    }\n    node.setParent(this);\n    children.add(-insertionPoint - 1, node);\n\n    if (node.getGroupName() == null) {\n      node.setGroup(getGroupName());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeDirectory.addSnapshotFeature": "  public DirectoryWithSnapshotFeature addSnapshotFeature(\n      DirectoryDiffList diffs) {\n    Preconditions.checkState(!isWithSnapshot(), \n        \"Directory is already with snapshot\");\n    DirectoryWithSnapshotFeature sf = new DirectoryWithSnapshotFeature(diffs);\n    addFeature(sf);\n    return sf;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeDirectory.searchChildren": "  int searchChildren(byte[] name) {\n    return children == null? -1: Collections.binarySearch(children, name);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.computeQuotaUsage": "  public final QuotaCounts computeQuotaUsage(BlockStoragePolicySuite bsps,\n      boolean useCache) {\n    final byte storagePolicyId = isSymlink() ?\n        HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED : getStoragePolicyID();\n    return computeQuotaUsage(bsps, storagePolicyId, useCache,\n        Snapshot.CURRENT_STATE_ID);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.isSymlink": "  public boolean isSymlink() {\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.getStoragePolicyID": "  public abstract byte getStoragePolicyID();\n\n  /**\n   * @return the storage policy directly specified on the INode. Return\n   * {@link HdfsConstants#BLOCK_STORAGE_POLICY_ID_UNSPECIFIED} if no policy has",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.getLocalName": "  public final String getLocalName() {\n    final byte[] name = getLocalNameBytes();\n    return name == null? null: DFSUtil.bytes2String(name);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.recoverUnclosedStreams": "  synchronized void recoverUnclosedStreams() {\n    Preconditions.checkState(\n        state == State.BETWEEN_LOG_SEGMENTS,\n        \"May not recover segments - wrong state: %s\", state);\n    try {\n      journalSet.recoverUnfinalizedSegments();\n    } catch (IOException ex) {\n      // All journals have failed, it is handled in logSync.\n      // TODO: are we sure this is OK?\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.cacheManager.startMonitorThread": "  public void startMonitorThread() {\n    crmLock.lock();\n    try {\n      if (this.monitor == null) {\n        this.monitor = new CacheReplicationMonitor(namesystem, this,\n            scanIntervalMs, crmLock);\n        this.monitor.start();\n      }\n    } finally {\n      crmLock.unlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.leaseManager.startMonitor": "  void startMonitor() {\n    Preconditions.checkState(lmthread == null,\n        \"Lease Monitor already running\");\n    shouldRunMonitor = true;\n    lmthread = new Daemon(new Monitor());\n    lmthread.start();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.initJournalsForWrite": "  public synchronized void initJournalsForWrite() {\n    Preconditions.checkState(state == State.UNINITIALIZED ||\n        state == State.CLOSED, \"Unexpected state: %s\", state);\n    \n    initJournals(this.editsDirs);\n    state = State.BETWEEN_LOG_SEGMENTS;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.initJournals": "  private synchronized void initJournals(List<URI> dirs) {\n    int minimumRedundantJournals = conf.getInt(\n        DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_MINIMUM_KEY,\n        DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_MINIMUM_DEFAULT);\n\n    synchronized(journalSetLock) {\n      journalSet = new JournalSet(minimumRedundantJournals);\n\n      for (URI u : dirs) {\n        boolean required = FSNamesystem.getRequiredNamespaceEditsDirs(conf)\n            .contains(u);\n        if (u.getScheme().equals(NNStorage.LOCAL_URI_SCHEME)) {\n          StorageDirectory sd = storage.getStorageDirectory(u);\n          if (sd != null) {\n            journalSet.add(new FileJournalManager(conf, sd, storage),\n                required, sharedEditsDirs.contains(u));\n          }\n        } else {\n          journalSet.add(createJournal(u), required,\n              sharedEditsDirs.contains(u));\n        }\n      }\n    }\n \n    if (journalSet.isEmpty()) {\n      LOG.error(\"No edits directories configured!\");\n    } \n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.setNextTxId": "  synchronized void setNextTxId(long nextTxId) {\n    Preconditions.checkArgument(synctxid <= txid &&\n       nextTxId >= txid,\n       \"May not decrease txid.\" +\n      \" synctxid=%s txid=%s nextTxId=%s\",\n      synctxid, txid, nextTxId);\n      \n    txid = nextTxId - 1;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp.warmUpEdekCache": "  static void warmUpEdekCache(final ExecutorService executor,\n      final FSDirectory fsd, final int delay, final int interval) {\n    fsd.readLock();\n    try {\n      String[] edeks  = fsd.ezManager.getKeyNames();\n      executor.execute(\n          new EDEKCacheLoader(edeks, fsd.getProvider(), delay, interval));\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.isOpenForWrite": "  synchronized boolean isOpenForWrite() {\n    return state == State.IN_SEGMENT ||\n      state == State.BETWEEN_LOG_SEGMENTS;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.leaseManager.renewAllLeases": "  synchronized void renewAllLeases() {\n    for (Lease l : leases.values()) {\n      renewLease(l);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.leaseManager.renewLease": "  synchronized void renewLease(Lease lease) {\n    if (lease != null) {\n      sortedLeases.remove(lease);\n      lease.renew();\n      sortedLeases.add(lease);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcWritable.wrap": "    static Buffer wrap(ByteBuffer bb) {\n      return new Buffer(bb);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcWritable.getValue": "    public <T> T getValue(T value) throws IOException {\n      return RpcWritable.wrap(value).readFrom(bb);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcWritable.readFrom": "    <T> T readFrom(ByteBuffer bb) throws IOException {\n      // effectively consume the rest of the buffer from the callers\n      // perspective.\n      this.bb = bb.slice();\n      bb.limit(bb.position());\n      return (T)this;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.updateMetrics": "  void updateMetrics(String name, int queueTime, int processingTime) {\n    rpcMetrics.addRpcQueueTime(queueTime);\n    rpcMetrics.addRpcProcessingTime(processingTime);\n    rpcDetailedMetrics.addProcessingTime(name, processingTime);\n    callQueue.addResponseTime(name, getPriorityLevel(), queueTime,\n        processingTime);\n\n    if (isLogSlowRPC()) {\n      logSlowRpcCalls(name, processingTime);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.logSlowRpcCalls": "  void logSlowRpcCalls(String methodName, int processingTime) {\n    final int deviation = 3;\n\n    // 1024 for minSampleSize just a guess -- not a number computed based on\n    // sample size analysis. It is chosen with the hope that this\n    // number is high enough to avoid spurious logging, yet useful\n    // in practice.\n    final int minSampleSize = 1024;\n    final double threeSigma = rpcMetrics.getProcessingMean() +\n        (rpcMetrics.getProcessingStdDev() * deviation);\n\n    if ((rpcMetrics.getProcessingSampleCount() > minSampleSize) &&\n        (processingTime > threeSigma)) {\n      if(LOG.isWarnEnabled()) {\n        String client = CurCall.get().connection.toString();\n        LOG.warn(\n            \"Slow RPC : \" + methodName + \" took \" + processingTime +\n                \" milliseconds to process from client \" + client);\n      }\n      rpcMetrics.incrSlowRpc();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.getPriorityLevel": "    public int getPriorityLevel() {\n      return this.priorityLevel;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.isLogSlowRPC": "  protected boolean isLogSlowRPC() {\n    return logSlowRPC;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcServerException.getRpcStatusProto": "  public RpcStatusProto getRpcStatusProto() {\n    return RpcStatusProto.ERROR;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.CallerContext.setCurrent": "  public static void setCurrent(CallerContext callerContext) {\n    CurrentCallerContextHolder.CALLER_CONTEXT.set(callerContext);\n  }"
        },
        "bug_report": {
            "Title": "Improve log message for edit loading failures caused by FS limit checks.",
            "Description": "We encountered a bug where Standby NameNode crashes due to an NPE when loading edits.\n\n{noformat}\n2016-08-05 15:06:00,983 ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: Encountered exception on operation AddOp [length=0, inodeId=789272719, path=[path], replication=3, mtime=1470379597935, atime=1470379597935, blockSize=134217728, blocks=[], permissions=<user>:supergroup:rw-r--r--, aclEntries=null, clientName=DFSClient_NONMAPREDUCE_1495395702_1, clientMachine=10.210.119.136, overwrite=true, RpcClientId=a1512eeb-65e4-43dc-8aa8-d7a1af37ed30, RpcCallId=417, storagePolicyId=0, opCode=OP_ADD, txid=4212503758]\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFileEncryptionInfo(FSDirectory.java:2914)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.createFileStatus(FSDirectory.java:2469)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:375)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:230)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:139)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:829)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:810)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:232)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:331)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:284)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:301)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:360)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1651)\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:410)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:297)\n{noformat}\n\nThe NameNode crashes and can not be restarted. After some research, we turned on debug log of org.apache.hadoop.hdfs.StateChange, restart the NN, and we saw the following exception which induced NPE:\n\n{noformat}\n16/08/07 18:51:15 DEBUG hdfs.StateChange: DIR* FSDirectory.unprotectedAddFile: exception when add [path] to the file system\norg.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException: The directory item limit of [path] is exceeded: limit=1048576 items=1049332\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:2060)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addChild(FSDirectory.java:2112)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:2081)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1900)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedAddFile(FSDirectory.java:368)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:365)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:230)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:139)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:829)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:810)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:232)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$1.run(EditLogTailer.java:188)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$1.run(EditLogTailer.java:182)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n        at org.apache.hadoop.security.SecurityUtil.doAsUser(SecurityUtil.java:445)\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUser(SecurityUtil.java:426)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.catchupDuringFailover(EditLogTailer.java:182)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1205)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1762)\n        at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)\n        at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)\n        at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1635)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1351)\n        at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)\n        at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:4460)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)\n{noformat}\n\nThe exception is thrown, caught and logged at debug level in {{FSDirectory#unprotectedAddFile}}. Afterwards, a null is returned, which is used and subsequently caused NPE. HDFS-7567 reported a similar bug, but it was deemed the NPE could only be thrown if edit is corrupt. However, here we see an example where NPE could be thrown without corrupt edits.\n\nLooks like the maximum number of items per directory is exceeded. This is similar to HDFS-6102 and HDFS-7482, but this happens at loading edits, rather than loading fsimage. \n\nA possible workaround is to increas the value of {{dfs.namenode.fs-limits.max-directory-items}} to 6400000. But I am not sure if it would cause any side effects."
        }
    },
    {
        "filename": "HDFS-13040.json",
        "creation_time": "2018-01-19T19:47:02.000+0000",
        "stack_trace": "```\norg.apache.hadoop.ipc.RemoteException(java.io.IOException): We encountered an error reading https://nn2.example.com:8481/getJournal?jid=ns1&segmentTxId=8662&storageInfo=-60%3A353531113%3A0%3Acluster3, https://nn1.example.com:8481/getJournal?jid=ns1&segmentTxId=8662&storageInfo=-60%3A353531113%3A0%3Acluster3.  During automatic edit log failover, we noticed that all of the remaining edit log streams are shorter than the current one!  The best remaining edit log ends at transaction 8683, but we thought we could read up to transaction 8684.  If you continue, metadata will be lost forever!\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:213)\n        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.readOp(NameNodeRpcServer.java:1701)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEditsFromTxid(NameNodeRpcServer.java:1763)\n        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getEditsFromTxid(AuthorizationProviderProxyClientProtocol.java:1011)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getEditsFromTxid(ClientNamenodeProtocolServerSideTranslatorPB.java:1490)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2212)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2210)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp": "  protected FSEditLogOp nextOp() throws IOException {\n    while (true) {\n      switch (state) {\n      case SKIP_UNTIL:\n       try {\n          if (prevTxId != HdfsServerConstants.INVALID_TXID) {\n            LOG.info(\"Fast-forwarding stream '\" + streams[curIdx].getName() +\n                \"' to transaction ID \" + (prevTxId + 1));\n            streams[curIdx].skipUntil(prevTxId + 1);\n          }\n        } catch (IOException e) {\n          prevException = e;\n          state = State.STREAM_FAILED;\n        }\n        state = State.OK;\n        break;\n      case OK:\n        try {\n          FSEditLogOp op = streams[curIdx].readOp();\n          if (op == null) {\n            state = State.EOF;\n            if (streams[curIdx].getLastTxId() == prevTxId) {\n              return null;\n            } else {\n              throw new PrematureEOFException(\"got premature end-of-file \" +\n                  \"at txid \" + prevTxId + \"; expected file to go up to \" +\n                  streams[curIdx].getLastTxId());\n            }\n          }\n          prevTxId = op.getTransactionId();\n          return op;\n        } catch (IOException e) {\n          prevException = e;\n          state = State.STREAM_FAILED;\n        }\n        break;\n      case STREAM_FAILED:\n        if (curIdx + 1 == streams.length) {\n          throw prevException;\n        }\n        long oldLast = streams[curIdx].getLastTxId();\n        long newLast = streams[curIdx + 1].getLastTxId();\n        if (newLast < oldLast) {\n          throw new IOException(\"We encountered an error reading \" +\n              streams[curIdx].getName() + \".  During automatic edit log \" +\n              \"failover, we noticed that all of the remaining edit log \" +\n              \"streams are shorter than the current one!  The best \" +\n              \"remaining edit log ends at transaction \" +\n              newLast + \", but we thought we could read up to transaction \" +\n              oldLast + \".  If you continue, metadata will be lost forever!\");\n        }\n        LOG.error(\"Got error reading edit log input stream \" +\n          streams[curIdx].getName() + \"; failing over to edit log \" +\n          streams[curIdx + 1].getName(), prevException);\n        curIdx++;\n        state = State.SKIP_UNTIL;\n        break;\n      case STREAM_FAILED_RESYNC:\n        if (curIdx + 1 == streams.length) {\n          if (prevException instanceof PrematureEOFException) {\n            // bypass early EOF check\n            state = State.EOF;\n          } else {\n            streams[curIdx].resync();\n            state = State.SKIP_UNTIL;\n          }\n        } else {\n          LOG.error(\"failing over to edit log \" +\n              streams[curIdx + 1].getName());\n          curIdx++;\n          state = State.SKIP_UNTIL;\n        }\n        break;\n      case EOF:\n        return null;\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.getLastTxId": "  public long getLastTxId() {\n    return streams[curIdx].getLastTxId();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.getName": "  public String getName() {\n    StringBuilder bld = new StringBuilder();\n    String prefix = \"\";\n    for (EditLogInputStream elis : streams) {\n      bld.append(prefix);\n      bld.append(elis.getName());\n      prefix = \", \";\n    }\n    return bld.toString();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp": "  public FSEditLogOp readOp() throws IOException {\n    FSEditLogOp ret;\n    if (cachedOp != null) {\n      ret = cachedOp;\n      cachedOp = null;\n      return ret;\n    }\n    return nextOp();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.nextOp": "  protected abstract FSEditLogOp nextOp() throws IOException;\n\n  /**\n   * Go through the next operation from the stream storage.\n   * @return the txid of the next operation.\n   */\n  protected long scanNextOp() throws IOException {\n    FSEditLogOp next = readOp();\n    return next != null ? next.txid : HdfsServerConstants.INVALID_TXID;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.readOp": "  private static FSEditLogOp readOp(EditLogInputStream elis)\n      throws IOException {\n    try {\n      return elis.readOp();\n      // we can get the below two exceptions if a segment is deleted\n      // (because we have accumulated too many edits) or (for the local journal/\n      // no-QJM case only) if a in-progress segment is finalized under us ...\n      // no need to throw an exception back to the client in this case\n    } catch (FileNotFoundException e) {\n      LOG.debug(\"Tried to read from deleted or moved edit log segment\", e);\n      return null;\n    } catch (HttpGetFailedException e) {\n      LOG.debug(\"Tried to read from deleted edit log segment\", e);\n      return null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEditsFromTxid": "  public EventBatchList getEditsFromTxid(long txid) throws IOException {\n    checkNNStartup();\n    namesystem.checkOperation(OperationCategory.READ); // only active\n    namesystem.checkSuperuserPrivilege();\n    int maxEventsPerRPC = nn.getConf().getInt(\n        DFSConfigKeys.DFS_NAMENODE_INOTIFY_MAX_EVENTS_PER_RPC_KEY,\n        DFSConfigKeys.DFS_NAMENODE_INOTIFY_MAX_EVENTS_PER_RPC_DEFAULT);\n    FSEditLog log = namesystem.getFSImage().getEditLog();\n    long syncTxid = log.getSyncTxId();\n    // If we haven't synced anything yet, we can only read finalized\n    // segments since we can't reliably determine which txns in in-progress\n    // segments have actually been committed (e.g. written to a quorum of JNs).\n    // If we have synced txns, we can definitely read up to syncTxid since\n    // syncTxid is only updated after a transaction is committed to all\n    // journals. (In-progress segments written by old writers are already\n    // discarded for us, so if we read any in-progress segments they are\n    // guaranteed to have been written by this NameNode.)\n    boolean readInProgress = syncTxid > 0;\n\n    List<EventBatch> batches = Lists.newArrayList();\n    int totalEvents = 0;\n    long maxSeenTxid = -1;\n    long firstSeenTxid = -1;\n\n    if (syncTxid > 0 && txid > syncTxid) {\n      // we can't read past syncTxid, so there's no point in going any further\n      return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);\n    }\n\n    Collection<EditLogInputStream> streams = null;\n    try {\n      streams = log.selectInputStreams(txid, 0, null, readInProgress);\n    } catch (IllegalStateException e) { // can happen if we have\n      // transitioned out of active and haven't yet transitioned to standby\n      // and are using QJM -- the edit log will be closed and this exception\n      // will result\n      LOG.info(\"NN is transitioning from active to standby and FSEditLog \" +\n      \"is closed -- could not read edits\");\n      return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);\n    }\n\n    boolean breakOuter = false;\n    for (EditLogInputStream elis : streams) {\n      // our assumption in this code is the EditLogInputStreams are ordered by\n      // starting txid\n      try {\n        FSEditLogOp op = null;\n        while ((op = readOp(elis)) != null) {\n          // break out of here in the unlikely event that syncTxid is so\n          // out of date that its segment has already been deleted, so the first\n          // txid we get is greater than syncTxid\n          if (syncTxid > 0 && op.getTransactionId() > syncTxid) {\n            breakOuter = true;\n            break;\n          }\n\n          EventBatch eventBatch = InotifyFSEditLogOpTranslator.translate(op);\n          if (eventBatch != null) {\n            batches.add(eventBatch);\n            totalEvents += eventBatch.getEvents().length;\n          }\n          if (op.getTransactionId() > maxSeenTxid) {\n            maxSeenTxid = op.getTransactionId();\n          }\n          if (firstSeenTxid == -1) {\n            firstSeenTxid = op.getTransactionId();\n          }\n          if (totalEvents >= maxEventsPerRPC || (syncTxid > 0 &&\n              op.getTransactionId() == syncTxid)) {\n            // we're done\n            breakOuter = true;\n            break;\n          }\n        }\n      } finally {\n        elis.close();\n      }\n      if (breakOuter) {\n        break;\n      }\n    }\n\n    return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkNNStartup": "  private void checkNNStartup() throws IOException {\n    if (!this.nn.isStarted()) {\n      String message = NameNode.composeNotStartedMessage(this.nn.getRole());\n      throw new RetriableException(message);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcProtobufRequest request = (RpcProtobufRequest) writableRequest;\n        RequestHeaderProto rpcRequest = request.getRequestHeader();\n        String methodName = rpcRequest.getMethodName();\n\n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = request.getValue(prototype);\n\n        Message result;\n        long startTime = Time.now();\n        int qTime = (int) (startTime - receiveTime);\n        Exception exception = null;\n        boolean isDeferred = false;\n        try {\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          currentCallInfo.set(new CallInfo(server, methodName));\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n          // Check if this needs to be a deferred response,\n          // by checking the ThreadLocal callback being set\n          if (currentCallback.get() != null) {\n            Server.getCurCall().get().deferResponse();\n            isDeferred = true;\n            currentCallback.set(null);\n            return null;\n          }\n        } catch (ServiceException e) {\n          exception = (Exception) e.getCause();\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          exception = e;\n          throw e;\n        } finally {\n          currentCallInfo.set(null);\n          int processingTime = (int) (Time.now() - startTime);\n          if (LOG.isDebugEnabled()) {\n            String msg =\n                \"Served: \" + methodName + (isDeferred ? \", deferred\" : \"\") +\n                    \", queueTime= \" + qTime +\n                    \" procesingTime= \" + processingTime;\n            if (exception != null) {\n              msg += \" exception= \" + exception.getClass().getSimpleName();\n            }\n            LOG.debug(msg);\n          }\n          String detailedMetricsName = (exception == null) ?\n              methodName :\n              exception.getClass().getSimpleName();\n          server.updateMetrics(detailedMetricsName, qTime, processingTime,\n              isDeferred);\n        }\n        return RpcWritable.wrap(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.get": "          public Message get(long timeout, TimeUnit unit) throws Exception {\n            return getReturnMessage(method, arr.get(timeout, unit));\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getRequestHeader": "    RequestHeaderProto getRequestHeader() throws IOException {\n      if (getByteBuffer() != null && requestHeader == null) {\n        requestHeader = getValue(RequestHeaderProto.getDefaultInstance());\n      }\n      return requestHeader;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getProtocolImpl": "      private static ProtoClassProtoImpl getProtocolImpl(RPC.Server server,\n          String protoName, long clientVersion) throws RpcServerException {\n        ProtoNameVer pv = new ProtoNameVer(protoName, clientVersion);\n        ProtoClassProtoImpl impl = \n            server.getProtocolImplMap(RPC.RpcKind.RPC_PROTOCOL_BUFFER).get(pv);\n        if (impl == null) { // no match for Protocol AND Version\n          VerProtocolImpl highest = \n              server.getHighestSupportedProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER, \n                  protoName);\n          if (highest == null) {\n            throw new RpcNoSuchProtocolException(\n                \"Unknown protocol: \" + protoName);\n          }\n          // protocol supported but not the version that client wants\n          throw new RPC.VersionMismatch(protoName, clientVersion,\n              highest.version);\n        }\n        return impl;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.call": "    public Writable call(RPC.RpcKind rpcKind, String protocol,\n        Writable rpcRequest, long receiveTime) throws Exception {\n      return getRpcInvoker(rpcKind).call(this, protocol, rpcRequest,\n          receiveTime);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(RpcCall call,\n      RpcResponseHeaderProto header, Writable rv) throws IOException {\n    final byte[] response;\n    if (rv == null || (rv instanceof RpcWritable.ProtobufWrapper)) {\n      response = setupResponseForProtobuf(header, rv);\n    } else {\n      response = setupResponseForWritable(header, rv);\n    }\n    if (response.length > maxRespSize) {\n      LOG.warn(\"Large response size \" + response.length + \" for call \"\n          + call.toString());\n    }\n    call.setResponse(ByteBuffer.wrap(response));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.isResponseDeferred": "    public boolean isResponseDeferred() {\n      return this.deferredResponse;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.remove": "    private boolean remove(Connection connection) {\n      boolean removed = connections.remove(connection);\n      if (removed) {\n        count.getAndDecrement();\n      }\n      return removed;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeAll": "    void closeAll() {\n      // use a copy of the connections to be absolutely sure the concurrent\n      // iterator doesn't miss a connection\n      for (Connection connection : toArray()) {\n        close(connection);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(RPC.RpcKind rpcKind, String protocol,\n      Writable param, long receiveTime) throws Exception;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param protocolName - the protocol\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  private void authorize(UserGroupInformation user, String protocolName,\n      InetAddress addr) throws AuthorizationException {\n    if (authorize) {\n      if (protocolName == null) {\n        throw new AuthorizationException(\"Null protocol not authorized\");\n      }\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(protocolName, getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         protocolName);\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    boolean close(Connection connection) {\n      boolean exists = remove(connection);\n      if (exists) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(Thread.currentThread().getName() +\n              \": disconnecting client \" + connection +\n              \". Number of active connections: \"+ size());\n        }\n        // only close if actually removed to avoid double-closing due\n        // to possible races\n        connection.close();\n        // Remove authorized users only\n        if (connection.user != null && connection.connectionContextRead) {\n          decrUserConnections(connection.user.getShortUserName());\n        }\n      }\n      return exists;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isWritable()) {\n                doAsyncWrite(key);\n              }\n            } catch (CancelledKeyException cke) {\n              // something else closed the connection, ex. reader or the\n              // listener doing an idle scan.  ignore it and let them clean\n              // up\n              RpcCall call = (RpcCall)key.attachment();\n              if (call != null) {\n                LOG.info(Thread.currentThread().getName() +\n                    \": connection aborted from \" + call.connection);\n              }\n            } catch (IOException e) {\n              LOG.info(Thread.currentThread().getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = Time.now();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<RpcCall> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<RpcCall>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              RpcCall call = (RpcCall)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n\n          for (RpcCall call : calls) {\n            doPurge(call, now);\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeIdle": "    synchronized void closeIdle(boolean scanAll) {\n      long minLastContact = Time.now() - maxIdleTime;\n      // concurrent iterator might miss new connections added\n      // during the iteration, but that's ok because they won't\n      // be idle yet anyway and will be caught on next scan\n      int closed = 0;\n      for (Connection connection : connections) {\n        // stop if connections dropped below threshold unless scanning all\n        if (!scanAll && size() < idleScanThreshold) {\n          break;\n        }\n        // stop if not scanning all and max connections are closed\n        if (connection.isIdle() &&\n            connection.getLastContact() < minLastContact &&\n            close(connection) &&\n            !scanAll && (++closed == maxIdleToClose)) {\n          break;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.startIdleScan": "    void startIdleScan() {\n      scheduleIdleScanTask();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.sendResponse": "    private void sendResponse(RpcCall call) throws IOException {\n      responder.doRespond(call);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.stopIdleScan": "    void stopIdleScan() {\n      idleScanTimer.cancel();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws InterruptedException, IOException,  OutOfMemoryError {\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        channel.socket().setKeepAlive(true);\n        \n        Reader reader = getReader();\n        Connection c = connectionManager.register(channel);\n        // If the connectionManager can't take it, close the connection.\n        if (c == null) {\n          if (channel.isOpen()) {\n            IOUtils.cleanup(null, channel);\n          }\n          connectionManager.droppedConnections.getAndIncrement();\n          continue;\n        }\n        key.attach(c);  // so closeCurrentConnection can get the object\n        reader.addConnection(c);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.scheduleIdleScanTask": "    private void scheduleIdleScanTask() {\n      if (!running) {\n        return;\n      }\n      TimerTask idleScanTask = new TimerTask(){\n        @Override\n        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }\n      };\n      idleScanTimer.schedule(idleScanTask, idleScanInterval);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getRemoteUser": "    public UserGroupInformation getRemoteUser() {\n      return connection.user;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.populateResponseParamsOnError": "    private void populateResponseParamsOnError(Throwable t,\n                                               ResponseParams responseParams) {\n      if (t instanceof UndeclaredThrowableException) {\n        t = t.getCause();\n      }\n      logException(Server.LOG, t, this);\n      if (t instanceof RpcServerException) {\n        RpcServerException rse = ((RpcServerException) t);\n        responseParams.returnStatus = rse.getRpcStatusProto();\n        responseParams.detailedErr = rse.getRpcErrorCodeProto();\n      } else {\n        responseParams.returnStatus = RpcStatusProto.ERROR;\n        responseParams.detailedErr = RpcErrorCodeProto.ERROR_APPLICATION;\n      }\n      responseParams.errorClass = t.getClass().getName();\n      responseParams.error = StringUtils.stringifyException(t);\n      // Remove redundant error class name from the beginning of the\n      // stack trace\n      String exceptionHdr = responseParams.errorClass + \": \";\n      if (responseParams.error.startsWith(exceptionHdr)) {\n        responseParams.error =\n            responseParams.error.substring(exceptionHdr.length());\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause == null) {\n        throw new RuntimeException(\"PrivilegedActionException with no \" +\n                \"underlying cause. UGI [\" + this + \"]\" +\": \" + pae, pae);\n      } else if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.getTransactionId": "  public long getTransactionId() {\n    Preconditions.checkState(txid != HdfsServerConstants.INVALID_TXID);\n    return txid;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.selectInputStreams": "  public Collection<EditLogInputStream> selectInputStreams(long fromTxId,\n      long toAtLeastTxId, MetaRecoveryContext recovery, boolean inProgressOk,\n      boolean onlyDurableTxns) throws IOException {\n\n    List<EditLogInputStream> streams = new ArrayList<EditLogInputStream>();\n    synchronized(journalSetLock) {\n      Preconditions.checkState(journalSet.isOpen(), \"Cannot call \" +\n          \"selectInputStreams() on closed FSEditLog\");\n      selectInputStreams(streams, fromTxId, inProgressOk, onlyDurableTxns);\n    }\n\n    try {\n      checkForGaps(streams, fromTxId, toAtLeastTxId, inProgressOk);\n    } catch (IOException e) {\n      if (recovery != null) {\n        // If recovery mode is enabled, continue loading even if we know we\n        // can't load up to toAtLeastTxId.\n        LOG.error(\"Exception while selecting input streams\", e);\n      } else {\n        closeAllStreams(streams);\n        throw e;\n      }\n    }\n    return streams;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.closeAllStreams": "  static void closeAllStreams(Iterable<EditLogInputStream> streams) {\n    for (EditLogInputStream s : streams) {\n      IOUtils.closeStream(s);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.checkForGaps": "  private void checkForGaps(List<EditLogInputStream> streams, long fromTxId,\n      long toAtLeastTxId, boolean inProgressOk) throws IOException {\n    Iterator<EditLogInputStream> iter = streams.iterator();\n    long txId = fromTxId;\n    while (true) {\n      if (txId > toAtLeastTxId) return;\n      if (!iter.hasNext()) break;\n      EditLogInputStream elis = iter.next();\n      if (elis.getFirstTxId() > txId) break;\n      long next = elis.getLastTxId();\n      if (next == HdfsServerConstants.INVALID_TXID) {\n        if (!inProgressOk) {\n          throw new RuntimeException(\"inProgressOk = false, but \" +\n              \"selectInputStreams returned an in-progress edit \" +\n              \"log input stream (\" + elis + \")\");\n        }\n        // We don't know where the in-progress stream ends.\n        // It could certainly go all the way up to toAtLeastTxId.\n        return;\n      }\n      txId = next + 1;\n    }\n    throw new IOException(String.format(\"Gap in transactions. Expected to \"\n        + \"be able to read up until at least txid %d but unable to find any \"\n        + \"edit logs containing txid %d\", toAtLeastTxId, txId));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.getSyncTxId": "  public synchronized long getSyncTxId() {\n    return synctxid;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate": "  public static EventBatch translate(FSEditLogOp op) {\n    switch(op.opCode) {\n    case OP_ADD:\n      FSEditLogOp.AddOp addOp = (FSEditLogOp.AddOp) op;\n      if (addOp.blocks.length == 0) { // create\n        return new EventBatch(op.txid,\n            new Event[] { new Event.CreateEvent.Builder().path(addOp.path)\n            .ctime(addOp.atime)\n            .replication(addOp.replication)\n            .ownerName(addOp.permissions.getUserName())\n            .groupName(addOp.permissions.getGroupName())\n            .perms(addOp.permissions.getPermission())\n            .overwrite(addOp.overwrite)\n            .defaultBlockSize(addOp.blockSize)\n            .iNodeType(Event.CreateEvent.INodeType.FILE).build() });\n      } else { // append\n        return new EventBatch(op.txid,\n            new Event[]{new Event.AppendEvent.Builder()\n                .path(addOp.path)\n                .build()});\n      }\n    case OP_CLOSE:\n      FSEditLogOp.CloseOp cOp = (FSEditLogOp.CloseOp) op;\n      return new EventBatch(op.txid, new Event[] {\n          new Event.CloseEvent(cOp.path, getSize(cOp), cOp.mtime) });\n    case OP_APPEND:\n      FSEditLogOp.AppendOp appendOp = (FSEditLogOp.AppendOp) op;\n      return new EventBatch(op.txid, new Event[] {new Event.AppendEvent\n          .Builder().path(appendOp.path).newBlock(appendOp.newBlock).build()});\n    case OP_SET_REPLICATION:\n      FSEditLogOp.SetReplicationOp setRepOp = (FSEditLogOp.SetReplicationOp) op;\n      return new EventBatch(op.txid,\n        new Event[] { new Event.MetadataUpdateEvent.Builder()\n          .metadataType(Event.MetadataUpdateEvent.MetadataType.REPLICATION)\n          .path(setRepOp.path)\n          .replication(setRepOp.replication).build() });\n    case OP_CONCAT_DELETE:\n      FSEditLogOp.ConcatDeleteOp cdOp = (FSEditLogOp.ConcatDeleteOp) op;\n      List<Event> events = Lists.newArrayList();\n      events.add(new Event.AppendEvent.Builder()\n          .path(cdOp.trg)\n          .build());\n      for (String src : cdOp.srcs) {\n        events.add(new Event.UnlinkEvent.Builder()\n          .path(src)\n          .timestamp(cdOp.timestamp)\n          .build());\n      }\n      events.add(new Event.CloseEvent(cdOp.trg, -1, cdOp.timestamp));\n      return new EventBatch(op.txid, events.toArray(new Event[0]));\n    case OP_RENAME_OLD:\n      FSEditLogOp.RenameOldOp rnOpOld = (FSEditLogOp.RenameOldOp) op;\n      return new EventBatch(op.txid, new Event[] {\n          new Event.RenameEvent.Builder()\n              .srcPath(rnOpOld.src)\n              .dstPath(rnOpOld.dst)\n              .timestamp(rnOpOld.timestamp)\n              .build() });\n    case OP_RENAME:\n      FSEditLogOp.RenameOp rnOp = (FSEditLogOp.RenameOp) op;\n      return new EventBatch(op.txid, new Event[] {\n          new Event.RenameEvent.Builder()\n            .srcPath(rnOp.src)\n            .dstPath(rnOp.dst)\n            .timestamp(rnOp.timestamp)\n            .build() });\n    case OP_DELETE:\n      FSEditLogOp.DeleteOp delOp = (FSEditLogOp.DeleteOp) op;\n      return new EventBatch(op.txid, new Event[] {\n          new Event.UnlinkEvent.Builder()\n            .path(delOp.path)\n            .timestamp(delOp.timestamp)\n            .build() });\n    case OP_MKDIR:\n      FSEditLogOp.MkdirOp mkOp = (FSEditLogOp.MkdirOp) op;\n      return new EventBatch(op.txid,\n        new Event[] { new Event.CreateEvent.Builder().path(mkOp.path)\n          .ctime(mkOp.timestamp)\n          .ownerName(mkOp.permissions.getUserName())\n          .groupName(mkOp.permissions.getGroupName())\n          .perms(mkOp.permissions.getPermission())\n          .iNodeType(Event.CreateEvent.INodeType.DIRECTORY).build() });\n    case OP_SET_PERMISSIONS:\n      FSEditLogOp.SetPermissionsOp permOp = (FSEditLogOp.SetPermissionsOp) op;\n      return new EventBatch(op.txid,\n        new Event[] { new Event.MetadataUpdateEvent.Builder()\n          .metadataType(Event.MetadataUpdateEvent.MetadataType.PERMS)\n          .path(permOp.src)\n          .perms(permOp.permissions).build() });\n    case OP_SET_OWNER:\n      FSEditLogOp.SetOwnerOp ownOp = (FSEditLogOp.SetOwnerOp) op;\n      return new EventBatch(op.txid,\n        new Event[] { new Event.MetadataUpdateEvent.Builder()\n          .metadataType(Event.MetadataUpdateEvent.MetadataType.OWNER)\n          .path(ownOp.src)\n          .ownerName(ownOp.username).groupName(ownOp.groupname).build() });\n    case OP_TIMES:\n      FSEditLogOp.TimesOp timesOp = (FSEditLogOp.TimesOp) op;\n      return new EventBatch(op.txid,\n        new Event[] { new Event.MetadataUpdateEvent.Builder()\n          .metadataType(Event.MetadataUpdateEvent.MetadataType.TIMES)\n          .path(timesOp.path)\n          .atime(timesOp.atime).mtime(timesOp.mtime).build() });\n    case OP_SYMLINK:\n      FSEditLogOp.SymlinkOp symOp = (FSEditLogOp.SymlinkOp) op;\n      return new EventBatch(op.txid,\n        new Event[] { new Event.CreateEvent.Builder().path(symOp.path)\n          .ctime(symOp.atime)\n          .ownerName(symOp.permissionStatus.getUserName())\n          .groupName(symOp.permissionStatus.getGroupName())\n          .perms(symOp.permissionStatus.getPermission())\n          .symlinkTarget(symOp.value)\n          .iNodeType(Event.CreateEvent.INodeType.SYMLINK).build() });\n    case OP_REMOVE_XATTR:\n      FSEditLogOp.RemoveXAttrOp rxOp = (FSEditLogOp.RemoveXAttrOp) op;\n      return new EventBatch(op.txid,\n        new Event[] { new Event.MetadataUpdateEvent.Builder()\n          .metadataType(Event.MetadataUpdateEvent.MetadataType.XATTRS)\n          .path(rxOp.src)\n          .xAttrs(rxOp.xAttrs)\n          .xAttrsRemoved(true).build() });\n    case OP_SET_XATTR:\n      FSEditLogOp.SetXAttrOp sxOp = (FSEditLogOp.SetXAttrOp) op;\n      return new EventBatch(op.txid,\n        new Event[] { new Event.MetadataUpdateEvent.Builder()\n          .metadataType(Event.MetadataUpdateEvent.MetadataType.XATTRS)\n          .path(sxOp.src)\n          .xAttrs(sxOp.xAttrs)\n          .xAttrsRemoved(false).build() });\n    case OP_SET_ACL:\n      FSEditLogOp.SetAclOp saOp = (FSEditLogOp.SetAclOp) op;\n      return new EventBatch(op.txid,\n        new Event[] { new Event.MetadataUpdateEvent.Builder()\n          .metadataType(Event.MetadataUpdateEvent.MetadataType.ACLS)\n          .path(saOp.src)\n          .acls(saOp.aclEntries).build() });\n    case OP_TRUNCATE:\n      FSEditLogOp.TruncateOp tOp = (FSEditLogOp.TruncateOp) op;\n      return new EventBatch(op.txid, new Event[] {\n          new Event.TruncateEvent(tOp.src, tOp.newLength, tOp.timestamp) });\n    default:\n      return null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.getSize": "  private static long getSize(FSEditLogOp.AddCloseOp acOp) {\n    long size = 0;\n    for (Block b : acOp.getBlocks()) {\n      size += b.getNumBytes();\n    }\n    return size;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getCurCall": "  public static ThreadLocal<Call> getCurCall() {\n    return CurCall;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcWritable.wrap": "    public static Buffer wrap(ByteBuffer bb) {\n      return new Buffer(bb);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.updateMetrics": "  void updateMetrics(String name, int queueTime, int processingTime,\n                     boolean deferredCall) {\n    rpcMetrics.addRpcQueueTime(queueTime);\n    if (!deferredCall) {\n      rpcMetrics.addRpcProcessingTime(processingTime);\n      rpcDetailedMetrics.addProcessingTime(name, processingTime);\n      callQueue.addResponseTime(name, getPriorityLevel(), queueTime,\n          processingTime);\n      if (isLogSlowRPC()) {\n        logSlowRpcCalls(name, processingTime);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.logSlowRpcCalls": "  void logSlowRpcCalls(String methodName, int processingTime) {\n    final int deviation = 3;\n\n    // 1024 for minSampleSize just a guess -- not a number computed based on\n    // sample size analysis. It is chosen with the hope that this\n    // number is high enough to avoid spurious logging, yet useful\n    // in practice.\n    final int minSampleSize = 1024;\n    final double threeSigma = rpcMetrics.getProcessingMean() +\n        (rpcMetrics.getProcessingStdDev() * deviation);\n\n    if ((rpcMetrics.getProcessingSampleCount() > minSampleSize) &&\n        (processingTime > threeSigma)) {\n      if(LOG.isWarnEnabled()) {\n        String client = CurCall.get().toString();\n        LOG.warn(\n            \"Slow RPC : \" + methodName + \" took \" + processingTime +\n                \" milliseconds to process from client \" + client);\n      }\n      rpcMetrics.incrSlowRpc();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.getPriorityLevel": "    public int getPriorityLevel() {\n      return this.priorityLevel;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.isLogSlowRPC": "  protected boolean isLogSlowRPC() {\n    return logSlowRPC;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.CallerContext.setCurrent": "  public static void setCurrent(CallerContext callerContext) {\n    CurrentCallerContextHolder.CALLER_CONTEXT.set(callerContext);\n  }"
        },
        "bug_report": {
            "Title": "Kerberized inotify client fails despite kinit properly",
            "Description": "This issue is similar to HDFS-10799.\r\n\r\nHDFS-10799 turned out to be a client side issue where client is responsible for renewing kerberos ticket actively.\r\n\r\nHowever we found in a slightly setup even if client has valid Kerberos credentials, inotify still fails.\r\n\r\nSuppose client uses principal hdfs@EXAMPLE.COM, \r\n namenode 1 uses server principal hdfs/nn1.example.com@EXAMPLE.COM\r\n namenode 2 uses server principal hdfs/nn2.example.com@EXAMPLE.COM\r\n\r\n*After Namenodes starts for longer than kerberos ticket lifetime*, the client fails with the following error:\r\n{noformat}\r\n18/01/19 11:23:02 WARN security.UserGroupInformation: PriviledgedActionException as:hdfs@GCE.CLOUDERA.COM (auth:KERBEROS) cause:org.apache.hadoop.ipc.RemoteException(java.io.IOException): We encountered an error reading https://nn2.example.com:8481/getJournal?jid=ns1&segmentTxId=8662&storageInfo=-60%3A353531113%3A0%3Acluster3, https://nn1.example.com:8481/getJournal?jid=ns1&segmentTxId=8662&storageInfo=-60%3A353531113%3A0%3Acluster3.  During automatic edit log failover, we noticed that all of the remaining edit log streams are shorter than the current one!  The best remaining edit log ends at transaction 8683, but we thought we could read up to transaction 8684.  If you continue, metadata will be lost forever!\r\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:213)\r\n        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.readOp(NameNodeRpcServer.java:1701)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEditsFromTxid(NameNodeRpcServer.java:1763)\r\n        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getEditsFromTxid(AuthorizationProviderProxyClientProtocol.java:1011)\r\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getEditsFromTxid(ClientNamenodeProtocolServerSideTranslatorPB.java:1490)\r\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\r\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\r\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216)\r\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2212)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:415)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)\r\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2210)\r\n{noformat}\r\nTypically if NameNode has an expired Kerberos ticket, the error handling for the typical edit log tailing would let NameNode to relogin with its own Kerberos principal. However, when inotify uses the same code path to retrieve edits, since the current user is the inotify client's principal, unless client uses the same principal as the NameNode, NameNode can't do it on behalf of the client.\r\n\r\nTherefore, a more appropriate approach is to use proxy user so that NameNode can retrieving edits on behalf of the client.\r\n\r\nI will attach a patch to fix it. This patch has been verified to work for a CDH5.10.2 cluster, however it seems impossible to craft a unit test for this fix because the way Hadoop UGI handles Kerberos credentials (I can't have a single process that logins as two Kerberos principals simultaneously and let them establish connection)\r\n\r\nA possible workaround is for the inotify client to use the active NameNode's server principal. However, that's not going to work when there's a namenode failover, because then the client's principal will not be consistent with the active NN's one, and then fails to authenticate.\r\n\r\nCredit: this bug was confirmed and reproduced by [~pifta] and [~r1pp3rj4ck]"
        }
    },
    {
        "filename": "HDFS-3374.json",
        "creation_time": "2012-05-04T22:08:19.000+0000",
        "stack_trace": "```\njava.lang.Exception: No edit streams are accessible\n    at org.apache.hadoop.hdfs.server.namenode.FSEditLog.fatalExit(FSEditLog.java:388)\n    at org.apache.hadoop.hdfs.server.namenode.FSEditLog.exitIfNoStreams(FSEditLog.java:407)\n    at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsAndStorageDir(FSEditLog.java:432)\n    at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsStreamsAndStorageDirs(FSEditLog.java:468)\n    at org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync(FSEditLog.java:1028)\n    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logUpdateMasterKey(FSNamesystem.java:5641)\n    at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager.logUpdateMasterKey(DelegationTokenSecretManager.java:286)\n    at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.updateCurrentKey(AbstractDelegationTokenSecretManager.java:150)\n    at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.rollMasterKey(AbstractDelegationTokenSecretManager.java:174)\n    at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:385)\n    at java.lang.Thread.run(Thread.java:662)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync": "  public void logSync() {\n    long syncStart = 0;\n\n    // Fetch the transactionId of this thread. \n    long mytxid = myTransactionId.get().txid;\n    \n    boolean sync = false;\n    try {\n      EditLogOutputStream logStream = null;\n      synchronized (this) {\n        try {\n          printStatistics(false);\n\n          // if somebody is already syncing, then wait\n          while (mytxid > synctxid && isSyncRunning) {\n            try {\n              wait(1000);\n            } catch (InterruptedException ie) {\n            }\n          }\n  \n          //\n          // If this transaction was already flushed, then nothing to do\n          //\n          if (mytxid <= synctxid) {\n            numTransactionsBatchedInSync++;\n            if (metrics != null) {\n              // Metrics is non-null only when used inside name node\n              metrics.incrTransactionsBatchedInSync();\n            }\n            return;\n          }\n     \n          // now, this thread will do the sync\n          syncStart = txid;\n          isSyncRunning = true;\n          sync = true;\n  \n          // swap buffers\n          try {\n            if (journalSet.isEmpty()) {\n              throw new IOException(\"No journals available to flush\");\n            }\n            editLogStream.setReadyToFlush();\n          } catch (IOException e) {\n            LOG.fatal(\"Could not sync enough journals to persistent storage. \"\n                + \"Unsynced transactions: \" + (txid - synctxid),\n                new Exception());\n            runtime.exit(1);\n          }\n        } finally {\n          // Prevent RuntimeException from blocking other log edit write \n          doneWithAutoSyncScheduling();\n        }\n        //editLogStream may become null,\n        //so store a local variable for flush.\n        logStream = editLogStream;\n      }\n      \n      // do the sync\n      long start = now();\n      try {\n        if (logStream != null) {\n          logStream.flush();\n        }\n      } catch (IOException ex) {\n        synchronized (this) {\n          LOG.fatal(\"Could not sync enough journals to persistent storage. \"\n              + \"Unsynced transactions: \" + (txid - synctxid), new Exception());\n          runtime.exit(1);\n        }\n      }\n      long elapsed = now() - start;\n  \n      if (metrics != null) { // Metrics non-null only when used inside name node\n        metrics.addSync(elapsed);\n      }\n      \n    } finally {\n      // Prevent RuntimeException from blocking other log edit sync \n      synchronized (this) {\n        if (sync) {\n          synctxid = syncStart;\n          isSyncRunning = false;\n        }\n        this.notifyAll();\n     }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.printStatistics": "  private void printStatistics(boolean force) {\n    long now = now();\n    if (lastPrintTime + 60000 > now && !force) {\n      return;\n    }\n    lastPrintTime = now;\n    StringBuilder buf = new StringBuilder();\n    buf.append(\"Number of transactions: \");\n    buf.append(numTransactions);\n    buf.append(\" Total time for transactions(ms): \");\n    buf.append(totalTimeTransactions);\n    buf.append(\"Number of transactions batched in Syncs: \");\n    buf.append(numTransactionsBatchedInSync);\n    buf.append(\" Number of syncs: \");\n    buf.append(editLogStream.getNumSync());\n    buf.append(\" SyncTimes(ms): \");\n    buf.append(journalSet.getSyncTimes());\n    LOG.info(buf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.doneWithAutoSyncScheduling": "  synchronized void doneWithAutoSyncScheduling() {\n    if (isAutoSyncScheduled) {\n      isAutoSyncScheduled = false;\n      notifyAll();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logUpdateMasterKey": "  public void logUpdateMasterKey(DelegationKey key) {\n    \n    assert !isInSafeMode() :\n      \"this should never be called while in safemode, since we stop \" +\n      \"the DT manager before entering safemode!\";\n    // No need to hold FSN lock since we don't access any internal\n    // structures, and this is stopped before the FSN shuts itself\n    // down, etc.\n    getEditLog().logUpdateMasterKey(key);\n    getEditLog().logSync();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEditLog": "  public FSEditLog getEditLog() {\n    return getFSImage().getEditLog();\n  }    ",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.isInSafeMode": "  public boolean isInSafeMode() {\n    // safeMode is volatile, and may be set to null at any time\n    SafeModeInfo safeMode = this.safeMode;\n    if (safeMode == null)\n      return false;\n    return safeMode.isOn();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager.logUpdateMasterKey": "  protected void logUpdateMasterKey(DelegationKey key)\n      throws IOException {\n    synchronized (noInterruptsLock) {\n      // The edit logging code will fail catastrophically if it\n      // is interrupted during a logSync, since the interrupt\n      // closes the edit log files. Doing this inside the\n      // above lock and then checking interruption status\n      // prevents this bug.\n      if (Thread.interrupted()) {\n        throw new InterruptedIOException(\n            \"Interrupted before updating master key\");\n      }\n      namesystem.logUpdateMasterKey(key);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.updateCurrentKey": "  private void updateCurrentKey() throws IOException {\n    LOG.info(\"Updating the current master key for generating delegation tokens\");\n    /* Create a new currentKey with an estimated expiry date. */\n    int newCurrentId;\n    synchronized (this) {\n      newCurrentId = currentId+1;\n    }\n    DelegationKey newKey = new DelegationKey(newCurrentId, System\n        .currentTimeMillis()\n        + keyUpdateInterval + tokenMaxLifetime, generateSecret());\n    //Log must be invoked outside the lock on 'this'\n    logUpdateMasterKey(newKey);\n    synchronized (this) {\n      currentId = newKey.getKeyId();\n      currentKey = newKey;\n      allKeys.put(currentKey.getKeyId(), currentKey);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.logUpdateMasterKey": "  protected void logUpdateMasterKey(DelegationKey key) throws IOException {\n    return;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.rollMasterKey": "  void rollMasterKey() throws IOException {\n    synchronized (this) {\n      removeExpiredKeys();\n      /* set final expiry date for retiring currentKey */\n      currentKey.setExpiryDate(System.currentTimeMillis() + tokenMaxLifetime);\n      /*\n       * currentKey might have been removed by removeExpiredKeys(), if\n       * updateMasterKey() isn't called at expected interval. Add it back to\n       * allKeys just in case.\n       */\n      allKeys.put(currentKey.getKeyId(), currentKey);\n    }\n    updateCurrentKey();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeExpiredKeys": "  private synchronized void removeExpiredKeys() {\n    long now = System.currentTimeMillis();\n    for (Iterator<Map.Entry<Integer, DelegationKey>> it = allKeys.entrySet()\n        .iterator(); it.hasNext();) {\n      Map.Entry<Integer, DelegationKey> e = it.next();\n      if (e.getValue().getExpiryDate() < now) {\n        it.remove();\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.run": "    public void run() {\n      LOG.info(\"Starting expired delegation token remover thread, \"\n          + \"tokenRemoverScanInterval=\" + tokenRemoverScanInterval\n          / (60 * 1000) + \" min(s)\");\n      try {\n        while (running) {\n          long now = System.currentTimeMillis();\n          if (lastMasterKeyUpdate + keyUpdateInterval < now) {\n            try {\n              rollMasterKey();\n              lastMasterKeyUpdate = now;\n            } catch (IOException e) {\n              LOG.error(\"Master key updating failed: \", e);\n            }\n          }\n          if (lastTokenCacheCleanup + tokenRemoverScanInterval < now) {\n            removeExpiredToken();\n            lastTokenCacheCleanup = now;\n          }\n          try {\n            Thread.sleep(Math.min(5000, keyUpdateInterval)); // 5 seconds\n          } catch (InterruptedException ie) {\n            LOG\n            .error(\"InterruptedExcpetion recieved for ExpiredTokenRemover thread \"\n                + ie);\n          }\n        }\n      } catch (Throwable t) {\n        LOG.error(\"ExpiredTokenRemover thread received unexpected exception. \"\n            + t);\n        Runtime.getRuntime().exit(-1);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeExpiredToken": "  private synchronized void removeExpiredToken() {\n    long now = System.currentTimeMillis();\n    Iterator<DelegationTokenInformation> i = currentTokens.values().iterator();\n    while (i.hasNext()) {\n      long renewDate = i.next().getRenewDate();\n      if (now > renewDate) {\n        i.remove();\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.flush": "  public void flush() throws IOException {\n    numSync++;\n    long start = now();\n    flushAndSync();\n    long end = now();\n    totalTimeSync += (end - start);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.flushAndSync": "  abstract protected void flushAndSync() throws IOException;\n\n  /**\n   * Flush data to persistent store.\n   * Collect sync metrics.\n   */\n  public void flush() throws IOException {\n    numSync++;\n    long start = now();\n    flushAndSync();\n    long end = now();\n    totalTimeSync += (end - start);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.journalSet.isEmpty": "  public boolean isEmpty() {\n    return !NameNodeResourcePolicy.areResourcesAvailable(journals,\n        minimumRedundantJournals);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.DelegationKey.getKeyId": "  public int getKeyId() {\n    return keyId;\n  }"
        },
        "bug_report": {
            "Title": "hdfs' TestDelegationToken fails intermittently with a race condition",
            "Description": "The testcase is failing because the MiniDFSCluster is shutdown before the secret manager can change the key, which calls system.exit with no edit streams available.\n\n{code}\n\n    [junit] 2012-05-04 15:03:51,521 WARN  common.Storage (FSImage.java:updateRemovedDirs(224)) - Removing storage dir /home/horton/src/hadoop/build/test/data/dfs/name1\n    [junit] 2012-05-04 15:03:51,522 FATAL namenode.FSNamesystem (FSEditLog.java:fatalExit(388)) - No edit streams are accessible\n    [junit] java.lang.Exception: No edit streams are accessible\n    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.fatalExit(FSEditLog.java:388)\n    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.exitIfNoStreams(FSEditLog.java:407)\n    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsAndStorageDir(FSEditLog.java:432)\n    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsStreamsAndStorageDirs(FSEditLog.java:468)\n    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync(FSEditLog.java:1028)\n    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logUpdateMasterKey(FSNamesystem.java:5641)\n    [junit]     at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager.logUpdateMasterKey(DelegationTokenSecretManager.java:286)\n    [junit]     at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.updateCurrentKey(AbstractDelegationTokenSecretManager.java:150)\n    [junit]     at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.rollMasterKey(AbstractDelegationTokenSecretManager.java:174)\n    [junit]     at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:385)\n    [junit]     at java.lang.Thread.run(Thread.java:662)\n    [junit] Running org.apache.hadoop.hdfs.security.TestDelegationToken\n    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec\n    [junit] Test org.apache.hadoop.hdfs.security.TestDelegationToken FAILED (crashed)\n{code}"
        }
    },
    {
        "filename": "HDFS-2359.json",
        "creation_time": "2011-09-23T00:20:53.000+0000",
        "stack_trace": "```\njava.io.IOException: Error in deleting blocks.\n        at org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate(FSDataset.java:1820)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.processCommand(DataNode.java:1074)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.processCommand(DataNode.java:1036)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.offerService(DataNode.java:891)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1419)\n        at java.lang.Thread.run(Thread.java:619)\n\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$LogFileHandler.appendLine(DataBlockScanner.java:788)\n        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.updateScanStatusInternal(DataBlockScanner.java:365)\n        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.verifiedByClient(DataBlockScanner.java:308)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:205)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:99)\n        at java.lang.Thread.run(Thread.java:619)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error = false;\n    for (int i = 0; i < invalidBlks.length; i++) {\n      File f = null;\n      FSVolume v;\n      synchronized (this) {\n        f = getFile(bpid, invalidBlks[i]);\n        ReplicaInfo dinfo = volumeMap.get(bpid, invalidBlks[i]);\n        if (dinfo == null || \n            dinfo.getGenerationStamp() != invalidBlks[i].getGenerationStamp()) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                           + invalidBlks[i] + \n                           \". BlockInfo not found in volumeMap.\");\n          error = true;\n          continue;\n        }\n        v = dinfo.getVolume();\n        if (f == null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Block not found in blockMap.\" +\n                            ((v == null) ? \" \" : \" Block found in volumeMap.\"));\n          error = true;\n          continue;\n        }\n        if (v == null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". No volume for this block.\" +\n                            \" Block found in blockMap. \" + f + \".\");\n          error = true;\n          continue;\n        }\n        File parent = f.getParentFile();\n        if (parent == null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Parent not found for file \" + f + \".\");\n          error = true;\n          continue;\n        }\n        ReplicaState replicaState = dinfo.getState();\n        if (replicaState == ReplicaState.FINALIZED || \n            (replicaState == ReplicaState.RUR && \n                ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() == \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      File metaFile = getMetaFile(f, invalidBlks[i].getGenerationStamp());\n      long dfsBytes = f.length() + metaFile.length();\n\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f, metaFile, dfsBytes,\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.FSDataset.clearPath": "    void clearPath(String bpid, File f) throws IOException {\n      BlockPoolSlice bp = getBlockPoolSlice(bpid);\n      bp.clearPath(f);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.FSDataset.getMetaFile": "  protected File getMetaFile(ExtendedBlock b) throws IOException {\n    return getMetaFile(getBlockFile(b), b.getGenerationStamp());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.FSDataset.getFile": "  private File getFile(String bpid, long blockId) {\n    ReplicaInfo info = volumeMap.get(bpid, blockId);\n    if (info != null) {\n      return info.getBlockFile();\n    }\n    return null;    \n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.processCommand": "    private boolean processCommand(DatanodeCommand cmd) throws IOException {\n      if (cmd == null)\n        return true;\n      final BlockCommand bcmd = \n        cmd instanceof BlockCommand? (BlockCommand)cmd: null;\n\n      switch(cmd.getAction()) {\n      case DatanodeProtocol.DNA_TRANSFER:\n        // Send a copy of a block to another datanode\n        transferBlocks(bcmd.getBlockPoolId(), bcmd.getBlocks(), bcmd.getTargets());\n        metrics.incrBlocksReplicated(bcmd.getBlocks().length);\n        break;\n      case DatanodeProtocol.DNA_INVALIDATE:\n        //\n        // Some local block(s) are obsolete and can be \n        // safely garbage-collected.\n        //\n        Block toDelete[] = bcmd.getBlocks();\n        try {\n          if (blockScanner != null) {\n            blockScanner.deleteBlocks(bcmd.getBlockPoolId(), toDelete);\n          }\n          // using global fsdataset\n          data.invalidate(bcmd.getBlockPoolId(), toDelete);\n        } catch(IOException e) {\n          checkDiskError();\n          throw e;\n        }\n        metrics.incrBlocksRemoved(toDelete.length);\n        break;\n      case DatanodeProtocol.DNA_SHUTDOWN:\n        // shut down the data node\n        shouldServiceRun = false;\n        return false;\n      case DatanodeProtocol.DNA_REGISTER:\n        // namenode requested a registration - at start or if NN lost contact\n        LOG.info(\"DatanodeCommand action: DNA_REGISTER\");\n        if (shouldRun && shouldServiceRun) {\n          register();\n        }\n        break;\n      case DatanodeProtocol.DNA_FINALIZE:\n        storage.finalizeUpgrade(((DatanodeCommand.Finalize) cmd)\n            .getBlockPoolId());\n        break;\n      case UpgradeCommand.UC_ACTION_START_UPGRADE:\n        // start distributed upgrade here\n        processDistributedUpgradeCommand((UpgradeCommand)cmd);\n        break;\n      case DatanodeProtocol.DNA_RECOVERBLOCK:\n        recoverBlocks(((BlockRecoveryCommand)cmd).getRecoveringBlocks());\n        break;\n      case DatanodeProtocol.DNA_ACCESSKEYUPDATE:\n        LOG.info(\"DatanodeCommand action: DNA_ACCESSKEYUPDATE\");\n        if (isBlockTokenEnabled) {\n          blockPoolTokenSecretManager.setKeys(blockPoolId, \n              ((KeyUpdateCommand) cmd).getExportedKeys());\n        }\n        break;\n      case DatanodeProtocol.DNA_BALANCERBANDWIDTHUPDATE:\n        LOG.info(\"DatanodeCommand action: DNA_BALANCERBANDWIDTHUPDATE\");\n        long bandwidth =\n                   ((BalancerBandwidthCommand) cmd).getBalancerBandwidthValue();\n        if (bandwidth > 0) {\n          DataXceiverServer dxcs =\n                       (DataXceiverServer) dataXceiverServer.getRunnable();\n          dxcs.balanceThrottler.setBandwidth(bandwidth);\n        }\n        break;\n      default:\n        LOG.warn(\"Unknown DatanodeCommand action: \" + cmd.getAction());\n      }\n      return true;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.checkDiskError": "  protected void checkDiskError( ) {\n    try {\n      data.checkDataDir();\n    } catch (DiskErrorException de) {\n      handleDiskError(de.getMessage());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getBlockPoolId": "    public String getBlockPoolId() {\n      return blockPoolId;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.register": "    void register() throws IOException {\n      LOG.info(\"in register: sid=\" + bpRegistration.getStorageID() + \";SI=\"\n          + bpRegistration.storageInfo); \n\n      // build and layout versions should match\n      String nsBuildVer = bpNamenode.versionRequest().getBuildVersion();\n      String stBuildVer = Storage.getBuildVersion();\n\n      if (!nsBuildVer.equals(stBuildVer)) {\n        LOG.warn(\"Data-node and name-node Build versions must be \" +\n          \"the same. Namenode build version: \" + nsBuildVer + \"Datanode \" +\n          \"build version: \" + stBuildVer);\n        throw new IncorrectVersionException(nsBuildVer, \"namenode\", stBuildVer);\n      }\n\n      if (HdfsConstants.LAYOUT_VERSION != bpNSInfo.getLayoutVersion()) {\n        LOG.warn(\"Data-node and name-node layout versions must be \" +\n          \"the same. Expected: \"+ HdfsConstants.LAYOUT_VERSION +\n          \" actual \"+ bpNSInfo.getLayoutVersion());\n        throw new IncorrectVersionException\n          (bpNSInfo.getLayoutVersion(), \"namenode\");\n      }\n\n      while(shouldRun && shouldServiceRun) {\n        try {\n          // Use returned registration from namenode with updated machine name.\n          bpRegistration = bpNamenode.registerDatanode(bpRegistration);\n\n          LOG.info(\"bpReg after =\" + bpRegistration.storageInfo + \n              \";sid=\" + bpRegistration.storageID + \";name=\"+bpRegistration.getName());\n\n          NetUtils.getHostname();\n          hostName = bpRegistration.getHost();\n          break;\n        } catch(SocketTimeoutException e) {  // namenode is busy\n          LOG.info(\"Problem connecting to server: \" + nnAddr);\n          try {\n            Thread.sleep(1000);\n          } catch (InterruptedException ie) {}\n        }\n      }\n\n      if (storage.getStorageID().equals(\"\")) {\n        storage.setStorageID(bpRegistration.getStorageID());\n        storage.writeAll();\n        LOG.info(\"New storage id \" + bpRegistration.getStorageID()\n            + \" is assigned to data-node \" + bpRegistration.getName());\n      } else if(!storage.getStorageID().equals(bpRegistration.getStorageID())) {\n        throw new IOException(\"Inconsistent storage IDs. Name-node returned \"\n            + bpRegistration.getStorageID() \n            + \". Expecting \" + storage.getStorageID());\n      }\n\n      if (!isBlockTokenInitialized) {\n        /* first time registering with NN */\n        ExportedBlockKeys keys = bpRegistration.exportedKeys;\n        isBlockTokenEnabled = keys.isBlockTokenEnabled();\n        if (isBlockTokenEnabled) {\n          long blockKeyUpdateInterval = keys.getKeyUpdateInterval();\n          long blockTokenLifetime = keys.getTokenLifetime();\n          LOG.info(\"Block token params received from NN: for block pool \" +\n              blockPoolId + \" keyUpdateInterval=\"\n              + blockKeyUpdateInterval / (60 * 1000)\n              + \" min(s), tokenLifetime=\" + blockTokenLifetime / (60 * 1000)\n              + \" min(s)\");\n          final BlockTokenSecretManager secretMgr = \n            new BlockTokenSecretManager(false, 0, blockTokenLifetime);\n          blockPoolTokenSecretManager.addBlockPool(blockPoolId, secretMgr);\n        }\n        isBlockTokenInitialized = true;\n      }\n\n      if (isBlockTokenEnabled) {\n        blockPoolTokenSecretManager.setKeys(blockPoolId,\n            bpRegistration.exportedKeys);\n        bpRegistration.exportedKeys = ExportedBlockKeys.DUMMY_KEYS;\n      }\n\n      LOG.info(\"in register:\" + \";bpDNR=\"+bpRegistration.storageInfo);\n\n      // random short delay - helps scatter the BR from all DNs\n      scheduleBlockReport(initialBlockReportDelay);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlocks": "  public Daemon recoverBlocks(final Collection<RecoveringBlock> blocks) {\n    Daemon d = new Daemon(threadGroup, new Runnable() {\n      /** Recover a list of blocks. It is run by the primary datanode. */\n      public void run() {\n        for(RecoveringBlock b : blocks) {\n          try {\n            logRecoverBlock(\"NameNode\", b.getBlock(), b.getLocations());\n            recoverBlock(b);\n          } catch (IOException e) {\n            LOG.warn(\"recoverBlocks FAILED: \" + b, e);\n          }\n        }\n      }\n    });\n    d.start();\n    return d;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.transferBlocks": "  private void transferBlocks(String poolId, Block blocks[],\n      DatanodeInfo xferTargets[][]) {\n    for (int i = 0; i < blocks.length; i++) {\n      try {\n        transferBlock(new ExtendedBlock(poolId, blocks[i]), xferTargets[i]);\n      } catch (IOException ie) {\n        LOG.warn(\"Failed to transfer block \" + blocks[i], ie);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.processDistributedUpgradeCommand": "    private void processDistributedUpgradeCommand(UpgradeCommand comm)\n    throws IOException {\n      UpgradeManagerDatanode upgradeManager = getUpgradeManager();\n      upgradeManager.processUpgradeCommand(comm);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.offerService": "    private void offerService() throws Exception {\n      LOG.info(\"For namenode \" + nnAddr + \" using DELETEREPORT_INTERVAL of \"\n          + deleteReportInterval + \" msec \" + \" BLOCKREPORT_INTERVAL of \"\n          + blockReportInterval + \"msec\" + \" Initial delay: \"\n          + initialBlockReportDelay + \"msec\" + \"; heartBeatInterval=\"\n          + heartBeatInterval);\n\n      //\n      // Now loop for a long time....\n      //\n      while (shouldRun && shouldServiceRun) {\n        try {\n          long startTime = now();\n\n          //\n          // Every so often, send heartbeat or block-report\n          //\n          if (startTime - lastHeartbeat > heartBeatInterval) {\n            //\n            // All heartbeat messages include following info:\n            // -- Datanode name\n            // -- data transfer port\n            // -- Total capacity\n            // -- Bytes remaining\n            //\n            lastHeartbeat = startTime;\n            if (!heartbeatsDisabledForTests) {\n              DatanodeCommand[] cmds = sendHeartBeat();\n              metrics.addHeartbeat(now() - startTime);\n              if (!processCommand(cmds))\n                continue;\n            }\n          }\n          if (pendingReceivedRequests > 0\n              || (startTime - lastDeletedReport > deleteReportInterval)) {\n            reportReceivedDeletedBlocks();\n            lastDeletedReport = startTime;\n          }\n\n          DatanodeCommand cmd = blockReport();\n          processCommand(cmd);\n\n          // Now safe to start scanning the block pool\n          if (blockScanner != null) {\n            blockScanner.addBlockPool(this.blockPoolId);\n          }\n\n          //\n          // There is no work to do;  sleep until hearbeat timer elapses, \n          // or work arrives, and then iterate again.\n          //\n          long waitTime = heartBeatInterval - \n          (System.currentTimeMillis() - lastHeartbeat);\n          synchronized(receivedAndDeletedBlockList) {\n            if (waitTime > 0 && pendingReceivedRequests == 0) {\n              try {\n                receivedAndDeletedBlockList.wait(waitTime);\n              } catch (InterruptedException ie) {\n                LOG.warn(\"BPOfferService for block pool=\"\n                    + this.getBlockPoolId() + \" received exception:\" + ie);\n              }\n            }\n          } // synchronized\n        } catch(RemoteException re) {\n          String reClass = re.getClassName();\n          if (UnregisteredNodeException.class.getName().equals(reClass) ||\n              DisallowedDatanodeException.class.getName().equals(reClass) ||\n              IncorrectVersionException.class.getName().equals(reClass)) {\n            LOG.warn(\"blockpool \" + blockPoolId + \" is shutting down\", re);\n            shouldServiceRun = false;\n            return;\n          }\n          LOG.warn(\"RemoteException in offerService\", re);\n          try {\n            long sleepTime = Math.min(1000, heartBeatInterval);\n            Thread.sleep(sleepTime);\n          } catch (InterruptedException ie) {\n            Thread.currentThread().interrupt();\n          }\n        } catch (IOException e) {\n          LOG.warn(\"IOException in offerService\", e);\n        }\n      } // while (shouldRun && shouldServiceRun)\n    } // offerService",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.reportReceivedDeletedBlocks": "    private void reportReceivedDeletedBlocks() throws IOException {\n\n      // check if there are newly received blocks\n      ReceivedDeletedBlockInfo[] receivedAndDeletedBlockArray = null;\n      int currentReceivedRequestsCounter;\n      synchronized (receivedAndDeletedBlockList) {\n        currentReceivedRequestsCounter = pendingReceivedRequests;\n        int numBlocks = receivedAndDeletedBlockList.size();\n        if (numBlocks > 0) {\n          //\n          // Send newly-received and deleted blockids to namenode\n          //\n          receivedAndDeletedBlockArray = receivedAndDeletedBlockList\n              .toArray(new ReceivedDeletedBlockInfo[numBlocks]);\n        }\n      }\n      if (receivedAndDeletedBlockArray != null) {\n        bpNamenode.blockReceivedAndDeleted(bpRegistration, blockPoolId,\n            receivedAndDeletedBlockArray);\n        synchronized (receivedAndDeletedBlockList) {\n          for (int i = 0; i < receivedAndDeletedBlockArray.length; i++) {\n            receivedAndDeletedBlockList.remove(receivedAndDeletedBlockArray[i]);\n          }\n          pendingReceivedRequests -= currentReceivedRequestsCounter;\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.blockReport": "    DatanodeCommand blockReport() throws IOException {\n      // send block report if timer has expired.\n      DatanodeCommand cmd = null;\n      long startTime = now();\n      if (startTime - lastBlockReport > blockReportInterval) {\n\n        // Create block report\n        long brCreateStartTime = now();\n        BlockListAsLongs bReport = data.getBlockReport(blockPoolId);\n\n        // Send block report\n        long brSendStartTime = now();\n        cmd = bpNamenode.blockReport(bpRegistration, blockPoolId, bReport\n            .getBlockListAsLongs());\n\n        // Log the block report processing stats from Datanode perspective\n        long brSendCost = now() - brSendStartTime;\n        long brCreateCost = brSendStartTime - brCreateStartTime;\n        metrics.addBlockReport(brSendCost);\n        LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n            + \" blocks took \" + brCreateCost + \" msec to generate and \"\n            + brSendCost + \" msecs for RPC and NN processing\");\n\n        // If we have sent the first block report, then wait a random\n        // time before we start the periodic block reports.\n        if (resetBlockReportTime) {\n          lastBlockReport = startTime - DFSUtil.getRandom().nextInt((int)(blockReportInterval));\n          resetBlockReportTime = false;\n        } else {\n          /* say the last block report was at 8:20:14. The current report\n           * should have started around 9:20:14 (default 1 hour interval).\n           * If current time is :\n           *   1) normal like 9:20:18, next report should be at 10:20:14\n           *   2) unexpected like 11:35:43, next report should be at 12:20:14\n           */\n          lastBlockReport += (now() - lastBlockReport) /\n          blockReportInterval * blockReportInterval;\n        }\n        LOG.info(\"sent block report, processed command:\" + cmd);\n      }\n      return cmd;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.addBlockPool": "    synchronized void addBlockPool(BPOfferService t) {\n      if (nameNodeThreads.get(t.getNNSocketAddress()) == null) {\n        throw new IllegalArgumentException(\n            \"Unknown BPOfferService thread for namenode address:\"\n                + t.getNNSocketAddress());\n      }\n      if (t.getBlockPoolId() == null) {\n        throw new IllegalArgumentException(\"Null blockpool id\");\n      }\n      bpMapping.put(t.getBlockPoolId(), t);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.sendHeartBeat": "    DatanodeCommand [] sendHeartBeat() throws IOException {\n      return bpNamenode.sendHeartbeat(bpRegistration,\n          data.getCapacity(),\n          data.getDfsUsed(),\n          data.getRemaining(),\n          data.getBlockPoolUsed(blockPoolId),\n          xmitsInProgress.get(),\n          getXceiverCount(), data.getNumFailedVolumes());\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.run": "      public void run() {\n        for(RecoveringBlock b : blocks) {\n          try {\n            logRecoverBlock(\"NameNode\", b.getBlock(), b.getLocations());\n            recoverBlock(b);\n          } catch (IOException e) {\n            LOG.warn(\"recoverBlocks FAILED: \" + b, e);\n          }\n        }\n      }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock": "  private void recoverBlock(RecoveringBlock rBlock) throws IOException {\n    ExtendedBlock block = rBlock.getBlock();\n    String blookPoolId = block.getBlockPoolId();\n    DatanodeInfo[] targets = rBlock.getLocations();\n    DatanodeID[] datanodeids = (DatanodeID[])targets;\n    List<BlockRecord> syncList = new ArrayList<BlockRecord>(datanodeids.length);\n    int errorCount = 0;\n\n    //check generation stamps\n    for(DatanodeID id : datanodeids) {\n      try {\n        BPOfferService bpos = blockPoolManager.get(blookPoolId);\n        DatanodeRegistration bpReg = bpos.bpRegistration;\n        InterDatanodeProtocol datanode = bpReg.equals(id)?\n            this: DataNode.createInterDataNodeProtocolProxy(id, getConf(),\n                socketTimeout);\n        ReplicaRecoveryInfo info = callInitReplicaRecovery(datanode, rBlock);\n        if (info != null &&\n            info.getGenerationStamp() >= block.getGenerationStamp() &&\n            info.getNumBytes() > 0) {\n          syncList.add(new BlockRecord(id, datanode, info));\n        }\n      } catch (RecoveryInProgressException ripE) {\n        InterDatanodeProtocol.LOG.warn(\n            \"Recovery for replica \" + block + \" on data-node \" + id\n            + \" is already in progress. Recovery id = \"\n            + rBlock.getNewGenerationStamp() + \" is aborted.\", ripE);\n        return;\n      } catch (IOException e) {\n        ++errorCount;\n        InterDatanodeProtocol.LOG.warn(\n            \"Failed to obtain replica info for block (=\" + block \n            + \") from datanode (=\" + id + \")\", e);\n      }\n    }\n\n    if (errorCount == datanodeids.length) {\n      throw new IOException(\"All datanodes failed: block=\" + block\n          + \", datanodeids=\" + Arrays.asList(datanodeids));\n    }\n\n    syncBlock(rBlock, syncList);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.start": "    void start() {\n      if ((bpThread != null) && (bpThread.isAlive())) {\n        //Thread is started already\n        return;\n      }\n      bpThread = new Thread(this, dnThreadName);\n      bpThread.setDaemon(true); // needed for JUnit testing\n      bpThread.start();\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.setupBP": "    void setupBP(Configuration conf, AbstractList<File> dataDirs) \n    throws IOException {\n      // get NN proxy\n      DatanodeProtocol dnp = \n        (DatanodeProtocol)RPC.waitForProxy(DatanodeProtocol.class,\n            DatanodeProtocol.versionID, nnAddr, conf);\n      setNameNode(dnp);\n\n      // handshake with NN\n      NamespaceInfo nsInfo = handshake();\n      setNamespaceInfo(nsInfo);\n      synchronized(DataNode.this) {\n        // we do not allow namenode from different cluster to register\n        if(clusterId != null && !clusterId.equals(nsInfo.clusterID)) {\n          throw new IOException(\n              \"cannot register with the namenode because clusterid do not match:\"\n              + \" nn=\" + nsInfo.getBlockPoolID() + \"; nn cid=\" + nsInfo.clusterID + \n              \";dn cid=\" + clusterId);\n        }\n\n        setupBPStorage();\n\n        setClusterId(nsInfo.clusterID);\n      }\n    \n      initPeriodicScanners(conf);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.createSocketAddr": "  public static InetSocketAddress createSocketAddr(String target\n                                                   ) throws IOException {\n    return NetUtils.createSocketAddr(target);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.cleanUp": "    private synchronized void cleanUp() {\n      \n      if(upgradeManager != null)\n        upgradeManager.shutdownUpgrade();\n      \n      blockPoolManager.remove(this);\n      shouldServiceRun = false;\n      RPC.stopProxy(bpNamenode);\n      if (blockScanner != null) {\n        blockScanner.removeBlockPool(this.getBlockPoolId());\n      }\n    \n      if (data != null) { \n        data.shutdownBlockPool(this.getBlockPoolId());\n      }\n\n      if (storage != null) {\n        storage.removeBlockPoolStorage(this.getBlockPoolId());\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.newSocket": "  protected Socket newSocket() throws IOException {\n    return (socketWriteTimeout > 0) ? \n           SocketChannel.open().socket() : new Socket();                                   \n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.startDistributedUpgradeIfNeeded": "    private void startDistributedUpgradeIfNeeded() throws IOException {\n      UpgradeManagerDatanode um = getUpgradeManager();\n      \n      if(!um.getUpgradeState())\n        return;\n      um.setUpgradeState(false, um.getUpgradeVersion());\n      um.startUpgrade();\n      return;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.logRecoverBlock": "  private static void logRecoverBlock(String who,\n      ExtendedBlock block, DatanodeID[] targets) {\n    StringBuilder msg = new StringBuilder(targets[0].getName());\n    for (int i = 1; i < targets.length; i++) {\n      msg.append(\", \" + targets[i].getName());\n    }\n    LOG.info(who + \" calls recoverBlock(block=\" + block\n        + \", targets=[\" + msg + \"])\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock": "  public void readBlock(final ExtendedBlock block,\n      final Token<BlockTokenIdentifier> blockToken,\n      final String clientName,\n      final long blockOffset,\n      final long length) throws IOException {\n    OutputStream baseStream = NetUtils.getOutputStream(s, \n        datanode.socketWriteTimeout);\n    DataOutputStream out = new DataOutputStream(new BufferedOutputStream(\n        baseStream, HdfsConstants.SMALL_BUFFER_SIZE));\n    checkAccess(out, true, block, blockToken,\n        Op.READ_BLOCK, BlockTokenSecretManager.AccessMode.READ);\n  \n    // send the block\n    BlockSender blockSender = null;\n    DatanodeRegistration dnR = \n      datanode.getDNRegistrationForBP(block.getBlockPoolId());\n    final String clientTraceFmt =\n      clientName.length() > 0 && ClientTraceLog.isInfoEnabled()\n        ? String.format(DN_CLIENTTRACE_FORMAT, localAddress, remoteAddress,\n            \"%d\", \"HDFS_READ\", clientName, \"%d\",\n            dnR.getStorageID(), block, \"%d\")\n        : dnR + \" Served block \" + block + \" to \" +\n            remoteAddress;\n\n    updateCurrentThreadName(\"Sending block \" + block);\n    try {\n      try {\n        blockSender = new BlockSender(block, blockOffset, length,\n            true, true, false, datanode, clientTraceFmt);\n      } catch(IOException e) {\n        LOG.info(\"opReadBlock \" + block + \" received exception \" + e);\n        sendResponse(s, ERROR, datanode.socketWriteTimeout);\n        throw e;\n      }\n      \n      // send op status\n      sendResponse(s, SUCCESS, datanode.socketWriteTimeout);\n\n      long read = blockSender.sendBlock(out, baseStream, null); // send data\n\n      if (blockSender.didSendEntireByteRange()) {\n        // If we sent the entire range, then we should expect the client\n        // to respond with a Status enum.\n        try {\n          ClientReadStatusProto stat = ClientReadStatusProto.parseFrom(\n              HdfsProtoUtil.vintPrefixed(in));\n          if (!stat.hasStatus()) {\n            LOG.warn(\"Client \" + s.getInetAddress() + \" did not send a valid status \" +\n                     \"code after reading. Will close connection.\");\n            IOUtils.closeStream(out);\n          }\n        } catch (IOException ioe) {\n          LOG.debug(\"Error reading client status response. Will close connection.\", ioe);\n          IOUtils.closeStream(out);\n        }\n      } else {\n        IOUtils.closeStream(out);\n      }\n      datanode.metrics.incrBytesRead((int) read);\n      datanode.metrics.incrBlocksRead();\n    } catch ( SocketException ignored ) {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(dnR + \":Ignoring exception while serving \" + block + \" to \" +\n            remoteAddress, ignored);\n      }\n      // Its ok for remote side to close the connection anytime.\n      datanode.metrics.incrBlocksRead();\n      IOUtils.closeStream(out);\n    } catch ( IOException ioe ) {\n      /* What exactly should we do here?\n       * Earlier version shutdown() datanode if there is disk error.\n       */\n      LOG.warn(dnR + \":Got exception while serving \" + block + \" to \"\n          + remoteAddress, ioe);\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(blockSender);\n    }\n\n    //update metrics\n    datanode.metrics.addReadBlockOp(elapsed());\n    datanode.metrics.incrReadsFromClient(isLocal);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.elapsed": "  private long elapsed() {\n    return now() - opStartTime;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess": "  private void checkAccess(DataOutputStream out, final boolean reply, \n      final ExtendedBlock blk,\n      final Token<BlockTokenIdentifier> t,\n      final Op op,\n      final BlockTokenSecretManager.AccessMode mode) throws IOException {\n    if (datanode.isBlockTokenEnabled) {\n      try {\n        datanode.blockPoolTokenSecretManager.checkAccess(t, null, blk, mode);\n      } catch(InvalidToken e) {\n        try {\n          if (reply) {\n            if (out == null) {\n              out = new DataOutputStream(\n                  NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n            }\n            \n            BlockOpResponseProto.Builder resp = BlockOpResponseProto.newBuilder()\n              .setStatus(ERROR_ACCESS_TOKEN);\n            if (mode == BlockTokenSecretManager.AccessMode.WRITE) {\n              DatanodeRegistration dnR = \n                datanode.getDNRegistrationForBP(blk.getBlockPoolId());\n              resp.setFirstBadLink(dnR.getName());\n            }\n            resp.build().writeDelimitedTo(out);\n            out.flush();\n          }\n          LOG.warn(\"Block token verification failed: op=\" + op\n              + \", remoteAddress=\" + remoteAddress\n              + \", message=\" + e.getLocalizedMessage());\n          throw e;\n        } finally {\n          IOUtils.closeStream(out);\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.sendResponse": "  private void sendResponse(Socket s, Status status,\n      long timeout) throws IOException {\n    DataOutputStream reply = \n      new DataOutputStream(NetUtils.getOutputStream(s, timeout));\n    \n    writeResponse(status, reply);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.updateCurrentThreadName": "  private void updateCurrentThreadName(String status) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"DataXceiver for client \").append(remoteAddress);\n    if (status != null) {\n      sb.append(\" [\").append(status).append(\"]\");\n    }\n    Thread.currentThread().setName(sb.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.run": "  public void run() {\n    int opsProcessed = 0;\n    Op op = null;\n    try {\n      int stdTimeout = s.getSoTimeout();\n\n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed != 0) {\n            assert socketKeepaliveTimeout > 0;\n            s.setSoTimeout(socketKeepaliveTimeout);\n          }\n          op = readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it's quite normal to get EOF here.\n          if (opsProcessed > 0 &&\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed != 0) {\n          s.setSoTimeout(stdTimeout);\n        }\n\n        // Make sure the xceiver count is not exceeded\n        int curXceiverCount = datanode.getXceiverCount();\n        if (curXceiverCount > dataXceiverServer.maxXceiverCount) {\n          throw new IOException(\"xceiverCount \" + curXceiverCount\n                                + \" exceeds the limit of concurrent xcievers \"\n                                + dataXceiverServer.maxXceiverCount);\n        }\n\n        opStartTime = now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!s.isClosed() && socketKeepaliveTimeout > 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getMachineName() + \":DataXceiver error processing \" +\n                ((op == null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.ReplicaInfo.getVolume": "  FSVolume getVolume() {\n    return volume;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    if( out == null ) {\n      throw new IOException( \"out stream is null\" );\n    }\n    this.throttler = throttler;\n\n    long initialOffset = offset;\n    long totalRead = 0;\n    OutputStream streamForSendChunks = out;\n    \n    final long startTime = ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n    try {\n      try {\n        checksum.writeHeader(out);\n        if ( chunkOffsetOK ) {\n          out.writeLong( offset );\n        }\n        out.flush();\n      } catch (IOException e) { //socket error\n        throw ioeToSocketException(e);\n      }\n      \n      int maxChunksPerPacket;\n      int pktSize = PacketHeader.PKT_HEADER_LEN;\n      \n      if (transferToAllowed && !verifyChecksum && \n          baseStream instanceof SocketOutputStream && \n          blockIn instanceof FileInputStream) {\n        \n        FileChannel fileChannel = ((FileInputStream)blockIn).getChannel();\n        \n        // blockInPosition also indicates sendChunks() uses transferTo.\n        blockInPosition = fileChannel.position();\n        streamForSendChunks = baseStream;\n        \n        // assure a mininum buffer size.\n        maxChunksPerPacket = (Math.max(HdfsConstants.IO_FILE_BUFFER_SIZE, \n                                       MIN_BUFFER_WITH_TRANSFERTO)\n                              + bytesPerChecksum - 1)/bytesPerChecksum;\n        \n        // allocate smaller buffer while using transferTo(). \n        pktSize += checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket = Math.max(1, (HdfsConstants.IO_FILE_BUFFER_SIZE\n            + bytesPerChecksum - 1) / bytesPerChecksum);\n        pktSize += (bytesPerChecksum + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf = ByteBuffer.allocate(pktSize);\n\n      while (endOffset > offset) {\n        long len = sendChunks(pktBuf, maxChunksPerPacket, \n                              streamForSendChunks);\n        offset += len;\n        totalRead += len + ((len + bytesPerChecksum - 1)/bytesPerChecksum*\n                            checksumSize);\n        seqno++;\n      }\n      try {\n        // send an empty packet to mark the end of the block\n        sendChunks(pktBuf, maxChunksPerPacket, streamForSendChunks);        \n        out.flush();\n      } catch (IOException e) { //socket error\n        throw ioeToSocketException(e);\n      }\n\n      sentEntireByteRange = true;\n    } finally {\n      if (clientTraceFmt != null) {\n        final long endTime = System.nanoTime();\n        ClientTraceLog.info(String.format(clientTraceFmt, totalRead, initialOffset, endTime - startTime));\n      }\n      close();\n    }\n\n    return totalRead;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.ioeToSocketException": "  private static IOException ioeToSocketException(IOException ioe) {\n    if (ioe.getClass().equals(IOException.class)) {\n      // \"se\" could be a new class in stead of SocketException.\n      IOException se = new SocketException(\"Original Exception : \" + ioe);\n      se.initCause(ioe);\n      /* Change the stacktrace so that original trace is not truncated\n       * when printed.*/ \n      se.setStackTrace(ioe.getStackTrace());\n      return se;\n    }\n    // otherwise just return the same exception.\n    return ioe;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.sendChunks": "  private int sendChunks(ByteBuffer pkt, int maxChunks, OutputStream out) \n                         throws IOException {\n    // Sends multiple chunks in one packet with a single write().\n\n    int len = (int) Math.min(endOffset - offset,\n                             (((long) bytesPerChecksum) * ((long) maxChunks)));\n    int numChunks = (len + bytesPerChecksum - 1)/bytesPerChecksum;\n    int packetLen = len + numChunks*checksumSize + 4;\n    boolean lastDataPacket = offset + len == endOffset && len > 0;\n    pkt.clear();\n\n\n    PacketHeader header = new PacketHeader(\n      packetLen, offset, seqno, (len == 0), len);\n    header.putInBuffer(pkt);\n\n    int checksumOff = pkt.position();\n    int checksumLen = numChunks * checksumSize;\n    byte[] buf = pkt.array();\n    \n    if (checksumSize > 0 && checksumIn != null) {\n      try {\n        checksumIn.readFully(buf, checksumOff, checksumLen);\n      } catch (IOException e) {\n        LOG.warn(\" Could not read or failed to veirfy checksum for data\"\n            + \" at offset \" + offset + \" for block \" + block, e);\n        IOUtils.closeStream(checksumIn);\n        checksumIn = null;\n        if (corruptChecksumOk) {\n          if (checksumOff < checksumLen) {\n            // Just fill the array with zeros.\n            Arrays.fill(buf, checksumOff, checksumLen, (byte) 0);\n          }\n        } else {\n          throw e;\n        }\n      }\n\n      // write in progress that we need to use to get last checksum\n      if (lastDataPacket && lastChunkChecksum != null) {\n        int start = checksumOff + checksumLen - checksumSize;\n        byte[] updatedChecksum = lastChunkChecksum.getChecksum();\n        \n        if (updatedChecksum != null) {\n          System.arraycopy(updatedChecksum, 0, buf, start, checksumSize);\n        }\n      }\n    }\n    \n    int dataOff = checksumOff + checksumLen;\n    \n    if (blockInPosition < 0) {\n      //normal transfer\n      IOUtils.readFully(blockIn, buf, dataOff, len);\n\n      if (verifyChecksum) {\n        int dOff = dataOff;\n        int cOff = checksumOff;\n        int dLeft = len;\n\n        for (int i=0; i<numChunks; i++) {\n          checksum.reset();\n          int dLen = Math.min(dLeft, bytesPerChecksum);\n          checksum.update(buf, dOff, dLen);\n          if (!checksum.compare(buf, cOff)) {\n            long failedPos = offset + len -dLeft;\n            throw new ChecksumException(\"Checksum failed at \" + \n                                        failedPos, failedPos);\n          }\n          dLeft -= dLen;\n          dOff += dLen;\n          cOff += checksumSize;\n        }\n      }\n      //writing is done below (mainly to handle IOException)\n    }\n    \n    try {\n      if (blockInPosition >= 0) {\n        //use transferTo(). Checks on out and blockIn are already done. \n\n        SocketOutputStream sockOut = (SocketOutputStream)out;\n        //first write the packet\n        sockOut.write(buf, 0, dataOff);\n        // no need to flush. since we know out is not a buffered stream. \n\n        sockOut.transferToFully(((FileInputStream)blockIn).getChannel(), \n                                blockInPosition, len);\n\n        blockInPosition += len;\n      } else {\n        // normal transfer\n        out.write(buf, 0, dataOff + len);\n      }\n      \n    } catch (IOException e) {\n      /* Exception while writing to the client. Connection closure from\n       * the other end is mostly the case and we do not care much about\n       * it. But other things can go wrong, especially in transferTo(),\n       * which we do not want to ignore.\n       *\n       * The message parsing below should not be considered as a good\n       * coding example. NEVER do it to drive a program logic. NEVER.\n       * It was done here because the NIO throws an IOException for EPIPE.\n       */\n      String ioem = e.getMessage();\n      if (!ioem.startsWith(\"Broken pipe\") && !ioem.startsWith(\"Connection reset\")) {\n        LOG.error(\"BlockSender.sendChunks() exception: \", e);\n      }\n      throw ioeToSocketException(e);\n    }\n\n    if (throttler != null) { // rebalancing so throttle\n      throttler.throttle(packetLen);\n    }\n\n    return len;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.close": "  public void close() throws IOException {\n    IOException ioe = null;\n    // close checksum file\n    if(checksumIn!=null) {\n      try {\n        checksumIn.close();\n      } catch (IOException e) {\n        ioe = e;\n      }\n      checksumIn = null;\n    }\n    // close data file\n    if(blockIn!=null) {\n      try {\n        blockIn.close();\n      } catch (IOException e) {\n        ioe = e;\n      }\n      blockIn = null;\n    }\n    // throw IOException if there is any\n    if(ioe!= null) {\n      throw ioe;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDNRegistrationForBP": "  DatanodeRegistration getDNRegistrationForBP(String bpid) \n  throws IOException {\n    BPOfferService bpos = blockPoolManager.get(bpid);\n    if(bpos==null || bpos.bpRegistration==null) {\n      throw new IOException(\"cannot find BPOfferService for bpid=\"+bpid);\n    }\n    return bpos.bpRegistration;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.get": "    synchronized BPOfferService get(String bpid) {\n      return bpMapping.get(bpid);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.didSendEntireByteRange": "  boolean didSendEntireByteRange() {\n    return sentEntireByteRange;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getMachineName": "  public String getMachineName() {\n    return hostName + \":\" + getPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getPort": "  int getPort() {\n    return selfAddr.getPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXceiverCount": "  int getXceiverCount() {\n    return threadGroup == null ? 0 : threadGroup.activeCount();\n  }"
        },
        "bug_report": {
            "Title": "NPE found in Datanode log while Disk failed during different HDFS operation",
            "Description": "Scenario:\nI have a cluster of 4 DN ,each of them have 12disks.\n\nIn hdfs-site.xml I have \"dfs.datanode.failed.volumes.tolerated=3\" \n\nDuring the execution of distcp (hdfs->hdfs), I am failing 3 disks in one Datanode, by making Data Directory permission 000, The distcp job is successful but , I am getting some NullPointerException in Datanode log\n\nIn one thread\n$hadoop distcp  /user/$HADOOPQA_USER/data1 /user/$HADOOPQA_USER/data3\n\nIn another thread in a datanode\n$ chmod 000 /xyz/{0,1,2}/hadoop/var/hdfs/data\n\nwhere [ dfs.data.dir is set as /xyz/{0..11}/hadoop/var/hdfs/data ]\n\nLog Snippet from the Datanode\n=============\n\n2011-09-19 12:43:40,314 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block\nblk_7065198814142552283_62557. BlockInfo not found in volumeMap.\n2011-09-19 12:43:40,314 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block\nblk_7066946313092770579_39189. BlockInfo not found in volumeMap.\n2011-09-19 12:43:40,314 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block\nblk_7070305189404753930_49359. BlockInfo not found in volumeMap.\n2011-09-19 12:43:40,327 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Error processing datanode Command\njava.io.IOException: Error in deleting blocks.\n        at org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate(FSDataset.java:1820)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.processCommand(DataNode.java:1074)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.processCommand(DataNode.java:1036)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.offerService(DataNode.java:891)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1419)\n        at java.lang.Thread.run(Thread.java:619)\n2011-09-19 12:43:41,304 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode:\nDatanodeRegistration(xx.xxx.xxx.xxx:xxxx, storageID=xx-xxxxxxxxxxxx-xx.xxx.xxx.xxx-xxxx-xxxxxxxxxxx, infoPort=1006,\nipcPort=8020):DataXceiver\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$LogFileHandler.appendLine(DataBlockScanner.java:788)\n        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.updateScanStatusInternal(DataBlockScanner.java:365)\n        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.verifiedByClient(DataBlockScanner.java:308)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:205)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:99)\n        at java.lang.Thread.run(Thread.java:619)\n2011-09-19 12:43:43,313 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block\nblk_7071818644980664768_40827. BlockInfo not found in volumeMap.\n2011-09-19 12:43:43,313 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block\nblk_7073840977856837621_62108. BlockInfo not found in volumeMap."
        }
    },
    {
        "filename": "HDFS-10986.json",
        "creation_time": "2016-10-08T00:50:36.000+0000",
        "stack_trace": "```\njava.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat org.apache.hadoop.hdfs.tools.DFSAdmin.run(DFSAdmin.java:2073)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\n\tat org.apache.hadoop.hdfs.tools.DFSAdmin.main(DFSAdmin.java:2225)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.run": "  public int run(String[] argv) throws Exception {\n\n    if (argv.length < 1) {\n      printUsage(\"\");\n      return -1;\n    }\n\n    int exitCode = -1;\n    int i = 0;\n    String cmd = argv[i++];\n\n    //\n    // verify that we have enough command line parameters\n    //\n    if (\"-safemode\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-allowSnapshot\".equalsIgnoreCase(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-disallowSnapshot\".equalsIgnoreCase(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-report\".equals(cmd)) {\n      if (argv.length < 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-saveNamespace\".equals(cmd)) {\n      if (argv.length != 1 && argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-rollEdits\".equals(cmd)) {\n      if (argv.length != 1) {\n        printUsage(cmd);\n        return exitCode;\n      }      \n    } else if (\"-restoreFailedStorage\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-refreshNodes\".equals(cmd)) {\n      if (argv.length != 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-finalizeUpgrade\".equals(cmd)) {\n      if (argv.length != 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (RollingUpgradeCommand.matches(cmd)) {\n      if (argv.length < 1 || argv.length > 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-metasave\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-refreshServiceAcl\".equals(cmd)) {\n      if (argv.length != 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-refresh\".equals(cmd)) {\n      if (argv.length < 3) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-refreshUserToGroupsMappings\".equals(cmd)) {\n      if (argv.length != 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-printTopology\".equals(cmd)) {\n      if(argv.length != 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-refreshNamenodes\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-reconfig\".equals(cmd)) {\n      if (argv.length != 4) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-deleteBlockPool\".equals(cmd)) {\n      if ((argv.length != 3) && (argv.length != 4)) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-setBalancerBandwidth\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-getBalancerBandwidth\".equalsIgnoreCase(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-fetchImage\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-shutdownDatanode\".equals(cmd)) {\n      if ((argv.length != 2) && (argv.length != 3)) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-getDatanodeInfo\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-triggerBlockReport\".equals(cmd)) {\n      if (argv.length < 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    }\n    \n    // initialize DFSAdmin\n    try {\n      init();\n    } catch (RPC.VersionMismatch v) {\n      System.err.println(\"Version Mismatch between client and server\"\n                         + \"... command aborted.\");\n      return exitCode;\n    } catch (IOException e) {\n      System.err.println(\"Bad connection to DFS... command aborted.\");\n      return exitCode;\n    }\n\n    Exception debugException = null;\n    exitCode = 0;\n    try {\n      if (\"-report\".equals(cmd)) {\n        report(argv, i);\n      } else if (\"-safemode\".equals(cmd)) {\n        setSafeMode(argv, i);\n      } else if (\"-allowSnapshot\".equalsIgnoreCase(cmd)) {\n        allowSnapshot(argv);\n      } else if (\"-disallowSnapshot\".equalsIgnoreCase(cmd)) {\n        disallowSnapshot(argv);\n      } else if (\"-saveNamespace\".equals(cmd)) {\n        exitCode = saveNamespace(argv);\n      } else if (\"-rollEdits\".equals(cmd)) {\n        exitCode = rollEdits();\n      } else if (\"-restoreFailedStorage\".equals(cmd)) {\n        exitCode = restoreFailedStorage(argv[i]);\n      } else if (\"-refreshNodes\".equals(cmd)) {\n        exitCode = refreshNodes();\n      } else if (\"-finalizeUpgrade\".equals(cmd)) {\n        exitCode = finalizeUpgrade();\n      } else if (RollingUpgradeCommand.matches(cmd)) {\n        exitCode = RollingUpgradeCommand.run(getDFS(), argv, i);\n      } else if (\"-metasave\".equals(cmd)) {\n        exitCode = metaSave(argv, i);\n      } else if (ClearQuotaCommand.matches(cmd)) {\n        exitCode = new ClearQuotaCommand(argv, i, getDFS()).runAll();\n      } else if (SetQuotaCommand.matches(cmd)) {\n        exitCode = new SetQuotaCommand(argv, i, getDFS()).runAll();\n      } else if (ClearSpaceQuotaCommand.matches(cmd)) {\n        exitCode = new ClearSpaceQuotaCommand(argv, i, getDFS()).runAll();\n      } else if (SetSpaceQuotaCommand.matches(cmd)) {\n        exitCode = new SetSpaceQuotaCommand(argv, i, getDFS()).runAll();\n      } else if (\"-refreshServiceAcl\".equals(cmd)) {\n        exitCode = refreshServiceAcl();\n      } else if (\"-refreshUserToGroupsMappings\".equals(cmd)) {\n        exitCode = refreshUserToGroupsMappings();\n      } else if (\"-refreshSuperUserGroupsConfiguration\".equals(cmd)) {\n        exitCode = refreshSuperUserGroupsConfiguration();\n      } else if (\"-refreshCallQueue\".equals(cmd)) {\n        exitCode = refreshCallQueue();\n      } else if (\"-refresh\".equals(cmd)) {\n        exitCode = genericRefresh(argv, i);\n      } else if (\"-printTopology\".equals(cmd)) {\n        exitCode = printTopology();\n      } else if (\"-refreshNamenodes\".equals(cmd)) {\n        exitCode = refreshNamenodes(argv, i);\n      } else if (\"-deleteBlockPool\".equals(cmd)) {\n        exitCode = deleteBlockPool(argv, i);\n      } else if (\"-setBalancerBandwidth\".equals(cmd)) {\n        exitCode = setBalancerBandwidth(argv, i);\n      } else if (\"-getBalancerBandwidth\".equals(cmd)) {\n        exitCode = getBalancerBandwidth(argv, i);\n      } else if (\"-fetchImage\".equals(cmd)) {\n        exitCode = fetchImage(argv, i);\n      } else if (\"-shutdownDatanode\".equals(cmd)) {\n        exitCode = shutdownDatanode(argv, i);\n      } else if (\"-evictWriters\".equals(cmd)) {\n        exitCode = evictWriters(argv, i);\n      } else if (\"-getDatanodeInfo\".equals(cmd)) {\n        exitCode = getDatanodeInfo(argv, i);\n      } else if (\"-reconfig\".equals(cmd)) {\n        exitCode = reconfig(argv, i);\n      } else if (\"-triggerBlockReport\".equals(cmd)) {\n        exitCode = triggerBlockReport(argv);\n      } else if (\"-help\".equals(cmd)) {\n        if (i < argv.length) {\n          printHelp(argv[i]);\n        } else {\n          printHelp(\"\");\n        }\n      } else {\n        exitCode = -1;\n        System.err.println(cmd.substring(1) + \": Unknown command\");\n        printUsage(\"\");\n      }\n    } catch (IllegalArgumentException arge) {\n      debugException = arge;\n      exitCode = -1;\n      System.err.println(cmd.substring(1) + \": \" + arge.getLocalizedMessage());\n      printUsage(cmd);\n    } catch (RemoteException e) {\n      //\n      // This is a error returned by hadoop server. Print\n      // out the first line of the error message, ignore the stack trace.\n      exitCode = -1;\n      debugException = e;\n      try {\n        String[] content;\n        content = e.getLocalizedMessage().split(\"\\n\");\n        System.err.println(cmd.substring(1) + \": \"\n                           + content[0]);\n      } catch (Exception ex) {\n        System.err.println(cmd.substring(1) + \": \"\n                           + ex.getLocalizedMessage());\n        debugException = ex;\n      }\n    } catch (Exception e) {\n      exitCode = -1;\n      debugException = e;\n      System.err.println(cmd.substring(1) + \": \"\n                         + e.getLocalizedMessage());\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Exception encountered:\", debugException);\n    }\n    return exitCode;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.evictWriters": "  private int evictWriters(String[] argv, int i) throws IOException {\n    final String dn = argv[i];\n    ClientDatanodeProtocol dnProxy = getDataNodeProxy(dn);\n    try {\n      dnProxy.evictWriters();\n      System.out.println(\"Requested writer eviction to datanode \" + dn);\n    } catch (IOException ioe) {\n      return -1;\n    }\n    return 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.getBalancerBandwidth": "  public int getBalancerBandwidth(String[] argv, int idx) throws IOException {\n    ClientDatanodeProtocol dnProxy = getDataNodeProxy(argv[idx]);\n    try {\n      long bandwidth = dnProxy.getBalancerBandwidth();\n      System.out.println(\"Balancer bandwidth is \" + bandwidth\n          + \" bytes per second.\");\n    } catch (IOException ioe) {\n      System.err.println(\"Datanode unreachable.\");\n      return -1;\n    }\n    return 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.refreshNodes": "  public int refreshNodes() throws IOException {\n    int exitCode = -1;\n\n    DistributedFileSystem dfs = getDFS();\n    Configuration dfsConf = dfs.getConf();\n    URI dfsUri = dfs.getUri();\n    boolean isHaEnabled = HAUtilClient.isLogicalUri(dfsConf, dfsUri);\n\n    if (isHaEnabled) {\n      String nsId = dfsUri.getHost();\n      List<ProxyAndInfo<ClientProtocol>> proxies =\n          HAUtil.getProxiesForAllNameNodesInNameservice(dfsConf,\n          nsId, ClientProtocol.class);\n      for (ProxyAndInfo<ClientProtocol> proxy: proxies) {\n        proxy.getProxy().refreshNodes();\n        System.out.println(\"Refresh nodes successful for \" +\n            proxy.getAddress());\n      }\n    } else {\n      dfs.refreshNodes();\n      System.out.println(\"Refresh nodes successful\");\n    }\n    exitCode = 0;\n   \n    return exitCode;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.genericRefresh": "  public int genericRefresh(String[] argv, int i) throws IOException {\n    String hostport = argv[i++];\n    String identifier = argv[i++];\n    String[] args = Arrays.copyOfRange(argv, i, argv.length);\n\n    // Get the current configuration\n    Configuration conf = getConf();\n\n    // for security authorization\n    // server principal for this call\n    // should be NN's one.\n    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY,\n      conf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, \"\"));\n\n    // Create the client\n    Class<?> xface = GenericRefreshProtocolPB.class;\n    InetSocketAddress address = NetUtils.createSocketAddr(hostport);\n    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n\n    RPC.setProtocolEngine(conf, xface, ProtobufRpcEngine.class);\n    GenericRefreshProtocolPB proxy = (GenericRefreshProtocolPB)\n      RPC.getProxy(xface, RPC.getProtocolVersion(xface), address,\n        ugi, conf, NetUtils.getDefaultSocketFactory(conf), 0);\n\n    Collection<RefreshResponse> responses = null;\n    try (GenericRefreshProtocolClientSideTranslatorPB xlator =\n        new GenericRefreshProtocolClientSideTranslatorPB(proxy);) {\n      // Refresh\n      responses = xlator.refresh(identifier, args);\n\n      int returnCode = 0;\n\n      // Print refresh responses\n      System.out.println(\"Refresh Responses:\\n\");\n      for (RefreshResponse response : responses) {\n        System.out.println(response.toString());\n\n        if (returnCode == 0 && response.getReturnCode() != 0) {\n          // This is the first non-zero return code, so we should return this\n          returnCode = response.getReturnCode();\n        } else if (returnCode != 0 && response.getReturnCode() != 0) {\n          // Then now we have multiple non-zero return codes,\n          // so we merge them into -1\n          returnCode = - 1;\n        }\n      }\n      return returnCode;\n    } finally {\n      if (responses == null) {\n        System.out.println(\"Failed to get response.\\n\");\n        return -1;\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.matches": "    static boolean matches(String cmd) {\n      return (\"-\"+NAME).equals(cmd); \n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.printUsage": "  private static void printUsage(String cmd) {\n    if (\"-report\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n          + \" [-report] [-live] [-dead] [-decommissioning]\");\n    } else if (\"-safemode\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n          + \" [-safemode enter | leave | get | wait | forceExit]\");\n    } else if (\"-allowSnapshot\".equalsIgnoreCase(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n          + \" [-allowSnapshot <snapshotDir>]\");\n    } else if (\"-disallowSnapshot\".equalsIgnoreCase(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n          + \" [-disallowSnapshot <snapshotDir>]\");\n    } else if (\"-saveNamespace\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n          + \" [-saveNamespace [-beforeShutdown]]\");\n    } else if (\"-rollEdits\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin [-rollEdits]\");\n    } else if (\"-restoreFailedStorage\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n          + \" [-restoreFailedStorage true|false|check ]\");\n    } else if (\"-refreshNodes\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n                         + \" [-refreshNodes]\");\n    } else if (\"-finalizeUpgrade\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n                         + \" [-finalizeUpgrade]\");\n    } else if (RollingUpgradeCommand.matches(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n          + \" [\" + RollingUpgradeCommand.USAGE+\"]\");\n    } else if (\"-metasave\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n          + \" [-metasave filename]\");\n    } else if (SetQuotaCommand.matches(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n                         + \" [\" + SetQuotaCommand.USAGE+\"]\");\n    } else if (ClearQuotaCommand.matches(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n                         + \" [\"+ClearQuotaCommand.USAGE+\"]\");\n    } else if (SetSpaceQuotaCommand.matches(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n                         + \" [\" + SetSpaceQuotaCommand.USAGE+\"]\");\n    } else if (ClearSpaceQuotaCommand.matches(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n                         + \" [\"+ClearSpaceQuotaCommand.USAGE+\"]\");\n    } else if (\"-refreshServiceAcl\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n                         + \" [-refreshServiceAcl]\");\n    } else if (\"-refreshUserToGroupsMappings\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n                         + \" [-refreshUserToGroupsMappings]\");\n    } else if (\"-refreshSuperUserGroupsConfiguration\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n                         + \" [-refreshSuperUserGroupsConfiguration]\");\n    } else if (\"-refreshCallQueue\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n                         + \" [-refreshCallQueue]\");\n    } else if (\"-reconfig\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n          + \" [-reconfig <namenode|datanode> <host:port> \"\n          + \"<start|status|properties>]\");\n    } else if (\"-refresh\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n                         + \" [-refresh <hostname:port> <resource_identifier> [arg1..argn]\");\n    } else if (\"-printTopology\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n                         + \" [-printTopology]\");\n    } else if (\"-refreshNamenodes\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n                         + \" [-refreshNamenodes datanode-host:port]\");\n    } else if (\"-deleteBlockPool\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n          + \" [-deleteBlockPool datanode-host:port blockpoolId [force]]\");\n    } else if (\"-setBalancerBandwidth\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n                  + \" [-setBalancerBandwidth <bandwidth in bytes per second>]\");\n    } else if (\"-getBalancerBandwidth\".equalsIgnoreCase(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n          + \" [-getBalancerBandwidth <datanode_host:ipc_port>]\");\n    } else if (\"-fetchImage\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n          + \" [-fetchImage <local directory>]\");\n    } else if (\"-shutdownDatanode\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n          + \" [-shutdownDatanode <datanode_host:ipc_port> [upgrade]]\");\n    } else if (\"-evictWriters\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n          + \" [-evictWriters <datanode_host:ipc_port>]\");\n    } else if (\"-getDatanodeInfo\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n          + \" [-getDatanodeInfo <datanode_host:ipc_port>]\");\n    } else if (\"-triggerBlockReport\".equals(cmd)) {\n      System.err.println(\"Usage: hdfs dfsadmin\"\n          + \" [-triggerBlockReport [-incremental] <datanode_host:ipc_port>]\");\n    } else {\n      System.err.println(\"Usage: hdfs dfsadmin\");\n      System.err.println(\"Note: Administrative commands can only be run as the HDFS superuser.\");\n      System.err.println(commonUsageSummary);\n      ToolRunner.printGenericCommandUsage(System.err);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.getDatanodeInfo": "  private int getDatanodeInfo(String[] argv, int i) throws IOException {\n    ClientDatanodeProtocol dnProxy = getDataNodeProxy(argv[i]);\n    try {\n      DatanodeLocalInfo dnInfo = dnProxy.getDatanodeInfo();\n      System.out.println(dnInfo.getDatanodeLocalReport());\n    } catch (IOException ioe) {\n      System.err.println(\"Datanode unreachable.\");\n      return -1;\n    }\n    return 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.printMessage": "    private static void printMessage(RollingUpgradeInfo info,\n        PrintStream out) {\n      if (info != null && info.isStarted()) {\n        if (!info.createdRollbackImages() && !info.isFinalized()) {\n          out.println(\n              \"Preparing for upgrade. Data is being saved for rollback.\"\n              + \"\\nRun \\\"dfsadmin -rollingUpgrade query\\\" to check the status\"\n              + \"\\nfor proceeding with rolling upgrade\");\n            out.println(info);\n        } else if (!info.isFinalized()) {\n          out.println(\"Proceed with rolling upgrade:\");\n          out.println(info);\n        } else {\n          out.println(\"Rolling upgrade is finalized.\");\n          out.println(info);\n        }\n      } else {\n        out.println(\"There is no rolling upgrade in progress or rolling \" +\n            \"upgrade has already been finalized.\");\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.metaSave": "  public int metaSave(String[] argv, int idx) throws IOException {\n    String pathname = argv[idx];\n    DistributedFileSystem dfs = getDFS();\n    Configuration dfsConf = dfs.getConf();\n    URI dfsUri = dfs.getUri();\n    boolean isHaEnabled = HAUtilClient.isLogicalUri(dfsConf, dfsUri);\n\n    if (isHaEnabled) {\n      String nsId = dfsUri.getHost();\n      List<ProxyAndInfo<ClientProtocol>> proxies =\n          HAUtil.getProxiesForAllNameNodesInNameservice(dfsConf,\n          nsId, ClientProtocol.class);\n      for (ProxyAndInfo<ClientProtocol> proxy : proxies) {\n        proxy.getProxy().metaSave(pathname);\n        System.out.println(\"Created metasave file \" + pathname + \" in the log \"\n            + \"directory of namenode \" + proxy.getAddress());\n      }\n    } else {\n      dfs.metaSave(pathname);\n      System.out.println(\"Created metasave file \" + pathname + \" in the log \" +\n          \"directory of namenode \" + dfs.getUri());\n    }\n    return 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.refreshUserToGroupsMappings": "  public int refreshUserToGroupsMappings() throws IOException {\n    // Get the current configuration\n    Configuration conf = getConf();\n    \n    // for security authorization\n    // server principal for this call   \n    // should be NN's one.\n    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n        conf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, \"\"));\n\n    DistributedFileSystem dfs = getDFS();\n    URI dfsUri = dfs.getUri();\n    boolean isHaEnabled = HAUtilClient.isLogicalUri(conf, dfsUri);\n\n    if (isHaEnabled) {\n      // Run refreshUserToGroupsMapings for all NNs if HA is enabled\n      String nsId = dfsUri.getHost();\n      List<ProxyAndInfo<RefreshUserMappingsProtocol>> proxies =\n          HAUtil.getProxiesForAllNameNodesInNameservice(conf, nsId,\n              RefreshUserMappingsProtocol.class);\n      for (ProxyAndInfo<RefreshUserMappingsProtocol> proxy : proxies) {\n        proxy.getProxy().refreshUserToGroupsMappings();\n        System.out.println(\"Refresh user to groups mapping successful for \"\n            + proxy.getAddress());\n      }\n    } else {\n      // Create the client\n      RefreshUserMappingsProtocol refreshProtocol =\n          NameNodeProxies.createProxy(conf, FileSystem.getDefaultUri(conf),\n              RefreshUserMappingsProtocol.class).getProxy();\n\n      // Refresh the user-to-groups mappings\n      refreshProtocol.refreshUserToGroupsMappings();\n      System.out.println(\"Refresh user to groups mapping successful\");\n    }\n    \n    return 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.setBalancerBandwidth": "  public int setBalancerBandwidth(String[] argv, int idx) throws IOException {\n    long bandwidth;\n    int exitCode = -1;\n\n    try {\n      bandwidth = Long.parseLong(argv[idx]);\n    } catch (NumberFormatException nfe) {\n      System.err.println(\"NumberFormatException: \" + nfe.getMessage());\n      System.err.println(\"Usage: hdfs dfsadmin\"\n                  + \" [-setBalancerBandwidth <bandwidth in bytes per second>]\");\n      return exitCode;\n    }\n\n    if (bandwidth < 0) {\n      System.err.println(\"Bandwidth should be a non-negative integer\");\n      return exitCode;\n    }\n\n    FileSystem fs = getFS();\n    if (!(fs instanceof DistributedFileSystem)) {\n      System.err.println(\"FileSystem is \" + fs.getUri());\n      return exitCode;\n    }\n\n    DistributedFileSystem dfs = (DistributedFileSystem) fs;\n    Configuration dfsConf = dfs.getConf();\n    URI dfsUri = dfs.getUri();\n    boolean isHaEnabled = HAUtilClient.isLogicalUri(dfsConf, dfsUri);\n\n    if (isHaEnabled) {\n      String nsId = dfsUri.getHost();\n      List<ProxyAndInfo<ClientProtocol>> proxies =\n          HAUtil.getProxiesForAllNameNodesInNameservice(dfsConf,\n          nsId, ClientProtocol.class);\n      for (ProxyAndInfo<ClientProtocol> proxy : proxies) {\n        proxy.getProxy().setBalancerBandwidth(bandwidth);\n        System.out.println(\"Balancer bandwidth is set to \" + bandwidth +\n            \" for \" + proxy.getAddress());\n      }\n    } else {\n      dfs.setBalancerBandwidth(bandwidth);\n      System.out.println(\"Balancer bandwidth is set to \" + bandwidth);\n    }\n    exitCode = 0;\n\n    return exitCode;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.finalizeUpgrade": "  public int finalizeUpgrade() throws IOException {\n    DistributedFileSystem dfs = getDFS();\n    \n    Configuration dfsConf = dfs.getConf();\n    URI dfsUri = dfs.getUri();\n    boolean isHaAndLogicalUri = HAUtilClient.isLogicalUri(dfsConf, dfsUri);\n    if (isHaAndLogicalUri) {\n      // In the case of HA and logical URI, run finalizeUpgrade for all\n      // NNs in this nameservice.\n      String nsId = dfsUri.getHost();\n      List<ClientProtocol> namenodes =\n          HAUtil.getProxiesForAllNameNodesInNameservice(dfsConf, nsId);\n      if (!HAUtil.isAtLeastOneActive(namenodes)) {\n        throw new IOException(\"Cannot finalize with no NameNode active\");\n      }\n\n      List<ProxyAndInfo<ClientProtocol>> proxies =\n          HAUtil.getProxiesForAllNameNodesInNameservice(dfsConf,\n          nsId, ClientProtocol.class);\n      for (ProxyAndInfo<ClientProtocol> proxy : proxies) {\n        proxy.getProxy().finalizeUpgrade();\n        System.out.println(\"Finalize upgrade successful for \" +\n            proxy.getAddress());\n      }\n    } else {\n      dfs.finalizeUpgrade();\n      System.out.println(\"Finalize upgrade successful\");\n    }\n    \n    return 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.restoreFailedStorage": "  public int restoreFailedStorage(String arg) throws IOException {\n    int exitCode = -1;\n    if(!arg.equals(\"check\") && !arg.equals(\"true\") && !arg.equals(\"false\")) {\n      System.err.println(\"restoreFailedStorage valid args are true|false|check\");\n      return exitCode;\n    }\n    \n    DistributedFileSystem dfs = getDFS();\n    Configuration dfsConf = dfs.getConf();\n    URI dfsUri = dfs.getUri();\n    boolean isHaEnabled = HAUtilClient.isLogicalUri(dfsConf, dfsUri);\n\n    if (isHaEnabled) {\n      String nsId = dfsUri.getHost();\n      List<ProxyAndInfo<ClientProtocol>> proxies =\n          HAUtil.getProxiesForAllNameNodesInNameservice(dfsConf,\n          nsId, ClientProtocol.class);\n      for (ProxyAndInfo<ClientProtocol> proxy : proxies) {\n        Boolean res = proxy.getProxy().restoreFailedStorage(arg);\n        System.out.println(\"restoreFailedStorage is set to \" + res + \" for \"\n            + proxy.getAddress());\n      }\n    } else {\n      Boolean res = dfs.restoreFailedStorage(arg);\n      System.out.println(\"restoreFailedStorage is set to \" + res);\n    }\n    exitCode = 0;\n\n    return exitCode;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.refreshNamenodes": "  private int refreshNamenodes(String[] argv, int i) throws IOException {\n    String datanode = argv[i];\n    ClientDatanodeProtocol refreshProtocol = getDataNodeProxy(datanode);\n    refreshProtocol.refreshNamenodes();\n    \n    return 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.triggerBlockReport": "  public int triggerBlockReport(String[] argv) throws IOException {\n    List<String> args = new LinkedList<String>();\n    for (int j = 1; j < argv.length; j++) {\n      args.add(argv[j]);\n    }\n    boolean incremental = StringUtils.popOption(\"-incremental\", args);\n    String hostPort = StringUtils.popFirstNonOption(args);\n    if (hostPort == null) {\n      System.err.println(\"You must specify a host:port pair.\");\n      return 1;\n    }\n    if (!args.isEmpty()) {\n      System.err.print(\"Can't understand arguments: \" +\n        Joiner.on(\" \").join(args) + \"\\n\");\n      return 1;\n    }\n    ClientDatanodeProtocol dnProxy = getDataNodeProxy(hostPort);\n    try {\n      dnProxy.triggerBlockReport(\n          new BlockReportOptions.Factory().\n              setIncremental(incremental).\n              build());\n    } catch (IOException e) {\n      System.err.println(\"triggerBlockReport error: \" + e);\n      return 1;\n    }\n    System.out.println(\"Triggering \" +\n        (incremental ? \"an incremental \" : \"a full \") +\n        \"block report on \" + hostPort + \".\");\n    return 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.fetchImage": "  public int fetchImage(final String[] argv, final int idx) throws IOException {\n    Configuration conf = getConf();\n    final URL infoServer = DFSUtil.getInfoServer(\n        HAUtil.getAddressOfActive(getDFS()), conf,\n        DFSUtil.getHttpClientScheme(conf)).toURL();\n    SecurityUtil.doAsCurrentUser(new PrivilegedExceptionAction<Void>() {\n      @Override\n      public Void run() throws Exception {\n        TransferFsImage.downloadMostRecentImageToDirectory(infoServer,\n            new File(argv[idx]));\n        return null;\n      }\n    });\n    return 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.setSafeMode": "  public void setSafeMode(String[] argv, int idx) throws IOException {\n    if (idx != argv.length - 1) {\n      printUsage(\"-safemode\");\n      return;\n    }\n    HdfsConstants.SafeModeAction action;\n    Boolean waitExitSafe = false;\n\n    if (\"leave\".equalsIgnoreCase(argv[idx])) {\n      action = HdfsConstants.SafeModeAction.SAFEMODE_LEAVE;\n    } else if (\"enter\".equalsIgnoreCase(argv[idx])) {\n      action = HdfsConstants.SafeModeAction.SAFEMODE_ENTER;\n    } else if (\"get\".equalsIgnoreCase(argv[idx])) {\n      action = HdfsConstants.SafeModeAction.SAFEMODE_GET;\n    } else if (\"wait\".equalsIgnoreCase(argv[idx])) {\n      action = HdfsConstants.SafeModeAction.SAFEMODE_GET;\n      waitExitSafe = true;\n    } else if (\"forceExit\".equalsIgnoreCase(argv[idx])){\n      action = HdfsConstants.SafeModeAction.SAFEMODE_FORCE_EXIT;\n    } else {\n      printUsage(\"-safemode\");\n      return;\n    }\n\n    DistributedFileSystem dfs = getDFS();\n    Configuration dfsConf = dfs.getConf();\n    URI dfsUri = dfs.getUri();\n    boolean isHaEnabled = HAUtilClient.isLogicalUri(dfsConf, dfsUri);\n\n    if (isHaEnabled) {\n      String nsId = dfsUri.getHost();\n      List<ProxyAndInfo<ClientProtocol>> proxies =\n          HAUtil.getProxiesForAllNameNodesInNameservice(\n          dfsConf, nsId, ClientProtocol.class);\n      for (ProxyAndInfo<ClientProtocol> proxy : proxies) {\n        ClientProtocol haNn = proxy.getProxy();\n        boolean inSafeMode = haNn.setSafeMode(action, false);\n        if (waitExitSafe) {\n          inSafeMode = waitExitSafeMode(haNn, inSafeMode);\n        }\n        System.out.println(\"Safe mode is \" + (inSafeMode ? \"ON\" : \"OFF\")\n            + \" in \" + proxy.getAddress());\n      }\n    } else {\n      boolean inSafeMode = dfs.setSafeMode(action);\n      if (waitExitSafe) {\n        inSafeMode = waitExitSafeMode(dfs, inSafeMode);\n      }\n      System.out.println(\"Safe mode is \" + (inSafeMode ? \"ON\" : \"OFF\"));\n    }\n\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.disallowSnapshot": "  public void disallowSnapshot(String[] argv) throws IOException {  \n    DistributedFileSystem dfs = getDFS();\n    try {\n      dfs.disallowSnapshot(new Path(argv[1]));\n    } catch (SnapshotException e) {\n      throw new RemoteException(e.getClass().getName(), e.getMessage());\n    }\n    System.out.println(\"Disallowing snaphot on \" + argv[1] + \" succeeded\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.getDFS": "  protected DistributedFileSystem getDFS() throws IOException {\n    FileSystem fs = getFS();\n    if (!(fs instanceof DistributedFileSystem)) {\n      throw new IllegalArgumentException(\"FileSystem \" + fs.getUri() + \n      \" is not an HDFS file system\");\n    }\n    return (DistributedFileSystem)fs;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.reconfig": "  public int reconfig(String[] argv, int i) throws IOException {\n    String nodeType = argv[i];\n    String address = argv[i + 1];\n    String op = argv[i + 2];\n\n    if (\"start\".equals(op)) {\n      return startReconfiguration(nodeType, address, System.out, System.err);\n    } else if (\"status\".equals(op)) {\n      return getReconfigurationStatus(nodeType, address, System.out, System.err);\n    } else if (\"properties\".equals(op)) {\n      return getReconfigurableProperties(nodeType, address, System.out,\n          System.err);\n    }\n    System.err.println(\"Unknown operation: \" + op);\n    return -1;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.saveNamespace": "  public int saveNamespace(String[] argv) throws IOException {\n    final DistributedFileSystem dfs = getDFS();\n    final Configuration dfsConf = dfs.getConf();\n\n    long timeWindow = 0;\n    long txGap = 0;\n    if (argv.length > 1 && \"-beforeShutdown\".equals(argv[1])) {\n      final long checkpointPeriod = dfsConf.getTimeDuration(\n          DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_KEY,\n          DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_DEFAULT,\n          TimeUnit.SECONDS);\n      final long checkpointTxnCount = dfsConf.getLong(\n          DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_KEY,\n          DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_DEFAULT);\n      final int toleratePeriodNum = dfsConf.getInt(\n          DFSConfigKeys.DFS_NAMENODE_MISSING_CHECKPOINT_PERIODS_BEFORE_SHUTDOWN_KEY,\n          DFSConfigKeys.DFS_NAMENODE_MISSING_CHECKPOINT_PERIODS_BEFORE_SHUTDOWN_DEFAULT);\n      timeWindow = checkpointPeriod * toleratePeriodNum;\n      txGap = checkpointTxnCount * toleratePeriodNum;\n      System.out.println(\"Do checkpoint if necessary before stopping \" +\n          \"namenode. The time window is \" + timeWindow + \" seconds, and the \" +\n          \"transaction gap is \" + txGap);\n    }\n\n    URI dfsUri = dfs.getUri();\n    boolean isHaEnabled = HAUtilClient.isLogicalUri(dfsConf, dfsUri);\n    if (isHaEnabled) {\n      String nsId = dfsUri.getHost();\n      List<ProxyAndInfo<ClientProtocol>> proxies =\n          HAUtil.getProxiesForAllNameNodesInNameservice(dfsConf,\n          nsId, ClientProtocol.class);\n      for (ProxyAndInfo<ClientProtocol> proxy : proxies) {\n        boolean saved = proxy.getProxy().saveNamespace(timeWindow, txGap);\n        if (saved) {\n          System.out.println(\"Save namespace successful for \" +\n              proxy.getAddress());\n        } else {\n          System.out.println(\"No extra checkpoint has been made for \"\n              + proxy.getAddress());\n        }\n      }\n    } else {\n      boolean saved = dfs.saveNamespace(timeWindow, txGap);\n      if (saved) {\n        System.out.println(\"Save namespace successful\");\n      } else {\n        System.out.println(\"No extra checkpoint has been made\");\n      }\n    }\n    return 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.deleteBlockPool": "  private int deleteBlockPool(String[] argv, int i) throws IOException {\n    ClientDatanodeProtocol dnProxy = getDataNodeProxy(argv[i]);\n    boolean force = false;\n    if (argv.length-1 == i+2) {\n      if (\"force\".equals(argv[i+2])) {\n        force = true;\n      } else {\n        printUsage(\"-deleteBlockPool\");\n        return -1;\n      }\n    }\n    dnProxy.deleteBlockPool(argv[i+1], force);\n    return 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.report": "  public void report(String[] argv, int i) throws IOException {\n    DistributedFileSystem dfs = getDFS();\n    FsStatus ds = dfs.getStatus();\n    long capacity = ds.getCapacity();\n    long used = ds.getUsed();\n    long remaining = ds.getRemaining();\n    long bytesInFuture = dfs.getBytesWithFutureGenerationStamps();\n    long presentCapacity = used + remaining;\n    boolean mode = dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n    if (mode) {\n      System.out.println(\"Safe mode is ON\");\n      if (bytesInFuture > 0) {\n        System.out.println(\"\\nWARNING: \");\n        System.out.println(\"Name node has detected blocks with generation \" +\n            \"stamps in future.\");\n        System.out.println(\"Forcing exit from safemode will cause \" +\n            bytesInFuture + \" byte(s) to be deleted.\");\n        System.out.println(\"If you are sure that the NameNode was started with\"\n            + \" the correct metadata files then you may proceed with \" +\n            \"'-safemode forceExit'\\n\");\n      }\n    }\n    System.out.println(\"Configured Capacity: \" + capacity\n                       + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n    System.out.println(\"Present Capacity: \" + presentCapacity\n        + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n    System.out.println(\"DFS Remaining: \" + remaining\n        + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n    System.out.println(\"DFS Used: \" + used\n                       + \" (\" + StringUtils.byteDesc(used) + \")\");\n    System.out.println(\"DFS Used%: \"\n        + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n    \n    /* These counts are not always upto date. They are updated after  \n     * iteration of an internal list. Should be updated in a few seconds to \n     * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n     * counts.\n     */\n    System.out.println(\"Under replicated blocks: \" + \n                       dfs.getUnderReplicatedBlocksCount());\n    System.out.println(\"Blocks with corrupt replicas: \" + \n                       dfs.getCorruptBlocksCount());\n    System.out.println(\"Missing blocks: \" + \n                       dfs.getMissingBlocksCount());\n    System.out.println(\"Missing blocks (with replication factor 1): \" +\n                      dfs.getMissingReplOneBlocksCount());\n    System.out.println(\"Pending deletion blocks: \" +\n        dfs.getPendingDeletionBlocksCount());\n\n    System.out.println();\n\n    System.out.println(\"-------------------------------------------------\");\n    \n    // Parse arguments for filtering the node list\n    List<String> args = Arrays.asList(argv);\n    // Truncate already handled arguments before parsing report()-specific ones\n    args = new ArrayList<String>(args.subList(i, args.size()));\n    final boolean listLive = StringUtils.popOption(\"-live\", args);\n    final boolean listDead = StringUtils.popOption(\"-dead\", args);\n    final boolean listDecommissioning =\n        StringUtils.popOption(\"-decommissioning\", args);\n\n    // If no filter flags are found, then list all DN types\n    boolean listAll = (!listLive && !listDead && !listDecommissioning);\n\n    if (listAll || listLive) {\n      DatanodeInfo[] live = dfs.getDataNodeStats(DatanodeReportType.LIVE);\n      if (live.length > 0 || listLive) {\n        System.out.println(\"Live datanodes (\" + live.length + \"):\\n\");\n      }\n      if (live.length > 0) {\n        for (DatanodeInfo dn : live) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n    }\n\n    if (listAll || listDead) {\n      DatanodeInfo[] dead = dfs.getDataNodeStats(DatanodeReportType.DEAD);\n      if (dead.length > 0 || listDead) {\n        System.out.println(\"Dead datanodes (\" + dead.length + \"):\\n\");\n      }\n      if (dead.length > 0) {\n        for (DatanodeInfo dn : dead) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n    }\n\n    if (listAll || listDecommissioning) {\n      DatanodeInfo[] decom =\n          dfs.getDataNodeStats(DatanodeReportType.DECOMMISSIONING);\n      if (decom.length > 0 || listDecommissioning) {\n        System.out.println(\"Decommissioning datanodes (\" + decom.length\n            + \"):\\n\");\n      }\n      if (decom.length > 0) {\n        for (DatanodeInfo dn : decom) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.refreshCallQueue": "  public int refreshCallQueue() throws IOException {\n    // Get the current configuration\n    Configuration conf = getConf();\n    \n    // for security authorization\n    // server principal for this call   \n    // should be NN's one.\n    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n        conf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, \"\"));\n\n    DistributedFileSystem dfs = getDFS();\n    URI dfsUri = dfs.getUri();\n    boolean isHaEnabled = HAUtilClient.isLogicalUri(conf, dfsUri);\n\n    if (isHaEnabled) {\n      // Run refreshCallQueue for all NNs if HA is enabled\n      String nsId = dfsUri.getHost();\n      List<ProxyAndInfo<RefreshCallQueueProtocol>> proxies =\n          HAUtil.getProxiesForAllNameNodesInNameservice(conf, nsId,\n              RefreshCallQueueProtocol.class);\n      for (ProxyAndInfo<RefreshCallQueueProtocol> proxy : proxies) {\n        proxy.getProxy().refreshCallQueue();\n        System.out.println(\"Refresh call queue successful for \"\n            + proxy.getAddress());\n      }\n    } else {\n      // Create the client\n      RefreshCallQueueProtocol refreshProtocol =\n          NameNodeProxies.createProxy(conf, FileSystem.getDefaultUri(conf),\n              RefreshCallQueueProtocol.class).getProxy();\n\n      // Refresh the call queue\n      refreshProtocol.refreshCallQueue();\n      System.out.println(\"Refresh call queue successful\");\n    }\n\n    return 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.shutdownDatanode": "  private int shutdownDatanode(String[] argv, int i) throws IOException {\n    final String dn = argv[i];\n    ClientDatanodeProtocol dnProxy = getDataNodeProxy(dn);\n    boolean upgrade = false;\n    if (argv.length-1 == i+1) {\n      if (\"upgrade\".equalsIgnoreCase(argv[i+1])) {\n        upgrade = true;\n      } else {\n        printUsage(\"-shutdownDatanode\");\n        return -1;\n      }\n    }\n    dnProxy.shutdownDatanode(upgrade);\n    System.out.println(\"Submitted a shutdown request to datanode \" + dn);\n    return 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.printHelp": "  private void printHelp(String cmd) {\n    String summary = \"hdfs dfsadmin performs DFS administrative commands.\\n\" +\n      \"Note: Administrative commands can only be run with superuser permission.\\n\" +\n      \"The full syntax is: \\n\\n\" +\n      \"hdfs dfsadmin\\n\" +\n      commonUsageSummary;\n\n    String report =\"-report [-live] [-dead] [-decommissioning]:\\n\" +\n      \"\\tReports basic filesystem information and statistics. \\n\" +\n      \"\\tThe dfs usage can be different from \\\"du\\\" usage, because it\\n\" +\n      \"\\tmeasures raw space used by replication, checksums, snapshots\\n\" +\n      \"\\tand etc. on all the DNs.\\n\" +\n      \"\\tOptional flags may be used to filter the list of displayed DNs.\\n\";\n\n    String safemode = \"-safemode <enter|leave|get|wait|forceExit>:  Safe mode \" +\n        \"maintenance command.\\n\" +\n      \"\\t\\tSafe mode is a Namenode state in which it\\n\" +\n      \"\\t\\t\\t1.  does not accept changes to the name space (read-only)\\n\" +\n      \"\\t\\t\\t2.  does not replicate or delete blocks.\\n\" +\n      \"\\t\\tSafe mode is entered automatically at Namenode startup, and\\n\" +\n      \"\\t\\tleaves safe mode automatically when the configured minimum\\n\" +\n      \"\\t\\tpercentage of blocks satisfies the minimum replication\\n\" +\n      \"\\t\\tcondition.  Safe mode can also be entered manually, but then\\n\" +\n      \"\\t\\tit can only be turned off manually as well.\\n\";\n\n    String saveNamespace = \"-saveNamespace [-beforeShutdown]:\\t\" +\n        \"Save current namespace into storage directories and reset edits \\n\" +\n        \"\\t\\t log. Requires safe mode.\\n\" +\n        \"\\t\\tIf the \\\"beforeShutdown\\\" option is given, the NameNode does a \\n\" +\n        \"\\t\\tcheckpoint if and only if there is no checkpoint done during \\n\" +\n        \"\\t\\ta time window (a configurable number of checkpoint periods).\\n\" +\n        \"\\t\\tThis is usually used before shutting down the NameNode to \\n\" +\n        \"\\t\\tprevent potential fsimage/editlog corruption.\\n\";\n\n    String rollEdits = \"-rollEdits:\\t\" +\n    \"Rolls the edit log.\\n\";\n    \n    String restoreFailedStorage = \"-restoreFailedStorage:\\t\" +\n    \"Set/Unset/Check flag to attempt restore of failed storage replicas if they become available.\\n\";\n    \n    String refreshNodes = \"-refreshNodes: \\tUpdates the namenode with the \" +\n      \"set of datanodes allowed to connect to the namenode.\\n\\n\" +\n      \"\\t\\tNamenode re-reads datanode hostnames from the file defined by \\n\" +\n      \"\\t\\tdfs.hosts, dfs.hosts.exclude configuration parameters.\\n\" +\n      \"\\t\\tHosts defined in dfs.hosts are the datanodes that are part of \\n\" +\n      \"\\t\\tthe cluster. If there are entries in dfs.hosts, only the hosts \\n\" +\n      \"\\t\\tin it are allowed to register with the namenode.\\n\\n\" +\n      \"\\t\\tEntries in dfs.hosts.exclude are datanodes that need to be \\n\" +\n      \"\\t\\tdecommissioned. Datanodes complete decommissioning when \\n\" + \n      \"\\t\\tall the replicas from them are replicated to other datanodes.\\n\" +\n      \"\\t\\tDecommissioned nodes are not automatically shutdown and \\n\" +\n      \"\\t\\tare not chosen for writing new replicas.\\n\";\n\n    String finalizeUpgrade = \"-finalizeUpgrade: Finalize upgrade of HDFS.\\n\" +\n      \"\\t\\tDatanodes delete their previous version working directories,\\n\" +\n      \"\\t\\tfollowed by Namenode doing the same.\\n\" + \n      \"\\t\\tThis completes the upgrade process.\\n\";\n\n    String metaSave = \"-metasave <filename>: \\tSave Namenode's primary data structures\\n\" +\n      \"\\t\\tto <filename> in the directory specified by hadoop.log.dir property.\\n\" +\n      \"\\t\\t<filename> is overwritten if it exists.\\n\" +\n      \"\\t\\t<filename> will contain one line for each of the following\\n\" +\n      \"\\t\\t\\t1. Datanodes heart beating with Namenode\\n\" +\n      \"\\t\\t\\t2. Blocks waiting to be replicated\\n\" +\n      \"\\t\\t\\t3. Blocks currrently being replicated\\n\" +\n      \"\\t\\t\\t4. Blocks waiting to be deleted\\n\";\n\n    String refreshServiceAcl = \"-refreshServiceAcl: Reload the service-level authorization policy file\\n\" +\n      \"\\t\\tNamenode will reload the authorization policy file.\\n\";\n    \n    String refreshUserToGroupsMappings = \n      \"-refreshUserToGroupsMappings: Refresh user-to-groups mappings\\n\";\n    \n    String refreshSuperUserGroupsConfiguration = \n      \"-refreshSuperUserGroupsConfiguration: Refresh superuser proxy groups mappings\\n\";\n\n    String refreshCallQueue = \"-refreshCallQueue: Reload the call queue from config\\n\";\n\n    String reconfig = \"-reconfig <namenode|datanode> <host:ipc_port> \" +\n        \"<start|status|properties>:\\n\" +\n        \"\\tStarts or gets the status of a reconfiguration operation, \\n\" +\n        \"\\tor gets a list of reconfigurable properties.\\n\" +\n\n        \"\\tThe second parameter specifies the node type\\n\";\n    String genericRefresh = \"-refresh: Arguments are <hostname:port> <resource_identifier> [arg1..argn]\\n\" +\n      \"\\tTriggers a runtime-refresh of the resource specified by <resource_identifier>\\n\" +\n      \"\\ton <hostname:port>. All other args after are sent to the host.\\n\";\n\n    String printTopology = \"-printTopology: Print a tree of the racks and their\\n\" +\n                           \"\\t\\tnodes as reported by the Namenode\\n\";\n    \n    String refreshNamenodes = \"-refreshNamenodes: Takes a datanodehost:port as argument,\\n\"+\n                              \"\\t\\tFor the given datanode, reloads the configuration files,\\n\" +\n                              \"\\t\\tstops serving the removed block-pools\\n\"+\n                              \"\\t\\tand starts serving new block-pools\\n\";\n    \n    String deleteBlockPool = \"-deleteBlockPool: Arguments are datanodehost:port, blockpool id\\n\"+\n                             \"\\t\\t and an optional argument \\\"force\\\". If force is passed,\\n\"+\n                             \"\\t\\t block pool directory for the given blockpool id on the given\\n\"+\n                             \"\\t\\t datanode is deleted along with its contents, otherwise\\n\"+\n                             \"\\t\\t the directory is deleted only if it is empty. The command\\n\" +\n                             \"\\t\\t will fail if datanode is still serving the block pool.\\n\" +\n                             \"\\t\\t   Refer to refreshNamenodes to shutdown a block pool\\n\" +\n                             \"\\t\\t service on a datanode.\\n\";\n\n    String setBalancerBandwidth = \"-setBalancerBandwidth <bandwidth>:\\n\" +\n      \"\\tChanges the network bandwidth used by each datanode during\\n\" +\n      \"\\tHDFS block balancing.\\n\\n\" +\n      \"\\t\\t<bandwidth> is the maximum number of bytes per second\\n\" +\n      \"\\t\\tthat will be used by each datanode. This value overrides\\n\" +\n      \"\\t\\tthe dfs.balance.bandwidthPerSec parameter.\\n\\n\" +\n      \"\\t\\t--- NOTE: The new value is not persistent on the DataNode.---\\n\";\n\n    String getBalancerBandwidth = \"-getBalancerBandwidth <datanode_host:ipc_port>:\\n\" +\n        \"\\tGet the network bandwidth for the given datanode.\\n\" +\n        \"\\tThis is the maximum network bandwidth used by the datanode\\n\" +\n        \"\\tduring HDFS block balancing.\\n\\n\" +\n        \"\\t--- NOTE: This value is not persistent on the DataNode.---\\n\";\n\n    String fetchImage = \"-fetchImage <local directory>:\\n\" +\n      \"\\tDownloads the most recent fsimage from the Name Node and saves it in\" +\n      \"\\tthe specified local directory.\\n\";\n    \n    String allowSnapshot = \"-allowSnapshot <snapshotDir>:\\n\" +\n        \"\\tAllow snapshots to be taken on a directory.\\n\";\n    \n    String disallowSnapshot = \"-disallowSnapshot <snapshotDir>:\\n\" +\n        \"\\tDo not allow snapshots to be taken on a directory any more.\\n\";\n\n    String shutdownDatanode = \"-shutdownDatanode <datanode_host:ipc_port> [upgrade]\\n\"\n        + \"\\tSubmit a shutdown request for the given datanode. If an optional\\n\"\n        + \"\\t\\\"upgrade\\\" argument is specified, clients accessing the datanode\\n\"\n        + \"\\twill be advised to wait for it to restart and the fast start-up\\n\"\n        + \"\\tmode will be enabled. When the restart does not happen in time,\\n\"\n        + \"\\tclients will timeout and ignore the datanode. In such case, the\\n\"\n        + \"\\tfast start-up mode will also be disabled.\\n\";\n\n    String evictWriters = \"-evictWriters <datanode_host:ipc_port>\\n\"\n        + \"\\tMake the datanode evict all clients that are writing a block.\\n\"\n        + \"\\tThis is useful if decommissioning is hung due to slow writers.\\n\";\n\n    String getDatanodeInfo = \"-getDatanodeInfo <datanode_host:ipc_port>\\n\"\n        + \"\\tGet the information about the given datanode. This command can\\n\"\n        + \"\\tbe used for checking if a datanode is alive.\\n\";\n\n    String triggerBlockReport =\n      \"-triggerBlockReport [-incremental] <datanode_host:ipc_port>\\n\"\n        + \"\\tTrigger a block report for the datanode.\\n\"\n        + \"\\tIf 'incremental' is specified, it will be an incremental\\n\"\n        + \"\\tblock report; otherwise, it will be a full block report.\\n\";\n\n    String help = \"-help [cmd]: \\tDisplays help for the given command or all commands if none\\n\" +\n      \"\\t\\tis specified.\\n\";\n\n    if (\"report\".equals(cmd)) {\n      System.out.println(report);\n    } else if (\"safemode\".equals(cmd)) {\n      System.out.println(safemode);\n    } else if (\"saveNamespace\".equals(cmd)) {\n      System.out.println(saveNamespace);\n    } else if (\"rollEdits\".equals(cmd)) {\n      System.out.println(rollEdits);\n    } else if (\"restoreFailedStorage\".equals(cmd)) {\n      System.out.println(restoreFailedStorage);\n    } else if (\"refreshNodes\".equals(cmd)) {\n      System.out.println(refreshNodes);\n    } else if (\"finalizeUpgrade\".equals(cmd)) {\n      System.out.println(finalizeUpgrade);\n    } else if (RollingUpgradeCommand.matches(\"-\"+cmd)) {\n      System.out.println(RollingUpgradeCommand.DESCRIPTION);\n    } else if (\"metasave\".equals(cmd)) {\n      System.out.println(metaSave);\n    } else if (SetQuotaCommand.matches(\"-\"+cmd)) {\n      System.out.println(SetQuotaCommand.DESCRIPTION);\n    } else if (ClearQuotaCommand.matches(\"-\"+cmd)) {\n      System.out.println(ClearQuotaCommand.DESCRIPTION);\n    } else if (SetSpaceQuotaCommand.matches(\"-\"+cmd)) {\n      System.out.println(SetSpaceQuotaCommand.DESCRIPTION);\n    } else if (ClearSpaceQuotaCommand.matches(\"-\"+cmd)) {\n      System.out.println(ClearSpaceQuotaCommand.DESCRIPTION);\n    } else if (\"refreshServiceAcl\".equals(cmd)) {\n      System.out.println(refreshServiceAcl);\n    } else if (\"refreshUserToGroupsMappings\".equals(cmd)) {\n      System.out.println(refreshUserToGroupsMappings);\n    } else if (\"refreshSuperUserGroupsConfiguration\".equals(cmd)) {\n      System.out.println(refreshSuperUserGroupsConfiguration);\n    } else if (\"refreshCallQueue\".equals(cmd)) {\n      System.out.println(refreshCallQueue);\n    } else if (\"refresh\".equals(cmd)) {\n      System.out.println(genericRefresh);\n    } else if (\"reconfig\".equals(cmd)) {\n      System.out.println(reconfig);\n    } else if (\"printTopology\".equals(cmd)) {\n      System.out.println(printTopology);\n    } else if (\"refreshNamenodes\".equals(cmd)) {\n      System.out.println(refreshNamenodes);\n    } else if (\"deleteBlockPool\".equals(cmd)) {\n      System.out.println(deleteBlockPool);\n    } else if (\"setBalancerBandwidth\".equals(cmd)) {\n      System.out.println(setBalancerBandwidth);\n    } else if (\"getBalancerBandwidth\".equals(cmd)) {\n      System.out.println(getBalancerBandwidth);\n    } else if (\"fetchImage\".equals(cmd)) {\n      System.out.println(fetchImage);\n    } else if (\"allowSnapshot\".equalsIgnoreCase(cmd)) {\n      System.out.println(allowSnapshot);\n    } else if (\"disallowSnapshot\".equalsIgnoreCase(cmd)) {\n      System.out.println(disallowSnapshot);\n    } else if (\"shutdownDatanode\".equalsIgnoreCase(cmd)) {\n      System.out.println(shutdownDatanode);\n    } else if (\"evictWriters\".equalsIgnoreCase(cmd)) {\n      System.out.println(evictWriters);\n    } else if (\"getDatanodeInfo\".equalsIgnoreCase(cmd)) {\n      System.out.println(getDatanodeInfo);\n    } else if (\"help\".equals(cmd)) {\n      System.out.println(help);\n    } else {\n      System.out.println(summary);\n      System.out.println(report);\n      System.out.println(safemode);\n      System.out.println(saveNamespace);\n      System.out.println(rollEdits);\n      System.out.println(restoreFailedStorage);\n      System.out.println(refreshNodes);\n      System.out.println(finalizeUpgrade);\n      System.out.println(RollingUpgradeCommand.DESCRIPTION);\n      System.out.println(metaSave);\n      System.out.println(SetQuotaCommand.DESCRIPTION);\n      System.out.println(ClearQuotaCommand.DESCRIPTION);\n      System.out.println(SetSpaceQuotaCommand.DESCRIPTION);\n      System.out.println(ClearSpaceQuotaCommand.DESCRIPTION);\n      System.out.println(refreshServiceAcl);\n      System.out.println(refreshUserToGroupsMappings);\n      System.out.println(refreshSuperUserGroupsConfiguration);\n      System.out.println(refreshCallQueue);\n      System.out.println(genericRefresh);\n      System.out.println(reconfig);\n      System.out.println(printTopology);\n      System.out.println(refreshNamenodes);\n      System.out.println(deleteBlockPool);\n      System.out.println(setBalancerBandwidth);\n      System.out.println(getBalancerBandwidth);\n      System.out.println(fetchImage);\n      System.out.println(allowSnapshot);\n      System.out.println(disallowSnapshot);\n      System.out.println(shutdownDatanode);\n      System.out.println(evictWriters);\n      System.out.println(getDatanodeInfo);\n      System.out.println(triggerBlockReport);\n      System.out.println(help);\n      System.out.println();\n      ToolRunner.printGenericCommandUsage(System.out);\n    }\n\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.refreshServiceAcl": "  public int refreshServiceAcl() throws IOException {\n    // Get the current configuration\n    Configuration conf = getConf();\n\n    // for security authorization\n    // server principal for this call   \n    // should be NN's one.\n    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n        conf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, \"\"));\n\n    DistributedFileSystem dfs = getDFS();\n    URI dfsUri = dfs.getUri();\n    boolean isHaEnabled = HAUtilClient.isLogicalUri(conf, dfsUri);\n\n    if (isHaEnabled) {\n      // Run refreshServiceAcl for all NNs if HA is enabled\n      String nsId = dfsUri.getHost();\n      List<ProxyAndInfo<RefreshAuthorizationPolicyProtocol>> proxies =\n          HAUtil.getProxiesForAllNameNodesInNameservice(conf, nsId,\n              RefreshAuthorizationPolicyProtocol.class);\n      for (ProxyAndInfo<RefreshAuthorizationPolicyProtocol> proxy : proxies) {\n        proxy.getProxy().refreshServiceAcl();\n        System.out.println(\"Refresh service acl successful for \"\n            + proxy.getAddress());\n      }\n    } else {\n      // Create the client\n      RefreshAuthorizationPolicyProtocol refreshProtocol =\n          NameNodeProxies.createProxy(conf, FileSystem.getDefaultUri(conf),\n              RefreshAuthorizationPolicyProtocol.class).getProxy();\n      // Refresh the authorization policy in-effect\n      refreshProtocol.refreshServiceAcl();\n      System.out.println(\"Refresh service acl successful\");\n    }\n    \n    return 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.refreshSuperUserGroupsConfiguration": "  public int refreshSuperUserGroupsConfiguration() throws IOException {\n    // Get the current configuration\n    Configuration conf = getConf();\n\n    // for security authorization\n    // server principal for this call \n    // should be NAMENODE's one.\n    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n        conf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, \"\"));\n\n    DistributedFileSystem dfs = getDFS();\n    URI dfsUri = dfs.getUri();\n    boolean isHaEnabled = HAUtilClient.isLogicalUri(conf, dfsUri);\n\n    if (isHaEnabled) {\n      // Run refreshSuperUserGroupsConfiguration for all NNs if HA is enabled\n      String nsId = dfsUri.getHost();\n      List<ProxyAndInfo<RefreshUserMappingsProtocol>> proxies =\n          HAUtil.getProxiesForAllNameNodesInNameservice(conf, nsId,\n              RefreshUserMappingsProtocol.class);\n      for (ProxyAndInfo<RefreshUserMappingsProtocol> proxy : proxies) {\n        proxy.getProxy().refreshSuperUserGroupsConfiguration();\n        System.out.println(\"Refresh super user groups configuration \" +\n            \"successful for \" + proxy.getAddress());\n      }\n    } else {\n      // Create the client\n      RefreshUserMappingsProtocol refreshProtocol =\n          NameNodeProxies.createProxy(conf, FileSystem.getDefaultUri(conf),\n              RefreshUserMappingsProtocol.class).getProxy();\n\n      // Refresh the user-to-groups mappings\n      refreshProtocol.refreshSuperUserGroupsConfiguration();\n      System.out.println(\"Refresh super user groups configuration successful\");\n    }\n\n    return 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.rollEdits": "  public int rollEdits() throws IOException {\n    DistributedFileSystem dfs = getDFS();\n    long txid = dfs.rollEdits();\n    System.out.println(\"Successfully rolled edit logs.\");\n    System.out.println(\"New segment starts at txid \" + txid);\n    return 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.allowSnapshot": "  public void allowSnapshot(String[] argv) throws IOException {   \n    DistributedFileSystem dfs = getDFS();\n    try {\n      dfs.allowSnapshot(new Path(argv[1]));\n    } catch (SnapshotException e) {\n      throw new RemoteException(e.getClass().getName(), e.getMessage());\n    }\n    System.out.println(\"Allowing snaphot on \" + argv[1] + \" succeeded\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.printTopology": "  public int printTopology() throws IOException {\n      DistributedFileSystem dfs = getDFS();\n      final DatanodeInfo[] report = dfs.getDataNodeStats();\n\n      // Build a map of rack -> nodes from the datanode report\n      HashMap<String, TreeSet<String> > tree = new HashMap<String, TreeSet<String>>();\n      for(DatanodeInfo dni : report) {\n        String location = dni.getNetworkLocation();\n        String name = dni.getName();\n        \n        if(!tree.containsKey(location)) {\n          tree.put(location, new TreeSet<String>());\n        }\n        \n        tree.get(location).add(name);\n      }\n      \n      // Sort the racks (and nodes) alphabetically, display in order\n      ArrayList<String> racks = new ArrayList<String>(tree.keySet());\n      Collections.sort(racks);\n      \n      for(String r : racks) {\n        System.out.println(\"Rack: \" + r);\n        TreeSet<String> nodes = tree.get(r);\n\n        for(String n : nodes) {\n          System.out.print(\"   \" + n);\n          String hostname = NetUtils.getHostNameOfIP(n);\n          if(hostname != null)\n            System.out.print(\" (\" + hostname + \")\");\n          System.out.println();\n        }\n\n        System.out.println();\n      }\n    return 0;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ToolRunner.run": "  public static int run(Tool tool, String[] args) \n    throws Exception{\n    return run(tool.getConf(), tool, args);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.main": "  public static void main(String[] argv) throws Exception {\n    int res = ToolRunner.run(new DFSAdmin(), argv);\n    System.exit(res);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs": "  public String[] getRemainingArgs() {\n    return (commandLine == null) ? new String[]{} : commandLine.getArgs();\n  }"
        },
        "bug_report": {
            "Title": "DFSAdmin should log detailed error message if any",
            "Description": "There are some subcommands in {{DFSAdmin}} that swallow IOException and give very limited error message, if any, to the stderr.\n\n{code}\n$ hdfs dfsadmin -getBalancerBandwidth 127.0.0.1:9866\nDatanode unreachable.\n$ hdfs dfsadmin -getDatanodeInfo localhost:9866\nDatanode unreachable.\n$ hdfs dfsadmin -evictWriters 127.0.0.1:9866\n$ echo $?\n-1\n{code}\n\nUser is not able to get the exception stack even the LOG level is DEBUG. This is not very user friendly. Fortunately, if the port number is not accessible (say 9999), users can infer the detailed error message by IPC logs:\n{code}\n$ hdfs dfsadmin -getBalancerBandwidth 127.0.0.1:9999\n2016-10-07 18:01:35,115 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2016-10-07 18:01:36,335 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:9999. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n.....\n2016-10-07 18:01:45,361 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:9999. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n2016-10-07 18:01:45,362 WARN ipc.Client: Failed to connect to server: localhost/127.0.0.1:9999: retries get failed due to exceeded maximum allowed retries number: 10\njava.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n        ...\n\tat org.apache.hadoop.hdfs.tools.DFSAdmin.run(DFSAdmin.java:2073)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\n\tat org.apache.hadoop.hdfs.tools.DFSAdmin.main(DFSAdmin.java:2225)\nDatanode unreachable.\n{code}\n\nWe should fix this by providing detailed error message. Actually, the {{DFSAdmin#run}} already handles exception carefully, including:\n# set the exit ret value to -1\n# print the error message\n# log the exception stack trace (in DEBUG level)\n\nAll we need to do is to not swallow exceptions without good reason."
        }
    },
    {
        "filename": "HDFS-6455.json",
        "creation_time": "2014-05-28T18:01:15.000+0000",
        "stack_trace": "```\nException in thread \"main\" java.lang.IllegalArgumentException: Incorrectly formatted line 'host1 ro:host2 rw'\n\tat org.apache.hadoop.nfs.NfsExports.getMatch(NfsExports.java:356)\n\tat org.apache.hadoop.nfs.NfsExports.<init>(NfsExports.java:151)\n\tat org.apache.hadoop.nfs.NfsExports.getInstance(NfsExports.java:54)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.<init>(RpcProgramNfs3.java:176)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.<init>(Nfs3.java:43)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.main(Nfs3.java:59)\n\nException in thread \"main\" java.lang.IllegalArgumentException: Incorrectly formatted line 'host1 ro:host2 rw'\n        at org.apache.hadoop.nfs.NfsExports.getMatch(NfsExports.java:356)\n        at org.apache.hadoop.nfs.NfsExports.<init>(NfsExports.java:151)\n        at org.apache.hadoop.nfs.NfsExports.getInstance(NfsExports.java:54)\n        at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.<init>(RpcProgramNfs3.java:176)\n        at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.<init>(Nfs3.java:43)\n        at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.main(Nfs3.java:59)```",
        "source_code": {
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.nfs.NfsExports.getMatch": "  private static Match getMatch(String line) {\n    String[] parts = line.split(\"\\\\s+\");\n    final String host;\n    AccessPrivilege privilege = AccessPrivilege.READ_ONLY;\n    switch (parts.length) {\n    case 1:\n      host = parts[0].toLowerCase().trim();\n      break;\n    case 2:\n      host = parts[0].toLowerCase().trim();\n      String option = parts[1].trim();\n      if (\"rw\".equalsIgnoreCase(option)) {\n        privilege = AccessPrivilege.READ_WRITE;\n      }\n      break;\n    default:\n      throw new IllegalArgumentException(\"Incorrectly formatted line '\" + line\n          + \"'\");\n    }\n    if (host.equals(\"*\")) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using match all for '\" + host + \"' and \" + privilege);\n      }\n      return new AnonymousMatch(privilege);\n    } else if (CIDR_FORMAT_SHORT.matcher(host).matches()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using CIDR match for '\" + host + \"' and \" + privilege);\n      }\n      return new CIDRMatch(privilege, new SubnetUtils(host).getInfo());\n    } else if (CIDR_FORMAT_LONG.matcher(host).matches()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using CIDR match for '\" + host + \"' and \" + privilege);\n      }\n      String[] pair = host.split(\"/\");\n      return new CIDRMatch(privilege,\n          new SubnetUtils(pair[0], pair[1]).getInfo());\n    } else if (host.contains(\"*\") || host.contains(\"?\") || host.contains(\"[\")\n        || host.contains(\"]\")) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using Regex match for '\" + host + \"' and \" + privilege);\n      }\n      return new RegexMatch(privilege, host);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Using exact match for '\" + host + \"' and \" + privilege);\n    }\n    return new ExactMatch(privilege, host);\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.nfs.NfsExports.equals": "    public boolean equals(Object obj) {\n      if (this == obj) {\n        return true;\n      }\n      if (obj instanceof AccessCacheEntry) {\n        AccessCacheEntry entry = (AccessCacheEntry) obj;\n        return this.hostAddr.equals(entry.hostAddr);\n      }\n      return false;\n    }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.nfs.NfsExports.getInstance": "  public static synchronized NfsExports getInstance(Configuration conf) {\n    if (exports == null) {\n      String matchHosts = conf.get(Nfs3Constant.EXPORTS_ALLOWED_HOSTS_KEY,\n          Nfs3Constant.EXPORTS_ALLOWED_HOSTS_KEY_DEFAULT);\n      int cacheSize = conf.getInt(Nfs3Constant.EXPORTS_CACHE_SIZE_KEY,\n          Nfs3Constant.EXPORTS_CACHE_SIZE_DEFAULT);\n      long expirationPeriodNano = conf.getLong(\n          Nfs3Constant.EXPORTS_CACHE_EXPIRYTIME_MILLIS_KEY,\n          Nfs3Constant.EXPORTS_CACHE_EXPIRYTIME_MILLIS_DEFAULT) * 1000 * 1000;\n      exports = new NfsExports(cacheSize, expirationPeriodNano, matchHosts);\n    }\n    return exports;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.main": "  public static void main(String[] args) throws IOException {\n    startService(args, null);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService": "  static void startService(String[] args,\n      DatagramSocket registrationSocket) throws IOException {\n    StringUtils.startupShutdownMessage(Nfs3.class, args, LOG);\n    Configuration conf = new Configuration();\n    boolean allowInsecurePorts = conf.getBoolean(\n        DFSConfigKeys.DFS_NFS_ALLOW_INSECURE_PORTS_KEY,\n        DFSConfigKeys.DFS_NFS_ALLOW_INSECURE_PORTS_DEFAULT);\n    final Nfs3 nfsServer = new Nfs3(new Configuration(), registrationSocket,\n        allowInsecurePorts);\n    nfsServer.startServiceInternal(true);\n  }"
        },
        "bug_report": {
            "Title": "NFS: Exception should be added in NFS log for invalid separator in nfs.exports.allowed.hosts",
            "Description": "The error for invalid separator in dfs.nfs.exports.allowed.hosts property should be added in nfs log file instead nfs.out file.\n\nSteps to reproduce:\n1. Pass invalid separator in dfs.nfs.exports.allowed.hosts\n{noformat}\n<property><name>dfs.nfs.exports.allowed.hosts</name><value>host1  ro:host2 rw</value></property>\n{noformat}\n\n2. restart NFS server. NFS server fails to start and print exception console.\n{noformat}\n[hrt_qa@host1 hwqe]$ ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null host1 \"sudo su - -c \\\"/usr/lib/hadoop/sbin/hadoop-daemon.sh start nfs3\\\" hdfs\"\nstarting nfs3, logging to /tmp/log/hadoop/hdfs/hadoop-hdfs-nfs3-horst1.out\nDEPRECATED: Use of this script to execute hdfs command is deprecated.\nInstead use the hdfs command for it.\n\nException in thread \"main\" java.lang.IllegalArgumentException: Incorrectly formatted line 'host1 ro:host2 rw'\n\tat org.apache.hadoop.nfs.NfsExports.getMatch(NfsExports.java:356)\n\tat org.apache.hadoop.nfs.NfsExports.<init>(NfsExports.java:151)\n\tat org.apache.hadoop.nfs.NfsExports.getInstance(NfsExports.java:54)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.<init>(RpcProgramNfs3.java:176)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.<init>(Nfs3.java:43)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.main(Nfs3.java:59)\n{noformat}\n\nNFS log does not print any error message. It directly shuts down. \n{noformat}\nSTARTUP_MSG:   java = 1.6.0_31\n************************************************************/\n2014-05-27 18:47:13,972 INFO  nfs3.Nfs3Base (SignalLogger.java:register(91)) - registered UNIX signal handlers for [TERM, HUP, INT]\n2014-05-27 18:47:14,169 INFO  nfs3.IdUserGroup (IdUserGroup.java:updateMapInternal(159)) - Updated user map size:259\n2014-05-27 18:47:14,179 INFO  nfs3.IdUserGroup (IdUserGroup.java:updateMapInternal(159)) - Updated group map size:73\n2014-05-27 18:47:14,192 INFO  nfs3.Nfs3Base (StringUtils.java:run(640)) - SHUTDOWN_MSG:\n/************************************************************\nSHUTDOWN_MSG: Shutting down Nfs3 at \n{noformat}\n\nNFS.out file has exception.\n{noformat}\nEPRECATED: Use of this script to execute hdfs command is deprecated.\nInstead use the hdfs command for it.\n\nException in thread \"main\" java.lang.IllegalArgumentException: Incorrectly formatted line 'host1 ro:host2 rw'\n        at org.apache.hadoop.nfs.NfsExports.getMatch(NfsExports.java:356)\n        at org.apache.hadoop.nfs.NfsExports.<init>(NfsExports.java:151)\n        at org.apache.hadoop.nfs.NfsExports.getInstance(NfsExports.java:54)\n        at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.<init>(RpcProgramNfs3.java:176)\n        at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.<init>(Nfs3.java:43)\n        at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.main(Nfs3.java:59)\nulimit -a for user hdfs\ncore file size          (blocks, -c) 409600\ndata seg size           (kbytes, -d) unlimited\nscheduling priority             (-e) 0\nfile size               (blocks, -f) unlimited\npending signals                 (-i) 188893\nmax locked memory       (kbytes, -l) unlimited\nmax memory size         (kbytes, -m) unlimited\nopen files                      (-n) 32768\npipe size            (512 bytes, -p) 8\nPOSIX message queues     (bytes, -q) 819200\nreal-time priority              (-r) 0\nstack size              (kbytes, -s) 10240\ncpu time               (seconds, -t) unlimited\nmax user processes              (-u) 65536\nvirtual memory          (kbytes, -v) unlimited\nfile locks                      (-x) unlimited\n{noformat}"
        }
    },
    {
        "filename": "HDFS-2882.json",
        "creation_time": "2012-02-02T18:11:20.000+0000",
        "stack_trace": "```\njava.io.IOException: Mkdirs failed to create /data/1/scratch/todd/styx-datadir/current/BP-448349972-172.29.5.192-1323816762969/tmp\n        at org.apache.hadoop.hdfs.server.datanode.FSDataset$BlockPoolSlice.<init>(FSDataset.java:335)```",
        "source_code": {},
        "bug_report": {
            "Title": "DN continues to start up, even if block pool fails to initialize",
            "Description": "I started a DN on a machine that was completely out of space on one of its drives. I saw the following:\n\n2012-02-02 09:56:50,499 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool BP-448349972-172.29.5.192-1323816762969 (storage id DS-507718931-172.29.5.194-11072-12978\n42002148) service to styx01.sf.cloudera.com/172.29.5.192:8021\njava.io.IOException: Mkdirs failed to create /data/1/scratch/todd/styx-datadir/current/BP-448349972-172.29.5.192-1323816762969/tmp\n        at org.apache.hadoop.hdfs.server.datanode.FSDataset$BlockPoolSlice.<init>(FSDataset.java:335)\n\nbut the DN continued to run, spewing NPEs when it tried to do block reports, etc. This was on the HDFS-1623 branch but may affect trunk as well."
        }
    },
    {
        "filename": "HDFS-5185.json",
        "creation_time": "2013-09-11T14:38:23.000+0000",
        "stack_trace": "```\njava.io.IOException: Mkdirs failed to create /opt/nish/data/current/BP-123456-1234567/tmp\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.<init>(BlockPoolSlice.java:105)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.addBlockPool(FsVolumeImpl.java:216)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addBlockPool(FsVolumeList.java:155)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addBlockPool(FsDatasetImpl.java:1593)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:834)\n        at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:311)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:217)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:660)\n        at java.lang.Thread.run(Thread.java:662)```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.addBlockPool": "  void addBlockPool(String bpid, Configuration conf) throws IOException {\n    File bpdir = new File(currentDir, bpid);\n    BlockPoolSlice bp = new BlockPoolSlice(bpid, this, bpdir, conf);\n    bpSlices.put(bpid, bp);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addBlockPool": "  void addBlockPool(final String bpid, final Configuration conf) throws IOException {\n    long totalStartTime = System.currentTimeMillis();\n    \n    final List<IOException> exceptions = Collections.synchronizedList(\n        new ArrayList<IOException>());\n    List<Thread> blockPoolAddingThreads = new ArrayList<Thread>();\n    for (final FsVolumeImpl v : volumes) {\n      Thread t = new Thread() {\n        public void run() {\n          try {\n            FsDatasetImpl.LOG.info(\"Scanning block pool \" + bpid +\n                \" on volume \" + v + \"...\");\n            long startTime = System.currentTimeMillis();\n            v.addBlockPool(bpid, conf);\n            long timeTaken = System.currentTimeMillis() - startTime;\n            FsDatasetImpl.LOG.info(\"Time taken to scan block pool \" + bpid +\n                \" on \" + v + \": \" + timeTaken + \"ms\");\n          } catch (IOException ioe) {\n            FsDatasetImpl.LOG.info(\"Caught exception while scanning \" + v +\n                \". Will throw later.\", ioe);\n            exceptions.add(ioe);\n          }\n        }\n      };\n      blockPoolAddingThreads.add(t);\n      t.start();\n    }\n    for (Thread t : blockPoolAddingThreads) {\n      try {\n        t.join();\n      } catch (InterruptedException ie) {\n        throw new IOException(ie);\n      }\n    }\n    if (!exceptions.isEmpty()) {\n      throw exceptions.get(0);\n    }\n    \n    long totalTimeTaken = System.currentTimeMillis() - totalStartTime;\n    FsDatasetImpl.LOG.info(\"Total time to scan all replicas for block pool \" +\n        bpid + \": \" + totalTimeTaken + \"ms\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addBlockPool": "  public synchronized void addBlockPool(String bpid, Configuration conf)\n      throws IOException {\n    LOG.info(\"Adding block pool \" + bpid);\n    volumes.addBlockPool(bpid, conf);\n    volumeMap.initBlockPool(bpid);\n    volumes.getVolumeMap(bpid, volumeMap);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool": "  void initBlockPool(BPOfferService bpos) throws IOException {\n    NamespaceInfo nsInfo = bpos.getNamespaceInfo();\n    if (nsInfo == null) {\n      throw new IOException(\"NamespaceInfo not found: Block pool \" + bpos\n          + \" should have retrieved namespace info before initBlockPool.\");\n    }\n    \n    // Register the new block pool with the BP manager.\n    blockPoolManager.addBlockPool(bpos);\n\n    setClusterId(nsInfo.clusterID, nsInfo.getBlockPoolID());\n    \n    // In the case that this is the first block pool to connect, initialize\n    // the dataset, block scanners, etc.\n    initStorage(nsInfo);\n    initPeriodicScanners(conf);\n    \n    data.addBlockPool(nsInfo.getBlockPoolID(), conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.setClusterId": "  private synchronized void setClusterId(final String nsCid, final String bpid\n      ) throws IOException {\n    if(clusterId != null && !clusterId.equals(nsCid)) {\n      throw new IOException (\"Cluster IDs not matched: dn cid=\" + clusterId \n          + \" but ns cid=\"+ nsCid + \"; bpid=\" + bpid);\n    }\n    // else\n    clusterId = nsCid;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage": "  private void initStorage(final NamespaceInfo nsInfo) throws IOException {\n    final FsDatasetSpi.Factory<? extends FsDatasetSpi<?>> factory\n        = FsDatasetSpi.Factory.getFactory(conf);\n    \n    if (!factory.isSimulated()) {\n      final StartupOption startOpt = getStartupOption(conf);\n      if (startOpt == null) {\n        throw new IOException(\"Startup option not set.\");\n      }\n      final String bpid = nsInfo.getBlockPoolID();\n      //read storage info, lock data dirs and transition fs state if necessary\n      storage.recoverTransitionRead(this, bpid, nsInfo, dataDirs, startOpt);\n      final StorageInfo bpStorage = storage.getBPStorage(bpid);\n      LOG.info(\"Setting up storage: nsid=\" + bpStorage.getNamespaceID()\n          + \";bpid=\" + bpid + \";lv=\" + storage.getLayoutVersion()\n          + \";nsInfo=\" + nsInfo);\n    }\n\n    synchronized(this)  {\n      if (data == null) {\n        data = factory.newInstance(this, storage, conf);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.initPeriodicScanners": "  private void initPeriodicScanners(Configuration conf) {\n    initDataBlockScanner(conf);\n    initDirectoryScanner(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo": "  synchronized void verifyAndSetNamespaceInfo(NamespaceInfo nsInfo) throws IOException {\n    if (this.bpNSInfo == null) {\n      this.bpNSInfo = nsInfo;\n      \n      // Now that we know the namespace ID, etc, we can pass this to the DN.\n      // The DN can now initialize its local storage if we are the\n      // first BP to handshake, etc.\n      dn.initBlockPool(this);\n      return;\n    } else {\n      checkNSEquality(bpNSInfo.getBlockPoolID(), nsInfo.getBlockPoolID(),\n          \"Blockpool ID\");\n      checkNSEquality(bpNSInfo.getNamespaceID(), nsInfo.getNamespaceID(),\n          \"Namespace ID\");\n      checkNSEquality(bpNSInfo.getClusterID(), nsInfo.getClusterID(),\n          \"Cluster ID\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPOfferService.checkNSEquality": "  private static void checkNSEquality(\n      Object ourID, Object theirID,\n      String idHelpText) throws IOException {\n    if (!ourID.equals(theirID)) {\n      throw new IOException(idHelpText + \" mismatch: \" +\n          \"previously connected to \" + idHelpText + \" \" + ourID + \n          \" but now connected to \" + idHelpText + \" \" + theirID);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake": "  private void connectToNNAndHandshake() throws IOException {\n    // get NN proxy\n    bpNamenode = dn.connectToNN(nnAddr);\n\n    // First phase of the handshake with NN - get the namespace\n    // info.\n    NamespaceInfo nsInfo = retrieveNamespaceInfo();\n    \n    // Verify that this matches the other NN in this HA pair.\n    // This also initializes our block pool in the DN if we are\n    // the first NN connection for this BP.\n    bpos.verifyAndSetNamespaceInfo(nsInfo);\n    \n    // Second phase of the handshake with the NN.\n    register();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo": "  NamespaceInfo retrieveNamespaceInfo() throws IOException {\n    NamespaceInfo nsInfo = null;\n    while (shouldRun()) {\n      try {\n        nsInfo = bpNamenode.versionRequest();\n        LOG.debug(this + \" received versionRequest response: \" + nsInfo);\n        break;\n      } catch(SocketTimeoutException e) {  // namenode is busy\n        LOG.warn(\"Problem connecting to server: \" + nnAddr);\n      } catch(IOException e ) {  // namenode is not available\n        LOG.warn(\"Problem connecting to server: \" + nnAddr);\n      }\n      \n      // try again in a second\n      sleepAndLogInterrupts(5000, \"requesting version info from NN\");\n    }\n    \n    if (nsInfo != null) {\n      checkNNVersion(nsInfo);\n    } else {\n      throw new IOException(\"DN shut down before block pool connected\");\n    }\n    return nsInfo;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.register": "  void register() throws IOException {\n    // The handshake() phase loaded the block pool storage\n    // off disk - so update the bpRegistration object from that info\n    bpRegistration = bpos.createRegistration();\n\n    LOG.info(this + \" beginning handshake with NN\");\n\n    while (shouldRun()) {\n      try {\n        // Use returned registration from namenode with updated fields\n        bpRegistration = bpNamenode.registerDatanode(bpRegistration);\n        break;\n      } catch(SocketTimeoutException e) {  // namenode is busy\n        LOG.info(\"Problem connecting to server: \" + nnAddr);\n        sleepAndLogInterrupts(1000, \"connecting to server\");\n      }\n    }\n    \n    LOG.info(\"Block pool \" + this + \" successfully registered with NN\");\n    bpos.registrationSucceeded(this, bpRegistration);\n\n    // random short delay - helps scatter the BR from all DNs\n    scheduleBlockReport(dnConf.initialBlockReportDelay);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run": "  public void run() {\n    LOG.info(this + \" starting to offer service\");\n\n    try {\n      // init stuff\n      try {\n        // setup storage\n        connectToNNAndHandshake();\n      } catch (IOException ioe) {\n        // Initial handshake, storage recovery or registration failed\n        // End BPOfferService thread\n        LOG.fatal(\"Initialization failed for block pool \" + this, ioe);\n        return;\n      }\n\n      initialized = true; // bp is initialized;\n      \n      while (shouldRun()) {\n        try {\n          offerService();\n        } catch (Exception ex) {\n          LOG.error(\"Exception in BPOfferService for \" + this, ex);\n          sleepAndLogInterrupts(5000, \"offering service\");\n        }\n      }\n    } catch (Throwable ex) {\n      LOG.warn(\"Unexpected exception in block pool \" + this, ex);\n    } finally {\n      LOG.warn(\"Ending block pool service for: \" + this);\n      cleanUp();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService": "  private void offerService() throws Exception {\n    LOG.info(\"For namenode \" + nnAddr + \" using DELETEREPORT_INTERVAL of \"\n        + dnConf.deleteReportInterval + \" msec \" + \" BLOCKREPORT_INTERVAL of \"\n        + dnConf.blockReportInterval + \"msec\" + \" Initial delay: \"\n        + dnConf.initialBlockReportDelay + \"msec\" + \"; heartBeatInterval=\"\n        + dnConf.heartBeatInterval);\n\n    //\n    // Now loop for a long time....\n    //\n    while (shouldRun()) {\n      try {\n        long startTime = now();\n\n        //\n        // Every so often, send heartbeat or block-report\n        //\n        if (startTime - lastHeartbeat >= dnConf.heartBeatInterval) {\n          //\n          // All heartbeat messages include following info:\n          // -- Datanode name\n          // -- data transfer port\n          // -- Total capacity\n          // -- Bytes remaining\n          //\n          lastHeartbeat = startTime;\n          if (!dn.areHeartbeatsDisabledForTests()) {\n            HeartbeatResponse resp = sendHeartBeat();\n            assert resp != null;\n            dn.getMetrics().addHeartbeat(now() - startTime);\n\n            // If the state of this NN has changed (eg STANDBY->ACTIVE)\n            // then let the BPOfferService update itself.\n            //\n            // Important that this happens before processCommand below,\n            // since the first heartbeat to a new active might have commands\n            // that we should actually process.\n            bpos.updateActorStatesFromHeartbeat(\n                this, resp.getNameNodeHaState());\n\n            long startProcessCommands = now();\n            if (!processCommand(resp.getCommands()))\n              continue;\n            long endProcessCommands = now();\n            if (endProcessCommands - startProcessCommands > 2000) {\n              LOG.info(\"Took \" + (endProcessCommands - startProcessCommands)\n                  + \"ms to process \" + resp.getCommands().length\n                  + \" commands from NN\");\n            }\n          }\n        }\n        if (pendingReceivedRequests > 0\n            || (startTime - lastDeletedReport > dnConf.deleteReportInterval)) {\n          reportReceivedDeletedBlocks();\n          lastDeletedReport = startTime;\n        }\n\n        DatanodeCommand cmd = blockReport();\n        processCommand(new DatanodeCommand[]{ cmd });\n\n        // Now safe to start scanning the block pool.\n        // If it has already been started, this is a no-op.\n        if (dn.blockScanner != null) {\n          dn.blockScanner.addBlockPool(bpos.getBlockPoolId());\n        }\n\n        //\n        // There is no work to do;  sleep until hearbeat timer elapses, \n        // or work arrives, and then iterate again.\n        //\n        long waitTime = dnConf.heartBeatInterval - \n        (Time.now() - lastHeartbeat);\n        synchronized(pendingIncrementalBR) {\n          if (waitTime > 0 && pendingReceivedRequests == 0) {\n            try {\n              pendingIncrementalBR.wait(waitTime);\n            } catch (InterruptedException ie) {\n              LOG.warn(\"BPOfferService for \" + this + \" interrupted\");\n            }\n          }\n        } // synchronized\n      } catch(RemoteException re) {\n        String reClass = re.getClassName();\n        if (UnregisteredNodeException.class.getName().equals(reClass) ||\n            DisallowedDatanodeException.class.getName().equals(reClass) ||\n            IncorrectVersionException.class.getName().equals(reClass)) {\n          LOG.warn(this + \" is shutting down\", re);\n          shouldServiceRun = false;\n          return;\n        }\n        LOG.warn(\"RemoteException in offerService\", re);\n        try {\n          long sleepTime = Math.min(1000, dnConf.heartBeatInterval);\n          Thread.sleep(sleepTime);\n        } catch (InterruptedException ie) {\n          Thread.currentThread().interrupt();\n        }\n      } catch (IOException e) {\n        LOG.warn(\"IOException in offerService\", e);\n      }\n    } // while (shouldRun())\n  } // offerService",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp": "  private synchronized void cleanUp() {\n    \n    shouldServiceRun = false;\n    IOUtils.cleanup(LOG, bpNamenode);\n    bpos.shutdownActor(this);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sleepAndLogInterrupts": "  private void sleepAndLogInterrupts(int millis,\n      String stateString) {\n    try {\n      Thread.sleep(millis);\n    } catch (InterruptedException ie) {\n      LOG.info(\"BPOfferService \" + this + \" interrupted while \" + stateString);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.shouldRun": "  private boolean shouldRun() {\n    return shouldServiceRun && dn.shouldRun();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.blockPoolManager.addBlockPool": "  synchronized void addBlockPool(BPOfferService bpos) {\n    Preconditions.checkArgument(offerServices.contains(bpos),\n        \"Unknown BPOS: %s\", bpos);\n    if (bpos.getBlockPoolId() == null) {\n      throw new IllegalArgumentException(\"Null blockpool id\");\n    }\n    bpByBlockPoolId.put(bpos.getBlockPoolId(), bpos);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPOfferService.getNamespaceInfo": "  synchronized NamespaceInfo getNamespaceInfo() {\n    return bpNSInfo;\n  }"
        },
        "bug_report": {
            "Title": "DN fails to startup if one of the data dir is full",
            "Description": "DataNode fails to startup if one of the data dirs configured is out of space. \n\n\nfails with following exception\n{noformat}2013-09-11 17:48:43,680 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool <registering> (storage id DS-308316523-xx.xx.xx.xx-64015-1378896293604) service to /nn1:65110\njava.io.IOException: Mkdirs failed to create /opt/nish/data/current/BP-123456-1234567/tmp\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.<init>(BlockPoolSlice.java:105)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.addBlockPool(FsVolumeImpl.java:216)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addBlockPool(FsVolumeList.java:155)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addBlockPool(FsDatasetImpl.java:1593)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:834)\n        at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:311)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:217)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:660)\n        at java.lang.Thread.run(Thread.java:662)\n{noformat}\n\n\nIt should continue to start-up with other data dirs available."
        }
    },
    {
        "filename": "HDFS-13164.json",
        "creation_time": "2018-02-17T00:40:21.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /DIR is exceeded: quota = 2000000 B = 1.91 MB but diskspace consumed = 404139552 B = 385.42 MB\n        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyDiskspaceQuota(DirectoryWithQuotaFeature.java:149)\n        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:159)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:2124)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1991)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1966)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:463)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3896)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3484)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:686)\n        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:217)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:506)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2226)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2222)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2220)\n\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.DSQuotaExceededException): The DiskSpace quota of /tmp/logs/systest/logs is exceeded: quota = 2000000 B = 1.91 MB but diskspace consumed = 404139552 B = 385.42 MB\n        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyDiskspaceQuota(DirectoryWithQuotaFeature.java:149)\n        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:159)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:2124)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1991)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1966)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:463)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3896)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3484)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:686)\n        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:217)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:506)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine$Server$ProtoBufRpcInvoker.java:617)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2226)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2222)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2220)\n\n        at org.apache.hadoop.ipc.Client.call(Client.java:1504)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1441)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)\n        at com.sun.proxy.$Proxy82.addBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:423)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(sun.reflect.NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:258)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:104)\n        at com.sun.proxy.$Proxy83.addBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1830)```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota": "  void verifyQuota(QuotaCounts counts) throws QuotaExceededException {\n    verifyNamespaceQuota(counts.getNameSpace());\n    verifyStoragespaceQuota(counts.getStorageSpace());\n    verifyQuotaByStorageType(counts.getTypeSpaces());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyStoragespaceQuota": "  private void verifyStoragespaceQuota(long delta) throws DSQuotaExceededException {\n    if (Quota.isViolated(quota.getStorageSpace(), usage.getStorageSpace(), delta)) {\n      throw new DSQuotaExceededException(quota.getStorageSpace(),\n          usage.getStorageSpace() + delta);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyNamespaceQuota": "  private void verifyNamespaceQuota(long delta) throws NSQuotaExceededException {\n    if (Quota.isViolated(quota.getNameSpace(), usage.getNameSpace(), delta)) {\n      throw new NSQuotaExceededException(quota.getNameSpace(),\n          usage.getNameSpace() + delta);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuotaByStorageType": "  private void verifyQuotaByStorageType(EnumCounters<StorageType> typeDelta)\n      throws QuotaByStorageTypeExceededException {\n    if (!isQuotaByStorageTypeSet()) {\n      return;\n    }\n    for (StorageType t: StorageType.getTypesSupportingQuota()) {\n      if (!isQuotaByStorageTypeSet(t)) {\n        continue;\n      }\n      if (Quota.isViolated(quota.getTypeSpace(t), usage.getTypeSpace(t),\n          typeDelta.get(t))) {\n        throw new QuotaByStorageTypeExceededException(\n          quota.getTypeSpace(t), usage.getTypeSpace(t) + typeDelta.get(t), t);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota": "  static void verifyQuota(INodesInPath iip, int pos, QuotaCounts deltas,\n                          INode commonAncestor) throws QuotaExceededException {\n    if (deltas.getNameSpace() <= 0 && deltas.getStorageSpace() <= 0\n        && deltas.getTypeSpaces().allLessOrEqual(0L)) {\n      // if quota is being freed or not being consumed\n      return;\n    }\n\n    // check existing components in the path\n    for(int i = (pos > iip.length() ? iip.length(): pos) - 1; i >= 0; i--) {\n      if (commonAncestor == iip.getINode(i)) {\n        // Stop checking for quota when common ancestor is reached\n        return;\n      }\n      final DirectoryWithQuotaFeature q\n          = iip.getINode(i).asDirectory().getDirectoryWithQuotaFeature();\n      if (q != null) { // a directory with quota\n        try {\n          q.verifyQuota(deltas);\n        } catch (QuotaExceededException e) {\n          e.setPathName(iip.getPath(i));\n          throw e;\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getINode": "  public INode getINode(String src, DirOp dirOp) throws UnresolvedLinkException,\n      AccessControlException, ParentNotDirectoryException {\n    return getINodesInPath(src, dirOp).getLastINode();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount": "  void updateCount(INodesInPath iip, int numOfINodes,\n                    QuotaCounts counts, boolean checkQuota)\n                    throws QuotaExceededException {\n    assert hasWriteLock();\n    if (!namesystem.isImageLoaded()) {\n      //still initializing. do not check or update quotas.\n      return;\n    }\n    if (numOfINodes > iip.length()) {\n      numOfINodes = iip.length();\n    }\n    if (checkQuota && !skipQuotaCheck) {\n      verifyQuota(iip, numOfINodes, counts, null);\n    }\n    unprotectedUpdateCount(iip, numOfINodes, counts);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedUpdateCount": "  static void unprotectedUpdateCount(INodesInPath inodesInPath,\n      int numOfINodes, QuotaCounts counts) {\n    for(int i=0; i < numOfINodes; i++) {\n      if (inodesInPath.getINode(i).isQuotaSet()) { // a directory with quota\n        inodesInPath.getINode(i).asDirectory().getDirectoryWithQuotaFeature()\n            .addSpaceConsumed2Cache(counts);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getStorageTypeDeltas": "  public EnumCounters<StorageType> getStorageTypeDeltas(byte storagePolicyID,\n      long dsDelta, short oldRep, short newRep) {\n    EnumCounters<StorageType> typeSpaceDeltas =\n        new EnumCounters<StorageType>(StorageType.class);\n    // empty file\n    if(dsDelta == 0){\n      return typeSpaceDeltas;\n    }\n    // Storage type and its quota are only available when storage policy is set\n    if (storagePolicyID != HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED) {\n      BlockStoragePolicy storagePolicy = getBlockManager().getStoragePolicy(storagePolicyID);\n\n      if (oldRep != newRep) {\n        List<StorageType> oldChosenStorageTypes =\n            storagePolicy.chooseStorageTypes(oldRep);\n\n        for (StorageType t : oldChosenStorageTypes) {\n          if (!t.supportTypeQuota()) {\n            continue;\n          }\n          Preconditions.checkArgument(dsDelta > 0);\n          typeSpaceDeltas.add(t, -dsDelta);\n        }\n      }\n\n      List<StorageType> newChosenStorageTypes =\n          storagePolicy.chooseStorageTypes(newRep);\n\n      for (StorageType t : newChosenStorageTypes) {\n        if (!t.supportTypeQuota()) {\n          continue;\n        }\n        typeSpaceDeltas.add(t, dsDelta);\n      }\n    }\n    return typeSpaceDeltas;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.hasWriteLock": "  boolean hasWriteLock() {\n    return this.dirLock.isWriteLockedByCurrentThread();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock": "  LocatedBlock getAdditionalBlock(\n      String src, long fileId, String clientName, ExtendedBlock previous,\n      DatanodeInfo[] excludedNodes, String[] favoredNodes,\n      EnumSet<AddBlockFlag> flags) throws IOException {\n    final String operationName = \"getAdditionalBlock\";\n    NameNode.stateChangeLog.debug(\"BLOCK* getAdditionalBlock: {}  inodeId {}\" +\n        \" for {}\", src, fileId, clientName);\n\n    LocatedBlock[] onRetryBlock = new LocatedBlock[1];\n    FSDirWriteFileOp.ValidateAddBlockResult r;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.READ);\n    readLock();\n    try {\n      checkOperation(OperationCategory.READ);\n      r = FSDirWriteFileOp.validateAddBlock(this, pc, src, fileId, clientName,\n                                            previous, onRetryBlock);\n    } finally {\n      readUnlock(operationName);\n    }\n\n    if (r == null) {\n      assert onRetryBlock[0] != null : \"Retry block is null\";\n      // This is a retry. Just return the last block.\n      return onRetryBlock[0];\n    }\n\n    DatanodeStorageInfo[] targets = FSDirWriteFileOp.chooseTargetForNewBlock(\n        blockManager, src, excludedNodes, favoredNodes, flags, r);\n\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    LocatedBlock lb;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      lb = FSDirWriteFileOp.storeAllocatedBlock(\n          this, src, fileId, clientName, previous, targets);\n    } finally {\n      writeUnlock(operationName);\n    }\n    getEditLog().logSync();\n    return lb;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEditLog": "  public FSEditLog getEditLog() {\n    return getFSImage().getEditLog();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock": "  public void writeUnlock(String opName, boolean suppressWriteLockReport) {\n    this.fsLock.writeUnlock(opName, suppressWriteLockReport);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock": "  public void writeLock() {\n    this.fsLock.writeLock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.readUnlock": "  public void readUnlock(String opName) {\n    this.fsLock.readUnlock(opName);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation": "  public void checkOperation(OperationCategory op) throws StandbyException {\n    if (haContext != null) {\n      // null in some unit tests\n      haContext.checkOperation(op);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker": "  FSPermissionChecker getPermissionChecker()\n      throws AccessControlException {\n    return dir.getPermissionChecker();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.readLock": "  public void readLock() {\n    this.fsLock.readLock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock": "  public LocatedBlock addBlock(String src, String clientName,\n      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n      String[] favoredNodes, EnumSet<AddBlockFlag> addBlockFlags)\n      throws IOException {\n    checkNNStartup();\n    LocatedBlock locatedBlock = namesystem.getAdditionalBlock(src, fileId,\n        clientName, previous, excludedNodes, favoredNodes, addBlockFlags);\n    if (locatedBlock != null) {\n      metrics.incrAddBlockOps();\n    }\n    return locatedBlock;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkNNStartup": "  private void checkNNStartup() throws IOException {\n    if (!this.nn.isStarted()) {\n      String message = NameNode.composeNotStartedMessage(this.nn.getRole());\n      throw new RetriableException(message);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcProtobufRequest request = (RpcProtobufRequest) writableRequest;\n        RequestHeaderProto rpcRequest = request.getRequestHeader();\n        String methodName = rpcRequest.getMethodName();\n\n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = request.getValue(prototype);\n\n        Message result;\n        long startTime = Time.now();\n        int qTime = (int) (startTime - receiveTime);\n        Exception exception = null;\n        boolean isDeferred = false;\n        try {\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          currentCallInfo.set(new CallInfo(server, methodName));\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n          // Check if this needs to be a deferred response,\n          // by checking the ThreadLocal callback being set\n          if (currentCallback.get() != null) {\n            Server.getCurCall().get().deferResponse();\n            isDeferred = true;\n            currentCallback.set(null);\n            return null;\n          }\n        } catch (ServiceException e) {\n          exception = (Exception) e.getCause();\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          exception = e;\n          throw e;\n        } finally {\n          currentCallInfo.set(null);\n          int processingTime = (int) (Time.now() - startTime);\n          if (LOG.isDebugEnabled()) {\n            String msg =\n                \"Served: \" + methodName + (isDeferred ? \", deferred\" : \"\") +\n                    \", queueTime= \" + qTime +\n                    \" procesingTime= \" + processingTime;\n            if (exception != null) {\n              msg += \" exception= \" + exception.getClass().getSimpleName();\n            }\n            LOG.debug(msg);\n          }\n          String detailedMetricsName = (exception == null) ?\n              methodName :\n              exception.getClass().getSimpleName();\n          server.updateMetrics(detailedMetricsName, qTime, processingTime,\n              isDeferred);\n        }\n        return RpcWritable.wrap(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.get": "          public Message get(long timeout, TimeUnit unit) throws Exception {\n            return getReturnMessage(method, arr.get(timeout, unit));\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getRequestHeader": "    RequestHeaderProto getRequestHeader() throws IOException {\n      if (getByteBuffer() != null && requestHeader == null) {\n        requestHeader = getValue(RequestHeaderProto.getDefaultInstance());\n      }\n      return requestHeader;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getProtocolImpl": "      private static ProtoClassProtoImpl getProtocolImpl(RPC.Server server,\n          String protoName, long clientVersion) throws RpcServerException {\n        ProtoNameVer pv = new ProtoNameVer(protoName, clientVersion);\n        ProtoClassProtoImpl impl = \n            server.getProtocolImplMap(RPC.RpcKind.RPC_PROTOCOL_BUFFER).get(pv);\n        if (impl == null) { // no match for Protocol AND Version\n          VerProtocolImpl highest = \n              server.getHighestSupportedProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER, \n                  protoName);\n          if (highest == null) {\n            throw new RpcNoSuchProtocolException(\n                \"Unknown protocol: \" + protoName);\n          }\n          // protocol supported but not the version that client wants\n          throw new RPC.VersionMismatch(protoName, clientVersion,\n              highest.version);\n        }\n        return impl;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.call": "    public Writable call(RPC.RpcKind rpcKind, String protocol,\n        Writable rpcRequest, long receiveTime) throws Exception {\n      return getRpcInvoker(rpcKind).call(this, protocol, rpcRequest,\n          receiveTime);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(RpcCall call,\n      RpcResponseHeaderProto header, Writable rv) throws IOException {\n    final byte[] response;\n    if (rv == null || (rv instanceof RpcWritable.ProtobufWrapper)) {\n      response = setupResponseForProtobuf(header, rv);\n    } else {\n      response = setupResponseForWritable(header, rv);\n    }\n    if (response.length > maxRespSize) {\n      LOG.warn(\"Large response size \" + response.length + \" for call \"\n          + call.toString());\n    }\n    call.setResponse(ByteBuffer.wrap(response));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.isResponseDeferred": "    public boolean isResponseDeferred() {\n      return this.deferredResponse;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.remove": "    private boolean remove(Connection connection) {\n      boolean removed = connections.remove(connection);\n      if (removed) {\n        count.getAndDecrement();\n      }\n      return removed;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeAll": "    void closeAll() {\n      // use a copy of the connections to be absolutely sure the concurrent\n      // iterator doesn't miss a connection\n      for (Connection connection : toArray()) {\n        close(connection);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(RPC.RpcKind rpcKind, String protocol,\n      Writable param, long receiveTime) throws Exception;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param protocolName - the protocol\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  private void authorize(UserGroupInformation user, String protocolName,\n      InetAddress addr) throws AuthorizationException {\n    if (authorize) {\n      if (protocolName == null) {\n        throw new AuthorizationException(\"Null protocol not authorized\");\n      }\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(protocolName, getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         protocolName);\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    boolean close(Connection connection) {\n      boolean exists = remove(connection);\n      if (exists) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(Thread.currentThread().getName() +\n              \": disconnecting client \" + connection +\n              \". Number of active connections: \"+ size());\n        }\n        // only close if actually removed to avoid double-closing due\n        // to possible races\n        connection.close();\n        // Remove authorized users only\n        if (connection.user != null && connection.connectionContextRead) {\n          decrUserConnections(connection.user.getShortUserName());\n        }\n      }\n      return exists;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isWritable()) {\n                doAsyncWrite(key);\n              }\n            } catch (CancelledKeyException cke) {\n              // something else closed the connection, ex. reader or the\n              // listener doing an idle scan.  ignore it and let them clean\n              // up\n              RpcCall call = (RpcCall)key.attachment();\n              if (call != null) {\n                LOG.info(Thread.currentThread().getName() +\n                    \": connection aborted from \" + call.connection);\n              }\n            } catch (IOException e) {\n              LOG.info(Thread.currentThread().getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = Time.now();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<RpcCall> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<RpcCall>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              RpcCall call = (RpcCall)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n\n          for (RpcCall call : calls) {\n            doPurge(call, now);\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeIdle": "    synchronized void closeIdle(boolean scanAll) {\n      long minLastContact = Time.now() - maxIdleTime;\n      // concurrent iterator might miss new connections added\n      // during the iteration, but that's ok because they won't\n      // be idle yet anyway and will be caught on next scan\n      int closed = 0;\n      for (Connection connection : connections) {\n        // stop if connections dropped below threshold unless scanning all\n        if (!scanAll && size() < idleScanThreshold) {\n          break;\n        }\n        // stop if not scanning all and max connections are closed\n        if (connection.isIdle() &&\n            connection.getLastContact() < minLastContact &&\n            close(connection) &&\n            !scanAll && (++closed == maxIdleToClose)) {\n          break;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.startIdleScan": "    void startIdleScan() {\n      scheduleIdleScanTask();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.sendResponse": "    private void sendResponse(RpcCall call) throws IOException {\n      responder.doRespond(call);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.stopIdleScan": "    void stopIdleScan() {\n      idleScanTimer.cancel();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws InterruptedException, IOException,  OutOfMemoryError {\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        channel.socket().setKeepAlive(true);\n        \n        Reader reader = getReader();\n        Connection c = connectionManager.register(channel);\n        // If the connectionManager can't take it, close the connection.\n        if (c == null) {\n          if (channel.isOpen()) {\n            IOUtils.cleanup(null, channel);\n          }\n          connectionManager.droppedConnections.getAndIncrement();\n          continue;\n        }\n        key.attach(c);  // so closeCurrentConnection can get the object\n        reader.addConnection(c);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.scheduleIdleScanTask": "    private void scheduleIdleScanTask() {\n      if (!running) {\n        return;\n      }\n      TimerTask idleScanTask = new TimerTask(){\n        @Override\n        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }\n      };\n      idleScanTimer.schedule(idleScanTask, idleScanInterval);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getRemoteUser": "    public UserGroupInformation getRemoteUser() {\n      return connection.user;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.populateResponseParamsOnError": "    private void populateResponseParamsOnError(Throwable t,\n                                               ResponseParams responseParams) {\n      if (t instanceof UndeclaredThrowableException) {\n        t = t.getCause();\n      }\n      logException(Server.LOG, t, this);\n      if (t instanceof RpcServerException) {\n        RpcServerException rse = ((RpcServerException) t);\n        responseParams.returnStatus = rse.getRpcStatusProto();\n        responseParams.detailedErr = rse.getRpcErrorCodeProto();\n      } else {\n        responseParams.returnStatus = RpcStatusProto.ERROR;\n        responseParams.detailedErr = RpcErrorCodeProto.ERROR_APPLICATION;\n      }\n      responseParams.errorClass = t.getClass().getName();\n      responseParams.error = StringUtils.stringifyException(t);\n      // Remove redundant error class name from the beginning of the\n      // stack trace\n      String exceptionHdr = responseParams.errorClass + \": \";\n      if (responseParams.error.startsWith(exceptionHdr)) {\n        responseParams.error =\n            responseParams.error.substring(exceptionHdr.length());\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause == null) {\n        throw new RuntimeException(\"PrivilegedActionException with no \" +\n                \"underlying cause. UGI [\" + this + \"]\" +\": \" + pae, pae);\n      } else if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.call": "  Writable call(RPC.RpcKind rpcKind, Writable rpcRequest,\n      ConnectionId remoteId, int serviceClass,\n      AtomicBoolean fallbackToSimpleAuth) throws IOException {\n    final Call call = createCall(rpcKind, rpcRequest);\n    final Connection connection = getConnection(remoteId, call, serviceClass,\n        fallbackToSimpleAuth);\n\n    try {\n      checkAsyncCall();\n      try {\n        connection.sendRpcRequest(call);                 // send the rpc request\n      } catch (RejectedExecutionException e) {\n        throw new IOException(\"connection has been closed\", e);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n        LOG.warn(\"interrupted waiting to send rpc request to server\", e);\n        throw new IOException(e);\n      }\n    } catch(Exception e) {\n      if (isAsynchronousMode()) {\n        releaseAsyncCall();\n      }\n      throw e;\n    }\n\n    if (isAsynchronousMode()) {\n      final AsyncGet<Writable, IOException> asyncGet\n          = new AsyncGet<Writable, IOException>() {\n        @Override\n        public Writable get(long timeout, TimeUnit unit)\n            throws IOException, TimeoutException{\n          boolean done = true;\n          try {\n            final Writable w = getRpcResponse(call, connection, timeout, unit);\n            if (w == null) {\n              done = false;\n              throw new TimeoutException(call + \" timed out \"\n                  + timeout + \" \" + unit);\n            }\n            return w;\n          } finally {\n            if (done) {\n              releaseAsyncCall();\n            }\n          }\n        }\n\n        @Override\n        public boolean isDone() {\n          synchronized (call) {\n            return call.done;\n          }\n        }\n      };\n\n      ASYNC_RPC_RESPONSE.set(asyncGet);\n      return null;\n    } else {\n      return getRpcResponse(call, connection, -1, null);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.sendRpcRequest": "    public void sendRpcRequest(final Call call)\n        throws InterruptedException, IOException {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n\n      // Serialize the call to be sent. This is done from the actual\n      // caller thread, rather than the sendParamsExecutor thread,\n      \n      // so that if the serialization throws an error, it is reported\n      // properly. This also parallelizes the serialization.\n      //\n      // Format of a call on the wire:\n      // 0) Length of rest below (1 + 2)\n      // 1) RpcRequestHeader  - is serialized Delimited hence contains length\n      // 2) RpcRequest\n      //\n      // Items '1' and '2' are prepared here. \n      RpcRequestHeaderProto header = ProtoUtil.makeRpcRequestHeader(\n          call.rpcKind, OperationProto.RPC_FINAL_PACKET, call.id, call.retry,\n          clientId);\n\n      final ResponseBuffer buf = new ResponseBuffer();\n      header.writeDelimitedTo(buf);\n      RpcWritable.wrap(call.rpcRequest).writeTo(buf);\n\n      synchronized (sendRpcRequestLock) {\n        Future<?> senderFuture = sendParamsExecutor.submit(new Runnable() {\n          @Override\n          public void run() {\n            try {\n              synchronized (ipcStreams.out) {\n                if (shouldCloseConnection.get()) {\n                  return;\n                }\n                if (LOG.isDebugEnabled()) {\n                  LOG.debug(getName() + \" sending #\" + call.id\n                      + \" \" + call.rpcRequest);\n                }\n                // RpcRequestHeader + RpcRequest\n                ipcStreams.sendRequest(buf.toByteArray());\n                ipcStreams.flush();\n              }\n            } catch (IOException e) {\n              // exception at this point would leave the connection in an\n              // unrecoverable state (eg half a call left on the wire).\n              // So, close the connection, killing any outstanding calls\n              markClosed(e);\n            } finally {\n              //the buffer is just an in-memory buffer, but it is still polite to\n              // close early\n              IOUtils.closeStream(buf);\n            }\n          }\n        });\n      \n        try {\n          senderFuture.get();\n        } catch (ExecutionException e) {\n          Throwable cause = e.getCause();\n          \n          // cause should only be a RuntimeException as the Runnable above\n          // catches IOException\n          if (cause instanceof RuntimeException) {\n            throw (RuntimeException) cause;\n          } else {\n            throw new RuntimeException(\"unexpected checked exception\", cause);\n          }\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.createCall": "  Call createCall(RPC.RpcKind rpcKind, Writable rpcRequest) {\n    return new Call(rpcKind, rpcRequest);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.checkAsyncCall": "  private void checkAsyncCall() throws IOException {\n    if (isAsynchronousMode()) {\n      if (asyncCallCounter.incrementAndGet() > maxAsyncCalls) {\n        asyncCallCounter.decrementAndGet();\n        String errMsg = String.format(\n            \"Exceeded limit of max asynchronous calls: %d, \" +\n            \"please configure %s to adjust it.\",\n            maxAsyncCalls,\n            CommonConfigurationKeys.IPC_CLIENT_ASYNC_CALLS_MAX_KEY);\n        throw new AsyncCallLimitExceededException(errMsg);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.isAsynchronousMode": "  public static boolean isAsynchronousMode() {\n    return asynchronousMode.get();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRpcResponse": "  private Writable getRpcResponse(final Call call, final Connection connection,\n      final long timeout, final TimeUnit unit) throws IOException {\n    synchronized (call) {\n      while (!call.done) {\n        try {\n          AsyncGet.Util.wait(call, timeout, unit);\n          if (timeout >= 0 && !call.done) {\n            return null;\n          }\n        } catch (InterruptedException ie) {\n          Thread.currentThread().interrupt();\n          throw new InterruptedIOException(\"Call interrupted\");\n        }\n      }\n\n      if (call.error != null) {\n        if (call.error instanceof RemoteException) {\n          call.error.fillInStackTrace();\n          throw call.error;\n        } else { // local exception\n          InetSocketAddress address = connection.getRemoteAddress();\n          throw NetUtils.wrapException(address.getHostName(),\n                  address.getPort(),\n                  NetUtils.getHostname(),\n                  0,\n                  call.error);\n        }\n      } else {\n        return call.getRpcResponse();\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnection": "  private Connection getConnection(ConnectionId remoteId,\n      Call call, int serviceClass, AtomicBoolean fallbackToSimpleAuth)\n      throws IOException {\n    if (!running.get()) {\n      // the client is stopped\n      throw new IOException(\"The client is stopped\");\n    }\n    Connection connection;\n    /* we could avoid this allocation for each RPC by having a  \n     * connectionsId object and with set() method. We need to manage the\n     * refs for keys in HashMap properly. For now its ok.\n     */\n    while (true) {\n      // These lines below can be shorten with computeIfAbsent in Java8\n      connection = connections.get(remoteId);\n      if (connection == null) {\n        connection = new Connection(remoteId, serviceClass);\n        Connection existing = connections.putIfAbsent(remoteId, connection);\n        if (existing != null) {\n          connection = existing;\n        }\n      }\n\n      if (connection.addCall(call)) {\n        break;\n      } else {\n        // This connection is closed, should be removed. But other thread could\n        // have already known this closedConnection, and replace it with a new\n        // connection. So we should call conditional remove to make sure we only\n        // remove this closedConnection.\n        connections.remove(remoteId, connection);\n      }\n    }\n\n    // If the server happens to be slow, the method below will take longer to\n    // establish a connection.\n    connection.setupIOstreams(fallbackToSimpleAuth);\n    return connection;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.releaseAsyncCall": "  private void releaseAsyncCall() {\n    asyncCallCounter.decrementAndGet();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.invoke": "    public Message invoke(Object proxy, final Method method, Object[] args)\n        throws ServiceException {\n      long startTime = 0;\n      if (LOG.isDebugEnabled()) {\n        startTime = Time.now();\n      }\n      \n      if (args.length != 2) { // RpcController + Message\n        throw new ServiceException(\n            \"Too many or few parameters for request. Method: [\"\n            + method.getName() + \"]\" + \", Expected: 2, Actual: \"\n            + args.length);\n      }\n      if (args[1] == null) {\n        throw new ServiceException(\"null param while calling Method: [\"\n            + method.getName() + \"]\");\n      }\n\n      // if Tracing is on then start a new span for this rpc.\n      // guard it in the if statement to make sure there isn't\n      // any extra string manipulation.\n      Tracer tracer = Tracer.curThreadTracer();\n      TraceScope traceScope = null;\n      if (tracer != null) {\n        traceScope = tracer.newScope(RpcClientUtil.methodToTraceString(method));\n      }\n\n      RequestHeaderProto rpcRequestHeader = constructRpcRequestHeader(method);\n      \n      if (LOG.isTraceEnabled()) {\n        LOG.trace(Thread.currentThread().getId() + \": Call -> \" +\n            remoteId + \": \" + method.getName() +\n            \" {\" + TextFormat.shortDebugString((Message) args[1]) + \"}\");\n      }\n\n\n      final Message theRequest = (Message) args[1];\n      final RpcWritable.Buffer val;\n      try {\n        val = (RpcWritable.Buffer) client.call(RPC.RpcKind.RPC_PROTOCOL_BUFFER,\n            new RpcProtobufRequest(rpcRequestHeader, theRequest), remoteId,\n            fallbackToSimpleAuth);\n\n      } catch (Throwable e) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Exception <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + e + \"}\");\n        }\n        if (traceScope != null) {\n          traceScope.addTimelineAnnotation(\"Call got exception: \" +\n              e.toString());\n        }\n        throw new ServiceException(e);\n      } finally {\n        if (traceScope != null) traceScope.close();\n      }\n\n      if (LOG.isDebugEnabled()) {\n        long callTime = Time.now() - startTime;\n        LOG.debug(\"Call: \" + method.getName() + \" took \" + callTime + \"ms\");\n      }\n      \n      if (Client.isAsynchronousMode()) {\n        final AsyncGet<RpcWritable.Buffer, IOException> arr\n            = Client.getAsyncRpcResponse();\n        final AsyncGet<Message, Exception> asyncGet\n            = new AsyncGet<Message, Exception>() {\n          @Override\n          public Message get(long timeout, TimeUnit unit) throws Exception {\n            return getReturnMessage(method, arr.get(timeout, unit));\n          }\n\n          @Override\n          public boolean isDone() {\n            return arr.isDone();\n          }\n        };\n        ASYNC_RETURN_MESSAGE.set(asyncGet);\n        return null;\n      } else {\n        return getReturnMessage(method, val);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.toString": "    public String toString() {\n      try {\n        RequestHeaderProto header = getRequestHeader();\n        return header.getDeclaringClassProtocolName() + \".\" +\n               header.getMethodName();\n      } catch (IOException e) {\n        throw new IllegalArgumentException(e);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.constructRpcRequestHeader": "    private RequestHeaderProto constructRpcRequestHeader(Method method) {\n      RequestHeaderProto.Builder builder = RequestHeaderProto\n          .newBuilder();\n      builder.setMethodName(method.getName());\n     \n\n      // For protobuf, {@code protocol} used when creating client side proxy is\n      // the interface extending BlockingInterface, which has the annotations \n      // such as ProtocolName etc.\n      //\n      // Using Method.getDeclaringClass(), as in WritableEngine to get at\n      // the protocol interface will return BlockingInterface, from where \n      // the annotation ProtocolName and Version cannot be\n      // obtained.\n      //\n      // Hence we simply use the protocol class used to create the proxy.\n      // For PB this may limit the use of mixins on client side.\n      builder.setDeclaringClassProtocolName(protocolName);\n      builder.setClientProtocolVersion(clientProtocolVersion);\n      return builder.build();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.close": "    public void close() throws IOException {\n      if (!isClosed) {\n        isClosed = true;\n        CLIENTS.stopClient(client);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.isDone": "          public boolean isDone() {\n            return arr.isDone();\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getReturnMessage": "    private Message getReturnMessage(final Method method,\n        final RpcWritable.Buffer buf) throws ServiceException {\n      Message prototype = null;\n      try {\n        prototype = getReturnProtoType(method);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      Message returnMessage;\n      try {\n        returnMessage = buf.getValue(prototype.getDefaultInstanceForType());\n\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Response <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + TextFormat.shortDebugString(returnMessage) + \"}\");\n        }\n\n      } catch (Throwable e) {\n        throw new ServiceException(e);\n      }\n      return returnMessage;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod": "  protected Object invokeMethod(Method method, Object[] args) throws Throwable {\n    try {\n      if (!method.isAccessible()) {\n        method.setAccessible(true);\n      }\n      final Object r = method.invoke(proxyDescriptor.getProxy(), args);\n      hasSuccessfulCall = true;\n      return r;\n    } catch (InvocationTargetException e) {\n      throw e.getCause();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.getProxy": "    synchronized T getProxy() {\n      return proxyInfo.proxy;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invoke": "  public Object invoke(Object proxy, Method method, Object[] args)\n      throws Throwable {\n    final boolean isRpc = isRpcInvocation(proxyDescriptor.getProxy());\n    final int callId = isRpc? Client.nextCallId(): RpcConstants.INVALID_CALL_ID;\n\n    final Call call = newCall(method, args, isRpc, callId);\n    while (true) {\n      final CallReturn c = call.invokeOnce();\n      final CallReturn.State state = c.getState();\n      if (state == CallReturn.State.ASYNC_INVOKED) {\n        return null; // return null for async calls\n      } else if (c.getState() != CallReturn.State.RETRY) {\n        return c.getReturnValue();\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.length": "  public int length() {\n    return inodes.length;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getPath": "  public String getPath(int pos) {\n    return DFSUtil.byteArray2PathString(path, 0, pos + 1); // it's a length...\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.QuotaCounts.negation": "  public QuotaCounts negation() {\n    QuotaCounts ret = new QuotaCounts.Builder().quotaCount(this).build();\n    ret.nsSsCounts.negation();\n    ret.tsCounts.negation();\n    return ret;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.QuotaCounts.build": "    public QuotaCounts build() {\n      return new QuotaCounts(this);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.QuotaCounts.quotaCount": "    public Builder quotaCount(QuotaCounts that) {\n      this.nsSsCounts.set(that.nsSsCounts);\n      this.tsCounts.set(that.tsCounts);\n      return this;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.fromINode": "  static INodesInPath fromINode(final INodeDirectory rootDir, INode inode) {\n    byte[][] paths = getPaths(getINodes(inode));\n    return resolve(rootDir, paths);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getPaths": "  private static byte[][] getPaths(final INode[] inodes) {\n    byte[][] paths = new byte[inodes.length][];\n    for (int i = 0; i < inodes.length; i++) {\n      paths[i] = inodes[i].getKey();\n    }\n    return paths;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.resolve": "  static INodesInPath resolve(final INodeDirectory startingDir,\n      byte[][] components, final boolean isRaw) {\n    Preconditions.checkArgument(startingDir.compareTo(components[0]) == 0);\n\n    INode curNode = startingDir;\n    int count = 0;\n    int inodeNum = 0;\n    INode[] inodes = new INode[components.length];\n    boolean isSnapshot = false;\n    int snapshotId = CURRENT_STATE_ID;\n\n    while (count < components.length && curNode != null) {\n      final boolean lastComp = (count == components.length - 1);\n      inodes[inodeNum++] = curNode;\n      final boolean isRef = curNode.isReference();\n      final boolean isDir = curNode.isDirectory();\n      final INodeDirectory dir = isDir? curNode.asDirectory(): null;\n      if (!isRef && isDir && dir.isWithSnapshot()) {\n        //if the path is a non-snapshot path, update the latest snapshot.\n        if (!isSnapshot && shouldUpdateLatestId(\n            dir.getDirectoryWithSnapshotFeature().getLastSnapshotId(),\n            snapshotId)) {\n          snapshotId = dir.getDirectoryWithSnapshotFeature().getLastSnapshotId();\n        }\n      } else if (isRef && isDir && !lastComp) {\n        // If the curNode is a reference node, need to check its dstSnapshot:\n        // 1. if the existing snapshot is no later than the dstSnapshot (which\n        // is the latest snapshot in dst before the rename), the changes \n        // should be recorded in previous snapshots (belonging to src).\n        // 2. however, if the ref node is already the last component, we still \n        // need to know the latest snapshot among the ref node's ancestors, \n        // in case of processing a deletion operation. Thus we do not overwrite\n        // the latest snapshot if lastComp is true. In case of the operation is\n        // a modification operation, we do a similar check in corresponding \n        // recordModification method.\n        if (!isSnapshot) {\n          int dstSnapshotId = curNode.asReference().getDstSnapshotId();\n          if (snapshotId == CURRENT_STATE_ID || // no snapshot in dst tree of rename\n              (dstSnapshotId != CURRENT_STATE_ID &&\n               dstSnapshotId >= snapshotId)) { // the above scenario\n            int lastSnapshot = CURRENT_STATE_ID;\n            DirectoryWithSnapshotFeature sf;\n            if (curNode.isDirectory() && \n                (sf = curNode.asDirectory().getDirectoryWithSnapshotFeature()) != null) {\n              lastSnapshot = sf.getLastSnapshotId();\n            }\n            snapshotId = lastSnapshot;\n          }\n        }\n      }\n      if (lastComp || !isDir) {\n        break;\n      }\n\n      final byte[] childName = components[++count];\n      // check if the next byte[] in components is for \".snapshot\"\n      if (isDotSnapshotDir(childName) && dir.isSnapshottable()) {\n        isSnapshot = true;\n        // check if \".snapshot\" is the last element of components\n        if (count == components.length - 1) {\n          break;\n        }\n        // Resolve snapshot root\n        final Snapshot s = dir.getSnapshot(components[count + 1]);\n        if (s == null) {\n          curNode = null; // snapshot not found\n        } else {\n          curNode = s.getRoot();\n          snapshotId = s.getId();\n        }\n        // combine .snapshot & name into 1 component element to ensure\n        // 1-to-1 correspondence between components and inodes arrays is\n        // preserved so a path can be reconstructed.\n        byte[][] componentsCopy =\n            Arrays.copyOf(components, components.length - 1);\n        componentsCopy[count] = DFSUtil.string2Bytes(\n            DFSUtil.byteArray2PathString(components, count, 2));\n        // shift the remaining components after snapshot name\n        int start = count + 2;\n        System.arraycopy(components, start, componentsCopy, count + 1,\n            components.length - start);\n        components = componentsCopy;\n        // reduce the inodes array to compensate for reduction in components\n        inodes = Arrays.copyOf(inodes, components.length);\n      } else {\n        // normal case, and also for resolving file/dir under snapshot root\n        curNode = dir.getChild(childName,\n            isSnapshot ? snapshotId : CURRENT_STATE_ID);\n      }\n    }\n    return new INodesInPath(inodes, components, isRaw, isSnapshot, snapshotId);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getINodes": "  private static INode[] getINodes(final INode inode) {\n    int depth = 0, index;\n    INode tmp = inode;\n    while (tmp != null) {\n      depth++;\n      tmp = tmp.getParent();\n    }\n    INode[] inodes = new INode[depth];\n    tmp = inode;\n    index = depth;\n    while (tmp != null) {\n      index--;\n      inodes[index] = tmp;\n      tmp = tmp.getParent();\n    }\n    return inodes;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.getStoragePolicyID": "  public byte getStoragePolicyID() {\n    byte id = getLocalStoragePolicyID();\n    if (id == BLOCK_STORAGE_POLICY_ID_UNSPECIFIED) {\n      id = this.getParent() != null ?\n          this.getParent().getStoragePolicyID() : id;\n    }\n\n    // For Striped EC files, we support only suitable policies. Current\n    // supported policies are HOT, COLD, ALL_SSD.\n    // If the file was set with any other policies, then we just treat policy as\n    // BLOCK_STORAGE_POLICY_ID_UNSPECIFIED.\n    if (isStriped() && id != BLOCK_STORAGE_POLICY_ID_UNSPECIFIED\n        && !ErasureCodingPolicyManager\n            .checkStoragePolicySuitableForECStripedMode(id)) {\n      id = HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"The current effective storage policy id : \" + id\n            + \" is not suitable for striped mode EC file : \" + getName()\n            + \". So, just returning unspecified storage policy id\");\n      }\n    }\n\n    return id;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.getName": "  public String getName() {\n    // Get the full path name of this inode.\n    return getFullPathName();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.isStriped": "  public boolean isStriped() {\n    return HeaderFormat.isStriped(header);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.getLocalStoragePolicyID": "  public byte getLocalStoragePolicyID() {\n    return HeaderFormat.getStoragePolicyID(header);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getLastINode": "  public INode getLastINode() {\n    return getINode(-1);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getINode": "  public INode getINode(int i) {\n    return inodes[(i < 0) ? inodes.length + i : i];\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeDirectory.getDirectoryWithQuotaFeature": "  public final DirectoryWithQuotaFeature getDirectoryWithQuotaFeature() {\n    return getFeature(DirectoryWithQuotaFeature.class);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.storeAllocatedBlock": "  static LocatedBlock storeAllocatedBlock(FSNamesystem fsn, String src,\n      long fileId, String clientName, ExtendedBlock previous,\n      DatanodeStorageInfo[] targets) throws IOException {\n    long offset;\n    // Run the full analysis again, since things could have changed\n    // while chooseTarget() was executing.\n    LocatedBlock[] onRetryBlock = new LocatedBlock[1];\n    INodesInPath iip = fsn.dir.resolvePath(null, src, fileId);\n    FileState fileState = analyzeFileState(fsn, iip, fileId, clientName,\n                                           previous, onRetryBlock);\n    final INodeFile pendingFile = fileState.inode;\n    src = fileState.path;\n\n    if (onRetryBlock[0] != null) {\n      if (onRetryBlock[0].getLocations().length > 0) {\n        // This is a retry. Just return the last block if having locations.\n        return onRetryBlock[0];\n      } else {\n        // add new chosen targets to already allocated block and return\n        BlockInfo lastBlockInFile = pendingFile.getLastBlock();\n        lastBlockInFile.getUnderConstructionFeature().setExpectedLocations(\n            lastBlockInFile, targets, pendingFile.getBlockType());\n        offset = pendingFile.computeFileSize();\n        return makeLocatedBlock(fsn, lastBlockInFile, targets, offset);\n      }\n    }\n\n    // commit the last block and complete it if it has minimum replicas\n    fsn.commitOrCompleteLastBlock(pendingFile, fileState.iip,\n                                  ExtendedBlock.getLocalBlock(previous));\n\n    // allocate new block, record block locations in INode.\n    final BlockType blockType = pendingFile.getBlockType();\n    // allocate new block, record block locations in INode.\n    Block newBlock = fsn.createNewBlock(blockType);\n    INodesInPath inodesInPath = INodesInPath.fromINode(pendingFile);\n    saveAllocatedBlock(fsn, src, inodesInPath, newBlock, targets, blockType);\n\n    persistNewBlock(fsn, src, pendingFile);\n    offset = pendingFile.computeFileSize();\n\n    // Return located block\n    return makeLocatedBlock(fsn, fsn.getStoredBlock(newBlock), targets, offset);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.makeLocatedBlock": "  static LocatedBlock makeLocatedBlock(FSNamesystem fsn, BlockInfo blk,\n      DatanodeStorageInfo[] locs, long offset) throws IOException {\n    LocatedBlock lBlk = BlockManager.newLocatedBlock(\n        fsn.getExtendedBlock(new Block(blk)), blk, locs, offset);\n    fsn.getBlockManager().setBlockToken(lBlk,\n        BlockTokenIdentifier.AccessMode.WRITE);\n    return lBlk;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.persistNewBlock": "  private static void persistNewBlock(\n      FSNamesystem fsn, String path, INodeFile file) {\n    Preconditions.checkArgument(file.isUnderConstruction());\n    fsn.getEditLog().logAddBlock(path, file);\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"persistNewBlock: \"\n              + path + \" with new block \" + file.getLastBlock().toString()\n              + \", current total block count is \" + file.getBlocks().length);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.saveAllocatedBlock": "  private static void saveAllocatedBlock(FSNamesystem fsn, String src,\n      INodesInPath inodesInPath, Block newBlock, DatanodeStorageInfo[] targets,\n      BlockType blockType) throws IOException {\n    assert fsn.hasWriteLock();\n    BlockInfo b = addBlock(fsn.dir, src, inodesInPath, newBlock, targets,\n        blockType);\n    logAllocatedBlock(src, b);\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState": "  private static FileState analyzeFileState(\n      FSNamesystem fsn, INodesInPath iip, long fileId, String clientName,\n      ExtendedBlock previous, LocatedBlock[] onRetryBlock)\n      throws IOException {\n    assert fsn.hasReadLock();\n    String src = iip.getPath();\n    checkBlock(fsn, previous);\n    onRetryBlock[0] = null;\n    fsn.checkNameNodeSafeMode(\"Cannot add block to \" + src);\n\n    // have we exceeded the configured limit of fs objects.\n    fsn.checkFsObjectLimit();\n\n    Block previousBlock = ExtendedBlock.getLocalBlock(previous);\n    final INodeFile file = fsn.checkLease(iip, clientName, fileId);\n    BlockInfo lastBlockInFile = file.getLastBlock();\n    if (!Block.matchingIdAndGenStamp(previousBlock, lastBlockInFile)) {\n      // The block that the client claims is the current last block\n      // doesn't match up with what we think is the last block. There are\n      // four possibilities:\n      // 1) This is the first block allocation of an append() pipeline\n      //    which started appending exactly at or exceeding the block boundary.\n      //    In this case, the client isn't passed the previous block,\n      //    so it makes the allocateBlock() call with previous=null.\n      //    We can distinguish this since the last block of the file\n      //    will be exactly a full block.\n      // 2) This is a retry from a client that missed the response of a\n      //    prior getAdditionalBlock() call, perhaps because of a network\n      //    timeout, or because of an HA failover. In that case, we know\n      //    by the fact that the client is re-issuing the RPC that it\n      //    never began to write to the old block. Hence it is safe to\n      //    to return the existing block.\n      // 3) This is an entirely bogus request/bug -- we should error out\n      //    rather than potentially appending a new block with an empty\n      //    one in the middle, etc\n      // 4) This is a retry from a client that timed out while\n      //    the prior getAdditionalBlock() is still being processed,\n      //    currently working on chooseTarget().\n      //    There are no means to distinguish between the first and\n      //    the second attempts in Part I, because the first one hasn't\n      //    changed the namesystem state yet.\n      //    We run this analysis again in Part II where case 4 is impossible.\n\n      BlockInfo penultimateBlock = file.getPenultimateBlock();\n      if (previous == null &&\n          lastBlockInFile != null &&\n          lastBlockInFile.getNumBytes() >= file.getPreferredBlockSize() &&\n          lastBlockInFile.isComplete()) {\n        // Case 1\n        if (NameNode.stateChangeLog.isDebugEnabled()) {\n           NameNode.stateChangeLog.debug(\n               \"BLOCK* NameSystem.allocateBlock: handling block allocation\" +\n               \" writing to a file with a complete previous block: src=\" +\n               src + \" lastBlock=\" + lastBlockInFile);\n        }\n      } else if (Block.matchingIdAndGenStamp(penultimateBlock, previousBlock)) {\n        if (lastBlockInFile.getNumBytes() != 0) {\n          throw new IOException(\n              \"Request looked like a retry to allocate block \" +\n              lastBlockInFile + \" but it already contains \" +\n              lastBlockInFile.getNumBytes() + \" bytes\");\n        }\n\n        // Case 2\n        // Return the last block.\n        NameNode.stateChangeLog.info(\"BLOCK* allocateBlock: caught retry for \" +\n            \"allocation of a new block in \" + src + \". Returning previously\" +\n            \" allocated block \" + lastBlockInFile);\n        long offset = file.computeFileSize();\n        BlockUnderConstructionFeature uc =\n            lastBlockInFile.getUnderConstructionFeature();\n        onRetryBlock[0] = makeLocatedBlock(fsn, lastBlockInFile,\n            uc.getExpectedStorageLocations(), offset);\n        return new FileState(file, src, iip);\n      } else {\n        // Case 3\n        throw new IOException(\"Cannot allocate block in \" + src + \": \" +\n            \"passed 'previous' block \" + previous + \" does not match actual \" +\n            \"last block in file \" + lastBlockInFile);\n      }\n    }\n    return new FileState(file, src, iip);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock": "  static ValidateAddBlockResult validateAddBlock(\n      FSNamesystem fsn, FSPermissionChecker pc,\n      String src, long fileId, String clientName,\n      ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n    final long blockSize;\n    final short numTargets;\n    final byte storagePolicyID;\n    String clientMachine;\n    final BlockType blockType;\n\n    INodesInPath iip = fsn.dir.resolvePath(pc, src, fileId);\n    FileState fileState = analyzeFileState(fsn, iip, fileId, clientName,\n                                           previous, onRetryBlock);\n    if (onRetryBlock[0] != null && onRetryBlock[0].getLocations().length > 0) {\n      // This is a retry. No need to generate new locations.\n      // Use the last block if it has locations.\n      return null;\n    }\n\n    final INodeFile pendingFile = fileState.inode;\n    if (!fsn.checkFileProgress(src, pendingFile, false)) {\n      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n    }\n    if (pendingFile.getBlocks().length >= fsn.maxBlocksPerFile) {\n      throw new IOException(\"File has reached the limit on maximum number of\"\n          + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n          + \"): \" + pendingFile.getBlocks().length + \" >= \"\n          + fsn.maxBlocksPerFile);\n    }\n    blockSize = pendingFile.getPreferredBlockSize();\n    clientMachine = pendingFile.getFileUnderConstructionFeature()\n        .getClientMachine();\n    blockType = pendingFile.getBlockType();\n    ErasureCodingPolicy ecPolicy = null;\n    if (blockType == BlockType.STRIPED) {\n      ecPolicy =\n          FSDirErasureCodingOp.unprotectedGetErasureCodingPolicy(fsn, iip);\n      numTargets = (short) (ecPolicy.getSchema().getNumDataUnits()\n          + ecPolicy.getSchema().getNumParityUnits());\n    } else {\n      numTargets = pendingFile.getFileReplication();\n    }\n    storagePolicyID = pendingFile.getStoragePolicyID();\n    return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n                                      clientMachine, blockType, ecPolicy);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock": "  static DatanodeStorageInfo[] chooseTargetForNewBlock(\n      BlockManager bm, String src, DatanodeInfo[] excludedNodes,\n      String[] favoredNodes, EnumSet<AddBlockFlag> flags,\n      ValidateAddBlockResult r) throws IOException {\n    Node clientNode = bm.getDatanodeManager()\n        .getDatanodeByHost(r.clientMachine);\n    if (clientNode == null) {\n      clientNode = getClientNode(bm, r.clientMachine);\n    }\n\n    Set<Node> excludedNodesSet = null;\n    if (excludedNodes != null) {\n      excludedNodesSet = new HashSet<>(excludedNodes.length);\n      Collections.addAll(excludedNodesSet, excludedNodes);\n    }\n    List<String> favoredNodesList = (favoredNodes == null) ? null\n        : Arrays.asList(favoredNodes);\n    // choose targets for the new block to be allocated.\n    return bm.chooseTarget4NewBlock(src, r.numTargets, clientNode,\n                                    excludedNodesSet, r.blockSize,\n                                    favoredNodesList, r.storagePolicyID,\n                                    r.blockType, r.ecPolicy, flags);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.getClientNode": "  static Node getClientNode(BlockManager bm, String clientMachine) {\n    List<String> hosts = new ArrayList<>(1);\n    hosts.add(clientMachine);\n    List<String> rName = bm.getDatanodeManager()\n        .resolveNetworkLocation(hosts);\n    Node clientNode = null;\n    if (rName != null) {\n      // Able to resolve clientMachine mapping.\n      // Create a temp node to findout the rack local nodes\n      clientNode = new NodeBase(rName.get(0) + NodeBase.PATH_SEPARATOR_STR\n          + clientMachine);\n    }\n    return clientNode;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getCurCall": "  public static ThreadLocal<Call> getCurCall() {\n    return CurCall;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcWritable.wrap": "    public static Buffer wrap(ByteBuffer bb) {\n      return new Buffer(bb);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.updateMetrics": "  void updateMetrics(String name, int queueTime, int processingTime,\n                     boolean deferredCall) {\n    rpcMetrics.addRpcQueueTime(queueTime);\n    if (!deferredCall) {\n      rpcMetrics.addRpcProcessingTime(processingTime);\n      rpcDetailedMetrics.addProcessingTime(name, processingTime);\n      callQueue.addResponseTime(name, getPriorityLevel(), queueTime,\n          processingTime);\n      if (isLogSlowRPC()) {\n        logSlowRpcCalls(name, processingTime);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.logSlowRpcCalls": "  void logSlowRpcCalls(String methodName, int processingTime) {\n    final int deviation = 3;\n\n    // 1024 for minSampleSize just a guess -- not a number computed based on\n    // sample size analysis. It is chosen with the hope that this\n    // number is high enough to avoid spurious logging, yet useful\n    // in practice.\n    final int minSampleSize = 1024;\n    final double threeSigma = rpcMetrics.getProcessingMean() +\n        (rpcMetrics.getProcessingStdDev() * deviation);\n\n    if ((rpcMetrics.getProcessingSampleCount() > minSampleSize) &&\n        (processingTime > threeSigma)) {\n      if(LOG.isWarnEnabled()) {\n        String client = CurCall.get().toString();\n        LOG.warn(\n            \"Slow RPC : \" + methodName + \" took \" + processingTime +\n                \" milliseconds to process from client \" + client);\n      }\n      rpcMetrics.incrSlowRpc();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.getPriorityLevel": "    public int getPriorityLevel() {\n      return this.priorityLevel;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.isLogSlowRPC": "  protected boolean isLogSlowRPC() {\n    return logSlowRPC;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.CallerContext.setCurrent": "  public static void setCurrent(CallerContext callerContext) {\n    CurrentCallerContextHolder.CALLER_CONTEXT.set(callerContext);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getAsyncRpcResponse": "  public static <T extends Writable> AsyncGet<T, IOException>\n      getAsyncRpcResponse() {\n    return (AsyncGet<T, IOException>) ASYNC_RPC_RESPONSE.get();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.get": "        public Writable get(long timeout, TimeUnit unit)\n            throws IOException, TimeoutException{\n          boolean done = true;\n          try {\n            final Writable w = getRpcResponse(call, connection, timeout, unit);\n            if (w == null) {\n              done = false;\n              throw new TimeoutException(call + \" timed out \"\n                  + timeout + \" \" + unit);\n            }\n            return w;\n          } finally {\n            if (done) {\n              releaseAsyncCall();\n            }\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcClientUtil.methodToTraceString": "  public static String methodToTraceString(Method method) {\n    Class<?> clazz = method.getDeclaringClass();\n    while (true) {\n      Class<?> next = clazz.getEnclosingClass();\n      if (next == null || next.getEnclosingClass() == null) break;\n      clazz = next;\n    }\n    return clazz.getSimpleName() + \"#\" + method.getName();\n  }"
        },
        "bug_report": {
            "Title": "File not closed if streamer fail with DSQuotaExceededException",
            "Description": "\u00a0This is found during yarn log aggregation but theoretically could happen to any client.\r\n\r\nIf the dir's space quota is exceeded, the following would happen when a file is created:\r\n - client {{startFile}} rpc to NN, gets a {{DFSOutputStream}}.\r\n - writing to the stream would trigger the streamer to {{getAdditionalBlock}} rpc to NN, which would get the DSQuotaExceededException\r\n - client closes the stream\r\n \u00a0\r\n The fact that this would leave a 0-sized (or whatever size left in the quota) file in HDFS is beyond the scope of this jira. However, the file would be left in openforwrite status (shown in\u00a0{{fsck -openforwrite)}} at least, and could potentially leak leaseRenewer too.\r\n\r\nThis is because in the close implementation,\r\n # {{isClosed}} is first checked, and the close call will be a no-op if {{isClosed == true}}.\r\n # {{flushInternal}} checks {{isClosed}}, and throws the exception right away if\u00a0true\r\n\r\n{{isClosed}} does this: {{return closed || getStreamer().streamerClosed;}}\r\n\r\nWhen the disk quota is reached, {{getAdditionalBlock}} will throw when the streamer calls. Because the streamer runs in a separate thread, at the time the client calls close on the stream, the streamer may or may not have reached the Quota exception. If it has, then due to #1, the close call on the stream will be no-op. If it hasn't, then due to #2 the {{completeFile}} logic will be skipped.\r\n{code:java}\r\nprotected synchronized void closeImpl() throws IOException {\r\n    if (isClosed()) {\r\n      IOException e = lastException.getAndSet(null);\r\n      if (e == null)\r\n        return;\r\n      else\r\n        throw e;\r\n    }\r\n  try {\r\n    flushBuffer(); // flush from all upper layers\r\n    ...\r\n    flushInternal(); // flush all data to Datanodes\r\n\r\n    // get last block before destroying the streamer\r\n    ExtendedBlock lastBlock = getStreamer().getBlock();\r\n\r\n    try (TraceScope ignored =\r\n       dfsClient.getTracer().newScope(\"completeFile\")) {\r\n       completeFile(lastBlock);\r\n    }\r\n   } catch (ClosedChannelException ignored) {\r\n   } finally {\r\n     closeThreads(true);\r\n   }\r\n }\r\n\r\n\u00a0{code}\r\nLog snippets:\r\n{noformat}\r\n2018-02-16 15:59:32,916 DEBUG org.apache.hadoop.hdfs.DFSClient: DataStreamer Quota Exception\r\norg.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /DIR is exceeded: quota = 2000000 B = 1.91 MB but diskspace consumed = 404139552 B = 385.42 MB\r\n        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyDiskspaceQuota(DirectoryWithQuotaFeature.java:149)\r\n        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:159)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:2124)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1991)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1966)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:463)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3896)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3484)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:686)\r\n        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:217)\r\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:506)\r\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\r\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\r\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2226)\r\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2222)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\r\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2220)\r\n\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\r\n        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\r\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1833)\r\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1626)\r\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:788)\r\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.DSQuotaExceededException): The DiskSpace quota of /tmp/logs/systest/logs is exceeded: quota = 2000000 B = 1.91 MB but diskspace consumed = 404139552 B = 385.42 MB\r\n        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyDiskspaceQuota(DirectoryWithQuotaFeature.java:149)\r\n        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:159)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:2124)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1991)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1966)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:463)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3896)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3484)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:686)\r\n        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:217)\r\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:506)\r\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\r\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\r\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2226)\r\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2222)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\r\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2220)\r\n\r\n        at org.apache.hadoop.ipc.Client.call(Client.java:1504)\r\n        at org.apache.hadoop.ipc.Client.call(Client.java:1441)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)\r\n        at com.sun.proxy.$Proxy82.addBlock(Unknown Source)\r\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:423)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:258)\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)\r\n        at com.sun.proxy.$Proxy83.addBlock(Unknown Source)\r\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1830)\r\n        ... 2 more\r\n{noformat}"
        }
    },
    {
        "filename": "HDFS-11508.json",
        "creation_time": "2017-03-07T04:55:50.000+0000",
        "stack_trace": "```\norg.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:4242\n\tat org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)\n\tat org.apache.hadoop.oncrpc.SimpleTcpServer.run(SimpleTcpServer.java:86)\n\tat org.apache.hadoop.mount.MountdBase.startTCPServer(MountdBase.java:83)\n\tat org.apache.hadoop.mount.MountdBase.start(MountdBase.java:98)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:71)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)\nCaused by: java.net.BindException: Address already in use\n\tat sun.nio.ch.Net.bind0(Native Method)\n\tat sun.nio.ch.Net.bind(Net.java:433)\n\tat sun.nio.ch.Net.bind(Net.java:425)\n\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)\n\tat sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)\n\tat org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)\n\tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)\n\tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)\n\tat org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)```",
        "source_code": {
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.SimpleTcpServer.run": "  public void run() {\n    // Configure the Server.\n    ChannelFactory factory;\n    if (workerCount == 0) {\n      // Use default workers: 2 * the number of available processors\n      factory = new NioServerSocketChannelFactory(\n          Executors.newCachedThreadPool(), Executors.newCachedThreadPool());\n    } else {\n      factory = new NioServerSocketChannelFactory(\n          Executors.newCachedThreadPool(), Executors.newCachedThreadPool(),\n          workerCount);\n    }\n\n    server = new ServerBootstrap(factory);\n    server.setPipelineFactory(new ChannelPipelineFactory() {\n\n      @Override\n      public ChannelPipeline getPipeline() throws Exception {\n        return Channels.pipeline(RpcUtil.constructRpcFrameDecoder(),\n            RpcUtil.STAGE_RPC_MESSAGE_PARSER, rpcProgram,\n            RpcUtil.STAGE_RPC_TCP_RESPONSE);\n      }\n    });\n    server.setOption(\"child.tcpNoDelay\", true);\n    server.setOption(\"child.keepAlive\", true);\n\n    // Listen to TCP port\n    ch = server.bind(new InetSocketAddress(port));\n    InetSocketAddress socketAddr = (InetSocketAddress) ch.getLocalAddress();\n    boundPort = socketAddr.getPort();\n\n    LOG.info(\"Started listening to TCP requests at port \" + boundPort + \" for \"\n        + rpcProgram + \" with workerCount \" + workerCount);\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.mount.MountdBase.startTCPServer": "  private void startTCPServer() {\n    SimpleTcpServer tcpServer = new SimpleTcpServer(rpcProgram.getPort(),\n        rpcProgram, 1);\n    rpcProgram.startDaemons();\n    try {\n      tcpServer.run();\n    } catch (Throwable e) {\n      LOG.fatal(\"Failed to start the TCP server.\", e);\n      if (tcpServer.getBoundPort() > 0) {\n        rpcProgram.unregister(PortmapMapping.TRANSPORT_TCP,\n            tcpServer.getBoundPort());\n      }\n      tcpServer.shutdown();\n      terminate(1, e);\n    }\n    tcpBoundPort = tcpServer.getBoundPort();\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.mount.MountdBase.run": "    public synchronized void run() {\n      rpcProgram.unregister(PortmapMapping.TRANSPORT_UDP, udpBoundPort);\n      rpcProgram.unregister(PortmapMapping.TRANSPORT_TCP, tcpBoundPort);\n    }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.mount.MountdBase.start": "  public void start(boolean register) {\n    startUDPServer();\n    startTCPServer();\n    if (register) {\n      ShutdownHookManager.get().addShutdownHook(new Unregister(),\n          SHUTDOWN_HOOK_PRIORITY);\n      try {\n        rpcProgram.register(PortmapMapping.TRANSPORT_UDP, udpBoundPort);\n        rpcProgram.register(PortmapMapping.TRANSPORT_TCP, tcpBoundPort);\n      } catch (Throwable e) {\n        LOG.fatal(\"Failed to register the MOUNT service.\", e);\n        terminate(1, e);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.mount.MountdBase.startUDPServer": "  private void startUDPServer() {\n    SimpleUdpServer udpServer = new SimpleUdpServer(rpcProgram.getPort(),\n        rpcProgram, 1);\n    rpcProgram.startDaemons();\n    try {\n      udpServer.run();\n    } catch (Throwable e) {\n      LOG.fatal(\"Failed to start the UDP server.\", e);\n      if (udpServer.getBoundPort() > 0) {\n        rpcProgram.unregister(PortmapMapping.TRANSPORT_UDP,\n            udpServer.getBoundPort());\n      }\n      udpServer.shutdown();\n      terminate(1, e);\n    }\n    udpBoundPort = udpServer.getBoundPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal": "  public void startServiceInternal(boolean register) throws IOException {\n    mountd.start(register); // Start mountd\n    start(register);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService": "  static void startService(String[] args,\n      DatagramSocket registrationSocket) throws IOException {\n    StringUtils.startupShutdownMessage(Nfs3.class, args, LOG);\n    NfsConfiguration conf = new NfsConfiguration();\n    boolean allowInsecurePorts = conf.getBoolean(\n        NfsConfigKeys.DFS_NFS_PORT_MONITORING_DISABLED_KEY,\n        NfsConfigKeys.DFS_NFS_PORT_MONITORING_DISABLED_DEFAULT);\n    final Nfs3 nfsServer = new Nfs3(conf, registrationSocket,\n        allowInsecurePorts);\n    nfsServer.startServiceInternal(true);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start": "  public void start() throws Exception {\n    Nfs3.startService(args, registrationSocket);\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcUtil.constructRpcFrameDecoder": "  public static FrameDecoder constructRpcFrameDecoder() {\n    return new RpcFrameDecoder();\n  }"
        },
        "bug_report": {
            "Title": "Fix bind failure in SimpleTCPServer & Portmap where bind fails because socket is in TIME_WAIT state",
            "Description": "Bind can fail in SimpleTCPServer & Portmap because socket is in TIME_WAIT state.\n\nSocket options should be changed here to use the setReuseAddress option.\n\n{noformat}\n2017-03-05 08:37:13,283 INFO  oncrpc.SimpleUdpServer (SimpleUdpServer.java:run(73)) - Started listening to UDP requests at port 4242 for Rpc program: mountd at localhost:4242 with workerCount 1\n2017-03-05 08:37:13,298 FATAL mount.MountdBase (MountdBase.java:startTCPServer(85)) - Failed to start the TCP server.\norg.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:4242\n\tat org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)\n\tat org.apache.hadoop.oncrpc.SimpleTcpServer.run(SimpleTcpServer.java:86)\n\tat org.apache.hadoop.mount.MountdBase.startTCPServer(MountdBase.java:83)\n\tat org.apache.hadoop.mount.MountdBase.start(MountdBase.java:98)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:71)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)\nCaused by: java.net.BindException: Address already in use\n\tat sun.nio.ch.Net.bind0(Native Method)\n\tat sun.nio.ch.Net.bind(Net.java:433)\n\tat sun.nio.ch.Net.bind(Net.java:425)\n\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)\n\tat sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)\n\tat org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)\n\tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)\n\tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)\n\tat org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n2017-03-05 08:37:13,304 INFO  util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 1\n2017-03-05 08:37:13,308 INFO  nfs3.Nfs3Base (LogAdapter.java:info(45)) - SHUTDOWN_MSG: \n/************************************************************\n{noformat}"
        }
    },
    {
        "filename": "HDFS-2991.json",
        "creation_time": "2012-02-23T02:06:51.000+0000",
        "stack_trace": "```\njava.io.IOException: Error replaying edit log at offset 1354251\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:418)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:93)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:79)\nCaused by: java.lang.ClassCastException: org.apache.hadoop.hdfs.server.namenode.INodeFile cannot be cast to org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:213)```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords": "  long loadEditRecords(int logVersion, EditLogInputStream in, boolean closeOnExit,\n                      long expectedStartingTxId)\n      throws IOException, EditLogInputException {\n    FSDirectory fsDir = fsNamesys.dir;\n    long numEdits = 0;\n\n    EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts =\n      new EnumMap<FSEditLogOpCodes, Holder<Integer>>(FSEditLogOpCodes.class);\n\n    fsNamesys.writeLock();\n    fsDir.writeLock();\n\n    long recentOpcodeOffsets[] = new long[4];\n    Arrays.fill(recentOpcodeOffsets, -1);\n\n    try {\n      long txId = expectedStartingTxId - 1;\n\n      try {\n        while (true) {\n          FSEditLogOp op;\n          try {\n            if ((op = in.readOp()) == null) {\n              break;\n            }\n          } catch (IOException ioe) {\n            String errorMessage = formatEditLogReplayError(in, recentOpcodeOffsets);\n            FSImage.LOG.error(errorMessage);\n            throw new EditLogInputException(errorMessage,\n                ioe, numEdits);\n          }\n          recentOpcodeOffsets[(int)(numEdits % recentOpcodeOffsets.length)] =\n            in.getPosition();\n          if (LayoutVersion.supports(Feature.STORED_TXIDS, logVersion)) {\n            long thisTxId = op.txid;\n            if (thisTxId != txId + 1) {\n              throw new IOException(\"Expected transaction ID \" +\n                  (txId + 1) + \" but got \" + thisTxId);\n            }\n            txId = thisTxId;\n          }\n\n          incrOpCount(op.opCode, opCounts);\n          try {\n            applyEditLogOp(op, fsDir, logVersion);\n          } catch (Throwable t) {\n            // Catch Throwable because in the case of a truly corrupt edits log, any\n            // sort of error might be thrown (NumberFormat, NullPointer, EOF, etc.)\n            String errorMessage = formatEditLogReplayError(in, recentOpcodeOffsets);\n            FSImage.LOG.error(errorMessage);\n            throw new IOException(errorMessage, t);\n          }\n          numEdits++;\n        }\n      } catch (IOException ex) {\n        check203UpgradeFailure(logVersion, ex);\n      } finally {\n        if(closeOnExit)\n          in.close();\n      }\n    } finally {\n      fsDir.writeUnlock();\n      fsNamesys.writeUnlock();\n      if (FSImage.LOG.isDebugEnabled()) {\n        dumpOpCounts(opCounts);\n      }\n    }\n    return numEdits;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp": "  private void applyEditLogOp(FSEditLogOp op, FSDirectory fsDir,\n      int logVersion) throws IOException {\n    switch (op.opCode) {\n    case OP_ADD: {\n      AddCloseOp addCloseOp = (AddCloseOp)op;\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + addCloseOp.path +\n            \" numblocks : \" + addCloseOp.blocks.length +\n            \" clientHolder \" + addCloseOp.clientName +\n            \" clientMachine \" + addCloseOp.clientMachine);\n      }\n      // There three cases here:\n      // 1. OP_ADD to create a new file\n      // 2. OP_ADD to update file blocks\n      // 3. OP_ADD to open file for append\n\n      // See if the file already exists (persistBlocks call)\n      INodeFile oldFile = getINodeFile(fsDir, addCloseOp.path);\n      INodeFile newFile = oldFile;\n      if (oldFile == null) { // this is OP_ADD on a new file (case 1)\n        // versions > 0 support per file replication\n        // get name and replication\n        final short replication  = fsNamesys.getBlockManager(\n            ).adjustReplication(addCloseOp.replication);\n        PermissionStatus permissions = fsNamesys.getUpgradePermission();\n        if (addCloseOp.permissions != null) {\n          permissions = addCloseOp.permissions;\n        }\n        long blockSize = addCloseOp.blockSize;\n\n        // Versions of HDFS prior to 0.17 may log an OP_ADD transaction\n        // which includes blocks in it. When we update the minimum\n        // upgrade version to something more recent than 0.17, we can\n        // simplify this code by asserting that OP_ADD transactions\n        // don't have any blocks.\n        \n        // Older versions of HDFS does not store the block size in inode.\n        // If the file has more than one block, use the size of the\n        // first block as the blocksize. Otherwise use the default\n        // block size.\n        if (-8 <= logVersion && blockSize == 0) {\n          if (addCloseOp.blocks.length > 1) {\n            blockSize = addCloseOp.blocks[0].getNumBytes();\n          } else {\n            long first = ((addCloseOp.blocks.length == 1)?\n                addCloseOp.blocks[0].getNumBytes(): 0);\n            blockSize = Math.max(fsNamesys.getDefaultBlockSize(), first);\n          }\n        }\n\n        // add to the file tree\n        newFile = (INodeFile)fsDir.unprotectedAddFile(\n            addCloseOp.path, permissions,\n            replication, addCloseOp.mtime,\n            addCloseOp.atime, blockSize,\n            true, addCloseOp.clientName, addCloseOp.clientMachine);\n        fsNamesys.leaseManager.addLease(addCloseOp.clientName, addCloseOp.path);\n\n      } else { // This is OP_ADD on an existing file\n        if (!oldFile.isUnderConstruction()) {\n          // This is case 3: a call to append() on an already-closed file.\n          if (FSNamesystem.LOG.isDebugEnabled()) {\n            FSNamesystem.LOG.debug(\"Reopening an already-closed file \" +\n                \"for append\");\n          }\n          fsNamesys.prepareFileForWrite(addCloseOp.path, oldFile,\n              addCloseOp.clientName, addCloseOp.clientMachine, null,\n              false);\n          newFile = getINodeFile(fsDir, addCloseOp.path);\n        }\n      }\n      // Fall-through for case 2.\n      // Regardless of whether it's a new file or an updated file,\n      // update the block list.\n      updateBlocks(fsDir, addCloseOp, newFile);\n      break;\n    }\n    case OP_CLOSE: {\n      AddCloseOp addCloseOp = (AddCloseOp)op;\n      \n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + addCloseOp.path +\n            \" numblocks : \" + addCloseOp.blocks.length +\n            \" clientHolder \" + addCloseOp.clientName +\n            \" clientMachine \" + addCloseOp.clientMachine);\n      }\n\n      INodeFile oldFile = getINodeFile(fsDir, addCloseOp.path);\n      if (oldFile == null) {\n        throw new IOException(\"Operation trying to close non-existent file \" +\n            addCloseOp.path);\n      }\n      \n      // Update in-memory data structures\n      updateBlocks(fsDir, addCloseOp, oldFile);\n\n      // Now close the file\n      INodeFileUnderConstruction ucFile = (INodeFileUnderConstruction) oldFile;\n      // TODO: we could use removeLease(holder, path) here, but OP_CLOSE\n      // doesn't seem to serialize the holder... unclear why!\n      fsNamesys.leaseManager.removeLeaseWithPrefixPath(addCloseOp.path);\n      INodeFile newFile = ucFile.convertToInodeFile();\n      fsDir.replaceNode(addCloseOp.path, ucFile, newFile);\n      break;\n    }\n    case OP_SET_REPLICATION: {\n      SetReplicationOp setReplicationOp = (SetReplicationOp)op;\n      short replication = fsNamesys.getBlockManager().adjustReplication(\n          setReplicationOp.replication);\n      fsDir.unprotectedSetReplication(setReplicationOp.path,\n                                      replication, null);\n      break;\n    }\n    case OP_CONCAT_DELETE: {\n      ConcatDeleteOp concatDeleteOp = (ConcatDeleteOp)op;\n      fsDir.unprotectedConcat(concatDeleteOp.trg, concatDeleteOp.srcs,\n          concatDeleteOp.timestamp);\n      break;\n    }\n    case OP_RENAME_OLD: {\n      RenameOldOp renameOp = (RenameOldOp)op;\n      HdfsFileStatus dinfo = fsDir.getFileInfo(renameOp.dst, false);\n      fsDir.unprotectedRenameTo(renameOp.src, renameOp.dst,\n                                renameOp.timestamp);\n      fsNamesys.unprotectedChangeLease(renameOp.src, renameOp.dst, dinfo);\n      break;\n    }\n    case OP_DELETE: {\n      DeleteOp deleteOp = (DeleteOp)op;\n      fsDir.unprotectedDelete(deleteOp.path, deleteOp.timestamp);\n      break;\n    }\n    case OP_MKDIR: {\n      MkdirOp mkdirOp = (MkdirOp)op;\n      PermissionStatus permissions = fsNamesys.getUpgradePermission();\n      if (mkdirOp.permissions != null) {\n        permissions = mkdirOp.permissions;\n      }\n\n      fsDir.unprotectedMkdir(mkdirOp.path, permissions,\n                             mkdirOp.timestamp);\n      break;\n    }\n    case OP_SET_GENSTAMP: {\n      SetGenstampOp setGenstampOp = (SetGenstampOp)op;\n      fsNamesys.setGenerationStamp(setGenstampOp.genStamp);\n      break;\n    }\n    case OP_SET_PERMISSIONS: {\n      SetPermissionsOp setPermissionsOp = (SetPermissionsOp)op;\n      fsDir.unprotectedSetPermission(setPermissionsOp.src,\n                                     setPermissionsOp.permissions);\n      break;\n    }\n    case OP_SET_OWNER: {\n      SetOwnerOp setOwnerOp = (SetOwnerOp)op;\n      fsDir.unprotectedSetOwner(setOwnerOp.src, setOwnerOp.username,\n                                setOwnerOp.groupname);\n      break;\n    }\n    case OP_SET_NS_QUOTA: {\n      SetNSQuotaOp setNSQuotaOp = (SetNSQuotaOp)op;\n      fsDir.unprotectedSetQuota(setNSQuotaOp.src,\n                                setNSQuotaOp.nsQuota,\n                                HdfsConstants.QUOTA_DONT_SET);\n      break;\n    }\n    case OP_CLEAR_NS_QUOTA: {\n      ClearNSQuotaOp clearNSQuotaOp = (ClearNSQuotaOp)op;\n      fsDir.unprotectedSetQuota(clearNSQuotaOp.src,\n                                HdfsConstants.QUOTA_RESET,\n                                HdfsConstants.QUOTA_DONT_SET);\n      break;\n    }\n\n    case OP_SET_QUOTA:\n      SetQuotaOp setQuotaOp = (SetQuotaOp)op;\n      fsDir.unprotectedSetQuota(setQuotaOp.src,\n                                setQuotaOp.nsQuota,\n                                setQuotaOp.dsQuota);\n      break;\n\n    case OP_TIMES: {\n      TimesOp timesOp = (TimesOp)op;\n\n      fsDir.unprotectedSetTimes(timesOp.path,\n                                timesOp.mtime,\n                                timesOp.atime, true);\n      break;\n    }\n    case OP_SYMLINK: {\n      SymlinkOp symlinkOp = (SymlinkOp)op;\n      fsDir.unprotectedSymlink(symlinkOp.path, symlinkOp.value,\n                               symlinkOp.mtime, symlinkOp.atime,\n                               symlinkOp.permissionStatus);\n      break;\n    }\n    case OP_RENAME: {\n      RenameOp renameOp = (RenameOp)op;\n\n      HdfsFileStatus dinfo = fsDir.getFileInfo(renameOp.dst, false);\n      fsDir.unprotectedRenameTo(renameOp.src, renameOp.dst,\n                                renameOp.timestamp, renameOp.options);\n      fsNamesys.unprotectedChangeLease(renameOp.src, renameOp.dst, dinfo);\n      break;\n    }\n    case OP_GET_DELEGATION_TOKEN: {\n      GetDelegationTokenOp getDelegationTokenOp\n        = (GetDelegationTokenOp)op;\n\n      fsNamesys.getDelegationTokenSecretManager()\n        .addPersistedDelegationToken(getDelegationTokenOp.token,\n                                     getDelegationTokenOp.expiryTime);\n      break;\n    }\n    case OP_RENEW_DELEGATION_TOKEN: {\n      RenewDelegationTokenOp renewDelegationTokenOp\n        = (RenewDelegationTokenOp)op;\n      fsNamesys.getDelegationTokenSecretManager()\n        .updatePersistedTokenRenewal(renewDelegationTokenOp.token,\n                                     renewDelegationTokenOp.expiryTime);\n      break;\n    }\n    case OP_CANCEL_DELEGATION_TOKEN: {\n      CancelDelegationTokenOp cancelDelegationTokenOp\n        = (CancelDelegationTokenOp)op;\n      fsNamesys.getDelegationTokenSecretManager()\n          .updatePersistedTokenCancellation(\n              cancelDelegationTokenOp.token);\n      break;\n    }\n    case OP_UPDATE_MASTER_KEY: {\n      UpdateMasterKeyOp updateMasterKeyOp = (UpdateMasterKeyOp)op;\n      fsNamesys.getDelegationTokenSecretManager()\n        .updatePersistedMasterKey(updateMasterKeyOp.key);\n      break;\n    }\n    case OP_REASSIGN_LEASE: {\n      ReassignLeaseOp reassignLeaseOp = (ReassignLeaseOp)op;\n\n      Lease lease = fsNamesys.leaseManager.getLease(\n          reassignLeaseOp.leaseHolder);\n      INodeFileUnderConstruction pendingFile =\n          (INodeFileUnderConstruction) fsDir.getFileINode(\n              reassignLeaseOp.path);\n      fsNamesys.reassignLeaseInternal(lease,\n          reassignLeaseOp.path, reassignLeaseOp.newHolder, pendingFile);\n      break;\n    }\n    case OP_START_LOG_SEGMENT:\n    case OP_END_LOG_SEGMENT: {\n      // no data in here currently.\n      break;\n    }\n    case OP_DATANODE_ADD:\n    case OP_DATANODE_REMOVE:\n      break;\n    default:\n      throw new IOException(\"Invalid operation read \" + op.opCode);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.incrOpCount": "  private void incrOpCount(FSEditLogOpCodes opCode,\n      EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts) {\n    Holder<Integer> holder = opCounts.get(opCode);\n    if (holder == null) {\n      holder = new Holder<Integer>(1);\n      opCounts.put(opCode, holder);\n    } else {\n      holder.held++;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.check203UpgradeFailure": "  private void check203UpgradeFailure(int logVersion, IOException ex)\n      throws IOException {\n    // 0.20.203 version version has conflicting opcodes with the later releases.\n    // The editlog must be emptied by restarting the namenode, before proceeding\n    // with the upgrade.\n    if (Storage.is203LayoutVersion(logVersion)\n        && logVersion != HdfsConstants.LAYOUT_VERSION) {\n      String msg = \"During upgrade failed to load the editlog version \"\n          + logVersion + \" from release 0.20.203. Please go back to the old \"\n          + \" release and restart the namenode. This empties the editlog \"\n          + \" and saves the namespace. Resume the upgrade after this step.\";\n      throw new IOException(msg, ex);\n    } else {\n      throw ex;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.formatEditLogReplayError": "  private static String formatEditLogReplayError(EditLogInputStream in,\n      long recentOpcodeOffsets[]) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"Error replaying edit log at offset \" + in.getPosition());\n    if (recentOpcodeOffsets[0] != -1) {\n      Arrays.sort(recentOpcodeOffsets);\n      sb.append(\"\\nRecent opcode offsets:\");\n      for (long offset : recentOpcodeOffsets) {\n        if (offset != -1) {\n          sb.append(' ').append(offset);\n        }\n      }\n    }\n    return sb.toString();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.dumpOpCounts": "  private static void dumpOpCounts(\n      EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"Summary of operations loaded from edit log:\\n  \");\n    Joiner.on(\"\\n  \").withKeyValueSeparator(\"=\").appendTo(sb, opCounts);\n    FSImage.LOG.debug(sb.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits": "  long loadFSEdits(EditLogInputStream edits, long expectedStartingTxId)\n      throws IOException {\n    long numEdits = 0;\n    int logVersion = edits.getVersion();\n\n    fsNamesys.writeLock();\n    try {\n      long startTime = now();\n      numEdits = loadEditRecords(logVersion, edits, false, \n                                 expectedStartingTxId);\n      FSImage.LOG.info(\"Edits file \" + edits.getName() \n          + \" of size \" + edits.length() + \" edits # \" + numEdits \n          + \" loaded in \" + (now()-startTime)/1000 + \" seconds.\");\n    } finally {\n      edits.close();\n      fsNamesys.writeUnlock();\n    }\n    \n    return numEdits;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.writeLock": "  void writeLock() {\n    this.dirLock.writeLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.writeUnlock": "  void writeUnlock() {\n    this.dirLock.writeLock().unlock();\n  }"
        },
        "bug_report": {
            "Title": "failure to load edits: ClassCastException",
            "Description": "In doing scale testing of trunk at r1291606, I hit the following:\n\njava.io.IOException: Error replaying edit log at offset 1354251\nRecent opcode offsets: 1350014 1350176 1350312 1354251\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:418)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:93)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:79)\n...\nCaused by: java.lang.ClassCastException: org.apache.hadoop.hdfs.server.namenode.INodeFile cannot be cast to org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:213)\n        ... 13 more\n"
        }
    },
    {
        "filename": "HDFS-4404.json",
        "creation_time": "2013-01-15T01:43:17.000+0000",
        "stack_trace": "```\njava.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]\n at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:524)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:489)\n at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:474)\n at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:568)\n at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:217)\n at org.apache.hadoop.ipc.Client.getConnection(Client.java:1286)\n at org.apache.hadoop.ipc.Client.call(Client.java:1156)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:184)\n at $Proxy9.create(Unknown Source)\n at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:187)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n at java.lang.reflect.Method.invoke(Method.java:597)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)\n at $Proxy10.create(Unknown Source)\n at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:1261)\n at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1280)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1128)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1086)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:232)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:75)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:806)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:715)\n at test.TestLease.main(TestLease.java:45)\n\njava.net.SocketTimeoutException: Call From szxy1x001833091/172.0.0.13 to vm2:8020 failed on socket timeout exception: java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout\n at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:743)\n at org.apache.hadoop.ipc.Client.call(Client.java:1180)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:184)\n at $Proxy9.create(Unknown Source)\n at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:187)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n at java.lang.reflect.Method.invoke(Method.java:597)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)\n at $Proxy10.create(Unknown Source)\n at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:1261)\n at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1280)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1128)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1086)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:232)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:75)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:806)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:715)\n at test.TestLease.main(TestLease.java:45)\n\nCaused by: java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]\n at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:524)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:489)\n at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:474)\n at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:568)\n at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:217)\n at org.apache.hadoop.ipc.Client.getConnection(Client.java:1286)\n at org.apache.hadoop.ipc.Client.call(Client.java:1156)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.connect": "  static void connect(SocketChannel channel, \n                      SocketAddress endpoint, int timeout) throws IOException {\n    \n    boolean blockingOn = channel.isBlocking();\n    if (blockingOn) {\n      channel.configureBlocking(false);\n    }\n    \n    try { \n      if (channel.connect(endpoint)) {\n        return;\n      }\n\n      long timeoutLeft = timeout;\n      long endTime = (timeout > 0) ? (Time.now() + timeout): 0;\n      \n      while (true) {\n        // we might have to call finishConnect() more than once\n        // for some channels (with user level protocols)\n        \n        int ret = selector.select((SelectableChannel)channel, \n                                  SelectionKey.OP_CONNECT, timeoutLeft);\n        \n        if (ret > 0 && channel.finishConnect()) {\n          return;\n        }\n        \n        if (ret == 0 ||\n            (timeout > 0 &&  \n              (timeoutLeft = (endTime - Time.now())) <= 0)) {\n          throw new SocketTimeoutException(\n                    timeoutExceptionString(channel, timeout, \n                                           SelectionKey.OP_CONNECT));\n        }\n      }\n    } catch (IOException e) {\n      // javadoc for SocketChannel.connect() says channel should be closed.\n      try {\n        channel.close();\n      } catch (IOException ignored) {}\n      throw e;\n    } finally {\n      if (blockingOn && channel.isOpen()) {\n        channel.configureBlocking(true);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.timeoutExceptionString": "  private static String timeoutExceptionString(SelectableChannel channel,\n                                               long timeout, int ops) {\n    \n    String waitingFor;\n    switch(ops) {\n    \n    case SelectionKey.OP_READ :\n      waitingFor = \"read\"; break;\n      \n    case SelectionKey.OP_WRITE :\n      waitingFor = \"write\"; break;      \n      \n    case SelectionKey.OP_CONNECT :\n      waitingFor = \"connect\"; break;\n      \n    default :\n      waitingFor = \"\" + ops;  \n    }\n    \n    return timeout + \" millis timeout while \" +\n           \"waiting for channel to be ready for \" + \n           waitingFor + \". ch : \" + channel;    \n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.select": "    int select(SelectableChannel channel, int ops, long timeout) \n                                                   throws IOException {\n     \n      SelectorInfo info = get(channel);\n      \n      SelectionKey key = null;\n      int ret = 0;\n      \n      try {\n        while (true) {\n          long start = (timeout == 0) ? 0 : Time.now();\n\n          key = channel.register(info.selector, ops);\n          ret = info.selector.select(timeout);\n          \n          if (ret != 0) {\n            return ret;\n          }\n          \n          /* Sometimes select() returns 0 much before timeout for \n           * unknown reasons. So select again if required.\n           */\n          if (timeout > 0) {\n            timeout -= Time.now() - start;\n            if (timeout <= 0) {\n              return 0;\n            }\n          }\n          \n          if (Thread.currentThread().isInterrupted()) {\n            throw new InterruptedIOException(\"Interruped while waiting for \" +\n                                             \"IO on channel \" + channel +\n                                             \". \" + timeout + \n                                             \" millis timeout left.\");\n          }\n        }\n      } finally {\n        if (key != null) {\n          key.cancel();\n        }\n        \n        //clear the canceled key.\n        try {\n          info.selector.selectNow();\n        } catch (IOException e) {\n          LOG.info(\"Unexpected Exception while clearing selector : \", e);\n          // don't put the selector back.\n          info.close();\n          return ret; \n        }\n        \n        release(info);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.close": "      void close() {\n        if (selector != null) {\n          try {\n            selector.close();\n          } catch (IOException e) {\n            LOG.warn(\"Unexpected exception while closing selector : \", e);\n          }\n        }\n      }    ",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.isOpen": "  boolean isOpen() {\n    return !closed && channel.isOpen();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.connect": "  public static void connect(Socket socket, \n                             SocketAddress endpoint,\n                             SocketAddress localAddr,\n                             int timeout) throws IOException {\n    if (socket == null || endpoint == null || timeout < 0) {\n      throw new IllegalArgumentException(\"Illegal argument for connect()\");\n    }\n    \n    SocketChannel ch = socket.getChannel();\n    \n    if (localAddr != null) {\n      Class localClass = localAddr.getClass();\n      Class remoteClass = endpoint.getClass();\n      Preconditions.checkArgument(localClass.equals(remoteClass),\n          \"Local address %s must be of same family as remote address %s.\",\n          localAddr, endpoint);\n      socket.bind(localAddr);\n    }\n\n    if (ch == null) {\n      // let the default implementation handle it.\n      socket.connect(endpoint, timeout);\n    } else {\n      SocketIOWithTimeout.connect(ch, endpoint, timeout);\n    }\n\n    // There is a very rare case allowed by the TCP specification, such that\n    // if we are trying to connect to an endpoint on the local machine,\n    // and we end up choosing an ephemeral port equal to the destination port,\n    // we will actually end up getting connected to ourself (ie any data we\n    // send just comes right back). This is only possible if the target\n    // daemon is down, so we'll treat it like connection refused.\n    if (socket.getLocalPort() == socket.getPort() &&\n        socket.getLocalAddress().equals(socket.getInetAddress())) {\n      LOG.info(\"Detected a loopback TCP socket, disconnecting it\");\n      socket.close();\n      throw new ConnectException(\n        \"Localhost targeted connection resulted in a loopback. \" +\n        \"No daemon is listening on the target port.\");\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupConnection": "    private synchronized void setupConnection() throws IOException {\n      short ioFailures = 0;\n      short timeoutFailures = 0;\n      while (true) {\n        try {\n          this.socket = socketFactory.createSocket();\n          this.socket.setTcpNoDelay(tcpNoDelay);\n          \n          /*\n           * Bind the socket to the host specified in the principal name of the\n           * client, to ensure Server matching address of the client connection\n           * to host name in principal passed.\n           */\n          if (UserGroupInformation.isSecurityEnabled()) {\n            KerberosInfo krbInfo = \n              remoteId.getProtocol().getAnnotation(KerberosInfo.class);\n            if (krbInfo != null && krbInfo.clientPrincipal() != null) {\n              String host = \n                SecurityUtil.getHostFromPrincipal(remoteId.getTicket().getUserName());\n              \n              // If host name is a valid local address then bind socket to it\n              InetAddress localAddr = NetUtils.getLocalInetAddress(host);\n              if (localAddr != null) {\n                this.socket.bind(new InetSocketAddress(localAddr, 0));\n              }\n            }\n          }\n          \n          // connection time out is 20s\n          NetUtils.connect(this.socket, server, 20000);\n          if (rpcTimeout > 0) {\n            pingInterval = rpcTimeout;  // rpcTimeout overwrites pingInterval\n          }\n          this.socket.setSoTimeout(pingInterval);\n          return;\n        } catch (SocketTimeoutException toe) {\n          /* Check for an address change and update the local reference.\n           * Reset the failure counter if the address was changed\n           */\n          if (updateAddress()) {\n            timeoutFailures = ioFailures = 0;\n          }\n          handleConnectionFailure(timeoutFailures++,\n              maxRetriesOnSocketTimeouts, toe);\n        } catch (IOException ie) {\n          if (updateAddress()) {\n            timeoutFailures = ioFailures = 0;\n          }\n          handleConnectionFailure(ioFailures++, ie);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getTicket": "    UserGroupInformation getTicket() {\n      return ticket;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.updateAddress": "    private synchronized boolean updateAddress() throws IOException {\n      // Do a fresh lookup with the old host name.\n      InetSocketAddress currentAddr = NetUtils.createSocketAddrForHost(\n                               server.getHostName(), server.getPort());\n\n      if (!server.equals(currentAddr)) {\n        LOG.warn(\"Address change detected. Old: \" + server.toString() +\n                                 \" New: \" + currentAddr.toString());\n        server = currentAddr;\n        return true;\n      }\n      return false;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.handleConnectionFailure": "    private void handleConnectionFailure(int curRetries, IOException ioe\n        ) throws IOException {\n      closeConnection();\n\n      final RetryAction action;\n      try {\n        action = connectionRetryPolicy.shouldRetry(ioe, curRetries, 0, true);\n      } catch(Exception e) {\n        throw e instanceof IOException? (IOException)e: new IOException(e);\n      }\n      if (action.action == RetryAction.RetryDecision.FAIL) {\n        if (action.reason != null) {\n          LOG.warn(\"Failed to connect to server: \" + server + \": \"\n              + action.reason, ioe);\n        }\n        throw ioe;\n      }\n\n      try {\n        Thread.sleep(action.delayMillis);\n      } catch (InterruptedException e) {\n        throw (IOException)new InterruptedIOException(\"Interrupted: action=\"\n            + action + \", retry policy=\" + connectionRetryPolicy).initCause(e);\n      }\n      LOG.info(\"Retrying connect to server: \" + server + \". Already tried \"\n          + curRetries + \" time(s); retry policy is \" + connectionRetryPolicy);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getProtocol": "    Class<?> getProtocol() {\n      return protocol;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupIOstreams": "    private synchronized void setupIOstreams() throws InterruptedException {\n      if (socket != null || shouldCloseConnection.get()) {\n        return;\n      } \n      try {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Connecting to \"+server);\n        }\n        short numRetries = 0;\n        final short MAX_RETRIES = 5;\n        Random rand = null;\n        while (true) {\n          setupConnection();\n          InputStream inStream = NetUtils.getInputStream(socket);\n          OutputStream outStream = NetUtils.getOutputStream(socket);\n          writeConnectionHeader(outStream);\n          if (authMethod != AuthMethod.SIMPLE) {\n            final InputStream in2 = inStream;\n            final OutputStream out2 = outStream;\n            UserGroupInformation ticket = remoteId.getTicket();\n            if (ticket.getRealUser() != null) {\n              ticket = ticket.getRealUser();\n            }\n            boolean continueSasl = false;\n            try {\n              continueSasl = ticket\n                  .doAs(new PrivilegedExceptionAction<Boolean>() {\n                    @Override\n                    public Boolean run() throws IOException {\n                      return setupSaslConnection(in2, out2);\n                    }\n                  });\n            } catch (Exception ex) {\n              if (rand == null) {\n                rand = new Random();\n              }\n              handleSaslConnectionFailure(numRetries++, MAX_RETRIES, ex, rand,\n                  ticket);\n              continue;\n            }\n            if (continueSasl) {\n              // Sasl connect is successful. Let's set up Sasl i/o streams.\n              inStream = saslRpcClient.getInputStream(inStream);\n              outStream = saslRpcClient.getOutputStream(outStream);\n            } else {\n              // fall back to simple auth because server told us so.\n              authMethod = AuthMethod.SIMPLE;\n            }\n          }\n        \n          if (doPing) {\n            this.in = new DataInputStream(new BufferedInputStream(\n                new PingInputStream(inStream)));\n          } else {\n            this.in = new DataInputStream(new BufferedInputStream(inStream));\n          }\n          this.out = new DataOutputStream(new BufferedOutputStream(outStream));\n          writeConnectionContext(remoteId, authMethod);\n\n          // update last activity time\n          touch();\n\n          // start the receiver thread after the socket connection has been set\n          // up\n          start();\n          return;\n        }\n      } catch (Throwable t) {\n        if (t instanceof IOException) {\n          markClosed((IOException)t);\n        } else {\n          markClosed(new IOException(\"Couldn't set up IO streams\", t));\n        }\n        close();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.writeConnectionHeader": "    private void writeConnectionHeader(OutputStream outStream)\n        throws IOException {\n      DataOutputStream out = new DataOutputStream(new BufferedOutputStream(outStream));\n      // Write out the header, version and authentication method\n      out.write(Server.HEADER.array());\n      out.write(Server.CURRENT_VERSION);\n      authMethod.write(out);\n      Server.IpcSerializationType.PROTOBUF.write(out);\n      out.flush();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.close": "    private synchronized void close() {\n      if (!shouldCloseConnection.get()) {\n        LOG.error(\"The connection is not in the closed state\");\n        return;\n      }\n\n      // release the resources\n      // first thing to do;take the connection out of the connection list\n      synchronized (connections) {\n        if (connections.get(remoteId) == this) {\n          connections.remove(remoteId);\n        }\n      }\n\n      // close the streams and therefore the socket\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(in);\n      disposeSasl();\n\n      // clean up all calls\n      if (closeException == null) {\n        if (!calls.isEmpty()) {\n          LOG.warn(\n              \"A connection is closed for no cause and calls are not empty\");\n\n          // clean up calls anyway\n          closeException = new IOException(\"Unexpected closed connection\");\n          cleanupCalls();\n        }\n      } else {\n        // log the info\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"closing ipc connection to \" + server + \": \" +\n              closeException.getMessage(),closeException);\n        }\n\n        // cleanup calls\n        cleanupCalls();\n      }\n      if (LOG.isDebugEnabled())\n        LOG.debug(getName() + \": closed\");\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.touch": "    private void touch() {\n      lastActivity.set(Time.now());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.handleSaslConnectionFailure": "    private synchronized void handleSaslConnectionFailure(\n        final int currRetries, final int maxRetries, final Exception ex,\n        final Random rand, final UserGroupInformation ugi) throws IOException,\n        InterruptedException {\n      ugi.doAs(new PrivilegedExceptionAction<Object>() {\n        @Override\n        public Object run() throws IOException, InterruptedException {\n          final short MAX_BACKOFF = 5000;\n          closeConnection();\n          disposeSasl();\n          if (shouldAuthenticateOverKrb()) {\n            if (currRetries < maxRetries) {\n              if(LOG.isDebugEnabled()) {\n                LOG.debug(\"Exception encountered while connecting to \"\n                    + \"the server : \" + ex);\n              }\n              // try re-login\n              if (UserGroupInformation.isLoginKeytabBased()) {\n                UserGroupInformation.getLoginUser().reloginFromKeytab();\n              } else {\n                UserGroupInformation.getLoginUser().reloginFromTicketCache();\n              }\n              // have granularity of milliseconds\n              //we are sleeping with the Connection lock held but since this\n              //connection instance is being used for connecting to the server\n              //in question, it is okay\n              Thread.sleep((rand.nextInt(MAX_BACKOFF) + 1));\n              return null;\n            } else {\n              String msg = \"Couldn't setup connection for \"\n                  + UserGroupInformation.getLoginUser().getUserName() + \" to \"\n                  + serverPrincipal;\n              LOG.warn(msg);\n              throw (IOException) new IOException(msg).initCause(ex);\n            }\n          } else {\n            LOG.warn(\"Exception encountered while connecting to \"\n                + \"the server : \" + ex);\n          }\n          if (ex instanceof RemoteException)\n            throw (RemoteException) ex;\n          throw new IOException(ex);\n        }\n      });\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.writeConnectionContext": "    private void writeConnectionContext(ConnectionId remoteId,\n                                        AuthMethod authMethod)\n                                            throws IOException {\n      // Write out the ConnectionHeader\n      DataOutputBuffer buf = new DataOutputBuffer();\n      ProtoUtil.makeIpcConnectionContext(\n          RPC.getProtocolName(remoteId.getProtocol()),\n          remoteId.getTicket(),\n          authMethod).writeTo(buf);\n      \n      // Write out the packet length\n      int bufLen = buf.getLength();\n\n      out.writeInt(bufLen);\n      out.write(buf.getData(), 0, bufLen);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupSaslConnection": "    private synchronized boolean setupSaslConnection(final InputStream in2, \n        final OutputStream out2) \n        throws IOException {\n      saslRpcClient = new SaslRpcClient(authMethod, token, serverPrincipal);\n      return saslRpcClient.saslConnect(in2, out2);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.markClosed": "    private synchronized void markClosed(IOException e) {\n      if (shouldCloseConnection.compareAndSet(false, true)) {\n        closeException = e;\n        notifyAll();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnection": "  private Connection getConnection(ConnectionId remoteId,\n                                   Call call)\n                                   throws IOException, InterruptedException {\n    if (!running.get()) {\n      // the client is stopped\n      throw new IOException(\"The client is stopped\");\n    }\n    Connection connection;\n    /* we could avoid this allocation for each RPC by having a  \n     * connectionsId object and with set() method. We need to manage the\n     * refs for keys in HashMap properly. For now its ok.\n     */\n    do {\n      synchronized (connections) {\n        connection = connections.get(remoteId);\n        if (connection == null) {\n          connection = new Connection(remoteId);\n          connections.put(remoteId, connection);\n        }\n      }\n    } while (!connection.addCall(call));\n    \n    //we don't invoke the method below inside \"synchronized (connections)\"\n    //block above. The reason for that is if the server happens to be slow,\n    //it will take longer to establish a connection and that will slow the\n    //entire system down.\n    connection.setupIOstreams();\n    return connection;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.addCall": "    private synchronized boolean addCall(Call call) {\n      if (shouldCloseConnection.get())\n        return false;\n      calls.put(call.id, call);\n      notify();\n      return true;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.call": "  public Writable call(RPC.RpcKind rpcKind, Writable rpcRequest,\n      ConnectionId remoteId) throws InterruptedException, IOException {\n    Call call = new Call(rpcKind, rpcRequest);\n    Connection connection = getConnection(remoteId, call);\n    try {\n      connection.sendRpcRequest(call);                 // send the rpc request\n    } catch (RejectedExecutionException e) {\n      throw new IOException(\"connection has been closed\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      LOG.warn(\"interrupted waiting to send rpc request to server\", e);\n      throw new IOException(e);\n    }\n\n    boolean interrupted = false;\n    synchronized (call) {\n      while (!call.done) {\n        try {\n          call.wait();                           // wait for the result\n        } catch (InterruptedException ie) {\n          // save the fact that we were interrupted\n          interrupted = true;\n        }\n      }\n\n      if (interrupted) {\n        // set the interrupt flag now that we are done waiting\n        Thread.currentThread().interrupt();\n      }\n\n      if (call.error != null) {\n        if (call.error instanceof RemoteException) {\n          call.error.fillInStackTrace();\n          throw call.error;\n        } else { // local exception\n          InetSocketAddress address = connection.getRemoteAddress();\n          throw NetUtils.wrapException(address.getHostName(),\n                  address.getPort(),\n                  NetUtils.getHostname(),\n                  0,\n                  call.error);\n        }\n      } else {\n        return call.getRpcResult();\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.sendRpcRequest": "    public void sendRpcRequest(final Call call)\n        throws InterruptedException, IOException {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n\n      // Serialize the call to be sent. This is done from the actual\n      // caller thread, rather than the SEND_PARAMS_EXECUTOR thread,\n      // so that if the serialization throws an error, it is reported\n      // properly. This also parallelizes the serialization.\n      //\n      // Format of a call on the wire:\n      // 0) Length of rest below (1 + 2)\n      // 1) RpcRequestHeader  - is serialized Delimited hence contains length\n      // 2) RpcRequest\n      //\n      // Items '1' and '2' are prepared here. \n      final DataOutputBuffer d = new DataOutputBuffer();\n      RpcRequestHeaderProto header = ProtoUtil.makeRpcRequestHeader(\n         call.rpcKind, OperationProto.RPC_FINAL_PACKET, call.id);\n      header.writeDelimitedTo(d);\n      call.rpcRequest.write(d);\n\n      synchronized (sendRpcRequestLock) {\n        Future<?> senderFuture = SEND_PARAMS_EXECUTOR.submit(new Runnable() {\n          @Override\n          public void run() {\n            try {\n              synchronized (Connection.this.out) {\n                if (shouldCloseConnection.get()) {\n                  return;\n                }\n                \n                if (LOG.isDebugEnabled())\n                  LOG.debug(getName() + \" sending #\" + call.id);\n         \n                byte[] data = d.getData();\n                int totalLength = d.getLength();\n                out.writeInt(totalLength); // Total Length\n                out.write(data, 0, totalLength);// RpcRequestHeader + RpcRequest\n                out.flush();\n              }\n            } catch (IOException e) {\n              // exception at this point would leave the connection in an\n              // unrecoverable state (eg half a call left on the wire).\n              // So, close the connection, killing any outstanding calls\n              markClosed(e);\n            } finally {\n              //the buffer is just an in-memory buffer, but it is still polite to\n              // close early\n              IOUtils.closeStream(d);\n            }\n          }\n        });\n      \n        try {\n          senderFuture.get();\n        } catch (ExecutionException e) {\n          Throwable cause = e.getCause();\n          \n          // cause should only be a RuntimeException as the Runnable above\n          // catches IOException\n          if (cause instanceof RuntimeException) {\n            throw (RuntimeException) cause;\n          } else {\n            throw new RuntimeException(\"unexpected checked exception\", cause);\n          }\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRpcResult": "    public synchronized Writable getRpcResult() {\n      return rpcResponse;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnectionId": "    static ConnectionId getConnectionId(InetSocketAddress addr,\n        Class<?> protocol, UserGroupInformation ticket, int rpcTimeout,\n        RetryPolicy connectionRetryPolicy, Configuration conf) throws IOException {\n\n      if (connectionRetryPolicy == null) {\n        final int max = conf.getInt(\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY,\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_DEFAULT);\n        connectionRetryPolicy = RetryPolicies.retryUpToMaximumCountWithFixedSleep(\n            max, 1, TimeUnit.SECONDS);\n      }\n\n      String remotePrincipal = getRemotePrincipal(conf, addr, protocol);\n      boolean doPing =\n        conf.getBoolean(CommonConfigurationKeys.IPC_CLIENT_PING_KEY, true);\n      return new ConnectionId(addr, protocol, ticket,\n          rpcTimeout, remotePrincipal,\n          conf.getInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECTION_MAXIDLETIME_KEY,\n              CommonConfigurationKeysPublic.IPC_CLIENT_CONNECTION_MAXIDLETIME_DEFAULT),\n          connectionRetryPolicy,\n          conf.getInt(\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_KEY,\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_DEFAULT),\n          conf.getBoolean(CommonConfigurationKeysPublic.IPC_CLIENT_TCPNODELAY_KEY,\n              CommonConfigurationKeysPublic.IPC_CLIENT_TCPNODELAY_DEFAULT),\n          doPing, \n          (doPing ? Client.getPingInterval(conf) : 0));\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRemoteAddress": "    public InetSocketAddress getRemoteAddress() {\n      return server;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.invoke": "    public Object invoke(Object proxy, Method method, Object[] args)\n        throws ServiceException {\n      long startTime = 0;\n      if (LOG.isDebugEnabled()) {\n        startTime = Time.now();\n      }\n\n      RequestProto rpcRequest = constructRpcRequest(method, args);\n      RpcResponseWritable val = null;\n      \n      if (LOG.isTraceEnabled()) {\n        LOG.trace(Thread.currentThread().getId() + \": Call -> \" +\n            remoteId + \": \" + method.getName() +\n            \" {\" + TextFormat.shortDebugString((Message) args[1]) + \"}\");\n      }\n      try {\n        val = (RpcResponseWritable) client.call(RPC.RpcKind.RPC_PROTOCOL_BUFFER,\n            new RpcRequestWritable(rpcRequest), remoteId);\n\n      } catch (Throwable e) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Exception <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + e + \"}\");\n        }\n\n        throw new ServiceException(e);\n      }\n\n      if (LOG.isDebugEnabled()) {\n        long callTime = Time.now() - startTime;\n        LOG.debug(\"Call: \" + method.getName() + \" took \" + callTime + \"ms\");\n      }\n      \n      Message prototype = null;\n      try {\n        prototype = getReturnProtoType(method);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      Message returnMessage;\n      try {\n        returnMessage = prototype.newBuilderForType()\n            .mergeFrom(val.responseMessage).build();\n\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Response <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + TextFormat.shortDebugString(returnMessage) + \"}\");\n        }\n\n      } catch (Throwable e) {\n        throw new ServiceException(e);\n      }\n      return returnMessage;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcRequestWritable request = (RpcRequestWritable) writableRequest;\n        RequestProto rpcRequest = request.message;\n        String methodName = rpcRequest.getMethodName();\n        \n        \n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcServerException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = prototype.newBuilderForType()\n            .mergeFrom(rpcRequest.getRequest()).build();\n        Message result;\n        try {\n          long startTime = Time.now();\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n          int processingTime = (int) (Time.now() - startTime);\n          int qTime = (int) (startTime - receiveTime);\n          if (LOG.isDebugEnabled()) {\n            LOG.info(\"Served: \" + methodName + \" queueTime= \" + qTime +\n                      \" procesingTime= \" + processingTime);\n          }\n          server.rpcMetrics.addRpcQueueTime(qTime);\n          server.rpcMetrics.addRpcProcessingTime(processingTime);\n          server.rpcDetailedMetrics.addProcessingTime(methodName,\n              processingTime);\n        } catch (ServiceException e) {\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          throw e;\n        }\n        return new RpcResponseWritable(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.constructRpcRequest": "    private RequestProto constructRpcRequest(Method method,\n        Object[] params) throws ServiceException {\n      RequestProto rpcRequest;\n      RequestProto.Builder builder = RequestProto\n          .newBuilder();\n      builder.setMethodName(method.getName());\n\n      if (params.length != 2) { // RpcController + Message\n        throw new ServiceException(\"Too many parameters for request. Method: [\"\n            + method.getName() + \"]\" + \", Expected: 2, Actual: \"\n            + params.length);\n      }\n      if (params[1] == null) {\n        throw new ServiceException(\"null param while calling Method: [\"\n            + method.getName() + \"]\");\n      }\n\n      Message param = (Message) params[1];\n      builder.setRequest(param.toByteString());\n      // For protobuf, {@code protocol} used when creating client side proxy is\n      // the interface extending BlockingInterface, which has the annotations \n      // such as ProtocolName etc.\n      //\n      // Using Method.getDeclaringClass(), as in WritableEngine to get at\n      // the protocol interface will return BlockingInterface, from where \n      // the annotation ProtocolName and Version cannot be\n      // obtained.\n      //\n      // Hence we simply use the protocol class used to create the proxy.\n      // For PB this may limit the use of mixins on client side.\n      builder.setDeclaringClassProtocolName(protocolName);\n      builder.setClientProtocolVersion(clientProtocolVersion);\n      rpcRequest = builder.build();\n      return rpcRequest;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getReturnProtoType": "    private Message getReturnProtoType(Method method) throws Exception {\n      if (returnTypes.containsKey(method.getName())) {\n        return returnTypes.get(method.getName());\n      }\n      \n      Class<?> returnType = method.getReturnType();\n      Method newInstMethod = returnType.getMethod(\"getDefaultInstance\");\n      newInstMethod.setAccessible(true);\n      Message prototype = (Message) newInstMethod.invoke(null, (Object[]) null);\n      returnTypes.put(method.getName(), prototype);\n      return prototype;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create": "  public void create(String src, FsPermission masked, String clientName,\n      EnumSetWritable<CreateFlag> flag, boolean createParent,\n      short replication, long blockSize) throws AccessControlException,\n      AlreadyBeingCreatedException, DSQuotaExceededException,\n      FileAlreadyExistsException, FileNotFoundException,\n      NSQuotaExceededException, ParentNotDirectoryException, SafeModeException,\n      UnresolvedLinkException, IOException {\n    CreateRequestProto req = CreateRequestProto.newBuilder()\n        .setSrc(src)\n        .setMasked(PBHelper.convert(masked))\n        .setClientName(clientName)\n        .setCreateFlag(PBHelper.convertCreateFlag(flag))\n        .setCreateParent(createParent)\n        .setReplication(replication)\n        .setBlockSize(blockSize)\n        .build();\n    try {\n      rpcProxy.create(null, req);\n    } catch (ServiceException e) {\n      throw ProtobufHelper.getRemoteException(e);\n    }\n\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setReplication": "  public boolean setReplication(String src, short replication)\n      throws AccessControlException, DSQuotaExceededException,\n      FileNotFoundException, SafeModeException, UnresolvedLinkException,\n      IOException {\n    SetReplicationRequestProto req = SetReplicationRequestProto.newBuilder()\n        .setSrc(src)\n        .setReplication(replication)\n        .build();\n    try {\n      return rpcProxy.setReplication(null, req).getResult();\n    } catch (ServiceException e) {\n      throw ProtobufHelper.getRemoteException(e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod": "  private Object invokeMethod(Method method, Object[] args) throws Throwable {\n    try {\n      if (!method.isAccessible()) {\n        method.setAccessible(true);\n      }\n      return method.invoke(currentProxy, args);\n    } catch (InvocationTargetException e) {\n      throw e.getCause();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invoke": "  public Object invoke(Object proxy, Method method, Object[] args)\n    throws Throwable {\n    RetryPolicy policy = methodNameToPolicyMap.get(method.getName());\n    if (policy == null) {\n      policy = defaultPolicy;\n    }\n    \n    // The number of times this method invocation has been failed over.\n    int invocationFailoverCount = 0;\n    int retries = 0;\n    while (true) {\n      // The number of times this invocation handler has ever been failed over,\n      // before this method invocation attempt. Used to prevent concurrent\n      // failed method invocations from triggering multiple failover attempts.\n      long invocationAttemptFailoverCount;\n      synchronized (proxyProvider) {\n        invocationAttemptFailoverCount = proxyProviderFailoverCount;\n      }\n      try {\n        Object ret = invokeMethod(method, args);\n        hasMadeASuccessfulCall = true;\n        return ret;\n      } catch (Exception e) {\n        boolean isMethodIdempotent = proxyProvider.getInterface()\n            .getMethod(method.getName(), method.getParameterTypes())\n            .isAnnotationPresent(Idempotent.class);\n        RetryAction action = policy.shouldRetry(e, retries++, invocationFailoverCount,\n            isMethodIdempotent);\n        if (action.action == RetryAction.RetryDecision.FAIL) {\n          if (action.reason != null) {\n            LOG.warn(\"Exception while invoking \" + \n                currentProxy.getClass() + \".\" + method.getName() +\n                \". Not retrying because \" + action.reason, e);\n          }\n          throw e;\n        } else { // retry or failover\n          // avoid logging the failover if this is the first call on this\n          // proxy object, and we successfully achieve the failover without\n          // any flip-flopping\n          boolean worthLogging = \n            !(invocationFailoverCount == 0 && !hasMadeASuccessfulCall);\n          worthLogging |= LOG.isDebugEnabled();\n          if (action.action == RetryAction.RetryDecision.FAILOVER_AND_RETRY &&\n              worthLogging) {\n            String msg = \"Exception while invoking \" + method.getName()\n              + \" of class \" + currentProxy.getClass().getSimpleName();\n            if (invocationFailoverCount > 0) {\n              msg += \" after \" + invocationFailoverCount + \" fail over attempts\"; \n            }\n            msg += \". Trying to fail over \" + formatSleepMessage(action.delayMillis);\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(msg, e);\n            } else {\n              LOG.warn(msg);\n            }\n          } else {\n            if(LOG.isDebugEnabled()) {\n              LOG.debug(\"Exception while invoking \" + method.getName()\n                  + \" of class \" + currentProxy.getClass().getSimpleName() +\n                  \". Retrying \" + formatSleepMessage(action.delayMillis), e);\n            }\n          }\n          \n          if (action.delayMillis > 0) {\n            ThreadUtil.sleepAtLeastIgnoreInterrupts(action.delayMillis);\n          }\n          \n          if (action.action == RetryAction.RetryDecision.FAILOVER_AND_RETRY) {\n            // Make sure that concurrent failed method invocations only cause a\n            // single actual fail over.\n            synchronized (proxyProvider) {\n              if (invocationAttemptFailoverCount == proxyProviderFailoverCount) {\n                proxyProvider.performFailover(currentProxy);\n                proxyProviderFailoverCount++;\n                currentProxy = proxyProvider.getProxy();\n              } else {\n                LOG.warn(\"A failover has occurred since the start of this method\"\n                    + \" invocation attempt.\");\n              }\n            }\n            invocationFailoverCount++;\n          }\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate": "  static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n      FsPermission masked, EnumSet<CreateFlag> flag, boolean createParent,\n      short replication, long blockSize, Progressable progress, int buffersize,\n      DataChecksum checksum) throws IOException {\n    final DFSOutputStream out = new DFSOutputStream(dfsClient, src, masked,\n        flag, createParent, replication, blockSize, progress, buffersize,\n        checksum);\n    out.streamer.start();\n    return out;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.create": "  public DFSOutputStream create(String src, \n                             FsPermission permission,\n                             EnumSet<CreateFlag> flag, \n                             boolean createParent,\n                             short replication,\n                             long blockSize,\n                             Progressable progress,\n                             int buffersize,\n                             ChecksumOpt checksumOpt) throws IOException {\n    checkOpen();\n    if (permission == null) {\n      permission = FsPermission.getFileDefault();\n    }\n    FsPermission masked = permission.applyUMask(dfsClientConf.uMask);\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(src + \": masked=\" + masked);\n    }\n    final DFSOutputStream result = DFSOutputStream.newStreamForCreate(this,\n        src, masked, flag, createParent, replication, blockSize, progress,\n        buffersize, dfsClientConf.createChecksum(checksumOpt));\n    beginFileLease(src, result);\n    return result;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.beginFileLease": "  private void beginFileLease(final String src, final DFSOutputStream out) \n      throws IOException {\n    getLeaseRenewer().put(src, out, this);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.createChecksum": "    private DataChecksum createChecksum(ChecksumOpt userOpt) \n        throws IOException {\n      // Fill in any missing field with the default.\n      ChecksumOpt myOpt = ChecksumOpt.processChecksumOpt(\n          defaultChecksumOpt, userOpt);\n      DataChecksum dataChecksum = DataChecksum.newDataChecksum(\n          myOpt.getChecksumType(),\n          myOpt.getBytesPerChecksum());\n      if (dataChecksum == null) {\n        throw new IOException(\"Invalid checksum type specified: \"\n            + myOpt.getChecksumType().name());\n      }\n      return dataChecksum;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.checkOpen": "  void checkOpen() throws IOException {\n    if (!clientRunning) {\n      IOException result = new IOException(\"Filesystem closed\");\n      throw result;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.create": "  public HdfsDataOutputStream create(Path f, FsPermission permission,\n    EnumSet<CreateFlag> cflags, int bufferSize, short replication, long blockSize,\n    Progressable progress, ChecksumOpt checksumOpt) throws IOException {\n    statistics.incrementWriteOps(1);\n    final DFSOutputStream out = dfs.create(getPathName(f), permission, cflags,\n        replication, blockSize, progress, bufferSize, checksumOpt);\n    return new HdfsDataOutputStream(out, statistics);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DistributedFileSystem.getPathName": "  private String getPathName(Path file) {\n    checkPath(file);\n    String result = makeAbsolute(file).toUri().getPath();\n    if (!DFSUtil.isValidName(result)) {\n      throw new IllegalArgumentException(\"Pathname \" + result + \" from \" +\n                                         file+\" is not a valid DFS filename.\");\n    }\n    return result;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.create": "  public FSDataOutputStream create(Path f,\n      FsPermission permission,\n      EnumSet<CreateFlag> flags,\n      int bufferSize,\n      short replication,\n      long blockSize,\n      Progressable progress,\n      ChecksumOpt checksumOpt) throws IOException {\n    // Checksum options are ignored by default. The file systems that\n    // implement checksum need to override this method. The full\n    // support is currently only available in DFS.\n    return create(f, permission, flags.contains(CreateFlag.OVERWRITE), \n        bufferSize, replication, blockSize, progress);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getDefaultBlockSize": "  public long getDefaultBlockSize(Path f) {\n    return getDefaultBlockSize();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.setPermission": "  public void setPermission(Path p, FsPermission permission\n      ) throws IOException {\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getDefaultReplication": "  public short getDefaultReplication(Path path) {\n    return getDefaultReplication();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.wrapException": "  public static IOException wrapException(final String destHost,\n                                          final int destPort,\n                                          final String localHost,\n                                          final int localPort,\n                                          final IOException exception) {\n    if (exception instanceof BindException) {\n      return new BindException(\n          \"Problem binding to [\"\n              + localHost\n              + \":\"\n              + localPort\n              + \"] \"\n              + exception\n              + \";\"\n              + see(\"BindException\"));\n    } else if (exception instanceof ConnectException) {\n      // connection refused; include the host:port in the error\n      return (ConnectException) new ConnectException(\n          \"Call From \"\n              + localHost\n              + \" to \"\n              + destHost\n              + \":\"\n              + destPort\n              + \" failed on connection exception: \"\n              + exception\n              + \";\"\n              + see(\"ConnectionRefused\"))\n          .initCause(exception);\n    } else if (exception instanceof UnknownHostException) {\n      return (UnknownHostException) new UnknownHostException(\n          \"Invalid host name: \"\n              + getHostDetailsAsString(destHost, destPort, localHost)\n              + exception\n              + \";\"\n              + see(\"UnknownHost\"))\n          .initCause(exception);\n    } else if (exception instanceof SocketTimeoutException) {\n      return (SocketTimeoutException) new SocketTimeoutException(\n          \"Call From \"\n              + localHost + \" to \" + destHost + \":\" + destPort\n              + \" failed on socket timeout exception: \" + exception\n              + \";\"\n              + see(\"SocketTimeout\"))\n          .initCause(exception);\n    } else if (exception instanceof NoRouteToHostException) {\n      return (NoRouteToHostException) new NoRouteToHostException(\n          \"No Route to Host from  \"\n              + localHost + \" to \" + destHost + \":\" + destPort\n              + \" failed on socket timeout exception: \" + exception\n              + \";\"\n              + see(\"NoRouteToHost\"))\n          .initCause(exception);\n    }\n    else {\n      return (IOException) new IOException(\"Failed on local exception: \"\n                                               + exception\n                                               + \"; Host Details : \"\n                                               + getHostDetailsAsString(destHost, destPort, localHost))\n          .initCause(exception);\n\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.see": "  private static String see(final String entry) {\n    return FOR_MORE_DETAILS_SEE + HADOOP_WIKI + entry;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.getHostDetailsAsString": "  private static String getHostDetailsAsString(final String destHost,\n                                               final int destPort,\n                                               final String localHost) {\n    StringBuilder hostDetails = new StringBuilder(27);\n    hostDetails.append(\"local host is: \")\n        .append(quoteHost(localHost))\n        .append(\"; \");\n    hostDetails.append(\"destination host is: \").append(quoteHost(destHost))\n        .append(\":\")\n        .append(destPort).append(\"; \");\n    return hostDetails.toString();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convert": "  public static HdfsProtos.ChecksumTypeProto convert(DataChecksum.Type type) {\n    return HdfsProtos.ChecksumTypeProto.valueOf(type.id);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertLocatedBlock": "  public static List<LocatedBlock> convertLocatedBlock(\n      List<LocatedBlockProto> lb) {\n    if (lb == null) return null;\n    final int len = lb.size();\n    List<LocatedBlock> result = \n        new ArrayList<LocatedBlock>(len);\n    for (int i = 0; i < len; ++i) {\n      result.add(PBHelper.convert(lb.get(i)));\n    }\n    return result;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertLocatedBlock2": "  public static List<LocatedBlockProto> convertLocatedBlock2(List<LocatedBlock> lb) {\n    if (lb == null) return null;\n    final int len = lb.size();\n    List<LocatedBlockProto> result = new ArrayList<LocatedBlockProto>(len);\n    for (int i = 0; i < len; ++i) {\n      result.add(PBHelper.convert(lb.get(i)));\n    }\n    return result;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertBlockKeys": "  public static BlockKey[] convertBlockKeys(List<BlockKeyProto> list) {\n    BlockKey[] ret = new BlockKey[list.size()];\n    int i = 0;\n    for (BlockKeyProto k : list) {\n      ret[i++] = convert(k);\n    }\n    return ret;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertCreateFlag": "  public static int convertCreateFlag(EnumSetWritable<CreateFlag> flag) {\n    int value = 0;\n    if (flag.contains(CreateFlag.APPEND)) {\n      value |= CreateFlagProto.APPEND.getNumber();\n    }\n    if (flag.contains(CreateFlag.CREATE)) {\n      value |= CreateFlagProto.CREATE.getNumber();\n    }\n    if (flag.contains(CreateFlag.OVERWRITE)) {\n      value |= CreateFlagProto.OVERWRITE.getNumber();\n    }\n    return value;\n  }"
        },
        "bug_report": {
            "Title": "Create file failure when the machine of first attempted NameNode is down",
            "Description": "test Environment: NN1,NN2,DN1,DN2,DN3\nmachine1:NN1,DN1\nmachine2:NN2,DN2\nmachine3:DN3\n\nmathine1 is down.\n\n2013-01-12 09:51:21,248 DEBUG ipc.Client (Client.java:setupIOstreams(562)) - Connecting to /160.161.0.155:8020\n2013-01-12 09:51:38,442 DEBUG ipc.Client (Client.java:close(932)) - closing ipc connection to vm2/160.161.0.155:8020: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]\njava.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]\n at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:524)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:489)\n at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:474)\n at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:568)\n at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:217)\n at org.apache.hadoop.ipc.Client.getConnection(Client.java:1286)\n at org.apache.hadoop.ipc.Client.call(Client.java:1156)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:184)\n at $Proxy9.create(Unknown Source)\n at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:187)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n at java.lang.reflect.Method.invoke(Method.java:597)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)\n at $Proxy10.create(Unknown Source)\n at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:1261)\n at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1280)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1128)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1086)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:232)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:75)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:806)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:715)\n at test.TestLease.main(TestLease.java:45)\n2013-01-12 09:51:38,443 DEBUG ipc.Client (Client.java:close(940)) - IPC Client (31594013) connection to /160.161.0.155:8020 from hdfs/hadoop@HADOOP.COM: closed\n2013-01-12 09:52:47,834 WARN  retry.RetryInvocationHandler (RetryInvocationHandler.java:invoke(95)) - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create. Not retrying because the invoked method is not idempotent, and unable to determine whether it was invoked\njava.net.SocketTimeoutException: Call From szxy1x001833091/172.0.0.13 to vm2:8020 failed on socket timeout exception: java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout\n at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:743)\n at org.apache.hadoop.ipc.Client.call(Client.java:1180)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:184)\n at $Proxy9.create(Unknown Source)\n at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:187)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n at java.lang.reflect.Method.invoke(Method.java:597)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)\n at $Proxy10.create(Unknown Source)\n at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:1261)\n at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1280)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1128)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1086)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:232)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:75)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:806)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:715)\n at test.TestLease.main(TestLease.java:45)\nCaused by: java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]\n at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:524)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:489)\n at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:474)\n at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:568)\n at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:217)\n at org.apache.hadoop.ipc.Client.getConnection(Client.java:1286)\n at org.apache.hadoop.ipc.Client.call(Client.java:1156)\n ... 20 more\njava.net.SocketTimeoutException: Call From szxy1x001833091/172.0.0.13 to vm2:8020 failed on socket timeout exception: java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout\n at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:743)\n at org.apache.hadoop.ipc.Client.call(Client.java:1180)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:184)\n at $Proxy9.create(Unknown Source)\n at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:187)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n at java.lang.reflect.Method.invoke(Method.java:597)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)\n at $Proxy10.create(Unknown Source)\n at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:1261)\n at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1280)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1128)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1086)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:232)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:75)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:806)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:715)\n at test.TestLease.main(TestLease.java:45)\nCaused by: java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]\n at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:524)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:489)\n at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:474)\n at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:568)\n at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:217)\n at org.apache.hadoop.ipc.Client.getConnection(Client.java:1286)\n at org.apache.hadoop.ipc.Client.call(Client.java:1156)\n ... 20 more\n2013-01-12 09:54:52,269 DEBUG ipc.Client (Client.java:stop(1021)) - Stopping client\n"
        }
    },
    {
        "filename": "HDFS-8276.json",
        "creation_time": "2015-04-28T10:11:35.000+0000",
        "stack_trace": "```\njava.lang.IllegalArgumentException: dfs.namenode.lazypersist.file.scrub.interval.sec must be non-zero.\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:828)```",
        "source_code": {},
        "bug_report": {
            "Title": "LazyPersistFileScrubber should be disabled if scrubber interval configured zero",
            "Description": "bq. but I think it is simple enough to change the meaning of the value so that zero means 'never scrub'. Let me post an updated patch.\n\nAs discussed in [HDFS-6929|https://issues.apache.org/jira/browse/HDFS-6929], scrubber should be disable if *dfs.namenode.lazypersist.file.scrub.interval.sec* is zero.\n\nCurrently namenode startup is failing if interval configured zero\n\n{code}\n2015-04-27 23:47:31,744 ERROR org.apache.hadoop.hdfs.server.namenode.FSNamesystem: FSNamesystem initialization failed.\njava.lang.IllegalArgumentException: dfs.namenode.lazypersist.file.scrub.interval.sec must be non-zero.\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:828)\n{code}"
        }
    },
    {
        "filename": "HDFS-12369.json",
        "creation_time": "2017-08-28T22:56:02.000+0000",
        "stack_trace": "```\njava.io.FileNotFoundException: File does not exist: /home/Events/CancellationSurvey_MySQL/2015/12/31/.part-00000.9nlJ3M\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:66)\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:429)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:232)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:141)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:897)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:750)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:318)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1125)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:789)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:614)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:676)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:844)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:823)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1547)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1615)```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf": "  public static INodeFile valueOf(INode inode, String path, boolean acceptNull)\n      throws FileNotFoundException {\n    if (inode == null) {\n      if (acceptNull) {\n        return null;\n      } else {\n        throw new FileNotFoundException(\"File does not exist: \" + path);\n      }\n    }\n    if (!inode.isFile()) {\n      throw new FileNotFoundException(\"Path is not a file: \" + path);\n    }\n    return inode.asFile();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.asFile": "  public final INodeFile asFile() {\n    return this;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.isFile": "  public final boolean isFile() {\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp": "  private long applyEditLogOp(FSEditLogOp op, FSDirectory fsDir,\n      StartupOption startOpt, int logVersion, long lastInodeId) throws IOException {\n    long inodeId = HdfsConstants.GRANDFATHER_INODE_ID;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"replaying edit log: \" + op);\n    }\n    final boolean toAddRetryCache = fsNamesys.hasRetryCache() && op.hasRpcIds();\n\n    switch (op.opCode) {\n    case OP_ADD: {\n      AddCloseOp addCloseOp = (AddCloseOp)op;\n      final String path =\n          renameReservedPathsOnUpgrade(addCloseOp.path, logVersion);\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + path +\n            \" numblocks : \" + addCloseOp.blocks.length +\n            \" clientHolder \" + addCloseOp.clientName +\n            \" clientMachine \" + addCloseOp.clientMachine);\n      }\n      // There are 3 cases here:\n      // 1. OP_ADD to create a new file\n      // 2. OP_ADD to update file blocks\n      // 3. OP_ADD to open file for append (old append)\n\n      // See if the file already exists (persistBlocks call)\n      INodesInPath iip = fsDir.getINodesInPath(path, DirOp.WRITE);\n      INodeFile oldFile = INodeFile.valueOf(iip.getLastINode(), path, true);\n      if (oldFile != null && addCloseOp.overwrite) {\n        // This is OP_ADD with overwrite\n        FSDirDeleteOp.deleteForEditLog(fsDir, iip, addCloseOp.mtime);\n        iip = INodesInPath.replace(iip, iip.length() - 1, null);\n        oldFile = null;\n      }\n      INodeFile newFile = oldFile;\n      if (oldFile == null) { // this is OP_ADD on a new file (case 1)\n        // versions > 0 support per file replication\n        // get name and replication\n        final short replication = fsNamesys.getBlockManager()\n            .adjustReplication(addCloseOp.replication);\n        assert addCloseOp.blocks.length == 0;\n\n        // add to the file tree\n        inodeId = getAndUpdateLastInodeId(addCloseOp.inodeId, logVersion, lastInodeId);\n        newFile = FSDirWriteFileOp.addFileForEditLog(fsDir, inodeId,\n            iip.getExistingINodes(), iip.getLastLocalName(),\n            addCloseOp.permissions, addCloseOp.aclEntries,\n            addCloseOp.xAttrs, replication, addCloseOp.mtime,\n            addCloseOp.atime, addCloseOp.blockSize, true,\n            addCloseOp.clientName, addCloseOp.clientMachine,\n            addCloseOp.storagePolicyId);\n        assert newFile != null;\n        iip = INodesInPath.replace(iip, iip.length() - 1, newFile);\n        fsNamesys.leaseManager.addLease(addCloseOp.clientName, newFile.getId());\n\n        // add the op into retry cache if necessary\n        if (toAddRetryCache) {\n          HdfsFileStatus stat =\n              FSDirStatAndListingOp.createFileStatusForEditLog(fsDir, iip);\n          fsNamesys.addCacheEntryWithPayload(addCloseOp.rpcClientId,\n              addCloseOp.rpcCallId, stat);\n        }\n      } else { // This is OP_ADD on an existing file (old append)\n        if (!oldFile.isUnderConstruction()) {\n          // This is case 3: a call to append() on an already-closed file.\n          if (FSNamesystem.LOG.isDebugEnabled()) {\n            FSNamesystem.LOG.debug(\"Reopening an already-closed file \" +\n                \"for append\");\n          }\n          LocatedBlock lb = FSDirAppendOp.prepareFileForAppend(fsNamesys, iip,\n              addCloseOp.clientName, addCloseOp.clientMachine, false, false,\n              false);\n          // add the op into retry cache if necessary\n          if (toAddRetryCache) {\n            HdfsFileStatus stat =\n                FSDirStatAndListingOp.createFileStatusForEditLog(fsDir, iip);\n            fsNamesys.addCacheEntryWithPayload(addCloseOp.rpcClientId,\n                addCloseOp.rpcCallId, new LastBlockWithStatus(lb, stat));\n          }\n        }\n      }\n      // Fall-through for case 2.\n      // Regardless of whether it's a new file or an updated file,\n      // update the block list.\n      \n      // Update the salient file attributes.\n      newFile.setAccessTime(addCloseOp.atime, Snapshot.CURRENT_STATE_ID);\n      newFile.setModificationTime(addCloseOp.mtime, Snapshot.CURRENT_STATE_ID);\n      ErasureCodingPolicy ecPolicy =\n          FSDirErasureCodingOp.unprotectedGetErasureCodingPolicy(\n              fsDir.getFSNamesystem(), iip);\n      updateBlocks(fsDir, addCloseOp, iip, newFile, ecPolicy);\n      break;\n    }\n    case OP_CLOSE: {\n      AddCloseOp addCloseOp = (AddCloseOp)op;\n      final String path =\n          renameReservedPathsOnUpgrade(addCloseOp.path, logVersion);\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + path +\n            \" numblocks : \" + addCloseOp.blocks.length +\n            \" clientHolder \" + addCloseOp.clientName +\n            \" clientMachine \" + addCloseOp.clientMachine);\n      }\n\n      final INodesInPath iip = fsDir.getINodesInPath(path, DirOp.READ);\n      final INodeFile file = INodeFile.valueOf(iip.getLastINode(), path);\n\n      // Update the salient file attributes.\n      file.setAccessTime(addCloseOp.atime, Snapshot.CURRENT_STATE_ID);\n      file.setModificationTime(addCloseOp.mtime, Snapshot.CURRENT_STATE_ID);\n      ErasureCodingPolicy ecPolicy =\n          FSDirErasureCodingOp.unprotectedGetErasureCodingPolicy(\n              fsDir.getFSNamesystem(), iip);\n      updateBlocks(fsDir, addCloseOp, iip, file, ecPolicy);\n\n      // Now close the file\n      if (!file.isUnderConstruction() &&\n          logVersion <= LayoutVersion.BUGFIX_HDFS_2991_VERSION) {\n        // There was a bug (HDFS-2991) in hadoop < 0.23.1 where OP_CLOSE\n        // could show up twice in a row. But after that version, this\n        // should be fixed, so we should treat it as an error.\n        throw new IOException(\n            \"File is not under construction: \" + path);\n      }\n      // One might expect that you could use removeLease(holder, path) here,\n      // but OP_CLOSE doesn't serialize the holder. So, remove the inode.\n      if (file.isUnderConstruction()) {\n        fsNamesys.getLeaseManager().removeLease(file.getId());\n        file.toCompleteFile(file.getModificationTime(), 0,\n            fsNamesys.getBlockManager().getMinReplication());\n      }\n      break;\n    }\n    case OP_APPEND: {\n      AppendOp appendOp = (AppendOp) op;\n      final String path = renameReservedPathsOnUpgrade(appendOp.path,\n          logVersion);\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + path +\n            \" clientName \" + appendOp.clientName +\n            \" clientMachine \" + appendOp.clientMachine +\n            \" newBlock \" + appendOp.newBlock);\n      }\n      INodesInPath iip = fsDir.getINodesInPath(path, DirOp.WRITE);\n      INodeFile file = INodeFile.valueOf(iip.getLastINode(), path);\n      if (!file.isUnderConstruction()) {\n        LocatedBlock lb = FSDirAppendOp.prepareFileForAppend(fsNamesys, iip,\n            appendOp.clientName, appendOp.clientMachine, appendOp.newBlock,\n            false, false);\n        // add the op into retry cache if necessary\n        if (toAddRetryCache) {\n          HdfsFileStatus stat =\n              FSDirStatAndListingOp.createFileStatusForEditLog(fsDir, iip);\n          fsNamesys.addCacheEntryWithPayload(appendOp.rpcClientId,\n              appendOp.rpcCallId, new LastBlockWithStatus(lb, stat));\n        }\n      }\n      break;\n    }\n    case OP_UPDATE_BLOCKS: {\n      UpdateBlocksOp updateOp = (UpdateBlocksOp)op;\n      final String path =\n          renameReservedPathsOnUpgrade(updateOp.path, logVersion);\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + path +\n            \" numblocks : \" + updateOp.blocks.length);\n      }\n      INodesInPath iip = fsDir.getINodesInPath(path, DirOp.READ);\n      INodeFile oldFile = INodeFile.valueOf(iip.getLastINode(), path);\n      // Update in-memory data structures\n      ErasureCodingPolicy ecPolicy =\n          FSDirErasureCodingOp.unprotectedGetErasureCodingPolicy(\n              fsDir.getFSNamesystem(), iip);\n      updateBlocks(fsDir, updateOp, iip, oldFile, ecPolicy);\n\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(updateOp.rpcClientId, updateOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_ADD_BLOCK: {\n      AddBlockOp addBlockOp = (AddBlockOp) op;\n      String path = renameReservedPathsOnUpgrade(addBlockOp.getPath(), logVersion);\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + path +\n            \" new block id : \" + addBlockOp.getLastBlock().getBlockId());\n      }\n      INodesInPath iip = fsDir.getINodesInPath(path, DirOp.READ);\n      INodeFile oldFile = INodeFile.valueOf(iip.getLastINode(), path);\n      // add the new block to the INodeFile\n      ErasureCodingPolicy ecPolicy =\n          FSDirErasureCodingOp.unprotectedGetErasureCodingPolicy(\n              fsDir.getFSNamesystem(), iip);\n      addNewBlock(addBlockOp, oldFile, ecPolicy);\n      break;\n    }\n    case OP_SET_REPLICATION: {\n      SetReplicationOp setReplicationOp = (SetReplicationOp)op;\n      String src = renameReservedPathsOnUpgrade(\n          setReplicationOp.path, logVersion);\n      INodesInPath iip = fsDir.getINodesInPath(src, DirOp.WRITE);\n      short replication = fsNamesys.getBlockManager().adjustReplication(\n          setReplicationOp.replication);\n      FSDirAttrOp.unprotectedSetReplication(fsDir, iip, replication);\n      break;\n    }\n    case OP_CONCAT_DELETE: {\n      ConcatDeleteOp concatDeleteOp = (ConcatDeleteOp)op;\n      String trg = renameReservedPathsOnUpgrade(concatDeleteOp.trg, logVersion);\n      String[] srcs = new String[concatDeleteOp.srcs.length];\n      for (int i=0; i<srcs.length; i++) {\n        srcs[i] =\n            renameReservedPathsOnUpgrade(concatDeleteOp.srcs[i], logVersion);\n      }\n      INodesInPath targetIIP = fsDir.getINodesInPath(trg, DirOp.WRITE);\n      INodeFile[] srcFiles = new INodeFile[srcs.length];\n      for (int i = 0; i < srcs.length; i++) {\n        INodesInPath srcIIP = fsDir.getINodesInPath(srcs[i], DirOp.WRITE);\n        srcFiles[i] = srcIIP.getLastINode().asFile();\n      }\n      FSDirConcatOp.unprotectedConcat(fsDir, targetIIP, srcFiles,\n          concatDeleteOp.timestamp);\n      \n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(concatDeleteOp.rpcClientId,\n            concatDeleteOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_RENAME_OLD: {\n      RenameOldOp renameOp = (RenameOldOp)op;\n      final String src = renameReservedPathsOnUpgrade(renameOp.src, logVersion);\n      final String dst = renameReservedPathsOnUpgrade(renameOp.dst, logVersion);\n      FSDirRenameOp.renameForEditLog(fsDir, src, dst, renameOp.timestamp);\n      \n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(renameOp.rpcClientId, renameOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_DELETE: {\n      DeleteOp deleteOp = (DeleteOp)op;\n      final String src = renameReservedPathsOnUpgrade(\n          deleteOp.path, logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(src, DirOp.WRITE_LINK);\n      FSDirDeleteOp.deleteForEditLog(fsDir, iip, deleteOp.timestamp);\n\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(deleteOp.rpcClientId, deleteOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_MKDIR: {\n      MkdirOp mkdirOp = (MkdirOp)op;\n      inodeId = getAndUpdateLastInodeId(mkdirOp.inodeId, logVersion,\n          lastInodeId);\n      FSDirMkdirOp.mkdirForEditLog(fsDir, inodeId,\n          renameReservedPathsOnUpgrade(mkdirOp.path, logVersion),\n          mkdirOp.permissions, mkdirOp.aclEntries, mkdirOp.timestamp);\n      break;\n    }\n    case OP_SET_GENSTAMP_V1: {\n      SetGenstampV1Op setGenstampV1Op = (SetGenstampV1Op)op;\n      blockManager.getBlockIdManager().setLegacyGenerationStamp(\n          setGenstampV1Op.genStampV1);\n      break;\n    }\n    case OP_SET_PERMISSIONS: {\n      SetPermissionsOp setPermissionsOp = (SetPermissionsOp)op;\n      final String src =\n          renameReservedPathsOnUpgrade(setPermissionsOp.src, logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(src, DirOp.WRITE);\n      FSDirAttrOp.unprotectedSetPermission(fsDir, iip,\n          setPermissionsOp.permissions);\n      break;\n    }\n    case OP_SET_OWNER: {\n      SetOwnerOp setOwnerOp = (SetOwnerOp)op;\n      final String src = renameReservedPathsOnUpgrade(\n          setOwnerOp.src, logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(src, DirOp.WRITE);\n      FSDirAttrOp.unprotectedSetOwner(fsDir, iip,\n          setOwnerOp.username, setOwnerOp.groupname);\n      break;\n    }\n    case OP_SET_NS_QUOTA: {\n      SetNSQuotaOp setNSQuotaOp = (SetNSQuotaOp)op;\n      final String src = renameReservedPathsOnUpgrade(\n          setNSQuotaOp.src, logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(src, DirOp.WRITE);\n      FSDirAttrOp.unprotectedSetQuota(fsDir, iip,\n          setNSQuotaOp.nsQuota, HdfsConstants.QUOTA_DONT_SET, null);\n      break;\n    }\n    case OP_CLEAR_NS_QUOTA: {\n      ClearNSQuotaOp clearNSQuotaOp = (ClearNSQuotaOp)op;\n      final String src = renameReservedPathsOnUpgrade(\n          clearNSQuotaOp.src, logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(src, DirOp.WRITE);\n      FSDirAttrOp.unprotectedSetQuota(fsDir, iip,\n          HdfsConstants.QUOTA_RESET, HdfsConstants.QUOTA_DONT_SET, null);\n      break;\n    }\n    case OP_SET_QUOTA: {\n      SetQuotaOp setQuotaOp = (SetQuotaOp) op;\n      final String src = renameReservedPathsOnUpgrade(\n          setQuotaOp.src, logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(src, DirOp.WRITE);\n      FSDirAttrOp.unprotectedSetQuota(fsDir, iip,\n          setQuotaOp.nsQuota, setQuotaOp.dsQuota, null);\n      break;\n    }\n    case OP_SET_QUOTA_BY_STORAGETYPE: {\n      FSEditLogOp.SetQuotaByStorageTypeOp setQuotaByStorageTypeOp =\n          (FSEditLogOp.SetQuotaByStorageTypeOp) op;\n      final String src = renameReservedPathsOnUpgrade(\n          setQuotaByStorageTypeOp.src, logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(src, DirOp.WRITE);\n      FSDirAttrOp.unprotectedSetQuota(fsDir, iip,\n          HdfsConstants.QUOTA_DONT_SET, setQuotaByStorageTypeOp.dsQuota,\n          setQuotaByStorageTypeOp.type);\n      break;\n    }\n    case OP_TIMES: {\n      TimesOp timesOp = (TimesOp)op;\n      final String src = renameReservedPathsOnUpgrade(\n          timesOp.path, logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(src, DirOp.WRITE);\n      FSDirAttrOp.unprotectedSetTimes(fsDir, iip,\n          timesOp.mtime, timesOp.atime, true);\n      break;\n    }\n    case OP_SYMLINK: {\n      if (!FileSystem.areSymlinksEnabled()) {\n        throw new IOException(\"Symlinks not supported - please remove symlink before upgrading to this version of HDFS\");\n      }\n      SymlinkOp symlinkOp = (SymlinkOp)op;\n      inodeId = getAndUpdateLastInodeId(symlinkOp.inodeId, logVersion,\n          lastInodeId);\n      final String path = renameReservedPathsOnUpgrade(symlinkOp.path,\n          logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(path, DirOp.WRITE_LINK);\n      FSDirSymlinkOp.unprotectedAddSymlink(fsDir, iip.getExistingINodes(),\n          iip.getLastLocalName(), inodeId, symlinkOp.value, symlinkOp.mtime,\n          symlinkOp.atime, symlinkOp.permissionStatus);\n      \n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(symlinkOp.rpcClientId, symlinkOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_RENAME: {\n      RenameOp renameOp = (RenameOp)op;\n      FSDirRenameOp.renameForEditLog(fsDir,\n          renameReservedPathsOnUpgrade(renameOp.src, logVersion),\n          renameReservedPathsOnUpgrade(renameOp.dst, logVersion),\n          renameOp.timestamp, renameOp.options);\n      \n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(renameOp.rpcClientId, renameOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_GET_DELEGATION_TOKEN: {\n      GetDelegationTokenOp getDelegationTokenOp\n        = (GetDelegationTokenOp)op;\n\n      fsNamesys.getDelegationTokenSecretManager()\n        .addPersistedDelegationToken(getDelegationTokenOp.token,\n                                     getDelegationTokenOp.expiryTime);\n      break;\n    }\n    case OP_RENEW_DELEGATION_TOKEN: {\n      RenewDelegationTokenOp renewDelegationTokenOp\n        = (RenewDelegationTokenOp)op;\n      fsNamesys.getDelegationTokenSecretManager()\n        .updatePersistedTokenRenewal(renewDelegationTokenOp.token,\n                                     renewDelegationTokenOp.expiryTime);\n      break;\n    }\n    case OP_CANCEL_DELEGATION_TOKEN: {\n      CancelDelegationTokenOp cancelDelegationTokenOp\n        = (CancelDelegationTokenOp)op;\n      fsNamesys.getDelegationTokenSecretManager()\n          .updatePersistedTokenCancellation(\n              cancelDelegationTokenOp.token);\n      break;\n    }\n    case OP_UPDATE_MASTER_KEY: {\n      UpdateMasterKeyOp updateMasterKeyOp = (UpdateMasterKeyOp)op;\n      fsNamesys.getDelegationTokenSecretManager()\n        .updatePersistedMasterKey(updateMasterKeyOp.key);\n      break;\n    }\n    case OP_REASSIGN_LEASE: {\n      ReassignLeaseOp reassignLeaseOp = (ReassignLeaseOp)op;\n\n      Lease lease = fsNamesys.leaseManager.getLease(\n          reassignLeaseOp.leaseHolder);\n      final String path =\n          renameReservedPathsOnUpgrade(reassignLeaseOp.path, logVersion);\n      INodeFile pendingFile = fsDir.getINode(path, DirOp.READ).asFile();\n      Preconditions.checkState(pendingFile.isUnderConstruction());\n      fsNamesys.reassignLeaseInternal(lease, reassignLeaseOp.newHolder,\n              pendingFile);\n      break;\n    }\n    case OP_START_LOG_SEGMENT:\n    case OP_END_LOG_SEGMENT: {\n      // no data in here currently.\n      break;\n    }\n    case OP_CREATE_SNAPSHOT: {\n      CreateSnapshotOp createSnapshotOp = (CreateSnapshotOp) op;\n      final String snapshotRoot =\n          renameReservedPathsOnUpgrade(createSnapshotOp.snapshotRoot,\n              logVersion);\n      INodesInPath iip = fsDir.getINodesInPath(snapshotRoot, DirOp.WRITE);\n      String path = fsNamesys.getSnapshotManager().createSnapshot(\n          fsDir.getFSNamesystem().getLeaseManager(),\n          iip, snapshotRoot, createSnapshotOp.snapshotName);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntryWithPayload(createSnapshotOp.rpcClientId,\n            createSnapshotOp.rpcCallId, path);\n      }\n      break;\n    }\n    case OP_DELETE_SNAPSHOT: {\n      DeleteSnapshotOp deleteSnapshotOp = (DeleteSnapshotOp) op;\n      BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n      List<INode> removedINodes = new ChunkedArrayList<INode>();\n      final String snapshotRoot =\n          renameReservedPathsOnUpgrade(deleteSnapshotOp.snapshotRoot,\n              logVersion);\n      INodesInPath iip = fsDir.getINodesInPath(snapshotRoot, DirOp.WRITE);\n      fsNamesys.getSnapshotManager().deleteSnapshot(iip,\n          deleteSnapshotOp.snapshotName,\n          new INode.ReclaimContext(fsNamesys.dir.getBlockStoragePolicySuite(),\n              collectedBlocks, removedINodes, null));\n      fsNamesys.getBlockManager().removeBlocksAndUpdateSafemodeTotal(\n          collectedBlocks);\n      collectedBlocks.clear();\n      fsNamesys.dir.removeFromInodeMap(removedINodes);\n      removedINodes.clear();\n\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(deleteSnapshotOp.rpcClientId,\n            deleteSnapshotOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_RENAME_SNAPSHOT: {\n      RenameSnapshotOp renameSnapshotOp = (RenameSnapshotOp) op;\n      final String snapshotRoot =\n          renameReservedPathsOnUpgrade(renameSnapshotOp.snapshotRoot,\n              logVersion);\n      INodesInPath iip = fsDir.getINodesInPath(snapshotRoot, DirOp.WRITE);\n      fsNamesys.getSnapshotManager().renameSnapshot(iip,\n          snapshotRoot, renameSnapshotOp.snapshotOldName,\n          renameSnapshotOp.snapshotNewName);\n      \n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(renameSnapshotOp.rpcClientId,\n            renameSnapshotOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_ALLOW_SNAPSHOT: {\n      AllowSnapshotOp allowSnapshotOp = (AllowSnapshotOp) op;\n      final String snapshotRoot =\n          renameReservedPathsOnUpgrade(allowSnapshotOp.snapshotRoot, logVersion);\n      fsNamesys.getSnapshotManager().setSnapshottable(\n          snapshotRoot, false);\n      break;\n    }\n    case OP_DISALLOW_SNAPSHOT: {\n      DisallowSnapshotOp disallowSnapshotOp = (DisallowSnapshotOp) op;\n      final String snapshotRoot =\n          renameReservedPathsOnUpgrade(disallowSnapshotOp.snapshotRoot,\n              logVersion);\n      fsNamesys.getSnapshotManager().resetSnapshottable(\n          snapshotRoot);\n      break;\n    }\n    case OP_SET_GENSTAMP_V2: {\n      SetGenstampV2Op setGenstampV2Op = (SetGenstampV2Op) op;\n      blockManager.getBlockIdManager().setGenerationStamp(\n          setGenstampV2Op.genStampV2);\n      break;\n    }\n    case OP_ALLOCATE_BLOCK_ID: {\n      AllocateBlockIdOp allocateBlockIdOp = (AllocateBlockIdOp) op;\n      if (BlockIdManager.isStripedBlockID(allocateBlockIdOp.blockId)) {\n        // ALLOCATE_BLOCK_ID is added for sequential block id, thus if the id\n        // is negative, it must belong to striped blocks\n        blockManager.getBlockIdManager().setLastAllocatedStripedBlockId(\n            allocateBlockIdOp.blockId);\n      } else {\n        blockManager.getBlockIdManager().setLastAllocatedContiguousBlockId(\n            allocateBlockIdOp.blockId);\n      }\n      break;\n    }\n    case OP_ROLLING_UPGRADE_START: {\n      if (startOpt == StartupOption.ROLLINGUPGRADE) {\n        final RollingUpgradeStartupOption rollingUpgradeOpt\n            = startOpt.getRollingUpgradeStartupOption(); \n        if (rollingUpgradeOpt == RollingUpgradeStartupOption.ROLLBACK) {\n          throw new RollingUpgradeOp.RollbackException();\n        }\n      }\n      // start rolling upgrade\n      final long startTime = ((RollingUpgradeOp) op).getTime();\n      fsNamesys.startRollingUpgradeInternal(startTime);\n      fsNamesys.triggerRollbackCheckpoint();\n      break;\n    }\n    case OP_ROLLING_UPGRADE_FINALIZE: {\n      final long finalizeTime = ((RollingUpgradeOp) op).getTime();\n      if (fsNamesys.isRollingUpgrade()) {\n        // Only do it when NN is actually doing rolling upgrade.\n        // We can get FINALIZE without corresponding START, if NN is restarted\n        // before this op is consumed and a new checkpoint is created.\n        fsNamesys.finalizeRollingUpgradeInternal(finalizeTime);\n      }\n      fsNamesys.getFSImage().updateStorageVersion();\n      fsNamesys.getFSImage().renameCheckpoint(NameNodeFile.IMAGE_ROLLBACK,\n          NameNodeFile.IMAGE);\n      break;\n    }\n    case OP_ADD_CACHE_DIRECTIVE: {\n      AddCacheDirectiveInfoOp addOp = (AddCacheDirectiveInfoOp) op;\n      CacheDirectiveInfo result = fsNamesys.\n          getCacheManager().addDirectiveFromEditLog(addOp.directive);\n      if (toAddRetryCache) {\n        Long id = result.getId();\n        fsNamesys.addCacheEntryWithPayload(op.rpcClientId, op.rpcCallId, id);\n      }\n      break;\n    }\n    case OP_MODIFY_CACHE_DIRECTIVE: {\n      ModifyCacheDirectiveInfoOp modifyOp =\n          (ModifyCacheDirectiveInfoOp) op;\n      fsNamesys.getCacheManager().modifyDirectiveFromEditLog(\n          modifyOp.directive);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(op.rpcClientId, op.rpcCallId);\n      }\n      break;\n    }\n    case OP_REMOVE_CACHE_DIRECTIVE: {\n      RemoveCacheDirectiveInfoOp removeOp =\n          (RemoveCacheDirectiveInfoOp) op;\n      fsNamesys.getCacheManager().removeDirective(removeOp.id, null);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(op.rpcClientId, op.rpcCallId);\n      }\n      break;\n    }\n    case OP_ADD_CACHE_POOL: {\n      AddCachePoolOp addOp = (AddCachePoolOp) op;\n      fsNamesys.getCacheManager().addCachePool(addOp.info);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(op.rpcClientId, op.rpcCallId);\n      }\n      break;\n    }\n    case OP_MODIFY_CACHE_POOL: {\n      ModifyCachePoolOp modifyOp = (ModifyCachePoolOp) op;\n      fsNamesys.getCacheManager().modifyCachePool(modifyOp.info);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(op.rpcClientId, op.rpcCallId);\n      }\n      break;\n    }\n    case OP_REMOVE_CACHE_POOL: {\n      RemoveCachePoolOp removeOp = (RemoveCachePoolOp) op;\n      fsNamesys.getCacheManager().removeCachePool(removeOp.poolName);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(op.rpcClientId, op.rpcCallId);\n      }\n      break;\n    }\n    case OP_SET_ACL: {\n      SetAclOp setAclOp = (SetAclOp) op;\n      INodesInPath iip = fsDir.getINodesInPath(setAclOp.src, DirOp.WRITE);\n      FSDirAclOp.unprotectedSetAcl(fsDir, iip, setAclOp.aclEntries, true);\n      break;\n    }\n    case OP_SET_XATTR: {\n      SetXAttrOp setXAttrOp = (SetXAttrOp) op;\n      INodesInPath iip = fsDir.getINodesInPath(setXAttrOp.src, DirOp.WRITE);\n      FSDirXAttrOp.unprotectedSetXAttrs(fsDir, iip,\n                                        setXAttrOp.xAttrs,\n                                        EnumSet.of(XAttrSetFlag.CREATE,\n                                                   XAttrSetFlag.REPLACE));\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(setXAttrOp.rpcClientId, setXAttrOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_REMOVE_XATTR: {\n      RemoveXAttrOp removeXAttrOp = (RemoveXAttrOp) op;\n      INodesInPath iip = fsDir.getINodesInPath(removeXAttrOp.src, DirOp.WRITE);\n      FSDirXAttrOp.unprotectedRemoveXAttrs(fsDir, iip,\n                                           removeXAttrOp.xAttrs);\n      if (toAddRetryCache) {\n        fsNamesys.addCacheEntry(removeXAttrOp.rpcClientId,\n            removeXAttrOp.rpcCallId);\n      }\n      break;\n    }\n    case OP_TRUNCATE: {\n      TruncateOp truncateOp = (TruncateOp) op;\n      INodesInPath iip = fsDir.getINodesInPath(truncateOp.src, DirOp.WRITE);\n      FSDirTruncateOp.unprotectedTruncate(fsNamesys, iip,\n          truncateOp.clientName, truncateOp.clientMachine,\n          truncateOp.newLength, truncateOp.timestamp, truncateOp.truncateBlock);\n      break;\n    }\n    case OP_SET_STORAGE_POLICY: {\n      SetStoragePolicyOp setStoragePolicyOp = (SetStoragePolicyOp) op;\n      final String path = renameReservedPathsOnUpgrade(setStoragePolicyOp.path,\n          logVersion);\n      final INodesInPath iip = fsDir.getINodesInPath(path, DirOp.WRITE);\n      FSDirAttrOp.unprotectedSetStoragePolicy(\n          fsDir, fsNamesys.getBlockManager(), iip,\n          setStoragePolicyOp.policyId);\n      break;\n    }\n    default:\n      throw new IOException(\"Invalid operation read \" + op.opCode);\n    }\n    return inodeId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.getAndUpdateLastInodeId": "  private long getAndUpdateLastInodeId(long inodeIdFromOp, int logVersion,\n      long lastInodeId) throws IOException {\n    long inodeId = inodeIdFromOp;\n\n    if (inodeId == HdfsConstants.GRANDFATHER_INODE_ID) {\n      if (NameNodeLayoutVersion.supports(\n          LayoutVersion.Feature.ADD_INODE_ID, logVersion)) {\n        throw new IOException(\"The layout version \" + logVersion\n            + \" supports inodeId but gave bogus inodeId\");\n      }\n      inodeId = fsNamesys.dir.allocateNewInodeId();\n    } else {\n      // need to reset lastInodeId. fsnamesys gets lastInodeId firstly from\n      // fsimage but editlog captures more recent inodeId allocations\n      if (inodeId > lastInodeId) {\n        fsNamesys.dir.resetLastInodeId(inodeId);\n      }\n    }\n    return inodeId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.addNewBlock": "  private void addNewBlock(AddBlockOp op, INodeFile file,\n      ErasureCodingPolicy ecPolicy) throws IOException {\n    BlockInfo[] oldBlocks = file.getBlocks();\n    Block pBlock = op.getPenultimateBlock();\n    Block newBlock= op.getLastBlock();\n    \n    if (pBlock != null) { // the penultimate block is not null\n      assert oldBlocks != null && oldBlocks.length > 0;\n      // compare pBlock with the last block of oldBlocks\n      BlockInfo oldLastBlock = oldBlocks[oldBlocks.length - 1];\n      if (oldLastBlock.getBlockId() != pBlock.getBlockId()\n          || oldLastBlock.getGenerationStamp() != pBlock.getGenerationStamp()) {\n        throw new IOException(\n            \"Mismatched block IDs or generation stamps for the old last block of file \"\n                + op.getPath() + \", the old last block is \" + oldLastBlock\n                + \", and the block read from editlog is \" + pBlock);\n      }\n      \n      oldLastBlock.setNumBytes(pBlock.getNumBytes());\n      if (!oldLastBlock.isComplete()) {\n        fsNamesys.getBlockManager().forceCompleteBlock(oldLastBlock);\n        fsNamesys.getBlockManager().processQueuedMessagesForBlock(pBlock);\n      }\n    } else { // the penultimate block is null\n      Preconditions.checkState(oldBlocks == null || oldBlocks.length == 0);\n    }\n    // add the new block\n    final BlockInfo newBlockInfo;\n    boolean isStriped = ecPolicy != null;\n    if (isStriped) {\n      newBlockInfo = new BlockInfoStriped(newBlock, ecPolicy);\n    } else {\n      newBlockInfo = new BlockInfoContiguous(newBlock,\n          file.getPreferredBlockReplication());\n    }\n    newBlockInfo.convertToBlockUnderConstruction(\n        BlockUCState.UNDER_CONSTRUCTION, null);\n    fsNamesys.getBlockManager().addBlockCollectionWithCheck(newBlockInfo, file);\n    file.addBlock(newBlockInfo);\n    fsNamesys.getBlockManager().processQueuedMessagesForBlock(newBlock);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.updateBlocks": "  private void updateBlocks(FSDirectory fsDir, BlockListUpdatingOp op,\n      INodesInPath iip, INodeFile file, ErasureCodingPolicy ecPolicy)\n      throws IOException {\n    // Update its block list\n    BlockInfo[] oldBlocks = file.getBlocks();\n    Block[] newBlocks = op.getBlocks();\n    String path = op.getPath();\n    \n    // Are we only updating the last block's gen stamp.\n    boolean isGenStampUpdate = oldBlocks.length == newBlocks.length;\n    \n    // First, update blocks in common\n    for (int i = 0; i < oldBlocks.length && i < newBlocks.length; i++) {\n      BlockInfo oldBlock = oldBlocks[i];\n      Block newBlock = newBlocks[i];\n      \n      boolean isLastBlock = i == newBlocks.length - 1;\n      if (oldBlock.getBlockId() != newBlock.getBlockId() ||\n          (oldBlock.getGenerationStamp() != newBlock.getGenerationStamp() && \n              !(isGenStampUpdate && isLastBlock))) {\n        throw new IOException(\"Mismatched block IDs or generation stamps, \" +\n            \"attempting to replace block \" + oldBlock + \" with \" + newBlock +\n            \" as block # \" + i + \"/\" + newBlocks.length + \" of \" +\n            path);\n      }\n      \n      oldBlock.setNumBytes(newBlock.getNumBytes());\n      boolean changeMade =\n        oldBlock.getGenerationStamp() != newBlock.getGenerationStamp();\n      oldBlock.setGenerationStamp(newBlock.getGenerationStamp());\n      \n      if (!oldBlock.isComplete() &&\n          (!isLastBlock || op.shouldCompleteLastBlock())) {\n        changeMade = true;\n        fsNamesys.getBlockManager().forceCompleteBlock(oldBlock);\n      }\n      if (changeMade) {\n        // The state or gen-stamp of the block has changed. So, we may be\n        // able to process some messages from datanodes that we previously\n        // were unable to process.\n        fsNamesys.getBlockManager().processQueuedMessagesForBlock(newBlock);\n      }\n    }\n    \n    if (newBlocks.length < oldBlocks.length) {\n      // We're removing a block from the file, e.g. abandonBlock(...)\n      if (!file.isUnderConstruction()) {\n        throw new IOException(\"Trying to remove a block from file \" +\n            path + \" which is not under construction.\");\n      }\n      if (newBlocks.length != oldBlocks.length - 1) {\n        throw new IOException(\"Trying to remove more than one block from file \"\n            + path);\n      }\n      Block oldBlock = oldBlocks[oldBlocks.length - 1];\n      boolean removed = FSDirWriteFileOp.unprotectedRemoveBlock(\n          fsDir, path, iip, file, oldBlock);\n      if (!removed && !(op instanceof UpdateBlocksOp)) {\n        throw new IOException(\"Trying to delete non-existant block \" + oldBlock);\n      }\n    } else if (newBlocks.length > oldBlocks.length) {\n      final boolean isStriped = ecPolicy != null;\n      // We're adding blocks\n      for (int i = oldBlocks.length; i < newBlocks.length; i++) {\n        Block newBlock = newBlocks[i];\n        final BlockInfo newBI;\n        if (!op.shouldCompleteLastBlock()) {\n          // TODO: shouldn't this only be true for the last block?\n          // what about an old-version fsync() where fsync isn't called\n          // until several blocks in?\n          if (isStriped) {\n            newBI = new BlockInfoStriped(newBlock, ecPolicy);\n          } else {\n            newBI = new BlockInfoContiguous(newBlock,\n                file.getPreferredBlockReplication());\n          }\n          newBI.convertToBlockUnderConstruction(\n              BlockUCState.UNDER_CONSTRUCTION, null);\n        } else {\n          // OP_CLOSE should add finalized blocks. This code path\n          // is only executed when loading edits written by prior\n          // versions of Hadoop. Current versions always log\n          // OP_ADD operations as each block is allocated.\n          if (isStriped) {\n            newBI = new BlockInfoStriped(newBlock, ecPolicy);\n          } else {\n            newBI = new BlockInfoContiguous(newBlock,\n                file.getFileReplication());\n          }\n        }\n        fsNamesys.getBlockManager().addBlockCollectionWithCheck(newBI, file);\n        file.addBlock(newBI);\n        fsNamesys.getBlockManager().processQueuedMessagesForBlock(newBlock);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords": "  long loadEditRecords(EditLogInputStream in, boolean closeOnExit,\n      long expectedStartingTxId, StartupOption startOpt,\n      MetaRecoveryContext recovery) throws IOException {\n    FSDirectory fsDir = fsNamesys.dir;\n\n    EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts =\n      new EnumMap<FSEditLogOpCodes, Holder<Integer>>(FSEditLogOpCodes.class);\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Acquiring write lock to replay edit log\");\n    }\n\n    fsNamesys.writeLock();\n    fsDir.writeLock();\n\n    long recentOpcodeOffsets[] = new long[4];\n    Arrays.fill(recentOpcodeOffsets, -1);\n    \n    long expectedTxId = expectedStartingTxId;\n    long numEdits = 0;\n    long lastTxId = in.getLastTxId();\n    long numTxns = (lastTxId - expectedStartingTxId) + 1;\n    StartupProgress prog = NameNode.getStartupProgress();\n    Step step = createStartupProgressStep(in);\n    prog.setTotal(Phase.LOADING_EDITS, step, numTxns);\n    Counter counter = prog.getCounter(Phase.LOADING_EDITS, step);\n    long lastLogTime = monotonicNow();\n    long lastInodeId = fsNamesys.dir.getLastInodeId();\n    \n    try {\n      while (true) {\n        try {\n          FSEditLogOp op;\n          try {\n            op = in.readOp();\n            if (op == null) {\n              break;\n            }\n          } catch (Throwable e) {\n            // Handle a problem with our input\n            check203UpgradeFailure(in.getVersion(true), e);\n            String errorMessage =\n              formatEditLogReplayError(in, recentOpcodeOffsets, expectedTxId);\n            FSImage.LOG.error(errorMessage, e);\n            if (recovery == null) {\n               // We will only try to skip over problematic opcodes when in\n               // recovery mode.\n              throw new EditLogInputException(errorMessage, e, numEdits);\n            }\n            MetaRecoveryContext.editLogLoaderPrompt(\n                \"We failed to read txId \" + expectedTxId,\n                recovery, \"skipping the bad section in the log\");\n            in.resync();\n            continue;\n          }\n          recentOpcodeOffsets[(int)(numEdits % recentOpcodeOffsets.length)] =\n            in.getPosition();\n          if (op.hasTransactionId()) {\n            if (op.getTransactionId() > expectedTxId) { \n              MetaRecoveryContext.editLogLoaderPrompt(\"There appears \" +\n                  \"to be a gap in the edit log.  We expected txid \" +\n                  expectedTxId + \", but got txid \" +\n                  op.getTransactionId() + \".\", recovery, \"ignoring missing \" +\n                  \" transaction IDs\");\n            } else if (op.getTransactionId() < expectedTxId) { \n              MetaRecoveryContext.editLogLoaderPrompt(\"There appears \" +\n                  \"to be an out-of-order edit in the edit log.  We \" +\n                  \"expected txid \" + expectedTxId + \", but got txid \" +\n                  op.getTransactionId() + \".\", recovery,\n                  \"skipping the out-of-order edit\");\n              continue;\n            }\n          }\n          try {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"op=\" + op + \", startOpt=\" + startOpt\n                  + \", numEdits=\" + numEdits + \", totalEdits=\" + totalEdits);\n            }\n            long inodeId = applyEditLogOp(op, fsDir, startOpt,\n                in.getVersion(true), lastInodeId);\n            if (lastInodeId < inodeId) {\n              lastInodeId = inodeId;\n            }\n          } catch (RollingUpgradeOp.RollbackException e) {\n            throw e;\n          } catch (Throwable e) {\n            LOG.error(\"Encountered exception on operation \" + op, e);\n            if (recovery == null) {\n              throw e instanceof IOException? (IOException)e: new IOException(e);\n            }\n\n            MetaRecoveryContext.editLogLoaderPrompt(\"Failed to \" +\n             \"apply edit log operation \" + op + \": error \" +\n             e.getMessage(), recovery, \"applying edits\");\n          }\n          // Now that the operation has been successfully decoded and\n          // applied, update our bookkeeping.\n          incrOpCount(op.opCode, opCounts, step, counter);\n          if (op.hasTransactionId()) {\n            lastAppliedTxId = op.getTransactionId();\n            expectedTxId = lastAppliedTxId + 1;\n          } else {\n            expectedTxId = lastAppliedTxId = expectedStartingTxId;\n          }\n          // log progress\n          if (op.hasTransactionId()) {\n            long now = monotonicNow();\n            if (now - lastLogTime > REPLAY_TRANSACTION_LOG_INTERVAL) {\n              long deltaTxId = lastAppliedTxId - expectedStartingTxId + 1;\n              int percent = Math.round((float) deltaTxId / numTxns * 100);\n              LOG.info(\"replaying edit log: \" + deltaTxId + \"/\" + numTxns\n                  + \" transactions completed. (\" + percent + \"%)\");\n              lastLogTime = now;\n            }\n          }\n          numEdits++;\n          totalEdits++;\n        } catch (RollingUpgradeOp.RollbackException e) {\n          LOG.info(\"Stopped at OP_START_ROLLING_UPGRADE for rollback.\");\n          break;\n        } catch (MetaRecoveryContext.RequestStopException e) {\n          MetaRecoveryContext.LOG.warn(\"Stopped reading edit log at \" +\n              in.getPosition() + \"/\"  + in.length());\n          break;\n        }\n      }\n    } finally {\n      fsNamesys.dir.resetLastInodeId(lastInodeId);\n      if(closeOnExit) {\n        in.close();\n      }\n      fsDir.writeUnlock();\n      fsNamesys.writeUnlock(\"loadEditRecords\");\n\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"replaying edit log finished\");\n      }\n\n      if (FSImage.LOG.isDebugEnabled()) {\n        dumpOpCounts(opCounts);\n      }\n    }\n    return numEdits;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.createStartupProgressStep": "  private static Step createStartupProgressStep(EditLogInputStream edits)\n      throws IOException {\n    long length = edits.length();\n    String name = edits.getCurrentStreamName();\n    return length != -1 ? new Step(name, length) : new Step(name);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.formatEditLogReplayError": "  private static String formatEditLogReplayError(EditLogInputStream in,\n      long recentOpcodeOffsets[], long txid) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"Error replaying edit log at offset \" + in.getPosition());\n    sb.append(\".  Expected transaction ID was \").append(txid);\n    if (recentOpcodeOffsets[0] != -1) {\n      Arrays.sort(recentOpcodeOffsets);\n      sb.append(\"\\nRecent opcode offsets:\");\n      for (long offset : recentOpcodeOffsets) {\n        if (offset != -1) {\n          sb.append(' ').append(offset);\n        }\n      }\n    }\n    return sb.toString();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.check203UpgradeFailure": "  private void check203UpgradeFailure(int logVersion, Throwable e)\n      throws IOException {\n    // 0.20.203 version version has conflicting opcodes with the later releases.\n    // The editlog must be emptied by restarting the namenode, before proceeding\n    // with the upgrade.\n    if (Storage.is203LayoutVersion(logVersion)\n        && logVersion != HdfsServerConstants.NAMENODE_LAYOUT_VERSION) {\n      String msg = \"During upgrade failed to load the editlog version \"\n          + logVersion + \" from release 0.20.203. Please go back to the old \"\n          + \" release and restart the namenode. This empties the editlog \"\n          + \" and saves the namespace. Resume the upgrade after this step.\";\n      throw new IOException(msg, e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.incrOpCount": "  private void incrOpCount(FSEditLogOpCodes opCode,\n      EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts, Step step,\n      Counter counter) {\n    Holder<Integer> holder = opCounts.get(opCode);\n    if (holder == null) {\n      holder = new Holder<Integer>(1);\n      opCounts.put(opCode, holder);\n    } else {\n      holder.held++;\n    }\n    counter.increment();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.dumpOpCounts": "  private static void dumpOpCounts(\n      EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"Summary of operations loaded from edit log:\\n  \");\n    Joiner.on(\"\\n  \").withKeyValueSeparator(\"=\").appendTo(sb, opCounts);\n    FSImage.LOG.debug(sb.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits": "  long loadFSEdits(EditLogInputStream edits, long expectedStartingTxId,\n      StartupOption startOpt, MetaRecoveryContext recovery) throws IOException {\n    StartupProgress prog = NameNode.getStartupProgress();\n    Step step = createStartupProgressStep(edits);\n    prog.beginStep(Phase.LOADING_EDITS, step);\n    fsNamesys.writeLock();\n    try {\n      long startTime = monotonicNow();\n      FSImage.LOG.info(\"Start loading edits file \" + edits.getName());\n      long numEdits = loadEditRecords(edits, false, expectedStartingTxId,\n          startOpt, recovery);\n      FSImage.LOG.info(\"Edits file \" + edits.getName() \n          + \" of size \" + edits.length() + \" edits # \" + numEdits \n          + \" loaded in \" + (monotonicNow()-startTime)/1000 + \" seconds\");\n      return numEdits;\n    } finally {\n      edits.close();\n      fsNamesys.writeUnlock(\"loadFSEdits\");\n      prog.endStep(Phase.LOADING_EDITS, step);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits": "  private long loadEdits(Iterable<EditLogInputStream> editStreams,\n      FSNamesystem target, StartupOption startOpt, MetaRecoveryContext recovery)\n      throws IOException {\n    LOG.debug(\"About to load edits:\\n  \" + Joiner.on(\"\\n  \").join(editStreams));\n    StartupProgress prog = NameNode.getStartupProgress();\n    prog.beginPhase(Phase.LOADING_EDITS);\n    \n    long prevLastAppliedTxId = lastAppliedTxId;  \n    try {    \n      FSEditLogLoader loader = new FSEditLogLoader(target, lastAppliedTxId);\n      \n      // Load latest edits\n      for (EditLogInputStream editIn : editStreams) {\n        LOG.info(\"Reading \" + editIn + \" expecting start txid #\" +\n              (lastAppliedTxId + 1));\n        try {\n          loader.loadFSEdits(editIn, lastAppliedTxId + 1, startOpt, recovery);\n        } finally {\n          // Update lastAppliedTxId even in case of error, since some ops may\n          // have been successfully applied before the error.\n          lastAppliedTxId = loader.getLastAppliedTxId();\n        }\n        // If we are in recovery mode, we may have skipped over some txids.\n        if (editIn.getLastTxId() != HdfsServerConstants.INVALID_TXID) {\n          lastAppliedTxId = editIn.getLastTxId();\n        }\n      }\n    } finally {\n      FSEditLog.closeAllStreams(editStreams);\n    }\n    prog.endPhase(Phase.LOADING_EDITS);\n    return lastAppliedTxId - prevLastAppliedTxId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getLastAppliedTxId": "  public synchronized long getLastAppliedTxId() {\n    return lastAppliedTxId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage": "  private void loadFSImage(File curFile, MD5Hash expectedMd5,\n      FSNamesystem target, MetaRecoveryContext recovery,\n      boolean requireSameLayoutVersion) throws IOException {\n    // BlockPoolId is required when the FsImageLoader loads the rolling upgrade\n    // information. Make sure the ID is properly set.\n    target.setBlockPoolId(this.getBlockPoolID());\n\n    FSImageFormat.LoaderDelegator loader = FSImageFormat.newLoader(conf, target);\n    loader.load(curFile, requireSameLayoutVersion);\n\n    // Check that the image digest we loaded matches up with what\n    // we expected\n    MD5Hash readImageMd5 = loader.getLoadedImageMd5();\n    if (expectedMd5 != null &&\n        !expectedMd5.equals(readImageMd5)) {\n      throw new IOException(\"Image file \" + curFile +\n          \" is corrupt with MD5 checksum of \" + readImageMd5 +\n          \" but expecting \" + expectedMd5);\n    }\n\n    long txId = loader.getLoadedImageTxId();\n    LOG.info(\"Loaded image for txid \" + txId + \" from \" + curFile);\n    lastAppliedTxId = txId;\n    storage.setMostRecentCheckpointInfo(txId, curFile.lastModified());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getBlockPoolID": "  public String getBlockPoolID() {\n    return storage.getBlockPoolID();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.initEditLog": "  public void initEditLog(StartupOption startOpt) throws IOException {\n    Preconditions.checkState(getNamespaceID() != 0,\n        \"Must know namespace ID before initting edit log\");\n    String nameserviceId = DFSUtil.getNamenodeNameServiceId(conf);\n    if (!HAUtil.isHAEnabled(conf, nameserviceId)) {\n      // If this NN is not HA\n      editLog.initJournalsForWrite();\n      editLog.recoverUnclosedStreams();\n    } else if (HAUtil.isHAEnabled(conf, nameserviceId)\n        && (startOpt == StartupOption.UPGRADE\n            || startOpt == StartupOption.UPGRADEONLY\n            || RollingUpgradeStartupOption.ROLLBACK.matches(startOpt))) {\n      // This NN is HA, but we're doing an upgrade or a rollback of rolling\n      // upgrade so init the edit log for write.\n      editLog.initJournalsForWrite();\n      if (startOpt == StartupOption.UPGRADE\n          || startOpt == StartupOption.UPGRADEONLY) {\n        long sharedLogCTime = editLog.getSharedLogCTime();\n        if (this.storage.getCTime() < sharedLogCTime) {\n          throw new IOException(\"It looks like the shared log is already \" +\n              \"being upgraded but this NN has not been upgraded yet. You \" +\n              \"should restart this NameNode with the '\" +\n              StartupOption.BOOTSTRAPSTANDBY.getName() + \"' option to bring \" +\n              \"this NN in sync with the other.\");\n        }\n      }\n      editLog.recoverUnclosedStreams();\n    } else {\n      // This NN is HA and we're not doing an upgrade.\n      editLog.initSharedJournalsForRead();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getLayoutVersion": "  public int getLayoutVersion() {\n    return storage.getLayoutVersion();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.needsResaveBasedOnStaleCheckpoint": "  private boolean needsResaveBasedOnStaleCheckpoint(\n      File imageFile, long numEditsLoaded) {\n    final long checkpointPeriod = conf.getTimeDuration(\n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_KEY,\n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_DEFAULT, TimeUnit.SECONDS);\n    final long checkpointTxnCount = conf.getLong(\n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_KEY,\n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_DEFAULT);\n    long checkpointAge = Time.now() - imageFile.lastModified();\n\n    return (checkpointAge > checkpointPeriod * 1000) ||\n           (numEditsLoaded > checkpointTxnCount);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.rollingRollback": "  private void rollingRollback(long discardSegmentTxId, long ckptId)\n      throws IOException {\n    // discard discard unnecessary editlog segments starting from the given id\n    this.editLog.discardSegments(discardSegmentTxId);\n    // rename the special checkpoint\n    renameCheckpoint(ckptId, NameNodeFile.IMAGE_ROLLBACK, NameNodeFile.IMAGE,\n        true);\n    // purge all the checkpoints after the marker\n    archivalManager.purgeCheckpoinsAfter(NameNodeFile.IMAGE, ckptId);\n    // HDFS-7939: purge all old fsimage_rollback_*\n    archivalManager.purgeCheckpoints(NameNodeFile.IMAGE_ROLLBACK);\n    String nameserviceId = DFSUtil.getNamenodeNameServiceId(conf);\n    if (HAUtil.isHAEnabled(conf, nameserviceId)) {\n      // close the editlog since it is currently open for write\n      this.editLog.close();\n      // reopen the editlog for read\n      this.editLog.initSharedJournalsForRead();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile": "  void loadFSImageFile(FSNamesystem target, MetaRecoveryContext recovery,\n      FSImageFile imageFile, StartupOption startupOption) throws IOException {\n    LOG.info(\"Planning to load image: \" + imageFile);\n    StorageDirectory sdForProperties = imageFile.sd;\n    storage.readProperties(sdForProperties, startupOption);\n\n    if (NameNodeLayoutVersion.supports(\n        LayoutVersion.Feature.TXID_BASED_LAYOUT, getLayoutVersion())) {\n      // For txid-based layout, we should have a .md5 file\n      // next to the image file\n      boolean isRollingRollback = RollingUpgradeStartupOption.ROLLBACK\n          .matches(startupOption);\n      loadFSImage(imageFile.getFile(), target, recovery, isRollingRollback);\n    } else if (NameNodeLayoutVersion.supports(\n        LayoutVersion.Feature.FSIMAGE_CHECKSUM, getLayoutVersion())) {\n      // In 0.22, we have the checksum stored in the VERSION file.\n      String md5 = storage.getDeprecatedProperty(\n          NNStorage.DEPRECATED_MESSAGE_DIGEST_PROPERTY);\n      if (md5 == null) {\n        throw new InconsistentFSStateException(sdForProperties.getRoot(),\n            \"Message digest property \" +\n            NNStorage.DEPRECATED_MESSAGE_DIGEST_PROPERTY +\n            \" not set for storage directory \" + sdForProperties.getRoot());\n      }\n      loadFSImage(imageFile.getFile(), new MD5Hash(md5), target, recovery,\n          false);\n    } else {\n      // We don't have any record of the md5sum\n      loadFSImage(imageFile.getFile(), null, target, recovery, false);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.isUpgradeFinalized": "  boolean isUpgradeFinalized() {\n    return isUpgradeFinalized;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead": "  boolean recoverTransitionRead(StartupOption startOpt, FSNamesystem target,\n      MetaRecoveryContext recovery)\n      throws IOException {\n    assert startOpt != StartupOption.FORMAT : \n      \"NameNode formatting should be performed before reading the image\";\n    \n    Collection<URI> imageDirs = storage.getImageDirectories();\n    Collection<URI> editsDirs = editLog.getEditURIs();\n\n    // none of the data dirs exist\n    if((imageDirs.size() == 0 || editsDirs.size() == 0) \n                             && startOpt != StartupOption.IMPORT)  \n      throw new IOException(\n          \"All specified directories are not accessible or do not exist.\");\n    \n    // 1. For each data directory calculate its state and \n    // check whether all is consistent before transitioning.\n    Map<StorageDirectory, StorageState> dataDirStates = \n             new HashMap<StorageDirectory, StorageState>();\n    boolean isFormatted = recoverStorageDirs(startOpt, storage, dataDirStates);\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Data dir states:\\n  \" +\n        Joiner.on(\"\\n  \").withKeyValueSeparator(\": \")\n        .join(dataDirStates));\n    }\n    \n    if (!isFormatted && startOpt != StartupOption.ROLLBACK \n                     && startOpt != StartupOption.IMPORT) {\n      throw new IOException(\"NameNode is not formatted.\");      \n    }\n\n\n    int layoutVersion = storage.getLayoutVersion();\n    if (startOpt == StartupOption.METADATAVERSION) {\n      System.out.println(\"HDFS Image Version: \" + layoutVersion);\n      System.out.println(\"Software format version: \" +\n        HdfsServerConstants.NAMENODE_LAYOUT_VERSION);\n      return false;\n    }\n\n    if (layoutVersion < Storage.LAST_PRE_UPGRADE_LAYOUT_VERSION) {\n      NNStorage.checkVersionUpgradable(storage.getLayoutVersion());\n    }\n    if (startOpt != StartupOption.UPGRADE\n        && startOpt != StartupOption.UPGRADEONLY\n        && !RollingUpgradeStartupOption.STARTED.matches(startOpt)\n        && layoutVersion < Storage.LAST_PRE_UPGRADE_LAYOUT_VERSION\n        && layoutVersion != HdfsServerConstants.NAMENODE_LAYOUT_VERSION) {\n      throw new IOException(\n          \"\\nFile system image contains an old layout version \" \n          + storage.getLayoutVersion() + \".\\nAn upgrade to version \"\n          + HdfsServerConstants.NAMENODE_LAYOUT_VERSION + \" is required.\\n\"\n          + \"Please restart NameNode with the \\\"\"\n          + RollingUpgradeStartupOption.STARTED.getOptionString()\n          + \"\\\" option if a rolling upgrade is already started;\"\n          + \" or restart NameNode with the \\\"\"\n          + StartupOption.UPGRADE.getName() + \"\\\" option to start\"\n          + \" a new upgrade.\");\n    }\n    \n    storage.processStartupOptionsForUpgrade(startOpt, layoutVersion);\n\n    // 2. Format unformatted dirs.\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      StorageState curState = dataDirStates.get(sd);\n      switch(curState) {\n      case NON_EXISTENT:\n        throw new IOException(StorageState.NON_EXISTENT + \n                              \" state cannot be here\");\n      case NOT_FORMATTED:\n        // Create a dir structure, but not the VERSION file. The presence of\n        // VERSION is checked in the inspector's needToSave() method and\n        // saveNamespace is triggered if it is absent. This will bring\n        // the storage state uptodate along with a new VERSION file.\n        // If HA is enabled, NNs start up as standby so saveNamespace is not\n        // triggered.\n        LOG.info(\"Storage directory \" + sd.getRoot() + \" is not formatted.\");\n        LOG.info(\"Formatting ...\");\n        sd.clearDirectory(); // create empty currrent dir\n        // For non-HA, no further action is needed here, as saveNamespace will\n        // take care of the rest.\n        if (!target.isHaEnabled()) {\n          continue;\n        }\n        // If HA is enabled, save the dirs to create a version file later when\n        // a checkpoint image is saved.\n        if (newDirs == null) {\n          newDirs = new HashSet<StorageDirectory>();\n        }\n        newDirs.add(sd);\n        break;\n      default:\n        break;\n      }\n    }\n\n    // 3. Do transitions\n    switch(startOpt) {\n    case UPGRADE:\n    case UPGRADEONLY:\n      doUpgrade(target);\n      return false; // upgrade saved image already\n    case IMPORT:\n      doImportCheckpoint(target);\n      return false; // import checkpoint saved image already\n    case ROLLBACK:\n      throw new AssertionError(\"Rollback is now a standalone command, \" +\n          \"NameNode should not be starting with this option.\");\n    case REGULAR:\n    default:\n      // just load the image\n    }\n    \n    return loadFSImage(target, startOpt, recovery);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs": "  public static boolean recoverStorageDirs(StartupOption startOpt,\n      NNStorage storage, Map<StorageDirectory, StorageState> dataDirStates)\n      throws IOException {\n    boolean isFormatted = false;\n    // This loop needs to be over all storage dirs, even shared dirs, to make\n    // sure that we properly examine their state, but we make sure we don't\n    // mutate the shared dir below in the actual loop.\n    for (Iterator<StorageDirectory> it = \n                      storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      StorageState curState;\n      if (startOpt == StartupOption.METADATAVERSION) {\n        /* All we need is the layout version. */\n        storage.readProperties(sd);\n        return true;\n      }\n\n      try {\n        curState = sd.analyzeStorage(startOpt, storage);\n        // sd is locked but not opened\n        switch(curState) {\n        case NON_EXISTENT:\n          // name-node fails if any of the configured storage dirs are missing\n          throw new InconsistentFSStateException(sd.getRoot(),\n                      \"storage directory does not exist or is not accessible.\");\n        case NOT_FORMATTED:\n          break;\n        case NORMAL:\n          break;\n        default:  // recovery is possible\n          sd.doRecover(curState);\n        }\n        if (curState != StorageState.NOT_FORMATTED \n            && startOpt != StartupOption.ROLLBACK) {\n          // read and verify consistency with other directories\n          storage.readProperties(sd, startOpt);\n          isFormatted = true;\n        }\n        if (startOpt == StartupOption.IMPORT && isFormatted)\n          // import of a checkpoint is allowed only into empty image directories\n          throw new IOException(\"Cannot import image from a checkpoint. \" \n              + \" NameNode already contains an image in \" + sd.getRoot());\n      } catch (IOException ioe) {\n        sd.unlock();\n        throw ioe;\n      }\n      dataDirStates.put(sd,curState);\n    }\n    return isFormatted;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade": "  void doUpgrade(FSNamesystem target) throws IOException {\n    checkUpgrade();\n\n    // load the latest image\n\n    // Do upgrade for each directory\n    this.loadFSImage(target, StartupOption.UPGRADE, null);\n    target.checkRollingUpgrade(\"upgrade namenode\");\n    \n    long oldCTime = storage.getCTime();\n    storage.cTime = now();  // generate new cTime for the state\n    int oldLV = storage.getLayoutVersion();\n    storage.layoutVersion = HdfsServerConstants.NAMENODE_LAYOUT_VERSION;\n    \n    List<StorageDirectory> errorSDs =\n      Collections.synchronizedList(new ArrayList<StorageDirectory>());\n    assert !editLog.isSegmentOpen() : \"Edits log must not be open.\";\n    LOG.info(\"Starting upgrade of local storage directories.\"\n        + \"\\n   old LV = \" + oldLV\n        + \"; old CTime = \" + oldCTime\n        + \".\\n   new LV = \" + storage.getLayoutVersion()\n        + \"; new CTime = \" + storage.getCTime());\n    // Do upgrade for each directory\n    for (Iterator<StorageDirectory> it = storage.dirIterator(false); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      try {\n        NNUpgradeUtil.doPreUpgrade(conf, sd);\n      } catch (Exception e) {\n        LOG.error(\"Failed to move aside pre-upgrade storage \" +\n            \"in image directory \" + sd.getRoot(), e);\n        errorSDs.add(sd);\n        continue;\n      }\n    }\n    if (target.isHaEnabled()) {\n      editLog.doPreUpgradeOfSharedLog();\n    }\n    storage.reportErrorsOnDirectories(errorSDs);\n    errorSDs.clear();\n\n    saveFSImageInAllDirs(target, editLog.getLastWrittenTxId());\n\n    // upgrade shared edit storage first\n    if (target.isHaEnabled()) {\n      editLog.doUpgradeOfSharedLog();\n    }\n    for (Iterator<StorageDirectory> it = storage.dirIterator(false); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      try {\n        NNUpgradeUtil.doUpgrade(sd, storage);\n      } catch (IOException ioe) {\n        errorSDs.add(sd);\n        continue;\n      }\n    }\n    storage.reportErrorsOnDirectories(errorSDs);\n    \n    isUpgradeFinalized = false;\n    if (!storage.getRemovedStorageDirs().isEmpty()) {\n      // during upgrade, it's a fatal error to fail any storage directory\n      throw new IOException(\"Upgrade failed in \"\n          + storage.getRemovedStorageDirs().size()\n          + \" storage directory(ies), previously logged.\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.doImportCheckpoint": "  void doImportCheckpoint(FSNamesystem target) throws IOException {\n    Collection<URI> checkpointDirs =\n      FSImage.getCheckpointDirs(conf, null);\n    List<URI> checkpointEditsDirs =\n      FSImage.getCheckpointEditsDirs(conf, null);\n\n    if (checkpointDirs == null || checkpointDirs.isEmpty()) {\n      throw new IOException(\"Cannot import image from a checkpoint. \"\n                            + \"\\\"dfs.namenode.checkpoint.dir\\\" is not set.\" );\n    }\n    \n    if (checkpointEditsDirs == null || checkpointEditsDirs.isEmpty()) {\n      throw new IOException(\"Cannot import image from a checkpoint. \"\n                            + \"\\\"dfs.namenode.checkpoint.dir\\\" is not set.\" );\n    }\n\n    FSImage realImage = target.getFSImage();\n    FSImage ckptImage = new FSImage(conf, \n                                    checkpointDirs, checkpointEditsDirs);\n    // load from the checkpoint dirs\n    try {\n      ckptImage.recoverTransitionRead(StartupOption.REGULAR, target, null);\n    } finally {\n      ckptImage.close();\n    }\n    // return back the real image\n    realImage.getStorage().setStorageInfo(ckptImage.getStorage());\n    realImage.getEditLog().setNextTxId(ckptImage.getEditLog().getLastWrittenTxId()+1);\n    realImage.initEditLog(StartupOption.IMPORT);\n\n    realImage.getStorage().setBlockPoolID(ckptImage.getBlockPoolID());\n\n    // and save it but keep the same checkpointTime\n    saveNamespace(target);\n    updateStorageVersion();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage": "  private void loadFSImage(StartupOption startOpt) throws IOException {\n    final FSImage fsImage = getFSImage();\n\n    // format before starting up if requested\n    if (startOpt == StartupOption.FORMAT) {\n      \n      fsImage.format(this, fsImage.getStorage().determineClusterId());// reuse current id\n\n      startOpt = StartupOption.REGULAR;\n    }\n    boolean success = false;\n    writeLock();\n    try {\n      // We shouldn't be calling saveNamespace if we've come up in standby state.\n      MetaRecoveryContext recovery = startOpt.createRecoveryContext();\n      final boolean staleImage\n          = fsImage.recoverTransitionRead(startOpt, this, recovery);\n      if (RollingUpgradeStartupOption.ROLLBACK.matches(startOpt)) {\n        rollingUpgradeInfo = null;\n      }\n      final boolean needToSave = staleImage && !haEnabled && !isRollingUpgrade(); \n      LOG.info(\"Need to save fs image? \" + needToSave\n          + \" (staleImage=\" + staleImage + \", haEnabled=\" + haEnabled\n          + \", isRollingUpgrade=\" + isRollingUpgrade() + \")\");\n      if (needToSave) {\n        fsImage.saveNamespace(this);\n      } else {\n        // No need to save, so mark the phase done.\n        StartupProgress prog = NameNode.getStartupProgress();\n        prog.beginPhase(Phase.SAVING_CHECKPOINT);\n        prog.endPhase(Phase.SAVING_CHECKPOINT);\n      }\n      // This will start a new log segment and write to the seen_txid file, so\n      // we shouldn't do it when coming up in standby state\n      if (!haEnabled || (haEnabled && startOpt == StartupOption.UPGRADE)\n          || (haEnabled && startOpt == StartupOption.UPGRADEONLY)) {\n        fsImage.openEditLogForWrite(getEffectiveLayoutVersion());\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        fsImage.close();\n      }\n      writeUnlock(\"loadFSImage\");\n    }\n    imageLoadComplete();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveNamespace": "  boolean saveNamespace(final long timeWindow, final long txGap)\n      throws IOException {\n    checkOperation(OperationCategory.UNCHECKED);\n    checkSuperuserPrivilege();\n\n    boolean saved = false;\n    cpLock();  // Block if a checkpointing is in progress on standby.\n    readLock();\n    try {\n      checkOperation(OperationCategory.UNCHECKED);\n\n      if (!isInSafeMode()) {\n        throw new IOException(\"Safe mode should be turned ON \"\n            + \"in order to create namespace image.\");\n      }\n      saved = getFSImage().saveNamespace(timeWindow, txGap, this);\n    } finally {\n      readUnlock(\"saveNamespace\");\n      cpUnlock();\n    }\n    if (saved) {\n      LOG.info(\"New namespace image has been created\");\n    }\n    return saved;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEffectiveLayoutVersion": "  static int getEffectiveLayoutVersion(boolean isRollingUpgrade, int storageLV,\n      int minCompatLV, int currentLV) {\n    if (isRollingUpgrade) {\n      if (storageLV <= minCompatLV) {\n        // The prior layout version satisfies the minimum compatible layout\n        // version of the current software.  Keep reporting the prior layout\n        // as the effective one.  Downgrade is possible.\n        return storageLV;\n      }\n    }\n    // The current software cannot satisfy the layout version of the prior\n    // software.  Proceed with using the current layout version.\n    return currentLV;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock": "  public void writeUnlock(String opName) {\n    this.fsLock.writeUnlock(opName);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.close": "  void close() {\n    fsRunning = false;\n    try {\n      stopCommonServices();\n    } finally {\n      // using finally to ensure we also wait for lease daemon\n      try {\n        stopActiveServices();\n        stopStandbyServices();\n      } catch (IOException ie) {\n      } finally {\n        IOUtils.cleanup(LOG, dir);\n        IOUtils.cleanup(LOG, fsImage);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFSImage": "  public FSImage getFSImage() {\n    return fsImage;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.isRollingUpgrade": "  public boolean isRollingUpgrade() {\n    return rollingUpgradeInfo != null && !rollingUpgradeInfo.isFinalized();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.imageLoadComplete": "  void imageLoadComplete() {\n    Preconditions.checkState(!imageLoaded, \"FSDirectory already loaded\");\n    setImageLoaded();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock": "  public void writeLock() {\n    this.fsLock.writeLock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk": "  static FSNamesystem loadFromDisk(Configuration conf) throws IOException {\n\n    checkConfiguration(conf);\n    FSImage fsImage = new FSImage(conf,\n        FSNamesystem.getNamespaceDirs(conf),\n        FSNamesystem.getNamespaceEditsDirs(conf));\n    FSNamesystem namesystem = new FSNamesystem(conf, fsImage, false);\n    StartupOption startOpt = NameNode.getStartupOption(conf);\n    if (startOpt == StartupOption.RECOVER) {\n      namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n    }\n\n    long loadStart = monotonicNow();\n    try {\n      namesystem.loadFSImage(startOpt);\n    } catch (IOException ioe) {\n      LOG.warn(\"Encountered exception loading fsimage\", ioe);\n      fsImage.close();\n      throw ioe;\n    }\n    long timeTakenToLoadFSImage = monotonicNow() - loadStart;\n    LOG.info(\"Finished loading FSImage in \" + timeTakenToLoadFSImage + \" msecs\");\n    NameNodeMetrics nnMetrics = NameNode.getNameNodeMetrics();\n    if (nnMetrics != null) {\n      nnMetrics.setFsImageLoadTime((int) timeTakenToLoadFSImage);\n    }\n    namesystem.getFSDirectory().createReservedStatuses(namesystem.getCTime());\n    return namesystem;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNamespaceEditsDirs": "  public static List<URI> getNamespaceEditsDirs(Configuration conf,\n      boolean includeShared)\n      throws IOException {\n    // Use a LinkedHashSet so that order is maintained while we de-dup\n    // the entries.\n    LinkedHashSet<URI> editsDirs = new LinkedHashSet<URI>();\n    \n    if (includeShared) {\n      List<URI> sharedDirs = getSharedEditsDirs(conf);\n  \n      // Fail until multiple shared edits directories are supported (HDFS-2782)\n      if (sharedDirs.size() > 1) {\n        throw new IOException(\n            \"Multiple shared edits directories are not yet supported\");\n      }\n  \n      // First add the shared edits dirs. It's critical that the shared dirs\n      // are added first, since JournalSet syncs them in the order they are listed,\n      // and we need to make sure all edits are in place in the shared storage\n      // before they are replicated locally. See HDFS-2874.\n      for (URI dir : sharedDirs) {\n        if (!editsDirs.add(dir)) {\n          LOG.warn(\"Edits URI \" + dir + \" listed multiple times in \" + \n              DFS_NAMENODE_SHARED_EDITS_DIR_KEY + \". Ignoring duplicates.\");\n        }\n      }\n    }    \n    // Now add the non-shared dirs.\n    for (URI dir : getStorageDirs(conf, DFS_NAMENODE_EDITS_DIR_KEY)) {\n      if (!editsDirs.add(dir)) {\n        LOG.warn(\"Edits URI \" + dir + \" listed multiple times in \" + \n            DFS_NAMENODE_SHARED_EDITS_DIR_KEY + \" and \" +\n            DFS_NAMENODE_EDITS_DIR_KEY + \". Ignoring duplicates.\");\n      }\n    }\n\n    if (editsDirs.isEmpty()) {\n      // If this is the case, no edit dirs have been explicitly configured.\n      // Image dirs are to be used for edits too.\n      return Lists.newArrayList(getNamespaceDirs(conf));\n    } else {\n      return Lists.newArrayList(editsDirs);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkConfiguration": "  private static void checkConfiguration(Configuration conf)\n      throws IOException {\n\n    final Collection<URI> namespaceDirs =\n        FSNamesystem.getNamespaceDirs(conf);\n    final Collection<URI> editsDirs =\n        FSNamesystem.getNamespaceEditsDirs(conf);\n    final Collection<URI> requiredEditsDirs =\n        FSNamesystem.getRequiredNamespaceEditsDirs(conf);\n    final Collection<URI> sharedEditsDirs =\n        FSNamesystem.getSharedEditsDirs(conf);\n\n    for (URI u : requiredEditsDirs) {\n      if (u.toString().compareTo(\n              DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_DEFAULT) == 0) {\n        continue;\n      }\n\n      // Each required directory must also be in editsDirs or in\n      // sharedEditsDirs.\n      if (!editsDirs.contains(u) &&\n          !sharedEditsDirs.contains(u)) {\n        throw new IllegalArgumentException(\"Required edits directory \" + u\n            + \" not found: \"\n            + DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_KEY + \"=\" + editsDirs + \"; \"\n            + DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_REQUIRED_KEY\n            + \"=\" + requiredEditsDirs + \"; \"\n            + DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY\n            + \"=\" + sharedEditsDirs);\n      }\n    }\n\n    if (namespaceDirs.size() == 1) {\n      LOG.warn(\"Only one image storage directory (\"\n          + DFS_NAMENODE_NAME_DIR_KEY + \") configured. Beware of data loss\"\n          + \" due to lack of redundant storage directories!\");\n    }\n    if (editsDirs.size() == 1) {\n      LOG.warn(\"Only one namespace edits storage directory (\"\n          + DFS_NAMENODE_EDITS_DIR_KEY + \") configured. Beware of data loss\"\n          + \" due to lack of redundant storage directories!\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setSafeMode": "  boolean setSafeMode(SafeModeAction action) throws IOException {\n    if (action != SafeModeAction.SAFEMODE_GET) {\n      checkSuperuserPrivilege();\n      switch(action) {\n      case SAFEMODE_LEAVE: // leave safe mode\n        leaveSafeMode(false);\n        break;\n      case SAFEMODE_ENTER: // enter safe mode\n        enterSafeMode(false);\n        break;\n      case SAFEMODE_FORCE_EXIT:\n        leaveSafeMode(true);\n        break;\n      default:\n        LOG.error(\"Unexpected safe mode action\");\n      }\n    }\n    return isInSafeMode();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFSDirectory": "  public FSDirectory getFSDirectory() {\n    return dir;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNamespaceDirs": "  public static Collection<URI> getNamespaceDirs(Configuration conf) {\n    return getStorageDirs(conf, DFS_NAMENODE_NAME_DIR_KEY);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getCTime": "  long getCTime() {\n    return fsImage == null ? 0 : fsImage.getStorage().getCTime();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem": "  protected void loadNamesystem(Configuration conf) throws IOException {\n    this.namesystem = FSNamesystem.loadFromDisk(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initialize": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) == null) {\n      String intervals = conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals != null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    pauseMonitor = new JvmPauseMonitor();\n    pauseMonitor.init(conf);\n    pauseMonitor.start();\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n\n    if (NamenodeRole.NAMENODE == role) {\n      startHttpServer(conf);\n    }\n\n    loadNamesystem(conf);\n\n    rpcServer = createRpcServer(conf);\n\n    initReconfigurableBackoffKey();\n\n    if (clientNamenodeAddress == null) {\n      // This is expected for MiniDFSCluster. Set it now using \n      // the RPC server's bind address.\n      clientNamenodeAddress = \n          NetUtils.getHostPortString(getNameNodeAddress());\n      LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n          + \" this namenode/service.\");\n    }\n    if (NamenodeRole.NAMENODE == role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n\n    startCommonServices(conf);\n    startMetricsLogger(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getRole": "  public NamenodeRole getRole() {\n    return role;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.startMetricsLogger": "  protected void startMetricsLogger(Configuration conf) {\n    long metricsLoggerPeriodSec =\n        conf.getInt(DFS_NAMENODE_METRICS_LOGGER_PERIOD_SECONDS_KEY,\n            DFS_NAMENODE_METRICS_LOGGER_PERIOD_SECONDS_DEFAULT);\n\n    if (metricsLoggerPeriodSec <= 0) {\n      return;\n    }\n\n    MetricsLoggerTask.makeMetricsLoggerAsync(MetricsLog);\n\n    // Schedule the periodic logging.\n    metricsLoggerTimer = new ScheduledThreadPoolExecutor(1);\n    metricsLoggerTimer.setExecuteExistingDelayedTasksAfterShutdownPolicy(\n        false);\n    metricsLoggerTimer.scheduleWithFixedDelay(new MetricsLoggerTask(MetricsLog,\n        \"NameNode\", (short) 128),\n        metricsLoggerPeriodSec,\n        metricsLoggerPeriodSec,\n        TimeUnit.SECONDS);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.loginAsNameNodeUser": "  void loginAsNameNodeUser(Configuration conf) throws IOException {\n    InetSocketAddress socAddr = getRpcServerAddress(conf);\n    SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n        DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, socAddr.getHostName());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer": "  private void startHttpServer(final Configuration conf) throws IOException {\n    httpServer = new NameNodeHttpServer(conf, this, getHttpServerBindAddress(conf));\n    httpServer.start();\n    httpServer.setStartupProgress(startupProgress);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getFSImage": "  public FSImage getFSImage() {\n    return namesystem.getFSImage();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initMetrics": "  public static void initMetrics(Configuration conf, NamenodeRole role) {\n    metrics = NameNodeMetrics.create(conf, role);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer": "  protected NameNodeRpcServer createRpcServer(Configuration conf)\n      throws IOException {\n    return new NameNodeRpcServer(conf, this);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.startCommonServices": "  private void startCommonServices(Configuration conf) throws IOException {\n    namesystem.startCommonServices(conf, haContext);\n    registerNNSMXBean();\n    if (NamenodeRole.NAMENODE != role) {\n      startHttpServer(conf);\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n    rpcServer.start();\n    try {\n      plugins = conf.getInstances(DFS_NAMENODE_PLUGINS_KEY,\n          ServicePlugin.class);\n    } catch (RuntimeException e) {\n      String pluginsValue = conf.get(DFS_NAMENODE_PLUGINS_KEY);\n      LOG.error(\"Unable to load NameNode plugins. Specified list of plugins: \" +\n          pluginsValue, e);\n      throw e;\n    }\n    for (ServicePlugin p: plugins) {\n      try {\n        p.start(this);\n      } catch (Throwable t) {\n        LOG.warn(\"ServicePlugin \" + p + \" could not be started\", t);\n      }\n    }\n    LOG.info(getRole() + \" RPC up at: \" + getNameNodeAddress());\n    if (rpcServer.getServiceRpcAddress() != null) {\n      LOG.info(getRole() + \" service RPC up at: \"\n          + rpcServer.getServiceRpcAddress());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initReconfigurableBackoffKey": "  private void initReconfigurableBackoffKey() {\n    ipcClientRPCBackoffEnable = buildBackoffEnableKey(rpcServer\n        .getClientRpcServer().getPort());\n    reconfigurableProperties.add(ipcClientRPCBackoffEnable);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getNameNodeAddress": "  public InetSocketAddress getNameNodeAddress() {\n    return rpcServer.getRpcAddress();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    LOG.info(\"createNameNode \" + Arrays.asList(argv));\n    if (conf == null)\n      conf = new HdfsConfiguration();\n    // Parse out some generic args into Configuration.\n    GenericOptionsParser hParser = new GenericOptionsParser(conf, argv);\n    argv = hParser.getRemainingArgs();\n    // Parse the rest, NN specific args.\n    StartupOption startOpt = parseArguments(argv);\n    if (startOpt == null) {\n      printUsage(System.err);\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n\n    boolean aborted = false;\n    switch (startOpt) {\n    case FORMAT:\n      aborted = format(conf, startOpt.getForceFormat(),\n          startOpt.getInteractiveFormat());\n      terminate(aborted ? 1 : 0);\n      return null; // avoid javac warning\n    case GENCLUSTERID:\n      System.err.println(\"Generating new cluster id:\");\n      System.out.println(NNStorage.newClusterID());\n      terminate(0);\n      return null;\n    case ROLLBACK:\n      aborted = doRollback(conf, true);\n      terminate(aborted ? 1 : 0);\n      return null; // avoid warning\n    case BOOTSTRAPSTANDBY:\n      String[] toolArgs = Arrays.copyOfRange(argv, 1, argv.length);\n      int rc = BootstrapStandby.run(toolArgs, conf);\n      terminate(rc);\n      return null; // avoid warning\n    case INITIALIZESHAREDEDITS:\n      aborted = initializeSharedEdits(conf,\n          startOpt.getForceFormat(),\n          startOpt.getInteractiveFormat());\n      terminate(aborted ? 1 : 0);\n      return null; // avoid warning\n    case BACKUP:\n    case CHECKPOINT:\n      NamenodeRole role = startOpt.toNodeRole();\n      DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n      return new BackupNode(conf, role);\n    case RECOVER:\n      NameNode.doRecovery(startOpt, conf);\n      return null;\n    case METADATAVERSION:\n      printMetadataVersion(conf);\n      terminate(0);\n      return null; // avoid javac warning\n    case UPGRADEONLY:\n      DefaultMetricsSystem.initialize(\"NameNode\");\n      new NameNode(conf);\n      terminate(0);\n      return null;\n    default:\n      DefaultMetricsSystem.initialize(\"NameNode\");\n      return new NameNode(conf);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.doRollback": "  public static boolean doRollback(Configuration conf,\n      boolean isConfirmationNeeded) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n\n    FSNamesystem nsys = new FSNamesystem(conf, new FSImage(conf));\n    System.err.print(\n        \"\\\"rollBack\\\" will remove the current state of the file system,\\n\"\n        + \"returning you to the state prior to initiating your recent.\\n\"\n        + \"upgrade. This action is permanent and cannot be undone. If you\\n\"\n        + \"are performing a rollback in an HA environment, you should be\\n\"\n        + \"certain that no NameNode process is running on any host.\");\n    if (isConfirmationNeeded) {\n      if (!confirmPrompt(\"Roll back file system state?\")) {\n        System.err.println(\"Rollback aborted.\");\n        return true;\n      }\n    }\n    nsys.getFSImage().doRollback(nsys);\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.printUsage": "  private static void printUsage(PrintStream out) {\n    out.println(USAGE + \"\\n\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits": "  private static boolean initializeSharedEdits(Configuration conf,\n      boolean force, boolean interactive) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    \n    if (conf.get(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY) == null) {\n      LOG.error(\"No shared edits directory configured for namespace \" +\n          nsId + \" namenode \" + namenodeId);\n      return false;\n    }\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      InetSocketAddress socAddr = DFSUtilClient.getNNAddress(conf);\n      SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n          DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, socAddr.getHostName());\n    }\n\n    NNStorage existingStorage = null;\n    FSImage sharedEditsImage = null;\n    try {\n      FSNamesystem fsns =\n          FSNamesystem.loadFromDisk(getConfigurationWithoutSharedEdits(conf));\n      \n      existingStorage = fsns.getFSImage().getStorage();\n      NamespaceInfo nsInfo = existingStorage.getNamespaceInfo();\n      \n      List<URI> sharedEditsDirs = FSNamesystem.getSharedEditsDirs(conf);\n      \n      sharedEditsImage = new FSImage(conf,\n          Lists.<URI>newArrayList(),\n          sharedEditsDirs);\n      sharedEditsImage.getEditLog().initJournalsForWrite();\n      \n      if (!sharedEditsImage.confirmFormat(force, interactive)) {\n        return true; // abort\n      }\n      \n      NNStorage newSharedStorage = sharedEditsImage.getStorage();\n      // Call Storage.format instead of FSImage.format here, since we don't\n      // actually want to save a checkpoint - just prime the dirs with\n      // the existing namespace info\n      newSharedStorage.format(nsInfo);\n      sharedEditsImage.getEditLog().formatNonFileJournals(nsInfo);\n\n      // Need to make sure the edit log segments are in good shape to initialize\n      // the shared edits dir.\n      fsns.getFSImage().getEditLog().close();\n      fsns.getFSImage().getEditLog().initJournalsForWrite();\n      fsns.getFSImage().getEditLog().recoverUnclosedStreams();\n\n      copyEditLogSegmentsToSharedDir(fsns, sharedEditsDirs, newSharedStorage,\n          conf);\n    } catch (IOException ioe) {\n      LOG.error(\"Could not initialize shared edits dir\", ioe);\n      return true; // aborted\n    } finally {\n      if (sharedEditsImage != null) {\n        try {\n          sharedEditsImage.close();\n        }  catch (IOException ioe) {\n          LOG.warn(\"Could not close sharedEditsImage\", ioe);\n        }\n      }\n      // Have to unlock storage explicitly for the case when we're running in a\n      // unit test, which runs in the same JVM as NNs.\n      if (existingStorage != null) {\n        try {\n          existingStorage.unlockAll();\n        } catch (IOException ioe) {\n          LOG.warn(\"Could not unlock storage directories\", ioe);\n          return true; // aborted\n        }\n      }\n    }\n    return false; // did not abort\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.setStartupOption": "  private static void setStartupOption(Configuration conf, StartupOption opt) {\n    conf.set(DFS_NAMENODE_STARTUP_KEY, opt.name());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.run": "          public FileSystem run() throws IOException {\n            return FileSystem.get(conf);\n          }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.parseArguments": "  static StartupOption parseArguments(String args[]) {\n    int argsLen = (args == null) ? 0 : args.length;\n    StartupOption startOpt = StartupOption.REGULAR;\n    for(int i=0; i < argsLen; i++) {\n      String cmd = args[i];\n      if (StartupOption.FORMAT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.FORMAT;\n        for (i = i + 1; i < argsLen; i++) {\n          if (args[i].equalsIgnoreCase(StartupOption.CLUSTERID.getName())) {\n            i++;\n            if (i >= argsLen) {\n              // if no cluster id specified, return null\n              LOG.error(\"Must specify a valid cluster ID after the \"\n                  + StartupOption.CLUSTERID.getName() + \" flag\");\n              return null;\n            }\n            String clusterId = args[i];\n            // Make sure an id is specified and not another flag\n            if (clusterId.isEmpty() ||\n                clusterId.equalsIgnoreCase(StartupOption.FORCE.getName()) ||\n                clusterId.equalsIgnoreCase(\n                    StartupOption.NONINTERACTIVE.getName())) {\n              LOG.error(\"Must specify a valid cluster ID after the \"\n                  + StartupOption.CLUSTERID.getName() + \" flag\");\n              return null;\n            }\n            startOpt.setClusterId(clusterId);\n          }\n\n          if (args[i].equalsIgnoreCase(StartupOption.FORCE.getName())) {\n            startOpt.setForceFormat(true);\n          }\n\n          if (args[i].equalsIgnoreCase(StartupOption.NONINTERACTIVE.getName())) {\n            startOpt.setInteractiveFormat(false);\n          }\n        }\n      } else if (StartupOption.GENCLUSTERID.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.GENCLUSTERID;\n      } else if (StartupOption.REGULAR.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.REGULAR;\n      } else if (StartupOption.BACKUP.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.BACKUP;\n      } else if (StartupOption.CHECKPOINT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.CHECKPOINT;\n      } else if (StartupOption.UPGRADE.getName().equalsIgnoreCase(cmd)\n          || StartupOption.UPGRADEONLY.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.UPGRADE.getName().equalsIgnoreCase(cmd) ? \n            StartupOption.UPGRADE : StartupOption.UPGRADEONLY;\n        /* Can be followed by CLUSTERID with a required parameter or\n         * RENAMERESERVED with an optional parameter\n         */\n        while (i + 1 < argsLen) {\n          String flag = args[i + 1];\n          if (flag.equalsIgnoreCase(StartupOption.CLUSTERID.getName())) {\n            if (i + 2 < argsLen) {\n              i += 2;\n              startOpt.setClusterId(args[i]);\n            } else {\n              LOG.error(\"Must specify a valid cluster ID after the \"\n                  + StartupOption.CLUSTERID.getName() + \" flag\");\n              return null;\n            }\n          } else if (flag.equalsIgnoreCase(StartupOption.RENAMERESERVED\n              .getName())) {\n            if (i + 2 < argsLen) {\n              FSImageFormat.setRenameReservedPairs(args[i + 2]);\n              i += 2;\n            } else {\n              FSImageFormat.useDefaultRenameReservedPairs();\n              i += 1;\n            }\n          } else {\n            LOG.error(\"Unknown upgrade flag \" + flag);\n            return null;\n          }\n        }\n      } else if (StartupOption.ROLLINGUPGRADE.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.ROLLINGUPGRADE;\n        ++i;\n        if (i >= argsLen) {\n          LOG.error(\"Must specify a rolling upgrade startup option \"\n              + RollingUpgradeStartupOption.getAllOptionString());\n          return null;\n        }\n        startOpt.setRollingUpgradeStartupOption(args[i]);\n      } else if (StartupOption.ROLLBACK.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.ROLLBACK;\n      } else if (StartupOption.IMPORT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.IMPORT;\n      } else if (StartupOption.BOOTSTRAPSTANDBY.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.BOOTSTRAPSTANDBY;\n        return startOpt;\n      } else if (StartupOption.INITIALIZESHAREDEDITS.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.INITIALIZESHAREDEDITS;\n        for (i = i + 1 ; i < argsLen; i++) {\n          if (StartupOption.NONINTERACTIVE.getName().equals(args[i])) {\n            startOpt.setInteractiveFormat(false);\n          } else if (StartupOption.FORCE.getName().equals(args[i])) {\n            startOpt.setForceFormat(true);\n          } else {\n            LOG.error(\"Invalid argument: \" + args[i]);\n            return null;\n          }\n        }\n        return startOpt;\n      } else if (StartupOption.RECOVER.getName().equalsIgnoreCase(cmd)) {\n        if (startOpt != StartupOption.REGULAR) {\n          throw new RuntimeException(\"Can't combine -recover with \" +\n              \"other startup options.\");\n        }\n        startOpt = StartupOption.RECOVER;\n        while (++i < argsLen) {\n          if (args[i].equalsIgnoreCase(\n                StartupOption.FORCE.getName())) {\n            startOpt.setForce(MetaRecoveryContext.FORCE_FIRST_CHOICE);\n          } else {\n            throw new RuntimeException(\"Error parsing recovery options: \" + \n              \"can't understand option \\\"\" + args[i] + \"\\\"\");\n          }\n        }\n      } else if (StartupOption.METADATAVERSION.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.METADATAVERSION;\n      } else {\n        return null;\n      }\n    }\n    return startOpt;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.printMetadataVersion": "  private static boolean printMetadataVersion(Configuration conf)\n    throws IOException {\n    final String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    final String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    NameNode.initializeGenericKeys(conf, nsId, namenodeId);\n    final FSImage fsImage = new FSImage(conf);\n    final FSNamesystem fs = new FSNamesystem(conf, fsImage, false);\n    return fsImage.recoverTransitionRead(\n      StartupOption.METADATAVERSION, fs, null);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.doRecovery": "  private static void doRecovery(StartupOption startOpt, Configuration conf)\n      throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    if (startOpt.getForce() < MetaRecoveryContext.FORCE_ALL) {\n      if (!confirmPrompt(\"You have selected Metadata Recovery mode.  \" +\n          \"This mode is intended to recover lost metadata on a corrupt \" +\n          \"filesystem.  Metadata recovery mode often permanently deletes \" +\n          \"data from your HDFS filesystem.  Please back up your edit log \" +\n          \"and fsimage before trying this!\\n\\n\" +\n          \"Are you ready to proceed? (Y/N)\\n\")) {\n        System.err.println(\"Recovery aborted at user request.\\n\");\n        return;\n      }\n    }\n    MetaRecoveryContext.LOG.info(\"starting recovery...\");\n    UserGroupInformation.setConfiguration(conf);\n    NameNode.initMetrics(conf, startOpt.toNodeRole());\n    FSNamesystem fsn = null;\n    try {\n      fsn = FSNamesystem.loadFromDisk(conf);\n      fsn.getFSImage().saveNamespace(fsn);\n      MetaRecoveryContext.LOG.info(\"RECOVERY COMPLETE\");\n    } catch (IOException e) {\n      MetaRecoveryContext.LOG.info(\"RECOVERY FAILED: caught exception\", e);\n      throw e;\n    } catch (RuntimeException e) {\n      MetaRecoveryContext.LOG.info(\"RECOVERY FAILED: caught exception\", e);\n      throw e;\n    } finally {\n      if (fsn != null)\n        fsn.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.format": "  private static boolean format(Configuration conf, boolean force,\n      boolean isInteractive) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    checkAllowFormat(conf);\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      InetSocketAddress socAddr = DFSUtilClient.getNNAddress(conf);\n      SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n          DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, socAddr.getHostName());\n    }\n    \n    Collection<URI> nameDirsToFormat = FSNamesystem.getNamespaceDirs(conf);\n    List<URI> sharedDirs = FSNamesystem.getSharedEditsDirs(conf);\n    List<URI> dirsToPrompt = new ArrayList<URI>();\n    dirsToPrompt.addAll(nameDirsToFormat);\n    dirsToPrompt.addAll(sharedDirs);\n    List<URI> editDirsToFormat = \n                 FSNamesystem.getNamespaceEditsDirs(conf);\n\n    // if clusterID is not provided - see if you can find the current one\n    String clusterId = StartupOption.FORMAT.getClusterId();\n    if(clusterId == null || clusterId.equals(\"\")) {\n      //Generate a new cluster id\n      clusterId = NNStorage.newClusterID();\n    }\n    System.out.println(\"Formatting using clusterid: \" + clusterId);\n    \n    FSImage fsImage = new FSImage(conf, nameDirsToFormat, editDirsToFormat);\n    try {\n      FSNamesystem fsn = new FSNamesystem(conf, fsImage);\n      fsImage.getEditLog().initJournalsForWrite();\n\n      if (!fsImage.confirmFormat(force, isInteractive)) {\n        return true; // aborted\n      }\n\n      fsImage.format(fsn, clusterId);\n    } catch (IOException ioe) {\n      LOG.warn(\"Encountered exception during format: \", ioe);\n      fsImage.close();\n      throw ioe;\n    }\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.main": "  public static void main(String argv[]) throws Exception {\n    if (DFSUtil.parseHelpArgument(argv, NameNode.USAGE, System.out, true)) {\n      System.exit(0);\n    }\n\n    try {\n      StringUtils.startupShutdownMessage(NameNode.class, argv, LOG);\n      NameNode namenode = createNameNode(argv, null);\n      if (namenode != null) {\n        namenode.join();\n      }\n    } catch (Throwable e) {\n      LOG.error(\"Failed to start namenode.\", e);\n      terminate(1, e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.join": "  public void join() {\n    try {\n      rpcServer.join();\n    } catch (InterruptedException ie) {\n      LOG.info(\"Caught interrupted exception \", ie);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirConcatOp.unprotectedConcat": "  static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,\n      INodeFile[] srcList, long timestamp) throws IOException {\n    assert fsd.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"\n          + targetIIP.getPath());\n    }\n\n    final INodeFile trgInode = targetIIP.getLastINode().asFile();\n    QuotaCounts deltas = computeQuotaDeltas(fsd, trgInode, srcList);\n    verifyQuota(fsd, targetIIP, deltas);\n\n    // the target file can be included in a snapshot\n    trgInode.recordModification(targetIIP.getLatestSnapshotId());\n    INodeDirectory trgParent = targetIIP.getINode(-2).asDirectory();\n    trgInode.concatBlocks(srcList, fsd.getBlockManager());\n\n    // since we are in the same dir - we can use same parent to remove files\n    int count = 0;\n    for (INodeFile nodeToRemove : srcList) {\n      if(nodeToRemove != null) {\n        nodeToRemove.clearBlocks();\n        nodeToRemove.getParent().removeChild(nodeToRemove);\n        fsd.getINodeMap().remove(nodeToRemove);\n        count++;\n      }\n    }\n\n    trgInode.setModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    trgParent.updateModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    // update quota on the parent directory with deltas\n    FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1, deltas);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirConcatOp.verifyQuota": "  private static void verifyQuota(FSDirectory fsd, INodesInPath targetIIP,\n      QuotaCounts deltas) throws QuotaExceededException {\n    if (!fsd.getFSNamesystem().isImageLoaded() || fsd.shouldSkipQuotaChecks()) {\n      // Do not check quota if editlog is still being processed\n      return;\n    }\n    FSDirectory.verifyQuota(targetIIP, targetIIP.length() - 1, deltas, null);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirConcatOp.computeQuotaDeltas": "  private static QuotaCounts computeQuotaDeltas(FSDirectory fsd,\n      INodeFile target, INodeFile[] srcList) {\n    QuotaCounts deltas = new QuotaCounts.Builder().build();\n    final short targetRepl = target.getPreferredBlockReplication();\n    for (INodeFile src : srcList) {\n      short srcRepl = src.getFileReplication();\n      long fileSize = src.computeFileSize();\n      if (targetRepl != srcRepl) {\n        deltas.addStorageSpace(fileSize * (targetRepl - srcRepl));\n        BlockStoragePolicy bsp =\n            fsd.getBlockStoragePolicySuite().getPolicy(src.getStoragePolicyID());\n        if (bsp != null) {\n          List<StorageType> srcTypeChosen = bsp.chooseStorageTypes(srcRepl);\n          for (StorageType t : srcTypeChosen) {\n            if (t.supportTypeQuota()) {\n              deltas.addTypeSpace(t, -fileSize);\n            }\n          }\n          List<StorageType> targetTypeChosen = bsp.chooseStorageTypes(targetRepl);\n          for (StorageType t : targetTypeChosen) {\n            if (t.supportTypeQuota()) {\n              deltas.addTypeSpace(t, fileSize);\n            }\n          }\n        }\n      }\n    }\n    return deltas;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFSNamesystem": "  FSNamesystem getFSNamesystem() {\n    return namesystem;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.isUnderConstruction": "  public boolean isUnderConstruction() {\n    return getFileUnderConstructionFeature() != null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.getFileUnderConstructionFeature": "  public final FileUnderConstructionFeature getFileUnderConstructionFeature() {\n    return getFeature(FileUnderConstructionFeature.class);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetQuota": "  static INodeDirectory unprotectedSetQuota(\n      FSDirectory fsd, INodesInPath iip, long nsQuota,\n      long ssQuota, StorageType type)\n      throws FileNotFoundException, PathIsNotDirectoryException,\n      QuotaExceededException, UnresolvedLinkException,\n      SnapshotAccessControlException, UnsupportedActionException {\n    assert fsd.hasWriteLock();\n    // sanity check\n    if ((nsQuota < 0 && nsQuota != HdfsConstants.QUOTA_DONT_SET &&\n         nsQuota != HdfsConstants.QUOTA_RESET) ||\n        (ssQuota < 0 && ssQuota != HdfsConstants.QUOTA_DONT_SET &&\n          ssQuota != HdfsConstants.QUOTA_RESET)) {\n      throw new IllegalArgumentException(\"Illegal value for nsQuota or \" +\n                                         \"ssQuota : \" + nsQuota + \" and \" +\n                                         ssQuota);\n    }\n    // sanity check for quota by storage type\n    if ((type != null) && (!fsd.isQuotaByStorageTypeEnabled() ||\n        nsQuota != HdfsConstants.QUOTA_DONT_SET)) {\n      throw new UnsupportedActionException(\n          \"Failed to set quota by storage type because either\" +\n          DFS_QUOTA_BY_STORAGETYPE_ENABLED_KEY + \" is set to \" +\n          fsd.isQuotaByStorageTypeEnabled() + \" or nsQuota value is illegal \" +\n          nsQuota);\n    }\n\n    INodeDirectory dirNode =\n        INodeDirectory.valueOf(iip.getLastINode(), iip.getPath());\n    if (dirNode.isRoot() && nsQuota == HdfsConstants.QUOTA_RESET) {\n      throw new IllegalArgumentException(\"Cannot clear namespace quota on root.\");\n    } else { // a directory inode\n      final QuotaCounts oldQuota = dirNode.getQuotaCounts();\n      final long oldNsQuota = oldQuota.getNameSpace();\n      final long oldSsQuota = oldQuota.getStorageSpace();\n\n      if (nsQuota == HdfsConstants.QUOTA_DONT_SET) {\n        nsQuota = oldNsQuota;\n      }\n      if (ssQuota == HdfsConstants.QUOTA_DONT_SET) {\n        ssQuota = oldSsQuota;\n      }\n\n      // unchanged space/namespace quota\n      if (type == null && oldNsQuota == nsQuota && oldSsQuota == ssQuota) {\n        return null;\n      }\n\n      // unchanged type quota\n      if (type != null) {\n          EnumCounters<StorageType> oldTypeQuotas = oldQuota.getTypeSpaces();\n          if (oldTypeQuotas != null && oldTypeQuotas.get(type) == ssQuota) {\n              return null;\n          }\n      }\n\n      final int latest = iip.getLatestSnapshotId();\n      dirNode.recordModification(latest);\n      dirNode.setQuota(fsd.getBlockStoragePolicySuite(), nsQuota, ssQuota, type);\n      return dirNode;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setQuota": "  static void setQuota(FSDirectory fsd, String src, long nsQuota, long ssQuota,\n      StorageType type) throws IOException {\n    FSPermissionChecker pc = fsd.getPermissionChecker();\n    if (fsd.isPermissionEnabled()) {\n      pc.checkSuperuserPrivilege();\n    }\n\n    fsd.writeLock();\n    try {\n      INodesInPath iip = fsd.resolvePath(pc, src, DirOp.WRITE);\n      INodeDirectory changed =\n          unprotectedSetQuota(fsd, iip, nsQuota, ssQuota, type);\n      if (changed != null) {\n        final QuotaCounts q = changed.getQuotaCounts();\n        if (type == null) {\n          fsd.getEditLog().logSetQuota(src, q.getNameSpace(), q.getStorageSpace());\n        } else {\n          fsd.getEditLog().logSetQuotaByStorageType(\n              src, q.getTypeSpaces().get(type), type);\n        }\n      }\n    } finally {\n      fsd.writeUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.deleteForEditLog": "  static void deleteForEditLog(FSDirectory fsd, INodesInPath iip, long mtime)\n      throws IOException {\n    assert fsd.hasWriteLock();\n    FSNamesystem fsn = fsd.getFSNamesystem();\n    BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n    List<INode> removedINodes = new ChunkedArrayList<>();\n    List<Long> removedUCFiles = new ChunkedArrayList<>();\n    if (!deleteAllowed(iip)) {\n      return;\n    }\n    List<INodeDirectory> snapshottableDirs = new ArrayList<>();\n    FSDirSnapshotOp.checkSnapshot(fsd, iip, snapshottableDirs);\n    boolean filesRemoved = unprotectedDelete(fsd, iip,\n        new ReclaimContext(fsd.getBlockStoragePolicySuite(),\n            collectedBlocks, removedINodes, removedUCFiles),\n        mtime);\n    fsn.removeSnapshottableDirs(snapshottableDirs);\n\n    if (filesRemoved) {\n      fsn.removeLeasesAndINodes(removedUCFiles, removedINodes, false);\n      fsn.getBlockManager().removeBlocksAndUpdateSafemodeTotal(collectedBlocks);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.unprotectedDelete": "  private static boolean unprotectedDelete(FSDirectory fsd, INodesInPath iip,\n      ReclaimContext reclaimContext, long mtime) {\n    assert fsd.hasWriteLock();\n\n    // check if target node exists\n    INode targetNode = iip.getLastINode();\n    if (targetNode == null) {\n      return false;\n    }\n\n    // record modification\n    final int latestSnapshot = iip.getLatestSnapshotId();\n    targetNode.recordModification(latestSnapshot);\n\n    // Remove the node from the namespace\n    long removed = fsd.removeLastINode(iip);\n    if (removed == -1) {\n      return false;\n    }\n\n    // set the parent's modification time\n    final INodeDirectory parent = targetNode.getParent();\n    parent.updateModificationTime(mtime, latestSnapshot);\n\n    // collect block and update quota\n    if (!targetNode.isInLatestSnapshot(latestSnapshot)) {\n      targetNode.destroyAndCollectBlocks(reclaimContext);\n    } else {\n      targetNode.cleanSubtree(reclaimContext, CURRENT_STATE_ID, latestSnapshot);\n    }\n\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.unprotectedDelete: \"\n          + iip.getPath() + \" is removed\");\n    }\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.deleteAllowed": "  private static boolean deleteAllowed(final INodesInPath iip) {\n    if (iip.length() < 1 || iip.getLastINode() == null) {\n      if (NameNode.stateChangeLog.isDebugEnabled()) {\n        NameNode.stateChangeLog.debug(\n            \"DIR* FSDirectory.unprotectedDelete: failed to remove \"\n                + iip.getPath() + \" because it does not exist\");\n      }\n      return false;\n    } else if (iip.length() == 1) { // src is the root\n      NameNode.stateChangeLog.warn(\n          \"DIR* FSDirectory.unprotectedDelete: failed to remove \" +\n              iip.getPath() + \" because the root is not allowed to be deleted\");\n      return false;\n    }\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirForEditLog": "  static void mkdirForEditLog(FSDirectory fsd, long inodeId, String src,\n      PermissionStatus permissions, List<AclEntry> aclEntries, long timestamp)\n      throws QuotaExceededException, UnresolvedLinkException, AclException,\n      FileAlreadyExistsException, ParentNotDirectoryException,\n      AccessControlException {\n    assert fsd.hasWriteLock();\n    INodesInPath iip = fsd.getINodesInPath(src, DirOp.WRITE_LINK);\n    final byte[] localName = iip.getLastLocalName();\n    final INodesInPath existing = iip.getParentINodesInPath();\n    Preconditions.checkState(existing.getLastINode() != null);\n    unprotectedMkdir(fsd, inodeId, existing, localName, permissions, aclEntries,\n        timestamp);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.unprotectedMkdir": "  private static INodesInPath unprotectedMkdir(FSDirectory fsd, long inodeId,\n      INodesInPath parent, byte[] name, PermissionStatus permission,\n      List<AclEntry> aclEntries, long timestamp)\n      throws QuotaExceededException, AclException, FileAlreadyExistsException {\n    assert fsd.hasWriteLock();\n    assert parent.getLastINode() != null;\n    if (!parent.getLastINode().isDirectory()) {\n      throw new FileAlreadyExistsException(\"Parent path is not a directory: \" +\n          parent.getPath() + \" \" + DFSUtil.bytes2String(name));\n    }\n    final INodeDirectory dir = new INodeDirectory(inodeId, name, permission,\n        timestamp);\n\n    INodesInPath iip =\n        fsd.addLastINode(parent, dir, permission.getPermission(), true);\n    if (iip != null && aclEntries != null) {\n      AclStorage.updateINodeAcl(dir, aclEntries, Snapshot.CURRENT_STATE_ID);\n    }\n    return iip;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirXAttrOp.unprotectedSetXAttrs": "  static INode unprotectedSetXAttrs(\n      FSDirectory fsd, final INodesInPath iip, final List<XAttr> xAttrs,\n      final EnumSet<XAttrSetFlag> flag)\n      throws IOException {\n    assert fsd.hasWriteLock();\n    INode inode = FSDirectory.resolveLastINode(iip);\n    List<XAttr> existingXAttrs = XAttrStorage.readINodeXAttrs(inode);\n    List<XAttr> newXAttrs = setINodeXAttrs(fsd, existingXAttrs, xAttrs, flag);\n    final boolean isFile = inode.isFile();\n\n    for (XAttr xattr : newXAttrs) {\n      final String xaName = XAttrHelper.getPrefixedName(xattr);\n\n      /*\n       * If we're adding the encryption zone xattr, then add src to the list\n       * of encryption zones.\n       */\n      if (CRYPTO_XATTR_ENCRYPTION_ZONE.equals(xaName)) {\n        final HdfsProtos.ZoneEncryptionInfoProto ezProto =\n            HdfsProtos.ZoneEncryptionInfoProto.parseFrom(xattr.getValue());\n        fsd.ezManager.addEncryptionZone(inode.getId(),\n            PBHelperClient.convert(ezProto.getSuite()),\n            PBHelperClient.convert(ezProto.getCryptoProtocolVersion()),\n            ezProto.getKeyName());\n\n        if (ezProto.hasReencryptionProto()) {\n          ReencryptionInfoProto reProto = ezProto.getReencryptionProto();\n          fsd.ezManager.getReencryptionStatus()\n              .updateZoneStatus(inode.getId(), iip.getPath(), reProto);\n        }\n      }\n\n      if (!isFile && SECURITY_XATTR_UNREADABLE_BY_SUPERUSER.equals(xaName)) {\n        throw new IOException(\"Can only set '\" +\n            SECURITY_XATTR_UNREADABLE_BY_SUPERUSER + \"' on a file.\");\n      }\n    }\n\n    XAttrStorage.updateINodeXAttrs(inode, newXAttrs, iip.getLatestSnapshotId());\n    return inode;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirXAttrOp.setINodeXAttrs": "  static List<XAttr> setINodeXAttrs(\n      FSDirectory fsd, final List<XAttr> existingXAttrs,\n      final List<XAttr> toSet, final EnumSet<XAttrSetFlag> flag)\n      throws IOException {\n    // Check for duplicate XAttrs in toSet\n    // We need to use a custom comparator, so using a HashSet is not suitable\n    for (int i = 0; i < toSet.size(); i++) {\n      for (int j = i + 1; j < toSet.size(); j++) {\n        if (toSet.get(i).equalsIgnoreValue(toSet.get(j))) {\n          throw new IOException(\"Cannot specify the same XAttr to be set \" +\n              \"more than once\");\n        }\n      }\n    }\n\n    // Count the current number of user-visible XAttrs for limit checking\n    int userVisibleXAttrsNum = 0; // Number of user visible xAttrs\n\n    // The XAttr list is copied to an exactly-sized array when it's stored,\n    // so there's no need to size it precisely here.\n    int newSize = (existingXAttrs != null) ? existingXAttrs.size() : 0;\n    newSize += toSet.size();\n    List<XAttr> xAttrs = Lists.newArrayListWithCapacity(newSize);\n\n    // Check if the XAttr already exists to validate with the provided flag\n    for (XAttr xAttr: toSet) {\n      boolean exist = false;\n      if (existingXAttrs != null) {\n        for (XAttr a : existingXAttrs) {\n          if (a.equalsIgnoreValue(xAttr)) {\n            exist = true;\n            break;\n          }\n        }\n      }\n      XAttrSetFlag.validate(xAttr.getName(), exist, flag);\n      // add the new XAttr since it passed validation\n      xAttrs.add(xAttr);\n      if (isUserVisible(xAttr)) {\n        userVisibleXAttrsNum++;\n      }\n    }\n\n    // Add the existing xattrs back in, if they weren't already set\n    if (existingXAttrs != null) {\n      for (XAttr existing : existingXAttrs) {\n        boolean alreadySet = false;\n        for (XAttr set : toSet) {\n          if (set.equalsIgnoreValue(existing)) {\n            alreadySet = true;\n            break;\n          }\n        }\n        if (!alreadySet) {\n          xAttrs.add(existing);\n          if (isUserVisible(existing)) {\n            userVisibleXAttrsNum++;\n          }\n        }\n      }\n    }\n\n    if (userVisibleXAttrsNum > fsd.getInodeXAttrsLimit()) {\n      throw new IOException(\"Cannot add additional XAttr to inode, \"\n          + \"would exceed limit of \" + fsd.getInodeXAttrsLimit());\n    }\n\n    return xAttrs;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetStoragePolicy": "  static void unprotectedSetStoragePolicy(FSDirectory fsd, BlockManager bm,\n      INodesInPath iip, final byte policyId)\n      throws IOException {\n    assert fsd.hasWriteLock();\n    final INode inode = iip.getLastINode();\n    if (inode == null) {\n      throw new FileNotFoundException(\"File/Directory does not exist: \"\n          + iip.getPath());\n    }\n    final int snapshotId = iip.getLatestSnapshotId();\n    if (inode.isFile()) {\n      if (policyId != HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED) {\n        BlockStoragePolicy newPolicy = bm.getStoragePolicy(policyId);\n        if (newPolicy.isCopyOnCreateFile()) {\n          throw new HadoopIllegalArgumentException(\"Policy \" + newPolicy\n              + \" cannot be set after file creation.\");\n        }\n      }\n\n      BlockStoragePolicy currentPolicy =\n          bm.getStoragePolicy(inode.getLocalStoragePolicyID());\n\n      if (currentPolicy != null && currentPolicy.isCopyOnCreateFile()) {\n        throw new HadoopIllegalArgumentException(\n            \"Existing policy \" + currentPolicy.getName() +\n                \" cannot be changed after file creation.\");\n      }\n      inode.asFile().setStoragePolicyID(policyId, snapshotId);\n    } else if (inode.isDirectory()) {\n      setDirStoragePolicy(fsd, iip, policyId);\n    } else {\n      throw new FileNotFoundException(iip.getPath()\n          + \" is not a file or directory\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.getStoragePolicy": "  static BlockStoragePolicy getStoragePolicy(FSDirectory fsd, BlockManager bm,\n      String path) throws IOException {\n    FSPermissionChecker pc = fsd.getPermissionChecker();\n    fsd.readLock();\n    try {\n      final INodesInPath iip = fsd.resolvePath(pc, path, DirOp.READ_LINK);\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.READ);\n      }\n      INode inode = iip.getLastINode();\n      if (inode == null) {\n        throw new FileNotFoundException(\"File/Directory does not exist: \"\n            + iip.getPath());\n      }\n      return bm.getStoragePolicy(inode.getStoragePolicyID());\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setDirStoragePolicy": "  private static void setDirStoragePolicy(\n      FSDirectory fsd, INodesInPath iip, byte policyId) throws IOException {\n    INode inode = FSDirectory.resolveLastINode(iip);\n    List<XAttr> existingXAttrs = XAttrStorage.readINodeXAttrs(inode);\n    XAttr xAttr = BlockStoragePolicySuite.buildXAttr(policyId);\n    List<XAttr> newXAttrs = null;\n    if (policyId == HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED) {\n      List<XAttr> toRemove = Lists.newArrayList();\n      toRemove.add(xAttr);\n      List<XAttr> removed = Lists.newArrayList();\n      newXAttrs = FSDirXAttrOp.filterINodeXAttrs(existingXAttrs, toRemove,\n          removed);\n    } else {\n      newXAttrs = FSDirXAttrOp.setINodeXAttrs(fsd, existingXAttrs,\n          Arrays.asList(xAttr),\n          EnumSet.of(XAttrSetFlag.CREATE, XAttrSetFlag.REPLACE));\n    }\n    XAttrStorage.updateINodeXAttrs(inode, newXAttrs, iip.getLatestSnapshotId());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAclOp.unprotectedSetAcl": "  static List<AclEntry> unprotectedSetAcl(FSDirectory fsd, INodesInPath iip,\n      List<AclEntry> aclSpec, boolean fromEdits) throws IOException {\n    assert fsd.hasWriteLock();\n\n    // ACL removal is logged to edits as OP_SET_ACL with an empty list.\n    if (aclSpec.isEmpty()) {\n      unprotectedRemoveAcl(fsd, iip);\n      return AclFeature.EMPTY_ENTRY_LIST;\n    }\n\n    INode inode = FSDirectory.resolveLastINode(iip);\n    int snapshotId = iip.getLatestSnapshotId();\n    List<AclEntry> newAcl = aclSpec;\n    if (!fromEdits) {\n      List<AclEntry> existingAcl = AclStorage.readINodeLogicalAcl(inode);\n      newAcl = AclTransformation.replaceAclEntries(existingAcl, aclSpec);\n    }\n    AclStorage.updateINodeAcl(inode, newAcl, snapshotId);\n    return newAcl;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAclOp.unprotectedRemoveAcl": "  private static void unprotectedRemoveAcl(FSDirectory fsd, INodesInPath iip)\n      throws IOException {\n    assert fsd.hasWriteLock();\n    INode inode = FSDirectory.resolveLastINode(iip);\n    int snapshotId = iip.getLatestSnapshotId();\n    AclFeature f = inode.getAclFeature();\n    if (f == null) {\n      return;\n    }\n\n    FsPermission perm = inode.getFsPermission();\n    List<AclEntry> featureEntries = AclStorage.getEntriesFromAclFeature(f);\n    if (featureEntries.get(0).getScope() == AclEntryScope.ACCESS) {\n      // Restore group permissions from the feature's entry to permission\n      // bits, overwriting the mask, which is not part of a minimal ACL.\n      AclEntry groupEntryKey = new AclEntry.Builder()\n          .setScope(AclEntryScope.ACCESS).setType(AclEntryType.GROUP).build();\n      int groupEntryIndex = Collections.binarySearch(\n          featureEntries, groupEntryKey,\n          AclTransformation.ACL_ENTRY_COMPARATOR);\n      Preconditions.checkPositionIndex(groupEntryIndex, featureEntries.size(),\n          \"Invalid group entry index after binary-searching inode: \" +\n              inode.getFullPathName() + \"(\" + inode.getId() + \") \"\n              + \"with featureEntries:\" + featureEntries);\n      FsAction groupPerm = featureEntries.get(groupEntryIndex).getPermission();\n      FsPermission newPerm = new FsPermission(perm.getUserAction(), groupPerm,\n          perm.getOtherAction(), perm.getStickyBit());\n      inode.setPermission(newPerm, snapshotId);\n    }\n\n    inode.removeAclFeature(snapshotId);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetPermission": "  static void unprotectedSetPermission(\n      FSDirectory fsd, INodesInPath iip, FsPermission permissions)\n      throws FileNotFoundException, UnresolvedLinkException,\n             QuotaExceededException, SnapshotAccessControlException {\n    assert fsd.hasWriteLock();\n    final INode inode = FSDirectory.resolveLastINode(iip);\n    int snapshotId = iip.getLatestSnapshotId();\n    inode.setPermission(permissions, snapshotId);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setPermission": "  static FileStatus setPermission(\n      FSDirectory fsd, final String src, FsPermission permission)\n      throws IOException {\n    if (FSDirectory.isExactReservedName(src)) {\n      throw new InvalidPathException(src);\n    }\n    FSPermissionChecker pc = fsd.getPermissionChecker();\n    INodesInPath iip;\n    fsd.writeLock();\n    try {\n      iip = fsd.resolvePath(pc, src, DirOp.WRITE);\n      fsd.checkOwner(pc, iip);\n      unprotectedSetPermission(fsd, iip, permission);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsd.getEditLog().logSetPermissions(iip.getPath(), permission);\n    return fsd.getAuditFileInfo(iip);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp.unprotectedTruncate": "  private static boolean unprotectedTruncate(FSNamesystem fsn,\n      INodesInPath iip, long newLength, BlocksMapUpdateInfo collectedBlocks,\n      long mtime, QuotaCounts delta) throws IOException {\n    assert fsn.hasWriteLock();\n\n    INodeFile file = iip.getLastINode().asFile();\n    int latestSnapshot = iip.getLatestSnapshotId();\n    file.recordModification(latestSnapshot, true);\n\n    verifyQuotaForTruncate(fsn, iip, file, newLength, delta);\n\n    Set<BlockInfo> toRetain = file.getSnapshotBlocksToRetain(latestSnapshot);\n    long remainingLength = file.collectBlocksBeyondMax(newLength,\n        collectedBlocks, toRetain);\n    file.setModificationTime(mtime);\n    // return whether on a block boundary\n    return (remainingLength - newLength) == 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp.verifyQuotaForTruncate": "  private static void verifyQuotaForTruncate(FSNamesystem fsn,\n      INodesInPath iip, INodeFile file, long newLength, QuotaCounts delta)\n      throws QuotaExceededException {\n    FSDirectory fsd = fsn.getFSDirectory();\n    if (!fsn.isImageLoaded() || fsd.shouldSkipQuotaChecks()) {\n      // Do not check quota if edit log is still being processed\n      return;\n    }\n    final BlockStoragePolicy policy = fsd.getBlockStoragePolicySuite()\n        .getPolicy(file.getStoragePolicyID());\n    file.computeQuotaDeltaForTruncate(newLength, policy, delta);\n    fsd.readLock();\n    try {\n      FSDirectory.verifyQuota(iip, iip.length() - 1, delta, null);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp.prepareFileForTruncate": "  static Block prepareFileForTruncate(FSNamesystem fsn, INodesInPath iip,\n      String leaseHolder, String clientMachine, long lastBlockDelta,\n      Block newBlock) throws IOException {\n    assert fsn.hasWriteLock();\n\n    INodeFile file = iip.getLastINode().asFile();\n    assert !file.isStriped();\n    file.recordModification(iip.getLatestSnapshotId());\n    file.toUnderConstruction(leaseHolder, clientMachine);\n    assert file.isUnderConstruction() : \"inode should be under construction.\";\n    fsn.getLeaseManager().addLease(\n        file.getFileUnderConstructionFeature().getClientName(), file.getId());\n    boolean shouldRecoverNow = (newBlock == null);\n    BlockInfo oldBlock = file.getLastBlock();\n\n    boolean shouldCopyOnTruncate = shouldCopyOnTruncate(fsn, file, oldBlock);\n    if (newBlock == null) {\n      newBlock = (shouldCopyOnTruncate) ?\n          fsn.createNewBlock(BlockType.CONTIGUOUS)\n          : new Block(oldBlock.getBlockId(), oldBlock.getNumBytes(),\n          fsn.nextGenerationStamp(fsn.getBlockManager().isLegacyBlock(\n              oldBlock)));\n    }\n\n    final BlockInfo truncatedBlockUC;\n    BlockManager blockManager = fsn.getFSDirectory().getBlockManager();\n    if (shouldCopyOnTruncate) {\n      // Add new truncateBlock into blocksMap and\n      // use oldBlock as a source for copy-on-truncate recovery\n      truncatedBlockUC = new BlockInfoContiguous(newBlock,\n          file.getPreferredBlockReplication());\n      truncatedBlockUC.convertToBlockUnderConstruction(\n          BlockUCState.UNDER_CONSTRUCTION, blockManager.getStorages(oldBlock));\n      truncatedBlockUC.setNumBytes(oldBlock.getNumBytes() - lastBlockDelta);\n      truncatedBlockUC.getUnderConstructionFeature().setTruncateBlock(oldBlock);\n      file.setLastBlock(truncatedBlockUC);\n      blockManager.addBlockCollection(truncatedBlockUC, file);\n\n      NameNode.stateChangeLog.debug(\n          \"BLOCK* prepareFileForTruncate: Scheduling copy-on-truncate to new\"\n              + \" size {}  new block {} old block {}\",\n          truncatedBlockUC.getNumBytes(), newBlock, oldBlock);\n    } else {\n      // Use new generation stamp for in-place truncate recovery\n      blockManager.convertLastBlockToUnderConstruction(file, lastBlockDelta);\n      oldBlock = file.getLastBlock();\n      assert !oldBlock.isComplete() : \"oldBlock should be under construction\";\n      BlockUnderConstructionFeature uc = oldBlock.getUnderConstructionFeature();\n      uc.setTruncateBlock(new Block(oldBlock));\n      uc.getTruncateBlock().setNumBytes(oldBlock.getNumBytes() - lastBlockDelta);\n      uc.getTruncateBlock().setGenerationStamp(newBlock.getGenerationStamp());\n      truncatedBlockUC = oldBlock;\n\n      NameNode.stateChangeLog.debug(\"BLOCK* prepareFileForTruncate: \" +\n          \"{} Scheduling in-place block truncate to new size {}\",\n          uc, uc.getTruncateBlock().getNumBytes());\n    }\n    if (shouldRecoverNow) {\n      truncatedBlockUC.getUnderConstructionFeature().initializeBlockRecovery(\n          truncatedBlockUC, newBlock.getGenerationStamp(), true);\n    }\n\n    return newBlock;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getINodesInPath": "  public INodesInPath getINodesInPath(byte[][] components, DirOp dirOp)\n      throws UnresolvedLinkException, AccessControlException,\n      ParentNotDirectoryException {\n    INodesInPath iip = INodesInPath.resolve(rootDir, components);\n    checkTraverse(null, iip, dirOp);\n    return iip;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse": "  void checkTraverse(FSPermissionChecker pc, INodesInPath iip,\n      DirOp dirOp) throws AccessControlException, UnresolvedPathException,\n          ParentNotDirectoryException {\n    final boolean resolveLink;\n    switch (dirOp) {\n      case READ_LINK:\n      case WRITE_LINK:\n      case CREATE_LINK:\n        resolveLink = false;\n        break;\n      default:\n        resolveLink = true;\n        break;\n    }\n    checkTraverse(pc, iip, resolveLink);\n    boolean allowSnapshot = (dirOp == DirOp.READ || dirOp == DirOp.READ_LINK);\n    if (!allowSnapshot && iip.isSnapshot()) {\n      throw new SnapshotAccessControlException(\n          \"Modification on a read-only snapshot is disallowed\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getPathComponents": "  public static byte[][] getPathComponents(INode inode) {\n    List<byte[]> components = new ArrayList<byte[]>();\n    components.add(0, inode.getLocalNameBytes());\n    while(inode.getParent() != null) {\n      components.add(0, inode.getParent().getLocalNameBytes());\n      inode = inode.getParent();\n    }\n    return components.toArray(new byte[components.size()][]);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirErasureCodingOp.unprotectedGetErasureCodingPolicy": "  static ErasureCodingPolicy unprotectedGetErasureCodingPolicy(\n      final FSNamesystem fsn, final INodesInPath iip) throws IOException {\n    assert fsn.hasReadLock();\n\n    return getErasureCodingPolicyForPath(fsn.getFSDirectory(), iip);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirErasureCodingOp.getErasureCodingPolicyForPath": "  private static ErasureCodingPolicy getErasureCodingPolicyForPath(\n      FSDirectory fsd, INodesInPath iip) throws IOException {\n    Preconditions.checkNotNull(iip, \"INodes cannot be null\");\n    fsd.readLock();\n    try {\n      for (int i = iip.length() - 1; i >= 0; i--) {\n        final INode inode = iip.getINode(i);\n        if (inode == null) {\n          continue;\n        }\n        if (inode.isFile()) {\n          byte id = inode.asFile().getErasureCodingPolicyID();\n          return id < 0 ? null :\n              fsd.getFSNamesystem().getErasureCodingPolicyManager().getByID(id);\n        }\n        // We don't allow setting EC policies on paths with a symlink. Thus\n        // if a symlink is encountered, the dir shouldn't have EC policy.\n        // TODO: properly support symlinks\n        if (inode.isSymlink()) {\n          return null;\n        }\n        final XAttrFeature xaf = inode.getXAttrFeature();\n        if (xaf != null) {\n          XAttr xattr = xaf.getXAttr(XATTR_ERASURECODING_POLICY);\n          if (xattr != null) {\n            ByteArrayInputStream bIn = new ByteArrayInputStream(xattr.getValue());\n            DataInputStream dIn = new DataInputStream(bIn);\n            String ecPolicyName = WritableUtils.readString(dIn);\n            return fsd.getFSNamesystem().getErasureCodingPolicyManager()\n              .getByName(ecPolicyName);\n          }\n        }\n      }\n    } finally {\n      fsd.readUnlock();\n    }\n    return null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.createFileStatusForEditLog": "  static HdfsFileStatus createFileStatusForEditLog(\n      FSDirectory fsd, INodesInPath iip) throws IOException {\n    return createFileStatus(fsd, iip,\n        null, HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, false);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.createFileStatus": "  private static HdfsFileStatus createFileStatus(\n      long length, boolean isdir,\n      int replication, long blocksize, long mtime, long atime,\n      FsPermission permission, EnumSet<HdfsFileStatus.Flags> flags,\n      String owner, String group, byte[] symlink, byte[] path, long fileId,\n      int childrenNum, FileEncryptionInfo feInfo, byte storagePolicy,\n      ErasureCodingPolicy ecPolicy, LocatedBlocks locations) {\n    if (locations == null) {\n      return new HdfsFileStatus(length, isdir, replication, blocksize,\n          mtime, atime, permission, flags, owner, group, symlink, path,\n          fileId, childrenNum, feInfo, storagePolicy, ecPolicy);\n    } else {\n      return new HdfsLocatedFileStatus(length, isdir, replication, blocksize,\n          mtime, atime, permission, flags, owner, group, symlink, path,\n          fileId, locations, childrenNum, feInfo, storagePolicy, ecPolicy);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.hasRpcIds": "  public boolean hasRpcIds() {\n    return rpcClientId != RpcConstants.DUMMY_CLIENT_ID\n        && rpcCallId != RpcConstants.INVALID_CALL_ID;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.prepareFileForAppend": "  static LocatedBlock prepareFileForAppend(final FSNamesystem fsn,\n      final INodesInPath iip, final String leaseHolder,\n      final String clientMachine, final boolean newBlock,\n      final boolean writeToEditLog, final boolean logRetryCache)\n      throws IOException {\n    assert fsn.hasWriteLock();\n\n    final INodeFile file = iip.getLastINode().asFile();\n    final QuotaCounts delta = verifyQuotaForUCBlock(fsn, file, iip);\n\n    file.recordModification(iip.getLatestSnapshotId());\n    file.toUnderConstruction(leaseHolder, clientMachine);\n\n    fsn.getLeaseManager().addLease(\n        file.getFileUnderConstructionFeature().getClientName(), file.getId());\n\n    LocatedBlock ret = null;\n    if (!newBlock) {\n      FSDirectory fsd = fsn.getFSDirectory();\n      ret = fsd.getBlockManager().convertLastBlockToUnderConstruction(file, 0);\n      if (ret != null && delta != null) {\n        Preconditions.checkState(delta.getStorageSpace() >= 0, \"appending to\"\n            + \" a block with size larger than the preferred block size\");\n        fsd.writeLock();\n        try {\n          fsd.updateCountNoQuotaCheck(iip, iip.length() - 1, delta);\n        } finally {\n          fsd.writeUnlock();\n        }\n      }\n    } else {\n      BlockInfo lastBlock = file.getLastBlock();\n      if (lastBlock != null) {\n        ExtendedBlock blk = new ExtendedBlock(fsn.getBlockPoolId(), lastBlock);\n        ret = new LocatedBlock(blk, new DatanodeInfo[0]);\n      }\n    }\n\n    if (writeToEditLog) {\n      final String path = iip.getPath();\n      if (NameNodeLayoutVersion.supports(Feature.APPEND_NEW_BLOCK,\n          fsn.getEffectiveLayoutVersion())) {\n        fsn.getEditLog().logAppendFile(path, file, newBlock, logRetryCache);\n      } else {\n        fsn.getEditLog().logOpenFile(path, file, false, logRetryCache);\n      }\n    }\n    return ret;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.verifyQuotaForUCBlock": "  private static QuotaCounts verifyQuotaForUCBlock(FSNamesystem fsn,\n      INodeFile file, INodesInPath iip) throws QuotaExceededException {\n    FSDirectory fsd = fsn.getFSDirectory();\n    if (!fsn.isImageLoaded() || fsd.shouldSkipQuotaChecks()) {\n      // Do not check quota if editlog is still being processed\n      return null;\n    }\n    if (file.getLastBlock() != null) {\n      final QuotaCounts delta = computeQuotaDeltaForUCBlock(fsn, file);\n      fsd.readLock();\n      try {\n        FSDirectory.verifyQuota(iip, iip.length() - 1, delta, null);\n        return delta;\n      } finally {\n        fsd.readUnlock();\n      }\n    }\n    return null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.toCompleteFile": "  void toCompleteFile(long mtime, int numCommittedAllowed, short minReplication) {\n    final FileUnderConstructionFeature uc = getFileUnderConstructionFeature();\n    Preconditions.checkNotNull(uc, \"File %s is not under construction\", this);\n    assertAllBlocksComplete(numCommittedAllowed, minReplication);\n    removeFeature(uc);\n    setModificationTime(mtime);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.assertAllBlocksComplete": "  private void assertAllBlocksComplete(int numCommittedAllowed,\n      short minReplication) {\n    for (int i = 0; i < blocks.length; i++) {\n      final String err = checkBlockComplete(blocks, i, numCommittedAllowed,\n          minReplication);\n      if(err != null) {\n        throw new IllegalStateException(String.format(\"Unexpected block state: \" +\n            \"%s, file=%s (%s), blocks=%s (i=%s)\", err, this,\n            getClass().getSimpleName(), Arrays.asList(blocks), i));\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirSymlinkOp.unprotectedAddSymlink": "  static INodeSymlink unprotectedAddSymlink(FSDirectory fsd, INodesInPath iip,\n      byte[] localName, long id, String target, long mtime, long atime,\n      PermissionStatus perm)\n      throws UnresolvedLinkException, QuotaExceededException {\n    assert fsd.hasWriteLock();\n    final INodeSymlink symlink = new INodeSymlink(id, null, perm, mtime, atime,\n        target);\n    symlink.setLocalName(localName);\n    return fsd.addINode(iip, symlink, perm.getPermission()) != null ?\n        symlink : null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirXAttrOp.unprotectedRemoveXAttrs": "  static List<XAttr> unprotectedRemoveXAttrs(\n      FSDirectory fsd, final INodesInPath iip, final List<XAttr> toRemove)\n      throws IOException {\n    assert fsd.hasWriteLock();\n    INode inode = FSDirectory.resolveLastINode(iip);\n    int snapshotId = iip.getLatestSnapshotId();\n    List<XAttr> existingXAttrs = XAttrStorage.readINodeXAttrs(inode);\n    List<XAttr> removedXAttrs = Lists.newArrayListWithCapacity(toRemove.size());\n    List<XAttr> newXAttrs = filterINodeXAttrs(existingXAttrs, toRemove,\n                                              removedXAttrs);\n    if (existingXAttrs.size() != newXAttrs.size()) {\n      XAttrStorage.updateINodeXAttrs(inode, newXAttrs, snapshotId);\n      return removedXAttrs;\n    }\n    return null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirXAttrOp.filterINodeXAttrs": "  static List<XAttr> filterINodeXAttrs(\n      final List<XAttr> existingXAttrs, final List<XAttr> toFilter,\n      final List<XAttr> filtered)\n    throws AccessControlException {\n    if (existingXAttrs == null || existingXAttrs.isEmpty() ||\n        toFilter == null || toFilter.isEmpty()) {\n      return existingXAttrs;\n    }\n\n    // Populate a new list with XAttrs that pass the filter\n    List<XAttr> newXAttrs =\n        Lists.newArrayListWithCapacity(existingXAttrs.size());\n    for (XAttr a : existingXAttrs) {\n      boolean add = true;\n      for (ListIterator<XAttr> it = toFilter.listIterator(); it.hasNext()\n          ;) {\n        XAttr filter = it.next();\n        Preconditions.checkArgument(\n            !KEYID_XATTR.equalsIgnoreValue(filter),\n            \"The encryption zone xattr should never be deleted.\");\n        if (UNREADABLE_BY_SUPERUSER_XATTR.equalsIgnoreValue(filter)) {\n          throw new AccessControlException(\"The xattr '\" +\n              SECURITY_XATTR_UNREADABLE_BY_SUPERUSER + \"' can not be deleted.\");\n        }\n        if (a.equalsIgnoreValue(filter)) {\n          add = false;\n          it.remove();\n          filtered.add(filter);\n          break;\n        }\n      }\n      if (add) {\n        newXAttrs.add(a);\n      }\n    }\n\n    return newXAttrs;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.length": "  public int length() {\n    return inodes.length;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog": "  static INodeFile addFileForEditLog(\n      FSDirectory fsd, long id, INodesInPath existing, byte[] localName,\n      PermissionStatus permissions, List<AclEntry> aclEntries,\n      List<XAttr> xAttrs, short replication, long modificationTime, long atime,\n      long preferredBlockSize, boolean underConstruction, String clientName,\n      String clientMachine, byte storagePolicyId) {\n    final INodeFile newNode;\n    Preconditions.checkNotNull(existing);\n    assert fsd.hasWriteLock();\n    try {\n      // check if the file has an EC policy\n      boolean isStriped = false;\n      ErasureCodingPolicy ecPolicy = FSDirErasureCodingOp.\n          unprotectedGetErasureCodingPolicy(fsd.getFSNamesystem(), existing);\n      if (ecPolicy != null) {\n        isStriped = true;\n      }\n      final BlockType blockType = isStriped ?\n          BlockType.STRIPED : BlockType.CONTIGUOUS;\n      final Short replicationFactor = (!isStriped ? replication : null);\n      final Byte ecPolicyID = (isStriped ? ecPolicy.getId() : null);\n      if (underConstruction) {\n        newNode = newINodeFile(id, permissions, modificationTime,\n            modificationTime, replicationFactor, ecPolicyID, preferredBlockSize,\n            storagePolicyId, blockType);\n        newNode.toUnderConstruction(clientName, clientMachine);\n      } else {\n        newNode = newINodeFile(id, permissions, modificationTime, atime,\n            replicationFactor, ecPolicyID, preferredBlockSize,\n            storagePolicyId, blockType);\n      }\n      newNode.setLocalName(localName);\n      INodesInPath iip = fsd.addINode(existing, newNode,\n          permissions.getPermission());\n      if (iip != null) {\n        if (aclEntries != null) {\n          AclStorage.updateINodeAcl(newNode, aclEntries, CURRENT_STATE_ID);\n        }\n        if (xAttrs != null) {\n          XAttrStorage.updateINodeXAttrs(newNode, xAttrs, CURRENT_STATE_ID);\n        }\n        return newNode;\n      }\n    } catch (IOException e) {\n      NameNode.stateChangeLog.warn(\n          \"DIR* FSDirectory.unprotectedAddFile: exception when add \" + existing\n              .getPath() + \" to the file system\", e);\n      if (e instanceof FSLimitException.MaxDirectoryItemsExceededException) {\n        NameNode.stateChangeLog.warn(\"Please increase \"\n            + \"dfs.namenode.fs-limits.max-directory-items and make it \"\n            + \"consistent across all NameNodes.\");\n      }\n    }\n    return null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.newINodeFile": "  private static INodeFile newINodeFile(long id, PermissionStatus permissions,\n      long mtime, long atime, Short replication, Byte ecPolicyID,\n      long preferredBlockSize, BlockType blockType) {\n    return newINodeFile(id, permissions, mtime, atime, replication, ecPolicyID,\n        preferredBlockSize, (byte)0, blockType);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetOwner": "  static void unprotectedSetOwner(\n      FSDirectory fsd, INodesInPath iip, String username, String groupname)\n      throws FileNotFoundException, UnresolvedLinkException,\n      QuotaExceededException, SnapshotAccessControlException {\n    assert fsd.hasWriteLock();\n    final INode inode = FSDirectory.resolveLastINode(iip);\n    if (username != null) {\n      inode.setUser(username, iip.getLatestSnapshotId());\n    }\n    if (groupname != null) {\n      inode.setGroup(groupname, iip.getLatestSnapshotId());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getLastINode": "  public INode getLastINode() {\n    return getINode(-1);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getINode": "  public INode getINode(int i) {\n    return inodes[(i < 0) ? inodes.length + i : i];\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getLastLocalName": "  byte[] getLastLocalName() {\n    return path[path.length - 1];\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetTimes": "  static boolean unprotectedSetTimes(\n      FSDirectory fsd, INodesInPath iip, long mtime, long atime, boolean force)\n          throws QuotaExceededException {\n    assert fsd.hasWriteLock();\n    boolean status = false;\n    INode inode = iip.getLastINode();\n    int latest = iip.getLatestSnapshotId();\n    if (mtime != -1) {\n      inode = inode.setModificationTime(mtime, latest);\n      status = true;\n    }\n\n    // if the last access time update was within the last precision interval,\n    // then no need to store access time\n    if (atime != -1 && (status || force\n        || atime > inode.getAccessTime() + fsd.getAccessTimePrecision())) {\n      inode.setAccessTime(atime, latest);\n      status = true;\n    }\n    return status;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getExistingINodes": "  public INodesInPath getExistingINodes() {\n    Preconditions.checkState(!isSnapshot());\n    for (int i = inodes.length; i > 0; i--) {\n      if (inodes[i - 1] != null) {\n        return (i == inodes.length) ? this : getAncestorINodesInPath(i);\n      }\n    }\n    return null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getAncestorINodesInPath": "  private INodesInPath getAncestorINodesInPath(int length) {\n    Preconditions.checkArgument(length >= 0 && length < inodes.length);\n    Preconditions.checkState(isDotSnapshotDir() || !isSnapshot());\n    final INode[] anodes = new INode[length];\n    final byte[][] apath = new byte[length][];\n    System.arraycopy(this.inodes, 0, anodes, 0, length);\n    System.arraycopy(this.path, 0, apath, 0, length);\n    return new INodesInPath(anodes, apath, isRaw, false, snapshotId);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.isSnapshot": "  boolean isSnapshot() {\n    return this.isSnapshot;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.replace": "  public static INodesInPath replace(INodesInPath iip, int pos, INode inode) {\n    Preconditions.checkArgument(iip.length() > 0 && pos > 0 // no for root\n        && pos < iip.length());\n    if (iip.getINode(pos) == null) {\n      Preconditions.checkState(iip.getINode(pos - 1) != null);\n    }\n    INode[] inodes = new INode[iip.inodes.length];\n    System.arraycopy(iip.inodes, 0, inodes, 0, inodes.length);\n    inodes[pos] = inode;\n    return new INodesInPath(inodes, iip.path, iip.isRaw,\n        iip.isSnapshot, iip.snapshotId);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetReplication": "  static BlockInfo[] unprotectedSetReplication(\n      FSDirectory fsd, INodesInPath iip, short replication)\n      throws QuotaExceededException, UnresolvedLinkException,\n      SnapshotAccessControlException, UnsupportedActionException {\n    assert fsd.hasWriteLock();\n\n    final BlockManager bm = fsd.getBlockManager();\n    final INode inode = iip.getLastINode();\n    if (inode == null || !inode.isFile() || inode.asFile().isStriped()) {\n      // TODO we do not support replication on stripe layout files yet\n      return null;\n    }\n\n    INodeFile file = inode.asFile();\n    // Make sure the directory has sufficient quotas\n    short oldBR = file.getPreferredBlockReplication();\n\n    long size = file.computeFileSize(true, true);\n    // Ensure the quota does not exceed\n    if (oldBR < replication) {\n      fsd.updateCount(iip, 0L, size, oldBR, replication, true);\n    }\n\n    file.setFileReplication(replication, iip.getLatestSnapshotId());\n    short targetReplication = (short) Math.max(\n        replication, file.getPreferredBlockReplication());\n\n    if (oldBR > replication) {\n      fsd.updateCount(iip, 0L, size, oldBR, targetReplication, true);\n    }\n    for (BlockInfo b : file.getBlocks()) {\n      bm.setReplication(oldBR, targetReplication, b);\n    }\n\n    if (oldBR != -1) {\n      if (oldBR > targetReplication) {\n        FSDirectory.LOG.info(\"Decreasing replication from {} to {} for {}\",\n                             oldBR, targetReplication, iip.getPath());\n      } else {\n        FSDirectory.LOG.info(\"Increasing replication from {} to {} for {}\",\n                             oldBR, targetReplication, iip.getPath());\n      }\n    }\n    return file.getBlocks();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setReplication": "  static boolean setReplication(\n      FSDirectory fsd, BlockManager bm, String src, final short replication)\n      throws IOException {\n    bm.verifyReplication(src, replication, null);\n    final boolean isFile;\n    FSPermissionChecker pc = fsd.getPermissionChecker();\n    fsd.writeLock();\n    try {\n      final INodesInPath iip = fsd.resolvePath(pc, src, DirOp.WRITE);\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n      }\n\n      final BlockInfo[] blocks = unprotectedSetReplication(fsd, iip,\n                                                           replication);\n      isFile = blocks != null;\n      if (isFile) {\n        fsd.getEditLog().logSetReplication(iip.getPath(), replication);\n      }\n    } finally {\n      fsd.writeUnlock();\n    }\n    return isFile;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getINode": "  public INode getINode(String src, DirOp dirOp) throws UnresolvedLinkException,\n      AccessControlException, ParentNotDirectoryException {\n    return getINodesInPath(src, dirOp).getLastINode();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog": "  static void renameForEditLog(\n      FSDirectory fsd, String src, String dst, long timestamp,\n      Options.Rename... options)\n      throws IOException {\n    BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n    final INodesInPath srcIIP = fsd.getINodesInPath(src, DirOp.WRITE_LINK);\n    final INodesInPath dstIIP = fsd.getINodesInPath(dst, DirOp.WRITE_LINK);\n    unprotectedRenameTo(fsd, srcIIP, dstIIP, timestamp,\n        collectedBlocks, options);\n    if (!collectedBlocks.getToDeleteList().isEmpty()) {\n      fsd.getFSNamesystem().getBlockManager()\n          .removeBlocksAndUpdateSafemodeTotal(collectedBlocks);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.dstForRenameTo": "  private static INodesInPath dstForRenameTo(\n      INodesInPath srcIIP, INodesInPath dstIIP) throws IOException {\n    INode dstINode = dstIIP.getLastINode();\n    if (dstINode != null && dstINode.isDirectory()) {\n      byte[] childName = srcIIP.getLastLocalName();\n      // new dest might exist so look it up.\n      INode childINode = dstINode.asDirectory().getChild(\n          childName, dstIIP.getPathSnapshotId());\n      dstIIP = INodesInPath.append(dstIIP, childINode, childName);\n    }\n    return dstIIP;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo": "  static RenameResult unprotectedRenameTo(FSDirectory fsd,\n      final INodesInPath srcIIP, final INodesInPath dstIIP, long timestamp,\n      BlocksMapUpdateInfo collectedBlocks, Options.Rename... options)\n      throws IOException {\n    assert fsd.hasWriteLock();\n    boolean overwrite = options != null\n        && Arrays.asList(options).contains(Options.Rename.OVERWRITE);\n\n    final String src = srcIIP.getPath();\n    final String dst = dstIIP.getPath();\n    final String error;\n    final INode srcInode = srcIIP.getLastINode();\n    validateRenameSource(fsd, srcIIP);\n\n    // validate the destination\n    if (dst.equals(src)) {\n      throw new FileAlreadyExistsException(\"The source \" + src +\n          \" and destination \" + dst + \" are the same\");\n    }\n    validateDestination(src, dst, srcInode);\n\n    if (dstIIP.length() == 1) {\n      error = \"rename destination cannot be the root\";\n      NameNode.stateChangeLog.warn(\"DIR* FSDirectory.unprotectedRenameTo: \" +\n          error);\n      throw new IOException(error);\n    }\n\n    BlockStoragePolicySuite bsps = fsd.getBlockStoragePolicySuite();\n    fsd.ezManager.checkMoveValidity(srcIIP, dstIIP);\n    final INode dstInode = dstIIP.getLastINode();\n    List<INodeDirectory> snapshottableDirs = new ArrayList<>();\n    if (dstInode != null) { // Destination exists\n      validateOverwrite(src, dst, overwrite, srcInode, dstInode);\n      FSDirSnapshotOp.checkSnapshot(fsd, dstIIP, snapshottableDirs);\n    }\n\n    INode dstParent = dstIIP.getINode(-2);\n    if (dstParent == null) {\n      error = \"rename destination parent \" + dst + \" not found.\";\n      NameNode.stateChangeLog.warn(\"DIR* FSDirectory.unprotectedRenameTo: \" +\n          error);\n      throw new FileNotFoundException(error);\n    }\n    if (!dstParent.isDirectory()) {\n      error = \"rename destination parent \" + dst + \" is a file.\";\n      NameNode.stateChangeLog.warn(\"DIR* FSDirectory.unprotectedRenameTo: \" +\n          error);\n      throw new ParentNotDirectoryException(error);\n    }\n\n    // Ensure dst has quota to accommodate rename\n    verifyFsLimitsForRename(fsd, srcIIP, dstIIP);\n    verifyQuotaForRename(fsd, srcIIP, dstIIP);\n\n    RenameOperation tx = new RenameOperation(fsd, srcIIP, dstIIP);\n\n    boolean undoRemoveSrc = true;\n    tx.removeSrc();\n\n    boolean undoRemoveDst = false;\n    long removedNum = 0;\n    try {\n      if (dstInode != null) { // dst exists, remove it\n        removedNum = tx.removeDst();\n        if (removedNum != -1) {\n          undoRemoveDst = true;\n        }\n      }\n\n      // add src as dst to complete rename\n      INodesInPath renamedIIP = tx.addSourceToDestination();\n      if (renamedIIP != null) {\n        undoRemoveSrc = false;\n        if (NameNode.stateChangeLog.isDebugEnabled()) {\n          NameNode.stateChangeLog.debug(\"DIR* FSDirectory.unprotectedRenameTo: \"\n              + src + \" is renamed to \" + dst);\n        }\n\n        tx.updateMtimeAndLease(timestamp);\n\n        // Collect the blocks and remove the lease for previous dst\n        boolean filesDeleted = false;\n        if (undoRemoveDst) {\n          undoRemoveDst = false;\n          if (removedNum > 0) {\n            filesDeleted = tx.cleanDst(bsps, collectedBlocks);\n          }\n        }\n\n        if (snapshottableDirs.size() > 0) {\n          // There are snapshottable directories (without snapshots) to be\n          // deleted. Need to update the SnapshotManager.\n          fsd.getFSNamesystem().removeSnapshottableDirs(snapshottableDirs);\n        }\n\n        tx.updateQuotasInSourceTree(bsps);\n        return createRenameResult(\n            fsd, renamedIIP, filesDeleted, collectedBlocks);\n      }\n    } finally {\n      if (undoRemoveSrc) {\n        tx.restoreSource();\n      }\n      if (undoRemoveDst) { // Rename failed - restore dst\n        tx.restoreDst(bsps);\n      }\n    }\n    NameNode.stateChangeLog.warn(\"DIR* FSDirectory.unprotectedRenameTo: \" +\n        \"failed to rename \" + src + \" to \" + dst);\n    throw new IOException(\"rename from \" + src + \" to \" + dst + \" failed.\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.editLogLoaderPrompt": "  public static void editLogLoaderPrompt(String prompt,\n        MetaRecoveryContext recovery, String contStr)\n        throws IOException, RequestStopException\n  {\n    if (recovery == null) {\n      throw new IOException(prompt);\n    }\n    LOG.error(prompt);\n    String answer = recovery.ask(\"\\nEnter 'c' to continue, \" + contStr + \"\\n\" +\n      \"Enter 's' to stop reading the edit log here, abandoning any later \" +\n        \"edits\\n\" +\n      \"Enter 'q' to quit without saving\\n\" +\n      \"Enter 'a' to always select the first choice in the future \" +\n      \"without prompting. \" + \n      \"(c/s/q/a)\\n\", \"c\", \"s\", \"q\", \"a\");\n    if (answer.equals(\"c\")) {\n      LOG.info(\"Continuing\");\n      return;\n    } else if (answer.equals(\"s\")) {\n      throw new RequestStopException(\"user requested stop\");\n    } else if (answer.equals(\"q\")) {\n      recovery.quit();\n    } else {\n      recovery.setForce(FORCE_FIRST_CHOICE);\n      return;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.quit": "  public void quit() {\n    LOG.error(\"Exiting on user request.\");\n    System.exit(0);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.ask": "  public String ask(String prompt, String firstChoice, String... choices) \n      throws IOException {\n    while (true) {\n      System.err.print(prompt);\n      if (force > FORCE_NONE) {\n        System.out.println(\"automatically choosing \" + firstChoice);\n        return firstChoice;\n      }\n      StringBuilder responseBuilder = new StringBuilder();\n      while (true) {\n        int c = System.in.read();\n        if (c == -1 || c == '\\r' || c == '\\n') {\n          break;\n        }\n        responseBuilder.append((char)c);\n      }\n      String response = responseBuilder.toString();\n      if (response.equalsIgnoreCase(firstChoice))\n        return firstChoice;\n      for (String c : choices) {\n        if (response.equalsIgnoreCase(c)) {\n          return c;\n        }\n      }\n      System.err.print(\"I'm sorry, I cannot understand your response.\\n\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.setForce": "  public void setForce(int force) {\n    this.force = force;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getStartupProgress": "  public static StartupProgress getStartupProgress() {\n    return startupProgress;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.hasTransactionId": "  public boolean hasTransactionId() {\n    return (txid != HdfsServerConstants.INVALID_TXID);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.writeUnlock": "  void writeUnlock() {\n    this.dirLock.writeLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.writeLock": "  void writeLock() {\n    this.dirLock.writeLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.getTransactionId": "  public long getTransactionId() {\n    Preconditions.checkState(txid != HdfsServerConstants.INVALID_TXID);\n    return txid;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.closeAllStreams": "  static void closeAllStreams(Iterable<EditLogInputStream> streams) {\n    for (EditLogInputStream s : streams) {\n      IOUtils.closeStream(s);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditLogStreams": "  static Iterable<EditLogInputStream> getEditLogStreams(NNStorage storage)\n      throws IOException {\n    FSImagePreTransactionalStorageInspector inspector \n      = new FSImagePreTransactionalStorageInspector();\n    storage.inspectStorageDirs(inspector);\n\n    List<EditLogInputStream> editStreams = new ArrayList<EditLogInputStream>();\n    for (File f : inspector.getLatestEditsFiles()) {\n      editStreams.add(new EditLogFileInputStream(f));\n    }\n    return editStreams;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getLatestEditsFiles": "  private List<File> getLatestEditsFiles() {\n    if (latestNameCheckpointTime > latestEditsCheckpointTime) {\n      // the image is already current, discard edits\n      LOG.debug(\n          \"Name checkpoint time is newer than edits, not loading edits.\");\n      return Collections.emptyList();\n    }\n    \n    return getEditsInStorageDir(latestEditsSD);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getLatestImages": "  abstract List<FSImageFile> getLatestImages() throws IOException;\n\n  /** \n   * Get the minimum tx id which should be loaded with this set of images.\n   */\n  abstract long getMaxSeenTxId();\n\n  /**\n   * @return true if the directories are in such a state that the image should be re-saved\n   * following the load\n   */\n  abstract boolean needToSave();\n\n  /**\n   * Record of an image that has been located and had its filename parsed.\n   */\n  static class FSImageFile {\n    final StorageDirectory sd;    \n    final long txId;\n    private final File file;\n    \n    FSImageFile(StorageDirectory sd, File file, long txId) {\n      assert txId >= 0 || txId == HdfsServerConstants.INVALID_TXID\n        : \"Invalid txid on \" + file +\": \" + txId;\n      \n      this.sd = sd;\n      this.txId = txId;\n      this.file = file;\n    } \n    \n    File getFile() {\n      return file;\n    }\n\n    public long getCheckpointTxId() {\n      return txId;\n    }\n    \n    @Override\n    public String toString() {\n      return String.format(\"FSImageFile(file=%s, cpktTxId=%019d)\", \n                           file.toString(), txId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion.supports": "  public static boolean supports(final LayoutFeature f, final int lv) {\n    return LayoutVersion.supports(FEATURES, f, lv);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.newLoader": "  public static LoaderDelegator newLoader(Configuration conf, FSNamesystem fsn) {\n    return new LoaderDelegator(conf, fsn);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.needToSave": "  abstract boolean needToSave();\n\n  /**\n   * Record of an image that has been located and had its filename parsed.\n   */\n  static class FSImageFile {\n    final StorageDirectory sd;    \n    final long txId;\n    private final File file;\n    \n    FSImageFile(StorageDirectory sd, File file, long txId) {\n      assert txId >= 0 || txId == HdfsServerConstants.INVALID_TXID\n        : \"Invalid txid on \" + file +\": \" + txId;\n      \n      this.sd = sd;\n      this.txId = txId;\n      this.file = file;\n    } \n    \n    File getFile() {\n      return file;\n    }\n\n    public long getCheckpointTxId() {\n      return txId;\n    }\n    \n    @Override\n    public String toString() {\n      return String.format(\"FSImageFile(file=%s, cpktTxId=%019d)\", \n                           file.toString(), txId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getMaxSeenTxId": "  abstract long getMaxSeenTxId();\n\n  /**\n   * @return true if the directories are in such a state that the image should be re-saved\n   * following the load\n   */\n  abstract boolean needToSave();\n\n  /**\n   * Record of an image that has been located and had its filename parsed.\n   */\n  static class FSImageFile {\n    final StorageDirectory sd;    \n    final long txId;\n    private final File file;\n    \n    FSImageFile(StorageDirectory sd, File file, long txId) {\n      assert txId >= 0 || txId == HdfsServerConstants.INVALID_TXID\n        : \"Invalid txid on \" + file +\": \" + txId;\n      \n      this.sd = sd;\n      this.txId = txId;\n      this.file = file;\n    } \n    \n    File getFile() {\n      return file;\n    }\n\n    public long getCheckpointTxId() {\n      return txId;\n    }\n    \n    @Override\n    public String toString() {\n      return String.format(\"FSImageFile(file=%s, cpktTxId=%019d)\", \n                           file.toString(), txId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.openEditLogForWrite": "  void openEditLogForWrite(int layoutVersion) throws IOException {\n    assert editLog != null : \"editLog must be initialized\";\n    editLog.openForWrite(layoutVersion);\n    storage.writeTransactionIdFileToStorage(editLog.getCurSegmentTxId());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.format": "  void format(FSNamesystem fsn, String clusterId) throws IOException {\n    long fileCount = fsn.getFilesTotal();\n    // Expect 1 file, which is the root inode\n    Preconditions.checkState(fileCount == 1,\n        \"FSImage.format should be called with an uninitialized namesystem, has \" +\n        fileCount + \" files\");\n    NamespaceInfo ns = NNStorage.newNamespaceInfo();\n    LOG.info(\"Allocated new BlockPoolId: \" + ns.getBlockPoolID());\n    ns.clusterID = clusterId;\n    \n    storage.format(ns);\n    editLog.formatNonFileJournals(ns);\n    saveFSImageInAllDirs(fsn, 0);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImageInAllDirs": "  private synchronized void saveFSImageInAllDirs(FSNamesystem source,\n      NameNodeFile nnf, long txid, Canceler canceler) throws IOException {\n    StartupProgress prog = NameNode.getStartupProgress();\n    prog.beginPhase(Phase.SAVING_CHECKPOINT);\n    if (storage.getNumStorageDirs(NameNodeDirType.IMAGE) == 0) {\n      throw new IOException(\"No image directories available!\");\n    }\n    if (canceler == null) {\n      canceler = new Canceler();\n    }\n    SaveNamespaceContext ctx = new SaveNamespaceContext(\n        source, txid, canceler);\n    \n    try {\n      List<Thread> saveThreads = new ArrayList<Thread>();\n      // save images into current\n      for (Iterator<StorageDirectory> it\n             = storage.dirIterator(NameNodeDirType.IMAGE); it.hasNext();) {\n        StorageDirectory sd = it.next();\n        FSImageSaver saver = new FSImageSaver(ctx, sd, nnf);\n        Thread saveThread = new Thread(saver, saver.toString());\n        saveThreads.add(saveThread);\n        saveThread.start();\n      }\n      waitForThreads(saveThreads);\n      saveThreads.clear();\n      storage.reportErrorsOnDirectories(ctx.getErrorSDs());\n  \n      if (storage.getNumStorageDirs(NameNodeDirType.IMAGE) == 0) {\n        throw new IOException(\n          \"Failed to save in any storage directories while saving namespace.\");\n      }\n      if (canceler.isCancelled()) {\n        deleteCancelledCheckpoint(txid);\n        ctx.checkCancelled(); // throws\n        assert false : \"should have thrown above!\";\n      }\n  \n      renameCheckpoint(txid, NameNodeFile.IMAGE_NEW, nnf, false);\n  \n      // Since we now have a new checkpoint, we can clean up some\n      // old edit logs and checkpoints.\n      purgeOldStorage(nnf);\n      archivalManager.purgeCheckpoints(NameNodeFile.IMAGE_NEW);\n    } finally {\n      // Notify any threads waiting on the checkpoint to be canceled\n      // that it is complete.\n      ctx.markComplete();\n      ctx = null;\n    }\n    prog.endPhase(Phase.SAVING_CHECKPOINT);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getStorage": "  public NNStorage getStorage() {\n    return storage;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getStartupOption": "  public static StartupOption getStartupOption(Configuration conf) {\n    return StartupOption.valueOf(conf.get(DFS_NAMENODE_STARTUP_KEY,\n                                          StartupOption.REGULAR.toString()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getNameNodeMetrics": "  public static NameNodeMetrics getNameNodeMetrics() {\n    return metrics;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.newClusterID": "  public static String newClusterID() {\n    return \"CID-\" + UUID.randomUUID().toString();\n  }"
        },
        "bug_report": {
            "Title": "Edit log corruption due to hard lease recovery of not-closed file which has snapshots",
            "Description": "HDFS-6257 and HDFS-7707 worked hard to prevent corruption from combinations of client operations.\n\nRecently, we have observed NN not able to start with the following exception:\n{noformat}\n2017-08-17 14:32:18,418 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.\njava.io.FileNotFoundException: File does not exist: /home/Events/CancellationSurvey_MySQL/2015/12/31/.part-00000.9nlJ3M\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:66)\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:429)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:232)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:141)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:897)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:750)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:318)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1125)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:789)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:614)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:676)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:844)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:823)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1547)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1615)\n{noformat}\n\nQuoting a nicely analysed edits:\n{quote}\nIn the edits logged about 1 hour later, we see this failing OP_CLOSE. The sequence in the edits shows the file going through:\n\n  OPEN\n  ADD_BLOCK\n  CLOSE\n  ADD_BLOCK # perhaps this was an append\n  DELETE\n  (about 1 hour later) CLOSE\n\nIt is interesting that there was no CLOSE logged before the delete.\n{quote}\n\nGrepping that file name, it turns out the close was triggered by {{LeaseManager}}, when the lease reaches hard limit.\n{noformat}\n2017-08-16 15:05:45,927 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: \n  Recovering [Lease.  Holder: DFSClient_NONMAPREDUCE_-1997177597_28, pending creates: 75], \n  src=/home/Events/CancellationSurvey_MySQL/2015/12/31/.part-00000.9nlJ3M\n2017-08-16 15:05:45,927 WARN org.apache.hadoop.hdfs.StateChange: BLOCK* \n  internalReleaseLease: All existing blocks are COMPLETE, lease removed, file \n  /home/Events/CancellationSurvey_MySQL/2015/12/31/.part-00000.9nlJ3M closed.\n{noformat}"
        }
    },
    {
        "filename": "HDFS-6462.json",
        "creation_time": "2014-05-29T00:19:03.000+0000",
        "stack_trace": "```\njavax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1414)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1363)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n        at com.sun.proxy.$Proxy14.getFsStats(Unknown Source)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)\n        at com.sun.proxy.$Proxy14.getFsStats(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getStats(ClientNamenodeProtocolTranslatorPB.java:554)\n        at org.apache.hadoop.hdfs.DFSClient.getDiskStatus(DFSClient.java:2165)\n        at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.fsstat(RpcProgramNfs3.java:1659)\n        at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.handleInternal(RpcProgramNfs3.java:1961)\n        at org.apache.hadoop.oncrpc.RpcProgram.messageReceived(RpcProgram.java:162)\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:281)\n        at org.apache.hadoop.oncrpc.RpcUtil$RpcMessageParserStage.messageReceived(RpcUtil.java:132)\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)\n        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)\n        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)\n        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\nCaused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:677)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)\n        at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:640)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:724)\n        at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1381)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.wrapException": "  public static IOException wrapException(final String destHost,\n                                          final int destPort,\n                                          final String localHost,\n                                          final int localPort,\n                                          final IOException exception) {\n    if (exception instanceof BindException) {\n      return new BindException(\n          \"Problem binding to [\"\n              + localHost\n              + \":\"\n              + localPort\n              + \"] \"\n              + exception\n              + \";\"\n              + see(\"BindException\"));\n    } else if (exception instanceof ConnectException) {\n      // connection refused; include the host:port in the error\n      return wrapWithMessage(exception, \n          \"Call From \"\n              + localHost\n              + \" to \"\n              + destHost\n              + \":\"\n              + destPort\n              + \" failed on connection exception: \"\n              + exception\n              + \";\"\n              + see(\"ConnectionRefused\"));\n    } else if (exception instanceof UnknownHostException) {\n      return wrapWithMessage(exception,\n          \"Invalid host name: \"\n              + getHostDetailsAsString(destHost, destPort, localHost)\n              + exception\n              + \";\"\n              + see(\"UnknownHost\"));\n    } else if (exception instanceof SocketTimeoutException) {\n      return wrapWithMessage(exception,\n          \"Call From \"\n              + localHost + \" to \" + destHost + \":\" + destPort\n              + \" failed on socket timeout exception: \" + exception\n              + \";\"\n              + see(\"SocketTimeout\"));\n    } else if (exception instanceof NoRouteToHostException) {\n      return wrapWithMessage(exception,\n          \"No Route to Host from  \"\n              + localHost + \" to \" + destHost + \":\" + destPort\n              + \" failed on socket timeout exception: \" + exception\n              + \";\"\n              + see(\"NoRouteToHost\"));\n    }\n    else {\n      return (IOException) new IOException(\"Failed on local exception: \"\n                                               + exception\n                                               + \"; Host Details : \"\n                                               + getHostDetailsAsString(destHost, destPort, localHost))\n          .initCause(exception);\n\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.getHostDetailsAsString": "  private static String getHostDetailsAsString(final String destHost,\n                                               final int destPort,\n                                               final String localHost) {\n    StringBuilder hostDetails = new StringBuilder(27);\n    hostDetails.append(\"local host is: \")\n        .append(quoteHost(localHost))\n        .append(\"; \");\n    hostDetails.append(\"destination host is: \").append(quoteHost(destHost))\n        .append(\":\")\n        .append(destPort).append(\"; \");\n    return hostDetails.toString();\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.see": "  private static String see(final String entry) {\n    return FOR_MORE_DETAILS_SEE + HADOOP_WIKI + entry;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.wrapWithMessage": "  private static <T extends IOException> T wrapWithMessage(\n      T exception, String msg) {\n    Class<? extends Throwable> clazz = exception.getClass();\n    try {\n      Constructor<? extends Throwable> ctor = clazz.getConstructor(String.class);\n      Throwable t = ctor.newInstance(msg);\n      return (T)(t.initCause(exception));\n    } catch (Throwable e) {\n      LOG.warn(\"Unable to wrap exception of type \" +\n          clazz + \": it has no (String) constructor\", e);\n      return exception;\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.call": "  public Writable call(RPC.RpcKind rpcKind, Writable rpcRequest,\n      ConnectionId remoteId, int serviceClass) throws IOException {\n    final Call call = createCall(rpcKind, rpcRequest);\n    Connection connection = getConnection(remoteId, call, serviceClass);\n    try {\n      connection.sendRpcRequest(call);                 // send the rpc request\n    } catch (RejectedExecutionException e) {\n      throw new IOException(\"connection has been closed\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      LOG.warn(\"interrupted waiting to send rpc request to server\", e);\n      throw new IOException(e);\n    }\n\n    boolean interrupted = false;\n    synchronized (call) {\n      while (!call.done) {\n        try {\n          call.wait();                           // wait for the result\n        } catch (InterruptedException ie) {\n          // save the fact that we were interrupted\n          interrupted = true;\n        }\n      }\n\n      if (interrupted) {\n        // set the interrupt flag now that we are done waiting\n        Thread.currentThread().interrupt();\n      }\n\n      if (call.error != null) {\n        if (call.error instanceof RemoteException) {\n          call.error.fillInStackTrace();\n          throw call.error;\n        } else { // local exception\n          InetSocketAddress address = connection.getRemoteAddress();\n          throw NetUtils.wrapException(address.getHostName(),\n                  address.getPort(),\n                  NetUtils.getHostname(),\n                  0,\n                  call.error);\n        }\n      } else {\n        return call.getRpcResponse();\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.sendRpcRequest": "    public void sendRpcRequest(final Call call)\n        throws InterruptedException, IOException {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n\n      // Serialize the call to be sent. This is done from the actual\n      // caller thread, rather than the sendParamsExecutor thread,\n      \n      // so that if the serialization throws an error, it is reported\n      // properly. This also parallelizes the serialization.\n      //\n      // Format of a call on the wire:\n      // 0) Length of rest below (1 + 2)\n      // 1) RpcRequestHeader  - is serialized Delimited hence contains length\n      // 2) RpcRequest\n      //\n      // Items '1' and '2' are prepared here. \n      final DataOutputBuffer d = new DataOutputBuffer();\n      RpcRequestHeaderProto header = ProtoUtil.makeRpcRequestHeader(\n          call.rpcKind, OperationProto.RPC_FINAL_PACKET, call.id, call.retry,\n          clientId);\n      header.writeDelimitedTo(d);\n      call.rpcRequest.write(d);\n\n      synchronized (sendRpcRequestLock) {\n        Future<?> senderFuture = sendParamsExecutor.submit(new Runnable() {\n          @Override\n          public void run() {\n            try {\n              synchronized (Connection.this.out) {\n                if (shouldCloseConnection.get()) {\n                  return;\n                }\n                \n                if (LOG.isDebugEnabled())\n                  LOG.debug(getName() + \" sending #\" + call.id);\n         \n                byte[] data = d.getData();\n                int totalLength = d.getLength();\n                out.writeInt(totalLength); // Total Length\n                out.write(data, 0, totalLength);// RpcRequestHeader + RpcRequest\n                out.flush();\n              }\n            } catch (IOException e) {\n              // exception at this point would leave the connection in an\n              // unrecoverable state (eg half a call left on the wire).\n              // So, close the connection, killing any outstanding calls\n              markClosed(e);\n            } finally {\n              //the buffer is just an in-memory buffer, but it is still polite to\n              // close early\n              IOUtils.closeStream(d);\n            }\n          }\n        });\n      \n        try {\n          senderFuture.get();\n        } catch (ExecutionException e) {\n          Throwable cause = e.getCause();\n          \n          // cause should only be a RuntimeException as the Runnable above\n          // catches IOException\n          if (cause instanceof RuntimeException) {\n            throw (RuntimeException) cause;\n          } else {\n            throw new RuntimeException(\"unexpected checked exception\", cause);\n          }\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnectionId": "    static ConnectionId getConnectionId(InetSocketAddress addr,\n        Class<?> protocol, UserGroupInformation ticket, int rpcTimeout,\n        RetryPolicy connectionRetryPolicy, Configuration conf) throws IOException {\n\n      if (connectionRetryPolicy == null) {\n        final int max = conf.getInt(\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY,\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_DEFAULT);\n        final int retryInterval = conf.getInt(\n            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_RETRY_INTERVAL_KEY,\n            CommonConfigurationKeysPublic\n                .IPC_CLIENT_CONNECT_RETRY_INTERVAL_DEFAULT);\n\n        connectionRetryPolicy = RetryPolicies.retryUpToMaximumCountWithFixedSleep(\n            max, retryInterval, TimeUnit.MILLISECONDS);\n      }\n\n      return new ConnectionId(addr, protocol, ticket, rpcTimeout,\n          connectionRetryPolicy, conf);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.createCall": "  Call createCall(RPC.RpcKind rpcKind, Writable rpcRequest) {\n    return new Call(rpcKind, rpcRequest);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRpcResponse": "    public synchronized Writable getRpcResponse() {\n      return rpcResponse;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getRemoteAddress": "    public InetSocketAddress getRemoteAddress() {\n      return server;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getConnection": "  private Connection getConnection(ConnectionId remoteId,\n      Call call, int serviceClass) throws IOException {\n    if (!running.get()) {\n      // the client is stopped\n      throw new IOException(\"The client is stopped\");\n    }\n    Connection connection;\n    /* we could avoid this allocation for each RPC by having a  \n     * connectionsId object and with set() method. We need to manage the\n     * refs for keys in HashMap properly. For now its ok.\n     */\n    do {\n      synchronized (connections) {\n        connection = connections.get(remoteId);\n        if (connection == null) {\n          connection = new Connection(remoteId, serviceClass);\n          connections.put(remoteId, connection);\n        }\n      }\n    } while (!connection.addCall(call));\n    \n    //we don't invoke the method below inside \"synchronized (connections)\"\n    //block above. The reason for that is if the server happens to be slow,\n    //it will take longer to establish a connection and that will slow the\n    //entire system down.\n    connection.setupIOstreams();\n    return connection;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.invoke": "    public Object invoke(Object proxy, Method method, Object[] args)\n        throws ServiceException {\n      long startTime = 0;\n      if (LOG.isDebugEnabled()) {\n        startTime = Time.now();\n      }\n      \n      if (args.length != 2) { // RpcController + Message\n        throw new ServiceException(\"Too many parameters for request. Method: [\"\n            + method.getName() + \"]\" + \", Expected: 2, Actual: \"\n            + args.length);\n      }\n      if (args[1] == null) {\n        throw new ServiceException(\"null param while calling Method: [\"\n            + method.getName() + \"]\");\n      }\n\n      RequestHeaderProto rpcRequestHeader = constructRpcRequestHeader(method);\n      \n      if (LOG.isTraceEnabled()) {\n        LOG.trace(Thread.currentThread().getId() + \": Call -> \" +\n            remoteId + \": \" + method.getName() +\n            \" {\" + TextFormat.shortDebugString((Message) args[1]) + \"}\");\n      }\n\n\n      Message theRequest = (Message) args[1];\n      final RpcResponseWrapper val;\n      try {\n        val = (RpcResponseWrapper) client.call(RPC.RpcKind.RPC_PROTOCOL_BUFFER,\n            new RpcRequestWrapper(rpcRequestHeader, theRequest), remoteId);\n\n      } catch (Throwable e) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Exception <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + e + \"}\");\n        }\n\n        throw new ServiceException(e);\n      }\n\n      if (LOG.isDebugEnabled()) {\n        long callTime = Time.now() - startTime;\n        LOG.debug(\"Call: \" + method.getName() + \" took \" + callTime + \"ms\");\n      }\n      \n      Message prototype = null;\n      try {\n        prototype = getReturnProtoType(method);\n      } catch (Exception e) {\n        throw new ServiceException(e);\n      }\n      Message returnMessage;\n      try {\n        returnMessage = prototype.newBuilderForType()\n            .mergeFrom(val.theResponseRead).build();\n\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(Thread.currentThread().getId() + \": Response <- \" +\n              remoteId + \": \" + method.getName() +\n                \" {\" + TextFormat.shortDebugString(returnMessage) + \"}\");\n        }\n\n      } catch (Throwable e) {\n        throw new ServiceException(e);\n      }\n      return returnMessage;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcRequestWrapper request = (RpcRequestWrapper) writableRequest;\n        RequestHeaderProto rpcRequest = request.requestHeader;\n        String methodName = rpcRequest.getMethodName();\n        \n        \n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = prototype.newBuilderForType()\n            .mergeFrom(request.theRequestRead).build();\n        \n        Message result;\n        try {\n          long startTime = Time.now();\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n          int processingTime = (int) (Time.now() - startTime);\n          int qTime = (int) (startTime - receiveTime);\n          if (LOG.isDebugEnabled()) {\n            LOG.info(\"Served: \" + methodName + \" queueTime= \" + qTime +\n                      \" procesingTime= \" + processingTime);\n          }\n          server.rpcMetrics.addRpcQueueTime(qTime);\n          server.rpcMetrics.addRpcProcessingTime(processingTime);\n          server.rpcDetailedMetrics.addProcessingTime(methodName,\n              processingTime);\n        } catch (ServiceException e) {\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          throw e;\n        }\n        return new RpcResponseWrapper(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.constructRpcRequestHeader": "    private RequestHeaderProto constructRpcRequestHeader(Method method) {\n      RequestHeaderProto.Builder builder = RequestHeaderProto\n          .newBuilder();\n      builder.setMethodName(method.getName());\n     \n\n      // For protobuf, {@code protocol} used when creating client side proxy is\n      // the interface extending BlockingInterface, which has the annotations \n      // such as ProtocolName etc.\n      //\n      // Using Method.getDeclaringClass(), as in WritableEngine to get at\n      // the protocol interface will return BlockingInterface, from where \n      // the annotation ProtocolName and Version cannot be\n      // obtained.\n      //\n      // Hence we simply use the protocol class used to create the proxy.\n      // For PB this may limit the use of mixins on client side.\n      builder.setDeclaringClassProtocolName(protocolName);\n      builder.setClientProtocolVersion(clientProtocolVersion);\n      return builder.build();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getReturnProtoType": "    private Message getReturnProtoType(Method method) throws Exception {\n      if (returnTypes.containsKey(method.getName())) {\n        return returnTypes.get(method.getName());\n      }\n      \n      Class<?> returnType = method.getReturnType();\n      Method newInstMethod = returnType.getMethod(\"getDefaultInstance\");\n      newInstMethod.setAccessible(true);\n      Message prototype = (Message) newInstMethod.invoke(null, (Object[]) null);\n      returnTypes.put(method.getName(), prototype);\n      return prototype;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod": "  protected Object invokeMethod(Method method, Object[] args) throws Throwable {\n    try {\n      if (!method.isAccessible()) {\n        method.setAccessible(true);\n      }\n      return method.invoke(currentProxy.proxy, args);\n    } catch (InvocationTargetException e) {\n      throw e.getCause();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryInvocationHandler.invoke": "  public Object invoke(Object proxy, Method method, Object[] args)\n    throws Throwable {\n    RetryPolicy policy = methodNameToPolicyMap.get(method.getName());\n    if (policy == null) {\n      policy = defaultPolicy;\n    }\n    \n    // The number of times this method invocation has been failed over.\n    int invocationFailoverCount = 0;\n    final boolean isRpc = isRpcInvocation(currentProxy.proxy);\n    final int callId = isRpc? Client.nextCallId(): RpcConstants.INVALID_CALL_ID;\n    int retries = 0;\n    while (true) {\n      // The number of times this invocation handler has ever been failed over,\n      // before this method invocation attempt. Used to prevent concurrent\n      // failed method invocations from triggering multiple failover attempts.\n      long invocationAttemptFailoverCount;\n      synchronized (proxyProvider) {\n        invocationAttemptFailoverCount = proxyProviderFailoverCount;\n      }\n\n      if (isRpc) {\n        Client.setCallIdAndRetryCount(callId, retries);\n      }\n      try {\n        Object ret = invokeMethod(method, args);\n        hasMadeASuccessfulCall = true;\n        return ret;\n      } catch (Exception e) {\n        boolean isIdempotentOrAtMostOnce = proxyProvider.getInterface()\n            .getMethod(method.getName(), method.getParameterTypes())\n            .isAnnotationPresent(Idempotent.class);\n        if (!isIdempotentOrAtMostOnce) {\n          isIdempotentOrAtMostOnce = proxyProvider.getInterface()\n              .getMethod(method.getName(), method.getParameterTypes())\n              .isAnnotationPresent(AtMostOnce.class);\n        }\n        RetryAction action = policy.shouldRetry(e, retries++,\n            invocationFailoverCount, isIdempotentOrAtMostOnce);\n        if (action.action == RetryAction.RetryDecision.FAIL) {\n          if (action.reason != null) {\n            LOG.warn(\"Exception while invoking \" + currentProxy.proxy.getClass()\n                + \".\" + method.getName() + \" over \" + currentProxy.proxyInfo\n                + \". Not retrying because \" + action.reason, e);\n          }\n          throw e;\n        } else { // retry or failover\n          // avoid logging the failover if this is the first call on this\n          // proxy object, and we successfully achieve the failover without\n          // any flip-flopping\n          boolean worthLogging = \n            !(invocationFailoverCount == 0 && !hasMadeASuccessfulCall);\n          worthLogging |= LOG.isDebugEnabled();\n          if (action.action == RetryAction.RetryDecision.FAILOVER_AND_RETRY &&\n              worthLogging) {\n            String msg = \"Exception while invoking \" + method.getName()\n                + \" of class \" + currentProxy.proxy.getClass().getSimpleName()\n                + \" over \" + currentProxy.proxyInfo;\n\n            if (invocationFailoverCount > 0) {\n              msg += \" after \" + invocationFailoverCount + \" fail over attempts\"; \n            }\n            msg += \". Trying to fail over \" + formatSleepMessage(action.delayMillis);\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(msg, e);\n            }\n          } else {\n            if(LOG.isDebugEnabled()) {\n              LOG.debug(\"Exception while invoking \" + method.getName()\n                  + \" of class \" + currentProxy.proxy.getClass().getSimpleName()\n                  + \" over \" + currentProxy.proxyInfo + \". Retrying \"\n                  + formatSleepMessage(action.delayMillis), e);\n            }\n          }\n          \n          if (action.delayMillis > 0) {\n            Thread.sleep(action.delayMillis);\n          }\n          \n          if (action.action == RetryAction.RetryDecision.FAILOVER_AND_RETRY) {\n            // Make sure that concurrent failed method invocations only cause a\n            // single actual fail over.\n            synchronized (proxyProvider) {\n              if (invocationAttemptFailoverCount == proxyProviderFailoverCount) {\n                proxyProvider.performFailover(currentProxy.proxy);\n                proxyProviderFailoverCount++;\n                currentProxy = proxyProvider.getProxy();\n              } else {\n                LOG.warn(\"A failover has occurred since the start of this method\"\n                    + \" invocation attempt.\");\n              }\n            }\n            invocationFailoverCount++;\n          }\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getStats": "  public long[] getStats() throws IOException {\n    try {\n      return PBHelper.convert(rpcProxy.getFsStats(null,\n          VOID_GET_FSSTATUS_REQUEST));\n    } catch (ServiceException e) {\n      throw ProtobufHelper.getRemoteException(e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.getDiskStatus": "  public FsStatus getDiskStatus() throws IOException {\n    long rawNums[] = namenode.getStats();\n    return new FsStatus(rawNums[0], rawNums[1], rawNums[2]);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.fsstat": "  public FSSTAT3Response fsstat(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    FSSTAT3Response response = new FSSTAT3Response(Nfs3Status.NFS3_OK);\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    FSSTAT3Request request = null;\n    try {\n      request = new FSSTAT3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid FSSTAT request\");\n      return new FSSTAT3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    FileHandle handle = request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS FSSTAT fileId: \" + handle.getFileId());\n    }\n\n    try {\n      // Use superUserClient to get file system status\n      FsStatus fsStatus = superUserClient.getDiskStatus();\n      long totalBytes = fsStatus.getCapacity();\n      long freeBytes = fsStatus.getRemaining();\n      \n      Nfs3FileAttributes attrs = writeManager.getFileAttr(dfsClient, handle,\n          iug);\n      if (attrs == null) {\n        LOG.info(\"Can't get path for fileId:\" + handle.getFileId());\n        return new FSSTAT3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n      \n      long maxFsObjects = config.getLong(\"dfs.max.objects\", 0);\n      if (maxFsObjects == 0) {\n        // A value of zero in HDFS indicates no limit to the number\n        // of objects that dfs supports. Using Integer.MAX_VALUE instead of\n        // Long.MAX_VALUE so 32bit client won't complain.\n        maxFsObjects = Integer.MAX_VALUE;\n      }\n      \n      return new FSSTAT3Response(Nfs3Status.NFS3_OK, attrs, totalBytes,\n          freeBytes, freeBytes, maxFsObjects, maxFsObjects, maxFsObjects, 0);\n    } catch (RemoteException r) {\n      LOG.warn(\"Exception \", r);\n      IOException io = r.unwrapRemoteException();\n      /**\n       * AuthorizationException can be thrown if the user can't be proxy'ed.\n       */\n      if (io instanceof AuthorizationException) {\n        return new FSSTAT3Response(Nfs3Status.NFS3ERR_ACCES);\n      } else {\n        return new FSSTAT3Response(Nfs3Status.NFS3ERR_IO);\n      }\n    } catch (IOException e) {\n      LOG.warn(\"Exception \", e);\n      return new FSSTAT3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.checkAccessPrivilege": "  private boolean checkAccessPrivilege(final InetAddress client,\n      final AccessPrivilege expected) {\n    AccessPrivilege access = exports.getAccessPrivilege(client);\n    if (access == AccessPrivilege.NONE) {\n      return false;\n    }\n    if (access == AccessPrivilege.READ_ONLY\n        && expected == AccessPrivilege.READ_WRITE) {\n      return false;\n    }\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.handleInternal": "  public void handleInternal(ChannelHandlerContext ctx, RpcInfo info) {\n    RpcCall rpcCall = (RpcCall) info.header();\n    final NFSPROC3 nfsproc3 = NFSPROC3.fromValue(rpcCall.getProcedure());\n    int xid = rpcCall.getXid();\n    byte[] data = new byte[info.data().readableBytes()];\n    info.data().readBytes(data);\n    XDR xdr = new XDR(data);\n    XDR out = new XDR();\n    InetAddress client = ((InetSocketAddress) info.remoteAddress())\n        .getAddress();\n    Channel channel = info.channel();\n\n    Credentials credentials = rpcCall.getCredential();\n    // Ignore auth only for NFSPROC3_NULL, especially for Linux clients.\n    if (nfsproc3 != NFSPROC3.NULL) {\n      if (credentials.getFlavor() != AuthFlavor.AUTH_SYS\n          && credentials.getFlavor() != AuthFlavor.RPCSEC_GSS) {\n        LOG.info(\"Wrong RPC AUTH flavor, \" + credentials.getFlavor()\n            + \" is not AUTH_SYS or RPCSEC_GSS.\");\n        XDR reply = new XDR();\n        RpcDeniedReply rdr = new RpcDeniedReply(xid,\n            RpcReply.ReplyState.MSG_ACCEPTED,\n            RpcDeniedReply.RejectState.AUTH_ERROR, new VerifierNone());\n        rdr.write(reply);\n\n        ChannelBuffer buf = ChannelBuffers.wrappedBuffer(reply.asReadOnlyWrap()\n            .buffer());\n        RpcResponse rsp = new RpcResponse(buf, info.remoteAddress());\n        RpcUtil.sendRpcResponse(ctx, rsp);\n        return;\n      }\n    }\n\n    if (!isIdempotent(rpcCall)) {\n      RpcCallCache.CacheEntry entry = rpcCallCache.checkOrAddToCache(client,\n          xid);\n      if (entry != null) { // in cache\n        if (entry.isCompleted()) {\n          LOG.info(\"Sending the cached reply to retransmitted request \" + xid);\n          RpcUtil.sendRpcResponse(ctx, entry.getResponse());\n          return;\n        } else { // else request is in progress\n          LOG.info(\"Retransmitted request, transaction still in progress \"\n              + xid);\n          // Ignore the request and do nothing\n          return;\n        }\n      }\n    }\n    \n    SecurityHandler securityHandler = getSecurityHandler(credentials,\n        rpcCall.getVerifier());\n    \n    NFS3Response response = null;\n    if (nfsproc3 == NFSPROC3.NULL) {\n      response = nullProcedure();\n    } else if (nfsproc3 == NFSPROC3.GETATTR) {\n      response = getattr(xdr, securityHandler, client);\n    } else if (nfsproc3 == NFSPROC3.SETATTR) {\n      response = setattr(xdr, securityHandler, client);\n    } else if (nfsproc3 == NFSPROC3.LOOKUP) {\n      response = lookup(xdr, securityHandler, client);\n    } else if (nfsproc3 == NFSPROC3.ACCESS) {\n      response = access(xdr, securityHandler, client);\n    } else if (nfsproc3 == NFSPROC3.READLINK) {\n      response = readlink(xdr, securityHandler, client);\n    } else if (nfsproc3 == NFSPROC3.READ) {\n      if (LOG.isDebugEnabled()) {\n          LOG.debug(Nfs3Utils.READ_RPC_START + xid);\n      }    \n      response = read(xdr, securityHandler, client);\n      if (LOG.isDebugEnabled() && (nfsproc3 == NFSPROC3.READ)) {\n        LOG.debug(Nfs3Utils.READ_RPC_END + xid);\n      }\n    } else if (nfsproc3 == NFSPROC3.WRITE) {\n      if (LOG.isDebugEnabled()) {\n          LOG.debug(Nfs3Utils.WRITE_RPC_START + xid);\n      }\n      response = write(xdr, channel, xid, securityHandler, client);\n      // Write end debug trace is in Nfs3Utils.writeChannel\n    } else if (nfsproc3 == NFSPROC3.CREATE) {\n      response = create(xdr, securityHandler, client);\n    } else if (nfsproc3 == NFSPROC3.MKDIR) {      \n      response = mkdir(xdr, securityHandler, client);\n    } else if (nfsproc3 == NFSPROC3.SYMLINK) {\n      response = symlink(xdr, securityHandler, client);\n    } else if (nfsproc3 == NFSPROC3.MKNOD) {\n      response = mknod(xdr, securityHandler, client);\n    } else if (nfsproc3 == NFSPROC3.REMOVE) {\n      response = remove(xdr, securityHandler, client);\n    } else if (nfsproc3 == NFSPROC3.RMDIR) {\n      response = rmdir(xdr, securityHandler, client);\n    } else if (nfsproc3 == NFSPROC3.RENAME) {\n      response = rename(xdr, securityHandler, client);\n    } else if (nfsproc3 == NFSPROC3.LINK) {\n      response = link(xdr, securityHandler, client);\n    } else if (nfsproc3 == NFSPROC3.READDIR) {\n      response = readdir(xdr, securityHandler, client);\n    } else if (nfsproc3 == NFSPROC3.READDIRPLUS) {\n      response = readdirplus(xdr, securityHandler, client);\n    } else if (nfsproc3 == NFSPROC3.FSSTAT) {\n      response = fsstat(xdr, securityHandler, client);\n    } else if (nfsproc3 == NFSPROC3.FSINFO) {\n      response = fsinfo(xdr, securityHandler, client);\n    } else if (nfsproc3 == NFSPROC3.PATHCONF) {\n      response = pathconf(xdr, securityHandler, client);\n    } else if (nfsproc3 == NFSPROC3.COMMIT) {\n      response = commit(xdr, channel, xid, securityHandler, client);\n    } else {\n      // Invalid procedure\n      RpcAcceptedReply.getInstance(xid,\n          RpcAcceptedReply.AcceptState.PROC_UNAVAIL, new VerifierNone()).write(\n          out);\n    }\n    if (response == null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"No sync response, expect an async response for request XID=\"\n            + rpcCall.getXid());\n      }\n      return;\n    }\n    // TODO: currently we just return VerifierNone\n    out = response.writeHeaderAndResponse(out, xid, new VerifierNone());\n    ChannelBuffer buf = ChannelBuffers.wrappedBuffer(out.asReadOnlyWrap()\n        .buffer());\n    RpcResponse rsp = new RpcResponse(buf, info.remoteAddress());\n\n    if (!isIdempotent(rpcCall)) {\n      rpcCallCache.callCompleted(client, xid, rsp);\n    }\n\n    RpcUtil.sendRpcResponse(ctx, rsp);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.remove": "  public REMOVE3Response remove(XDR xdr,\n      SecurityHandler securityHandler, InetAddress client) {\n    REMOVE3Response response = new REMOVE3Response(Nfs3Status.NFS3_OK);\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    REMOVE3Request request = null;\n    try {\n      request = new REMOVE3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid REMOVE request\");\n      return new REMOVE3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n    FileHandle dirHandle = request.getHandle();\n    String fileName = request.getName();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS REMOVE dir fileId: \" + dirHandle.getFileId()\n          + \" fileName: \" + fileName);\n    }\n\n    String dirFileIdPath = Nfs3Utils.getFileIdPath(dirHandle);\n    Nfs3FileAttributes preOpDirAttr = null;\n    Nfs3FileAttributes postOpDirAttr = null;\n    try {\n      preOpDirAttr =  Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\n      if (preOpDirAttr == null) {\n        LOG.info(\"Can't get path for dir fileId:\" + dirHandle.getFileId());\n        return new REMOVE3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n\n      String fileIdPath = dirFileIdPath + \"/\" + fileName;\n      HdfsFileStatus fstat = Nfs3Utils.getFileStatus(dfsClient, fileIdPath);\n      if (fstat == null) {\n        WccData dirWcc = new WccData(Nfs3Utils.getWccAttr(preOpDirAttr),\n            preOpDirAttr);\n        return new REMOVE3Response(Nfs3Status.NFS3ERR_NOENT, dirWcc);\n      }\n      if (fstat.isDir()) {\n        WccData dirWcc = new WccData(Nfs3Utils.getWccAttr(preOpDirAttr),\n            preOpDirAttr);\n        return new REMOVE3Response(Nfs3Status.NFS3ERR_ISDIR, dirWcc);\n      }\n\n      boolean result = dfsClient.delete(fileIdPath, false);\n      WccData dirWcc = Nfs3Utils.createWccData(\n          Nfs3Utils.getWccAttr(preOpDirAttr), dfsClient, dirFileIdPath, iug);\n\n      if (!result) {\n        return new REMOVE3Response(Nfs3Status.NFS3ERR_ACCES, dirWcc);\n      }\n      return new REMOVE3Response(Nfs3Status.NFS3_OK, dirWcc);\n    } catch (IOException e) {\n      LOG.warn(\"Exception \", e);\n      // Try to return correct WccData\n      if (postOpDirAttr == null) {\n        try {\n          postOpDirAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\n        } catch (IOException e1) {\n          LOG.info(\"Can't get postOpDirAttr for \" + dirFileIdPath, e1);\n        }\n      }\n      WccData dirWcc = new WccData(Nfs3Utils.getWccAttr(preOpDirAttr),\n          postOpDirAttr);\n      if (e instanceof AccessControlException) {\n        return new REMOVE3Response(Nfs3Status.NFS3ERR_PERM, dirWcc);\n      } else {\n        return new REMOVE3Response(Nfs3Status.NFS3ERR_IO, dirWcc);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.rmdir": "  public RMDIR3Response rmdir(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    RMDIR3Response response = new RMDIR3Response(Nfs3Status.NFS3_OK);\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    RMDIR3Request request = null;\n    try {\n      request = new RMDIR3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid RMDIR request\");\n      return new RMDIR3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n    FileHandle dirHandle = request.getHandle();\n    String fileName = request.getName();\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS RMDIR dir fileId: \" + dirHandle.getFileId()\n          + \" fileName: \" + fileName);\n    }\n\n    String dirFileIdPath = Nfs3Utils.getFileIdPath(dirHandle);\n    Nfs3FileAttributes preOpDirAttr = null;\n    Nfs3FileAttributes postOpDirAttr = null;\n    try {\n      preOpDirAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\n      if (preOpDirAttr == null) {\n        LOG.info(\"Can't get path for dir fileId:\" + dirHandle.getFileId());\n        return new RMDIR3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n      \n      WccData errWcc = new WccData(Nfs3Utils.getWccAttr(preOpDirAttr),\n          preOpDirAttr);\n      if (!checkAccessPrivilege(client, AccessPrivilege.READ_WRITE)) {\n        return new RMDIR3Response(Nfs3Status.NFS3ERR_ACCES, errWcc); \n      }\n\n      String fileIdPath = dirFileIdPath + \"/\" + fileName;\n      HdfsFileStatus fstat = Nfs3Utils.getFileStatus(dfsClient, fileIdPath);\n      if (fstat == null) {\n        return new RMDIR3Response(Nfs3Status.NFS3ERR_NOENT, errWcc);\n      }\n      if (!fstat.isDir()) {\n        return new RMDIR3Response(Nfs3Status.NFS3ERR_NOTDIR, errWcc);\n      }\n      \n      if (fstat.getChildrenNum() > 0) {\n        return new RMDIR3Response(Nfs3Status.NFS3ERR_NOTEMPTY, errWcc);\n      }\n\n      boolean result = dfsClient.delete(fileIdPath, false);\n      WccData dirWcc = Nfs3Utils.createWccData(\n          Nfs3Utils.getWccAttr(preOpDirAttr), dfsClient, dirFileIdPath, iug);\n      if (!result) {\n        return new RMDIR3Response(Nfs3Status.NFS3ERR_ACCES, dirWcc);\n      }\n\n      return new RMDIR3Response(Nfs3Status.NFS3_OK, dirWcc);\n    } catch (IOException e) {\n      LOG.warn(\"Exception \", e);\n      // Try to return correct WccData\n      if (postOpDirAttr == null) {\n        try {\n          postOpDirAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\n        } catch (IOException e1) {\n          LOG.info(\"Can't get postOpDirAttr for \" + dirFileIdPath, e1);\n        }\n      }\n      WccData dirWcc = new WccData(Nfs3Utils.getWccAttr(preOpDirAttr),\n          postOpDirAttr);\n      if (e instanceof AccessControlException) {\n        return new RMDIR3Response(Nfs3Status.NFS3ERR_PERM, dirWcc);\n      } else {\n        return new RMDIR3Response(Nfs3Status.NFS3ERR_IO, dirWcc);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.mknod": "  public READDIR3Response mknod(XDR xdr,\n      SecurityHandler securityHandler, InetAddress client) {\n    return new READDIR3Response(Nfs3Status.NFS3ERR_NOTSUPP);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.rename": "  public RENAME3Response rename(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    RENAME3Response response = new RENAME3Response(Nfs3Status.NFS3_OK);\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    RENAME3Request request = null;\n    try {\n      request = new RENAME3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid RENAME request\");\n      return new RENAME3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    FileHandle fromHandle = request.getFromDirHandle();\n    String fromName = request.getFromName();\n    FileHandle toHandle = request.getToDirHandle();\n    String toName = request.getToName();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS RENAME from: \" + fromHandle.getFileId() + \"/\" + fromName\n          + \" to: \" + toHandle.getFileId() + \"/\" + toName);\n    }\n\n    String fromDirFileIdPath = Nfs3Utils.getFileIdPath(fromHandle);\n    String toDirFileIdPath = Nfs3Utils.getFileIdPath(toHandle);\n    Nfs3FileAttributes fromPreOpAttr = null;\n    Nfs3FileAttributes toPreOpAttr = null;\n    WccData fromDirWcc = null;\n    WccData toDirWcc = null;\n    try {\n      fromPreOpAttr = Nfs3Utils.getFileAttr(dfsClient, fromDirFileIdPath, iug);\n      if (fromPreOpAttr == null) {\n        LOG.info(\"Can't get path for fromHandle fileId:\"\n            + fromHandle.getFileId());\n        return new RENAME3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n\n      toPreOpAttr = Nfs3Utils.getFileAttr(dfsClient, toDirFileIdPath, iug);\n      if (toPreOpAttr == null) {\n        LOG.info(\"Can't get path for toHandle fileId:\" + toHandle.getFileId());\n        return new RENAME3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n      \n      if (!checkAccessPrivilege(client, AccessPrivilege.READ_WRITE)) {\n        WccData fromWcc = new WccData(Nfs3Utils.getWccAttr(fromPreOpAttr),\n            fromPreOpAttr);\n        WccData toWcc = new WccData(Nfs3Utils.getWccAttr(toPreOpAttr),\n            toPreOpAttr);\n        return new RENAME3Response(Nfs3Status.NFS3ERR_ACCES, fromWcc, toWcc);\n      }\n\n      String src = fromDirFileIdPath + \"/\" + fromName;\n      String dst = toDirFileIdPath + \"/\" + toName;\n\n      dfsClient.rename(src, dst, Options.Rename.NONE);\n\n      // Assemble the reply\n      fromDirWcc = Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(fromPreOpAttr),\n          dfsClient, fromDirFileIdPath, iug);\n      toDirWcc = Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(toPreOpAttr),\n          dfsClient, toDirFileIdPath, iug);\n      return new RENAME3Response(Nfs3Status.NFS3_OK, fromDirWcc, toDirWcc);\n    } catch (IOException e) {\n      LOG.warn(\"Exception \", e);\n      // Try to return correct WccData      \n      try {\n        fromDirWcc = Nfs3Utils.createWccData(\n            Nfs3Utils.getWccAttr(fromPreOpAttr), dfsClient, fromDirFileIdPath,\n            iug);\n        toDirWcc = Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(toPreOpAttr),\n            dfsClient, toDirFileIdPath, iug);\n      } catch (IOException e1) {\n        LOG.info(\"Can't get postOpDirAttr for \" + fromDirFileIdPath + \" or\"\n            + toDirFileIdPath, e1);\n      }\n      if (e instanceof AccessControlException) {\n        return new RENAME3Response(Nfs3Status.NFS3ERR_PERM, fromDirWcc,\n            toDirWcc);\n      } else {\n        return new RENAME3Response(Nfs3Status.NFS3ERR_IO, fromDirWcc, toDirWcc);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.readdir": "  public READDIR3Response readdir(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    READDIR3Response response = new READDIR3Response(Nfs3Status.NFS3_OK);\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    READDIR3Request request = null;\n    try {\n      request = new READDIR3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READDIR request\");\n      return new READDIR3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n    FileHandle handle = request.getHandle();\n    long cookie = request.getCookie();\n    if (cookie < 0) {\n      LOG.error(\"Invalid READDIR request, with negitve cookie:\" + cookie);\n      return new READDIR3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n    long count = request.getCount();\n    if (count <= 0) {\n      LOG.info(\"Nonpositive count in invalid READDIR request:\" + count);\n      return new READDIR3Response(Nfs3Status.NFS3_OK);\n    }\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READDIR fileId: \" + handle.getFileId() + \" cookie: \"\n          + cookie + \" count: \" + count);\n    }\n\n    HdfsFileStatus dirStatus = null;\n    DirectoryListing dlisting = null;\n    Nfs3FileAttributes postOpAttr = null;\n    long dotdotFileId = 0;\n    try {\n      String dirFileIdPath = Nfs3Utils.getFileIdPath(handle);\n      dirStatus = dfsClient.getFileInfo(dirFileIdPath);\n      if (dirStatus == null) {\n        LOG.info(\"Can't get path for fileId:\" + handle.getFileId());\n        return new READDIR3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n      if (!dirStatus.isDir()) {\n        LOG.error(\"Can't readdir for regular file, fileId:\"\n            + handle.getFileId());\n        return new READDIR3Response(Nfs3Status.NFS3ERR_NOTDIR);\n      }\n      long cookieVerf = request.getCookieVerf();\n      if ((cookieVerf != 0) && (cookieVerf != dirStatus.getModificationTime())) {\n        LOG.error(\"CookierVerf mismatch. request cookierVerf:\" + cookieVerf\n            + \" dir cookieVerf:\" + dirStatus.getModificationTime());\n        return new READDIR3Response(Nfs3Status.NFS3ERR_BAD_COOKIE);\n      }\n\n      if (cookie == 0) {\n        // Get dotdot fileId\n        String dotdotFileIdPath = dirFileIdPath + \"/..\";\n        HdfsFileStatus dotdotStatus = dfsClient.getFileInfo(dotdotFileIdPath);\n\n        if (dotdotStatus == null) {\n          // This should not happen\n          throw new IOException(\"Can't get path for handle path:\"\n              + dotdotFileIdPath);\n        }\n        dotdotFileId = dotdotStatus.getFileId();\n      }\n\n      // Get the list from the resume point\n      byte[] startAfter;\n      if(cookie == 0 ) {\n        startAfter = HdfsFileStatus.EMPTY_NAME;\n      } else {\n        String inodeIdPath = Nfs3Utils.getFileIdPath(cookie);\n        startAfter = inodeIdPath.getBytes();\n      }\n      \n      dlisting = listPaths(dfsClient, dirFileIdPath, startAfter);\n      postOpAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\n      if (postOpAttr == null) {\n        LOG.error(\"Can't get path for fileId:\" + handle.getFileId());\n        return new READDIR3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n    } catch (IOException e) {\n      LOG.warn(\"Exception \", e);\n      return new READDIR3Response(Nfs3Status.NFS3ERR_IO);\n    }\n\n    /**\n     * Set up the dirents in the response. fileId is used as the cookie with one\n     * exception. Linux client can either be stuck with \"ls\" command (on REHL)\n     * or report \"Too many levels of symbolic links\" (Ubuntu).\n     * \n     * The problem is that, only two items returned, \".\" and \"..\" when the\n     * namespace is empty. Both of them are \"/\" with the same cookie(root\n     * fileId). Linux client doesn't think such a directory is a real directory.\n     * Even though NFS protocol specifies cookie is an opaque data, Linux client\n     * somehow doesn't like an empty dir returns same cookie for both \".\" and\n     * \"..\".\n     * \n     * The workaround is to use 0 as the cookie for \".\" and always return \".\" as\n     * the first entry in readdir/readdirplus response.\n     */\n    HdfsFileStatus[] fstatus = dlisting.getPartialListing();    \n    int n = (int) Math.min(fstatus.length, count-2);\n    boolean eof = (n < fstatus.length) ? false : (dlisting\n        .getRemainingEntries() == 0);\n    \n    Entry3[] entries;\n    if (cookie == 0) {\n      entries = new Entry3[n + 2];\n      entries[0] = new READDIR3Response.Entry3(postOpAttr.getFileId(), \".\", 0);\n      entries[1] = new READDIR3Response.Entry3(dotdotFileId, \"..\", dotdotFileId);\n\n      for (int i = 2; i < n + 2; i++) {\n        entries[i] = new READDIR3Response.Entry3(fstatus[i - 2].getFileId(),\n            fstatus[i - 2].getLocalName(), fstatus[i - 2].getFileId());\n      }\n    } else {\n      // Resume from last readdirplus. If the cookie is \"..\", the result\n      // list is up the directory content since HDFS uses name as resume point.    \n      entries = new Entry3[n];    \n      for (int i = 0; i < n; i++) {\n        entries[i] = new READDIR3Response.Entry3(fstatus[i].getFileId(),\n            fstatus[i].getLocalName(), fstatus[i].getFileId());\n      }\n    }\n    \n    DirList3 dirList = new READDIR3Response.DirList3(entries, eof);\n    return new READDIR3Response(Nfs3Status.NFS3_OK, postOpAttr,\n        dirStatus.getModificationTime(), dirList);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.readlink": "  public READLINK3Response readlink(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    READLINK3Response response = new READLINK3Response(Nfs3Status.NFS3_OK);\n\n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n\n    READLINK3Request request = null;\n\n    try {\n      request = new READLINK3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READLINK request\");\n      return new READLINK3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    FileHandle handle = request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READLINK fileId: \" + handle.getFileId());\n    }\n\n    String fileIdPath = Nfs3Utils.getFileIdPath(handle);\n    try {\n      String target = dfsClient.getLinkTarget(fileIdPath);\n\n      Nfs3FileAttributes postOpAttr = Nfs3Utils.getFileAttr(dfsClient,\n          fileIdPath, iug);\n      if (postOpAttr == null) {\n        LOG.info(\"Can't get path for fileId:\" + handle.getFileId());\n        return new READLINK3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n      if (postOpAttr.getType() != NfsFileType.NFSLNK.toValue()) {\n        LOG.error(\"Not a symlink, fileId:\" + handle.getFileId());\n        return new READLINK3Response(Nfs3Status.NFS3ERR_INVAL);\n      }\n      if (target == null) {\n        LOG.error(\"Symlink target should not be null, fileId:\"\n            + handle.getFileId());\n        return new READLINK3Response(Nfs3Status.NFS3ERR_SERVERFAULT);\n      }\n      int rtmax = config.getInt(Nfs3Constant.MAX_READ_TRANSFER_SIZE_KEY,\n              Nfs3Constant.MAX_READ_TRANSFER_SIZE_DEFAULT);\n      if (rtmax < target.getBytes().length) {\n        LOG.error(\"Link size: \" + target.getBytes().length\n            + \" is larger than max transfer size: \" + rtmax);\n        return new READLINK3Response(Nfs3Status.NFS3ERR_IO, postOpAttr,\n            new byte[0]);\n      }\n\n      return new READLINK3Response(Nfs3Status.NFS3_OK, postOpAttr,\n          target.getBytes());\n\n    } catch (IOException e) {\n      LOG.warn(\"Readlink error: \" + e.getClass(), e);\n      if (e instanceof FileNotFoundException) {\n        return new READLINK3Response(Nfs3Status.NFS3ERR_STALE);\n      } else if (e instanceof AccessControlException) {\n        return new READLINK3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n      return new READLINK3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.getattr": "  public GETATTR3Response getattr(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    GETATTR3Response response = new GETATTR3Response(Nfs3Status.NFS3_OK);\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    GETATTR3Request request = null;\n    try {\n      request = new GETATTR3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid GETATTR request\");\n      response.setStatus(Nfs3Status.NFS3ERR_INVAL);\n      return response;\n    }\n\n    FileHandle handle = request.getHandle();\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"GETATTR for fileId: \" + handle.getFileId());\n    }\n\n    Nfs3FileAttributes attrs = null;\n    try {\n      attrs = writeManager.getFileAttr(dfsClient, handle, iug);\n    } catch (RemoteException r) {\n      LOG.warn(\"Exception \", r);\n      IOException io = r.unwrapRemoteException();\n      /**\n       * AuthorizationException can be thrown if the user can't be proxy'ed.\n       */\n      if (io instanceof AuthorizationException) {\n        return new GETATTR3Response(Nfs3Status.NFS3ERR_ACCES);\n      } else {\n        return new GETATTR3Response(Nfs3Status.NFS3ERR_IO);\n      }\n    } catch (IOException e) {\n      LOG.info(\"Can't get file attribute, fileId=\" + handle.getFileId(), e);\n      response.setStatus(Nfs3Status.NFS3ERR_IO);\n      return response;\n    }\n    if (attrs == null) {\n      LOG.error(\"Can't get path for fileId:\" + handle.getFileId());\n      response.setStatus(Nfs3Status.NFS3ERR_STALE);\n      return response;\n    }\n    response.setPostOpAttr(attrs);\n    return response;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.write": "  public WRITE3Response write(XDR xdr, Channel channel, int xid,\n      SecurityHandler securityHandler, InetAddress client) {\n    WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3_OK);\n\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    WRITE3Request request = null;\n\n    try {\n      request = new WRITE3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid WRITE request\");\n      return new WRITE3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset = request.getOffset();\n    int count = request.getCount();\n    WriteStableHow stableHow = request.getStableHow();\n    byte[] data = request.getData().array();\n    if (data.length < count) {\n      LOG.error(\"Invalid argument, data size is less than count in request\");\n      return new WRITE3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    FileHandle handle = request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS WRITE fileId: \" + handle.getFileId() + \" offset: \"\n          + offset + \" length:\" + count + \" stableHow:\" + stableHow.getValue()\n          + \" xid:\" + xid);\n    }\n\n    Nfs3FileAttributes preOpAttr = null;\n    try {\n      preOpAttr = writeManager.getFileAttr(dfsClient, handle, iug);\n      if (preOpAttr == null) {\n        LOG.error(\"Can't get path for fileId:\" + handle.getFileId());\n        return new WRITE3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n      \n      if (!checkAccessPrivilege(client, AccessPrivilege.READ_WRITE)) {\n        return new WRITE3Response(Nfs3Status.NFS3ERR_ACCES, new WccData(\n            Nfs3Utils.getWccAttr(preOpAttr), preOpAttr), 0, stableHow,\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n      \n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"requesed offset=\" + offset + \" and current filesize=\"\n            + preOpAttr.getSize());\n      }\n\n      writeManager.handleWrite(dfsClient, request, channel, xid, preOpAttr);\n\n    } catch (IOException e) {\n      LOG.info(\"Error writing to fileId \" + handle.getFileId() + \" at offset \"\n          + offset + \" and length \" + data.length, e);\n      // Try to return WccData\n      Nfs3FileAttributes postOpAttr = null;\n      try {\n        postOpAttr = writeManager.getFileAttr(dfsClient, handle, iug);\n      } catch (IOException e1) {\n        LOG.info(\"Can't get postOpAttr for fileId: \" + handle.getFileId(), e1);\n      }\n      WccAttr attr = preOpAttr == null ? null : Nfs3Utils.getWccAttr(preOpAttr);\n      WccData fileWcc = new WccData(attr, postOpAttr);\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, fileWcc, 0,\n          request.getStableHow(), Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n\n    return null;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.link": "  public READDIR3Response link(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    return new READDIR3Response(Nfs3Status.NFS3ERR_NOTSUPP);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.pathconf": "  public PATHCONF3Response pathconf(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    PATHCONF3Response response = new PATHCONF3Response(Nfs3Status.NFS3_OK);\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    PATHCONF3Request request = null;\n    try {\n      request = new PATHCONF3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid PATHCONF request\");\n      return new PATHCONF3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    FileHandle handle = request.getHandle();\n    Nfs3FileAttributes attrs;\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS PATHCONF fileId: \" + handle.getFileId());\n    }\n\n    try {\n      attrs = Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (attrs == null) {\n        LOG.info(\"Can't get path for fileId:\" + handle.getFileId());\n        return new PATHCONF3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n\n      return new PATHCONF3Response(Nfs3Status.NFS3_OK, attrs, 0,\n          HdfsConstants.MAX_PATH_LENGTH, true, false, false, true);\n    } catch (IOException e) {\n      LOG.warn(\"Exception \", e);\n      return new PATHCONF3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.getSecurityHandler": "  private SecurityHandler getSecurityHandler(Credentials credentials,\n      Verifier verifier) {\n    if (credentials instanceof CredentialsSys) {\n      return new SysSecurityHandler((CredentialsSys) credentials, iug);\n    } else {\n      // TODO: support GSS and handle other cases\n      return null;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.readdirplus": "  public READDIRPLUS3Response readdirplus(XDR xdr,\n      SecurityHandler securityHandler, InetAddress client) {\n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_ACCES);\n    }\n    \n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_SERVERFAULT);\n    }\n    \n    READDIRPLUS3Request request = null;\n    try {\n      request = new READDIRPLUS3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READDIRPLUS request\");\n      return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    FileHandle handle = request.getHandle();\n    long cookie = request.getCookie();\n    if (cookie < 0) {\n      LOG.error(\"Invalid READDIRPLUS request, with negitve cookie:\" + cookie);\n      return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n    long dirCount = request.getDirCount();\n    if (dirCount <= 0) {\n      LOG.info(\"Nonpositive dircount in invalid READDIRPLUS request:\" + dirCount);\n      return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n    int maxCount = request.getMaxCount();\n    if (maxCount <= 0) {\n      LOG.info(\"Nonpositive maxcount in invalid READDIRPLUS request:\" + maxCount);\n      return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READDIRPLUS fileId: \" + handle.getFileId() + \" cookie: \"\n          + cookie + \" dirCount: \" + dirCount + \" maxCount: \" + maxCount);\n    }\n\n    HdfsFileStatus dirStatus;\n    DirectoryListing dlisting = null;\n    Nfs3FileAttributes postOpDirAttr = null;\n    long dotdotFileId = 0;\n    try {\n      String dirFileIdPath = Nfs3Utils.getFileIdPath(handle);\n      dirStatus = dfsClient.getFileInfo(dirFileIdPath);\n      if (dirStatus == null) {\n        LOG.info(\"Can't get path for fileId:\" + handle.getFileId());\n        return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n      if (!dirStatus.isDir()) {\n        LOG.error(\"Can't readdirplus for regular file, fileId:\"\n            + handle.getFileId());\n        return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_NOTDIR);\n      }\n      long cookieVerf = request.getCookieVerf();\n      if ((cookieVerf != 0) && (cookieVerf != dirStatus.getModificationTime())) {\n        LOG.error(\"CookierVerf mismatch. request cookierVerf:\" + cookieVerf\n            + \" dir cookieVerf:\" + dirStatus.getModificationTime());\n        return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_BAD_COOKIE);\n      }\n\n      if (cookie == 0) {\n        // Get dotdot fileId\n        String dotdotFileIdPath = dirFileIdPath + \"/..\";\n        HdfsFileStatus dotdotStatus = dfsClient.getFileInfo(dotdotFileIdPath);\n\n        if (dotdotStatus == null) {\n          // This should not happen\n          throw new IOException(\"Can't get path for handle path:\"\n              + dotdotFileIdPath);\n        }\n        dotdotFileId = dotdotStatus.getFileId();\n      }\n\n      // Get the list from the resume point\n      byte[] startAfter;\n      if (cookie == 0) {\n        startAfter = HdfsFileStatus.EMPTY_NAME;\n      } else {\n        String inodeIdPath = Nfs3Utils.getFileIdPath(cookie);\n        startAfter = inodeIdPath.getBytes();\n      }\n      \n      dlisting = listPaths(dfsClient, dirFileIdPath, startAfter);\n      postOpDirAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\n      if (postOpDirAttr == null) {\n        LOG.info(\"Can't get path for fileId:\" + handle.getFileId());\n        return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n    } catch (IOException e) {\n      LOG.warn(\"Exception \", e);\n      return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_IO);\n    }\n    \n    // Set up the dirents in the response\n    HdfsFileStatus[] fstatus = dlisting.getPartialListing();\n    int n = (int) Math.min(fstatus.length, dirCount-2);\n    boolean eof = (n < fstatus.length) ? false : (dlisting\n        .getRemainingEntries() == 0);\n    \n    READDIRPLUS3Response.EntryPlus3[] entries;\n    if (cookie == 0) {\n      entries = new READDIRPLUS3Response.EntryPlus3[n+2];\n      \n      entries[0] = new READDIRPLUS3Response.EntryPlus3(\n          postOpDirAttr.getFileId(), \".\", 0, postOpDirAttr, new FileHandle(\n              postOpDirAttr.getFileId()));\n      entries[1] = new READDIRPLUS3Response.EntryPlus3(dotdotFileId, \"..\",\n          dotdotFileId, postOpDirAttr, new FileHandle(dotdotFileId));\n\n      for (int i = 2; i < n + 2; i++) {\n        long fileId = fstatus[i - 2].getFileId();\n        FileHandle childHandle = new FileHandle(fileId);\n        Nfs3FileAttributes attr;\n        try {\n          attr = writeManager.getFileAttr(dfsClient, childHandle, iug);\n        } catch (IOException e) {\n          LOG.error(\"Can't get file attributes for fileId:\" + fileId, e);\n          continue;\n        }\n        entries[i] = new READDIRPLUS3Response.EntryPlus3(fileId,\n            fstatus[i - 2].getLocalName(), fileId, attr, childHandle);\n      }\n    } else {\n      // Resume from last readdirplus. If the cookie is \"..\", the result\n      // list is up the directory content since HDFS uses name as resume point.\n      entries = new READDIRPLUS3Response.EntryPlus3[n]; \n      for (int i = 0; i < n; i++) {\n        long fileId = fstatus[i].getFileId();\n        FileHandle childHandle = new FileHandle(fileId);\n        Nfs3FileAttributes attr;\n        try {\n          attr = writeManager.getFileAttr(dfsClient, childHandle, iug);\n        } catch (IOException e) {\n          LOG.error(\"Can't get file attributes for fileId:\" + fileId, e);\n          continue;\n        }\n        entries[i] = new READDIRPLUS3Response.EntryPlus3(fileId,\n            fstatus[i].getLocalName(), fileId, attr, childHandle);\n      }\n    }\n\n    DirListPlus3 dirListPlus = new READDIRPLUS3Response.DirListPlus3(entries,\n        eof);\n    return new READDIRPLUS3Response(Nfs3Status.NFS3_OK, postOpDirAttr,\n        dirStatus.getModificationTime(), dirListPlus);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.fsinfo": "  public FSINFO3Response fsinfo(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    FSINFO3Response response = new FSINFO3Response(Nfs3Status.NFS3_OK);\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    FSINFO3Request request = null;\n    try {\n      request = new FSINFO3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid FSINFO request\");\n      return new FSINFO3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    FileHandle handle = request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS FSINFO fileId: \" + handle.getFileId());\n    }\n\n    try {\n      int rtmax = config.getInt(Nfs3Constant.MAX_READ_TRANSFER_SIZE_KEY,\n              Nfs3Constant.MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int wtmax = config.getInt(Nfs3Constant.MAX_WRITE_TRANSFER_SIZE_KEY,\n              Nfs3Constant.MAX_WRITE_TRANSFER_SIZE_DEFAULT);\n      int dtperf = config.getInt(Nfs3Constant.MAX_READDIR_TRANSFER_SIZE_KEY,\n              Nfs3Constant.MAX_READDIR_TRANSFER_SIZE_DEFAULT);\n\n      Nfs3FileAttributes attrs = Nfs3Utils.getFileAttr(dfsClient,\n          Nfs3Utils.getFileIdPath(handle), iug);\n      if (attrs == null) {\n        LOG.info(\"Can't get path for fileId:\" + handle.getFileId());\n        return new FSINFO3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n      \n      int fsProperty = Nfs3Constant.FSF3_CANSETTIME\n          | Nfs3Constant.FSF3_HOMOGENEOUS;\n\n      return new FSINFO3Response(Nfs3Status.NFS3_OK, attrs, rtmax, rtmax, 1,\n          wtmax, wtmax, 1, dtperf, Long.MAX_VALUE, new NfsTime(1), fsProperty);\n    } catch (IOException e) {\n      LOG.warn(\"Exception \", e);\n      return new FSINFO3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.commit": "  public COMMIT3Response commit(XDR xdr, Channel channel, int xid,\n      SecurityHandler securityHandler, InetAddress client) {\n    COMMIT3Response response = new COMMIT3Response(Nfs3Status.NFS3_OK);\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    COMMIT3Request request = null;\n    try {\n      request = new COMMIT3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid COMMIT request\");\n      response.setStatus(Nfs3Status.NFS3ERR_INVAL);\n      return response;\n    }\n\n    FileHandle handle = request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS COMMIT fileId: \" + handle.getFileId() + \" offset=\"\n          + request.getOffset() + \" count=\" + request.getCount());\n    }\n\n    String fileIdPath = Nfs3Utils.getFileIdPath(handle);\n    Nfs3FileAttributes preOpAttr = null;\n    try {\n      preOpAttr = Nfs3Utils.getFileAttr(dfsClient, fileIdPath, iug);\n      if (preOpAttr == null) {\n        LOG.info(\"Can't get path for fileId:\" + handle.getFileId());\n        return new COMMIT3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n      \n      if (!checkAccessPrivilege(client, AccessPrivilege.READ_WRITE)) {\n        return new COMMIT3Response(Nfs3Status.NFS3ERR_ACCES, new WccData(\n            Nfs3Utils.getWccAttr(preOpAttr), preOpAttr),\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n      \n      long commitOffset = (request.getCount() == 0) ? 0\n          : (request.getOffset() + request.getCount());\n      \n      // Insert commit as an async request\n      writeManager.handleCommit(dfsClient, handle, commitOffset, channel, xid,\n          preOpAttr);\n      return null;\n    } catch (IOException e) {\n      LOG.warn(\"Exception \", e);\n      Nfs3FileAttributes postOpAttr = null;\n      try {\n        postOpAttr = writeManager.getFileAttr(dfsClient, handle, iug);\n      } catch (IOException e1) {\n        LOG.info(\"Can't get postOpAttr for fileId: \" + handle.getFileId(), e1);\n      }\n      WccData fileWcc = new WccData(Nfs3Utils.getWccAttr(preOpAttr), postOpAttr);\n      return new COMMIT3Response(Nfs3Status.NFS3ERR_IO, fileWcc,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.mkdir": "  public MKDIR3Response mkdir(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    MKDIR3Response response = new MKDIR3Response(Nfs3Status.NFS3_OK);\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    MKDIR3Request request = null;\n\n    try {\n      request = new MKDIR3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid MKDIR request\");\n      return new MKDIR3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n    FileHandle dirHandle = request.getHandle();\n    String fileName = request.getName();\n\n    if (request.getObjAttr().getUpdateFields().contains(SetAttrField.SIZE)) {\n      LOG.error(\"Setting file size is not supported when mkdir: \" + fileName\n          + \" in dirHandle\" + dirHandle);\n      return new MKDIR3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    String dirFileIdPath = Nfs3Utils.getFileIdPath(dirHandle);\n    Nfs3FileAttributes preOpDirAttr = null;\n    Nfs3FileAttributes postOpDirAttr = null;\n    Nfs3FileAttributes postOpObjAttr = null;\n    FileHandle objFileHandle = null;\n    try {\n      preOpDirAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\n      if (preOpDirAttr == null) {\n        LOG.info(\"Can't get path for dir fileId:\" + dirHandle.getFileId());\n        return new MKDIR3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n\n      if (!checkAccessPrivilege(client, AccessPrivilege.READ_WRITE)) {\n        return new MKDIR3Response(Nfs3Status.NFS3ERR_ACCES, null, preOpDirAttr,\n            new WccData(Nfs3Utils.getWccAttr(preOpDirAttr), preOpDirAttr));\n      }\n      \n      final String fileIdPath = dirFileIdPath + \"/\" + fileName;\n      SetAttr3 setAttr3 = request.getObjAttr();\n      FsPermission permission = setAttr3.getUpdateFields().contains(\n          SetAttrField.MODE) ? new FsPermission((short) setAttr3.getMode())\n          : FsPermission.getDefault().applyUMask(umask);\n\n      if (!dfsClient.mkdirs(fileIdPath, permission, false)) {\n        WccData dirWcc = Nfs3Utils.createWccData(\n            Nfs3Utils.getWccAttr(preOpDirAttr), dfsClient, dirFileIdPath, iug);\n        return new MKDIR3Response(Nfs3Status.NFS3ERR_IO, null, null, dirWcc);\n      }\n\n      // Set group if it's not specified in the request.\n      if (!setAttr3.getUpdateFields().contains(SetAttrField.GID)) {\n        setAttr3.getUpdateFields().add(SetAttrField.GID);\n        setAttr3.setGid(securityHandler.getGid());\n      }\n      setattrInternal(dfsClient, fileIdPath, setAttr3, false);\n      \n      postOpObjAttr = Nfs3Utils.getFileAttr(dfsClient, fileIdPath, iug);\n      objFileHandle = new FileHandle(postOpObjAttr.getFileId());\n      WccData dirWcc = Nfs3Utils.createWccData(\n          Nfs3Utils.getWccAttr(preOpDirAttr), dfsClient, dirFileIdPath, iug);\n      return new MKDIR3Response(Nfs3Status.NFS3_OK, new FileHandle(\n          postOpObjAttr.getFileId()), postOpObjAttr, dirWcc);\n    } catch (IOException e) {\n      LOG.warn(\"Exception \", e);\n      // Try to return correct WccData\n      if (postOpDirAttr == null) {\n        try {\n          postOpDirAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\n        } catch (IOException e1) {\n          LOG.info(\"Can't get postOpDirAttr for \" + dirFileIdPath, e);\n        }\n      }\n      WccData dirWcc = new WccData(Nfs3Utils.getWccAttr(preOpDirAttr),\n          postOpDirAttr);\n      if (e instanceof AccessControlException) {\n        return new MKDIR3Response(Nfs3Status.NFS3ERR_PERM, objFileHandle,\n            postOpObjAttr, dirWcc);\n      } else {\n        return new MKDIR3Response(Nfs3Status.NFS3ERR_IO, objFileHandle,\n            postOpObjAttr, dirWcc);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.create": "  public CREATE3Response create(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    CREATE3Response response = new CREATE3Response(Nfs3Status.NFS3_OK);\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    CREATE3Request request = null;\n\n    try {\n      request = new CREATE3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid CREATE request\");\n      return new CREATE3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    FileHandle dirHandle = request.getHandle();\n    String fileName = request.getName();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS CREATE dir fileId: \" + dirHandle.getFileId()\n          + \" filename: \" + fileName);\n    }\n\n    int createMode = request.getMode();\n    if ((createMode != Nfs3Constant.CREATE_EXCLUSIVE)\n        && request.getObjAttr().getUpdateFields().contains(SetAttrField.SIZE)\n        && request.getObjAttr().getSize() != 0) {\n      LOG.error(\"Setting file size is not supported when creating file: \"\n          + fileName + \" dir fileId:\" + dirHandle.getFileId());\n      return new CREATE3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    HdfsDataOutputStream fos = null;\n    String dirFileIdPath = Nfs3Utils.getFileIdPath(dirHandle);\n    Nfs3FileAttributes preOpDirAttr = null;\n    Nfs3FileAttributes postOpObjAttr = null;\n    FileHandle fileHandle = null;\n    WccData dirWcc = null;\n    try {\n      preOpDirAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\n      if (preOpDirAttr == null) {\n        LOG.error(\"Can't get path for dirHandle:\" + dirHandle);\n        return new CREATE3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n      \n      if (!checkAccessPrivilege(client, AccessPrivilege.READ_WRITE)) {\n        return new CREATE3Response(Nfs3Status.NFS3ERR_ACCES, null,\n            preOpDirAttr, new WccData(Nfs3Utils.getWccAttr(preOpDirAttr),\n                preOpDirAttr));\n      }\n\n      String fileIdPath = Nfs3Utils.getFileIdPath(dirHandle) + \"/\" + fileName;\n      SetAttr3 setAttr3 = request.getObjAttr();\n      assert (setAttr3 != null);\n      FsPermission permission = setAttr3.getUpdateFields().contains(\n          SetAttrField.MODE) ? new FsPermission((short) setAttr3.getMode())\n          : FsPermission.getDefault().applyUMask(umask);\n          \n      EnumSet<CreateFlag> flag = (createMode != Nfs3Constant.CREATE_EXCLUSIVE) ? \n          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE) : \n          EnumSet.of(CreateFlag.CREATE);\n      \n      fos = new HdfsDataOutputStream(dfsClient.create(fileIdPath, permission,\n          flag, false, replication, blockSize, null, bufferSize, null),\n          statistics);\n      \n      if ((createMode == Nfs3Constant.CREATE_UNCHECKED)\n          || (createMode == Nfs3Constant.CREATE_GUARDED)) {\n        // Set group if it's not specified in the request.\n        if (!setAttr3.getUpdateFields().contains(SetAttrField.GID)) {\n          setAttr3.getUpdateFields().add(SetAttrField.GID);\n          setAttr3.setGid(securityHandler.getGid());\n        }\n        setattrInternal(dfsClient, fileIdPath, setAttr3, false);\n      }\n\n      postOpObjAttr = Nfs3Utils.getFileAttr(dfsClient, fileIdPath, iug);\n      dirWcc = Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(preOpDirAttr),\n          dfsClient, dirFileIdPath, iug);\n      \n      // Add open stream\n      OpenFileCtx openFileCtx = new OpenFileCtx(fos, postOpObjAttr,\n          writeDumpDir + \"/\" + postOpObjAttr.getFileId(), dfsClient, iug);\n      fileHandle = new FileHandle(postOpObjAttr.getFileId());\n      if (!writeManager.addOpenFileStream(fileHandle, openFileCtx)) {\n        LOG.warn(\"Can't add more stream, close it.\"\n            + \" Future write will become append\");\n        fos.close();\n        fos = null;\n      } else {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Opened stream for file:\" + fileName + \", fileId:\"\n              + fileHandle.getFileId());\n        }\n      }\n      \n    } catch (IOException e) {\n      LOG.error(\"Exception\", e);\n      if (fos != null) {\n        try {\n          fos.close();\n        } catch (IOException e1) {\n          LOG.error(\"Can't close stream for dirFileId:\" + dirHandle.getFileId()\n              + \" filename: \" + fileName, e1);\n        }\n      }\n      if (dirWcc == null) {\n        try {\n          dirWcc = Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(preOpDirAttr),\n              dfsClient, dirFileIdPath, iug);\n        } catch (IOException e1) {\n          LOG.error(\"Can't get postOpDirAttr for dirFileId:\"\n              + dirHandle.getFileId(), e1);\n        }\n      }\n      if (e instanceof AccessControlException) {\n        return new CREATE3Response(Nfs3Status.NFS3ERR_ACCES, fileHandle,\n            postOpObjAttr, dirWcc);\n      } else {\n        return new CREATE3Response(Nfs3Status.NFS3ERR_IO, fileHandle,\n            postOpObjAttr, dirWcc);\n      }\n    }\n    \n    return new CREATE3Response(Nfs3Status.NFS3_OK, fileHandle, postOpObjAttr,\n        dirWcc);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.read": "  public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    READ3Response response = new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName = securityHandler.getUser();\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient = clientCache.getDfsClient(userName);\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    READ3Request request = null;\n\n    try {\n      request = new READ3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset = request.getOffset();\n    int count = request.getCount();\n\n    FileHandle handle = request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count == 0) {\n      // Only do access check.\n      try {\n        // Don't read from cache. Client may not have read permission.\n        attrs = Nfs3Utils.getFileAttr(dfsClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs == null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can't get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access = Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(), attrs);\n      if ((access & Nfs3Constant.ACCESS3_READ) != 0) {\n        eof = offset < attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n    \n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret = writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret != Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn't succeed with ret=\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int rtmax = config.getInt(Nfs3Constant.MAX_READ_TRANSFER_SIZE_KEY,\n              Nfs3Constant.MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int buffSize = Math.min(rtmax, count);\n      byte[] readbuffer = new byte[buffSize];\n\n      int readCount = 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i = 0; i < 1; ++i) {\n        FSDataInputStream fis = clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        try {\n          readCount = fis.read(offset, readbuffer, 0, count);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage() == \"Stream closed\") {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs = Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount < count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount < 0) {\n        readCount = 0;\n      }\n      eof = (offset + readCount) < attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      return new READ3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.lookup": "  public LOOKUP3Response lookup(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    LOOKUP3Response response = new LOOKUP3Response(Nfs3Status.NFS3_OK);\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    LOOKUP3Request request = null;\n    try {\n      request = new LOOKUP3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid LOOKUP request\");\n      return new LOOKUP3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    FileHandle dirHandle = request.getHandle();\n    String fileName = request.getName();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS LOOKUP dir fileId: \" + dirHandle.getFileId() + \" name: \"\n          + fileName);\n    }\n\n    try {\n      String dirFileIdPath = Nfs3Utils.getFileIdPath(dirHandle);\n      Nfs3FileAttributes postOpObjAttr = writeManager.getFileAttr(dfsClient,\n          dirHandle, fileName);\n      if (postOpObjAttr == null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"NFS LOOKUP fileId: \" + dirHandle.getFileId() + \" name:\"\n              + fileName + \" does not exist\");\n        }\n        Nfs3FileAttributes postOpDirAttr = Nfs3Utils.getFileAttr(dfsClient,\n            dirFileIdPath, iug);\n        return new LOOKUP3Response(Nfs3Status.NFS3ERR_NOENT, null, null,\n            postOpDirAttr);\n      }\n\n      Nfs3FileAttributes postOpDirAttr = Nfs3Utils.getFileAttr(dfsClient,\n          dirFileIdPath, iug);\n      if (postOpDirAttr == null) {\n        LOG.info(\"Can't get path for dir fileId:\" + dirHandle.getFileId());\n        return new LOOKUP3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n      FileHandle fileHandle = new FileHandle(postOpObjAttr.getFileId());\n      return new LOOKUP3Response(Nfs3Status.NFS3_OK, fileHandle, postOpObjAttr,\n          postOpDirAttr);\n\n    } catch (IOException e) {\n      LOG.warn(\"Exception \", e);\n      return new LOOKUP3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.isIdempotent": "  protected boolean isIdempotent(RpcCall call) {\n    final NFSPROC3 nfsproc3 = NFSPROC3.fromValue(call.getProcedure()); \n    return nfsproc3 == null || nfsproc3.isIdempotent();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.setattr": "  public SETATTR3Response setattr(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    SETATTR3Response response = new SETATTR3Response(Nfs3Status.NFS3_OK);\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    SETATTR3Request request = null;\n    try {\n      request = new SETATTR3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid SETATTR request\");\n      response.setStatus(Nfs3Status.NFS3ERR_INVAL);\n      return response;\n    }\n\n    FileHandle handle = request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS SETATTR fileId: \" + handle.getFileId());\n    }\n\n    if (request.getAttr().getUpdateFields().contains(SetAttrField.SIZE)) {\n      LOG.error(\"Setting file size is not supported when setattr, fileId: \"\n          + handle.getFileId());\n      response.setStatus(Nfs3Status.NFS3ERR_INVAL);\n      return response;\n    }\n\n    String fileIdPath = Nfs3Utils.getFileIdPath(handle);\n    Nfs3FileAttributes preOpAttr = null;\n    try {\n      preOpAttr = Nfs3Utils.getFileAttr(dfsClient, fileIdPath, iug);\n      if (preOpAttr == null) {\n        LOG.info(\"Can't get path for fileId:\" + handle.getFileId());\n        response.setStatus(Nfs3Status.NFS3ERR_STALE);\n        return response;\n      }\n      WccAttr preOpWcc = Nfs3Utils.getWccAttr(preOpAttr);\n      if (request.isCheck()) {\n        if (!preOpAttr.getCtime().equals(request.getCtime())) {\n          WccData wccData = new WccData(preOpWcc, preOpAttr);\n          return new SETATTR3Response(Nfs3Status.NFS3ERR_NOT_SYNC, wccData);\n        }\n      }\n      \n      // check the write access privilege\n      if (!checkAccessPrivilege(client, AccessPrivilege.READ_WRITE)) {\n        return new SETATTR3Response(Nfs3Status.NFS3ERR_ACCES, new WccData(\n            preOpWcc, preOpAttr));\n      }\n\n      setattrInternal(dfsClient, fileIdPath, request.getAttr(), true);\n      Nfs3FileAttributes postOpAttr = Nfs3Utils.getFileAttr(dfsClient,\n          fileIdPath, iug);\n      WccData wccData = new WccData(preOpWcc, postOpAttr);\n      return new SETATTR3Response(Nfs3Status.NFS3_OK, wccData);\n    } catch (IOException e) {\n      LOG.warn(\"Exception \", e);\n      WccData wccData = null;\n      try {\n        wccData = Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(preOpAttr),\n            dfsClient, fileIdPath, iug);\n      } catch (IOException e1) {\n        LOG.info(\"Can't get postOpAttr for fileIdPath: \" + fileIdPath, e1);\n      }\n      if (e instanceof AccessControlException) {\n        return new SETATTR3Response(Nfs3Status.NFS3ERR_ACCES, wccData);\n      } else {\n        return new SETATTR3Response(Nfs3Status.NFS3ERR_IO, wccData);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.symlink": "  public SYMLINK3Response symlink(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    SYMLINK3Response response = new SYMLINK3Response(Nfs3Status.NFS3_OK);\n\n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_WRITE)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n\n    SYMLINK3Request request = null;\n    try {\n      request = new SYMLINK3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid SYMLINK request\");\n      response.setStatus(Nfs3Status.NFS3ERR_INVAL);\n      return response;\n    }\n\n    FileHandle dirHandle = request.getHandle();\n    String name = request.getName();\n    String symData = request.getSymData();\n    String linkDirIdPath = Nfs3Utils.getFileIdPath(dirHandle);\n    // Don't do any name check to source path, just leave it to HDFS\n    String linkIdPath = linkDirIdPath + \"/\" + name;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS SYMLINK, target: \" + symData + \" link: \" + linkIdPath);\n    }\n\n    try {\n      WccData dirWcc = response.getDirWcc();\n      WccAttr preOpAttr = Nfs3Utils.getWccAttr(dfsClient, linkDirIdPath);\n      dirWcc.setPreOpAttr(preOpAttr);\n\n      dfsClient.createSymlink(symData, linkIdPath, false);\n      // Set symlink attr is considered as to change the attr of the target\n      // file. So no need to set symlink attr here after it's created.\n\n      HdfsFileStatus linkstat = dfsClient.getFileLinkInfo(linkIdPath);\n      Nfs3FileAttributes objAttr = Nfs3Utils.getNfs3FileAttrFromFileStatus(\n          linkstat, iug);\n      dirWcc\n          .setPostOpAttr(Nfs3Utils.getFileAttr(dfsClient, linkDirIdPath, iug));\n\n      return new SYMLINK3Response(Nfs3Status.NFS3_OK, new FileHandle(\n          objAttr.getFileId()), objAttr, dirWcc);\n\n    } catch (IOException e) {\n      LOG.warn(\"Exception:\" + e);\n      response.setStatus(Nfs3Status.NFS3ERR_IO);\n      return response;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.nullProcedure": "  public NFS3Response nullProcedure() {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS NULL\");\n    }\n    return new NFS3Response(Nfs3Status.NFS3_OK);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.access": "  public ACCESS3Response access(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    ACCESS3Response response = new ACCESS3Response(Nfs3Status.NFS3_OK);\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    ACCESS3Request request = null;\n    try {\n      request = new ACCESS3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid ACCESS request\");\n      return new ACCESS3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    FileHandle handle = request.getHandle();\n    Nfs3FileAttributes attrs;\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS ACCESS fileId: \" + handle.getFileId());\n    } \n\n    try {\n      // HDFS-5804 removed supserUserClient access\n      attrs = writeManager.getFileAttr(dfsClient, handle, iug);\n\n      if (attrs == null) {\n        LOG.error(\"Can't get path for fileId:\" + handle.getFileId());\n        return new ACCESS3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n      int access = Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(), attrs);\n      \n      return new ACCESS3Response(Nfs3Status.NFS3_OK, attrs, access);\n    } catch (RemoteException r) {\n      LOG.warn(\"Exception \", r);\n      IOException io = r.unwrapRemoteException();\n      /**\n       * AuthorizationException can be thrown if the user can't be proxy'ed.\n       */\n      if (io instanceof AuthorizationException) {\n        return new ACCESS3Response(Nfs3Status.NFS3ERR_ACCES);\n      } else {\n        return new ACCESS3Response(Nfs3Status.NFS3ERR_IO);\n      }\n    } catch (IOException e) {\n      LOG.warn(\"Exception \", e);\n      return new ACCESS3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcProgram.messageReceived": "  public void messageReceived(ChannelHandlerContext ctx, MessageEvent e)\n      throws Exception {\n    RpcInfo info = (RpcInfo) e.getMessage();\n    RpcCall call = (RpcCall) info.header();\n    \n    SocketAddress remoteAddress = info.remoteAddress();\n    if (!allowInsecurePorts) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Will not allow connections from unprivileged ports. \" +\n            \"Checking for valid client port...\");\n      }\n      if (remoteAddress instanceof InetSocketAddress) {\n        InetSocketAddress inetRemoteAddress = (InetSocketAddress) remoteAddress;\n        if (inetRemoteAddress.getPort() > 1023) {\n          LOG.warn(\"Connection attempted from '\" + inetRemoteAddress + \"' \"\n              + \"which is an unprivileged port. Rejecting connection.\");\n          sendRejectedReply(call, remoteAddress, ctx);\n          return;\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Accepting connection from '\" + remoteAddress + \"'\");\n          }\n        }\n      } else {\n        LOG.warn(\"Could not determine remote port of socket address '\" +\n            remoteAddress + \"'. Rejecting connection.\");\n        sendRejectedReply(call, remoteAddress, ctx);\n        return;\n      }\n    }\n    \n    if (LOG.isTraceEnabled()) {\n      LOG.trace(program + \" procedure #\" + call.getProcedure());\n    }\n    \n    if (this.progNumber != call.getProgram()) {\n      LOG.warn(\"Invalid RPC call program \" + call.getProgram());\n      sendAcceptedReply(call, remoteAddress, AcceptState.PROG_UNAVAIL, ctx);\n      return;\n    }\n\n    int ver = call.getVersion();\n    if (ver < lowProgVersion || ver > highProgVersion) {\n      LOG.warn(\"Invalid RPC call version \" + ver);\n      sendAcceptedReply(call, remoteAddress, AcceptState.PROG_MISMATCH, ctx);\n      return;\n    }\n    \n    handleInternal(ctx, info);\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcProgram.handleInternal": "  protected abstract void handleInternal(ChannelHandlerContext ctx, RpcInfo info);\n  \n  @Override\n  public String toString() {\n    return \"Rpc program: \" + program + \" at \" + host + \":\" + port;\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcProgram.getPort": "  public int getPort() {\n    return port;\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcProgram.sendRejectedReply": "  private static void sendRejectedReply(RpcCall call,\n      SocketAddress remoteAddress, ChannelHandlerContext ctx) {\n    XDR out = new XDR();\n    RpcDeniedReply reply = new RpcDeniedReply(call.getXid(),\n        RpcReply.ReplyState.MSG_DENIED,\n        RpcDeniedReply.RejectState.AUTH_ERROR, new VerifierNone());\n    reply.write(out);\n    ChannelBuffer buf = ChannelBuffers.wrappedBuffer(out.asReadOnlyWrap()\n        .buffer());\n    RpcResponse rsp = new RpcResponse(buf, remoteAddress);\n    RpcUtil.sendRpcResponse(ctx, rsp);\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcProgram.sendAcceptedReply": "  private void sendAcceptedReply(RpcCall call, SocketAddress remoteAddress,\n      AcceptState acceptState, ChannelHandlerContext ctx) {\n    RpcAcceptedReply reply = RpcAcceptedReply.getInstance(call.getXid(),\n        acceptState, Verifier.VERIFIER_NONE);\n\n    XDR out = new XDR();\n    reply.write(out);\n    if (acceptState == AcceptState.PROG_MISMATCH) {\n      out.writeInt(lowProgVersion);\n      out.writeInt(highProgVersion);\n    }\n    ChannelBuffer b = ChannelBuffers.wrappedBuffer(out.asReadOnlyWrap()\n        .buffer());\n    RpcResponse rsp = new RpcResponse(b, remoteAddress);\n    RpcUtil.sendRpcResponse(ctx, rsp);\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcUtil.messageReceived": "    public void messageReceived(ChannelHandlerContext ctx, MessageEvent e)\n        throws Exception {\n      RpcResponse r = (RpcResponse) e.getMessage();\n      e.getChannel().write(r.data(), r.remoteAddress());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.run": "          public void run() {\n            try {\n              synchronized (Connection.this.out) {\n                if (shouldCloseConnection.get()) {\n                  return;\n                }\n                \n                if (LOG.isDebugEnabled())\n                  LOG.debug(getName() + \" sending #\" + call.id);\n         \n                byte[] data = d.getData();\n                int totalLength = d.getLength();\n                out.writeInt(totalLength); // Total Length\n                out.write(data, 0, totalLength);// RpcRequestHeader + RpcRequest\n                out.flush();\n              }\n            } catch (IOException e) {\n              // exception at this point would leave the connection in an\n              // unrecoverable state (eg half a call left on the wire).\n              // So, close the connection, killing any outstanding calls\n              markClosed(e);\n            } finally {\n              //the buffer is just an in-memory buffer, but it is still polite to\n              // close early\n              IOUtils.closeStream(d);\n            }\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.shouldAuthenticateOverKrb": "    private synchronized boolean shouldAuthenticateOverKrb() throws IOException {\n      UserGroupInformation loginUser = UserGroupInformation.getLoginUser();\n      UserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\n      UserGroupInformation realUser = currentUser.getRealUser();\n      if (authMethod == AuthMethod.KERBEROS && loginUser != null &&\n      // Make sure user logged in using Kerberos either keytab or TGT\n          loginUser.hasKerberosCredentials() &&\n          // relogin only in case it is the login user (e.g. JT)\n          // or superuser (like oozie).\n          (loginUser.equals(currentUser) || loginUser.equals(realUser))) {\n        return true;\n      }\n      return false;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.waitForWork": "    private synchronized boolean waitForWork() {\n      if (calls.isEmpty() && !shouldCloseConnection.get()  && running.get())  {\n        long timeout = maxIdleTime-\n              (Time.now()-lastActivity.get());\n        if (timeout>0) {\n          try {\n            wait(timeout);\n          } catch (InterruptedException e) {}\n        }\n      }\n      \n      if (!calls.isEmpty() && !shouldCloseConnection.get() && running.get()) {\n        return true;\n      } else if (shouldCloseConnection.get()) {\n        return false;\n      } else if (calls.isEmpty()) { // idle connection closed or stopped\n        markClosed(null);\n        return false;\n      } else { // get stopped but there are still pending requests \n        markClosed((IOException)new IOException().initCause(\n            new InterruptedException()));\n        return false;\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.receiveRpcResponse": "    private void receiveRpcResponse() {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n      touch();\n      \n      try {\n        int totalLen = in.readInt();\n        RpcResponseHeaderProto header = \n            RpcResponseHeaderProto.parseDelimitedFrom(in);\n        checkResponse(header);\n\n        int headerLen = header.getSerializedSize();\n        headerLen += CodedOutputStream.computeRawVarint32Size(headerLen);\n\n        int callId = header.getCallId();\n        if (LOG.isDebugEnabled())\n          LOG.debug(getName() + \" got value #\" + callId);\n\n        Call call = calls.get(callId);\n        RpcStatusProto status = header.getStatus();\n        if (status == RpcStatusProto.SUCCESS) {\n          Writable value = ReflectionUtils.newInstance(valueClass, conf);\n          value.readFields(in);                 // read value\n          calls.remove(callId);\n          call.setRpcResponse(value);\n          \n          // verify that length was correct\n          // only for ProtobufEngine where len can be verified easily\n          if (call.getRpcResponse() instanceof ProtobufRpcEngine.RpcWrapper) {\n            ProtobufRpcEngine.RpcWrapper resWrapper = \n                (ProtobufRpcEngine.RpcWrapper) call.getRpcResponse();\n            if (totalLen != headerLen + resWrapper.getLength()) { \n              throw new RpcClientException(\n                  \"RPC response length mismatch on rpc success\");\n            }\n          }\n        } else { // Rpc Request failed\n          // Verify that length was correct\n          if (totalLen != headerLen) {\n            throw new RpcClientException(\n                \"RPC response length mismatch on rpc error\");\n          }\n          \n          final String exceptionClassName = header.hasExceptionClassName() ?\n                header.getExceptionClassName() : \n                  \"ServerDidNotSetExceptionClassName\";\n          final String errorMsg = header.hasErrorMsg() ? \n                header.getErrorMsg() : \"ServerDidNotSetErrorMsg\" ;\n          final RpcErrorCodeProto erCode = \n                    (header.hasErrorDetail() ? header.getErrorDetail() : null);\n          if (erCode == null) {\n             LOG.warn(\"Detailed error code not set by server on rpc error\");\n          }\n          RemoteException re = \n              ( (erCode == null) ? \n                  new RemoteException(exceptionClassName, errorMsg) :\n              new RemoteException(exceptionClassName, errorMsg, erCode));\n          if (status == RpcStatusProto.ERROR) {\n            calls.remove(callId);\n            call.setException(re);\n          } else if (status == RpcStatusProto.FATAL) {\n            // Close the connection\n            markClosed(re);\n          }\n        }\n      } catch (IOException e) {\n        markClosed(e);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.close": "    private synchronized void close() {\n      if (!shouldCloseConnection.get()) {\n        LOG.error(\"The connection is not in the closed state\");\n        return;\n      }\n\n      // release the resources\n      // first thing to do;take the connection out of the connection list\n      synchronized (connections) {\n        if (connections.get(remoteId) == this) {\n          connections.remove(remoteId);\n        }\n      }\n\n      // close the streams and therefore the socket\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(in);\n      disposeSasl();\n\n      // clean up all calls\n      if (closeException == null) {\n        if (!calls.isEmpty()) {\n          LOG.warn(\n              \"A connection is closed for no cause and calls are not empty\");\n\n          // clean up calls anyway\n          closeException = new IOException(\"Unexpected closed connection\");\n          cleanupCalls();\n        }\n      } else {\n        // log the info\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"closing ipc connection to \" + server + \": \" +\n              closeException.getMessage(),closeException);\n        }\n\n        // cleanup calls\n        cleanupCalls();\n      }\n      closeConnection();\n      if (LOG.isDebugEnabled())\n        LOG.debug(getName() + \": closed\");\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.disposeSasl": "    private synchronized void disposeSasl() {\n      if (saslRpcClient != null) {\n        try {\n          saslRpcClient.dispose();\n          saslRpcClient = null;\n        } catch (IOException ignored) {\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupSaslConnection": "    private synchronized AuthMethod setupSaslConnection(final InputStream in2, \n        final OutputStream out2) throws IOException {\n      // Do not use Client.conf here! We must use ConnectionId.conf, since the\n      // Client object is cached and shared between all RPC clients, even those\n      // for separate services.\n      saslRpcClient = new SaslRpcClient(remoteId.getTicket(),\n          remoteId.getProtocol(), remoteId.getAddress(), remoteId.conf);\n      return saslRpcClient.saslConnect(in2, out2);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.closeConnection": "    private void closeConnection() {\n      if (socket == null) {\n        return;\n      }\n      // close the current connection\n      try {\n        socket.close();\n      } catch (IOException e) {\n        LOG.warn(\"Not able to close a socket\", e);\n      }\n      // set socket to null so that the next call to setupIOstreams\n      // can start the process of connect all over again.\n      socket = null;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.markClosed": "    private synchronized void markClosed(IOException e) {\n      if (shouldCloseConnection.compareAndSet(false, true)) {\n        closeException = e;\n        notifyAll();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.handleSaslConnectionFailure": "    private synchronized void handleSaslConnectionFailure(\n        final int currRetries, final int maxRetries, final Exception ex,\n        final Random rand, final UserGroupInformation ugi) throws IOException,\n        InterruptedException {\n      ugi.doAs(new PrivilegedExceptionAction<Object>() {\n        @Override\n        public Object run() throws IOException, InterruptedException {\n          final short MAX_BACKOFF = 5000;\n          closeConnection();\n          disposeSasl();\n          if (shouldAuthenticateOverKrb()) {\n            if (currRetries < maxRetries) {\n              if(LOG.isDebugEnabled()) {\n                LOG.debug(\"Exception encountered while connecting to \"\n                    + \"the server : \" + ex);\n              }\n              // try re-login\n              if (UserGroupInformation.isLoginKeytabBased()) {\n                UserGroupInformation.getLoginUser().reloginFromKeytab();\n              } else if (UserGroupInformation.isLoginTicketBased()) {\n                UserGroupInformation.getLoginUser().reloginFromTicketCache();\n              }\n              // have granularity of milliseconds\n              //we are sleeping with the Connection lock held but since this\n              //connection instance is being used for connecting to the server\n              //in question, it is okay\n              Thread.sleep((rand.nextInt(MAX_BACKOFF) + 1));\n              return null;\n            } else {\n              String msg = \"Couldn't setup connection for \"\n                  + UserGroupInformation.getLoginUser().getUserName() + \" to \"\n                  + remoteId;\n              LOG.warn(msg);\n              throw (IOException) new IOException(msg).initCause(ex);\n            }\n          } else {\n            LOG.warn(\"Exception encountered while connecting to \"\n                + \"the server : \" + ex);\n          }\n          if (ex instanceof RemoteException)\n            throw (RemoteException) ex;\n          throw new IOException(ex);\n        }\n      });\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupIOstreams": "    private synchronized void setupIOstreams() {\n      if (socket != null || shouldCloseConnection.get()) {\n        return;\n      } \n      try {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Connecting to \"+server);\n        }\n        short numRetries = 0;\n        final short MAX_RETRIES = 5;\n        Random rand = null;\n        while (true) {\n          setupConnection();\n          InputStream inStream = NetUtils.getInputStream(socket);\n          OutputStream outStream = NetUtils.getOutputStream(socket);\n          writeConnectionHeader(outStream);\n          if (authProtocol == AuthProtocol.SASL) {\n            final InputStream in2 = inStream;\n            final OutputStream out2 = outStream;\n            UserGroupInformation ticket = remoteId.getTicket();\n            if (ticket.getRealUser() != null) {\n              ticket = ticket.getRealUser();\n            }\n            try {\n              authMethod = ticket\n                  .doAs(new PrivilegedExceptionAction<AuthMethod>() {\n                    @Override\n                    public AuthMethod run()\n                        throws IOException, InterruptedException {\n                      return setupSaslConnection(in2, out2);\n                    }\n                  });\n            } catch (Exception ex) {\n              authMethod = saslRpcClient.getAuthMethod();\n              if (rand == null) {\n                rand = new Random();\n              }\n              handleSaslConnectionFailure(numRetries++, MAX_RETRIES, ex, rand,\n                  ticket);\n              continue;\n            }\n            if (authMethod != AuthMethod.SIMPLE) {\n              // Sasl connect is successful. Let's set up Sasl i/o streams.\n              inStream = saslRpcClient.getInputStream(inStream);\n              outStream = saslRpcClient.getOutputStream(outStream);\n              // for testing\n              remoteId.saslQop =\n                  (String)saslRpcClient.getNegotiatedProperty(Sasl.QOP);\n              LOG.debug(\"Negotiated QOP is :\" + remoteId.saslQop);\n            } else if (UserGroupInformation.isSecurityEnabled() &&\n                       !fallbackAllowed) {\n              throw new IOException(\"Server asks us to fall back to SIMPLE \" +\n                  \"auth, but this client is configured to only allow secure \" +\n                  \"connections.\");\n            }\n          }\n        \n          if (doPing) {\n            inStream = new PingInputStream(inStream);\n          }\n          this.in = new DataInputStream(new BufferedInputStream(inStream));\n\n          // SASL may have already buffered the stream\n          if (!(outStream instanceof BufferedOutputStream)) {\n            outStream = new BufferedOutputStream(outStream);\n          }\n          this.out = new DataOutputStream(outStream);\n          \n          writeConnectionContext(remoteId, authMethod);\n\n          // update last activity time\n          touch();\n\n          // start the receiver thread after the socket connection has been set\n          // up\n          start();\n          return;\n        }\n      } catch (Throwable t) {\n        if (t instanceof IOException) {\n          markClosed((IOException)t);\n        } else {\n          markClosed(new IOException(\"Couldn't set up IO streams\", t));\n        }\n        close();\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.writeConnectionHeader": "    private void writeConnectionHeader(OutputStream outStream)\n        throws IOException {\n      DataOutputStream out = new DataOutputStream(new BufferedOutputStream(outStream));\n      // Write out the header, version and authentication method\n      out.write(RpcConstants.HEADER.array());\n      out.write(RpcConstants.CURRENT_VERSION);\n      out.write(serviceClass);\n      out.write(authProtocol.callId);\n      out.flush();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.touch": "    private void touch() {\n      lastActivity.set(Time.now());\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.writeConnectionContext": "    private void writeConnectionContext(ConnectionId remoteId,\n                                        AuthMethod authMethod)\n                                            throws IOException {\n      // Write out the ConnectionHeader\n      IpcConnectionContextProto message = ProtoUtil.makeIpcConnectionContext(\n          RPC.getProtocolName(remoteId.getProtocol()),\n          remoteId.getTicket(),\n          authMethod);\n      RpcRequestHeaderProto connectionContextHeader = ProtoUtil\n          .makeRpcRequestHeader(RpcKind.RPC_PROTOCOL_BUFFER,\n              OperationProto.RPC_FINAL_PACKET, CONNECTION_CONTEXT_CALL_ID,\n              RpcConstants.INVALID_RETRY_COUNT, clientId);\n      RpcRequestMessageWrapper request =\n          new RpcRequestMessageWrapper(connectionContextHeader, message);\n      \n      // Write out the packet length\n      out.writeInt(request.getLength());\n      request.write(out);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.getTicket": "    UserGroupInformation getTicket() {\n      return ticket;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupConnection": "    private synchronized void setupConnection() throws IOException {\n      short ioFailures = 0;\n      short timeoutFailures = 0;\n      while (true) {\n        try {\n          this.socket = socketFactory.createSocket();\n          this.socket.setTcpNoDelay(tcpNoDelay);\n          this.socket.setKeepAlive(true);\n          \n          /*\n           * Bind the socket to the host specified in the principal name of the\n           * client, to ensure Server matching address of the client connection\n           * to host name in principal passed.\n           */\n          UserGroupInformation ticket = remoteId.getTicket();\n          if (ticket != null && ticket.hasKerberosCredentials()) {\n            KerberosInfo krbInfo = \n              remoteId.getProtocol().getAnnotation(KerberosInfo.class);\n            if (krbInfo != null && krbInfo.clientPrincipal() != null) {\n              String host = \n                SecurityUtil.getHostFromPrincipal(remoteId.getTicket().getUserName());\n              \n              // If host name is a valid local address then bind socket to it\n              InetAddress localAddr = NetUtils.getLocalInetAddress(host);\n              if (localAddr != null) {\n                this.socket.bind(new InetSocketAddress(localAddr, 0));\n              }\n            }\n          }\n          \n          NetUtils.connect(this.socket, server, connectionTimeout);\n          if (rpcTimeout > 0) {\n            pingInterval = rpcTimeout;  // rpcTimeout overwrites pingInterval\n          }\n          this.socket.setSoTimeout(pingInterval);\n          return;\n        } catch (ConnectTimeoutException toe) {\n          /* Check for an address change and update the local reference.\n           * Reset the failure counter if the address was changed\n           */\n          if (updateAddress()) {\n            timeoutFailures = ioFailures = 0;\n          }\n          handleConnectionTimeout(timeoutFailures++,\n              maxRetriesOnSocketTimeouts, toe);\n        } catch (IOException ie) {\n          if (updateAddress()) {\n            timeoutFailures = ioFailures = 0;\n          }\n          handleConnectionFailure(ioFailures++, ie);\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convert": "  public static ShmId convert(ShortCircuitShmIdProto shmId) {\n    return new ShmId(shmId.getHi(), shmId.getLo());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertAclEntry": "  public static List<AclEntry> convertAclEntry(List<AclEntryProto> aclSpec) {\n    ArrayList<AclEntry> r = Lists.newArrayListWithCapacity(aclSpec.size());\n    for (AclEntryProto e : aclSpec) {\n      AclEntry.Builder builder = new AclEntry.Builder();\n      builder.setType(convert(e.getType()));\n      builder.setScope(convert(e.getScope()));\n      builder.setPermission(convert(e.getPermissions()));\n      if (e.hasName()) {\n        builder.setName(e.getName());\n      }\n      r.add(builder.build());\n    }\n    return r;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertLocatedBlock": "  public static List<LocatedBlock> convertLocatedBlock(\n      List<LocatedBlockProto> lb) {\n    if (lb == null) return null;\n    final int len = lb.size();\n    List<LocatedBlock> result = \n        new ArrayList<LocatedBlock>(len);\n    for (int i = 0; i < len; ++i) {\n      result.add(PBHelper.convert(lb.get(i)));\n    }\n    return result;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertStorageTypeProtos": "  private static StorageType[] convertStorageTypeProtos(\n      List<StorageTypeProto> storageTypesList) {\n    final StorageType[] storageTypes = new StorageType[storageTypesList.size()];\n    for (int i = 0; i < storageTypes.length; ++i) {\n      storageTypes[i] = PBHelper.convertType(storageTypesList.get(i));\n    }\n    return storageTypes;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertType": "  private static StorageType convertType(StorageTypeProto type) {\n    switch(type) {\n      case DISK:\n        return StorageType.DISK;\n      case SSD:\n        return StorageType.SSD;\n      default:\n        throw new IllegalStateException(\n            \"BUG: StorageTypeProto not found, type=\" + type);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertXAttrs": "  public static List<XAttr> convertXAttrs(List<XAttrProto> xAttrSpec) {\n    ArrayList<XAttr> xAttrs = Lists.newArrayListWithCapacity(xAttrSpec.size());\n    for (XAttrProto a : xAttrSpec) {\n      XAttr.Builder builder = new XAttr.Builder();\n      builder.setNameSpace(convert(a.getNamespace()));\n      if (a.hasName()) {\n        builder.setName(a.getName());\n      }\n      if (a.hasValue()) {\n        builder.setValue(a.getValue().toByteArray());\n      }\n      xAttrs.add(builder.build());\n    }\n    return xAttrs;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertLocatedBlock2": "  public static List<LocatedBlockProto> convertLocatedBlock2(List<LocatedBlock> lb) {\n    if (lb == null) return null;\n    final int len = lb.size();\n    List<LocatedBlockProto> result = new ArrayList<LocatedBlockProto>(len);\n    for (int i = 0; i < len; ++i) {\n      result.add(PBHelper.convert(lb.get(i)));\n    }\n    return result;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertAclEntryProto": "  public static List<AclEntryProto> convertAclEntryProto(\n      List<AclEntry> aclSpec) {\n    ArrayList<AclEntryProto> r = Lists.newArrayListWithCapacity(aclSpec.size());\n    for (AclEntry e : aclSpec) {\n      AclEntryProto.Builder builder = AclEntryProto.newBuilder();\n      builder.setType(convert(e.getType()));\n      builder.setScope(convert(e.getScope()));\n      builder.setPermissions(convert(e.getPermission()));\n      if (e.getName() != null) {\n        builder.setName(e.getName());\n      }\n      r.add(builder.build());\n    }\n    return r;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertState": "  private static State convertState(StorageState state) {\n    switch(state) {\n    case READ_ONLY_SHARED:\n      return DatanodeStorage.State.READ_ONLY_SHARED;\n    case NORMAL:\n    default:\n      return DatanodeStorage.State.NORMAL;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertRollingUpgradeStatus": "  public static RollingUpgradeStatusProto convertRollingUpgradeStatus(\n      RollingUpgradeStatus status) {\n    return RollingUpgradeStatusProto.newBuilder()\n        .setBlockPoolId(status.getBlockPoolId())\n        .build();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertStorageType": "  private static StorageTypeProto convertStorageType(\n      StorageType type) {\n    switch(type) {\n    case DISK:\n      return StorageTypeProto.DISK;\n    case SSD:\n      return StorageTypeProto.SSD;\n    default:\n      throw new IllegalStateException(\n          \"BUG: StorageType not found, type=\" + type);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.castEnum": "  private static <T extends Enum<T>, U extends Enum<U>> U castEnum(T from, U[] to) {\n    return to[from.ordinal()];\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocolPB.PBHelper.convertBlockKeys": "  public static BlockKey[] convertBlockKeys(List<BlockKeyProto> list) {\n    BlockKey[] ret = new BlockKey[list.size()];\n    int i = 0;\n    for (BlockKeyProto k : list) {\n      ret[i++] = convert(k);\n    }\n    return ret;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.writeManager.getFileAttr": "  Nfs3FileAttributes getFileAttr(DFSClient client, FileHandle dirHandle,\n      String fileName) throws IOException {\n    String fileIdPath = Nfs3Utils.getFileIdPath(dirHandle) + \"/\" + fileName;\n    Nfs3FileAttributes attr = Nfs3Utils.getFileAttr(client, fileIdPath, iug);\n\n    if ((attr != null) && (attr.getType() == NfsFileType.NFSREG.toValue())) {\n      OpenFileCtx openFileCtx = fileContextCache.get(new FileHandle(attr\n          .getFileId()));\n\n      if (openFileCtx != null) {\n        attr.setSize(openFileCtx.getNextOffset());\n        attr.setUsed(openFileCtx.getNextOffset());\n      }\n    }\n    return attr;\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcInfo.remoteAddress": "  public SocketAddress remoteAddress() {\n    return remoteAddress;\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcCall.getProgram": "  public int getProgram() {\n    return program;\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcCall.getProcedure": "  public int getProcedure() {\n    return procedure;\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcCall.getVersion": "  public int getVersion() {\n    return version;\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcInfo.header": "  public RpcMessage header() {\n    return header;\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcResponse.remoteAddress": "  public SocketAddress remoteAddress() {\n    return remoteAddress;\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcResponse.data": "  public ChannelBuffer data() {\n    return data;\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.XDR.recordMark": "  static byte[] recordMark(int size, boolean last) {\n    byte[] b = new byte[SIZEOF_INT];\n    ByteBuffer buf = ByteBuffer.wrap(b);\n    buf.putInt(!last ? size : size | 0x80000000);\n    return b;\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.XDR.buffer": "  public ByteBuffer buffer() {\n    return buf.duplicate();\n  }",
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.RpcCall.read": "  public static RpcCall read(XDR xdr) {\n    return new RpcCall(xdr.readInt(), RpcMessage.Type.fromValue(xdr.readInt()),\n        xdr.readInt(), xdr.readInt(), xdr.readInt(), xdr.readInt(), \n        Credentials.readFlavorAndCredentials(xdr),\n        Verifier.readFlavorAndVerifier(xdr));\n  }"
        },
        "bug_report": {
            "Title": "NFS: fsstat request fails with the secure hdfs",
            "Description": "Fsstat fails in secure environment with below error.\n\nSteps to reproduce:\n1) Create user named UserB and UserA\n2) Create group named GroupB\n3) Add root and UserB users to GroupB\n    Make sure UserA is not in GroupB\n4) Set below properties\n{noformat}\n===================================\nhdfs-site.xml\n===================================\n <property>\n    <name>dfs.nfs.keytab.file</name>\n    <value>/tmp/keytab/UserA.keytab</value>\n  </property>\n  <property>\n    <name>dfs.nfs.kerberos.principal</name>\n    <value>UserA@EXAMPLE.COM</value>\n  </property>\n==================================\ncore-site.xml\n==================================\n<property>\n    <name>hadoop.proxyuser.UserA.groups</name>\n   <value>GroupB</value>\n </property>\n<property>\n   <name>hadoop.proxyuser.UserA.hosts</name>\n   <value>*</value>\n </property>\n{noformat}\n4) start nfs server as UserA\n5) mount nfs as root user\n6) run below command \n{noformat}\n[root@host1 ~]# df /tmp/tmp_mnt/\ndf: `/tmp/tmp_mnt/': Input/output error\ndf: no file systems processed\n{noformat}\n\nNFS Logs complains as below\n{noformat}\n2014-05-29 00:09:13,698 DEBUG nfs3.RpcProgramNfs3 (RpcProgramNfs3.java:fsstat(1654)) - NFS FSSTAT fileId: 16385\n2014-05-29 00:09:13,706 WARN  ipc.Client (Client.java:run(672)) - Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n2014-05-29 00:09:13,710 WARN  nfs3.RpcProgramNfs3 (RpcProgramNfs3.java:fsstat(1681)) - Exception\njava.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: \"host1/0.0.0.0\"; destination host is: \"host1\":8020;\n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1414)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1363)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n        at com.sun.proxy.$Proxy14.getFsStats(Unknown Source)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)\n        at com.sun.proxy.$Proxy14.getFsStats(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getStats(ClientNamenodeProtocolTranslatorPB.java:554)\n        at org.apache.hadoop.hdfs.DFSClient.getDiskStatus(DFSClient.java:2165)\n        at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.fsstat(RpcProgramNfs3.java:1659)\n        at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.handleInternal(RpcProgramNfs3.java:1961)\n        at org.apache.hadoop.oncrpc.RpcProgram.messageReceived(RpcProgram.java:162)\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:281)\n        at org.apache.hadoop.oncrpc.RpcUtil$RpcMessageParserStage.messageReceived(RpcUtil.java:132)\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\n        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\n        at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\n        at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)\n        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)\n        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)\n        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\nCaused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:677)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)\n        at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:640)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:724)\n        at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1381)\n        ... 42 more\n{noformat}"
        }
    },
    {
        "filename": "HDFS-5425.json",
        "creation_time": "2013-10-25T06:53:28.000+0000",
        "stack_trace": "```\njava.lang.IllegalStateException\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:133)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$ChildrenDiff.replace(INodeDirectoryWithSnapshot.java:82)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$ChildrenDiff.access$700(INodeDirectoryWithSnapshot.java:62)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$DirectoryDiffList.replaceChild(INodeDirectoryWithSnapshot.java:397)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$DirectoryDiffList.access$900(INodeDirectoryWithSnapshot.java:376)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot.replaceChild(INodeDirectoryWithSnapshot.java:598)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedReplaceINodeFile(FSDirectory.java:1548)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.replaceINodeFile(FSDirectory.java:1537)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.loadFilesUnderConstruction(FSImageFormat.java:855)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.load(FSImageFormat.java:350)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:910)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:899)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:751)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:720)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:266)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:784)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:563)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:422)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:472)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:670)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:655)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1245)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1311)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot.replace": "    private final boolean replace(final ListType type,\n        final INode oldChild, final INode newChild) {\n      final List<INode> list = getList(type); \n      final int i = search(list, oldChild.getLocalNameBytes());\n      if (i < 0) {\n        return false;\n      }\n\n      final INode removed = list.set(i, newChild);\n      Preconditions.checkState(removed == oldChild);\n      return true;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot.replaceChild": "  public void replaceChild(final INode oldChild, final INode newChild,\n      final INodeMap inodeMap) {\n    super.replaceChild(oldChild, newChild, inodeMap);\n    diffs.replaceChild(ListType.CREATED, oldChild, newChild);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot.get": "        public INode get(int i) {\n          return initChildren().get(i);\n        }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot.size": "        public int size() {\n          return childrenSize;\n        }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedReplaceINodeFile": "  void unprotectedReplaceINodeFile(final String path, final INodeFile oldnode,\n      final INodeFile newnode) {\n    Preconditions.checkState(hasWriteLock());\n\n    oldnode.getParent().replaceChild(oldnode, newnode, inodeMap);\n    oldnode.clear();\n\n    /* Currently oldnode and newnode are assumed to contain the same\n     * blocks. Otherwise, blocks need to be removed from the blocksMap.\n     */\n    int index = 0;\n    for (BlockInfo b : newnode.getBlocks()) {\n      BlockInfo info = getBlockManager().addBlockCollection(b, newnode);\n      newnode.setBlock(index, info); // inode refers to the block in BlocksMap\n      index++;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getBlockManager": "  private BlockManager getBlockManager() {\n    return getFSNamesystem().getBlockManager();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.hasWriteLock": "  boolean hasWriteLock() {\n    return this.dirLock.isWriteLockedByCurrentThread();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.replaceINodeFile": "  void replaceINodeFile(String path, INodeFile oldnode,\n      INodeFile newnode) throws IOException {\n    writeLock();\n    try {\n      unprotectedReplaceINodeFile(path, oldnode, newnode);\n    } finally {\n      writeUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.writeLock": "  void writeLock() {\n    this.dirLock.writeLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.writeUnlock": "  void writeUnlock() {\n    this.dirLock.writeLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.loadFilesUnderConstruction": "    private void loadFilesUnderConstruction(DataInput in,\n        boolean supportSnapshot, Counter counter) throws IOException {\n      FSDirectory fsDir = namesystem.dir;\n      int size = in.readInt();\n\n      LOG.info(\"Number of files under construction = \" + size);\n\n      for (int i = 0; i < size; i++) {\n        INodeFileUnderConstruction cons = FSImageSerialization\n            .readINodeUnderConstruction(in, namesystem, getLayoutVersion());\n        counter.increment();\n\n        // verify that file exists in namespace\n        String path = cons.getLocalName();\n        final INodesInPath iip = fsDir.getLastINodeInPath(path);\n        INodeFile oldnode = INodeFile.valueOf(iip.getINode(0), path);\n        cons.setLocalName(oldnode.getLocalNameBytes());\n        cons.setParent(oldnode.getParent());\n\n        if (oldnode instanceof INodeFileWithSnapshot) {\n          cons = new INodeFileUnderConstructionWithSnapshot(cons,\n              ((INodeFileWithSnapshot)oldnode).getDiffs());\n        }\n\n        fsDir.replaceINodeFile(path, oldnode, cons);\n        namesystem.leaseManager.addLease(cons.getClientName(), path); \n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.getParent": "    byte[][] getParent(byte[][] path) {\n      byte[][] result = new byte[path.length - 1][];\n      for (int i = 0; i < result.length; i++) {\n        result[i] = new byte[path[i].length];\n        System.arraycopy(path[i], 0, result[i], 0, path[i].length);\n      }\n      return result;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.getLayoutVersion": "    private int getLayoutVersion() {\n      return namesystem.getFSImage().getStorage().getLayoutVersion();\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.load": "    void load(File curFile) throws IOException {\n      checkNotLoaded();\n      assert curFile != null : \"curFile is null\";\n\n      StartupProgress prog = NameNode.getStartupProgress();\n      Step step = new Step(StepType.INODES);\n      prog.beginStep(Phase.LOADING_FSIMAGE, step);\n      long startTime = now();\n\n      //\n      // Load in bits\n      //\n      MessageDigest digester = MD5Hash.getDigester();\n      DigestInputStream fin = new DigestInputStream(\n           new FileInputStream(curFile), digester);\n\n      DataInputStream in = new DataInputStream(fin);\n      try {\n        // read image version: first appeared in version -1\n        int imgVersion = in.readInt();\n        if (getLayoutVersion() != imgVersion) {\n          throw new InconsistentFSStateException(curFile, \n              \"imgVersion \" + imgVersion +\n              \" expected to be \" + getLayoutVersion());\n        }\n        boolean supportSnapshot = LayoutVersion.supports(Feature.SNAPSHOT,\n            imgVersion);\n\n        // read namespaceID: first appeared in version -2\n        in.readInt();\n\n        long numFiles = in.readLong();\n\n        // read in the last generation stamp for legacy blocks.\n        long genstamp = in.readLong();\n        namesystem.setGenerationStampV1(genstamp);\n        \n        if (LayoutVersion.supports(Feature.SEQUENTIAL_BLOCK_ID, imgVersion)) {\n          // read the starting generation stamp for sequential block IDs\n          genstamp = in.readLong();\n          namesystem.setGenerationStampV2(genstamp);\n\n          // read the last generation stamp for blocks created after\n          // the switch to sequential block IDs.\n          long stampAtIdSwitch = in.readLong();\n          namesystem.setGenerationStampV1Limit(stampAtIdSwitch);\n\n          // read the max sequential block ID.\n          long maxSequentialBlockId = in.readLong();\n          namesystem.setLastAllocatedBlockId(maxSequentialBlockId);\n        } else {\n          long startingGenStamp = namesystem.upgradeGenerationStampToV2();\n          // This is an upgrade.\n          LOG.info(\"Upgrading to sequential block IDs. Generation stamp \" +\n                   \"for new blocks set to \" + startingGenStamp);\n        }\n\n        // read the transaction ID of the last edit represented by\n        // this image\n        if (LayoutVersion.supports(Feature.STORED_TXIDS, imgVersion)) {\n          imgTxId = in.readLong();\n        } else {\n          imgTxId = 0;\n        }\n\n        // read the last allocated inode id in the fsimage\n        if (LayoutVersion.supports(Feature.ADD_INODE_ID, imgVersion)) {\n          long lastInodeId = in.readLong();\n          namesystem.resetLastInodeId(lastInodeId);\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"load last allocated InodeId from fsimage:\" + lastInodeId);\n          }\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Old layout version doesn't have inode id.\"\n                + \" Will assign new id for each inode.\");\n          }\n        }\n        \n        if (supportSnapshot) {\n          snapshotMap = namesystem.getSnapshotManager().read(in, this);\n        }\n\n        // read compression related info\n        FSImageCompression compression;\n        if (LayoutVersion.supports(Feature.FSIMAGE_COMPRESSION, imgVersion)) {\n          compression = FSImageCompression.readCompressionHeader(conf, in);\n        } else {\n          compression = FSImageCompression.createNoopCompression();\n        }\n        in = compression.unwrapInputStream(fin);\n\n        LOG.info(\"Loading image file \" + curFile + \" using \" + compression);\n        \n        // load all inodes\n        LOG.info(\"Number of files = \" + numFiles);\n        prog.setTotal(Phase.LOADING_FSIMAGE, step, numFiles);\n        Counter counter = prog.getCounter(Phase.LOADING_FSIMAGE, step);\n        if (LayoutVersion.supports(Feature.FSIMAGE_NAME_OPTIMIZATION,\n            imgVersion)) {\n          if (supportSnapshot) {\n            loadLocalNameINodesWithSnapshot(numFiles, in, counter);\n          } else {\n            loadLocalNameINodes(numFiles, in, counter);\n          }\n        } else {\n          loadFullNameINodes(numFiles, in, counter);\n        }\n\n        loadFilesUnderConstruction(in, supportSnapshot, counter);\n        prog.endStep(Phase.LOADING_FSIMAGE, step);\n        // Now that the step is finished, set counter equal to total to adjust\n        // for possible under-counting due to reference inodes.\n        prog.setCount(Phase.LOADING_FSIMAGE, step, numFiles);\n\n        loadSecretManagerState(in);\n\n        loadCacheManagerState(in);\n\n        // make sure to read to the end of file\n        boolean eof = (in.read() == -1);\n        assert eof : \"Should have reached the end of image file \" + curFile;\n      } finally {\n        in.close();\n      }\n\n      imgDigest = new MD5Hash(digester.digest());\n      loaded = true;\n      \n      LOG.info(\"Image file \" + curFile + \" of size \" + curFile.length() +\n          \" bytes loaded in \" + (now() - startTime)/1000 + \" seconds.\");\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.loadFullNameINodes": "  private void loadFullNameINodes(long numFiles, DataInput in, Counter counter)\n      throws IOException {\n    byte[][] pathComponents;\n    byte[][] parentPath = {{}};      \n    FSDirectory fsDir = namesystem.dir;\n    INodeDirectory parentINode = fsDir.rootDir;\n    for (long i = 0; i < numFiles; i++) {\n      pathComponents = FSImageSerialization.readPathComponents(in);\n      final INode newNode = loadINode(\n          pathComponents[pathComponents.length-1], false, in, counter);\n\n      if (isRoot(pathComponents)) { // it is the root\n        // update the root's attributes\n        updateRootAttr(newNode.asDirectory());\n        continue;\n      }\n      // check if the new inode belongs to the same parent\n      if(!isParent(pathComponents, parentPath)) {\n        parentINode = getParentINodeDirectory(pathComponents);\n        parentPath = getParent(pathComponents);\n      }\n\n      // add new inode\n      addToParent(parentINode, newNode);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.loadSecretManagerState": "    private void loadSecretManagerState(DataInput in)\n        throws IOException {\n      int imgVersion = getLayoutVersion();\n\n      if (!LayoutVersion.supports(Feature.DELEGATION_TOKEN, imgVersion)) {\n        //SecretManagerState is not available.\n        //This must not happen if security is turned on.\n        return; \n      }\n      namesystem.loadSecretManagerState(in);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.loadLocalNameINodesWithSnapshot": "    private void loadLocalNameINodesWithSnapshot(long numFiles, DataInput in,\n        Counter counter) throws IOException {\n      assert LayoutVersion.supports(Feature.FSIMAGE_NAME_OPTIMIZATION,\n          getLayoutVersion());\n      assert LayoutVersion.supports(Feature.SNAPSHOT, getLayoutVersion());\n      \n      // load root\n      loadRoot(in, counter);\n      // load rest of the nodes recursively\n      loadDirectoryWithSnapshot(in, counter);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.checkNotLoaded": "    private void checkNotLoaded() {\n      if (loaded) {\n        throw new IllegalStateException(\"Image already loaded!\");\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.loadLocalNameINodes": "   private void loadLocalNameINodes(long numFiles, DataInput in, Counter counter)\n       throws IOException {\n     assert LayoutVersion.supports(Feature.FSIMAGE_NAME_OPTIMIZATION,\n         getLayoutVersion());\n     assert numFiles > 0;\n\n     // load root\n     loadRoot(in, counter);\n     // have loaded the first file (the root)\n     numFiles--; \n\n     // load rest of the nodes directory by directory\n     while (numFiles > 0) {\n       numFiles -= loadDirectory(in, counter);\n     }\n     if (numFiles != 0) {\n       throw new IOException(\"Read unexpect number of files: \" + -numFiles);\n     }\n   }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.loadCacheManagerState": "    private void loadCacheManagerState(DataInput in) throws IOException {\n      int imgVersion = getLayoutVersion();\n      if (!LayoutVersion.supports(Feature.CACHING, imgVersion)) {\n        return;\n      }\n      namesystem.getCacheManager().loadState(in);\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage": "  private void loadFSImage(File curFile, MD5Hash expectedMd5,\n      FSNamesystem target, MetaRecoveryContext recovery) throws IOException {\n    FSImageFormat.Loader loader = new FSImageFormat.Loader(\n        conf, target);\n    loader.load(curFile);\n    target.setBlockPoolId(this.getBlockPoolID());\n\n    // Check that the image digest we loaded matches up with what\n    // we expected\n    MD5Hash readImageMd5 = loader.getLoadedImageMd5();\n    if (expectedMd5 != null &&\n        !expectedMd5.equals(readImageMd5)) {\n      throw new IOException(\"Image file \" + curFile +\n          \" is corrupt with MD5 checksum of \" + readImageMd5 +\n          \" but expecting \" + expectedMd5);\n    }\n\n    long txId = loader.getLoadedImageTxId();\n    LOG.info(\"Loaded image for txid \" + txId + \" from \" + curFile);\n    lastAppliedTxId = txId;\n    storage.setMostRecentCheckpointInfo(txId, curFile.lastModified());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getBlockPoolID": "  public String getBlockPoolID() {\n    return storage.getBlockPoolID();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.initEditLog": "  public void initEditLog() {\n    Preconditions.checkState(getNamespaceID() != 0,\n        \"Must know namespace ID before initting edit log\");\n    String nameserviceId = DFSUtil.getNamenodeNameServiceId(conf);\n    if (!HAUtil.isHAEnabled(conf, nameserviceId)) {\n      editLog.initJournalsForWrite();\n      editLog.recoverUnclosedStreams();\n    } else {\n      editLog.initSharedJournalsForRead();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getLayoutVersion": "  public int getLayoutVersion() {\n    return storage.getLayoutVersion();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.needsResaveBasedOnStaleCheckpoint": "  private boolean needsResaveBasedOnStaleCheckpoint(\n      File imageFile, long numEditsLoaded) {\n    final long checkpointPeriod = conf.getLong(\n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_KEY, \n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_DEFAULT);\n    final long checkpointTxnCount = conf.getLong(\n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_KEY, \n        DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_DEFAULT);\n    long checkpointAge = Time.now() - imageFile.lastModified();\n\n    return (checkpointAge > checkpointPeriod * 1000) ||\n           (numEditsLoaded > checkpointTxnCount);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile": "  void loadFSImageFile(FSNamesystem target, MetaRecoveryContext recovery,\n      FSImageFile imageFile) throws IOException {\n    LOG.debug(\"Planning to load image :\\n\" + imageFile);\n    StorageDirectory sdForProperties = imageFile.sd;\n    storage.readProperties(sdForProperties);\n\n    if (LayoutVersion.supports(Feature.TXID_BASED_LAYOUT,\n                               getLayoutVersion())) {\n      // For txid-based layout, we should have a .md5 file\n      // next to the image file\n      loadFSImage(imageFile.getFile(), target, recovery);\n    } else if (LayoutVersion.supports(Feature.FSIMAGE_CHECKSUM,\n                                      getLayoutVersion())) {\n      // In 0.22, we have the checksum stored in the VERSION file.\n      String md5 = storage.getDeprecatedProperty(\n          NNStorage.DEPRECATED_MESSAGE_DIGEST_PROPERTY);\n      if (md5 == null) {\n        throw new InconsistentFSStateException(sdForProperties.getRoot(),\n            \"Message digest property \" +\n            NNStorage.DEPRECATED_MESSAGE_DIGEST_PROPERTY +\n            \" not set for storage directory \" + sdForProperties.getRoot());\n      }\n      loadFSImage(imageFile.getFile(), new MD5Hash(md5), target, recovery);\n    } else {\n      // We don't have any record of the md5sum\n      loadFSImage(imageFile.getFile(), null, target, recovery);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.isUpgradeFinalized": "  boolean isUpgradeFinalized() {\n    return isUpgradeFinalized;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits": "  public long loadEdits(Iterable<EditLogInputStream> editStreams,\n      FSNamesystem target, MetaRecoveryContext recovery) throws IOException {\n    LOG.debug(\"About to load edits:\\n  \" + Joiner.on(\"\\n  \").join(editStreams));\n    StartupProgress prog = NameNode.getStartupProgress();\n    prog.beginPhase(Phase.LOADING_EDITS);\n    \n    long prevLastAppliedTxId = lastAppliedTxId;  \n    try {    \n      FSEditLogLoader loader = new FSEditLogLoader(target, lastAppliedTxId);\n      \n      // Load latest edits\n      for (EditLogInputStream editIn : editStreams) {\n        LOG.info(\"Reading \" + editIn + \" expecting start txid #\" +\n              (lastAppliedTxId + 1));\n        try {\n          loader.loadFSEdits(editIn, lastAppliedTxId + 1, recovery);\n        } finally {\n          // Update lastAppliedTxId even in case of error, since some ops may\n          // have been successfully applied before the error.\n          lastAppliedTxId = loader.getLastAppliedTxId();\n        }\n        // If we are in recovery mode, we may have skipped over some txids.\n        if (editIn.getLastTxId() != HdfsConstants.INVALID_TXID) {\n          lastAppliedTxId = editIn.getLastTxId();\n        }\n      }\n    } finally {\n      FSEditLog.closeAllStreams(editStreams);\n      // update the counts\n      updateCountForQuota(target.dir.rootDir);   \n    }\n    prog.endPhase(Phase.LOADING_EDITS);\n    return lastAppliedTxId - prevLastAppliedTxId;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead": "  boolean recoverTransitionRead(StartupOption startOpt, FSNamesystem target,\n      MetaRecoveryContext recovery) throws IOException {\n    assert startOpt != StartupOption.FORMAT : \n      \"NameNode formatting should be performed before reading the image\";\n    \n    Collection<URI> imageDirs = storage.getImageDirectories();\n    Collection<URI> editsDirs = editLog.getEditURIs();\n\n    // none of the data dirs exist\n    if((imageDirs.size() == 0 || editsDirs.size() == 0) \n                             && startOpt != StartupOption.IMPORT)  \n      throw new IOException(\n          \"All specified directories are not accessible or do not exist.\");\n    \n    // 1. For each data directory calculate its state and \n    // check whether all is consistent before transitioning.\n    Map<StorageDirectory, StorageState> dataDirStates = \n             new HashMap<StorageDirectory, StorageState>();\n    boolean isFormatted = recoverStorageDirs(startOpt, dataDirStates);\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Data dir states:\\n  \" +\n        Joiner.on(\"\\n  \").withKeyValueSeparator(\": \")\n        .join(dataDirStates));\n    }\n    \n    if (!isFormatted && startOpt != StartupOption.ROLLBACK \n                     && startOpt != StartupOption.IMPORT) {\n      throw new IOException(\"NameNode is not formatted.\");      \n    }\n\n\n    int layoutVersion = storage.getLayoutVersion();\n    if (layoutVersion < Storage.LAST_PRE_UPGRADE_LAYOUT_VERSION) {\n      NNStorage.checkVersionUpgradable(storage.getLayoutVersion());\n    }\n    if (startOpt != StartupOption.UPGRADE\n        && layoutVersion < Storage.LAST_PRE_UPGRADE_LAYOUT_VERSION\n        && layoutVersion != HdfsConstants.LAYOUT_VERSION) {\n      throw new IOException(\n          \"\\nFile system image contains an old layout version \" \n          + storage.getLayoutVersion() + \".\\nAn upgrade to version \"\n          + HdfsConstants.LAYOUT_VERSION + \" is required.\\n\"\n          + \"Please restart NameNode with -upgrade option.\");\n    }\n    \n    storage.processStartupOptionsForUpgrade(startOpt, layoutVersion);\n\n    // 2. Format unformatted dirs.\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      StorageState curState = dataDirStates.get(sd);\n      switch(curState) {\n      case NON_EXISTENT:\n        throw new IOException(StorageState.NON_EXISTENT + \n                              \" state cannot be here\");\n      case NOT_FORMATTED:\n        LOG.info(\"Storage directory \" + sd.getRoot() + \" is not formatted.\");\n        LOG.info(\"Formatting ...\");\n        sd.clearDirectory(); // create empty currrent dir\n        break;\n      default:\n        break;\n      }\n    }\n\n    // 3. Do transitions\n    switch(startOpt) {\n    case UPGRADE:\n      doUpgrade(target);\n      return false; // upgrade saved image already\n    case IMPORT:\n      doImportCheckpoint(target);\n      return false; // import checkpoint saved image already\n    case ROLLBACK:\n      doRollback();\n      break;\n    case REGULAR:\n    default:\n      // just load the image\n    }\n    \n    return loadFSImage(target, recovery);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.doRollback": "  private void doRollback() throws IOException {\n    // Rollback is allowed only if there is \n    // a previous fs states in at least one of the storage directories.\n    // Directories that don't have previous state do not rollback\n    boolean canRollback = false;\n    FSImage prevState = new FSImage(conf);\n    prevState.getStorage().layoutVersion = HdfsConstants.LAYOUT_VERSION;\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      File prevDir = sd.getPreviousDir();\n      if (!prevDir.exists()) {  // use current directory then\n        LOG.info(\"Storage directory \" + sd.getRoot()\n                 + \" does not contain previous fs state.\");\n        // read and verify consistency with other directories\n        storage.readProperties(sd);\n        continue;\n      }\n\n      // read and verify consistency of the prev dir\n      prevState.getStorage().readPreviousVersionProperties(sd);\n\n      if (prevState.getLayoutVersion() != HdfsConstants.LAYOUT_VERSION) {\n        throw new IOException(\n          \"Cannot rollback to storage version \" +\n          prevState.getLayoutVersion() +\n          \" using this version of the NameNode, which uses storage version \" +\n          HdfsConstants.LAYOUT_VERSION + \". \" +\n          \"Please use the previous version of HDFS to perform the rollback.\");\n      }\n      canRollback = true;\n    }\n    if (!canRollback)\n      throw new IOException(\"Cannot rollback. None of the storage \"\n                            + \"directories contain previous fs state.\");\n\n    // Now that we know all directories are going to be consistent\n    // Do rollback for each directory containing previous state\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      File prevDir = sd.getPreviousDir();\n      if (!prevDir.exists())\n        continue;\n\n      LOG.info(\"Rolling back storage directory \" + sd.getRoot()\n               + \".\\n   new LV = \" + prevState.getStorage().getLayoutVersion()\n               + \"; new CTime = \" + prevState.getStorage().getCTime());\n      File tmpDir = sd.getRemovedTmp();\n      assert !tmpDir.exists() : \"removed.tmp directory must not exist.\";\n      // rename current to tmp\n      File curDir = sd.getCurrentDir();\n      assert curDir.exists() : \"Current directory must exist.\";\n      NNStorage.rename(curDir, tmpDir);\n      // rename previous to current\n      NNStorage.rename(prevDir, curDir);\n\n      // delete tmp dir\n      NNStorage.deleteDir(tmpDir);\n      LOG.info(\"Rollback of \" + sd.getRoot()+ \" is complete.\");\n    }\n    isUpgradeFinalized = true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs": "  private boolean recoverStorageDirs(StartupOption startOpt,\n      Map<StorageDirectory, StorageState> dataDirStates) throws IOException {\n    boolean isFormatted = false;\n    for (Iterator<StorageDirectory> it = \n                      storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      StorageState curState;\n      try {\n        curState = sd.analyzeStorage(startOpt, storage);\n        String nameserviceId = DFSUtil.getNamenodeNameServiceId(conf);\n        if (curState != StorageState.NORMAL && HAUtil.isHAEnabled(conf, nameserviceId)) {\n          throw new IOException(\"Cannot start an HA namenode with name dirs \" +\n              \"that need recovery. Dir: \" + sd + \" state: \" + curState);\n        }\n        // sd is locked but not opened\n        switch(curState) {\n        case NON_EXISTENT:\n          // name-node fails if any of the configured storage dirs are missing\n          throw new InconsistentFSStateException(sd.getRoot(),\n                      \"storage directory does not exist or is not accessible.\");\n        case NOT_FORMATTED:\n          break;\n        case NORMAL:\n          break;\n        default:  // recovery is possible\n          sd.doRecover(curState);      \n        }\n        if (curState != StorageState.NOT_FORMATTED \n            && startOpt != StartupOption.ROLLBACK) {\n          // read and verify consistency with other directories\n          storage.readProperties(sd);\n          isFormatted = true;\n        }\n        if (startOpt == StartupOption.IMPORT && isFormatted)\n          // import of a checkpoint is allowed only into empty image directories\n          throw new IOException(\"Cannot import image from a checkpoint. \" \n              + \" NameNode already contains an image in \" + sd.getRoot());\n      } catch (IOException ioe) {\n        sd.unlock();\n        throw ioe;\n      }\n      dataDirStates.put(sd,curState);\n    }\n    return isFormatted;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade": "  private void doUpgrade(FSNamesystem target) throws IOException {\n    // Upgrade is allowed only if there are \n    // no previous fs states in any of the directories\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      if (sd.getPreviousDir().exists())\n        throw new InconsistentFSStateException(sd.getRoot(),\n            \"previous fs state should not exist during upgrade. \"\n            + \"Finalize or rollback first.\");\n    }\n\n    // load the latest image\n    this.loadFSImage(target, null);\n\n    // Do upgrade for each directory\n    long oldCTime = storage.getCTime();\n    storage.cTime = now();  // generate new cTime for the state\n    int oldLV = storage.getLayoutVersion();\n    storage.layoutVersion = HdfsConstants.LAYOUT_VERSION;\n    \n    List<StorageDirectory> errorSDs =\n      Collections.synchronizedList(new ArrayList<StorageDirectory>());\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      LOG.info(\"Starting upgrade of image directory \" + sd.getRoot()\n               + \".\\n   old LV = \" + oldLV\n               + \"; old CTime = \" + oldCTime\n               + \".\\n   new LV = \" + storage.getLayoutVersion()\n               + \"; new CTime = \" + storage.getCTime());\n      try {\n        File curDir = sd.getCurrentDir();\n        File prevDir = sd.getPreviousDir();\n        File tmpDir = sd.getPreviousTmp();\n        assert curDir.exists() : \"Current directory must exist.\";\n        assert !prevDir.exists() : \"previous directory must not exist.\";\n        assert !tmpDir.exists() : \"previous.tmp directory must not exist.\";\n        assert !editLog.isSegmentOpen() : \"Edits log must not be open.\";\n\n        // rename current to tmp\n        NNStorage.rename(curDir, tmpDir);\n        \n        if (!curDir.mkdir()) {\n          throw new IOException(\"Cannot create directory \" + curDir);\n        }\n      } catch (Exception e) {\n        LOG.error(\"Failed to move aside pre-upgrade storage \" +\n            \"in image directory \" + sd.getRoot(), e);\n        errorSDs.add(sd);\n        continue;\n      }\n    }\n    storage.reportErrorsOnDirectories(errorSDs);\n    errorSDs.clear();\n\n    saveFSImageInAllDirs(target, editLog.getLastWrittenTxId());\n\n    for (Iterator<StorageDirectory> it = storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      try {\n        // Write the version file, since saveFsImage above only makes the\n        // fsimage_<txid>, and the directory is otherwise empty.\n        storage.writeProperties(sd);\n        \n        File prevDir = sd.getPreviousDir();\n        File tmpDir = sd.getPreviousTmp();\n        // rename tmp to previous\n        NNStorage.rename(tmpDir, prevDir);\n      } catch (IOException ioe) {\n        LOG.error(\"Unable to rename temp to previous for \" + sd.getRoot(), ioe);\n        errorSDs.add(sd);\n        continue;\n      }\n      LOG.info(\"Upgrade of \" + sd.getRoot() + \" is complete.\");\n    }\n    storage.reportErrorsOnDirectories(errorSDs);\n\n    isUpgradeFinalized = false;\n    if (!storage.getRemovedStorageDirs().isEmpty()) {\n      //during upgrade, it's a fatal error to fail any storage directory\n      throw new IOException(\"Upgrade failed in \"\n          + storage.getRemovedStorageDirs().size()\n          + \" storage directory(ies), previously logged.\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.doImportCheckpoint": "  void doImportCheckpoint(FSNamesystem target) throws IOException {\n    Collection<URI> checkpointDirs =\n      FSImage.getCheckpointDirs(conf, null);\n    List<URI> checkpointEditsDirs =\n      FSImage.getCheckpointEditsDirs(conf, null);\n\n    if (checkpointDirs == null || checkpointDirs.isEmpty()) {\n      throw new IOException(\"Cannot import image from a checkpoint. \"\n                            + \"\\\"dfs.namenode.checkpoint.dir\\\" is not set.\" );\n    }\n    \n    if (checkpointEditsDirs == null || checkpointEditsDirs.isEmpty()) {\n      throw new IOException(\"Cannot import image from a checkpoint. \"\n                            + \"\\\"dfs.namenode.checkpoint.dir\\\" is not set.\" );\n    }\n\n    FSImage realImage = target.getFSImage();\n    FSImage ckptImage = new FSImage(conf, \n                                    checkpointDirs, checkpointEditsDirs);\n    target.dir.fsImage = ckptImage;\n    // load from the checkpoint dirs\n    try {\n      ckptImage.recoverTransitionRead(StartupOption.REGULAR, target, null);\n    } finally {\n      ckptImage.close();\n    }\n    // return back the real image\n    realImage.getStorage().setStorageInfo(ckptImage.getStorage());\n    realImage.getEditLog().setNextTxId(ckptImage.getEditLog().getLastWrittenTxId()+1);\n    realImage.initEditLog();\n\n    target.dir.fsImage = realImage;\n    realImage.getStorage().setBlockPoolID(ckptImage.getBlockPoolID());\n\n    // and save it but keep the same checkpointTime\n    saveNamespace(target);\n    getStorage().writeAll();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage": "  void loadFSImage(StartupOption startOpt, FSImage fsImage, boolean haEnabled)\n      throws IOException {\n    // format before starting up if requested\n    if (startOpt == StartupOption.FORMAT) {\n      \n      fsImage.format(this, fsImage.getStorage().determineClusterId());// reuse current id\n\n      startOpt = StartupOption.REGULAR;\n    }\n    boolean success = false;\n    writeLock();\n    try {\n      // We shouldn't be calling saveNamespace if we've come up in standby state.\n      MetaRecoveryContext recovery = startOpt.createRecoveryContext();\n      boolean needToSave =\n        fsImage.recoverTransitionRead(startOpt, this, recovery) && !haEnabled;\n      if (needToSave) {\n        fsImage.saveNamespace(this);\n      } else {\n        // No need to save, so mark the phase done.\n        StartupProgress prog = NameNode.getStartupProgress();\n        prog.beginPhase(Phase.SAVING_CHECKPOINT);\n        prog.endPhase(Phase.SAVING_CHECKPOINT);\n      }\n      // This will start a new log segment and write to the seen_txid file, so\n      // we shouldn't do it when coming up in standby state\n      if (!haEnabled) {\n        fsImage.openEditLogForWrite();\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        fsImage.close();\n      }\n      writeUnlock();\n    }\n    dir.imageLoadComplete();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.close": "  void close() {\n    fsRunning = false;\n    try {\n      stopCommonServices();\n      if (smmthread != null) smmthread.interrupt();\n    } finally {\n      // using finally to ensure we also wait for lease daemon\n      try {\n        stopActiveServices();\n        stopStandbyServices();\n        if (dir != null) {\n          dir.close();\n        }\n      } catch (IOException ie) {\n        LOG.error(\"Error closing FSDirectory\", ie);\n        IOUtils.cleanup(LOG, dir);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock": "  public void writeUnlock() {\n    this.fsLock.writeLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock": "  public void writeLock() {\n    this.fsLock.writeLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveNamespace": "  void saveNamespace() throws AccessControlException, IOException {\n    checkOperation(OperationCategory.UNCHECKED);\n    checkSuperuserPrivilege();\n    \n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    boolean success = false;\n    readLock();\n    try {\n      checkOperation(OperationCategory.UNCHECKED);\n      if (!isInSafeMode()) {\n        throw new IOException(\"Safe mode should be turned ON \"\n            + \"in order to create namespace image.\");\n      }\n      getFSImage().saveNamespace(this);\n      success = true;\n    } finally {\n      readUnlock();\n      RetryCache.setState(cacheEntry, success);\n    }\n    LOG.info(\"New namespace image has been created\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk": "  public static FSNamesystem loadFromDisk(Configuration conf)\n      throws IOException {\n\n    checkConfiguration(conf);\n    FSImage fsImage = new FSImage(conf,\n        FSNamesystem.getNamespaceDirs(conf),\n        FSNamesystem.getNamespaceEditsDirs(conf));\n    FSNamesystem namesystem = new FSNamesystem(conf, fsImage, false);\n    StartupOption startOpt = NameNode.getStartupOption(conf);\n    if (startOpt == StartupOption.RECOVER) {\n      namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n    }\n\n    long loadStart = now();\n    String nameserviceId = DFSUtil.getNamenodeNameServiceId(conf);\n    namesystem.loadFSImage(startOpt, fsImage,\n      HAUtil.isHAEnabled(conf, nameserviceId));\n    long timeTakenToLoadFSImage = now() - loadStart;\n    LOG.info(\"Finished loading FSImage in \" + timeTakenToLoadFSImage + \" msecs\");\n    NameNodeMetrics nnMetrics = NameNode.getNameNodeMetrics();\n    if (nnMetrics != null) {\n      nnMetrics.setFsImageLoadTime((int) timeTakenToLoadFSImage);\n    }\n    return namesystem;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNamespaceEditsDirs": "  public static List<URI> getNamespaceEditsDirs(Configuration conf,\n      boolean includeShared)\n      throws IOException {\n    // Use a LinkedHashSet so that order is maintained while we de-dup\n    // the entries.\n    LinkedHashSet<URI> editsDirs = new LinkedHashSet<URI>();\n    \n    if (includeShared) {\n      List<URI> sharedDirs = getSharedEditsDirs(conf);\n  \n      // Fail until multiple shared edits directories are supported (HDFS-2782)\n      if (sharedDirs.size() > 1) {\n        throw new IOException(\n            \"Multiple shared edits directories are not yet supported\");\n      }\n  \n      // First add the shared edits dirs. It's critical that the shared dirs\n      // are added first, since JournalSet syncs them in the order they are listed,\n      // and we need to make sure all edits are in place in the shared storage\n      // before they are replicated locally. See HDFS-2874.\n      for (URI dir : sharedDirs) {\n        if (!editsDirs.add(dir)) {\n          LOG.warn(\"Edits URI \" + dir + \" listed multiple times in \" + \n              DFS_NAMENODE_SHARED_EDITS_DIR_KEY + \". Ignoring duplicates.\");\n        }\n      }\n    }    \n    // Now add the non-shared dirs.\n    for (URI dir : getStorageDirs(conf, DFS_NAMENODE_EDITS_DIR_KEY)) {\n      if (!editsDirs.add(dir)) {\n        LOG.warn(\"Edits URI \" + dir + \" listed multiple times in \" + \n            DFS_NAMENODE_SHARED_EDITS_DIR_KEY + \" and \" +\n            DFS_NAMENODE_EDITS_DIR_KEY + \". Ignoring duplicates.\");\n      }\n    }\n\n    if (editsDirs.isEmpty()) {\n      // If this is the case, no edit dirs have been explicitly configured.\n      // Image dirs are to be used for edits too.\n      return Lists.newArrayList(getNamespaceDirs(conf));\n    } else {\n      return Lists.newArrayList(editsDirs);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkConfiguration": "  private static void checkConfiguration(Configuration conf)\n      throws IOException {\n\n    final Collection<URI> namespaceDirs =\n        FSNamesystem.getNamespaceDirs(conf);\n    final Collection<URI> editsDirs =\n        FSNamesystem.getNamespaceEditsDirs(conf);\n    final Collection<URI> requiredEditsDirs =\n        FSNamesystem.getRequiredNamespaceEditsDirs(conf);\n    final Collection<URI> sharedEditsDirs =\n        FSNamesystem.getSharedEditsDirs(conf);\n\n    for (URI u : requiredEditsDirs) {\n      if (u.toString().compareTo(\n              DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_DEFAULT) == 0) {\n        continue;\n      }\n\n      // Each required directory must also be in editsDirs or in\n      // sharedEditsDirs.\n      if (!editsDirs.contains(u) &&\n          !sharedEditsDirs.contains(u)) {\n        throw new IllegalArgumentException(\n            \"Required edits directory \" + u.toString() + \" not present in \" +\n            DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_KEY + \". \" +\n            DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_KEY + \"=\" +\n            editsDirs.toString() + \"; \" +\n            DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_REQUIRED_KEY + \"=\" +\n            requiredEditsDirs.toString() + \". \" +\n            DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY + \"=\" +\n            sharedEditsDirs.toString() + \".\");\n      }\n    }\n\n    if (namespaceDirs.size() == 1) {\n      LOG.warn(\"Only one image storage directory (\"\n          + DFS_NAMENODE_NAME_DIR_KEY + \") configured. Beware of dataloss\"\n          + \" due to lack of redundant storage directories!\");\n    }\n    if (editsDirs.size() == 1) {\n      LOG.warn(\"Only one namespace edits storage directory (\"\n          + DFS_NAMENODE_EDITS_DIR_KEY + \") configured. Beware of dataloss\"\n          + \" due to lack of redundant storage directories!\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setSafeMode": "  boolean setSafeMode(SafeModeAction action) throws IOException {\n    if (action != SafeModeAction.SAFEMODE_GET) {\n      checkSuperuserPrivilege();\n      switch(action) {\n      case SAFEMODE_LEAVE: // leave safe mode\n        leaveSafeMode();\n        break;\n      case SAFEMODE_ENTER: // enter safe mode\n        enterSafeMode(false);\n        break;\n      default:\n        LOG.error(\"Unexpected safe mode action\");\n      }\n    }\n    return isInSafeMode();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNamespaceDirs": "  public static Collection<URI> getNamespaceDirs(Configuration conf) {\n    return getStorageDirs(conf, DFS_NAMENODE_NAME_DIR_KEY);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem": "  protected void loadNamesystem(Configuration conf) throws IOException {\n    this.namesystem = FSNamesystem.loadFromDisk(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initialize": "  protected void initialize(Configuration conf) throws IOException {\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    if (NamenodeRole.NAMENODE == role) {\n      startHttpServer(conf);\n      validateConfigurationSettingsOrAbort(conf);\n    }\n    loadNamesystem(conf);\n\n    rpcServer = createRpcServer(conf);\n    if (NamenodeRole.NAMENODE == role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    } else {\n      validateConfigurationSettingsOrAbort(conf);\n    }\n    \n    pauseMonitor = new JvmPauseMonitor(conf);\n    pauseMonitor.start();\n\n    startCommonServices(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getRole": "  public NamenodeRole getRole() {\n    return role;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getFSImage": "  public FSImage getFSImage() {\n    return namesystem.dir.fsImage;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.startCommonServices": "  private void startCommonServices(Configuration conf) throws IOException {\n    namesystem.startCommonServices(conf, haContext);\n    registerNNSMXBean();\n    if (NamenodeRole.NAMENODE != role) {\n      startHttpServer(conf);\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n    rpcServer.start();\n    plugins = conf.getInstances(DFS_NAMENODE_PLUGINS_KEY,\n        ServicePlugin.class);\n    for (ServicePlugin p: plugins) {\n      try {\n        p.start(this);\n      } catch (Throwable t) {\n        LOG.warn(\"ServicePlugin \" + p + \" could not be started\", t);\n      }\n    }\n    LOG.info(getRole() + \" RPC up at: \" + rpcServer.getRpcAddress());\n    if (rpcServer.getServiceRpcAddress() != null) {\n      LOG.info(getRole() + \" service RPC up at: \"\n          + rpcServer.getServiceRpcAddress());\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initMetrics": "  static void initMetrics(Configuration conf, NamenodeRole role) {\n    metrics = NameNodeMetrics.create(conf, role);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer": "  private void startHttpServer(final Configuration conf) throws IOException {\n    httpServer = new NameNodeHttpServer(conf, this, getHttpServerAddress(conf));\n    httpServer.start();\n    httpServer.setStartupProgress(startupProgress);\n    setHttpServerAddress(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.validateConfigurationSettingsOrAbort": "  private void validateConfigurationSettingsOrAbort(Configuration conf)\n      throws IOException {\n    try {\n      validateConfigurationSettings(conf);\n    } catch (IOException e) {\n      LOG.fatal(e.toString());\n      throw e;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer": "  protected NameNodeRpcServer createRpcServer(Configuration conf)\n      throws IOException {\n    return new NameNodeRpcServer(conf, this);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getNameNodeAddress": "  public InetSocketAddress getNameNodeAddress() {\n    return rpcServer.getRpcAddress();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.loginAsNameNodeUser": "  void loginAsNameNodeUser(Configuration conf) throws IOException {\n    InetSocketAddress socAddr = getRpcServerAddress(conf);\n    SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n        DFS_NAMENODE_USER_NAME_KEY, socAddr.getHostName());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf == null)\n      conf = new HdfsConfiguration();\n    // Parse out some generic args into Configuration.\n    GenericOptionsParser hParser = new GenericOptionsParser(conf, argv);\n    argv = hParser.getRemainingArgs();\n    // Parse the rest, NN specific args.\n    StartupOption startOpt = parseArguments(argv);\n    if (startOpt == null) {\n      printUsage(System.err);\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n    \n    if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) &&\n        (startOpt == StartupOption.UPGRADE ||\n         startOpt == StartupOption.ROLLBACK ||\n         startOpt == StartupOption.FINALIZE)) {\n      throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n          \"Cannot perform DFS upgrade with HA enabled.\");\n    }\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted = format(conf, startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        terminate(0);\n        return null;\n      }\n      case FINALIZE: {\n        boolean aborted = finalize(conf, true);\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] = Arrays.copyOfRange(argv, 1, argv.length);\n        int rc = BootstrapStandby.run(toolArgs, conf);\n        terminate(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted = initializeSharedEdits(conf,\n            startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role = startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      case RECOVER: {\n        NameNode.doRecovery(startOpt, conf);\n        return null;\n      }\n      default: {\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.printUsage": "  private static void printUsage(PrintStream out) {\n    out.println(USAGE + \"\\n\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits": "  private static boolean initializeSharedEdits(Configuration conf,\n      boolean force, boolean interactive) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    \n    if (conf.get(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY) == null) {\n      LOG.fatal(\"No shared edits directory configured for namespace \" +\n          nsId + \" namenode \" + namenodeId);\n      return false;\n    }\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      InetSocketAddress socAddr = getAddress(conf);\n      SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n          DFS_NAMENODE_USER_NAME_KEY, socAddr.getHostName());\n    }\n\n    NNStorage existingStorage = null;\n    try {\n      FSNamesystem fsns =\n          FSNamesystem.loadFromDisk(getConfigurationWithoutSharedEdits(conf));\n      \n      existingStorage = fsns.getFSImage().getStorage();\n      NamespaceInfo nsInfo = existingStorage.getNamespaceInfo();\n      \n      List<URI> sharedEditsDirs = FSNamesystem.getSharedEditsDirs(conf);\n      \n      FSImage sharedEditsImage = new FSImage(conf,\n          Lists.<URI>newArrayList(),\n          sharedEditsDirs);\n      sharedEditsImage.getEditLog().initJournalsForWrite();\n      \n      if (!sharedEditsImage.confirmFormat(force, interactive)) {\n        return true; // abort\n      }\n      \n      NNStorage newSharedStorage = sharedEditsImage.getStorage();\n      // Call Storage.format instead of FSImage.format here, since we don't\n      // actually want to save a checkpoint - just prime the dirs with\n      // the existing namespace info\n      newSharedStorage.format(nsInfo);\n      sharedEditsImage.getEditLog().formatNonFileJournals(nsInfo);\n\n      // Need to make sure the edit log segments are in good shape to initialize\n      // the shared edits dir.\n      fsns.getFSImage().getEditLog().close();\n      fsns.getFSImage().getEditLog().initJournalsForWrite();\n      fsns.getFSImage().getEditLog().recoverUnclosedStreams();\n\n      copyEditLogSegmentsToSharedDir(fsns, sharedEditsDirs, newSharedStorage,\n          conf);\n    } catch (IOException ioe) {\n      LOG.error(\"Could not initialize shared edits dir\", ioe);\n      return true; // aborted\n    } finally {\n      // Have to unlock storage explicitly for the case when we're running in a\n      // unit test, which runs in the same JVM as NNs.\n      if (existingStorage != null) {\n        try {\n          existingStorage.unlockAll();\n        } catch (IOException ioe) {\n          LOG.warn(\"Could not unlock storage directories\", ioe);\n          return true; // aborted\n        }\n      }\n    }\n    return false; // did not abort\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.setStartupOption": "  private static void setStartupOption(Configuration conf, StartupOption opt) {\n    conf.set(DFS_NAMENODE_STARTUP_KEY, opt.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.run": "          public FileSystem run() throws IOException {\n            return FileSystem.get(conf);\n          }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.parseArguments": "  private static StartupOption parseArguments(String args[]) {\n    int argsLen = (args == null) ? 0 : args.length;\n    StartupOption startOpt = StartupOption.REGULAR;\n    for(int i=0; i < argsLen; i++) {\n      String cmd = args[i];\n      if (StartupOption.FORMAT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.FORMAT;\n        for (i = i + 1; i < argsLen; i++) {\n          if (args[i].equalsIgnoreCase(StartupOption.CLUSTERID.getName())) {\n            i++;\n            if (i >= argsLen) {\n              // if no cluster id specified, return null\n              LOG.fatal(\"Must specify a valid cluster ID after the \"\n                  + StartupOption.CLUSTERID.getName() + \" flag\");\n              return null;\n            }\n            String clusterId = args[i];\n            // Make sure an id is specified and not another flag\n            if (clusterId.isEmpty() ||\n                clusterId.equalsIgnoreCase(StartupOption.FORCE.getName()) ||\n                clusterId.equalsIgnoreCase(\n                    StartupOption.NONINTERACTIVE.getName())) {\n              LOG.fatal(\"Must specify a valid cluster ID after the \"\n                  + StartupOption.CLUSTERID.getName() + \" flag\");\n              return null;\n            }\n            startOpt.setClusterId(clusterId);\n          }\n\n          if (args[i].equalsIgnoreCase(StartupOption.FORCE.getName())) {\n            startOpt.setForceFormat(true);\n          }\n\n          if (args[i].equalsIgnoreCase(StartupOption.NONINTERACTIVE.getName())) {\n            startOpt.setInteractiveFormat(false);\n          }\n        }\n      } else if (StartupOption.GENCLUSTERID.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.GENCLUSTERID;\n      } else if (StartupOption.REGULAR.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.REGULAR;\n      } else if (StartupOption.BACKUP.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.BACKUP;\n      } else if (StartupOption.CHECKPOINT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.CHECKPOINT;\n      } else if (StartupOption.UPGRADE.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.UPGRADE;\n        // might be followed by two args\n        if (i + 2 < argsLen\n            && args[i + 1].equalsIgnoreCase(StartupOption.CLUSTERID.getName())) {\n          i += 2;\n          startOpt.setClusterId(args[i]);\n        }\n      } else if (StartupOption.ROLLBACK.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.ROLLBACK;\n      } else if (StartupOption.FINALIZE.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.FINALIZE;\n      } else if (StartupOption.IMPORT.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.IMPORT;\n      } else if (StartupOption.BOOTSTRAPSTANDBY.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.BOOTSTRAPSTANDBY;\n        return startOpt;\n      } else if (StartupOption.INITIALIZESHAREDEDITS.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.INITIALIZESHAREDEDITS;\n        for (i = i + 1 ; i < argsLen; i++) {\n          if (StartupOption.NONINTERACTIVE.getName().equals(args[i])) {\n            startOpt.setInteractiveFormat(false);\n          } else if (StartupOption.FORCE.getName().equals(args[i])) {\n            startOpt.setForceFormat(true);\n          } else {\n            LOG.fatal(\"Invalid argument: \" + args[i]);\n            return null;\n          }\n        }\n        return startOpt;\n      } else if (StartupOption.RECOVER.getName().equalsIgnoreCase(cmd)) {\n        if (startOpt != StartupOption.REGULAR) {\n          throw new RuntimeException(\"Can't combine -recover with \" +\n              \"other startup options.\");\n        }\n        startOpt = StartupOption.RECOVER;\n        while (++i < argsLen) {\n          if (args[i].equalsIgnoreCase(\n                StartupOption.FORCE.getName())) {\n            startOpt.setForce(MetaRecoveryContext.FORCE_FIRST_CHOICE);\n          } else {\n            throw new RuntimeException(\"Error parsing recovery options: \" + \n              \"can't understand option \\\"\" + args[i] + \"\\\"\");\n          }\n        }\n      } else {\n        return null;\n      }\n    }\n    return startOpt;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.finalize": "  private static boolean finalize(Configuration conf,\n                               boolean isConfirmationNeeded\n                               ) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n\n    FSNamesystem nsys = new FSNamesystem(conf, new FSImage(conf));\n    System.err.print(\n        \"\\\"finalize\\\" will remove the previous state of the files system.\\n\"\n        + \"Recent upgrade will become permanent.\\n\"\n        + \"Rollback option will not be available anymore.\\n\");\n    if (isConfirmationNeeded) {\n      if (!confirmPrompt(\"Finalize filesystem state?\")) {\n        System.err.println(\"Finalize aborted.\");\n        return true;\n      }\n    }\n    nsys.dir.fsImage.finalizeUpgrade();\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.doRecovery": "  private static void doRecovery(StartupOption startOpt, Configuration conf)\n      throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    if (startOpt.getForce() < MetaRecoveryContext.FORCE_ALL) {\n      if (!confirmPrompt(\"You have selected Metadata Recovery mode.  \" +\n          \"This mode is intended to recover lost metadata on a corrupt \" +\n          \"filesystem.  Metadata recovery mode often permanently deletes \" +\n          \"data from your HDFS filesystem.  Please back up your edit log \" +\n          \"and fsimage before trying this!\\n\\n\" +\n          \"Are you ready to proceed? (Y/N)\\n\")) {\n        System.err.println(\"Recovery aborted at user request.\\n\");\n        return;\n      }\n    }\n    MetaRecoveryContext.LOG.info(\"starting recovery...\");\n    UserGroupInformation.setConfiguration(conf);\n    NameNode.initMetrics(conf, startOpt.toNodeRole());\n    FSNamesystem fsn = null;\n    try {\n      fsn = FSNamesystem.loadFromDisk(conf);\n      fsn.saveNamespace();\n      MetaRecoveryContext.LOG.info(\"RECOVERY COMPLETE\");\n    } catch (IOException e) {\n      MetaRecoveryContext.LOG.info(\"RECOVERY FAILED: caught exception\", e);\n      throw e;\n    } catch (RuntimeException e) {\n      MetaRecoveryContext.LOG.info(\"RECOVERY FAILED: caught exception\", e);\n      throw e;\n    } finally {\n      if (fsn != null)\n        fsn.close();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.format": "  private static boolean format(Configuration conf, boolean force,\n      boolean isInteractive) throws IOException {\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n    String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n    initializeGenericKeys(conf, nsId, namenodeId);\n    checkAllowFormat(conf);\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      InetSocketAddress socAddr = getAddress(conf);\n      SecurityUtil.login(conf, DFS_NAMENODE_KEYTAB_FILE_KEY,\n          DFS_NAMENODE_USER_NAME_KEY, socAddr.getHostName());\n    }\n    \n    Collection<URI> nameDirsToFormat = FSNamesystem.getNamespaceDirs(conf);\n    List<URI> sharedDirs = FSNamesystem.getSharedEditsDirs(conf);\n    List<URI> dirsToPrompt = new ArrayList<URI>();\n    dirsToPrompt.addAll(nameDirsToFormat);\n    dirsToPrompt.addAll(sharedDirs);\n    List<URI> editDirsToFormat = \n                 FSNamesystem.getNamespaceEditsDirs(conf);\n\n    // if clusterID is not provided - see if you can find the current one\n    String clusterId = StartupOption.FORMAT.getClusterId();\n    if(clusterId == null || clusterId.equals(\"\")) {\n      //Generate a new cluster id\n      clusterId = NNStorage.newClusterID();\n    }\n    System.out.println(\"Formatting using clusterid: \" + clusterId);\n    \n    FSImage fsImage = new FSImage(conf, nameDirsToFormat, editDirsToFormat);\n    FSNamesystem fsn = new FSNamesystem(conf, fsImage);\n    fsImage.getEditLog().initJournalsForWrite();\n    \n    if (!fsImage.confirmFormat(force, isInteractive)) {\n      return true; // aborted\n    }\n    \n    fsImage.format(fsn, clusterId);\n    return false;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.main": "  public static void main(String argv[]) throws Exception {\n    if (DFSUtil.parseHelpArgument(argv, NameNode.USAGE, System.out, true)) {\n      System.exit(0);\n    }\n\n    try {\n      StringUtils.startupShutdownMessage(NameNode.class, argv, LOG);\n      NameNode namenode = createNameNode(argv, null);\n      if (namenode != null) {\n        namenode.join();\n      }\n    } catch (Throwable e) {\n      LOG.fatal(\"Exception in namenode join\", e);\n      terminate(1, e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.join": "  public void join() {\n    try {\n      rpcServer.join();\n    } catch (InterruptedException ie) {\n      LOG.info(\"Caught interrupted exception \", ie);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.getINode": "  public INode getINode(int i) {\n    return inodes[i >= 0? i: inodes.length + i];\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.getClientName": "  public String getClientName() {\n    return clientName;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageSerialization.readINodeUnderConstruction": "  static INodeFileUnderConstruction readINodeUnderConstruction(\n      DataInput in, FSNamesystem fsNamesys, int imgVersion)\n      throws IOException {\n    byte[] name = readBytes(in);\n    long inodeId = LayoutVersion.supports(Feature.ADD_INODE_ID, imgVersion) ? in\n        .readLong() : fsNamesys.allocateNewInodeId();\n    short blockReplication = in.readShort();\n    long modificationTime = in.readLong();\n    long preferredBlockSize = in.readLong();\n  \n    int numBlocks = in.readInt();\n    BlockInfo[] blocks = new BlockInfo[numBlocks];\n    Block blk = new Block();\n    int i = 0;\n    for (; i < numBlocks-1; i++) {\n      blk.readFields(in);\n      blocks[i] = new BlockInfo(blk, blockReplication);\n    }\n    // last block is UNDER_CONSTRUCTION\n    if(numBlocks > 0) {\n      blk.readFields(in);\n      blocks[i] = new BlockInfoUnderConstruction(\n        blk, blockReplication, BlockUCState.UNDER_CONSTRUCTION, null);\n    }\n    PermissionStatus perm = PermissionStatus.read(in);\n    String clientName = readString(in);\n    String clientMachine = readString(in);\n\n    // We previously stored locations for the last block, now we\n    // just record that there are none\n    int numLocs = in.readInt();\n    assert numLocs == 0 : \"Unexpected block locations\";\n\n    return new INodeFileUnderConstruction(inodeId,\n                                          name,\n                                          blockReplication, \n                                          modificationTime,\n                                          preferredBlockSize,\n                                          blocks,\n                                          perm,\n                                          clientName,\n                                          clientMachine,\n                                          null);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageSerialization.readBytes": "  public static byte[] readBytes(DataInput in) throws IOException {\n    DeprecatedUTF8 ustr = TL_DATA.get().U_STR;\n    ustr.readFields(in);\n    int len = ustr.getLength();\n    byte[] bytes = new byte[len];\n    System.arraycopy(ustr.getBytes(), 0, bytes, 0, len);\n    return bytes;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageSerialization.readInt": "  static int readInt(DataInput in) throws IOException {\n    IntWritable uInt = TL_DATA.get().U_INT;\n    uInt.readFields(in);\n    return uInt.get();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageSerialization.readLong": "  static long readLong(DataInput in) throws IOException {\n    LongWritable uLong = TL_DATA.get().U_LONG;\n    uLong.readFields(in);\n    return uLong.get();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageSerialization.readString": "  public static String readString(DataInput in) throws IOException {\n    DeprecatedUTF8 ustr = TL_DATA.get().U_STR;\n    ustr.readFields(in);\n    return ustr.toStringChecked();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageSerialization.readShort": "  static short readShort(DataInput in) throws IOException {\n    ShortWritable uShort = TL_DATA.get().U_SHORT;\n    uShort.readFields(in);\n    return uShort.get();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf": "  public static INodeFile valueOf(INode inode, String path, boolean acceptNull)\n      throws FileNotFoundException {\n    if (inode == null) {\n      if (acceptNull) {\n        return null;\n      } else {\n        throw new FileNotFoundException(\"File does not exist: \" + path);\n      }\n    }\n    if (!inode.isFile()) {\n      throw new FileNotFoundException(\"Path is not a file: \" + path);\n    }\n    return inode.asFile();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.asFile": "  public final INodeFile asFile() {\n    return this;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.isFile": "  public final boolean isFile() {\n    return true;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getLastINodeInPath": "  public INodesInPath getLastINodeInPath(String src)\n       throws UnresolvedLinkException {\n    readLock();\n    try {\n      return rootDir.getLastINodeInPath(src, true);\n    } finally {\n      readUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.readUnlock": "  void readUnlock() {\n    this.dirLock.readLock().unlock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.readLock": "  void readLock() {\n    this.dirLock.readLock().lock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageCompression.unwrapInputStream": "  DataInputStream unwrapInputStream(InputStream is) throws IOException {\n    if (imageCodec != null) {\n      return new DataInputStream(imageCodec.createInputStream(is));\n    } else {\n      return new DataInputStream(new BufferedInputStream(is));\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageCompression.createNoopCompression": "  static FSImageCompression createNoopCompression() {\n    return new FSImageCompression();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageCompression.readCompressionHeader": "  static FSImageCompression readCompressionHeader(\n    Configuration conf, DataInput in) throws IOException\n  {\n    boolean isCompressed = in.readBoolean();\n\n    if (!isCompressed) {\n      return createNoopCompression();\n    } else {\n      String codecClassName = Text.readString(in);\n      return createCompression(conf, codecClassName);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageCompression.createCompression": "  private static FSImageCompression createCompression(Configuration conf,\n                                                      String codecClassName)\n    throws IOException {\n\n    CompressionCodecFactory factory = new CompressionCodecFactory(conf);\n    CompressionCodec codec = factory.getCodecByClassName(codecClassName);\n    if (codec == null) {\n      throw new IOException(\"Not a supported codec: \" + codecClassName);\n    }\n\n    return new FSImageCompression(codec);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getStartupProgress": "  public static StartupProgress getStartupProgress() {\n    return startupProgress;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditLogStreams": "  static Iterable<EditLogInputStream> getEditLogStreams(NNStorage storage)\n      throws IOException {\n    FSImagePreTransactionalStorageInspector inspector \n      = new FSImagePreTransactionalStorageInspector();\n    storage.inspectStorageDirs(inspector);\n\n    List<EditLogInputStream> editStreams = new ArrayList<EditLogInputStream>();\n    for (File f : inspector.getLatestEditsFiles()) {\n      editStreams.add(new EditLogFileInputStream(f));\n    }\n    return editStreams;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getLatestEditsFiles": "  private List<File> getLatestEditsFiles() {\n    if (latestNameCheckpointTime > latestEditsCheckpointTime) {\n      // the image is already current, discard edits\n      LOG.debug(\n          \"Name checkpoint time is newer than edits, not loading edits.\");\n      return Collections.<File>emptyList();\n    }\n    \n    return getEditsInStorageDir(latestEditsSD);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.closeAllStreams": "  static void closeAllStreams(Iterable<EditLogInputStream> streams) {\n    for (EditLogInputStream s : streams) {\n      IOUtils.closeStream(s);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getLatestImages": "  abstract List<FSImageFile> getLatestImages() throws IOException;\n\n  /** \n   * Get the minimum tx id which should be loaded with this set of images.\n   */\n  abstract long getMaxSeenTxId();\n\n  /**\n   * @return true if the directories are in such a state that the image should be re-saved\n   * following the load\n   */\n  abstract boolean needToSave();\n\n  /**\n   * Record of an image that has been located and had its filename parsed.\n   */\n  static class FSImageFile {\n    final StorageDirectory sd;    \n    final long txId;\n    private final File file;\n    \n    FSImageFile(StorageDirectory sd, File file, long txId) {\n      assert txId >= 0 || txId == HdfsConstants.INVALID_TXID \n        : \"Invalid txid on \" + file +\": \" + txId;\n      \n      this.sd = sd;\n      this.txId = txId;\n      this.file = file;\n    } \n    \n    File getFile() {\n      return file;\n    }\n\n    public long getCheckpointTxId() {\n      return txId;\n    }\n    \n    @Override\n    public String toString() {\n      return String.format(\"FSImageFile(file=%s, cpktTxId=%019d)\", \n                           file.toString(), txId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.needToSave": "  abstract boolean needToSave();\n\n  /**\n   * Record of an image that has been located and had its filename parsed.\n   */\n  static class FSImageFile {\n    final StorageDirectory sd;    \n    final long txId;\n    private final File file;\n    \n    FSImageFile(StorageDirectory sd, File file, long txId) {\n      assert txId >= 0 || txId == HdfsConstants.INVALID_TXID \n        : \"Invalid txid on \" + file +\": \" + txId;\n      \n      this.sd = sd;\n      this.txId = txId;\n      this.file = file;\n    } \n    \n    File getFile() {\n      return file;\n    }\n\n    public long getCheckpointTxId() {\n      return txId;\n    }\n    \n    @Override\n    public String toString() {\n      return String.format(\"FSImageFile(file=%s, cpktTxId=%019d)\", \n                           file.toString(), txId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getMaxSeenTxId": "  abstract long getMaxSeenTxId();\n\n  /**\n   * @return true if the directories are in such a state that the image should be re-saved\n   * following the load\n   */\n  abstract boolean needToSave();\n\n  /**\n   * Record of an image that has been located and had its filename parsed.\n   */\n  static class FSImageFile {\n    final StorageDirectory sd;    \n    final long txId;\n    private final File file;\n    \n    FSImageFile(StorageDirectory sd, File file, long txId) {\n      assert txId >= 0 || txId == HdfsConstants.INVALID_TXID \n        : \"Invalid txid on \" + file +\": \" + txId;\n      \n      this.sd = sd;\n      this.txId = txId;\n      this.file = file;\n    } \n    \n    File getFile() {\n      return file;\n    }\n\n    public long getCheckpointTxId() {\n      return txId;\n    }\n    \n    @Override\n    public String toString() {\n      return String.format(\"FSImageFile(file=%s, cpktTxId=%019d)\", \n                           file.toString(), txId);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.format": "  void format(FSNamesystem fsn, String clusterId) throws IOException {\n    long fileCount = fsn.getTotalFiles();\n    // Expect 1 file, which is the root inode\n    Preconditions.checkState(fileCount == 1,\n        \"FSImage.format should be called with an uninitialized namesystem, has \" +\n        fileCount + \" files\");\n    NamespaceInfo ns = NNStorage.newNamespaceInfo();\n    ns.clusterID = clusterId;\n    \n    storage.format(ns);\n    editLog.formatNonFileJournals(ns);\n    saveFSImageInAllDirs(fsn, 0);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImageInAllDirs": "  protected synchronized void saveFSImageInAllDirs(FSNamesystem source, long txid,\n      Canceler canceler)\n      throws IOException {    \n    StartupProgress prog = NameNode.getStartupProgress();\n    prog.beginPhase(Phase.SAVING_CHECKPOINT);\n    if (storage.getNumStorageDirs(NameNodeDirType.IMAGE) == 0) {\n      throw new IOException(\"No image directories available!\");\n    }\n    if (canceler == null) {\n      canceler = new Canceler();\n    }\n    SaveNamespaceContext ctx = new SaveNamespaceContext(\n        source, txid, canceler);\n    \n    try {\n      List<Thread> saveThreads = new ArrayList<Thread>();\n      // save images into current\n      for (Iterator<StorageDirectory> it\n             = storage.dirIterator(NameNodeDirType.IMAGE); it.hasNext();) {\n        StorageDirectory sd = it.next();\n        FSImageSaver saver = new FSImageSaver(ctx, sd);\n        Thread saveThread = new Thread(saver, saver.toString());\n        saveThreads.add(saveThread);\n        saveThread.start();\n      }\n      waitForThreads(saveThreads);\n      saveThreads.clear();\n      storage.reportErrorsOnDirectories(ctx.getErrorSDs());\n  \n      if (storage.getNumStorageDirs(NameNodeDirType.IMAGE) == 0) {\n        throw new IOException(\n          \"Failed to save in any storage directories while saving namespace.\");\n      }\n      if (canceler.isCancelled()) {\n        deleteCancelledCheckpoint(txid);\n        ctx.checkCancelled(); // throws\n        assert false : \"should have thrown above!\";\n      }\n  \n      renameCheckpoint(txid);\n  \n      // Since we now have a new checkpoint, we can clean up some\n      // old edit logs and checkpoints.\n      purgeOldStorage();\n    } finally {\n      // Notify any threads waiting on the checkpoint to be canceled\n      // that it is complete.\n      ctx.markComplete();\n      ctx = null;\n    }\n    prog.endPhase(Phase.SAVING_CHECKPOINT);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.getStorage": "  public NNStorage getStorage() {\n    return storage;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImage.openEditLogForWrite": "  void openEditLogForWrite() throws IOException {\n    assert editLog != null : \"editLog must be initialized\";\n    editLog.openForWrite();\n    storage.writeTransactionIdFileToStorage(editLog.getCurSegmentTxId());\n  };",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getStartupOption": "  static StartupOption getStartupOption(Configuration conf) {\n    return StartupOption.valueOf(conf.get(DFS_NAMENODE_STARTUP_KEY,\n                                          StartupOption.REGULAR.toString()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getNameNodeMetrics": "  public static NameNodeMetrics getNameNodeMetrics() {\n    return metrics;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.newClusterID": "  public static String newClusterID() {\n    return \"CID-\" + UUID.randomUUID().toString();\n  }"
        },
        "bug_report": {
            "Title": "Renaming underconstruction file with snapshots can make NN failure on restart",
            "Description": "I faced this When i am doing some snapshot operations like createSnapshot,renameSnapshot,i restarted my NN,it is shutting down with exception,\n2013-10-24 21:07:03,040 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join\njava.lang.IllegalStateException\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:133)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$ChildrenDiff.replace(INodeDirectoryWithSnapshot.java:82)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$ChildrenDiff.access$700(INodeDirectoryWithSnapshot.java:62)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$DirectoryDiffList.replaceChild(INodeDirectoryWithSnapshot.java:397)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$DirectoryDiffList.access$900(INodeDirectoryWithSnapshot.java:376)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot.replaceChild(INodeDirectoryWithSnapshot.java:598)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedReplaceINodeFile(FSDirectory.java:1548)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.replaceINodeFile(FSDirectory.java:1537)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.loadFilesUnderConstruction(FSImageFormat.java:855)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.load(FSImageFormat.java:350)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:910)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:899)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:751)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:720)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:266)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:784)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:563)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:422)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:472)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:670)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:655)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1245)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1311)\n2013-10-24 21:07:03,050 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1\n2013-10-24 21:07:03,052 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: \n"
        }
    },
    {
        "filename": "HDFS-13145.json",
        "creation_time": "2018-02-13T19:22:56.000+0000",
        "stack_trace": "```\njava.lang.IllegalStateException: Cannot start writing at txid 24312595802 when there is a stream available for read: ......\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.openForWrite(FSEditLog.java:329)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1196)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1839)\n        at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)\n        at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)\n        at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1707)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1622)\n        at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)\n        at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:4460)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:851)\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:794)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2490)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.openForWrite": "  synchronized void openForWrite(int layoutVersion) throws IOException {\n    Preconditions.checkState(state == State.BETWEEN_LOG_SEGMENTS,\n        \"Bad state: %s\", state);\n\n    long segmentTxId = getLastWrittenTxId() + 1;\n    // Safety check: we should never start a segment if there are\n    // newer txids readable.\n    List<EditLogInputStream> streams = new ArrayList<EditLogInputStream>();\n    journalSet.selectInputStreams(streams, segmentTxId, true, false);\n    if (!streams.isEmpty()) {\n      String error = String.format(\"Cannot start writing at txid %s \" +\n        \"when there is a stream available for read: %s\",\n        segmentTxId, streams.get(0));\n      IOUtils.cleanupWithLogger(LOG,\n          streams.toArray(new EditLogInputStream[0]));\n      throw new IllegalStateException(error);\n    }\n    \n    startLogSegmentAndWriteHeaderTxn(segmentTxId, layoutVersion);\n    assert state == State.IN_SEGMENT : \"Bad state: \" + state;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.selectInputStreams": "  public Collection<EditLogInputStream> selectInputStreams(long fromTxId,\n      long toAtLeastTxId, MetaRecoveryContext recovery, boolean inProgressOk,\n      boolean onlyDurableTxns) throws IOException {\n\n    List<EditLogInputStream> streams = new ArrayList<EditLogInputStream>();\n    synchronized(journalSetLock) {\n      Preconditions.checkState(journalSet.isOpen(), \"Cannot call \" +\n          \"selectInputStreams() on closed FSEditLog\");\n      selectInputStreams(streams, fromTxId, inProgressOk, onlyDurableTxns);\n    }\n\n    try {\n      checkForGaps(streams, fromTxId, toAtLeastTxId, inProgressOk);\n    } catch (IOException e) {\n      if (recovery != null) {\n        // If recovery mode is enabled, continue loading even if we know we\n        // can't load up to toAtLeastTxId.\n        LOG.error(\"Exception while selecting input streams\", e);\n      } else {\n        closeAllStreams(streams);\n        throw e;\n      }\n    }\n    return streams;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.getLastWrittenTxId": "  public synchronized long getLastWrittenTxId() {\n    return txid;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.startLogSegmentAndWriteHeaderTxn": "  synchronized void startLogSegmentAndWriteHeaderTxn(final long segmentTxId,\n      int layoutVersion) throws IOException {\n    startLogSegment(segmentTxId, layoutVersion);\n\n    logEdit(LogSegmentOp.getInstance(cache.get(),\n        FSEditLogOpCodes.OP_START_LOG_SEGMENT));\n    logSync();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices": "  void startActiveServices() throws IOException {\n    startingActiveService = true;\n    LOG.info(\"Starting services required for active state\");\n    writeLock();\n    try {\n      FSEditLog editLog = getFSImage().getEditLog();\n      \n      if (!editLog.isOpenForWrite()) {\n        // During startup, we're already open for write during initialization.\n        editLog.initJournalsForWrite();\n        // May need to recover\n        editLog.recoverUnclosedStreams();\n        \n        LOG.info(\"Catching up to latest edits from old active before \" +\n            \"taking over writer role in edits logs\");\n        editLogTailer.catchupDuringFailover();\n        \n        blockManager.setPostponeBlocksFromFuture(false);\n        blockManager.getDatanodeManager().markAllDatanodesStale();\n        blockManager.clearQueues();\n        blockManager.processAllPendingDNMessages();\n\n        // Only need to re-process the queue, If not in SafeMode.\n        if (!isInSafeMode()) {\n          LOG.info(\"Reprocessing replication and invalidation queues\");\n          blockManager.initializeReplQueues();\n        }\n\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"NameNode metadata after re-processing \" +\n              \"replication and invalidation queues during failover:\\n\" +\n              metaSaveAsString());\n        }\n\n        long nextTxId = getFSImage().getLastAppliedTxId() + 1;\n        LOG.info(\"Will take over writing edit logs at txnid \" + \n            nextTxId);\n        editLog.setNextTxId(nextTxId);\n\n        getFSImage().editLog.openForWrite(getEffectiveLayoutVersion());\n      }\n\n      // Initialize the quota.\n      dir.updateCountForQuota();\n      // Enable quota checks.\n      dir.enableQuotaChecks();\n      dir.ezManager.startReencryptThreads();\n\n      if (haEnabled) {\n        // Renew all of the leases before becoming active.\n        // This is because, while we were in standby mode,\n        // the leases weren't getting renewed on this NN.\n        // Give them all a fresh start here.\n        leaseManager.renewAllLeases();\n      }\n      leaseManager.startMonitor();\n      startSecretManagerIfNecessary();\n\n      //ResourceMonitor required only at ActiveNN. See HDFS-2914\n      this.nnrmthread = new Daemon(new NameNodeResourceMonitor());\n      nnrmthread.start();\n\n      nnEditLogRoller = new Daemon(new NameNodeEditLogRoller(\n          editLogRollerThreshold, editLogRollerInterval));\n      nnEditLogRoller.start();\n\n      if (lazyPersistFileScrubIntervalSec > 0) {\n        lazyPersistFileScrubber = new Daemon(new LazyPersistFileScrubber(\n            lazyPersistFileScrubIntervalSec));\n        lazyPersistFileScrubber.start();\n      } else {\n        LOG.warn(\"Lazy persist file scrubber is disabled,\"\n            + \" configured scrub interval is zero.\");\n      }\n\n      cacheManager.startMonitorThread();\n      blockManager.getDatanodeManager().setShouldSendCachingCommands(true);\n      if (provider != null) {\n        edekCacheLoader = Executors.newSingleThreadExecutor(\n            new ThreadFactoryBuilder().setDaemon(true)\n                .setNameFormat(\"Warm Up EDEK Cache Thread #%d\")\n                .build());\n        FSDirEncryptionZoneOp.warmUpEdekCache(edekCacheLoader, dir,\n            edekCacheLoaderDelay, edekCacheLoaderInterval);\n      }\n    } finally {\n      startingActiveService = false;\n      blockManager.checkSafeMode();\n      writeUnlock(\"startActiveServices\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.metaSaveAsString": "  private String metaSaveAsString() {\n    StringWriter sw = new StringWriter();\n    PrintWriter pw = new PrintWriter(sw);\n    metaSave(pw);\n    pw.flush();\n    return sw.toString();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEditLog": "  public FSEditLog getEditLog() {\n    return getFSImage().getEditLog();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEffectiveLayoutVersion": "  static int getEffectiveLayoutVersion(boolean isRollingUpgrade, int storageLV,\n      int minCompatLV, int currentLV) {\n    if (isRollingUpgrade) {\n      if (storageLV <= minCompatLV) {\n        // The prior layout version satisfies the minimum compatible layout\n        // version of the current software.  Keep reporting the prior layout\n        // as the effective one.  Downgrade is possible.\n        return storageLV;\n      }\n    }\n    // The current software cannot satisfy the layout version of the prior\n    // software.  Proceed with using the current layout version.\n    return currentLV;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock": "  public void writeUnlock(String opName, boolean suppressWriteLockReport) {\n    this.fsLock.writeUnlock(opName, suppressWriteLockReport);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.isInSafeMode": "  public boolean isInSafeMode() {\n    return isInManualOrResourceLowSafeMode() || blockManager.isInSafeMode();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFSImage": "  public FSImage getFSImage() {\n    return fsImage;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startSecretManagerIfNecessary": "  public void startSecretManagerIfNecessary() {\n    assert hasWriteLock() : \"Starting secret manager needs write lock\";\n    boolean shouldRun = shouldUseDelegationTokens() &&\n      !isInSafeMode() && getEditLog().isOpenForWrite();\n    boolean running = dtSecretManager.isRunning();\n    if (shouldRun && !running) {\n      startSecretManager();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock": "  public void writeLock() {\n    this.fsLock.writeLock();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.startActiveServices": "    public void startActiveServices() throws IOException {\n      try {\n        namesystem.startActiveServices();\n        startTrashEmptier(getConf());\n      } catch (Throwable t) {\n        doImmediateShutdown(t);\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.startTrashEmptier": "  private void startTrashEmptier(final Configuration conf) throws IOException {\n    long trashInterval =\n        conf.getLong(FS_TRASH_INTERVAL_KEY, FS_TRASH_INTERVAL_DEFAULT);\n    if (trashInterval == 0) {\n      return;\n    } else if (trashInterval < 0) {\n      throw new IOException(\"Cannot start trash emptier with negative interval.\"\n          + \" Set \" + FS_TRASH_INTERVAL_KEY + \" to a positive value.\");\n    }\n    \n    // This may be called from the transitionToActive code path, in which\n    // case the current user is the administrator, not the NN. The trash\n    // emptier needs to run as the NN. See HDFS-3972.\n    FileSystem fs = SecurityUtil.doAsLoginUser(\n        new PrivilegedExceptionAction<FileSystem>() {\n          @Override\n          public FileSystem run() throws IOException {\n            return FileSystem.get(conf);\n          }\n        });\n    this.emptier = new Thread(new Trash(fs, conf).getEmptier(), \"Trash Emptier\");\n    this.emptier.setDaemon(true);\n    this.emptier.start();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.doImmediateShutdown": "  protected synchronized void doImmediateShutdown(Throwable t)\n      throws ExitException {\n    String message = \"Error encountered requiring NN shutdown. \" +\n        \"Shutting down immediately.\";\n    try {\n      LOG.error(message, t);\n    } catch (Throwable ignored) {\n      // This is unlikely to happen, but there's nothing we can do if it does.\n    }\n    terminate(1, t);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState": "  public void enterState(HAContext context) throws ServiceFailedException {\n    try {\n      context.startActiveServices();\n    } catch (IOException e) {\n      throw new ServiceFailedException(\"Failed to start active services\", e);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal": "  protected final void setStateInternal(final HAContext context, final HAState s)\n      throws ServiceFailedException {\n    prepareToExitState(context);\n    s.prepareToEnterState(context);\n    context.writeLock();\n    try {\n      exitState(context);\n      context.setState(s);\n      s.enterState(context);\n      s.updateLastHATransitionTime();\n    } finally {\n      context.writeUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.exitState": "  public abstract void exitState(final HAContext context)\n      throws ServiceFailedException;\n\n  /**\n   * Move from the existing state to a new state\n   * @param context HA context\n   * @param s new state\n   * @throws ServiceFailedException on failure to transition to new state.\n   */\n  public void setState(HAContext context, HAState s) throws ServiceFailedException {\n    if (this == s) { // Aleady in the new state\n      return;\n    }\n    throw new ServiceFailedException(\"Transtion from state \" + this + \" to \"\n        + s + \" is not allowed.\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.setState": "  public void setState(HAContext context, HAState s) throws ServiceFailedException {\n    if (this == s) { // Aleady in the new state\n      return;\n    }\n    throw new ServiceFailedException(\"Transtion from state \" + this + \" to \"\n        + s + \" is not allowed.\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.updateLastHATransitionTime": "  private void updateLastHATransitionTime() {\n    lastHATransitionTime = Time.now();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.prepareToExitState": "  public void prepareToExitState(final HAContext context)\n      throws ServiceFailedException {}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.enterState": "  public abstract void enterState(final HAContext context)\n      throws ServiceFailedException;\n\n  /**\n   * Method to be overridden by subclasses to prepare to exit a state.\n   * This method is called <em>without</em> the context being locked.\n   * This is used by the standby state to cancel any checkpoints\n   * that are going on. It can also be used to check any preconditions\n   * for the state transition.\n   * \n   * This method should not make any destructuve changes to the state\n   * (eg stopping threads) since {@link #prepareToEnterState(HAContext)}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.HAState.prepareToEnterState": "  public void prepareToEnterState(final HAContext context)\n      throws ServiceFailedException {}",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState": "  public void setState(HAContext context, HAState s) throws ServiceFailedException {\n    if (s == NameNode.ACTIVE_STATE) {\n      setStateInternal(context, s);\n      return;\n    }\n    super.setState(context, s);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive": "  synchronized void transitionToActive() \n      throws ServiceFailedException, AccessControlException {\n    namesystem.checkSuperuserPrivilege();\n    if (!haEnabled) {\n      throw new ServiceFailedException(\"HA for namenode is not enabled\");\n    }\n    state.setState(haContext, ACTIVE_STATE);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.setState": "    public void setState(HAState s) {\n      state = s;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive": "  public synchronized void transitionToActive(StateChangeRequestInfo req) \n      throws ServiceFailedException, AccessControlException, IOException {\n    checkNNStartup();\n    nn.checkHaStateChange(req);\n    nn.transitionToActive();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkNNStartup": "  private void checkNNStartup() throws IOException {\n    if (!this.nn.isStarted()) {\n      String message = NameNode.composeNotStartedMessage(this.nn.getRole());\n      throw new RetriableException(message);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive": "  public TransitionToActiveResponseProto transitionToActive(\n      RpcController controller, TransitionToActiveRequestProto request)\n      throws ServiceException {\n    try {\n      server.transitionToActive(convert(request.getReqInfo()));\n      return TRANSITION_TO_ACTIVE_RESP;\n    } catch(IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.convert": "  private StateChangeRequestInfo convert(HAStateChangeRequestInfoProto proto) {\n    RequestSource src;\n    switch (proto.getReqSource()) {\n    case REQUEST_BY_USER:\n      src = RequestSource.REQUEST_BY_USER;\n      break;\n    case REQUEST_BY_USER_FORCED:\n      src = RequestSource.REQUEST_BY_USER_FORCED;\n      break;\n    case REQUEST_BY_ZKFC:\n      src = RequestSource.REQUEST_BY_ZKFC;\n      break;\n    default:\n      LOG.warn(\"Unknown request source: \" + proto.getReqSource());\n      src = null;\n    }\n    \n    return new StateChangeRequestInfo(src);\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.call": "      public Writable call(RPC.Server server, String connectionProtocolName,\n          Writable writableRequest, long receiveTime) throws Exception {\n        RpcProtobufRequest request = (RpcProtobufRequest) writableRequest;\n        RequestHeaderProto rpcRequest = request.getRequestHeader();\n        String methodName = rpcRequest.getMethodName();\n\n        /** \n         * RPCs for a particular interface (ie protocol) are done using a\n         * IPC connection that is setup using rpcProxy.\n         * The rpcProxy's has a declared protocol name that is \n         * sent form client to server at connection time. \n         * \n         * Each Rpc call also sends a protocol name \n         * (called declaringClassprotocolName). This name is usually the same\n         * as the connection protocol name except in some cases. \n         * For example metaProtocols such ProtocolInfoProto which get info\n         * about the protocol reuse the connection but need to indicate that\n         * the actual protocol is different (i.e. the protocol is\n         * ProtocolInfoProto) since they reuse the connection; in this case\n         * the declaringClassProtocolName field is set to the ProtocolInfoProto.\n         */\n\n        String declaringClassProtoName = \n            rpcRequest.getDeclaringClassProtocolName();\n        long clientVersion = rpcRequest.getClientProtocolVersion();\n        if (server.verbose)\n          LOG.info(\"Call: connectionProtocolName=\" + connectionProtocolName + \n              \", method=\" + methodName);\n        \n        ProtoClassProtoImpl protocolImpl = getProtocolImpl(server, \n                              declaringClassProtoName, clientVersion);\n        BlockingService service = (BlockingService) protocolImpl.protocolImpl;\n        MethodDescriptor methodDescriptor = service.getDescriptorForType()\n            .findMethodByName(methodName);\n        if (methodDescriptor == null) {\n          String msg = \"Unknown method \" + methodName + \" called on \" \n                                + connectionProtocolName + \" protocol.\";\n          LOG.warn(msg);\n          throw new RpcNoSuchMethodException(msg);\n        }\n        Message prototype = service.getRequestPrototype(methodDescriptor);\n        Message param = request.getValue(prototype);\n\n        Message result;\n        long startTime = Time.now();\n        int qTime = (int) (startTime - receiveTime);\n        Exception exception = null;\n        boolean isDeferred = false;\n        try {\n          server.rpcDetailedMetrics.init(protocolImpl.protocolClass);\n          currentCallInfo.set(new CallInfo(server, methodName));\n          result = service.callBlockingMethod(methodDescriptor, null, param);\n          // Check if this needs to be a deferred response,\n          // by checking the ThreadLocal callback being set\n          if (currentCallback.get() != null) {\n            Server.getCurCall().get().deferResponse();\n            isDeferred = true;\n            currentCallback.set(null);\n            return null;\n          }\n        } catch (ServiceException e) {\n          exception = (Exception) e.getCause();\n          throw (Exception) e.getCause();\n        } catch (Exception e) {\n          exception = e;\n          throw e;\n        } finally {\n          currentCallInfo.set(null);\n          int processingTime = (int) (Time.now() - startTime);\n          if (LOG.isDebugEnabled()) {\n            String msg =\n                \"Served: \" + methodName + (isDeferred ? \", deferred\" : \"\") +\n                    \", queueTime= \" + qTime +\n                    \" procesingTime= \" + processingTime;\n            if (exception != null) {\n              msg += \" exception= \" + exception.getClass().getSimpleName();\n            }\n            LOG.debug(msg);\n          }\n          String detailedMetricsName = (exception == null) ?\n              methodName :\n              exception.getClass().getSimpleName();\n          server.updateMetrics(detailedMetricsName, qTime, processingTime,\n              isDeferred);\n        }\n        return RpcWritable.wrap(result);\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.get": "          public Message get(long timeout, TimeUnit unit) throws Exception {\n            return getReturnMessage(method, arr.get(timeout, unit));\n          }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getRequestHeader": "    RequestHeaderProto getRequestHeader() throws IOException {\n      if (getByteBuffer() != null && requestHeader == null) {\n        requestHeader = getValue(RequestHeaderProto.getDefaultInstance());\n      }\n      return requestHeader;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.getProtocolImpl": "      private static ProtoClassProtoImpl getProtocolImpl(RPC.Server server,\n          String protoName, long clientVersion) throws RpcServerException {\n        ProtoNameVer pv = new ProtoNameVer(protoName, clientVersion);\n        ProtoClassProtoImpl impl = \n            server.getProtocolImplMap(RPC.RpcKind.RPC_PROTOCOL_BUFFER).get(pv);\n        if (impl == null) { // no match for Protocol AND Version\n          VerProtocolImpl highest = \n              server.getHighestSupportedProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER, \n                  protoName);\n          if (highest == null) {\n            throw new RpcNoSuchProtocolException(\n                \"Unknown protocol: \" + protoName);\n          }\n          // protocol supported but not the version that client wants\n          throw new RPC.VersionMismatch(protoName, clientVersion,\n              highest.version);\n        }\n        return impl;\n      }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RPC.call": "    public Writable call(RPC.RpcKind rpcKind, String protocol,\n        Writable rpcRequest, long receiveTime) throws Exception {\n      return getRpcInvoker(rpcKind).call(this, protocol, rpcRequest,\n          receiveTime);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.run": "        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.setupResponse": "  private void setupResponse(RpcCall call,\n      RpcResponseHeaderProto header, Writable rv) throws IOException {\n    final byte[] response;\n    if (rv == null || (rv instanceof RpcWritable.ProtobufWrapper)) {\n      response = setupResponseForProtobuf(header, rv);\n    } else {\n      response = setupResponseForWritable(header, rv);\n    }\n    if (response.length > maxRespSize) {\n      LOG.warn(\"Large response size \" + response.length + \" for call \"\n          + call.toString());\n    }\n    call.setResponse(ByteBuffer.wrap(response));\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.isResponseDeferred": "    public boolean isResponseDeferred() {\n      return this.deferredResponse;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.remove": "    private boolean remove(Connection connection) {\n      boolean removed = connections.remove(connection);\n      if (removed) {\n        count.getAndDecrement();\n      }\n      return removed;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeAll": "    void closeAll() {\n      // use a copy of the connections to be absolutely sure the concurrent\n      // iterator doesn't miss a connection\n      for (Connection connection : toArray()) {\n        close(connection);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.call": "  public abstract Writable call(RPC.RpcKind rpcKind, String protocol,\n      Writable param, long receiveTime) throws Exception;\n  \n  /**\n   * Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param protocolName - the protocol\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol\n   */\n  private void authorize(UserGroupInformation user, String protocolName,\n      InetAddress addr) throws AuthorizationException {\n    if (authorize) {\n      if (protocolName == null) {\n        throw new AuthorizationException(\"Null protocol not authorized\");\n      }\n      Class<?> protocol = null;\n      try {\n        protocol = getProtocolClass(protocolName, getConf());\n      } catch (ClassNotFoundException cfne) {\n        throw new AuthorizationException(\"Unknown protocol: \" + \n                                         protocolName);\n      }\n      serviceAuthorizationManager.authorize(user, protocol, getConf(), addr);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.close": "    boolean close(Connection connection) {\n      boolean exists = remove(connection);\n      if (exists) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(Thread.currentThread().getName() +\n              \": disconnecting client \" + connection +\n              \". Number of active connections: \"+ size());\n        }\n        // only close if actually removed to avoid double-closing due\n        // to possible races\n        connection.close();\n        // Remove authorized users only\n        if (connection.user != null && connection.connectionContextRead) {\n          decrUserConnections(connection.user.getShortUserName());\n        }\n      }\n      return exists;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doRunLoop": "    private void doRunLoop() {\n      long lastPurgeTime = 0;   // last check for old calls.\n\n      while (running) {\n        try {\n          waitPending();     // If a channel is being registered, wait.\n          writeSelector.select(PURGE_INTERVAL);\n          Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();\n          while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            try {\n              if (key.isWritable()) {\n                doAsyncWrite(key);\n              }\n            } catch (CancelledKeyException cke) {\n              // something else closed the connection, ex. reader or the\n              // listener doing an idle scan.  ignore it and let them clean\n              // up\n              RpcCall call = (RpcCall)key.attachment();\n              if (call != null) {\n                LOG.info(Thread.currentThread().getName() +\n                    \": connection aborted from \" + call.connection);\n              }\n            } catch (IOException e) {\n              LOG.info(Thread.currentThread().getName() + \": doAsyncWrite threw exception \" + e);\n            }\n          }\n          long now = Time.now();\n          if (now < lastPurgeTime + PURGE_INTERVAL) {\n            continue;\n          }\n          lastPurgeTime = now;\n          //\n          // If there were some calls that have not been sent out for a\n          // long time, discard them.\n          //\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Checking for old call responses.\");\n          }\n          ArrayList<RpcCall> calls;\n          \n          // get the list of channels from list of keys.\n          synchronized (writeSelector.keys()) {\n            calls = new ArrayList<RpcCall>(writeSelector.keys().size());\n            iter = writeSelector.keys().iterator();\n            while (iter.hasNext()) {\n              SelectionKey key = iter.next();\n              RpcCall call = (RpcCall)key.attachment();\n              if (call != null && key.channel() == call.connection.channel) { \n                calls.add(call);\n              }\n            }\n          }\n\n          for (RpcCall call : calls) {\n            doPurge(call, now);\n          }\n        } catch (OutOfMemoryError e) {\n          //\n          // we can run out of memory if we have too many threads\n          // log the event and sleep for a minute and give\n          // some thread(s) a chance to finish\n          //\n          LOG.warn(\"Out of Memory in server select\", e);\n          try { Thread.sleep(60000); } catch (Exception ie) {}\n        } catch (Exception e) {\n          LOG.warn(\"Exception in Responder\", e);\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeIdle": "    synchronized void closeIdle(boolean scanAll) {\n      long minLastContact = Time.now() - maxIdleTime;\n      // concurrent iterator might miss new connections added\n      // during the iteration, but that's ok because they won't\n      // be idle yet anyway and will be caught on next scan\n      int closed = 0;\n      for (Connection connection : connections) {\n        // stop if connections dropped below threshold unless scanning all\n        if (!scanAll && size() < idleScanThreshold) {\n          break;\n        }\n        // stop if not scanning all and max connections are closed\n        if (connection.isIdle() &&\n            connection.getLastContact() < minLastContact &&\n            close(connection) &&\n            !scanAll && (++closed == maxIdleToClose)) {\n          break;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.startIdleScan": "    void startIdleScan() {\n      scheduleIdleScanTask();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.closeCurrentConnection": "    private void closeCurrentConnection(SelectionKey key, Throwable e) {\n      if (key != null) {\n        Connection c = (Connection)key.attachment();\n        if (c != null) {\n          closeConnection(c);\n          c = null;\n        }\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.sendResponse": "    private void sendResponse(RpcCall call) throws IOException {\n      responder.doRespond(call);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.stopIdleScan": "    void stopIdleScan() {\n      idleScanTimer.cancel();\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getSelector": "    synchronized Selector getSelector() { return selector; }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.doAccept": "    void doAccept(SelectionKey key) throws InterruptedException, IOException,  OutOfMemoryError {\n      ServerSocketChannel server = (ServerSocketChannel) key.channel();\n      SocketChannel channel;\n      while ((channel = server.accept()) != null) {\n\n        channel.configureBlocking(false);\n        channel.socket().setTcpNoDelay(tcpNoDelay);\n        channel.socket().setKeepAlive(true);\n        \n        Reader reader = getReader();\n        Connection c = connectionManager.register(channel);\n        // If the connectionManager can't take it, close the connection.\n        if (c == null) {\n          if (channel.isOpen()) {\n            IOUtils.cleanup(null, channel);\n          }\n          connectionManager.droppedConnections.getAndIncrement();\n          continue;\n        }\n        key.attach(c);  // so closeCurrentConnection can get the object\n        reader.addConnection(c);\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.scheduleIdleScanTask": "    private void scheduleIdleScanTask() {\n      if (!running) {\n        return;\n      }\n      TimerTask idleScanTask = new TimerTask(){\n        @Override\n        public void run() {\n          if (!running) {\n            return;\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(Thread.currentThread().getName()+\": task running\");\n          }\n          try {\n            closeIdle(false);\n          } finally {\n            // explicitly reschedule so next execution occurs relative\n            // to the end of this scan, not the beginning\n            scheduleIdleScanTask();\n          }\n        }\n      };\n      idleScanTimer.schedule(idleScanTask, idleScanInterval);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getRemoteUser": "    public UserGroupInformation getRemoteUser() {\n      return connection.user;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.populateResponseParamsOnError": "    private void populateResponseParamsOnError(Throwable t,\n                                               ResponseParams responseParams) {\n      if (t instanceof UndeclaredThrowableException) {\n        t = t.getCause();\n      }\n      logException(Server.LOG, t, this);\n      if (t instanceof RpcServerException) {\n        RpcServerException rse = ((RpcServerException) t);\n        responseParams.returnStatus = rse.getRpcStatusProto();\n        responseParams.detailedErr = rse.getRpcErrorCodeProto();\n      } else {\n        responseParams.returnStatus = RpcStatusProto.ERROR;\n        responseParams.detailedErr = RpcErrorCodeProto.ERROR_APPLICATION;\n      }\n      responseParams.errorClass = t.getClass().getName();\n      responseParams.error = StringUtils.stringifyException(t);\n      // Remove redundant error class name from the beginning of the\n      // stack trace\n      String exceptionHdr = responseParams.errorClass + \": \";\n      if (responseParams.error.startsWith(exceptionHdr)) {\n        responseParams.error =\n            responseParams.error.substring(exceptionHdr.length());\n      }\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PrivilegedActionException as:\" + this + \" cause:\" + cause);\n      }\n      if (cause == null) {\n        throw new RuntimeException(\"PrivilegedActionException with no \" +\n                \"underlying cause. UGI [\" + this + \"]\" +\": \" + pae, pae);\n      } else if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction": "  private void logPrivilegedAction(Subject subject, Object action) {\n    if (LOG.isDebugEnabled()) {\n      // would be nice if action included a descriptive toString()\n      String where = new Throwable().getStackTrace()[2].toString();\n      LOG.debug(\"PrivilegedAction as:\"+this+\" from:\"+where);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.recoverUnclosedStreams": "  synchronized void recoverUnclosedStreams() {\n    Preconditions.checkState(\n        state == State.BETWEEN_LOG_SEGMENTS,\n        \"May not recover segments - wrong state: %s\", state);\n    try {\n      journalSet.recoverUnfinalizedSegments();\n    } catch (IOException ex) {\n      // All journals have failed, it is handled in logSync.\n      // TODO: are we sure this is OK?\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.cacheManager.startMonitorThread": "  public void startMonitorThread() {\n    crmLock.lock();\n    try {\n      if (this.monitor == null) {\n        this.monitor = new CacheReplicationMonitor(namesystem, this,\n            scanIntervalMs, crmLock);\n        this.monitor.start();\n      }\n    } finally {\n      crmLock.unlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.leaseManager.startMonitor": "  void startMonitor() {\n    Preconditions.checkState(lmthread == null,\n        \"Lease Monitor already running\");\n    shouldRunMonitor = true;\n    lmthread = new Daemon(new Monitor());\n    lmthread.start();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.initJournalsForWrite": "  public synchronized void initJournalsForWrite() {\n    Preconditions.checkState(state == State.UNINITIALIZED ||\n        state == State.CLOSED, \"Unexpected state: %s\", state);\n    \n    initJournals(this.editsDirs);\n    state = State.BETWEEN_LOG_SEGMENTS;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.initJournals": "  private synchronized void initJournals(List<URI> dirs) {\n    int minimumRedundantJournals = conf.getInt(\n        DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_MINIMUM_KEY,\n        DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_MINIMUM_DEFAULT);\n\n    synchronized(journalSetLock) {\n      journalSet = new JournalSet(minimumRedundantJournals);\n\n      for (URI u : dirs) {\n        boolean required = FSNamesystem.getRequiredNamespaceEditsDirs(conf)\n            .contains(u);\n        if (u.getScheme().equals(NNStorage.LOCAL_URI_SCHEME)) {\n          StorageDirectory sd = storage.getStorageDirectory(u);\n          if (sd != null) {\n            journalSet.add(new FileJournalManager(conf, sd, storage),\n                required, sharedEditsDirs.contains(u));\n          }\n        } else {\n          journalSet.add(createJournal(u), required,\n              sharedEditsDirs.contains(u));\n        }\n      }\n    }\n \n    if (journalSet.isEmpty()) {\n      LOG.error(\"No edits directories configured!\");\n    } \n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.setNextTxId": "  synchronized void setNextTxId(long nextTxId) {\n    Preconditions.checkArgument(synctxid <= txid &&\n       nextTxId >= txid,\n       \"May not decrease txid.\" +\n      \" synctxid=%s txid=%s nextTxId=%s\",\n      synctxid, txid, nextTxId);\n      \n    txid = nextTxId - 1;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp.warmUpEdekCache": "  static void warmUpEdekCache(final ExecutorService executor,\n      final FSDirectory fsd, final int delay, final int interval) {\n    fsd.readLock();\n    try {\n      String[] edeks  = fsd.ezManager.getKeyNames();\n      executor.execute(\n          new EDEKCacheLoader(edeks, fsd.getProvider(), delay, interval));\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLog.isOpenForWrite": "  synchronized boolean isOpenForWrite() {\n    return state == State.IN_SEGMENT ||\n      state == State.BETWEEN_LOG_SEGMENTS;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.leaseManager.renewAllLeases": "  synchronized void renewAllLeases() {\n    for (Lease l : leases.values()) {\n      renewLease(l);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.leaseManager.renewLease": "  synchronized void renewLease(Lease lease) {\n    if (lease != null) {\n      sortedLeases.remove(lease);\n      lease.renew();\n      sortedLeases.add(lease);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.getCurCall": "  public static ThreadLocal<Call> getCurCall() {\n    return CurCall;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.RpcWritable.wrap": "    public static Buffer wrap(ByteBuffer bb) {\n      return new Buffer(bb);\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.updateMetrics": "  void updateMetrics(String name, int queueTime, int processingTime,\n                     boolean deferredCall) {\n    rpcMetrics.addRpcQueueTime(queueTime);\n    if (!deferredCall) {\n      rpcMetrics.addRpcProcessingTime(processingTime);\n      rpcDetailedMetrics.addProcessingTime(name, processingTime);\n      callQueue.addResponseTime(name, getPriorityLevel(), queueTime,\n          processingTime);\n      if (isLogSlowRPC()) {\n        logSlowRpcCalls(name, processingTime);\n      }\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.logSlowRpcCalls": "  void logSlowRpcCalls(String methodName, int processingTime) {\n    final int deviation = 3;\n\n    // 1024 for minSampleSize just a guess -- not a number computed based on\n    // sample size analysis. It is chosen with the hope that this\n    // number is high enough to avoid spurious logging, yet useful\n    // in practice.\n    final int minSampleSize = 1024;\n    final double threeSigma = rpcMetrics.getProcessingMean() +\n        (rpcMetrics.getProcessingStdDev() * deviation);\n\n    if ((rpcMetrics.getProcessingSampleCount() > minSampleSize) &&\n        (processingTime > threeSigma)) {\n      if(LOG.isWarnEnabled()) {\n        String client = CurCall.get().toString();\n        LOG.warn(\n            \"Slow RPC : \" + methodName + \" took \" + processingTime +\n                \" milliseconds to process from client \" + client);\n      }\n      rpcMetrics.incrSlowRpc();\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.getPriorityLevel": "    public int getPriorityLevel() {\n      return this.priorityLevel;\n    }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.server.isLogSlowRPC": "  protected boolean isLogSlowRPC() {\n    return logSlowRPC;\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.CallerContext.setCurrent": "  public static void setCurrent(CallerContext callerContext) {\n    CurrentCallerContextHolder.CALLER_CONTEXT.set(callerContext);\n  }"
        },
        "bug_report": {
            "Title": "SBN crash when transition to ANN with in-progress edit tailing enabled",
            "Description": "With edit log in-progress edit log tailing enabled, {{QuorumOutputStream}} will send two batches to JNs, one normal edit batch followed by a dummy batch to update the commit ID on JNs.\r\n\r\n{code}\r\n      QuorumCall<AsyncLogger, Void> qcall = loggers.sendEdits(\r\n          segmentTxId, firstTxToFlush,\r\n          numReadyTxns, data);\r\n      loggers.waitForWriteQuorum(qcall, writeTimeoutMs, \"sendEdits\");\r\n      \r\n      // Since we successfully wrote this batch, let the loggers know. Any future\r\n      // RPCs will thus let the loggers know of the most recent transaction, even\r\n      // if a logger has fallen behind.\r\n      loggers.setCommittedTxId(firstTxToFlush + numReadyTxns - 1);\r\n\r\n      // If we don't have this dummy send, committed TxId might be one-batch\r\n      // stale on the Journal Nodes\r\n      if (updateCommittedTxId) {\r\n        QuorumCall<AsyncLogger, Void> fakeCall = loggers.sendEdits(\r\n            segmentTxId, firstTxToFlush,\r\n            0, new byte[0]);\r\n        loggers.waitForWriteQuorum(fakeCall, writeTimeoutMs, \"sendEdits\");\r\n      }\r\n{code}\r\n\r\nBetween each batch, it will wait for the JNs to reach a quorum. However, if the ANN crashes in between, then SBN will crash while transiting to ANN:\r\n\r\n{code}\r\njava.lang.IllegalStateException: Cannot start writing at txid 24312595802 when there is a stream available for read: ......\r\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.openForWrite(FSEditLog.java:329)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1196)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1839)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1707)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1622)\r\n        at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)\r\n        at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:4460)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\r\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\r\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:851)\r\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:794)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)\r\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2490)\r\n2018-02-13 00:43:20,728 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1\r\n{code}\r\n\r\nThis is because without the dummy batch, the {{commitTxnId}} will lag behind the {{endTxId}}, which caused the check in {{openForWrite}} to fail:\r\n{code}\r\n    List<EditLogInputStream> streams = new ArrayList<EditLogInputStream>();\r\n    journalSet.selectInputStreams(streams, segmentTxId, true, false);\r\n    if (!streams.isEmpty()) {\r\n      String error = String.format(\"Cannot start writing at txid %s \" +\r\n        \"when there is a stream available for read: %s\",\r\n        segmentTxId, streams.get(0));\r\n      IOUtils.cleanupWithLogger(LOG,\r\n          streams.toArray(new EditLogInputStream[0]));\r\n      throw new IllegalStateException(error);\r\n    }\r\n{code}\r\n\r\nIn our environment, this can be reproduced pretty consistently, which will leave the cluster with no running namenodes. Even though we are using a 2.8.2 backport, I believe the same issue also exist in 3.0.x. "
        }
    },
    {
        "filename": "HDFS-8807.json",
        "creation_time": "2015-07-22T20:45:55.000+0000",
        "stack_trace": "```\njava.lang.IllegalArgumentException: java.net.URISyntaxException: Illegal character in scheme name at index 0:  file://tmp/hadoop-aengineer/disk1/dfs/data\n        at org.apache.hadoop.fs.Path.initialize(Path.java:204)\n        at org.apache.hadoop.fs.Path.<init>(Path.java:170)\n        at org.apache.hadoop.hdfs.server.datanode.StorageLocation.parse(StorageLocation.java:97)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.getStorageLocations(DataNode.java:2314)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2298)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2349)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2529)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2553)\nCaused by: java.net.URISyntaxException: Illegal character in scheme name at index 0:  file://tmp/hadoop-aengineer/disk1/dfs/data\n        at java.net.URI$Parser.fail(URI.java:2829)\n        at java.net.URI$Parser.checkChars(URI.java:3002)\n        at java.net.URI$Parser.checkChar(URI.java:3012)\n        at java.net.URI$Parser.parse(URI.java:3028)\n        at java.net.URI.<init>(URI.java:753)\n        at org.apache.hadoop.fs.Path.initialize(Path.java:201)```",
        "source_code": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Path.initialize": "  private void initialize(String scheme, String authority, String path,\n      String fragment) {\n    try {\n      this.uri = new URI(scheme, authority, normalizePath(scheme, path), null, fragment)\n        .normalize();\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n  }",
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.Path.normalizePath": "  private static String normalizePath(String scheme, String path) {\n    // Remove double forward slashes.\n    path = StringUtils.replace(path, \"//\", \"/\");\n\n    // Remove backslashes if this looks like a Windows path. Avoid\n    // the substitution if it looks like a non-local URI.\n    if (WINDOWS &&\n        (hasWindowsDrive(path) ||\n         (scheme == null) ||\n         (scheme.isEmpty()) ||\n         (scheme.equals(\"file\")))) {\n      path = StringUtils.replace(path, \"\\\\\", \"/\");\n    }\n    \n    // trim trailing slash from non-root path (ignoring windows drive)\n    int minLength = startPositionWithoutWindowsDrive(path) + 1;\n    if (path.length() > minLength && path.endsWith(SEPARATOR)) {\n      path = path.substring(0, path.length()-1);\n    }\n    \n    return path;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.StorageLocation.parse": "  public static StorageLocation parse(String rawLocation)\n      throws IOException, SecurityException {\n    Matcher matcher = regex.matcher(rawLocation);\n    StorageType storageType = StorageType.DEFAULT;\n    String location = rawLocation;\n\n    if (matcher.matches()) {\n      String classString = matcher.group(1);\n      location = matcher.group(2);\n      if (!classString.isEmpty()) {\n        storageType =\n            StorageType.valueOf(StringUtils.toUpperCase(classString));\n      }\n    }\n\n    return new StorageLocation(storageType, new Path(location).toUri());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getStorageLocations": "  public static List<StorageLocation> getStorageLocations(Configuration conf) {\n    Collection<String> rawLocations =\n        conf.getTrimmedStringCollection(DFS_DATANODE_DATA_DIR_KEY);\n    List<StorageLocation> locations =\n        new ArrayList<StorageLocation>(rawLocations.size());\n\n    for(String locationString : rawLocations) {\n      final StorageLocation location;\n      try {\n        location = StorageLocation.parse(locationString);\n      } catch (IOException ioe) {\n        LOG.error(\"Failed to initialize storage directory \" + locationString\n            + \". Exception details: \" + ioe);\n        // Ignore the exception.\n        continue;\n      } catch (SecurityException se) {\n        LOG.error(\"Failed to initialize storage directory \" + locationString\n                     + \". Exception details: \" + se);\n        // Ignore the exception.\n        continue;\n      }\n\n      locations.add(location);\n    }\n\n    return locations;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode": "  public static DataNode instantiateDataNode(String args [], Configuration conf,\n      SecureResources resources) throws IOException {\n    if (conf == null)\n      conf = new HdfsConfiguration();\n    \n    if (args != null) {\n      // parse generic hadoop options\n      GenericOptionsParser hParser = new GenericOptionsParser(conf, args);\n      args = hParser.getRemainingArgs();\n    }\n    \n    if (!parseArguments(args, conf)) {\n      printUsage(System.err);\n      return null;\n    }\n    Collection<StorageLocation> dataLocations = getStorageLocations(conf);\n    UserGroupInformation.setConfiguration(conf);\n    SecurityUtil.login(conf, DFS_DATANODE_KEYTAB_FILE_KEY,\n        DFS_DATANODE_KERBEROS_PRINCIPAL_KEY);\n    return makeInstance(dataLocations, conf, resources);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance": "  static DataNode makeInstance(Collection<StorageLocation> dataDirs,\n      Configuration conf, SecureResources resources) throws IOException {\n    LocalFileSystem localFS = FileSystem.getLocal(conf);\n    FsPermission permission = new FsPermission(\n        conf.get(DFS_DATANODE_DATA_DIR_PERMISSION_KEY,\n                 DFS_DATANODE_DATA_DIR_PERMISSION_DEFAULT));\n    DataNodeDiskChecker dataNodeDiskChecker =\n        new DataNodeDiskChecker(permission);\n    List<StorageLocation> locations =\n        checkStorageLocations(dataDirs, localFS, dataNodeDiskChecker);\n    DefaultMetricsSystem.initialize(\"DataNode\");\n\n    assert locations.size() > 0 : \"number of data directories should be > 0\";\n    return new DataNode(conf, locations, resources);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.printUsage": "  private static void printUsage(PrintStream out) {\n    out.println(USAGE + \"\\n\");\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.parseArguments": "  static boolean parseArguments(String args[], Configuration conf) {\n    StartupOption startOpt = StartupOption.REGULAR;\n    int i = 0;\n\n    if (args != null && args.length != 0) {\n      String cmd = args[i++];\n      if (\"-r\".equalsIgnoreCase(cmd) || \"--rack\".equalsIgnoreCase(cmd)) {\n        LOG.error(\"-r, --rack arguments are not supported anymore. RackID \" +\n            \"resolution is handled by the NameNode.\");\n        return false;\n      } else if (StartupOption.ROLLBACK.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.ROLLBACK;\n      } else if (StartupOption.REGULAR.getName().equalsIgnoreCase(cmd)) {\n        startOpt = StartupOption.REGULAR;\n      } else {\n        return false;\n      }\n    }\n\n    setStartupOption(conf, startOpt);\n    return (args == null || i == args.length);    // Fail if more than one cmd specified!\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode": "  public static DataNode createDataNode(String args[], Configuration conf,\n      SecureResources resources) throws IOException {\n    DataNode dn = instantiateDataNode(args, conf, resources);\n    if (dn != null) {\n      dn.runDatanodeDaemon();\n    }\n    return dn;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.runDatanodeDaemon": "  public void runDatanodeDaemon() throws IOException {\n    blockPoolManager.startAll();\n\n    // start dataXceiveServer\n    dataXceiverServer.start();\n    if (localDataXceiverServer != null) {\n      localDataXceiverServer.start();\n    }\n    ipcServer.start();\n    startPlugins(conf);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain": "  public static void secureMain(String args[], SecureResources resources) {\n    int errorCode = 0;\n    try {\n      StringUtils.startupShutdownMessage(DataNode.class, args, LOG);\n      DataNode datanode = createDataNode(args, null, resources);\n      if (datanode != null) {\n        datanode.join();\n      } else {\n        errorCode = 1;\n      }\n    } catch (Throwable e) {\n      LOG.fatal(\"Exception in secureMain\", e);\n      terminate(1, e);\n    } finally {\n      // We need to terminate the process here because either shutdown was called\n      // or some disk related conditions like volumes tolerated or volumes required\n      // condition was not met. Also, In secure mode, control will go to Jsvc\n      // and Datanode process hangs if it does not exit.\n      LOG.warn(\"Exiting Datanode\");\n      terminate(errorCode);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.join": "  void join() {\n    while (shouldRun) {\n      try {\n        blockPoolManager.joinAll();\n        if (blockPoolManager.getAllNamenodeThreads().size() == 0) {\n          shouldRun = false;\n        }\n        // Terminate if shutdown is complete or 2 seconds after all BPs\n        // are shutdown.\n        synchronized(this) {\n          wait(2000);\n        }\n      } catch (InterruptedException ex) {\n        LOG.warn(\"Received exception in Datanode#join: \" + ex);\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.main": "  public static void main(String args[]) {\n    if (DFSUtil.parseHelpArgument(args, DataNode.USAGE, System.out, true)) {\n      System.exit(0);\n    }\n\n    secureMain(args, null);\n  }"
        },
        "bug_report": {
            "Title": "dfs.datanode.data.dir does not handle spaces between storageType and URI correctly",
            "Description": "if you add a space between the storage type and file URI then datanodes fail during startup.\nHere is an example of \"mis-configration\" that leads to datanode failure.\n{code}\n<property>\n    <name>dfs.datanode.data.dir</name>\n    <value>\n      [DISK] file://tmp/hadoop-aengineer/disk1/dfs/data\n    </value>\n  </property>\n{code}\nHere is the \"fixed\" version. Please *note* the lack of space between \\[DISK\\] and file URI\n{code}\n<property>\n    <name>dfs.datanode.data.dir</name>\n    <value>\n      [DISK]file://tmp/hadoop-aengineer/disk1/dfs/data\n    </value>\n  </property>\n{code}\nwe fail with a parsing error, here is the info from the datanode logs.\n{code}\n2015-07-22 13:34:36,812 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain\njava.lang.IllegalArgumentException: java.net.URISyntaxException: Illegal character in scheme name at index 0:  file://tmp/hadoop-aengineer/disk1/dfs/data\n        at org.apache.hadoop.fs.Path.initialize(Path.java:204)\n        at org.apache.hadoop.fs.Path.<init>(Path.java:170)\n        at org.apache.hadoop.hdfs.server.datanode.StorageLocation.parse(StorageLocation.java:97)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.getStorageLocations(DataNode.java:2314)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2298)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2349)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2529)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2553)\nCaused by: java.net.URISyntaxException: Illegal character in scheme name at index 0:  file://tmp/hadoop-aengineer/disk1/dfs/data\n        at java.net.URI$Parser.fail(URI.java:2829)\n        at java.net.URI$Parser.checkChars(URI.java:3002)\n        at java.net.URI$Parser.checkChar(URI.java:3012)\n        at java.net.URI$Parser.parse(URI.java:3028)\n        at java.net.URI.<init>(URI.java:753)\n        at org.apache.hadoop.fs.Path.initialize(Path.java:201)\n        ... 7 more\n{code}"
        }
    },
    {
        "filename": "HDFS-3436.json",
        "creation_time": "2012-05-17T10:36:31.000+0000",
        "stack_trace": "```\njava.io.IOException: Bad connect ack with firstBadLink as *******:50010\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1053)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:943)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n\njava.io.EOFException: Premature EOF: no length prefix available\n\tat org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n\njava.io.EOFException: Premature EOF: no length prefix available\n\tat org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n\njava.io.EOFException: Premature EOF: no length prefix available\n\tat org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n\njava.io.IOException: BP-2001850558-xx.xx.xx.xx-1337249347060:blk_-8165642083860293107_1002 is neither a RBW nor a Finalized, r=ReplicaBeingWritten, blk_-8165642083860293107_1003, RBW\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.transferReplicaForPipelineRecovery(DataNode.java:2038)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.transferBlock(DataXceiver.java:525)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opTransferBlock(Receiver.java:114)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:78)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:189)\n\tat java.lang.Thread.run(Unknown Source)\n```",
        "source_code": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.createBlockOutputStream": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      Status pipelineStatus = SUCCESS;\n      String firstBadLink = \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i = 0; i < nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline = \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      boolean result = false;\n      DataOutputStream out = null;\n      try {\n        assert null == s : \"Previous socket unclosed\";\n        s = createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout = dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        //\n        // Xmit header info to datanode\n        //\n        out = new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(s, writeTimeout),\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        \n        assert null == blockReplyStream : \"Previous blockReplyStream unclosed\";\n        blockReplyStream = new DataInputStream(NetUtils.getInputStream(s));\n\n        // send the request\n        new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n            nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n            nodes.length, block.getNumBytes(), bytesSent, newGS, checksum);\n\n        // receive ack for connect\n        BlockOpResponseProto resp = BlockOpResponseProto.parseFrom(\n            HdfsProtoUtil.vintPrefixed(blockReplyStream));\n        pipelineStatus = resp.getStatus();\n        firstBadLink = resp.getFirstBadLink();\n        \n        if (pipelineStatus != SUCCESS) {\n          if (pipelineStatus == Status.ERROR_ACCESS_TOKEN) {\n            throw new InvalidBlockTokenException(\n                \"Got access token error for connect ack with firstBadLink as \"\n                    + firstBadLink);\n          } else {\n            throw new IOException(\"Bad connect ack with firstBadLink as \"\n                + firstBadLink);\n          }\n        }\n        assert null == blockStream : \"Previous blockStream unclosed\";\n        blockStream = out;\n        result =  true; // success\n\n      } catch (IOException ie) {\n\n        DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n\n        // find the datanode that matches\n        if (firstBadLink.length() != 0) {\n          for (int i = 0; i < nodes.length; i++) {\n            if (nodes[i].getXferAddr().equals(firstBadLink)) {\n              errorIndex = i;\n              break;\n            }\n          }\n        } else {\n          errorIndex = 0;\n        }\n        hasError = true;\n        setLastException(ie);\n        result =  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s = null;\n          IOUtils.closeStream(out);\n          out = null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream = null;\n        }\n      }\n      return result;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.closeStream": "    private void closeStream() {\n      if (blockStream != null) {\n        try {\n          blockStream.close();\n        } catch (IOException e) {\n          setLastException(e);\n        } finally {\n          blockStream = null;\n        }\n      }\n      if (blockReplyStream != null) {\n        try {\n          blockReplyStream.close();\n        } catch (IOException e) {\n          setLastException(e);\n        } finally {\n          blockReplyStream = null;\n        }\n      }\n      if (null != s) {\n        try {\n          s.close();\n        } catch (IOException e) {\n          setLastException(e);\n        } finally {\n          s = null;\n        }\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Connecting to datanode \" + first);\n    }\n    final InetSocketAddress isa =\n      NetUtils.createSocketAddr(first.getXferAddr());\n    final Socket sock = client.socketFactory.createSocket();\n    final int timeout = client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), timeout);\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n    }\n    return sock;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.setLastException": "    private void setLastException(IOException e) {\n      if (lastException == null) {\n        lastException = e;\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.setupPipelineForAppendOrRecovery": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes == null || nodes.length == 0) {\n        String msg = \"Could not get block locations. \" + \"Source file \\\"\"\n            + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed = true;\n        return false;\n      }\n      \n      boolean success = false;\n      long newGS = 0L;\n      while (!success && !streamerClosed && dfsClient.clientRunning) {\n        boolean isRecovery = hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex >= 0) {\n          StringBuilder pipelineMsg = new StringBuilder();\n          for (int j = 0; j < nodes.length; j++) {\n            pipelineMsg.append(nodes[j]);\n            if (j < nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length <= 1) {\n            lastException = new IOException(\"All datanodes \" + pipelineMsg\n                + \" are bad. Aborting...\");\n            streamerClosed = true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n              \" in pipeline \" + pipelineMsg + \n              \": bad datanode \" + nodes[errorIndex]);\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes = new DatanodeInfo[nodes.length-1];\n          System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n          System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n              newnodes.length-errorIndex);\n          nodes = newnodes;\n          hasError = false;\n          lastException = null;\n          errorIndex = -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n            nodes, isAppend, isHflushed)) {\n          addDatanode2ExistingPipeline();\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb = dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS = lb.getBlock().getGenerationStamp();\n        accessToken = lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        success = createBlockOutputStream(nodes, newGS, isRecovery);\n      }\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock = new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock, nodes);\n        // update client side generation stamp\n        block = newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.getBlockToken": "  synchronized Token<BlockTokenIdentifier> getBlockToken() {\n    return streamer.getBlockToken();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.addDatanode2ExistingPipeline": "    private void addDatanode2ExistingPipeline() throws IOException {\n      if (DataTransferProtocol.LOG.isDebugEnabled()) {\n        DataTransferProtocol.LOG.debug(\"lastAckedSeqno = \" + lastAckedSeqno);\n      }\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       * \n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens \n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       * \n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n      if (!isAppend && lastAckedSeqno < 0\n          && stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n        //no data have been written\n        return;\n      } else if (stage == BlockConstructionStage.PIPELINE_CLOSE\n          || stage == BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        //pipeline is closing\n        return;\n      }\n\n      //get a new datanode\n      final DatanodeInfo[] original = nodes;\n      final LocatedBlock lb = dfsClient.namenode.getAdditionalDatanode(\n          src, block, nodes, failed.toArray(new DatanodeInfo[failed.size()]),\n          1, dfsClient.clientName);\n      nodes = lb.getLocations();\n\n      //find the new datanode\n      final int d = findNewDatanode(original);\n\n      //transfer replica\n      final DatanodeInfo src = d == 0? nodes[1]: nodes[d - 1];\n      final DatanodeInfo[] targets = {nodes[d]};\n      transfer(src, targets, lb.getBlockToken());\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.getBlock": "    ExtendedBlock getBlock() {\n      return block;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.run": "      public void run() {\n\n        setName(\"ResponseProcessor for block \" + block);\n        PipelineAck ack = new PipelineAck();\n\n        while (!responderClosed && dfsClient.clientRunning && !isLastPacketInBlock) {\n          // process responses from datanodes.\n          try {\n            // read an ack from the pipeline\n            ack.readFields(blockReplyStream);\n            if (DFSClient.LOG.isDebugEnabled()) {\n              DFSClient.LOG.debug(\"DFSClient \" + ack);\n            }\n            \n            long seqno = ack.getSeqno();\n            // processes response status from datanodes.\n            for (int i = ack.getNumOfReplies()-1; i >=0  && dfsClient.clientRunning; i--) {\n              final Status reply = ack.getReply(i);\n              if (reply != SUCCESS) {\n                errorIndex = i; // first bad datanode\n                throw new IOException(\"Bad response \" + reply +\n                    \" for block \" + block +\n                    \" from datanode \" + \n                    targets[i]);\n              }\n            }\n            \n            assert seqno != PipelineAck.UNKOWN_SEQNO : \n              \"Ack for unkown seqno should be a failed ack: \" + ack;\n            if (seqno == Packet.HEART_BEAT_SEQNO) {  // a heartbeat ack\n              continue;\n            }\n\n            // a success ack for a data packet\n            Packet one = null;\n            synchronized (dataQueue) {\n              one = ackQueue.getFirst();\n            }\n            if (one.seqno != seqno) {\n              throw new IOException(\"Responseprocessor: Expecting seqno \" +\n                                    \" for block \" + block +\n                                    one.seqno + \" but received \" + seqno);\n            }\n            isLastPacketInBlock = one.lastPacketInBlock;\n            // update bytesAcked\n            block.setNumBytes(one.getLastByteOffsetBlock());\n\n            synchronized (dataQueue) {\n              lastAckedSeqno = seqno;\n              ackQueue.removeFirst();\n              dataQueue.notifyAll();\n            }\n          } catch (Exception e) {\n            if (!responderClosed) {\n              if (e instanceof IOException) {\n                setLastException((IOException)e);\n              }\n              hasError = true;\n              errorIndex = errorIndex==-1 ? 0 : errorIndex;\n              synchronized (dataQueue) {\n                dataQueue.notifyAll();\n              }\n              DFSClient.LOG.warn(\"DFSOutputStream ResponseProcessor exception \"\n                  + \" for block \" + block, e);\n              responderClosed = true;\n            }\n          }\n        }\n      }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.endBlock": "    private void endBlock() {\n      if(DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"Closing old block \" + block);\n      }\n      this.setName(\"DataStreamer for file \" + src);\n      closeResponder();\n      closeStream();\n      nodes = null;\n      stage = BlockConstructionStage.PIPELINE_SETUP_CREATE;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.getBuffer": "    ByteBuffer getBuffer() {\n      /* Once this is called, no more data can be added to the packet.\n       * setting 'buf' to null ensures that.\n       * This is called only when the packet is ready to be sent.\n       */\n      if (buffer != null) {\n        return buffer;\n      }\n      \n      //prepare the header and close any gap between checksum and data.\n      \n      int dataLen = dataPos - dataStart;\n      int checksumLen = checksumPos - checksumStart;\n      \n      if (checksumPos != dataStart) {\n        /* move the checksum to cover the gap.\n         * This can happen for the last packet.\n         */\n        System.arraycopy(buf, checksumStart, buf, \n                         dataStart - checksumLen , checksumLen); \n      }\n      \n      int pktLen = HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\n      \n      //normally dataStart == checksumPos, i.e., offset is zero.\n      buffer = ByteBuffer.wrap(\n        buf, dataStart - checksumPos,\n        PacketHeader.PKT_HEADER_LEN + pktLen - HdfsConstants.BYTES_IN_INTEGER);\n      buf = null;\n      buffer.mark();\n\n      PacketHeader header = new PacketHeader(\n        pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen);\n      header.putInBuffer(buffer);\n      \n      buffer.reset();\n      return buffer;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.close": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e = lastException;\n      if (e == null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket != null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock != 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket = new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n            bytesCurBlock);\n        currentPacket.lastPacketInBlock = true;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock = streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.leaserenewer.closeFile(src, dfsClient);\n    } finally {\n      closed = true;\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.initDataStreaming": "    private void initDataStreaming() {\n      this.setName(\"DataStreamer for file \" + src +\n          \" block \" + block);\n      response = new ResponseProcessor(nodes);\n      response.start();\n      stage = BlockConstructionStage.DATA_STREAMING;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.closeInternal": "    private void closeInternal() {\n      closeResponder();       // close and join\n      closeStream();\n      streamerClosed = true;\n      closed = true;\n      synchronized (dataQueue) {\n        dataQueue.notifyAll();\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.getLastByteOffsetBlock": "    long getLastByteOffsetBlock() {\n      return offsetInBlock + dataPos - dataStart;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.processDatanodeError": "    private boolean processDatanodeError() throws IOException {\n      if (response != null) {\n        DFSClient.LOG.info(\"Error Recovery for block \" + block +\n        \" waiting for responder to exit. \");\n        return true;\n      }\n      closeStream();\n\n      // move packets from ack queue to front of the data queue\n      synchronized (dataQueue) {\n        dataQueue.addAll(0, ackQueue);\n        ackQueue.clear();\n      }\n\n      boolean doSleep = setupPipelineForAppendOrRecovery();\n      \n      if (!streamerClosed && dfsClient.clientRunning) {\n        if (stage == BlockConstructionStage.PIPELINE_CLOSE) {\n\n          // If we had an error while closing the pipeline, we go through a fast-path\n          // where the BlockReceiver does not run. Instead, the DataNode just finalizes\n          // the block immediately during the 'connect ack' process. So, we want to pull\n          // the end-of-block packet from the dataQueue, since we don't actually have\n          // a true pipeline to send it over.\n          //\n          // We also need to set lastAckedSeqno to the end-of-block Packet's seqno, so that\n          // a client waiting on close() will be aware that the flush finished.\n          synchronized (dataQueue) {\n            assert dataQueue.size() == 1;\n            Packet endOfBlockPacket = dataQueue.remove();  // remove the end of block packet\n            assert endOfBlockPacket.lastPacketInBlock;\n            assert lastAckedSeqno == endOfBlockPacket.seqno - 1;\n            lastAckedSeqno = endOfBlockPacket.seqno;\n            dataQueue.notifyAll();\n          }\n          endBlock();\n        } else {\n          initDataStreaming();\n        }\n      }\n      \n      return doSleep;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.nextBlockOutputStream": "    private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n      LocatedBlock lb = null;\n      DatanodeInfo[] nodes = null;\n      int count = dfsClient.getConf().nBlockWriteRetry;\n      boolean success = false;\n      ExtendedBlock oldBlock = block;\n      do {\n        hasError = false;\n        lastException = null;\n        errorIndex = -1;\n        success = false;\n\n        long startTime = System.currentTimeMillis();\n        DatanodeInfo[] excluded = excludedNodes.toArray(\n            new DatanodeInfo[excludedNodes.size()]);\n        block = oldBlock;\n        lb = locateFollowingBlock(startTime,\n            excluded.length > 0 ? excluded : null);\n        block = lb.getBlock();\n        block.setNumBytes(0);\n        accessToken = lb.getBlockToken();\n        nodes = lb.getLocations();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success = createBlockOutputStream(nodes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning block \" + block);\n          dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n          block = null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.add(nodes[errorIndex]);\n        }\n      } while (!success && --count >= 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return nodes;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.isHeartbeatPacket": "    private boolean isHeartbeatPacket() {\n      return seqno == HEART_BEAT_SEQNO;\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed": "  public static InputStream vintPrefixed(final InputStream input)\n  throws IOException {\n    final int firstByte = input.read();\n    if (firstByte == -1) {\n      throw new EOFException(\"Premature EOF: no length prefix available\");\n    }\n    \n    int size = CodedInputStream.readRawVarint32(firstByte, input);\n    assert size >= 0;\n  \n    return new ExactSizeInputStream(input, size);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.transfer": "    private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n        final Token<BlockTokenIdentifier> blockToken) throws IOException {\n      //transfer replica to the new datanode\n      Socket sock = null;\n      DataOutputStream out = null;\n      DataInputStream in = null;\n      try {\n        sock = createSocketForPipeline(src, 2, dfsClient);\n        final long writeTimeout = dfsClient.getDatanodeWriteTimeout(2);\n        out = new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(sock, writeTimeout),\n            HdfsConstants.SMALL_BUFFER_SIZE));\n\n        //send the TRANSFER_BLOCK request\n        new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n            targets);\n\n        //ack\n        in = new DataInputStream(NetUtils.getInputStream(sock));\n        BlockOpResponseProto response =\n          BlockOpResponseProto.parseFrom(HdfsProtoUtil.vintPrefixed(in));\n        if (SUCCESS != response.getStatus()) {\n          throw new IOException(\"Failed to add a datanode\");\n        }\n      } finally {\n        IOUtils.closeStream(in);\n        IOUtils.closeStream(out);\n        IOUtils.closeSocket(sock);\n      }\n    }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.transferReplicaForPipelineRecovery": "  void transferReplicaForPipelineRecovery(final ExtendedBlock b,\n      final DatanodeInfo[] targets, final String client) throws IOException {\n    final long storedGS;\n    final long visible;\n    final BlockConstructionStage stage;\n\n    //get replica information\n    synchronized(data) {\n      if (data.isValidRbw(b)) {\n        stage = BlockConstructionStage.TRANSFER_RBW;\n      } else if (data.isValidBlock(b)) {\n        stage = BlockConstructionStage.TRANSFER_FINALIZED;\n      } else {\n        final String r = data.getReplicaString(b.getBlockPoolId(), b.getBlockId());\n        throw new IOException(b + \" is neither a RBW nor a Finalized, r=\" + r);\n      }\n\n      storedGS = data.getStoredBlock(b.getBlockPoolId(),\n          b.getBlockId()).getGenerationStamp();\n      if (storedGS < b.getGenerationStamp()) {\n        throw new IOException(\n            storedGS + \" = storedGS < b.getGenerationStamp(), b=\" + b);        \n      }\n      visible = data.getReplicaVisibleLength(b);\n    }\n\n    //set storedGS and visible length\n    b.setGenerationStamp(storedGS);\n    b.setNumBytes(visible);\n\n    if (targets.length > 0) {\n      new DataTransfer(targets, b, stage, client).run();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.run": "      public void run() {\n        for(RecoveringBlock b : blocks) {\n          try {\n            logRecoverBlock(\"NameNode\", b.getBlock(), b.getLocations());\n            recoverBlock(b);\n          } catch (IOException e) {\n            LOG.warn(\"recoverBlocks FAILED: \" + b, e);\n          }\n        }\n      }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getReplicaVisibleLength": "  public long getReplicaVisibleLength(final ExtendedBlock block) throws IOException {\n    checkWriteAccess(block);\n    return data.getReplicaVisibleLength(block);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.transferBlock": "  public void transferBlock(final ExtendedBlock blk,\n      final Token<BlockTokenIdentifier> blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets) throws IOException {\n    checkAccess(null, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n    previousOpClientName = clientName;\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out = new DataOutputStream(\n        NetUtils.getOutputStream(s, dnConf.socketWriteTimeout));\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n      writeResponse(Status.SUCCESS, null, out);\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.updateCurrentThreadName": "  private void updateCurrentThreadName(String status) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"DataXceiver for client \");\n    if (previousOpClientName != null) {\n      sb.append(previousOpClientName).append(\" at \");\n    }\n    sb.append(remoteAddress);\n    if (status != null) {\n      sb.append(\" [\").append(status).append(\"]\");\n    }\n    Thread.currentThread().setName(sb.toString());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeResponse": "  private static void writeResponse(Status status, String message, OutputStream out)\n  throws IOException {\n    BlockOpResponseProto.Builder response = BlockOpResponseProto.newBuilder()\n      .setStatus(status);\n    if (message != null) {\n      response.setMessage(message);\n    }\n    response.build().writeDelimitedTo(out);\n    out.flush();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess": "  private void checkAccess(DataOutputStream out, final boolean reply, \n      final ExtendedBlock blk,\n      final Token<BlockTokenIdentifier> t,\n      final Op op,\n      final BlockTokenSecretManager.AccessMode mode) throws IOException {\n    if (datanode.isBlockTokenEnabled) {\n      try {\n        datanode.blockPoolTokenSecretManager.checkAccess(t, null, blk, mode);\n      } catch(InvalidToken e) {\n        try {\n          if (reply) {\n            if (out == null) {\n              out = new DataOutputStream(\n                  NetUtils.getOutputStream(s, dnConf.socketWriteTimeout));\n            }\n            \n            BlockOpResponseProto.Builder resp = BlockOpResponseProto.newBuilder()\n              .setStatus(ERROR_ACCESS_TOKEN);\n            if (mode == BlockTokenSecretManager.AccessMode.WRITE) {\n              DatanodeRegistration dnR = \n                datanode.getDNRegistrationForBP(blk.getBlockPoolId());\n              resp.setFirstBadLink(dnR.getXferAddr());\n            }\n            resp.build().writeDelimitedTo(out);\n            out.flush();\n          }\n          LOG.warn(\"Block token verification failed: op=\" + op\n              + \", remoteAddress=\" + remoteAddress\n              + \", message=\" + e.getLocalizedMessage());\n          throw e;\n        } finally {\n          IOUtils.closeStream(out);\n        }\n      }\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opTransferBlock": "  private void opTransferBlock(DataInputStream in) throws IOException {\n    final OpTransferBlockProto proto =\n      OpTransferBlockProto.parseFrom(vintPrefixed(in));\n    transferBlock(fromProto(proto.getHeader().getBaseHeader().getBlock()),\n        fromProto(proto.getHeader().getBaseHeader().getToken()),\n        proto.getHeader().getClientName(),\n        fromProtos(proto.getTargetsList()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp": "  protected final void processOp(Op op) throws IOException {\n    switch(op) {\n    case READ_BLOCK:\n      opReadBlock();\n      break;\n    case WRITE_BLOCK:\n      opWriteBlock(in);\n      break;\n    case REPLACE_BLOCK:\n      opReplaceBlock(in);\n      break;\n    case COPY_BLOCK:\n      opCopyBlock(in);\n      break;\n    case BLOCK_CHECKSUM:\n      opBlockChecksum(in);\n      break;\n    case TRANSFER_BLOCK:\n      opTransferBlock(in);\n      break;\n    default:\n      throw new IOException(\"Unknown op \" + op + \" in data stream\");\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReplaceBlock": "  private void opReplaceBlock(DataInputStream in) throws IOException {\n    OpReplaceBlockProto proto = OpReplaceBlockProto.parseFrom(vintPrefixed(in));\n    replaceBlock(fromProto(proto.getHeader().getBlock()),\n        fromProto(proto.getHeader().getToken()),\n        proto.getDelHint(),\n        fromProto(proto.getSource()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opBlockChecksum": "  private void opBlockChecksum(DataInputStream in) throws IOException {\n    OpBlockChecksumProto proto = OpBlockChecksumProto.parseFrom(vintPrefixed(in));\n    \n    blockChecksum(fromProto(proto.getHeader().getBlock()),\n        fromProto(proto.getHeader().getToken()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opCopyBlock": "  private void opCopyBlock(DataInputStream in) throws IOException {\n    OpCopyBlockProto proto = OpCopyBlockProto.parseFrom(vintPrefixed(in));\n    copyBlock(fromProto(proto.getHeader().getBlock()),\n        fromProto(proto.getHeader().getToken()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock": "  private void opReadBlock() throws IOException {\n    OpReadBlockProto proto = OpReadBlockProto.parseFrom(vintPrefixed(in));\n    readBlock(fromProto(proto.getHeader().getBaseHeader().getBlock()),\n        fromProto(proto.getHeader().getBaseHeader().getToken()),\n        proto.getHeader().getClientName(),\n        proto.getOffset(),\n        proto.getLen());\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock": "  private void opWriteBlock(DataInputStream in) throws IOException {\n    final OpWriteBlockProto proto = OpWriteBlockProto.parseFrom(vintPrefixed(in));\n    writeBlock(fromProto(proto.getHeader().getBaseHeader().getBlock()),\n        fromProto(proto.getHeader().getBaseHeader().getToken()),\n        proto.getHeader().getClientName(),\n        fromProtos(proto.getTargetsList()),\n        fromProto(proto.getSource()),\n        fromProto(proto.getStage()),\n        proto.getPipelineSize(),\n        proto.getMinBytesRcvd(), proto.getMaxBytesRcvd(),\n        proto.getLatestGenerationStamp(),\n        fromProto(proto.getRequestedChecksum()));\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.run": "  public void run() {\n    int opsProcessed = 0;\n    Op op = null;\n    dataXceiverServer.childSockets.add(s);\n    try {\n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed != 0) {\n            assert dnConf.socketKeepaliveTimeout > 0;\n            socketInputWrapper.setTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            socketInputWrapper.setTimeout(dnConf.socketTimeout);\n          }\n          op = readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it's quite normal to get EOF here.\n          if (opsProcessed > 0 &&\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed != 0) {\n          s.setSoTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime = now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!s.isClosed() && dnConf.socketKeepaliveTimeout > 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                ((op == null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.dfsClient.getDatanodeWriteTimeout": "  int getDatanodeWriteTimeout(int numNodes) {\n    return (dfsClientConf.confTime > 0) ?\n      (dfsClientConf.confTime + HdfsServerConstants.WRITE_TIMEOUT_EXTENSION * numNodes) : 0;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.dfsClient.getConf": "  Conf getConf() {\n    return dfsClientConf;\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.transferReplicaForPipelineRecovery": "  void transferReplicaForPipelineRecovery(final ExtendedBlock b,\n      final DatanodeInfo[] targets, final String client) throws IOException {\n    final long storedGS;\n    final long visible;\n    final BlockConstructionStage stage;\n\n    //get replica information\n    synchronized(data) {\n      if (data.isValidRbw(b)) {\n        stage = BlockConstructionStage.TRANSFER_RBW;\n      } else if (data.isValidBlock(b)) {\n        stage = BlockConstructionStage.TRANSFER_FINALIZED;\n      } else {\n        final String r = data.getReplicaString(b.getBlockPoolId(), b.getBlockId());\n        throw new IOException(b + \" is neither a RBW nor a Finalized, r=\" + r);\n      }\n\n      storedGS = data.getStoredBlock(b.getBlockPoolId(),\n          b.getBlockId()).getGenerationStamp();\n      if (storedGS < b.getGenerationStamp()) {\n        throw new IOException(\n            storedGS + \" = storedGS < b.getGenerationStamp(), b=\" + b);        \n      }\n      visible = data.getReplicaVisibleLength(b);\n    }\n\n    //set storedGS and visible length\n    b.setGenerationStamp(storedGS);\n    b.setNumBytes(visible);\n\n    if (targets.length > 0) {\n      new DataTransfer(targets, b, stage, client).run();\n    }\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.run": "      public void run() {\n        for(RecoveringBlock b : blocks) {\n          try {\n            logRecoverBlock(\"NameNode\", b.getBlock(), b.getLocations());\n            recoverBlock(b);\n          } catch (IOException e) {\n            LOG.warn(\"recoverBlocks FAILED: \" + b, e);\n          }\n        }\n      }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getReplicaVisibleLength": "  public long getReplicaVisibleLength(final ExtendedBlock block) throws IOException {\n    checkWriteAccess(block);\n    return data.getReplicaVisibleLength(block);\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getDisplayName": "  public String getDisplayName() {\n    // NB: our DatanodeID may not be set yet\n    return hostName + \":\" + getXferPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXferPort": "  int getXferPort() {\n    return streamingAddr.getPort();\n  }",
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.datanode.getXceiverCount": "  public int getXceiverCount() {\n    return threadGroup == null ? 0 : threadGroup.activeCount();\n  }"
        },
        "bug_report": {
            "Title": "adding new datanode to existing  pipeline fails in case of Append/Recovery",
            "Description": "Scenario:\n=========\n\n1. Cluster with 4 DataNodes.\n2. Written file to 3 DNs, DN1->DN2->DN3\n3. Stopped DN3,\nNow Append to file is failing due to addDatanode2ExistingPipeline is failed.\n\n *CLinet Trace* \n{noformat}\n2012-04-24 22:06:09,947 INFO  hdfs.DFSClient (DFSOutputStream.java:createBlockOutputStream(1063)) - Exception in createBlockOutputStream\njava.io.IOException: Bad connect ack with firstBadLink as *******:50010\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1053)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:943)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n2012-04-24 22:06:09,947 WARN  hdfs.DFSClient (DFSOutputStream.java:setupPipelineForAppendOrRecovery(916)) - Error Recovery for block BP-1023239-10.18.40.233-1335275282109:blk_296651611851855249_1253 in pipeline *****:50010, ******:50010, *****:50010: bad datanode ******:50010\n2012-04-24 22:06:10,072 WARN  hdfs.DFSClient (DFSOutputStream.java:run(549)) - DataStreamer Exception\njava.io.EOFException: Premature EOF: no length prefix available\n\tat org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n2012-04-24 22:06:10,072 WARN  hdfs.DFSClient (DFSOutputStream.java:hflush(1515)) - Error while syncing\njava.io.EOFException: Premature EOF: no length prefix available\n\tat org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\njava.io.EOFException: Premature EOF: no length prefix available\n\tat org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n{noformat}\n\n *DataNode Trace*  \n\n{noformat}\n\n2012-05-17 15:39:12,261 ERROR datanode.DataNode (DataXceiver.java:run(193)) - host0.foo.com:49744:DataXceiver error processing TRANSFER_BLOCK operation  src: /127.0.0.1:49811 dest: /127.0.0.1:49744\njava.io.IOException: BP-2001850558-xx.xx.xx.xx-1337249347060:blk_-8165642083860293107_1002 is neither a RBW nor a Finalized, r=ReplicaBeingWritten, blk_-8165642083860293107_1003, RBW\n  getNumBytes()     = 1024\n  getBytesOnDisk()  = 1024\n  getVisibleLength()= 1024\n  getVolume()       = E:\\MyWorkSpace\\branch-2\\Test\\build\\test\\data\\dfs\\data\\data1\\current\n  getBlockFile()    = E:\\MyWorkSpace\\branch-2\\Test\\build\\test\\data\\dfs\\data\\data1\\current\\BP-2001850558-xx.xx.xx.xx-1337249347060\\current\\rbw\\blk_-8165642083860293107\n  bytesAcked=1024\n  bytesOnDisk=102\nat org.apache.hadoop.hdfs.server.datanode.DataNode.transferReplicaForPipelineRecovery(DataNode.java:2038)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.transferBlock(DataXceiver.java:525)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opTransferBlock(Receiver.java:114)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:78)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:189)\n\tat java.lang.Thread.run(Unknown Source)\n{noformat}"
        }
    }
]