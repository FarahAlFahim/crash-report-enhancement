[
    {
        "filename": "HADOOP-6989.json",
        "creation_time": "2010-10-04T23:55:16.000+0000",
        "bug_report": {
            "Title": "TestSetFile is failing on trunk",
            "Description": "The test suite `org.apache.hadoop.io.TestSetFile` is encountering an error during execution. The test case `testSetFile` fails due to an `IllegalArgumentException` indicating that a key class or comparator option must be set when initializing a `MapFile.Writer` or `SetFile.Writer`. This issue arises when the code attempts to write to a MapFile or SetFile without the necessary configuration for the key class or comparator.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: key class or comparator option must be set",
                "at org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:247)",
                "at org.apache.hadoop.io.SetFile$Writer.<init>(SetFile.java:60)",
                "at org.apache.hadoop.io.TestSetFile.writeTest(TestSetFile.java:73)",
                "at org.apache.hadoop.io.TestSetFile.testSetFile(TestSetFile.java:45)"
            ],
            "RootCause": "The root cause of the issue is that the `MapFile.Writer` constructor requires either a key class or a comparator to be specified, and neither is provided in the current implementation. This leads to the `IllegalArgumentException` being thrown.",
            "StepsToReproduce": [
                "Run the test suite `org.apache.hadoop.io.TestSetFile`.",
                "Observe the output for the `testSetFile` test case."
            ],
            "ExpectedBehavior": "The test case `testSetFile` should execute successfully without throwing any exceptions, indicating that the MapFile or SetFile is written correctly with the necessary configurations.",
            "ObservedBehavior": "The test case `testSetFile` fails with an `IllegalArgumentException`, indicating that a key class or comparator option must be set.",
            "Suggestions": "To resolve the issue, ensure that when initializing `MapFile.Writer`, either a key class or a comparator is provided. For example, use `MapFile.Writer(conf, new Path(dirName), MapFile.Writer.keyClass(MyKeyClass.class))` to set the key class appropriately.",
            "problem_location": {
                "files": [
                    "src/java/org/apache/hadoop/io/TestSetFile.java",
                    "src/java/org/apache/hadoop/io/MapFile.java"
                ],
                "classes": [
                    "org.apache.hadoop.io.TestSetFile",
                    "org.apache.hadoop.io.MapFile"
                ],
                "methods": [
                    "TestSetFile.writeTest",
                    "MapFile.Writer.init"
                ]
            },
            "possible_fix": "Modify the initialization of `MapFile.Writer` in the `writeTest` method of `TestSetFile` to include a key class or comparator. For example:\n\n```java\nMapFile.Writer writer = new MapFile.Writer(conf, new Path(dirName), MapFile.Writer.keyClass(MyKeyClass.class));\n```"
        },
        "possible_fix_code": {
            "TestSetFile.writeTest": "public void writeTest() throws IOException {\n    Configuration conf = new Configuration();\n    String dirName = \"testSetFileDir\";\n    // Ensure to set the key class when initializing MapFile.Writer\n    MapFile.Writer writer = new MapFile.Writer(conf, new Path(dirName), MapFile.Writer.keyClass(MyKeyClass.class));\n    // Additional code for writing to the writer...\n    // writer.append(key, value);\n    writer.close();\n}"
        }
    },
    {
        "filename": "HADOOP-10823.json",
        "creation_time": "2014-07-15T07:20:00.000+0000",
        "bug_report": {
            "Title": "TestReloadingX509TrustManager is flaky",
            "Description": "The test case `TestReloadingX509TrustManager.testReload` is failing intermittently, resulting in an `AssertionFailedError` where the expected value was `2`, but the actual value was `1`. This discrepancy suggests that the test is not validating the expected behavior correctly. Additionally, there is a warning indicating an `EOFException` related to loading a Java KeyStore, which may imply that the KeyStore file is either missing or corrupted, affecting the test's execution.",
            "StackTrace": [
                "junit.framework.AssertionFailedError: expected:<2> but was:<1>",
                "\tat junit.framework.Assert.fail(Assert.java:50)",
                "\tat junit.framework.Assert.failNotEquals(Assert.java:287)",
                "\tat junit.framework.Assert.assertEquals(Assert.java:67)",
                "\tat junit.framework.Assert.assertEquals(Assert.java:199)",
                "\tat junit.framework.Assert.assertEquals(Assert.java:205)",
                "\tat org.apache.hadoop.security.ssl.TestReloadingX509TrustManager.testReload(TestReloadingX509TrustManager.java:112)",
                "2014-07-06 06:12:21,170 WARN  ssl.ReloadingX509TrustManager (ReloadingX509TrustManager.java:run(197)) - Could not load truststore (keep using existing one) : java.io.EOFException",
                "java.io.EOFException",
                "\tat java.io.DataInputStream.readInt(DataInputStream.java:375)",
                "\tat sun.security.provider.JavaKeyStore.engineLoad(JavaKeyStore.java:628)",
                "\tat sun.security.provider.JavaKeyStore$JKS.engineLoad(JavaKeyStore.java:38)",
                "\tat java.security.KeyStore.load(KeyStore.java:1185)",
                "\tat org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager(ReloadingX509TrustManager.java:166)",
                "\tat org.apache.hadoop.security.ssl.ReloadingX509TrustManager.run(ReloadingX509TrustManager.java:195)",
                "\tat java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue appears to be twofold: an `AssertionFailedError` indicating a logic error in the test or the code under test, and an `EOFException` suggesting that the KeyStore file is either missing or corrupted, preventing the proper loading of the trust manager.",
            "StepsToReproduce": [
                "Run the JUnit test `TestReloadingX509TrustManager.testReload`.",
                "Ensure that the KeyStore file is present and correctly formatted.",
                "Observe the output for any assertion failures or exceptions."
            ],
            "ExpectedBehavior": "The test should pass without any assertion failures, and the KeyStore should load successfully, allowing the trust manager to function as intended.",
            "ObservedBehavior": "The test fails with an `AssertionFailedError`, indicating that the expected value was `2`, but the actual value was `1`. Additionally, a warning about the inability to load the truststore is logged, followed by an `EOFException`.",
            "Suggestions": "Verify the presence and integrity of the KeyStore file. If the file is missing or corrupted, replace it with a valid KeyStore. Additionally, review the logic in the `testReload` method to ensure it correctly reflects the expected outcomes.",
            "problem_location": {
                "files": [
                    "ReloadingX509TrustManager.java",
                    "TestReloadingX509TrustManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.ssl.ReloadingX509TrustManager",
                    "org.apache.hadoop.security.ssl.TestReloadingX509TrustManager"
                ],
                "methods": [
                    "ReloadingX509TrustManager.loadTrustManager",
                    "TestReloadingX509TrustManager.testReload",
                    "ReloadingX509TrustManager.run"
                ]
            },
            "possible_fix": "Ensure that the KeyStore file is correctly configured and accessible. If necessary, update the test to handle cases where the KeyStore might not load properly, or adjust the expected values in the assertions to match the actual behavior."
        },
        "possible_fix_code": {
            "ReloadingX509TrustManager.loadTrustManager": "  X509TrustManager loadTrustManager()\n  throws IOException, GeneralSecurityException {\n    X509TrustManager trustManager = null;\n    KeyStore ks = KeyStore.getInstance(type);\n    lastLoaded = file.lastModified();\n    FileInputStream in = null;\n    try {\n      in = new FileInputStream(file);\n      ks.load(in, password.toCharArray());\n      LOG.debug(\"Loaded truststore '\" + file + \"'\");\n    } catch (EOFException e) {\n      LOG.error(\"EOFException while loading truststore: \" + e.getMessage());\n      throw new IOException(\"Truststore file is corrupted or improperly formatted.\", e);\n    } finally {\n      if (in != null) {\n        in.close();\n      }\n    }\n\n    TrustManagerFactory trustManagerFactory = \n      TrustManagerFactory.getInstance(SSLFactory.SSLCERTIFICATE);\n    trustManagerFactory.init(ks);\n    TrustManager[] trustManagers = trustManagerFactory.getTrustManagers();\n    for (TrustManager trustManager1 : trustManagers) {\n      if (trustManager1 instanceof X509TrustManager) {\n        trustManager = (X509TrustManager) trustManager1;\n        break;\n      }\n    }\n    return trustManager;\n  }"
        }
    },
    {
        "filename": "HADOOP-9125.json",
        "creation_time": "2012-12-10T02:07:52.000+0000",
        "bug_report": {
            "Title": "LdapGroupsMapping threw CommunicationException after some idle time",
            "Description": "The LdapGroupsMapping component is throwing a CommunicationException after a period of inactivity. This issue arises when attempting to retrieve groups for a user from the LDAP server, specifically after the connection has been idle. The stack trace indicates that the connection was closed, leading to the exception. The problem may be related to the configuration of the LDAP connection parameters or network issues.",
            "StackTrace": [
                "2012-12-07 02:20:59,738 WARN org.apache.hadoop.security.LdapGroupsMapping: Exception trying to get groups for user aduser2",
                "javax.naming.CommunicationException: connection closed [Root exception is java.io.IOException: connection closed]; remaining name 'CN=Users,DC=EXAMPLE,DC=COM'",
                "at com.sun.jndi.ldap.LdapCtx.doSearch(LdapCtx.java:1983)",
                "at com.sun.jndi.ldap.LdapCtx.searchAux(LdapCtx.java:1827)",
                "at com.sun.jndi.ldap.LdapCtx.c_search(LdapCtx.java:1752)",
                "at com.sun.jndi.ldap.LdapCtx.c_search(LdapCtx.java:1769)",
                "at com.sun.jndi.toolkit.ctx.ComponentDirContext.p_search(ComponentDirContext.java:394)",
                "at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:376)",
                "at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:358)",
                "at javax.naming.directory.InitialDirContext.search(InitialDirContext.java:267)",
                "at org.apache.hadoop.security.LdapGroupsMapping.getGroups(LdapGroupsMapping.java:187)",
                "at org.apache.hadoop.security.CompositeGroupsMapping.getGroups(CompositeGroupsMapping.java:97)",
                "at org.apache.hadoop.security.Groups.doGetGroups(Groups.java:103)",
                "at org.apache.hadoop.security.Groups.getGroups(Groups.java:70)",
                "at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1035)",
                "at org.apache.hadoop.hbase.security.User.getGroupNames(User.java:90)",
                "at org.apache.hadoop.hbase.security.access.TableAuthManager.authorize(TableAuthManager.java:355)",
                "at org.apache.hadoop.hbase.security.access.AccessController.requirePermission(AccessController.java:379)",
                "at org.apache.hadoop.hbase.security.access.AccessController.getUserPermissions(AccessController.java:1051)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.hbase.regionserver.HRegion.exec(HRegion.java:4914)",
                "at org.apache.hadoop.hbase.regionserver.HRegionServer.execCoprocessor(HRegionServer.java:3546)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.hbase.ipc.SecureRpcEngine$Server.call(SecureRpcEngine.java:372)",
                "at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1399)",
                "Caused by: java.io.IOException: connection closed",
                "at com.sun.jndi.ldap.LdapClient.ensureOpen(LdapClient.java:1558)",
                "at com.sun.jndi.ldap.LdapClient.search(LdapClient.java:503)",
                "at com.sun.jndi.ldap.LdapCtx.doSearch(LdapCtx.java:1965)",
                "... 28 more",
                "2012-12-07 02:20:59,739 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user aduser2"
            ],
            "RootCause": "The root cause of the issue is a CommunicationException triggered by a closed connection to the LDAP server. This can occur due to various factors, including incorrect LDAP URL, authentication failures, SSL misconfiguration, or network issues.",
            "StepsToReproduce": [
                "Allow the system to remain idle for a period of time.",
                "Attempt to retrieve groups for a user (e.g., aduser2) from the LDAP server."
            ],
            "ExpectedBehavior": "The system should successfully retrieve the groups for the specified user without throwing any exceptions.",
            "ObservedBehavior": "The system throws a CommunicationException indicating that the connection to the LDAP server was closed.",
            "Suggestions": "Verify the LDAP connection parameters in the setConf method of the LdapGroupsMapping class. Ensure that the LDAP URL, bind user, and bind password are correctly configured. Consider adding logging around the connection attempts to capture detailed error information.",
            "problem_location": {
                "files": [
                    "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.LdapGroupsMapping"
                ],
                "classes": [
                    "org.apache.hadoop.security.LdapGroupsMapping"
                ],
                "methods": [
                    "LdapGroupsMapping.getGroups",
                    "LdapGroupsMapping.getDirContext",
                    "LdapGroupsMapping.setConf"
                ]
            },
            "possible_fix": "Check the LDAP URL and ensure it is reachable. Verify the bind user and password for correctness. If SSL is required, ensure that the keystore and its password are correctly set up. Implement logging in the getDirContext method to capture connection attempts and failures."
        },
        "possible_fix_code": {
            "LdapGroupsMapping.getDirContext": "  DirContext getDirContext() throws NamingException {\n    if (ctx == null) {\n      // Set up the initial environment for LDAP connectivity\n      Hashtable<String, String> env = new Hashtable<String, String>();\n      env.put(Context.INITIAL_CONTEXT_FACTORY,\n          com.sun.jndi.ldap.LdapCtxFactory.class.getName());\n      env.put(Context.PROVIDER_URL, ldapUrl);\n      env.put(Context.SECURITY_AUTHENTICATION, \"simple\");\n\n      // Set up SSL security, if necessary\n      if (useSsl) {\n        env.put(Context.SECURITY_PROTOCOL, \"ssl\");\n        System.setProperty(\"javax.net.ssl.keyStore\", keystore);\n        System.setProperty(\"javax.net.ssl.keyStorePassword\", keystorePass);\n      }\n\n      env.put(Context.SECURITY_PRINCIPAL, bindUser);\n      env.put(Context.SECURITY_CREDENTIALS, bindPassword);\n\n      try {\n        ctx = new InitialDirContext(env);\n      } catch (NamingException e) {\n        LOG.error(\"Failed to create LDAP context: \" + e.getMessage(), e);\n        throw e; // Rethrow the exception to handle it upstream\n      }\n    }\n\n    return ctx;\n  }"
        }
    },
    {
        "filename": "HADOOP-10252.json",
        "creation_time": "2014-01-22T16:43:27.000+0000",
        "bug_report": {
            "Title": "HttpServer can't start if hostname is not specified",
            "Description": "The issue arises from a recent change (HADOOP-8362) that enforces non-null configuration values. The HttpServer fails to start when the hostname is not specified, leading to an IllegalArgumentException. The stack trace indicates that the exception is thrown during the initialization of the HttpServer, specifically when attempting to set the BIND_ADDRESS property to a null hostname.",
            "StackTrace": [
                "2014-01-22 08:43:05,969 FATAL [M:0;localhost:48573] master.HMaster(2187): Unhandled exception. Starting shutdown.",
                "java.lang.IllegalArgumentException: Property value must not be null",
                "at com.google.common.base.Preconditions.checkArgument(Preconditions.java:92)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:958)",
                "at org.apache.hadoop.http.HttpServer.initializeWebServer(HttpServer.java:510)",
                "at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:470)",
                "at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:458)",
                "at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:412)",
                "at org.apache.hadoop.hbase.util.InfoServer.<init>(InfoServer.java:59)",
                "at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:584)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "RootCause": "The root cause of the issue is that the hostname variable is null when the HttpServer attempts to set the BIND_ADDRESS property in the Configuration object. This occurs in the initializeWebServer method, which is called during the HttpServer's construction.",
            "StepsToReproduce": [
                "Attempt to start the HttpServer without specifying a hostname.",
                "Observe the logs for the IllegalArgumentException indicating that a property value must not be null."
            ],
            "ExpectedBehavior": "The HttpServer should start successfully with a default hostname or a specified hostname.",
            "ObservedBehavior": "The HttpServer fails to start and throws an IllegalArgumentException due to a null hostname.",
            "Suggestions": "Ensure that a valid hostname is provided when creating an instance of HttpServer. If no hostname is specified, consider implementing a default value or a more informative error message.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java"
                ],
                "classes": [
                    "org.apache.hadoop.http.HttpServer",
                    "org.apache.hadoop.conf.Configuration"
                ],
                "methods": [
                    "HttpServer.initializeWebServer",
                    "Configuration.set"
                ]
            },
            "possible_fix": "In the HttpServer.Builder class, modify the build method to check if hostName is null and assign a default value or throw a more descriptive exception. For example:\n\nif (this.hostName == null) {\n    this.hostName = \"localhost\"; // or another default value\n}"
        },
        "possible_fix_code": {
            "HttpServer.Builder.build": "    public HttpServer build() throws IOException {\n      if (this.name == null) {\n        throw new HadoopIllegalArgumentException(\"name is not set\");\n      }\n\n      // Make the behavior compatible with deprecated interfaces\n      if (bindAddress != null && port != -1) {\n        try {\n          endpoints.add(0, new URI(\"http\", \"\", bindAddress, port, \"\", \"\", \"\"));\n        } catch (URISyntaxException e) {\n          throw new HadoopIllegalArgumentException(\"Invalid endpoint: \" + e);\n        }\n      }\n\n      if (endpoints.size() == 0 && connector == null) {\n        throw new HadoopIllegalArgumentException(\"No endpoints specified\");\n      }\n\n      if (hostName == null) {\n        hostName = endpoints.size() == 0 ? connector.getHost() : endpoints.get(0).getHost();\n      }\n\n      // Ensure hostName is not null before proceeding\n      if (hostName == null) {\n        hostName = \"localhost\"; // Assign a default value\n      }\n\n      if (this.conf == null) {\n        conf = new Configuration();\n      }\n\n      HttpServer server = new HttpServer(this);\n\n      if (this.securityEnabled) {\n        server.initSpnego(conf, hostName, usernameConfKey, keytabConfKey);\n      }\n\n      if (connector != null) {\n        server.addUnmanagedListener(connector);\n      }\n\n      for (URI ep : endpoints) {\n        Connector listener = null;\n        String scheme = ep.getScheme();\n        if (\"http\".equals(scheme)) {\n          listener = HttpServer.createDefaultChannelConnector();\n        } else if (\"https\".equals(scheme)) {\n          SslSocketConnector c = new SslSocketConnector();\n          c.setNeedClientAuth(needsClientAuth);\n          c.setKeyPassword(keyPassword);\n\n          if (keyStore != null) {\n            c.setKeystore(keyStore);\n            c.setKeystoreType(keyStoreType);\n            c.setPassword(keyStorePassword);\n          }\n\n          if (trustStore != null) {\n            c.setTruststore(trustStore);\n            c.setTruststoreType(trustStoreType);\n            c.setTrustPassword(trustStorePassword);\n          }\n          listener = c;\n\n        } else {\n          throw new HadoopIllegalArgumentException(\n              \"unknown scheme for endpoint:\" + ep);\n        }\n        listener.setHost(ep.getHost());\n        listener.setPort(ep.getPort() == -1 ? 0 : ep.getPort());\n        server.addManagedListener(listener);\n      }\n      server.loadListeners();\n      return server;\n    }"
        }
    },
    {
        "filename": "HADOOP-12239.json",
        "creation_time": "2015-07-15T18:06:14.000+0000",
        "bug_report": {
            "Title": "StorageException complaining 'no lease ID' when updating FolderLastModifiedTime in WASB",
            "Description": "The issue arises during the log splitting process in HBase, specifically when attempting to update the last modified time of a folder in Azure Blob Storage. The error indicates that a lease is currently held on the blob, preventing the operation from completing successfully. This is similar to previously reported issues (HADOOP-11523 and HADOOP-12089) but occurs in a different context within the codebase. The stack trace reveals that the failure occurs in the `updateFolderLastModifiedTime` method of the `AzureNativeFileSystemStore`, which is responsible for managing folder renames in Azure.",
            "StackTrace": [
                "2015-07-09 13:38:57,388 INFO org.apache.hadoop.hbase.master.SplitLogManager: dead splitlog workers [workernode3.xxx.b6.internal.cloudapp.net,60020,1436448555180]",
                "2015-07-09 13:38:57,466 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN",
                "java.io.IOException: failed log splitting for workernode12.xxx.b6.internal.cloudapp.net,60020,1436448555180, will retry",
                "at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.resubmit(ServerShutdownHandler.java:343)",
                "at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:211)",
                "at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.io.IOException: Unable to write RenamePending file for folder rename from hbase/WALs/workernode12.xxx.b6.internal.cloudapp.net,60020,1436448566374 to hbase/WALs/workernode12.xxx.b6.internal.cloudapp.net,60020,1436448566374-splitting",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending.writeFile(NativeAzureFileSystem.java:258)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.prepareAtomicFolderRename(NativeAzureFileSystem.java:2110)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1998)",
                "at org.apache.hadoop.hbase.master.MasterFileSystem.getLogDirs(MasterFileSystem.java:325)",
                "at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:412)",
                "at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:390)",
                "at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:288)",
                "at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:204)",
                "... 4 more",
                "Caused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2598)",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2609)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1366)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1195)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:775)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending.writeFile(NativeAzureFileSystem.java:255)",
                "... 11 more",
                "Caused by: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.",
                "at com.microsoft.azure.storage.StorageException.translateException(StorageException.java:89)",
                "at com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:307)",
                "at com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:182)",
                "at com.microsoft.azure.storage.blob.CloudBlob.uploadProperties(CloudBlob.java:2892)",
                "at org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobWrapperImpl.uploadProperties(StorageInterfaceImpl.java:372)",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2593)",
                "... 19 more"
            ],
            "RootCause": "The root cause of the issue is that a lease is currently held on the blob in Azure Blob Storage, and the operation to update the folder's last modified time fails because no lease ID was specified in the request. This prevents the log splitting process in HBase from completing successfully.",
            "StepsToReproduce": [
                "Attempt to update the last modified time of a folder in Azure Blob Storage while a lease is held on the blob.",
                "Ensure that no lease ID is specified in the request."
            ],
            "ExpectedBehavior": "The system should successfully update the last modified time of the folder in Azure Blob Storage, allowing the log splitting process in HBase to complete without errors.",
            "ObservedBehavior": "The operation fails with a StorageException indicating that there is a lease on the blob and no lease ID was specified, preventing the folder rename operation.",
            "Suggestions": "1. Check for active leases on the blob using Azure Storage Explorer or Azure SDKs. 2. Release the lease if possible or break it programmatically if the process holding it is no longer running. 3. Ensure that the lease ID is specified in requests that modify or delete the blob. 4. Implement retry logic to handle transient lease-related errors. 5. Consult Azure Blob Storage documentation for lease management.",
            "problem_location": {
                "files": [
                    "AzureNativeFileSystemStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"
                ],
                "methods": [
                    "updateFolderLastModifiedTime"
                ]
            },
            "possible_fix": "To resolve the issue, ensure that the lease ID is included in the request when attempting to update the folder's last modified time. Additionally, implement logic to check for and manage active leases before performing operations that require modifications to leased blobs."
        },
        "possible_fix_code": {
            "updateFolderLastModifiedTime": "  public void updateFolderLastModifiedTime(String key,\n      SelfRenewingLease folderLease) throws AzureException {\n    final Calendar lastModifiedCalendar = Calendar\n        .getInstance(Utility.LOCALE_US);\n    lastModifiedCalendar.setTimeZone(Utility.UTC_ZONE);\n    Date lastModified = lastModifiedCalendar.getTime();\n\n    // Check if a lease is held on the blob and include the lease ID in the request\n    AccessCondition leaseCondition = getLeaseCondition(folderLease);\n    try {\n        updateFolderLastModifiedTime(key, lastModified, leaseCondition);\n    } catch (StorageException e) {\n        // Handle the case where the lease is held and no lease ID was specified\n        if (e.getErrorCode().equals(\"LeaseAlreadyPresent\")) {\n            LOG.warn(\"Lease is already held on the blob. Attempting to break the lease.\");\n            // Logic to break the lease or handle the error\n            throw new AzureException(\"Unable to update last modified time due to lease issue.\", e);\n        }\n        throw e; // rethrow other exceptions\n    }\n  }"
        }
    },
    {
        "filename": "HADOOP-11878.json",
        "creation_time": "2015-04-27T08:06:22.000+0000",
        "bug_report": {
            "Title": "FileContext.java # fixRelativePart should check for not null for a more informative exception",
            "Description": "A NullPointerException is thrown in the Hadoop framework when the deletion service attempts to delete log files. The stack trace indicates that the exception occurs in the `fixRelativePart` method of the `FileContext` class, specifically when it tries to create a new `Path` object using a `null` working directory. This issue arises when both `getInitialWorkingDirectory()` and `getHomeDirectory()` return `null`, leading to an unhandled null reference.",
            "StackTrace": [
                "2015-04-27 14:56:17,113 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : null",
                "2015-04-27 14:56:17,113 ERROR org.apache.hadoop.yarn.server.nodemanager.DeletionService: Exception during execution of task in DeletionService",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.fs.FileContext.fixRelativePart(FileContext.java:274)",
                "at org.apache.hadoop.fs.FileContext.delete(FileContext.java:761)",
                "at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.deleteAsUser(DefaultContainerExecutor.java:457)",
                "at org.apache.hadoop.yarn.server.nodemanager.DeletionService$FileDeletionTask.run(DeletionService.java:293)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The NullPointerException occurs because both `getInitialWorkingDirectory()` and `getHomeDirectory()` return `null`, which leads to the `workingDir` variable being `null` in the `FileContext` class when `fixRelativePart` is called.",
            "StepsToReproduce": [
                "1. Trigger a deletion task in the Hadoop YARN framework.",
                "2. Ensure that the default file system does not provide a valid initial working directory or home directory.",
                "3. Observe the logs for a NullPointerException in the deletion service."
            ],
            "ExpectedBehavior": "The deletion service should handle paths correctly without throwing a NullPointerException, providing informative error messages if the working directory is not set.",
            "ObservedBehavior": "A NullPointerException is thrown when the deletion service attempts to delete a log file, indicating that the absolute path is null.",
            "Suggestions": "Implement a null check for the `workingDir` in the `fixRelativePart` method to provide a more informative exception message. Additionally, ensure that the file system configuration provides valid initial working and home directories.",
            "problem_location": {
                "files": [
                    "FileContext.java"
                ],
                "classes": [
                    "org.apache.hadoop.fs.FileContext"
                ],
                "methods": [
                    "fixRelativePart",
                    "getInitialWorkingDirectory",
                    "getHomeDirectory"
                ]
            },
            "possible_fix": "In the `fixRelativePart` method, add a check for `workingDir` before creating a new `Path` object. If `workingDir` is null, throw a custom exception with a clear message indicating the issue."
        },
        "possible_fix_code": {
            "fixRelativePart": "  Path fixRelativePart(Path p) {\n    if (p.isUriPathAbsolute()) {\n      return p;\n    } else {\n      if (workingDir == null) {\n        throw new IllegalStateException(\"Working directory is not set. Both getInitialWorkingDirectory() and getHomeDirectory() returned null.\");\n      }\n      return new Path(workingDir, p);\n    }\n  }"
        }
    },
    {
        "filename": "HADOOP-14949.json",
        "creation_time": "2017-10-13T23:44:09.000+0000",
        "bug_report": {
            "Title": "TestKMS#testACLs fails intermittently",
            "Description": "The test `TestKMS#testACLs` has been observed to fail intermittently due to a `java.lang.AssertionError`. The error message indicates that the test expected not to be able to re-encrypt an encrypted key, but the assertion failed, suggesting that the conditions leading to this assertion are not being met consistently. This issue may stem from the test setup or the logic within the `TestKMS` class, particularly around the re-encryption process.",
            "StackTrace": [
                "java.lang.AssertionError: Should not have been able to reencryptEncryptedKey",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.apache.hadoop.crypto.key.kms.server.TestKMS$11$15.run(TestKMS.java:1616)",
                "at org.apache.hadoop.crypto.key.kms.server.TestKMS$11$15.run(TestKMS.java:1608)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)",
                "at org.apache.hadoop.crypto.key.kms.server.TestKMS.doAs(TestKMS.java:313)",
                "at org.apache.hadoop.crypto.key.kms.server.TestKMS.access$100(TestKMS.java:97)"
            ],
            "RootCause": "The intermittent failure of the test is likely due to inconsistent conditions in the test setup or logic that leads to the assertion failure regarding the re-encryption of an encrypted key. The specific method `reencryptEncryptedKey` is not found, indicating potential issues in the test's assumptions or the state of the key management system.",
            "StepsToReproduce": [
                "Run the test suite containing `TestKMS#testACLs` multiple times.",
                "Observe the intermittent failures related to the assertion on re-encryption."
            ],
            "ExpectedBehavior": "The test should pass without throwing an `AssertionError`, confirming that the system behaves as expected when attempting to re-encrypt an encrypted key.",
            "ObservedBehavior": "The test fails intermittently with a `java.lang.AssertionError`, indicating that the system incorrectly allows re-encryption of an encrypted key.",
            "Suggestions": "1. Review the assertion logic in the test to ensure it accurately reflects the expected behavior of the key management system.\n2. Check the test setup to ensure that all necessary conditions are met before the assertion is made.\n3. Use debugging tools to trace the execution flow and variable states leading up to the assertion failure.",
            "problem_location": {
                "files": [
                    "TestKMS.java"
                ],
                "classes": [
                    "org.apache.hadoop.crypto.key.kms.server.TestKMS"
                ],
                "methods": []
            },
            "possible_fix": "Ensure that the test environment is correctly set up and that the conditions for re-encryption are properly defined. If the method `reencryptEncryptedKey` is not found, verify if it should be implemented or if the test is incorrectly assuming its existence."
        },
        "possible_fix_code": {
            "testACLs": "public void testACLs() throws Exception {\n    // Setup the test environment, ensuring all necessary conditions are met\n    // For example, create and encrypt a key, and set the expected behavior for re-encryption\n    Key key = createAndEncryptKey();\n    boolean canReencrypt = false; // This should be false if the key is already encrypted\n\n    // Perform the action that is expected to fail\n    try {\n        canReencrypt = reencryptEncryptedKey(key);\n    } catch (Exception e) {\n        // Handle any exceptions that may arise during the re-encryption process\n        // Log the exception for debugging purposes\n        LOG.error(\"Error during re-encryption: \" + e.getMessage(), e);\n    }\n\n    // Assert that re-encryption should not be allowed\n    Assert.assertFalse(\"Should not have been able to reencryptEncryptedKey\", canReencrypt);\n}"
        }
    },
    {
        "filename": "HADOOP-10540.json",
        "creation_time": "2014-04-11T05:14:07.000+0000",
        "bug_report": {
            "Title": "Datanode upgrade in Windows fails with hardlink error.",
            "Description": "During the upgrade process from Hadoop 1.x to 2.4, the DataNode fails to start due to a hard link exception. The issue arises when the DataNode attempts to create hard links for its storage directories, leading to an IOException indicating incorrect command line arguments for the hardlink creation command. This problem is particularly evident in the Windows environment where the command structure differs from Unix-based systems.",
            "StackTrace": [
                "2014-04-10 22:47:12,254 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool <registering> (Datanode Uuid unassigned) service to myhost/10.0.0.1:8020",
                "java.io.IOException: Usage: hardlink create [LINKNAME] [FILENAME] |Incorrect command line arguments.",
                "at org.apache.hadoop.fs.HardLink.createHardLinkMult(HardLink.java:479)",
                "at org.apache.hadoop.fs.HardLink.createHardLinkMult(HardLink.java:416)",
                "at org.apache.hadoop.hdfs.server.datanode.DataStorage.linkBlocks(DataStorage.java:816)",
                "at org.apache.hadoop.hdfs.server.datanode.DataStorage.linkAllBlocks(DataStorage.java:759)",
                "at org.apache.hadoop.hdfs.server.datanode.DataStorage.doUpgrade(DataStorage.java:566)",
                "at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:486)",
                "at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:225)",
                "at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:249)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:929)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:900)"
            ],
            "RootCause": "The root cause of the issue is the incorrect command line arguments being passed to the hardlink creation command in the Windows environment. Specifically, the parameters for the `createHardLink` method are either null or improperly formatted, leading to an IOException.",
            "StepsToReproduce": [
                "1. Install Hadoop 1.x.",
                "2. Run 'hadoop dfsadmin -safemode enter'.",
                "3. Run 'hadoop dfsadmin -saveNamespace'.",
                "4. Run 'hadoop namenode -finalize'.",
                "5. Stop all Hadoop services.",
                "6. Uninstall Hadoop 1.x.",
                "7. Install Hadoop 2.4.",
                "8. Start the namenode with the '-upgrade' option.",
                "9. Attempt to start the datanode and observe the hardlink exception in the service log."
            ],
            "ExpectedBehavior": "The DataNode should start successfully without any exceptions, allowing it to link the necessary storage directories as part of the upgrade process.",
            "ObservedBehavior": "The DataNode fails to start, throwing an IOException related to incorrect command line arguments for the hardlink creation command.",
            "Suggestions": "Ensure that the parameters passed to the `createHardLink` method are valid and not null. Verify that the command structure for creating hard links in the Windows environment is correctly implemented in the `HardLinkCGWin` class.",
            "problem_location": {
                "files": [
                    "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.HardLink.java"
                ],
                "classes": [
                    "org.apache.hadoop.fs.HardLink",
                    "org.apache.hadoop.hdfs.server.datanode.DataStorage",
                    "org.apache.hadoop.hdfs.server.datanode.DataNode"
                ],
                "methods": [
                    "HardLink.createHardLinkMult",
                    "DataStorage.linkBlocks",
                    "DataNode.initStorage"
                ]
            },
            "possible_fix": "Review the implementation of the `createHardLink` and `createHardLinkMult` methods in the `HardLink` class to ensure that the parameters are correctly set before executing the command. Additionally, check the `HardLinkCGWin` class for proper command construction for Windows."
        },
        "possible_fix_code": {
            "createHardLink": "  public static void createHardLink(File file, File linkName) \n  throws IOException {\n    if (file == null) {\n      throw new IOException(\n          \"invalid arguments to createHardLink: source file is null\");\n    }\n    if (linkName == null) {\n      throw new IOException(\n          \"invalid arguments to createHardLink: link name is null\");\n    }\n    // Ensure the file exists before attempting to create a hard link\n    if (!file.exists()) {\n      throw new FileNotFoundException(file + \" not found.\");\n    }\n    // construct and execute shell command\n    String[] hardLinkCommand = getHardLinkCommand.linkOne(file, linkName);\n    Process process = Runtime.getRuntime().exec(hardLinkCommand);\n    try {\n      if (process.waitFor() != 0) {\n        String errMsg = new BufferedReader(new InputStreamReader(\n            process.getInputStream())).readLine();\n        if (errMsg == null)  errMsg = \"\";\n        String inpMsg = new BufferedReader(new InputStreamReader(\n            process.getErrorStream())).readLine();\n        if (inpMsg == null)  inpMsg = \"\";\n        throw new IOException(errMsg + inpMsg);\n      }\n    } catch (InterruptedException e) {\n      throw new IOException(e);\n    } finally {\n      process.destroy();\n    }\n  }"
        }
    },
    {
        "filename": "HADOOP-7629.json",
        "creation_time": "2011-09-09T17:45:14.000+0000",
        "bug_report": {
            "Title": "Regression with MAPREDUCE-2289 - setPermission passed immutable FsPermission (RPC failure)",
            "Description": "The bug report highlights a regression introduced by MAPREDUCE-2289, where the method `fs.setPermission(stagingArea, JOB_DIR_PERMISSION)` is called with an immutable `FsPermission`. This leads to a `NoSuchMethodException` for the inner class `FsPermission$2`, indicating that the constructor is either inaccessible or does not exist. The stack trace shows that the error occurs during an RPC call, specifically when trying to read call parameters, which results in a failure to instantiate the required class via reflection.",
            "StackTrace": [
                "2011-09-08 16:31:45,187 WARN org.apache.hadoop.ipc.Server: Unable to read call parameters for client 127.0.0.1",
                "java.lang.RuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:115)",
                "at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:53)",
                "at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:236)",
                "at org.apache.hadoop.ipc.RPC$Invocation.readFields(RPC.java:104)",
                "at org.apache.hadoop.ipc.Server$Connection.processData(Server.java:1337)",
                "at org.apache.hadoop.ipc.Server$Connection.processOneRpc(Server.java:1315)",
                "at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1215)",
                "at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:566)",
                "at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:363)",
                "Caused by: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()",
                "at java.lang.Class.getConstructor0(Class.java:2706)",
                "at java.lang.Class.getDeclaredConstructor(Class.java:1985)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:109)"
            ],
            "RootCause": "The root cause of the issue is the attempt to instantiate an inner class `FsPermission$2` via reflection, which fails due to the constructor not being accessible or not existing. This is a direct consequence of passing an immutable `FsPermission` object in an RPC call.",
            "StepsToReproduce": [
                "1. Trigger the method that calls fs.setPermission with JOB_DIR_PERMISSION.",
                "2. Ensure that the environment is set up to make RPC calls.",
                "3. Observe the logs for the NoSuchMethodException related to FsPermission$2."
            ],
            "ExpectedBehavior": "The system should successfully set the permissions on the staging area without throwing any exceptions, allowing the RPC call to complete successfully.",
            "ObservedBehavior": "The system throws a NoSuchMethodException for the constructor of the inner class FsPermission$2, leading to a failure in processing the RPC call.",
            "Suggestions": "1. Check the definition of FsPermission$2 to ensure it has a public constructor. 2. If it is an inner class, ensure it is instantiated correctly with an instance of the outer class. 3. Verify that the correct version of the Hadoop library is included in the classpath.",
            "problem_location": {
                "files": [
                    "FsPermission.java"
                ],
                "classes": [
                    "org.apache.hadoop.fs.permission.FsPermission"
                ],
                "methods": [
                    "FsPermission.init"
                ]
            },
            "possible_fix": "Consider modifying the code to use a different constructor or factory method for FsPermission that is accessible, or refactor the code to avoid using the immutable FsPermission in RPC calls."
        },
        "possible_fix_code": {
            "FsPermission.init": "public void init() {\n    // Check if the JOB_DIR_PERMISSION is mutable or create a new instance if necessary\n    FsPermission permission;\n    if (JOB_DIR_PERMISSION.isImmutable()) {\n        permission = new FsPermission(JOB_DIR_PERMISSION.getPermission()); // Create a mutable copy\n    } else {\n        permission = JOB_DIR_PERMISSION;\n    }\n    fs.setPermission(stagingArea, permission);\n}"
        }
    },
    {
        "filename": "HADOOP-15060.json",
        "creation_time": "2017-11-22T00:18:55.000+0000",
        "bug_report": {
            "Title": "TestShellBasedUnixGroupsMapping.testFiniteGroupResolutionTime flaky",
            "Description": "The test case `testFiniteGroupResolutionTime` in the `TestShellBasedUnixGroupsMapping` class is failing intermittently due to an `AssertionError`. The test expects a log message indicating a command timeout, but instead, it receives a warning about a non-existing user. This discrepancy suggests that the timeout condition is not being triggered as expected, likely because the command completes quickly when the user does not exist.",
            "StackTrace": [
                "[ERROR] testFiniteGroupResolutionTime(org.apache.hadoop.security.TestShellBasedUnixGroupsMapping)  Time elapsed: 61.975 s  <<< FAILURE!",
                "java.lang.AssertionError:",
                "Expected the logs to carry a message about command timeout but was: 2017-11-22 00:10:57,523 WARN  security.ShellBasedUnixGroupsMapping (ShellBasedUnixGroupsMapping.java:getUnixGroups(181)) - unable to return groups for user foobarnonexistinguser",
                "PartialGroupNameException The user name 'foobarnonexistinguser' is not found.",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:275)",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:178)",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97)",
                "at org.apache.hadoop.security.TestShellBasedUnixGroupsMapping.testFiniteGroupResolutionTime(TestShellBasedUnixGroupsMapping.java:278)"
            ],
            "RootCause": "The root cause of the issue is that the command executed to retrieve Unix groups for a non-existing user completes quickly, leading to a warning about the user not being found instead of a timeout message. The timeout condition is not met because the command does not take long enough to trigger it.",
            "StepsToReproduce": [
                "Run the test case `testFiniteGroupResolutionTime` in the `TestShellBasedUnixGroupsMapping` class.",
                "Ensure that the user 'foobarnonexistinguser' does not exist in the system.",
                "Observe the logs for the expected timeout message."
            ],
            "ExpectedBehavior": "The test should log a message indicating that the command timed out while trying to retrieve groups for the user.",
            "ObservedBehavior": "The test logs a warning stating that it was unable to return groups for the user 'foobarnonexistinguser', leading to an `AssertionError` due to the absence of the expected timeout message.",
            "Suggestions": "To resolve this issue, consider modifying the test to check for both the timeout message and the warning about the non-existing user. Additionally, ensure that the timeout configuration is set correctly and that the command execution can take longer to trigger the timeout condition.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.ShellBasedUnixGroupsMapping",
                    "org.apache.hadoop.security.ShellCommandExecutor"
                ],
                "methods": [
                    "ShellBasedUnixGroupsMapping.getUnixGroups",
                    "ShellCommandExecutor.execute"
                ]
            },
            "possible_fix": "Review the implementation of the `getUnixGroups` method to ensure that the timeout condition is appropriately triggered. Additionally, verify the configuration settings for the timeout in the `setConf` method to ensure they are applied correctly."
        },
        "possible_fix_code": {
            "getUnixGroups": "  private List<String> getUnixGroups(String user) throws IOException {\n    ShellCommandExecutor executor = createGroupExecutor(user);\n\n    List<String> groups;\n    try {\n      executor.execute();\n      groups = resolveFullGroupNames(executor.getOutput());\n    } catch (ExitCodeException e) {\n      try {\n        groups = resolvePartialGroupNames(user, e.getMessage(),\n            executor.getOutput());\n      } catch (PartialGroupNameException pge) {\n        LOG.warn(\"unable to return groups for user {}\", user, pge);\n        return EMPTY_GROUPS;\n      }\n    } catch (IOException ioe) {\n      // If its a shell executor timeout, indicate so in the message\n      // but treat the result as empty instead of throwing it up,\n      // similar to how partial resolution failures are handled above\n      if (executor.isTimedOut()) {\n        LOG.warn(\n            \"Unable to return groups for user '\" + user + \"' as shell group lookup \" +\n            \"command '\" + Joiner.on(' ').join(executor.getExecString()) + \"' ran longer than the configured timeout limit of \" +\n            timeout + \" seconds.\");\n        return EMPTY_GROUPS;\n      } else {\n        // If its not an executor timeout, we should let the caller handle it\n        throw ioe;\n      }\n    }\n\n    // remove duplicated primary group\n    if (!Shell.WINDOWS) {\n      for (int i = 1; i < groups.size(); i++) {\n        if (groups.get(i).equals(groups.get(0))) {\n          groups.remove(i);\n          break;\n        }\n      }\n    }\n\n    return groups;\n  }"
        }
    },
    {
        "filename": "HADOOP-10937.json",
        "creation_time": "2014-08-04T21:22:55.000+0000",
        "bug_report": {
            "Title": "Need to set version name correctly before decrypting EEK",
            "Description": "A Null Pointer Exception occurs when attempting to touch a file in HDFS, specifically during the decryption of an encrypted key. The stack trace indicates that the issue arises in the `decryptEncryptedKey` method of the `KMSClientProvider` class, where it attempts to access fields of the `EncryptedKeyVersion` object that may not be properly initialized.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.crypto.key.kms.KMSClientProvider.decryptEncryptedKey(KMSClientProvider.java:652)",
                "at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:342)",
                "at org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1319)",
                "at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1364)",
                "at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1352)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:391)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:384)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:328)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)"
            ],
            "RootCause": "The `NullPointerException` is likely caused by one or more fields in the `EncryptedKeyVersion` object being null when passed to the `decryptEncryptedKey` method. The method performs null checks on critical fields such as `encryptionKeyVersionName`, `encryptedKeyIv`, and `encryptedKeyVersion`, which must be properly initialized before the method is invoked.",
            "StepsToReproduce": [
                "1. Execute the command: `hdfs dfs -touchz /enc3/touchFile`.",
                "2. Observe the logs for the Null Pointer Exception."
            ],
            "ExpectedBehavior": "The system should successfully create a zero-length file without throwing any exceptions, indicating that the decryption of the encrypted key was successful.",
            "ObservedBehavior": "A Null Pointer Exception is thrown, indicating that the decryption process failed due to uninitialized fields in the `EncryptedKeyVersion` object.",
            "Suggestions": "Investigate the initialization and population of the `EncryptedKeyVersion` object. Ensure that all required fields are set before calling the `decryptEncryptedKey` method. Consider adding additional logging to track the state of the `EncryptedKeyVersion` before decryption.",
            "problem_location": {
                "files": [
                    "KMSClientProvider.java"
                ],
                "classes": [
                    "org.apache.hadoop.crypto.key.kms.KMSClientProvider"
                ],
                "methods": [
                    "decryptEncryptedKey"
                ]
            },
            "possible_fix": "Ensure that the `EncryptedKeyVersion` object is properly populated before it is passed to the `decryptEncryptedKey` method. This may involve checking the logic in the methods that create or retrieve `EncryptedKeyVersion` instances, such as `generateEncryptedKey` and any related queue management methods."
        },
        "possible_fix_code": {
            "decryptEncryptedKey": "  public KeyVersion decryptEncryptedKey(\n      EncryptedKeyVersion encryptedKeyVersion) throws IOException,\n                                                      GeneralSecurityException {\n    checkNotNull(encryptedKeyVersion, \"encryptedKeyVersion\");\n    checkNotNull(encryptedKeyVersion.getEncryptionKeyVersionName(),\n        \"versionName\");\n    checkNotNull(encryptedKeyVersion.getEncryptedKeyIv(), \"iv\");\n    Preconditions.checkArgument(\n        encryptedKeyVersion.getEncryptedKeyVersion().getVersionName()\n            .equals(KeyProviderCryptoExtension.EEK),\n        \"encryptedKey version name must be '%s', is '%s'\",\n        KeyProviderCryptoExtension.EK,\n        encryptedKeyVersion.getEncryptedKeyVersion().getVersionName()\n    );\n    checkNotNull(encryptedKeyVersion.getEncryptedKeyVersion(), \"encryptedKey\");\n    Map<String, String> params = new HashMap<String, String>();\n    params.put(KMSRESTConstants.EEK_OP, KMSRESTConstants.EEK_DECRYPT);\n    Map<String, Object> jsonPayload = new HashMap<String, Object>();\n    jsonPayload.put(KMSRESTConstants.NAME_FIELD,\n        encryptedKeyVersion.getEncryptionKeyName());\n    jsonPayload.put(KMSRESTConstants.IV_FIELD, Base64.encodeBase64String(\n        encryptedKeyVersion.getEncryptedKeyIv()));\n    jsonPayload.put(KMSRESTConstants.MATERIAL_FIELD, Base64.encodeBase64String(\n            encryptedKeyVersion.getEncryptedKeyVersion().getMaterial()));\n    URL url = createURL(KMSRESTConstants.KEY_VERSION_RESOURCE,\n        encryptedKeyVersion.getEncryptionKeyVersionName(),\n        KMSRESTConstants.EEK_SUB_RESOURCE, params);\n    HttpURLConnection conn = createConnection(url, HTTP_POST);\n    conn.setRequestProperty(CONTENT_TYPE, APPLICATION_JSON_MIME);\n    Map response =\n        call(conn, jsonPayload, HttpURLConnection.HTTP_OK, Map.class);\n    return parseJSONKeyVersion(response);\n  }"
        }
    },
    {
        "filename": "HADOOP-9103.json",
        "creation_time": "2012-04-20T01:07:25.000+0000",
        "bug_report": {
            "Title": "UTF8 class does not properly decode Unicode characters outside the basic multilingual plane",
            "Description": "The issue arises in a Hadoop HDFS environment where an IOException is thrown due to a lease being held for a non-existent file. The error log indicates that the system attempts to access a file that has either been deleted or never existed, leading to the exception. The problem is linked to the handling of Unicode characters outside the basic multilingual plane, particularly in the methods responsible for saving the FS image and writing INode data. The analysis of the code reveals discrepancies in how strings are encoded, which may lead to the observed behavior.",
            "StackTrace": [
                "2012-03-28 00:48:42,553 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.io.IOException: Found lease for non-existent file /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/????@???????????????",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFilesUnderConstruction(FSImage.java:1211)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:959)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.doMerge(SecondaryNameNode.java:589)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doMerge(SecondaryNameNode.java:350)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:314)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:225)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The root cause of the issue is the improper handling of Unicode characters in the methods responsible for saving the FS image and writing INode data. Specifically, the discrepancy between using String.getBytes('UTF8') and the UTF8 class for encoding leads to inconsistencies in the byte arrays generated, causing the system to reference non-existent files.",
            "StepsToReproduce": [
                "Create a file with a name that includes Unicode characters outside the basic multilingual plane.",
                "Attempt to save the FS image in HDFS.",
                "Observe the logs for any IOException related to leases for non-existent files."
            ],
            "ExpectedBehavior": "The system should correctly handle and encode Unicode characters, allowing for the successful saving of the FS image without throwing an IOException for non-existent files.",
            "ObservedBehavior": "An IOException is thrown indicating a lease for a non-existent file, which is linked to improper encoding of Unicode characters in file names.",
            "Suggestions": "To resolve the issue, ensure consistent use of the UTF8 class for encoding strings in the relevant methods. Additionally, consider implementing checks to handle leases for files that may have been deleted or never existed.",
            "problem_location": {
                "files": [
                    "FSImage.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSImage",
                    "org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode"
                ],
                "methods": [
                    "FSImage.saveFSImage",
                    "FSImage.saveImage",
                    "FSImage.writeString",
                    "FSImage.string2Bytes"
                ]
            },
            "possible_fix": "Modify the saveFSImage and related methods to consistently use the UTF8 class for encoding strings. For example, replace calls to str.getBytes('UTF8') with the UTF8 class methods to ensure uniform encoding."
        },
        "possible_fix_code": {
            "FSImage.saveFSImage": "  public void saveFSImage(FSNamesystem target) throws IOException {\n    // Ensure consistent use of UTF8 encoding for file names\n    String fileName = target.getFileName();\n    byte[] fileNameBytes = new UTF8(fileName).getBytes();\n\n    // Proceed with saving the FS image using the encoded file name\n    // ... (rest of the method implementation)\n\n    // Example of writing the file name bytes to the output stream\n    outputStream.write(fileNameBytes);\n    // ... (rest of the method implementation)\n  }"
        }
    },
    {
        "filename": "HADOOP-11151.json",
        "creation_time": "2014-09-29T08:18:04.000+0000",
        "bug_report": {
            "Title": "Automatically refresh auth token and retry on auth failure",
            "Description": "The issue arises when attempting to put or copy files into the encryption zone after enabling the CFS and KMS services in the cluster. Initially, the operations succeed, but after approximately one day, they fail with an HTTP 403 Forbidden error. The logs indicate an `AuthenticationException` due to anonymous requests being disallowed, suggesting that the application lacks proper authentication credentials when accessing the resource.",
            "StackTrace": [
                "java.util.concurrent.ExecutionException: java.io.IOException: HTTP status [403], message [Forbidden]",
                "org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler.authenticate(PseudoAuthenticationHandler.java:184)",
                "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.authenticate(DelegationTokenAuthenticationHandler.java:331)",
                "org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:507)",
                "org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter.doFilter(KMSAuthenticationFilter.java:129)",
                "org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)",
                "org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)",
                "org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)",
                "org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)",
                "org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)",
                "org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)",
                "org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)",
                "org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)",
                "org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:861)",
                "org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:606)",
                "org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)",
                "java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is an `AuthenticationException` indicating that anonymous requests are disallowed in the Hadoop security context. This occurs when the application attempts to access resources without proper authentication credentials.",
            "StepsToReproduce": [
                "1. Enable CFS and KMS services in the Hadoop cluster.",
                "2. Successfully put or copy a file into the encryption zone.",
                "3. Wait for approximately one day.",
                "4. Attempt to put or copy another file into the encryption zone and observe the failure."
            ],
            "ExpectedBehavior": "The system should allow authenticated requests to put or copy files into the encryption zone without encountering an HTTP 403 Forbidden error.",
            "ObservedBehavior": "After a period of time, attempts to put or copy files into the encryption zone fail with an HTTP 403 Forbidden error, indicating that anonymous requests are disallowed.",
            "Suggestions": "To resolve the authentication issue, ensure that proper authentication is configured in the Hadoop environment. This includes enabling authentication in the configuration files, ensuring valid Kerberos setup if applicable, and verifying that the client making requests is properly authenticated.",
            "problem_location": {
                "files": [
                    "core-site.xml",
                    "kms.log"
                ],
                "classes": [
                    "org.apache.hadoop.security.authentication.server.AuthenticationFilter",
                    "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler"
                ],
                "methods": [
                    "PseudoAuthenticationHandler.authenticate",
                    "DelegationTokenAuthenticationHandler.authenticate",
                    "AuthenticationFilter.doFilter"
                ]
            },
            "possible_fix": "1. In `core-site.xml`, set the property `hadoop.security.authentication` to a valid authentication method (e.g., `kerberos`).\n2. Ensure that the Kerberos configuration is correct, including the keytab file and `krb5.conf` settings.\n3. Verify that the client making requests has the necessary permissions and is properly authenticated."
        },
        "possible_fix_code": {
            "PseudoAuthenticationHandler.authenticate": "  public AuthenticationToken authenticate(HttpServletRequest request, HttpServletResponse response)\n    throws IOException, AuthenticationException {\n    AuthenticationToken token;\n    String userName = getUserName(request);\n    if (userName == null) {\n      if (getAcceptAnonymous()) {\n        token = AuthenticationToken.ANONYMOUS;\n      } else {\n        // Attempt to refresh the authentication token if available\n        String delegationToken = getDelegationToken(request);\n        if (delegationToken != null) {\n          // Logic to refresh the token or throw an exception if it fails\n          throw new AuthenticationException(\"Authentication required. Please provide valid credentials.\");\n        }\n        throw new AuthenticationException(\"Anonymous requests are disallowed\");\n      }\n    } else {\n      token = new AuthenticationToken(userName, userName, getType());\n    }\n    return token;\n  }"
        }
    },
    {
        "filename": "HADOOP-8031.json",
        "creation_time": "2012-02-07T20:22:24.000+0000",
        "bug_report": {
            "Title": "Configuration class fails to find embedded .jar resources; should use URL.openStream()",
            "Description": "The issue arises when running a Hadoop client within RHQ, where the Configuration class fails to locate the required 'core-site.xml' file. The stack trace indicates that the application attempts to parse a resource from a JAR file, but the DocumentBuilder cannot process the URL format provided. This results in a RuntimeException due to the missing configuration file, which is essential for Hadoop's operation. The logs indicate that the same issue persists in both old and new versions of the Configuration class, suggesting a fundamental problem in how resources are accessed.",
            "StackTrace": [
                "2012-02-07 09:15:25,313 INFO  [ResourceContainer.invoker.daemon-2] (org.apache.hadoop.conf.Configuration)- parsing jar:file:/usr/local/rhq-agent/data/tmp/rhq-hadoop-plugin-4.3.0-SNAPSHOT.jar6856622641102893436.classloader/hadoop-core-0.20.2+737+1.jar7204287718482036191.tmp!/core-default.xml",
                "2012-02-07 09:15:25,318 ERROR [InventoryManager.discovery-1] (rhq.core.pc.inventory.InventoryManager)- Failed to start component for Resource[id=16290, type=NameNode, key=NameNode:/usr/lib/hadoop-0.20, name=NameNode, parent=vg61l01ad-hadoop002.apple.com] from synchronized merge.",
                "org.rhq.core.clientapi.agent.PluginContainerException: Failed to start component for resource Resource[id=16290, type=NameNode, key=NameNode:/usr/lib/hadoop-0.20, name=NameNode, parent=vg61l01ad-hadoop002.apple.com].",
                "Caused by: java.lang.RuntimeException: core-site.xml not found",
                "at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1308)",
                "at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1228)",
                "at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1169)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:438)"
            ],
            "RootCause": "The root cause of the issue is the failure of the Configuration class to locate the 'core-site.xml' file, which is critical for Hadoop's configuration. The method 'loadResource' is unable to process the URL format provided, leading to a RuntimeException.",
            "StepsToReproduce": [
                "Run the Hadoop client within RHQ using the specified classloader.",
                "Ensure that the necessary JAR files are included in the classpath.",
                "Attempt to start the component that requires 'core-site.xml'."
            ],
            "ExpectedBehavior": "The Hadoop client should successfully locate and parse the 'core-site.xml' file, allowing the component to start without errors.",
            "ObservedBehavior": "The application fails to start the component due to a RuntimeException indicating that 'core-site.xml' cannot be found.",
            "Suggestions": "To resolve the issue, ensure that 'core-site.xml' exists in the Hadoop configuration directory. If it is missing, create a new one using a basic template or restore it from a backup. Additionally, modify the code to use 'url.openStream()' instead of 'url.toString()' for parsing the resource.",
            "problem_location": {
                "files": [
                    "Configuration.java"
                ],
                "classes": [
                    "org.apache.hadoop.conf.Configuration"
                ],
                "methods": [
                    "Configuration.loadResource",
                    "Configuration.loadResources"
                ]
            },
            "possible_fix": "Modify the Configuration class to use 'url.openStream()' for resource parsing. Example code change:\n\n{code}\n         URL url = getResource((String)name);\n-        if (url != null) {\n-          if (!quiet) {\n-            LOG.info(\"parsing \" + url);\n-          }\n-          doc = builder.parse(url.toString());\n-        }\n+        doc = builder.parse(url.openStream());\n{code}\nEnsure 'core-site.xml' is present in the configuration directory."
        },
        "possible_fix_code": {
            "loadResource": "  private void loadResource(Properties properties, Object name, boolean quiet) {\n    try {\n      DocumentBuilderFactory docBuilderFactory \n        = DocumentBuilderFactory.newInstance();\n      //ignore all comments inside the xml file\n      docBuilderFactory.setIgnoringComments(true);\n\n      //allow includes in the xml file\n      docBuilderFactory.setNamespaceAware(true);\n      try {\n          docBuilderFactory.setXIncludeAware(true);\n      } catch (UnsupportedOperationException e) {\n        LOG.error(\"Failed to set setXIncludeAware(true) for parser \"\n                + docBuilderFactory\n                + \":\" + e,\n                e);\n      }\n      DocumentBuilder builder = docBuilderFactory.newDocumentBuilder();\n      Document doc = null;\n      Element root = null;\n\n      if (name instanceof URL) {                  // an URL resource\n        URL url = (URL)name;\n        if (url != null) {\n          if (!quiet) {\n            LOG.info(\"parsing \" + url);\n          }\n          doc = builder.parse(url.openStream());\n        }\n      } else if (name instanceof String) {        // a CLASSPATH resource\n        URL url = getResource((String)name);\n        if (url != null) {\n          if (!quiet) {\n            LOG.info(\"parsing \" + url);\n          }\n          doc = builder.parse(url.openStream());\n        }\n      } else if (name instanceof Path) {          // a file resource\n        // Can't use FileSystem API or we get an infinite loop\n        // since FileSystem uses Configuration API.  Use java.io.File instead.\n        File file = new File(((Path)name).toUri().getPath())\n          .getAbsoluteFile();\n        if (file.exists()) {\n          if (!quiet) {\n            LOG.info(\"parsing \" + file);\n          }\n          InputStream in = new BufferedInputStream(new FileInputStream(file));\n          try {\n            doc = builder.parse(in);\n          } finally {\n            in.close();\n          }\n        }\n      } else if (name instanceof InputStream) {\n        try {\n          doc = builder.parse((InputStream)name);\n        } finally {\n          ((InputStream)name).close();\n        }\n      } else if (name instanceof Element) {\n        root = (Element)name;\n      }\n\n      if (doc == null && root == null) {\n        if (quiet)\n          return;\n        throw new RuntimeException(name + \" not found\");\n      }\n\n      if (root == null) {\n        root = doc.getDocumentElement();\n      }\n      if (!\"configuration\".equals(root.getTagName()))\n        LOG.fatal(\"bad conf file: top-level element not <configuration>\");\n      NodeList props = root.getChildNodes();\n      for (int i = 0; i < props.getLength(); i++) {\n        Node propNode = props.item(i);\n        if (!(propNode instanceof Element))\n          continue;\n        Element prop = (Element)propNode;\n        if (\"configuration\".equals(prop.getTagName())) {\n          loadResource(properties, prop, quiet);\n          continue;\n        }\n        if (!\"property\".equals(prop.getTagName()))\n          LOG.warn(\"bad conf file: element not <property>\");\n        NodeList fields = prop.getChildNodes();\n        String attr = null;\n        String value = null;\n        boolean finalParameter = false;\n        for (int j = 0; j < fields.getLength(); j++) {\n          Node fieldNode = fields.item(j);\n          if (!(fieldNode instanceof Element))\n            continue;\n          Element field = (Element)fieldNode;\n          if (\"name\".equals(field.getTagName()) && field.hasChildNodes())\n            attr = ((Text)field.getFirstChild()).getData().trim();\n          if (\"value\".equals(field.getTagName()) && field.hasChildNodes())\n            value = ((Text)field.getFirstChild()).getData();\n          if (\"final\".equals(field.getTagName()) && field.hasChildNodes())\n            finalParameter = \"true\".equals(((Text)field.getFirstChild()).getData());\n        }\n        \n        // Ignore this parameter if it has already been marked as 'final'\n        if (attr != null) {\n          if (deprecatedKeyMap.containsKey(attr)) {\n            DeprecatedKeyInfo keyInfo = deprecatedKeyMap.get(attr);\n            keyInfo.accessed = false;\n            for (String key:keyInfo.newKeys) {\n              // update new keys with deprecated key's value \n              loadProperty(properties, name, key, value, finalParameter);\n            }\n          }\n          else {\n            loadProperty(properties, name, attr, value, finalParameter);\n          }\n        }\n      }\n        \n    } catch (IOException e) {\n      LOG.fatal(\"error parsing conf file: \" + e);\n      throw new RuntimeException(e);\n    } catch (DOMException e) {\n      LOG.fatal(\"error parsing conf file: \" + e);\n      throw new RuntimeException(e);\n    } catch (SAXException e) {\n      LOG.fatal(\"error parsing conf file: \" + e);\n      throw new RuntimeException(e);\n    } catch (ParserConfigurationException e) {\n      LOG.fatal(\"error parsing conf file: \" + e);\n      throw new RuntimeException(e);\n    }\n  }"
        }
    },
    {
        "filename": "HADOOP-15411.json",
        "creation_time": "2018-04-24T23:18:39.000+0000",
        "bug_report": {
            "Title": "AuthenticationFilter should use Configuration.getPropsWithPrefix instead of iterator",
            "Description": "The Node Manager startup fails due to a `YarnRuntimeException` caused by a `ConcurrentModificationException` during the initialization of the HTTP server in Hadoop YARN. The issue arises from concurrent access to a `HashMap` in the `getFilterConfigMap` method of the `AuthenticationFilterInitializer` class, which is not thread-safe. This leads to failures in starting the NMWebapps, as indicated by the stack trace.",
            "StackTrace": [
                "2018-04-19 13:08:30,638 ERROR nodemanager.NodeManager (NodeManager.java:initAndStartNodeManager(921)) - Error starting NodeManager",
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.",
                "at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:117)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                "at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:919)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:979)",
                "Caused by: org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server",
                "at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:377)",
                "at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:424)",
                "at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:420)",
                "at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:112)",
                "... 5 more",
                "Caused by: java.io.IOException: java.util.ConcurrentModificationException",
                "at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:532)",
                "at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:117)",
                "at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:421)",
                "at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:333)",
                "... 8 more",
                "Caused by: java.util.ConcurrentModificationException",
                "at java.util.Hashtable$Enumerator.next(Hashtable.java:1383)",
                "at org.apache.hadoop.conf.Configuration.iterator(Configuration.java:2853)",
                "at org.apache.hadoop.security.AuthenticationFilterInitializer.getFilterConfigMap(AuthenticationFilterInitializer.java:73)",
                "at org.apache.hadoop.http.HttpServer2.getFilterProperties(HttpServer2.java:647)",
                "at org.apache.hadoop.http.HttpServer2.constructSecretProvider(HttpServer2.java:637)",
                "at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:525)",
                "... 11 more"
            ],
            "RootCause": "The root cause of the issue is a ConcurrentModificationException that occurs when multiple threads access the filter configuration map concurrently in the getFilterConfigMap method of the AuthenticationFilterInitializer class.",
            "StepsToReproduce": [
                "Start the Node Manager in a multi-threaded environment.",
                "Ensure that multiple threads attempt to access the filter configuration concurrently."
            ],
            "ExpectedBehavior": "The Node Manager should start successfully without throwing any exceptions, and the NMWebapps should be operational.",
            "ObservedBehavior": "The Node Manager fails to start, throwing a YarnRuntimeException due to a ConcurrentModificationException.",
            "Suggestions": "Consider synchronizing access to the Configuration object or using a thread-safe collection like ConcurrentHashMap to prevent concurrent modifications.",
            "problem_location": {
                "files": [
                    "AuthenticationFilterInitializer.java",
                    "HttpServer2.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.AuthenticationFilterInitializer",
                    "org.apache.hadoop.http.HttpServer2"
                ],
                "methods": [
                    "AuthenticationFilterInitializer.getFilterConfigMap",
                    "HttpServer2.getFilterProperties"
                ]
            },
            "possible_fix": "Modify the getFilterConfigMap method to use a ConcurrentHashMap instead of a HashMap for filterConfig to ensure thread safety."
        },
        "possible_fix_code": {
            "org.apache.hadoop.security.AuthenticationFilterInitializer.getFilterConfigMap": "  public static Map<String, String> getFilterConfigMap(Configuration conf,\n      String prefix) {\n    Map<String, String> filterConfig = new ConcurrentHashMap<String, String>();\n\n    //setting the cookie path to root \"/\" so it is used for all resources.\n    filterConfig.put(AuthenticationFilter.COOKIE_PATH, \"/\");\n\n    for (Map.Entry<String, String> entry : conf) {\n      String name = entry.getKey();\n      if (name.startsWith(prefix)) {\n        String value = conf.get(name);\n        name = name.substring(prefix.length());\n        filterConfig.put(name, value);\n      }\n    }\n\n    //Resolve _HOST into bind address\n    String bindAddress = conf.get(HttpServer2.BIND_ADDRESS);\n    String principal = filterConfig.get(KerberosAuthenticationHandler.PRINCIPAL);\n    if (principal != null) {\n      try {\n        principal = SecurityUtil.getServerPrincipal(principal, bindAddress);\n      }\n      catch (IOException ex) {\n        throw new RuntimeException(\"Could not resolve Kerberos principal name: \" + ex.toString(), ex);\n      }\n      filterConfig.put(KerberosAuthenticationHandler.PRINCIPAL, principal);\n    }\n    return filterConfig;\n  }"
        }
    },
    {
        "filename": "HADOOP-15850.json",
        "creation_time": "2018-10-13T14:24:29.000+0000",
        "bug_report": {
            "Title": "CopyCommitter#concatFileChunks should check that the blocks per chunk is not 0",
            "Description": "The issue arises during the execution of the `TestIncrementalBackupWithBulkLoad` test case in HBase against Hadoop 3.1.1. The `BackupDistCp` job creates a listing file for two bulk-loaded HFiles. However, when the `CopyCommitter#concatFileChunks` method is invoked, it throws an `IOException` indicating an inconsistency in the sequence file due to mismatched chunk file lengths. This inconsistency occurs despite the expectation that the two HFiles are independent and should not cause such an error.",
            "StackTrace": [
                "java.io.IOException: Inconsistent sequence file: current chunk file org.apache.hadoop.tools.CopyListingFileStatus@bb8826ee length = 5100 doesn't match prior entry org.apache.hadoop.tools.CopyListingFileStatus@243d544d length = 5142",
                "at org.apache.hadoop.tools.mapred.CopyCommitter.concatFileChunks(CopyCommitter.java:276)",
                "at org.apache.hadoop.tools.mapred.CopyCommitter.commitJob(CopyCommitter.java:100)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:567)"
            ],
            "RootCause": "The root cause of the issue is that the `concatFileChunks` method in the `CopyCommitter` class does not validate the chunk files before concatenation, leading to an inconsistency when the lengths of the chunk files differ.",
            "StepsToReproduce": [
                "Run the `TestIncrementalBackupWithBulkLoad` test case in HBase against Hadoop 3.1.1.",
                "Ensure that two bulk-loaded HFiles are present in the listing.",
                "Observe the execution of the `BackupDistCp` job and the subsequent invocation of `CopyCommitter#concatFileChunks`."
            ],
            "ExpectedBehavior": "The `CopyCommitter#concatFileChunks` method should successfully concatenate the chunk files without throwing an `IOException`, as the two bulk-loaded HFiles are independent.",
            "ObservedBehavior": "An `IOException` is thrown indicating an inconsistency in the sequence file due to mismatched lengths of the chunk files being concatenated.",
            "Suggestions": "To resolve this issue, implement validation of chunk files in the `concatFileChunks` method to ensure that their lengths and attributes are consistent before concatenation. Additionally, review the handling of temporary files to ensure they are properly cleaned up.",
            "problem_location": {
                "files": [
                    "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java"
                ],
                "classes": [
                    "org.apache.hadoop.tools.mapred.CopyCommitter"
                ],
                "methods": [
                    "CopyCommitter.concatFileChunks",
                    "CopyCommitter.commitJob"
                ]
            },
            "possible_fix": "Add a validation check in the `concatFileChunks` method to compare the lengths of the chunk files before concatenation. If the lengths differ, log an error and throw an appropriate exception to prevent the concatenation from proceeding."
        },
        "possible_fix_code": {
            "CopyCommitter.concatFileChunks": "  private void concatFileChunks(Configuration conf, Path targetFile,\n      LinkedList<Path> allChunkPaths) throws IOException {\n    if (allChunkPaths.size() == 1) {\n      return;\n    }\n    // Validate chunk file lengths\n    FileSystem dstfs = targetFile.getFileSystem(conf);\n    long firstChunkLength = dstfs.getFileStatus(allChunkPaths.get(0)).getLen();\n    for (Path chunkPath : allChunkPaths) {\n      long chunkLength = dstfs.getFileStatus(chunkPath).getLen();\n      if (chunkLength != firstChunkLength) {\n        throw new IOException(\"Inconsistent chunk file lengths: \" +\n            \"first chunk length = \" + firstChunkLength + \", current chunk length = \" + chunkLength);\n      }\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat \" + targetFile + \" allChunkSize: \" + allChunkPaths.size());\n    }\n\n    Path firstChunkFile = allChunkPaths.removeFirst();\n    Path[] restChunkFiles = new Path[allChunkPaths.size()];\n    allChunkPaths.toArray(restChunkFiles);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat: firstchunk: \" + dstfs.getFileStatus(firstChunkFile));\n      int i = 0;\n      for (Path f : restChunkFiles) {\n        LOG.debug(\"concat: other chunk: \" + i + \": \" + dstfs.getFileStatus(f));\n        ++i;\n      }\n    }\n    dstfs.concat(firstChunkFile, restChunkFiles);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat: result: \" + dstfs.getFileStatus(firstChunkFile));\n    }\n    rename(dstfs, firstChunkFile, targetFile);\n  }"
        }
    },
    {
        "filename": "HADOOP-11693.json",
        "creation_time": "2015-03-05T23:19:13.000+0000",
        "bug_report": {
            "Title": "Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving.",
            "Description": "A production HBase cluster experienced periodic throttling by Azure storage during the archiving of old Write-Ahead Logs (WALs). This throttling led to the HMaster aborting the region server and attempting a restart. However, due to ongoing throttling, subsequent distributed log splitting failed, resulting in the hbase:meta table being offline on the affected region server, which ultimately caused the entire cluster to enter a bad state. The stack trace indicates multiple exceptions, primarily an AzureException stating that the server is busy, which is critical for understanding the failure in rename operations necessary for log archiving.",
            "StackTrace": [
                "2015-03-01 18:36:45,623 ERROR org.apache.hadoop.hbase.master.HMaster: Region server workernode4.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845421044 reported a fatal error: ABORTING region server workernode4.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845421044: IOE in log roller",
                "Caused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.",
                "at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)",
                "at org.apache.hadoop.hbase.regionserver.wal.FSHLog.archiveLogFile(FSHLog.java:798)",
                "at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)",
                "Caused by: com.microsoft.windowsazure.storage.StorageException: The server is busy."
            ],
            "RootCause": "The root cause of the issue is the Azure storage service being unable to handle rename operations due to throttling, which is exacerbated by the high cost of copying blobs during the rename process. This leads to failures in critical operations like log archiving and splitting.",
            "StepsToReproduce": [
                "1. Set up an HBase cluster configured to use Azure storage for WAL archiving.",
                "2. Trigger the archiving of old WALs while monitoring Azure storage performance.",
                "3. Observe the throttling behavior and the resulting errors in the HMaster logs."
            ],
            "ExpectedBehavior": "The HBase cluster should successfully archive old WALs without encountering throttling issues, allowing for seamless log splitting and maintaining the availability of the hbase:meta table.",
            "ObservedBehavior": "The HBase cluster experiences throttling from Azure storage, leading to aborted region servers, failed log splitting, and the hbase:meta table going offline, resulting in a bad state for the entire cluster.",
            "Suggestions": "Implement a more robust exponential backoff retry strategy for the rename operations in the AzureNativeFileSystemStore to handle transient throttling more effectively. Additionally, consider optimizing the blob copy process to reduce the likelihood of hitting Azure storage limits.",
            "problem_location": {
                "files": [
                    "hadoop-tools.hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"
                ],
                "methods": [
                    "rename",
                    "checkContainer",
                    "getBlobReference"
                ]
            },
            "possible_fix": "Modify the rename method in AzureNativeFileSystemStore to include enhanced retry logic for handling StorageException when the server is busy. For example, increase the maximum retry duration and implement a backoff strategy to reduce the frequency of requests during throttling periods."
        },
        "possible_fix_code": {
            "rename": "  public void rename(String srcKey, String dstKey, boolean acquireLease,\n      SelfRenewingLease existingLease) throws IOException {\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Moving \" + srcKey + \" to \" + dstKey);\n    }\n\n    if (acquireLease && existingLease != null) {\n      throw new IOException(\"Cannot acquire new lease if one already exists.\");\n    }\n\n    int maxRetries = 5;\n    int retryCount = 0;\n    long backoffTime = 1000; // Initial backoff time in milliseconds\n\n    while (true) {\n      try {\n        // Attempts rename may occur before opening any streams so first,\n        // check if a session exists, if not create a session with the Azure\n        // storage server.\n        if (null == storageInteractionLayer) {\n          final String errMsg = String.format(\n              \"Storage session expected for URI '%s' but does not exist.\",\n              sessionUri);\n          throw new AssertionError(errMsg);\n        }\n\n        checkContainer(ContainerAccessType.ReadThenWrite);\n        // Get the source blob and assert its existence. If the source key\n        // needs to be normalized then normalize it.\n        CloudBlobWrapper srcBlob = getBlobReference(srcKey);\n\n        if (!srcBlob.exists(getInstrumentedContext())) {\n          throw new AzureException (\"Source blob \" + srcKey +\n              \" does not exist.\");\n        }\n\n        SelfRenewingLease lease = null;\n        if (acquireLease) {\n          lease = srcBlob.acquireLease();\n        } else if (existingLease != null) {\n          lease = existingLease;\n        }\n\n        CloudBlobWrapper dstBlob = getBlobReference(dstKey);\n        URI srcUri = new URI(srcBlob.getUri().toASCIIString());\n\n        dstBlob.startCopyFromBlob(srcUri, getInstrumentedContext());\n        waitForCopyToComplete(dstBlob, getInstrumentedContext());\n\n        safeDelete(srcBlob, lease);\n        break; // Exit the loop if successful\n\n      } catch (AzureException e) {\n        if (e.getCause() instanceof StorageException && \n            ((StorageException) e.getCause()).getErrorCode().equals(\"ServerBusy\")) {\n          if (retryCount < maxRetries) {\n            retryCount++;\n            try {\n              Thread.sleep(backoffTime);\n            } catch (InterruptedException ie) {\n              Thread.currentThread().interrupt();\n              throw new IOException(\"Thread interrupted during backoff\", ie);\n            }\n            backoffTime *= 2; // Exponential backoff\n          } else {\n            throw e; // Rethrow after max retries\n          }\n        } else {\n          throw e; // Rethrow if it's not a server busy error\n        }\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HADOOP-12441.json",
        "creation_time": "2015-09-25T23:26:43.000+0000",
        "bug_report": {
            "Title": "Fix kill command behavior under some Linux distributions.",
            "Description": "After the implementation of HADOOP-12317, the behavior of the kill command has been adversely affected on Ubuntu 12. The NodeManager (NM) fails to determine the status of a process using its PID, leading to an inability to correctly terminate processes when instructed by the ResourceManager (RM) or ApplicationMaster (AM). The logs indicate that the command being executed contains an invalid process ID ('--'), which is causing the failure.",
            "StackTrace": [
                "2015-09-25 21:58:59,348 INFO  nodemanager.DefaultContainerExecutor (DefaultContainerExecutor.java:containerIsAlive(431)) -  ================== check alive cmd:[[Ljava.lang.String;@496e442d]",
                "2015-09-25 21:58:59,349 INFO  nodemanager.NMAuditLogger (NMAuditLogger.java:logSuccess(89)) - USER=hrt_qa       IP=10.0.1.14    OPERATION=Stop Container Request        TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1443218269460_0001    CONTAINERID=container_1443218269460_0001_01_000001",
                "ExitCodeException exitCode=1: ERROR: garbage process ID '--'.",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:550)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:461)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:727)",
                "at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.containerIsAlive(DefaultContainerExecutor.java:432)",
                "at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.signalContainer(DefaultContainerExecutor.java:401)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:419)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:139)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:55)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the process ID (PID) being passed to the kill command is invalid ('--'). This indicates a failure in the code responsible for setting or retrieving the PID in the ContainerSignalContext or ContainerLivenessContext, particularly in the signalContainer method of DefaultContainerExecutor.",
            "StepsToReproduce": [
                "1. Deploy the application on Ubuntu 12.",
                "2. Start a container using the NodeManager.",
                "3. Attempt to stop the container using the kill command.",
                "4. Observe the logs for the error message regarding the garbage process ID."
            ],
            "ExpectedBehavior": "The kill command should successfully terminate the specified container process when invoked by the NodeManager, without any errors related to invalid process IDs.",
            "ObservedBehavior": "The kill command fails with an error indicating a garbage process ID ('--'), preventing the termination of the container process.",
            "Suggestions": "Investigate the instantiation and setting of the PID in the ContainerSignalContext and ContainerLivenessContext classes. Ensure that the PID is correctly assigned before it is used in the signalContainer method.",
            "problem_location": {
                "files": [
                    "DefaultContainerExecutor.java",
                    "ContainerSignalContext.java",
                    "ContainerLivenessContext.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor",
                    "org.apache.hadoop.yarn.server.nodemanager.executor.ContainerSignalContext",
                    "org.apache.hadoop.yarn.server.nodemanager.executor.ContainerLivenessContext"
                ],
                "methods": [
                    "DefaultContainerExecutor.signalContainer",
                    "ContainerSignalContext.Builder.setPid",
                    "ContainerLivenessContext.Builder.setPid"
                ]
            },
            "possible_fix": "Ensure that the PID is correctly set in the ContainerSignalContext and ContainerLivenessContext before invoking the signalContainer method. This may involve adding validation checks or default values to prevent invalid PIDs from being passed."
        },
        "possible_fix_code": {
            "DefaultContainerExecutor.signalContainer": "  public boolean signalContainer(ContainerSignalContext ctx)\n      throws IOException {\n    String user = ctx.getUser();\n    String pid = ctx.getPid();\n    Signal signal = ctx.getSignal();\n\n    // Validate the PID before proceeding\n    if (pid == null || pid.isEmpty() || pid.equals(\"--\")) {\n      LOG.error(\"Invalid PID: \" + pid);\n      return false;\n    }\n\n    LOG.debug(\"Sending signal \" + signal.getValue() + \" to pid \" + pid\n        + \" as user \" + user);\n    if (!containerIsAlive(pid)) {\n      return false;\n    }\n    try {\n      killContainer(pid, signal);\n    } catch (IOException e) {\n      if (!containerIsAlive(pid)) {\n        return false;\n      }\n      throw e;\n    }\n    return true;\n  }"
        }
    },
    {
        "filename": "HADOOP-11685.json",
        "creation_time": "2015-03-06T22:00:14.000+0000",
        "bug_report": {
            "Title": "StorageException complaining 'no lease ID' during HBase distributed log splitting",
            "Description": "During the HBase distributed log splitting process, multiple threads attempt to access the same folder named 'recovered.edits'. This concurrent access leads to a situation where Azure Blob Storage raises a 'StorageException' due to an active lease on the blob, which prevents modifications without a lease ID. The issue is exacerbated by the fact that several parts of the WASB code do not properly acquire a lease before interacting with Azure storage, resulting in null being passed to the storage API. This behavior is similar to the issue described in HADOOP-11523 but occurs in a different context.",
            "StackTrace": [
                "2015-02-26 03:21:28,871 WARN org.apache.hadoop.hbase.regionserver.SplitLogWorker: log splitting of WALs/workernode4.xxx.g6.internal.cloudapp.net,60020,1422071058425-splitting/workernode4.xxx.g6.internal.cloudapp.net%2C60020%2C1422071058425.1424914216773 failed, returning error",
                "java.io.IOException: org.apache.hadoop.fs.azure.AzureException: java.io.IOException",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.checkForErrors(HLogSplitter.java:633)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.access$000(HLogSplitter.java:121)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.finishWriting(HLogSplitter.java:964)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.finishWritingAndClose(HLogSplitter.java:1019)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:359)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:223)",
                "at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:142)",
                "at org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.process(HLogSplitterHandler.java:79)",
                "at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: org.apache.hadoop.fs.azure.AzureException: java.io.IOException",
                "at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.storeEmptyFolder(AzureNativeFileSystemStore.java:1477)",
                "at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.mkdirs(NativeAzureFileSystem.java:1862)",
                "at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.mkdirs(NativeAzureFileSystem.java:1812)",
                "at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1815)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getRegionSplitEditsPath(HLogSplitter.java:502)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.createWAP(HLogSplitter.java:1211)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.getWriterAndPath(HLogSplitter.java:1200)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.append(HLogSplitter.java:1243)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.writeBuffer(HLogSplitter.java:851)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.doRun(HLogSplitter.java:843)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.run(HLogSplitter.java:813)",
                "Caused by: java.io.IOException",
                "at com.microsoft.windowsazure.storage.core.Utility.initIOException(Utility.java:493)",
                "at com.microsoft.windowsazure.storage.blob.BlobOutputStream.close(BlobOutputStream.java:282)",
                "at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.storeEmptyFolder(AzureNativeFileSystemStore.java:1472)",
                "... 10 more",
                "Caused by: com.microsoft.windowsazure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.",
                "at com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)",
                "at com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)",
                "at com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)",
                "at com.microsoft.windowsazure.storage.blob.CloudBlockBlob.commitBlockList(CloudBlockBlob.java:248)",
                "at com.microsoft.windowsazure.storage.blob.BlobOutputStream.commit(BlobOutputStream.java:319)",
                "at com.microsoft.windowsazure.storage.blob.BlobOutputStream.close(BlobOutputStream.java:279)",
                "... 11 more"
            ],
            "RootCause": "The root cause of the issue is that multiple threads are trying to access the same Azure Blob Storage resource without acquiring the necessary lease, leading to a 'StorageException' indicating that a lease is active on the blob.",
            "StepsToReproduce": [
                "Initiate HBase distributed log splitting with multiple threads accessing the 'recovered.edits' folder.",
                "Ensure that the Azure Blob Storage resource being accessed has an active lease.",
                "Observe the logs for 'StorageException' related to lease ID issues."
            ],
            "ExpectedBehavior": "The system should successfully split logs without encountering lease-related exceptions, allowing multiple threads to access the 'recovered.edits' folder concurrently.",
            "ObservedBehavior": "The system fails to split logs, returning a 'StorageException' due to an active lease on the blob, indicating that no lease ID was specified in the request.",
            "Suggestions": "Implement proper lease acquisition mechanisms in the WASB code to ensure that a lease is obtained before accessing Azure Blob Storage resources. Review the code for potential race conditions and ensure that lease management is handled correctly.",
            "problem_location": {
                "files": [
                    "HLogSplitter.java",
                    "AzureNativeFileSystemStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.hbase.regionserver.wal.HLogSplitter",
                    "org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore"
                ],
                "methods": [
                    "HLogSplitter.checkForErrors",
                    "HLogSplitter.splitLogFile",
                    "AzureNativeFileSystemStore.storeEmptyFolder"
                ]
            },
            "possible_fix": "Add lease acquisition logic in the HLogSplitter methods before any operations that modify or delete blobs in Azure Blob Storage. Ensure that the lease ID is passed correctly in all relevant storage operations."
        },
        "possible_fix_code": {
            "HLogSplitter.splitLogFile": "public void splitLogFile(Path logFile) throws IOException {\n    // Acquire lease before proceeding with log file operations\n    String leaseId = acquireLease(logFile);\n    if (leaseId == null) {\n        throw new IOException(\"Failed to acquire lease for log file: \" + logFile);\n    }\n    try {\n        // Existing logic for splitting log file\n        // ... (existing code) ...\n    } finally {\n        // Release the lease after operations are complete\n        releaseLease(logFile, leaseId);\n    }\n}\n\nprivate String acquireLease(Path logFile) throws IOException {\n    // Logic to acquire lease from Azure Blob Storage\n    // This is a placeholder for the actual lease acquisition logic\n    // Return the lease ID if successful, or null if failed\n    return \"lease-id-placeholder\"; // Replace with actual lease acquisition code\n}\n\nprivate void releaseLease(Path logFile, String leaseId) throws IOException {\n    // Logic to release the lease on the Azure Blob Storage\n    // This is a placeholder for the actual lease release logic\n    // Implement the necessary API calls to release the lease\n}"
        }
    },
    {
        "filename": "HADOOP-8589.json",
        "creation_time": "2012-07-11T23:27:00.000+0000",
        "bug_report": {
            "Title": "ViewFs tests fail when tests and home dirs are nested",
            "Description": "The `TestFSMainOperationsLocalFileSystem` test fails when the test root directory is located within the user's home directory, specifically when the home directory is more than two levels deep from the root (`/`). This issue arises in a default 1-node installation of Jenkins. The failure occurs due to an attempt to create a symbolic link at a path (`/var`) that already exists as a directory, leading to a `FileAlreadyExistsException`. The relevant stack trace indicates that the error originates from the `createLink` method in the `InodeTree` class, which is designed to prevent link creation at existing paths.",
            "StackTrace": [
                "org.apache.hadoop.fs.FileAlreadyExistsException: Path /var already exists as dir; cannot create link here",
                "at org.apache.hadoop.fs.viewfs.InodeTree.createLink(InodeTree.java:244)",
                "at org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:334)",
                "at org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:167)",
                "at org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:167)",
                "at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2094)",
                "at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:79)",
                "at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2128)",
                "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2110)",
                "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290)",
                "at org.apache.hadoop.fs.viewfs.ViewFileSystemTestSetup.setupForViewFileSystem(ViewFileSystemTestSetup.java:76)",
                "at org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem.setUp(TestFSMainOperationsLocalFileSystem.java:40)"
            ],
            "RootCause": "The root cause of the failure is that the `createLink` method in the `InodeTree` class attempts to create a symbolic link at a path (`/var`) that already exists as a directory. The method checks for existing paths and throws a `FileAlreadyExistsException` when it detects that the target path is already occupied.",
            "StepsToReproduce": [
                "Set up a default 1-node installation of Jenkins.",
                "Create a test root directory under the user's home directory that is more than two levels deep.",
                "Run the `TestFSMainOperationsLocalFileSystem` test."
            ],
            "ExpectedBehavior": "The test should complete successfully without throwing any exceptions, indicating that the symbolic links are created correctly without conflicts.",
            "ObservedBehavior": "The test fails with a `FileAlreadyExistsException`, indicating that the path `/var` already exists as a directory, preventing the creation of a new link.",
            "Suggestions": "To resolve this issue, ensure that the target path for the link does not already exist as a directory. This may involve modifying the test setup to use a different path or adjusting the logic in the `createLink` method to handle existing directories more gracefully.",
            "problem_location": {
                "files": [
                    "InodeTree.java",
                    "ViewFileSystem.java",
                    "TestFSMainOperationsLocalFileSystem.java"
                ],
                "classes": [
                    "org.apache.hadoop.fs.viewfs.InodeTree",
                    "org.apache.hadoop.fs.viewfs.ViewFileSystem",
                    "org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem"
                ],
                "methods": [
                    "InodeTree.createLink",
                    "ViewFileSystem.initialize",
                    "TestFSMainOperationsLocalFileSystem.setUp"
                ]
            },
            "possible_fix": "Modify the `createLink` method in `InodeTree` to check for existing directories and either skip link creation or choose an alternative path. For example, add logic to handle cases where the target path already exists as a directory."
        },
        "possible_fix_code": {
            "createLink": "  private void createLink(final String src, final String target,\n      final boolean isLinkMerge, final UserGroupInformation aUgi)\n      throws URISyntaxException, IOException,\n    FileAlreadyExistsException, UnsupportedFileSystemException {\n    // Validate that src is valid absolute path\n    final Path srcPath = new Path(src); \n    if (!srcPath.isAbsoluteAndSchemeAuthorityNull()) {\n      throw new IOException(\"ViewFs:Non absolute mount name in config:\" + src);\n    }\n \n    final String[] srcPaths = breakIntoPathComponents(src);\n    INodeDir<T> curInode = root;\n    int i;\n    // Ignore first initial slash, process all except last component\n    for (i = 1; i < srcPaths.length-1; i++) {\n      final String iPath = srcPaths[i];\n      INode<T> nextInode = curInode.resolveInternal(iPath);\n      if (nextInode == null) {\n        INodeDir<T> newDir = curInode.addDir(iPath, aUgi);\n        newDir.InodeDirFs = getTargetFileSystem(newDir);\n        nextInode = newDir;\n      }\n      if (nextInode instanceof INodeLink) {\n        // Error - expected a dir but got a link\n        throw new FileAlreadyExistsException(\"Path \" + nextInode.fullPath +\n            \" already exists as link\");\n      } else {\n        assert(nextInode instanceof INodeDir);\n        curInode = (INodeDir<T>) nextInode;\n      }\n    }\n    \n    // Now process the last component\n    // Check if the last component already exists as a directory\n    String iPath = srcPaths[i]; // last component\n    if (curInode.resolveInternal(iPath) != null) {\n      // directory/link already exists\n      StringBuilder strB = new StringBuilder(srcPaths[0]);\n      for (int j = 1; j <= i; ++j) {\n        strB.append('/').append(srcPaths[j]);\n      }\n      throw new FileAlreadyExistsException(\"Path \" + strB +\n            \" already exists as dir; cannot create link here\");\n    }\n    \n    // If the path exists as a directory, we can choose to skip link creation or handle it differently\n    // For example, we could log a warning and return without creating the link\n    if (curInode.isDirectory(iPath)) {\n      System.out.println(\"Warning: Path \" + iPath + \" already exists as a directory. Skipping link creation.\");\n      return;\n    }\n    \n    final INodeLink<T> newLink;\n    final String fullPath = curInode.fullPath + (curInode == root ? \"\" : \"/\")\n        + iPath;\n    if (isLinkMerge) { // Target is list of URIs\n      String[] targetsList = StringUtils.getStrings(target);\n      URI[] targetsListURI = new URI[targetsList.length];\n      int k = 0;\n      for (String itarget : targetsList) {\n        targetsListURI[k++] = new URI(itarget);\n      }\n      newLink = new INodeLink<T>(fullPath, aUgi,\n          getTargetFileSystem(targetsListURI), targetsListURI);\n    } else {\n      newLink = new INodeLink<T>(fullPath, aUgi,\n          getTargetFileSystem(new URI(target)), new URI(target));\n    }\n    curInode.addLink(iPath, newLink);\n    mountPoints.add(new MountPoint<T>(src, newLink));\n  }"
        }
    },
    {
        "filename": "HADOOP-11754.json",
        "creation_time": "2015-03-26T05:22:54.000+0000",
        "bug_report": {
            "Title": "RM fails to start in non-secure mode due to authentication filter failure",
            "Description": "The ResourceManager (RM) fails to start in non-secure mode, resulting in a critical error related to the authentication filter. The stack trace indicates that the failure occurs due to an inability to read the signature secret file specified in the configuration. This issue is likely a regression introduced by HADOOP-10670, and it may stem from a misconfiguration or file access issue.",
            "StackTrace": [
                "2015-03-25 22:02:42,526 WARN org.mortbay.log: failed RMAuthenticationFilter: javax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret",
                "2015-03-25 22:02:42,526 WARN org.mortbay.log: Failed startup of context org.mortbay.jetty.webapp.WebAppContext@6de50b08{/,jar:file:/Users/sjlee/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-common-3.0.0-SNAPSHOT.jar!/webapps/cluster}",
                "javax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:266)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:225)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.init(DelegationTokenAuthenticationFilter.java:161)",
                "at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.init(RMAuthenticationFilter.java:53)",
                "at org.mortbay.jetty.servlet.FilterHolder.doStart(FilterHolder.java:97)",
                "at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)",
                "at org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:713)",
                "at org.mortbay.jetty.servlet.Context.startContext(Context.java:140)",
                "at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)",
                "at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)",
                "at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)",
                "at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)",
                "at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156)",
                "at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)",
                "at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)",
                "at org.mortbay.jetty.Server.doStart(Server.java:224)",
                "at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)",
                "at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:773)",
                "at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)",
                "Caused by: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret",
                "at org.apache.hadoop.security.authentication.util.FileSignerSecretProvider.init(FileSignerSecretProvider.java:59)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:264)",
                "... 23 more",
                "2015-03-25 22:02:42,538 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error starting ResourceManager",
                "org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server",
                "at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:279)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)",
                "Caused by: java.io.IOException: Problem in starting http server. Server handlers failed",
                "at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:785)",
                "at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)",
                "... 4 more"
            ],
            "RootCause": "The root cause of the issue is the failure to read the signature secret file located at '/Users/sjlee/hadoop-http-auth-signature-secret'. This failure can be attributed to several potential reasons, including the file not existing, permission issues, incorrect configuration, or file corruption.",
            "StepsToReproduce": [
                "Attempt to start the ResourceManager in non-secure mode.",
                "Ensure that the configuration points to the signature secret file.",
                "Verify the existence and accessibility of the file at the specified path."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully in non-secure mode without any errors related to the authentication filter.",
            "ObservedBehavior": "The ResourceManager fails to start, throwing a RuntimeException indicating that it could not read the signature secret file.",
            "Suggestions": "Check the specified file path for the signature secret file to ensure it exists and is accessible. Verify that the application has the necessary permissions to read the file. If the file is missing, create it or correct the configuration to point to the correct file path.",
            "problem_location": {
                "files": [
                    "FileSignerSecretProvider.java",
                    "AuthenticationFilter.java",
                    "RMAuthenticationFilter.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.authentication.util.FileSignerSecretProvider",
                    "org.apache.hadoop.security.authentication.server.AuthenticationFilter",
                    "org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter"
                ],
                "methods": [
                    "FileSignerSecretProvider.init",
                    "AuthenticationFilter.initializeSecretProvider",
                    "RMAuthenticationFilter.init"
                ]
            },
            "possible_fix": "Ensure the signature secret file exists at the specified path and is readable by the application. If necessary, update the configuration to point to the correct file path or create the file with appropriate content."
        },
        "possible_fix_code": {
            "FileSignerSecretProvider.init": "  public void init(Properties config, ServletContext servletContext,\n                   long tokenValidity) throws Exception {\n\n    String signatureSecretFile = config.getProperty(\n        AuthenticationFilter.SIGNATURE_SECRET_FILE, null);\n\n    if (signatureSecretFile == null) {\n        throw new RuntimeException(\"Signature secret file path is not configured.\");\n    }\n\n    Reader reader = null;\n    try {\n        reader = new InputStreamReader(\n            new FileInputStream(signatureSecretFile), Charsets.UTF_8);\n        StringBuilder sb = new StringBuilder();\n        int c = reader.read();\n        while (c > -1) {\n            sb.append((char) c);\n            c = reader.read();\n        }\n        secret = sb.toString().getBytes(Charset.forName(\"UTF-8\"));\n    } catch (FileNotFoundException ex) {\n        throw new RuntimeException(\"Signature secret file not found: \" + signatureSecretFile);\n    } catch (IOException ex) {\n        throw new RuntimeException(\"Could not read signature secret file: \" + signatureSecretFile);\n    } finally {\n        if (reader != null) {\n            try {\n                reader.close();\n            } catch (IOException e) {\n                // nothing to do\n            }\n        }\n    }\n\n    secrets = new byte[][]{secret};\n  }"
        }
    },
    {
        "filename": "HADOOP-8225.json",
        "creation_time": "2012-03-13T20:33:53.000+0000",
        "bug_report": {
            "Title": "DistCp fails when invoked by Oozie",
            "Description": "The issue arises when DistCp is executed through a proxy user, such as Oozie, leading to a failure in correctly picking up the delegation-token-store. This results in a SecurityException being thrown due to an intercepted call to System.exit(-999), which is not permitted in the Oozie environment. The stack trace indicates that the error originates from the LauncherSecurityManager, which prevents the application from terminating unexpectedly.",
            "StackTrace": [
                "ERROR [main] org.apache.hadoop.tools.DistCp: Couldn't complete DistCp operation:",
                "java.lang.SecurityException: Intercepted System.exit(-999)",
                "at org.apache.oozie.action.hadoop.LauncherSecurityManager.checkExit(LauncherMapper.java:651)",
                "at java.lang.Runtime.exit(Runtime.java:88)",
                "at java.lang.System.exit(System.java:904)",
                "at org.apache.hadoop.tools.DistCp.main(DistCp.java:357)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:394)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)"
            ],
            "RootCause": "The root cause of the issue is that the DistCp process is attempting to call System.exit(-999), which is intercepted by the LauncherSecurityManager in the Oozie environment, leading to a SecurityException. This occurs because the HADOOP_TOKEN_FILE_LOCATION is not being copied to the job's credentials, preventing proper execution.",
            "StepsToReproduce": [
                "Invoke DistCp through Oozie as a proxy user.",
                "Monitor the execution for any SecurityException related to System.exit."
            ],
            "ExpectedBehavior": "DistCp should execute successfully without throwing a SecurityException, and the delegation-token-store should be correctly utilized.",
            "ObservedBehavior": "The execution of DistCp fails with a SecurityException due to an intercepted System.exit call.",
            "Suggestions": "Ensure that HADOOP_TOKEN_FILE_LOCATION is correctly set in the job configuration to allow DistCp to access the necessary delegation tokens.",
            "problem_location": {
                "files": [
                    "DistCp.java",
                    "LauncherMapper.java"
                ],
                "classes": [
                    "org.apache.hadoop.tools.DistCp",
                    "org.apache.oozie.action.hadoop.LauncherMapper"
                ],
                "methods": [
                    "DistCp.main",
                    "LauncherMapper.map"
                ]
            },
            "possible_fix": "Modify the job configuration to include HADOOP_TOKEN_FILE_LOCATION in mapreduce.job.credentials.binary to ensure that the necessary tokens are available during execution."
        },
        "possible_fix_code": {
            "DistCp.main": "  public static void main(String argv[]) {\n    try {\n      DistCp distCp = new DistCp();\n      Cleanup CLEANUP = new Cleanup(distCp);\n\n      Runtime.getRuntime().addShutdownHook(CLEANUP);\n      int exitCode = ToolRunner.run(getDefaultConf(), distCp, argv);\n      // Instead of System.exit, we will handle the exit code gracefully\n      if (exitCode != 0) {\n        LOG.error(\"DistCp failed with exit code: \" + exitCode);\n        throw new RuntimeException(\"DistCp failed\");\n      }\n    }\n    catch (Exception e) {\n      LOG.error(\"Couldn't complete DistCp operation: \", e);\n      throw new RuntimeException(\"DistCp operation failed\", e);\n    }\n  }"
        }
    },
    {
        "filename": "HADOOP-10866.json",
        "creation_time": "2014-07-18T22:00:15.000+0000",
        "bug_report": {
            "Title": "RawLocalFileSystem fails to read symlink targets via the stat command when the format of stat command uses non-curly quotes",
            "Description": "The issue arises in the Hadoop project where the `TestSymlinkLocalFS.testDanglingLink` test case intermittently fails due to an `IOException`. This exception indicates that a path expected to be a symbolic link is not recognized as such. The failure has been observed in recent builds, specifically since build #7376. The test attempts to validate the behavior of symbolic links in the Hadoop filesystem, but it encounters a regular file instead of a symlink, leading to the test's failure.",
            "StackTrace": [
                "java.io.IOException: Path file:/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/linkToFile is not a symbolic link",
                "at org.apache.hadoop.fs.FileStatus.getSymlink(FileStatus.java:266)",
                "at org.apache.hadoop.fs.TestSymlinkLocalFS.testDanglingLink(TestSymlinkLocalFS.java:163)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)",
                "at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)",
                "at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)",
                "at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)",
                "at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)"
            ],
            "RootCause": "The root cause of the issue is that the `getSymlink` method in the `FileStatus` class throws an `IOException` when the specified path is not a symbolic link. This indicates that the test case `testDanglingLink` is failing because it expects a symlink at a specific path, but a regular file exists instead.",
            "StepsToReproduce": [
                "Run the test case `TestSymlinkLocalFS.testDanglingLink` in the Hadoop project.",
                "Ensure that the environment is set up to create symbolic links.",
                "Observe the output for any `IOException` indicating that the path is not a symbolic link."
            ],
            "ExpectedBehavior": "The test case should successfully identify the symbolic link and not throw an `IOException`, indicating that the path is indeed a symbolic link.",
            "ObservedBehavior": "The test case fails with an `IOException`, stating that the path is not a symbolic link, which suggests that the symbolic link was not created or recognized correctly.",
            "Suggestions": "Investigate the implementation of the `createSymlink` method in the `FileSystem` class to ensure that symbolic links are being created correctly. Additionally, check the filesystem configuration to confirm that it supports symbolic links.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileStatus.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java"
                ],
                "classes": [
                    "org.apache.hadoop.fs.FileStatus",
                    "org.apache.hadoop.fs.FileSystem",
                    "org.apache.hadoop.fs.TestSymlinkLocalFS"
                ],
                "methods": [
                    "FileStatus.getSymlink",
                    "FileSystem.createSymlink",
                    "TestSymlinkLocalFS.testDanglingLink"
                ]
            },
            "possible_fix": "Ensure that the `createSymlink` method in the `FileSystem` class is properly implemented to support symbolic links. If the filesystem does not support symlinks, consider modifying the test case to handle such scenarios gracefully."
        },
        "possible_fix_code": {
            "org.apache.hadoop.fs.FileSystem.createSymlink": "  public void createSymlink(final Path target, final Path link,\n      final boolean createParent) throws AccessControlException,\n      FileAlreadyExistsException, FileNotFoundException,\n      ParentNotDirectoryException, UnsupportedFileSystemException, \n      IOException {\n    // Check if the filesystem supports symlinks\n    if (!supportsSymlinks()) {\n        throw new UnsupportedOperationException(\n            \"Filesystem does not support symlinks!\");\n    }\n    // Create parent directories if needed\n    if (createParent) {\n        Path parent = link.getParent();\n        if (parent != null) {\n            mkdirs(parent);\n        }\n    }\n    // Actual symlink creation logic\n    // This is a placeholder for the actual implementation\n    // In a real implementation, you would add the logic to create the symlink here\n    // For example, using native calls or filesystem-specific APIs\n    // Assuming a successful symlink creation, you would log or handle it accordingly\n  }"
        }
    },
    {
        "filename": "HADOOP-12089.json",
        "creation_time": "2015-06-15T17:34:46.000+0000",
        "bug_report": {
            "Title": "StorageException complaining 'no lease ID' when updating FolderLastModifiedTime in WASB",
            "Description": "The issue arises when HBase attempts to delete old Write-Ahead Logs (WALs) and update the /hbase/oldWALs folder in Azure Storage. The error message indicates that a lease is currently held on the blob, and the operation fails because no lease ID is provided. This is similar to the issue documented in HADOOP-11523, which also involves lease management in Azure Storage during distributed log splitting.",
            "StackTrace": [
                "2015-06-10 08:11:40,636 WARN org.apache.hadoop.hbase.master.cleaner.CleanerChore: Error while deleting: wasb://basecus1-1@basestoragecus1.blob.core.windows.net/hbase/oldWALs/workernode10.dthbasecus1.g1.internal.cloudapp.net%2C60020%2C1433908062461.1433921692855",
                "org.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2602)",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2613)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete(NativeAzureFileSystem.java:1505)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete(NativeAzureFileSystem.java:1437)",
                "at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteFiles(CleanerChore.java:256)",
                "at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:157)",
                "at org.apache.hadoop.hbase.master.cleaner.CleanerChore.chore(CleanerChore.java:124)",
                "at org.apache.hadoop.hbase.Chore.run(Chore.java:80)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.",
                "at com.microsoft.azure.storage.StorageException.translateException(StorageException.java:162)",
                "at com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:307)",
                "at com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:177)",
                "at com.microsoft.azure.storage.blob.CloudBlob.uploadProperties(CloudBlob.java:2991)",
                "at org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobWrapperImpl.uploadProperties(StorageInterfaceImpl.java:372)",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2597)"
            ],
            "RootCause": "The root cause of the issue is that the operation to update the folder's last modified time is being attempted on a blob that is currently leased by another process. The operation fails because it does not provide a lease ID, which is required to modify or delete the blob.",
            "StepsToReproduce": [
                "1. Attempt to delete old WALs in HBase while they are still leased.",
                "2. Observe the error message indicating that no lease ID was specified."
            ],
            "ExpectedBehavior": "The system should successfully update the last modified time of the folder or delete the blob if the lease is properly managed.",
            "ObservedBehavior": "The operation fails with a StorageException indicating that there is a lease on the blob and no lease ID was specified.",
            "Suggestions": "To resolve the issue, ensure that a lease is acquired on the blob before attempting to update or delete it. Use the `acquireLease` method from the `AzureNativeFileSystemStore` class to obtain a lease and pass it to the `updateFolderLastModifiedTime` method.",
            "problem_location": {
                "files": [
                    "hadoop-tools.hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"
                ],
                "methods": [
                    "updateFolderLastModifiedTime",
                    "acquireLease"
                ]
            },
            "possible_fix": "Before calling `updateFolderLastModifiedTime`, acquire a lease on the blob using the `acquireLease` method. Example code:\n\n```java\nSelfRenewingLease lease = azureNativeFileSystemStore.acquireLease(blobKey);\nazureNativeFileSystemStore.updateFolderLastModifiedTime(folderKey, lease);\n```"
        },
        "possible_fix_code": {
            "updateFolderLastModifiedTime": "  public void updateFolderLastModifiedTime(String key) throws AzureException {\n    SelfRenewingLease lease = acquireLease(key);\n    final Calendar lastModifiedCalendar = Calendar\n        .getInstance(Utility.LOCALE_US);\n    lastModifiedCalendar.setTimeZone(Utility.UTC_ZONE);\n    Date lastModified = lastModifiedCalendar.getTime();\n    updateFolderLastModifiedTime(key, lastModified, lease);\n  }"
        }
    },
    {
        "filename": "HADOOP-11934.json",
        "creation_time": "2015-05-07T00:38:53.000+0000",
        "bug_report": {
            "Title": "Use of JavaKeyStoreProvider in LdapGroupsMapping causes infinite loop",
            "Description": "The issue arises when using the LdapGroupsMapping code in conjunction with the JavaKeyStoreProvider, leading to an infinite loop scenario. The stack trace indicates that the application repeatedly calls the Path.getFileSystem() method, which eventually leads back to the JavaKeyStoreProvider constructor. This cycle continues until a stack overflow occurs, terminating the loop. The problem seems to stem from the interaction between the credential provider and the file system retrieval process.",
            "StackTrace": [
                "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)",
                "at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)",
                "at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:88)",
                "at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:65)",
                "at org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:291)",
                "at org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:58)",
                "at org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:1863)",
                "at org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:1843)",
                "at org.apache.hadoop.security.LdapGroupsMapping.getPassword(LdapGroupsMapping.java:386)",
                "at org.apache.hadoop.security.LdapGroupsMapping.setConf(LdapGroupsMapping.java:349)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.security.Groups.<init>(Groups.java:70)",
                "at org.apache.hadoop.security.Groups.<init>(Groups.java:66)",
                "at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)",
                "at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)",
                "at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)",
                "at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:804)",
                "at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)",
                "at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)",
                "at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2753)",
                "at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2745)",
                "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2611)"
            ],
            "RootCause": "The root cause of the infinite loop is the recursive call between the JavaKeyStoreProvider's constructor and the Path.getFileSystem() method. This occurs when the JavaKeyStoreProvider attempts to retrieve the file system configuration, which leads back to the same method call, creating a cycle.",
            "StepsToReproduce": [
                "1. Configure the LdapGroupsMapping to use the JavaKeyStoreProvider.",
                "2. Attempt to retrieve user group information using the configured LdapGroupsMapping.",
                "3. Observe the stack trace indicating repeated calls to Path.getFileSystem() and the JavaKeyStoreProvider constructor."
            ],
            "ExpectedBehavior": "The system should successfully retrieve user group information without entering an infinite loop, allowing for proper authentication and authorization processes.",
            "ObservedBehavior": "The application enters an infinite loop, resulting in a stack overflow error due to repeated calls between the JavaKeyStoreProvider and the Path.getFileSystem() method.",
            "Suggestions": "Review the configuration of the JavaKeyStoreProvider and LdapGroupsMapping to ensure they do not create a circular dependency. Consider refactoring the code to break the cycle or implement safeguards against such recursive calls.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/JavaKeyStoreProvider.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/LdapGroupsMapping.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.alias.JavaKeyStoreProvider",
                    "org.apache.hadoop.security.LdapGroupsMapping",
                    "org.apache.hadoop.fs.FileSystem"
                ],
                "methods": [
                    "JavaKeyStoreProvider.init",
                    "LdapGroupsMapping.getPassword",
                    "FileSystem.get"
                ]
            },
            "possible_fix": "To resolve the issue, consider modifying the JavaKeyStoreProvider constructor to avoid calling Path.getFileSystem() directly. Instead, retrieve the file system instance in a way that does not lead back to the same method, or implement a caching mechanism to prevent repeated calls."
        },
        "possible_fix_code": {
            "JavaKeyStoreProvider.init": "public void init(Configuration conf) throws IOException {\n    // Avoid calling Path.getFileSystem() directly to prevent infinite loop\n    // Instead, use a cached FileSystem instance or a different method to retrieve the FileSystem\n    FileSystem fs = FileSystem.getLocal(conf); // Example of using a local FileSystem\n    // Proceed with the rest of the initialization using the retrieved FileSystem\n    // Additional initialization logic here\n}"
        }
    },
    {
        "filename": "HADOOP-11722.json",
        "creation_time": "2015-03-16T21:39:07.000+0000",
        "bug_report": {
            "Title": "Some Instances of Services using ZKDelegationTokenSecretManager go down when old token cannot be deleted",
            "Description": "The delete node code in ZKDelegationTokenSecretManager is causing issues when multiple instances attempt to delete the same node simultaneously. This leads to a race condition where only one instance succeeds, while others throw a NoNodeException, resulting in a RuntimeException and bringing down the node.",
            "StackTrace": [
                "2015-03-15 10:24:54,000 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover thread received unexpected exception",
                "java.lang.RuntimeException: Could not remove Stored Token ZKDTSMDelegationToken_28",
                "at org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:770)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeExpiredToken(AbstractDelegationTokenSecretManager.java:605)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.access$400(AbstractDelegationTokenSecretManager.java:54)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:656)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /zkdtsm/ZKDTSMRoot/ZKDTSMTokensRoot/DT_28",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)",
                "at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:238)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:233)",
                "at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:214)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:41)",
                "at org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:764)",
                "... 4 more"
            ],
            "RootCause": "The root cause of the issue is a race condition where multiple instances of services using ZKDelegationTokenSecretManager attempt to delete the same token node simultaneously, leading to a NoNodeException when the node does not exist.",
            "StepsToReproduce": [
                "Start multiple instances of a service that uses ZKDelegationTokenSecretManager.",
                "Trigger the deletion of a token node in ZooKeeper.",
                "Observe the logs for RuntimeException and NoNodeException."
            ],
            "ExpectedBehavior": "Only one instance should successfully delete the token node, while others should handle the situation gracefully without throwing exceptions.",
            "ObservedBehavior": "Multiple instances throw a RuntimeException due to NoNodeException when trying to delete a non-existent token node.",
            "Suggestions": "Implement checks to ensure the token exists before attempting deletion, and add error handling for NoNodeException to prevent service crashes.",
            "problem_location": {
                "files": [
                    "ZKDelegationTokenSecretManager.java",
                    "AbstractDelegationTokenSecretManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager",
                    "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager"
                ],
                "methods": [
                    "removeStoredToken",
                    "removeExpiredToken"
                ]
            },
            "possible_fix": "Before attempting to delete a token, check if it exists in ZooKeeper. Implement error handling for NoNodeException to log the error and continue without failing the operation. Review the token lifecycle management to avoid race conditions."
        },
        "possible_fix_code": {
            "removeStoredToken": "  protected void removeStoredToken(TokenIdent ident)\n      throws IOException {\n    String nodeRemovePath =\n        getNodePath(ZK_DTSM_TOKENS_ROOT, DELEGATION_TOKEN_PREFIX\n            + ident.getSequenceNumber());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Removing ZKDTSMDelegationToken_\"\n          + ident.getSequenceNumber());\n    }\n    try {\n      if (zkClient.checkExists().forPath(nodeRemovePath) != null) {\n        while(zkClient.checkExists().forPath(nodeRemovePath) != null){\n          zkClient.delete().guaranteed().forPath(nodeRemovePath);\n        }\n      } else {\n        LOG.debug(\"Attempted to remove a non-existing znode \" + nodeRemovePath);\n      }\n    } catch (org.apache.zookeeper.KeeperException.NoNodeException e) {\n      LOG.warn(\"NoNodeException: The node does not exist, cannot remove token: \" + ident.getSequenceNumber());\n    } catch (Exception e) {\n      throw new RuntimeException(\n          \"Could not remove Stored Token ZKDTSMDelegationToken_\"\n          + ident.getSequenceNumber(), e);\n    }\n  }"
        }
    },
    {
        "filename": "HADOOP-15331.json",
        "creation_time": "2018-03-21T01:15:56.000+0000",
        "bug_report": {
            "Title": "Fix a race condition causing parsing error of java.io.BufferedInputStream in class org.apache.hadoop.conf.Configuration",
            "Description": "A race condition exists in the Hadoop Configuration class when multiple threads interact with the same Configuration instance. Specifically, if one thread adds resources while another thread clones the Configuration, it can lead to a situation where both threads attempt to access the same input stream. This results in a 'Stream closed' exception when one thread closes the stream after parsing, leaving the other thread with an invalid reference. The issue is exacerbated by the lazy loading of resources in the loadResources() method, which can lead to unexpected behavior if not properly synchronized.",
            "StackTrace": [
                "2018-02-28 08:23:19,589 ERROR org.apache.hadoop.conf.Configuration: error parsing conf java.io.BufferedInputStream@7741d346",
                "com.ctc.wstx.exc.WstxIOException: Stream closed",
                "at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)",
                "at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)",
                "at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)",
                "at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)",
                "at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)",
                "at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)",
                "at org.apache.hadoop.conf.Configuration.get(Configuration.java:1420)",
                "at org.apache.hadoop.security.authorize.ServiceAuthorizationManager.refreshWithLoadedConfiguration(ServiceAuthorizationManager.java:161)",
                "at org.apache.hadoop.ipc.Server.refreshServiceAclWithLoadedConfiguration(Server.java:607)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:586)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.startServer(AdminService.java:188)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1231)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                "at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1421)"
            ],
            "RootCause": "The root cause of the issue is improper management of input streams in the Configuration class, particularly in the loadResource and loadResources methods. When multiple threads access the same Configuration instance, one thread may close the input stream while another thread is still attempting to read from it, leading to a 'Stream closed' exception.",
            "StepsToReproduce": [
                "Create a Configuration instance.",
                "In one thread, add a resource to the Configuration.",
                "In another thread, clone the Configuration instance.",
                "Attempt to access configuration properties from both the original and cloned instances."
            ],
            "ExpectedBehavior": "The Configuration class should allow concurrent access without causing exceptions. Each thread should be able to read from its own instance of the input stream without interference from other threads.",
            "ObservedBehavior": "When one thread closes the input stream after parsing, another thread attempting to access the same input stream encounters a 'Stream closed' exception, leading to application crashes.",
            "Suggestions": "Implement proper synchronization mechanisms to manage access to shared resources in the Configuration class. Consider using thread-local storage for input streams or ensuring that streams are not closed until all threads have completed their operations.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java"
                ],
                "classes": [
                    "org.apache.hadoop.conf.Configuration",
                    "org.apache.hadoop.security.authorize.ServiceAuthorizationManager"
                ],
                "methods": [
                    "Configuration.loadResource",
                    "Configuration.loadResources",
                    "Configuration.get"
                ]
            },
            "possible_fix": "To fix the race condition, modify the loadResource method to ensure that input streams are not closed until all threads have finished using them. This can be achieved by implementing a reference counting mechanism or using thread-local input streams."
        },
        "possible_fix_code": {
            "loadResource": "  private Resource loadResource(Properties properties,\n                                Resource wrapper, boolean quiet) {\n    String name = UNKNOWN_RESOURCE;\n    try {\n      Object resource = wrapper.getResource();\n      name = wrapper.getName();\n      XMLStreamReader2 reader = null;\n      boolean returnCachedProperties = false;\n      boolean isRestricted = wrapper.isParserRestricted();\n\n      InputStream inputStream = null;\n      if (resource instanceof URL) {                  // an URL resource\n        reader = (XMLStreamReader2)parse((URL)resource, isRestricted);\n      } else if (resource instanceof String) {        // a CLASSPATH resource\n        URL url = getResource((String)resource);\n        reader = (XMLStreamReader2)parse(url, isRestricted);\n      } else if (resource instanceof Path) {          // a file resource\n        File file = new File(((Path)resource).toUri().getPath()).getAbsoluteFile();\n        if (file.exists()) {\n          if (!quiet) {\n            LOG.debug(\"parsing File \" + file);\n          }\n          inputStream = new BufferedInputStream(new FileInputStream(file));\n          reader = (XMLStreamReader2)parse(inputStream, ((Path)resource).toString(), isRestricted);\n        }\n      } else if (resource instanceof InputStream) {\n        inputStream = (InputStream)resource;\n        reader = (XMLStreamReader2)parse(inputStream, null, isRestricted);\n        returnCachedProperties = true;\n      } else if (resource instanceof Properties) {\n        overlay(properties, (Properties)resource);\n      }\n\n      if (reader == null) {\n        if (quiet) {\n          return null;\n        }\n        throw new RuntimeException(resource + \" not found\");\n      }\n      Properties toAddTo = properties;\n      if(returnCachedProperties) {\n        toAddTo = new Properties();\n      }\n      DeprecationContext deprecations = deprecationContext.get();\n\n      StringBuilder token = new StringBuilder();\n      String confName = null;\n      String confValue = null;\n      String confInclude = null;\n      String confTag = null;\n      boolean confFinal = false;\n      boolean fallbackAllowed = false;\n      boolean fallbackEntered = false;\n      boolean parseToken = false;\n      LinkedList<String> confSource = new LinkedList<String>();\n\n      while (reader.hasNext()) {\n        switch (reader.next()) {\n        case XMLStreamConstants.START_ELEMENT:\n          switch (reader.getLocalName()) {\n          case \"property\":\n            confName = null;\n            confValue = null;\n            confFinal = false;\n            confTag = null;\n            confSource.clear();\n\n            int attrCount = reader.getAttributeCount();\n            for (int i = 0; i < attrCount; i++) {\n              String propertyAttr = reader.getAttributeLocalName(i);\n              if (\"name\".equals(propertyAttr)) {\n                confName = StringInterner.weakIntern(reader.getAttributeValue(i));\n              } else if (\"value\".equals(propertyAttr)) {\n                confValue = StringInterner.weakIntern(reader.getAttributeValue(i));\n              } else if (\"final\".equals(propertyAttr)) {\n                confFinal = \"true\".equals(reader.getAttributeValue(i));\n              } else if (\"source\".equals(propertyAttr)) {\n                confSource.add(StringInterner.weakIntern(reader.getAttributeValue(i)));\n              } else if (\"tag\".equals(propertyAttr)) {\n                confTag = StringInterner.weakIntern(reader.getAttributeValue(i));\n              }\n            }\n            break;\n          case \"name\":\n          case \"value\":\n          case \"final\":\n          case \"source\":\n          case \"tag\":\n            parseToken = true;\n            token.setLength(0);\n            break;\n          case \"include\":\n            confInclude = null;\n            attrCount = reader.getAttributeCount();\n            for (int i = 0; i < attrCount; i++) {\n              String attrName = reader.getAttributeLocalName(i);\n              if (\"href\".equals(attrName)) {\n                confInclude = reader.getAttributeValue(i);\n              }\n            }\n            if (confInclude == null) {\n              break;\n            }\n            if (isRestricted) {\n              throw new RuntimeException(\"Error parsing resource \" + wrapper + \": XInclude is not supported for restricted resources\");\n            }\n            URL include = getResource(confInclude);\n            if (include != null) {\n              Resource classpathResource = new Resource(include, name, wrapper.isParserRestricted());\n              loadResource(properties, classpathResource, quiet);\n            } else {\n              URL url;\n              try {\n                url = new URL(confInclude);\n                url.openConnection().connect();\n              } catch (IOException ioe) {\n                File href = new File(confInclude);\n                if (!href.isAbsolute()) {\n                  File baseFile = new File(name).getParentFile();\n                  href = new File(baseFile, href.getPath());\n                }\n                if (!href.exists()) {\n                  fallbackAllowed = true;\n                  break;\n                }\n                url = href.toURI().toURL();\n              }\n              Resource uriResource = new Resource(url, name, wrapper.isParserRestricted());\n              loadResource(properties, uriResource, quiet);\n            }\n            break;\n          case \"fallback\":\n            fallbackEntered = true;\n            break;\n          case \"configuration\":\n            break;\n          default:\n            break;\n          }\n          break;\n\n        case XMLStreamConstants.CHARACTERS:\n          if (parseToken) {\n            char[] text = reader.getTextCharacters();\n            token.append(text, reader.getTextStart(), reader.getTextLength());\n          }\n          break;\n\n        case XMLStreamConstants.END_ELEMENT:\n          switch (reader.getLocalName()) {\n          case \"name\":\n            if (token.length() > 0) {\n              confName = StringInterner.weakIntern(token.toString().trim());\n            }\n            break;\n          case \"value\":\n            if (token.length() > 0) {\n              confValue = StringInterner.weakIntern(token.toString());\n            }\n            break;\n          case \"final\":\n            confFinal = \"true\".equals(token.toString());\n            break;\n          case \"source\":\n            confSource.add(StringInterner.weakIntern(token.toString()));\n            break;\n          case \"tag\":\n            if (token.length() > 0) {\n              confTag = StringInterner.weakIntern(token.toString());\n            }\n            break;\n          case \"include\":\n            if (fallbackAllowed && !fallbackEntered) {\n              throw new IOException(\"Fetch fail on include for '\" + confInclude + \"' with no fallback while loading '\" + name + \"'\");\n            }\n            fallbackAllowed = false;\n            fallbackEntered = false;\n            break;\n          case \"property\":\n            if (confName == null || (!fallbackAllowed && fallbackEntered)) {\n              break;\n            }\n            confSource.add(name);\n            if (confTag != null) {\n              readTagFromConfig(confTag, confName, confValue, confSource);\n            }\n\n            DeprecatedKeyInfo keyInfo = deprecations.getDeprecatedKeyMap().get(confName);\n            if (keyInfo != null) {\n              keyInfo.clearAccessed();\n              for (String key : keyInfo.newKeys) {\n                loadProperty(toAddTo, name, key, confValue, confFinal, confSource.toArray(new String[confSource.size()]));\n              }\n            } else {\n              loadProperty(toAddTo, name, confName, confValue, confFinal, confSource.toArray(new String[confSource.size()]));\n            }\n            break;\n          default:\n            break;\n          }\n        default:\n          break;\n        }\n      }\n      if (inputStream != null) {\n        inputStream.close();\n      }\n      reader.close();\n\n      if (returnCachedProperties) {\n        overlay(properties, toAddTo);\n        return new Resource(toAddTo, name, wrapper.isParserRestricted());\n      }\n      return null;\n    } catch (IOException e) {\n      LOG.error(\"error parsing conf \" + name, e);\n      throw new RuntimeException(e);\n    } catch (XMLStreamException e) {\n      LOG.error(\"error parsing conf \" + name, e);\n      throw new RuntimeException(e);\n    }\n  }"
        }
    },
    {
        "filename": "HADOOP-14062.json",
        "creation_time": "2016-12-20T00:35:48.000+0000",
        "bug_report": {
            "Title": "ApplicationMasterProtocolPBClientImpl.allocate fails with EOFException when RPC privacy is enabled",
            "Description": "When RPC privacy is enabled (hadoop.rpc.protection = privacy), the method ApplicationMasterProtocolPBClientImpl.allocate intermittently fails with an EOFException. This issue has been reproduced using Spark 2.0.2 on the latest branch-2.8 while executing a distcp job. The EOFException indicates an unexpected termination of the stream during a remote procedure call (RPC) between the application master and the resource manager, which can be attributed to network issues, server crashes, or misconfigurations.",
            "StackTrace": [
                "java.io.EOFException: End of File Exception between local host is: \"<application_master_host>/<ip_addr>\"; destination host is: \"<rm_host>\":8030; : java.io.EOFException",
                "at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1486)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1428)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1338)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.makeRemoteRequest(RMContainerRequestor.java:204)"
            ],
            "RootCause": "The EOFException occurs due to unexpected termination of the RPC stream, likely caused by connection failures or misconfigurations in the network setup. The methods responsible for handling RPC responses and connection management, such as getRpcResponse and handleConnectionFailure, indicate that the system may not be properly managing retries or connection setups under certain conditions.",
            "StepsToReproduce": [
                "Set hadoop.rpc.protection equal to privacy.",
                "Write data to HDFS using Spark with the following command: sc.parallelize(1 to (5*1024*1024)).map(k => Seq(k, org.apache.commons.lang.RandomStringUtils.random(1024, \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWxyZ0123456789\")).mkString(\"|\")).toDF().repartition(100).write.parquet(\"hdfs:///tmp/testData\").",
                "Attempt to distcp that data to another location in HDFS using: hadoop distcp -Dmapreduce.framework.name=yarn hdfs:///tmp/testData hdfs:///tmp/testDataCopy."
            ],
            "ExpectedBehavior": "The distcp command should successfully copy the data from the source HDFS location to the destination without any exceptions.",
            "ObservedBehavior": "The distcp command fails with an EOFException, indicating that the RPC call to allocate resources is not completing successfully.",
            "Suggestions": "Investigate network stability and configuration settings related to RPC privacy. Ensure that the connection management logic in the Client class is robust enough to handle transient network issues. Consider implementing additional logging around the connection setup and response handling to capture more context when failures occur.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/ApplicationMasterProtocolPBClientImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.ipc.Client",
                    "org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl"
                ],
                "methods": [
                    "Client.getRpcResponse",
                    "Client.handleConnectionFailure",
                    "ApplicationMasterProtocolPBClientImpl.allocate"
                ]
            },
            "possible_fix": "Review and enhance the error handling in the getRpcResponse and handleConnectionFailure methods to ensure that connection retries are managed effectively. Additionally, ensure that the setupIOstreams method is robust against failures during connection establishment."
        },
        "possible_fix_code": {
            "Client.getRpcResponse": "  private Writable getRpcResponse(final Call call, final Connection connection,\n      final long timeout, final TimeUnit unit) throws IOException {\n    synchronized (call) {\n      while (!call.done) {\n        try {\n          AsyncGet.Util.wait(call, timeout, unit);\n          if (timeout >= 0 && !call.done) {\n            return null;\n          }\n        } catch (InterruptedException ie) {\n          Thread.currentThread().interrupt();\n          throw new InterruptedIOException(\"Call interrupted\");\n        }\n      }\n\n      if (call.error != null) {\n        if (call.error instanceof RemoteException) {\n          call.error.fillInStackTrace();\n          throw call.error;\n        } else { // local exception\n          InetSocketAddress address = connection.getRemoteAddress();\n          throw NetUtils.wrapException(address.getHostName(),\n                  address.getPort(),\n                  NetUtils.getHostname(),\n                  0,\n                  call.error);\n        }\n      } else {\n        return call.getRpcResponse();\n      }\n    }\n  }",
            "Client.handleConnectionFailure": "    private void handleConnectionFailure(int curRetries, IOException ioe\n        ) throws IOException {\n      closeConnection();\n\n      final RetryAction action;\n      try {\n        action = connectionRetryPolicy.shouldRetry(ioe, curRetries, 0, true);\n      } catch(Exception e) {\n        throw e instanceof IOException? (IOException)e: new IOException(e);\n      }\n      if (action.action == RetryAction.RetryDecision.FAIL) {\n        if (action.reason != null) {\n          LOG.warn(\"Failed to connect to server: \" + server + \": \"\n              + action.reason, ioe);\n        }\n        throw ioe;\n      }\n\n      // Throw the exception if the thread is interrupted\n      if (Thread.currentThread().isInterrupted()) {\n        LOG.warn(\"Interrupted while trying for connection\");\n        throw ioe;\n      }\n\n      try {\n        Thread.sleep(action.delayMillis);\n      } catch (InterruptedException e) {\n        throw (IOException)new InterruptedIOException(\"Interrupted: action=\"\n            + action + \", retry policy=\" + connectionRetryPolicy).initCause(e);\n      }\n      LOG.info(\"Retrying connect to server: \" + server + \". Already tried \"\n          + curRetries + \" time(s); retry policy is \" + connectionRetryPolicy);\n    }",
            "Client.setupIOstreams": "    private synchronized void setupIOstreams(\n        AtomicBoolean fallbackToSimpleAuth) {\n      if (socket != null || shouldCloseConnection.get()) {\n        return;\n      } \n      try {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Connecting to \"+server);\n        }\n        Span span = Tracer.getCurrentSpan();\n        if (span != null) {\n          span.addTimelineAnnotation(\"IPC client connecting to \" + server);\n        }\n        short numRetries = 0;\n        Random rand = null;\n        while (true) {\n          setupConnection();\n          ipcStreams = new IpcStreams(socket, maxResponseLength);\n          writeConnectionHeader(ipcStreams);\n          if (authProtocol == AuthProtocol.SASL) {\n            UserGroupInformation ticket = remoteId.getTicket();\n            if (ticket.getRealUser() != null) {\n              ticket = ticket.getRealUser();\n            }\n            try {\n              authMethod = ticket\n                  .doAs(new PrivilegedExceptionAction<AuthMethod>() {\n                    @Override\n                    public AuthMethod run()\n                        throws IOException, InterruptedException {\n                      return setupSaslConnection(ipcStreams);\n                    }\n                  });\n            } catch (IOException ex) {\n              if (saslRpcClient == null) {\n                // whatever happened -it can't be handled, so rethrow\n                throw ex;\n              }\n              // otherwise, assume a connection problem\n              authMethod = saslRpcClient.getAuthMethod();\n              if (rand == null) {\n                rand = new Random();\n              }\n              handleSaslConnectionFailure(numRetries++, maxRetriesOnSasl, ex,\n                  rand, ticket);\n              continue;\n            }\n            if (authMethod != AuthMethod.SIMPLE) {\n              // Sasl connect is successful. Let's set up Sasl i/o streams.\n              ipcStreams.setSaslClient(saslRpcClient);\n              // for testing\n              remoteId.saslQop =\n                  (String)saslRpcClient.getNegotiatedProperty(Sasl.QOP);\n              LOG.debug(\"Negotiated QOP is :\" + remoteId.saslQop);\n              if (fallbackToSimpleAuth != null) {\n                fallbackToSimpleAuth.set(false);\n              }\n            } else if (UserGroupInformation.isSecurityEnabled()) {\n              if (!fallbackAllowed) {\n                throw new IOException(\"Server asks us to fall back to SIMPLE \" +\n                    \"auth, but this client is configured to only allow secure \" +\n                    \"connections.\");\n              }\n              if (fallbackToSimpleAuth != null) {\n                fallbackToSimpleAuth.set(true);\n              }\n            }\n          }\n\n          if (doPing) {\n            ipcStreams.setInputStream(new PingInputStream(ipcStreams.in));\n          }\n\n          writeConnectionContext(remoteId, authMethod);\n\n          // update last activity time\n          touch();\n\n          span = Tracer.getCurrentSpan();\n          if (span != null) {\n            span.addTimelineAnnotation(\"IPC client connected to \" + server);\n          }\n\n          // start the receiver thread after the socket connection has been set\n          // up\n          start();\n          return;\n        }\n      } catch (Throwable t) {\n        if (t instanceof IOException) {\n          markClosed((IOException)t);\n        } else {\n          markClosed(new IOException(\"Couldn't set up IO streams: \" + t, t));\n        }\n        close();\n      }\n    }"
        }
    },
    {
        "filename": "HADOOP-11149.json",
        "creation_time": "2014-09-27T16:37:06.000+0000",
        "bug_report": {
            "Title": "Increase the timeout of TestZKFailoverController",
            "Description": "The test `org.apache.hadoop.ha.TestZKFailoverController` is failing due to a timeout error occurring in the `waitForActiveAttempt` method of the `ZKFailoverController` class. The test is designed to validate the graceful failover process in a high-availability setup using ZooKeeper. The timeout is set to 25 seconds, and the error indicates that the test did not complete within this timeframe, leading to an exception being thrown.",
            "StackTrace": [
                "java.lang.Exception: test timed out after 25000 milliseconds",
                "at java.lang.Object.wait(Native Method)",
                "at org.apache.hadoop.ha.ZKFailoverController.waitForActiveAttempt(ZKFailoverController.java:467)",
                "at org.apache.hadoop.ha.ZKFailoverController.doGracefulFailover(ZKFailoverController.java:657)",
                "at org.apache.hadoop.ha.ZKFailoverController.access$400(ZKFailoverController.java:61)",
                "at org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:602)",
                "at org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:599)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1621)",
                "at org.apache.hadoop.ha.ZKFailoverController.gracefulFailoverToYou(ZKFailoverController.java:599)",
                "at org.apache.hadoop.ha.ZKFCRpcServer.gracefulFailover(ZKFCRpcServer.java:94)",
                "at org.apache.hadoop.ha.TestZKFailoverController.testGracefulFailover(TestZKFailoverController.java:448)"
            ],
            "RootCause": "The timeout in the `ZKFailoverController` during the test is likely caused by the service entering an unhealthy state, a lack of active attempts being recorded within the timeout period, or synchronization issues. Common causes include service configuration problems, resource exhaustion, network issues, dependency failures, and internal errors.",
            "StepsToReproduce": [
                "Run the test suite for `org.apache.hadoop.ha.TestZKFailoverController`.",
                "Observe the test `testGracefulFailover` which triggers the timeout error."
            ],
            "ExpectedBehavior": "The test should complete successfully within the specified timeout, indicating that the failover process is functioning correctly without any delays or errors.",
            "ObservedBehavior": "The test fails with a timeout error after 25 seconds, indicating that the expected active attempt was not recorded in time.",
            "Suggestions": "Consider increasing the timeout duration for the test to allow for longer failover processes. Additionally, investigate the health state of the service and ensure that it remains healthy during the test execution.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java"
                ],
                "classes": [
                    "org.apache.hadoop.ha.ZKFailoverController"
                ],
                "methods": [
                    "waitForActiveAttempt",
                    "getLastHealthState",
                    "setLastHealthState",
                    "checkEligibleForFailover"
                ]
            },
            "possible_fix": "Increase the timeout value in the test configuration. Additionally, review the service health checks and ensure that the service is properly configured and has sufficient resources during the test execution."
        },
        "possible_fix_code": {
            "waitForActiveAttempt": "  private ActiveAttemptRecord waitForActiveAttempt(int timeoutMillis)\n      throws InterruptedException {\n    long st = System.nanoTime();\n    long waitUntil = st + TimeUnit.NANOSECONDS.convert(\n        timeoutMillis, TimeUnit.MILLISECONDS);\n    \n    do {\n      // periodically check health state, because entering an\n      // unhealthy state could prevent us from ever attempting to\n      // become active. We can detect this and respond to the user\n      // immediately.\n      synchronized (this) {\n        if (lastHealthState != State.SERVICE_HEALTHY) {\n          // early out if service became unhealthy\n          return null;\n        }\n      }\n\n      synchronized (activeAttemptRecordLock) {\n        if ((lastActiveAttemptRecord != null &&\n            lastActiveAttemptRecord.nanoTime >= st)) {\n          return lastActiveAttemptRecord;\n        }\n        // Only wait 1sec so that we periodically recheck the health state\n        // above.\n        activeAttemptRecordLock.wait(1000);\n      }\n    } while (System.nanoTime() < waitUntil);\n    \n    // Timeout elapsed.\n    LOG.warn(timeoutMillis + \"ms timeout elapsed waiting for an attempt \" +\n        \"to become active\");\n    return null;\n  }"
        }
    },
    {
        "filename": "HADOOP-15059.json",
        "creation_time": "2017-11-21T20:54:52.000+0000",
        "bug_report": {
            "Title": "3.0 deployment cannot work with old version MR tar ball which breaks rolling upgrade",
            "Description": "The deployment of a Hadoop 3.0 cluster using a 2.9 MR tar ball fails due to a critical error in the MRAppMaster. The error log indicates a `RuntimeException` caused by an `IOException` when attempting to read a token storage file, which has an unknown version. This issue arises from incompatibilities in token storage formats between versions 2.9 and 3.0, which disrupts the expected behavior during a rolling upgrade.",
            "StackTrace": [
                "2017-11-21 12:42:50,911 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1511295641738_0003_000001",
                "2017-11-21 12:42:51,070 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable",
                "2017-11-21 12:42:51,118 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster",
                "java.lang.RuntimeException: Unable to determine current user",
                "\tat org.apache.hadoop.conf.Configuration$Resource.getRestrictParserDefault(Configuration.java:254)",
                "\tat org.apache.hadoop.conf.Configuration$Resource.<init>(Configuration.java:220)",
                "\tat org.apache.hadoop.conf.Configuration$Resource.<init>(Configuration.java:212)",
                "\tat org.apache.hadoop.conf.Configuration.addResource(Configuration.java:888)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1638)",
                "Caused by: java.io.IOException: Exception reading /tmp/nm-local-dir/usercache/jdu/appcache/application_1511295641738_0003/container_e03_1511295641738_0003_01_000001/container_tokens",
                "\tat org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:208)",
                "\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:907)",
                "\tat org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:820)",
                "\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:689)",
                "\tat org.apache.hadoop.conf.Configuration$Resource.getRestrictParserDefault(Configuration.java:252)",
                "... 4 more",
                "Caused by: java.io.IOException: Unknown version 1 in token storage.",
                "\tat org.apache.hadoop.security.Credentials.readTokenStorageStream(Credentials.java:226)",
                "\tat org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:205)",
                "... 8 more",
                "2017-11-21 12:42:51,122 INFO [main] org.apache.hadoop.util.ExitUtil: Exiting with status 1: java.lang.RuntimeException: Unable to determine current user"
            ],
            "RootCause": "The root cause of the issue is an `IOException` triggered by the `readTokenStorageFile` method in the `Credentials` class, which fails to read a token storage file due to an unknown version. This indicates that the token storage format used in version 2.9 is incompatible with the expectations of version 3.0.",
            "StepsToReproduce": [
                "Attempt to deploy a Hadoop 3.0 cluster using a 2.9 MR tar ball.",
                "Monitor the logs for errors related to MRAppMaster and token storage."
            ],
            "ExpectedBehavior": "The Hadoop 3.0 cluster should successfully deploy and run MR jobs without errors, even when using a 2.9 MR tar ball, as per the rolling upgrade support claim.",
            "ObservedBehavior": "The deployment fails with a `RuntimeException` indicating an inability to determine the current user, caused by an `IOException` related to an unknown version in the token storage file.",
            "Suggestions": "To resolve this issue, ensure that the token storage format is compatible between versions 2.9 and 3.0. This may involve updating the token storage files or modifying the code to handle version discrepancies.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/Credentials.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.UserGroupInformation",
                    "org.apache.hadoop.security.Credentials"
                ],
                "methods": [
                    "UserGroupInformation.loginUserFromSubject",
                    "Credentials.readTokenStorageFile",
                    "Credentials.readTokenStorageStream"
                ]
            },
            "possible_fix": "Modify the `readTokenStorageStream` method in the `Credentials` class to handle unknown versions more gracefully, or implement a migration strategy for token storage files when upgrading from 2.9 to 3.0."
        },
        "possible_fix_code": {
            "Credentials.readTokenStorageStream": "  public void readTokenStorageStream(DataInputStream in) throws IOException {\n    byte[] magic = new byte[TOKEN_STORAGE_MAGIC.length];\n    in.readFully(magic);\n    if (!Arrays.equals(magic, TOKEN_STORAGE_MAGIC)) {\n      throw new IOException(\"Bad header found in token storage.\");\n    }\n    byte version = in.readByte();\n    if (version != TOKEN_STORAGE_VERSION &&\n        version != OLD_TOKEN_STORAGE_VERSION) {\n      // Handle unknown version gracefully\n      LOG.warn(\"Unknown version \" + version + \" in token storage. Attempting to migrate...\");\n      // Implement migration logic or fallback strategy here\n      return; // or throw a more informative exception\n    }\n    if (version == OLD_TOKEN_STORAGE_VERSION) {\n      readFields(in);\n    } else if (version == TOKEN_STORAGE_VERSION) {\n      readProto(in);\n    }\n  }"
        }
    },
    {
        "filename": "HADOOP-15307.json",
        "creation_time": "2018-03-12T14:56:59.000+0000",
        "bug_report": {
            "Title": "NFS: flavor AUTH_SYS should use VerifierNone",
            "Description": "The NFS gateway fails to start when the portmapper request is denied by rpcbind, particularly due to the absence of localhost in /etc/hosts.allow. This results in an UnsupportedOperationException related to the AUTH_SYS verifier flavor, which is not supported in the current implementation of Hadoop's ONC RPC. The stack trace indicates that the failure occurs during the reading of the verifier flavor in the Verifier class, specifically in the readFlavorAndVerifier method.",
            "StackTrace": [
                "2018-03-05 12:49:31,976 INFO org.apache.hadoop.oncrpc.SimpleUdpServer: Started listening to UDP requests at port 4242 for Rpc program: mountd at localhost:4242 with workerCount 1",
                "2018-03-05 12:49:31,988 INFO org.apache.hadoop.oncrpc.SimpleTcpServer: Started listening to TCP requests at port 4242 for Rpc program: mountd at localhost:4242 with workerCount 1",
                "2018-03-05 12:49:31,993 TRACE org.apache.hadoop.oncrpc.RpcCall: Xid:692394656, messageType:RPC_CALL, rpcVersion:2, program:100000, version:2, procedure:1, credential:(AuthFlavor:AUTH_NONE), verifier:(AuthFlavor:AUTH_NONE)",
                "2018-03-05 12:49:31,998 FATAL org.apache.hadoop.mount.MountdBase: Failed to start the server. Cause:",
                "java.lang.UnsupportedOperationException: Unsupported verifier flavorAUTH_SYS",
                "at org.apache.hadoop.oncrpc.security.Verifier.readFlavorAndVerifier(Verifier.java:45)",
                "at org.apache.hadoop.oncrpc.RpcDeniedReply.read(RpcDeniedReply.java:50)",
                "at org.apache.hadoop.oncrpc.RpcReply.read(RpcReply.java:67)",
                "at org.apache.hadoop.oncrpc.SimpleUdpClient.run(SimpleUdpClient.java:71)",
                "at org.apache.hadoop.oncrpc.RpcProgram.register(RpcProgram.java:130)",
                "at org.apache.hadoop.oncrpc.RpcProgram.register(RpcProgram.java:101)",
                "at org.apache.hadoop.mount.MountdBase.start(MountdBase.java:83)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:60)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)",
                "2018-03-05 12:49:32,007 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1"
            ],
            "RootCause": "The root cause of the issue is that the Hadoop ONC RPC implementation does not support the AUTH_SYS verifier flavor, which leads to an UnsupportedOperationException when the system attempts to use it. The Verifier class is designed to handle only AUTH_NONE and RPCSEC_GSS flavors.",
            "StepsToReproduce": [
                "Ensure that the localhost entry is missing from /etc/hosts.allow.",
                "Start the NFS gateway.",
                "Observe the logs for the UnsupportedOperationException related to AUTH_SYS."
            ],
            "ExpectedBehavior": "The NFS gateway should start successfully without throwing any exceptions, regardless of the portmapper request being denied.",
            "ObservedBehavior": "The NFS gateway fails to start and throws an UnsupportedOperationException indicating that the AUTH_SYS verifier flavor is unsupported.",
            "Suggestions": "To resolve this issue, the implementation should either support the AUTH_SYS verifier flavor or ensure that it is not used in configurations where it is not supported. A potential fix could involve modifying the Verifier class to handle AUTH_SYS appropriately.",
            "problem_location": {
                "files": [
                    "hadoop-common-project.hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/Verifier.java",
                    "hadoop-common-project.hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RpcDeniedReply.java",
                    "hadoop-common-project.hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RpcReply.java"
                ],
                "classes": [
                    "org.apache.hadoop.oncrpc.security.Verifier",
                    "org.apache.hadoop.oncrpc.RpcDeniedReply",
                    "org.apache.hadoop.oncrpc.RpcReply"
                ],
                "methods": [
                    "Verifier.readFlavorAndVerifier",
                    "RpcDeniedReply.read",
                    "RpcReply.read"
                ]
            },
            "possible_fix": "Modify the Verifier class to include support for AUTH_SYS, or ensure that the system configuration does not attempt to use AUTH_SYS as a verifier flavor."
        },
        "possible_fix_code": {
            "Verifier.readFlavorAndVerifier": "  public static Verifier readFlavorAndVerifier(XDR xdr) {\n    AuthFlavor flavor = AuthFlavor.fromValue(xdr.readInt());\n    final Verifier verifer;\n    if(flavor == AuthFlavor.AUTH_NONE) {\n      verifer = new VerifierNone();\n    } else if(flavor == AuthFlavor.RPCSEC_GSS) {\n      verifer = new VerifierGSS();\n    } else if(flavor == AuthFlavor.AUTH_SYS) {\n      // Handle AUTH_SYS appropriately, possibly by returning a default verifier or logging a warning\n      throw new UnsupportedOperationException(\"AUTH_SYS verifier flavor is not supported. Please configure to use AUTH_NONE or RPCSEC_GSS.\");\n    } else {\n      throw new UnsupportedOperationException(\"Unsupported verifier flavor: \" + flavor);\n    }\n    verifer.read(xdr);\n    return verifer;\n  }"
        }
    },
    {
        "filename": "HADOOP-11446.json",
        "creation_time": "2014-12-23T22:15:23.000+0000",
        "bug_report": {
            "Title": "S3AOutputStream should use shared thread pool to avoid OutOfMemoryError",
            "Description": "During the process of exporting HBase snapshots to S3A, an OutOfMemoryError (OOME) was encountered, indicating that the application was unable to create new native threads. This issue arises from the creation of multiple TransferManager instances, each initializing its own thread pool, which leads to excessive thread consumption. The error was observed when the nofile ulimit was increased to 102400, suggesting that the application is hitting the thread limit imposed by the operating system.",
            "StackTrace": [
                "2014-12-19 13:15:03,895 INFO  [main] s3a.S3AFileSystem: OutputStream for key 'FastQueryPOC/2014-12-11/EVENT1-IDX-snapshot/.hbase-snapshot/.tmp/EVENT1_IDX_snapshot_2012_12_11/650a5678810fbdaa91809668d11ccf09/.regioninfo' closed. Now beginning upload",
                "2014-12-19 13:15:03,895 INFO  [main] s3a.S3AFileSystem: Minimum upload part size: 16777216 threshold2147483647",
                "Exception in thread \"main\" java.lang.OutOfMemoryError: unable to create new native thread",
                "at java.lang.Thread.start0(Native Method)",
                "at java.lang.Thread.start(Thread.java:713)",
                "at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)",
                "at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1360)",
                "at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:132)",
                "at com.amazonaws.services.s3.transfer.internal.UploadMonitor.<init>(UploadMonitor.java:129)",
                "at com.amazonaws.services.s3.transfer.TransferManager.upload(TransferManager.java:449)",
                "at com.amazonaws.services.s3.transfer.TransferManager.upload(TransferManager.java:382)",
                "at org.apache.hadoop.fs.s3a.S3AOutputStream.close(S3AOutputStream.java:127)",
                "at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)",
                "at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)",
                "at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:54)",
                "at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:112)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:366)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:356)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:356)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)",
                "at org.apache.hadoop.hbase.snapshot.ExportSnapshot.run(ExportSnapshot.java:791)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)",
                "at org.apache.hadoop.hbase.snapshot.ExportSnapshot.innerMain(ExportSnapshot.java:882)",
                "at org.apache.hadoop.hbase.snapshot.ExportSnapshot.main(ExportSnapshot.java:886)"
            ],
            "RootCause": "The root cause of the OutOfMemoryError is the creation of multiple TransferManager instances, each with its own thread pool, leading to an excessive number of threads being created and ultimately hitting the system's thread limit.",
            "StepsToReproduce": [
                "1. Increase the nofile ulimit to a high value (e.g., 102400).",
                "2. Attempt to export an HBase snapshot to S3A using the S3AOutputStream.",
                "3. Observe the logs for the OutOfMemoryError related to thread creation."
            ],
            "ExpectedBehavior": "The application should successfully export the HBase snapshot to S3A without encountering an OutOfMemoryError, utilizing a shared thread pool for the TransferManager.",
            "ObservedBehavior": "The application fails with an OutOfMemoryError, indicating it is unable to create new native threads due to excessive thread consumption from multiple TransferManager instances.",
            "Suggestions": "To resolve this issue, modify the S3AOutputStream implementation to use a shared thread pool for the TransferManager instead of creating a new one for each instance. Additionally, review the thread pool configuration and JVM settings to ensure they are optimized for the expected workload.",
            "problem_location": {
                "files": [
                    "S3AOutputStream.java"
                ],
                "classes": [
                    "org.apache.hadoop.fs.s3a.S3AOutputStream"
                ],
                "methods": [
                    "S3AOutputStream.close"
                ]
            },
            "possible_fix": "Refactor the S3AOutputStream class to accept a shared ThreadPoolExecutor instance for the TransferManager, ensuring that all uploads utilize the same thread pool. Example modification:\n\n```java\nTransferManager transfers = new TransferManager(client, sharedThreadPool);\n```"
        },
        "possible_fix_code": {
            "S3AOutputStream.close": "  public synchronized void close() throws IOException {\n    if (closed) {\n      return;\n    }\n\n    backupStream.close();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"OutputStream for key '\" + key + \"' closed. Now beginning upload\");\n      LOG.debug(\"Minimum upload part size: \" + partSize + \" threshold \" + partSizeThreshold);\n    }\n\n    try {\n      TransferManagerConfiguration transferConfiguration = new TransferManagerConfiguration();\n      transferConfiguration.setMinimumUploadPartSize(partSize);\n      transferConfiguration.setMultipartUploadThreshold(partSizeThreshold);\n\n      // Use a shared TransferManager instance instead of creating a new one\n      TransferManager transfers = new TransferManager(client, sharedThreadPool);\n      transfers.setConfiguration(transferConfiguration);\n\n      final ObjectMetadata om = new ObjectMetadata();\n      if (StringUtils.isNotBlank(serverSideEncryptionAlgorithm)) {\n        om.setServerSideEncryption(serverSideEncryptionAlgorithm);\n      }\n      PutObjectRequest putObjectRequest = new PutObjectRequest(bucket, key, backupFile);\n      putObjectRequest.setCannedAcl(cannedACL);\n      putObjectRequest.setMetadata(om);\n\n      Upload upload = transfers.upload(putObjectRequest);\n\n      ProgressableProgressListener listener = \n        new ProgressableProgressListener(upload, progress, statistics);\n      upload.addProgressListener(listener);\n\n      upload.waitForUploadResult();\n\n      long delta = upload.getProgress().getBytesTransferred() - listener.getLastBytesTransferred();\n      if (statistics != null && delta != 0) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"S3A write delta changed after finished: \" + delta + \" bytes\");\n        }\n        statistics.incrementBytesWritten(delta);\n      }\n\n      // This will delete unnecessary fake parent directories\n      fs.finishedWrite(key);\n    } catch (InterruptedException e) {\n      throw new IOException(e);\n    } finally {\n      if (!backupFile.delete()) {\n        LOG.warn(\"Could not delete temporary s3a file: {}\", backupFile);\n      }\n      super.close();\n      closed = true;\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"OutputStream for key '\" + key + \"' upload complete\");\n    }\n  }"
        }
    },
    {
        "filename": "HADOOP-12689.json",
        "creation_time": "2016-01-04T23:30:49.000+0000",
        "bug_report": {
            "Title": "S3 filesystem operations stopped working correctly",
            "Description": "After the resolution of HADOOP-10542, which replaced 'return null;' with throwing an IOException, several S3 filesystem operations have begun to fail. The affected methods include S3FileSystem.getFileStatus(), FileSystem.exists(), and S3FileSystem.create(). These methods now throw IOException instead of returning null or false, leading to failures in operations that expect a different behavior. The issue manifests when executing the command 'hadoop distcp hdfs://localhost:9000/test s3://xxx:yyy@com.bar.foo/', resulting in an IOException indicating that the specified path '/test' does not exist.",
            "StackTrace": [
                "2015-12-11 10:04:34,030 FATAL [IPC Server handler 6 on 44861] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1449826461866_0005_m_000006_0 - exited : java.io.IOException: /test doesn't exist",
                "at org.apache.hadoop.fs.s3.Jets3tFileSystemStore.get(Jets3tFileSystemStore.java:170)",
                "at org.apache.hadoop.fs.s3.Jets3tFileSystemStore.retrieveINode(Jets3tFileSystemStore.java:221)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)",
                "at com.sun.proxy.$Proxy17.retrieveINode(Unknown Source)",
                "at org.apache.hadoop.fs.s3.S3FileSystem.getFileStatus(S3FileSystem.java:340)",
                "at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:230)",
                "at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:50)",
                "at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)"
            ],
            "RootCause": "The root cause of the issue is the change in behavior of the S3 filesystem methods due to the modification in HADOOP-10542, which now throws IOException instead of returning null or false when a file or directory does not exist. This leads to failures in operations that expect a different response.",
            "StepsToReproduce": [
                "Ensure the Hadoop S3 filesystem is configured correctly.",
                "Run the command: hadoop distcp hdfs://localhost:9000/test s3://xxx:yyy@com.bar.foo/",
                "Observe the resulting IOException indicating that the path '/test' does not exist."
            ],
            "ExpectedBehavior": "The command should successfully copy files from HDFS to the specified S3 path without throwing an IOException, assuming the path exists and is accessible.",
            "ObservedBehavior": "The command fails with an IOException stating that the path '/test' does not exist, indicating that the S3 filesystem operations are not functioning as expected.",
            "Suggestions": "To resolve the issue, consider reverting the change made in HADOOP-10542 that causes IOException to be thrown instead of returning null. Additionally, ensure that the path '/test' exists and is correctly configured in the S3 filesystem.",
            "problem_location": {
                "files": [
                    "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3/S3FileSystem.java",
                    "hadoop-tools.hadoop-aws/src/main/java/org/apache/hadoop/fs/s3/Jets3tFileSystemStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.fs.s3.S3FileSystem",
                    "org.apache.hadoop.fs.s3.Jets3tFileSystemStore"
                ],
                "methods": [
                    "S3FileSystem.getFileStatus",
                    "S3FileSystem.listStatus",
                    "Jets3tFileSystemStore.get",
                    "Jets3tFileSystemStore.retrieveINode"
                ]
            },
            "possible_fix": "Revert the change in the getFileStatus and listStatus methods to return null or false instead of throwing IOException when a file or directory does not exist. This will restore the expected behavior for existing code that relies on these methods."
        },
        "possible_fix_code": {
            "S3FileSystem.getFileStatus": "  public FileStatus getFileStatus(Path f)  throws IOException {\n    INode inode = store.retrieveINode(makeAbsolute(f));\n    if (inode == null) {\n      return null; // Changed from throwing FileNotFoundException to returning null\n    }\n    return new S3FileStatus(f.makeQualified(this), inode);\n  }",
            "S3FileSystem.listStatus": "  public FileStatus[] listStatus(Path f) throws IOException {\n    Path absolutePath = makeAbsolute(f);\n    INode inode = store.retrieveINode(absolutePath);\n    if (inode == null) {\n      return null; // Changed from throwing FileNotFoundException to returning null\n    }\n    if (inode.isFile()) {\n      return new FileStatus[] {\n        new S3FileStatus(f.makeQualified(this), inode)\n      };\n    }\n    ArrayList<FileStatus> ret = new ArrayList<FileStatus>();\n    for (Path p : store.listSubPaths(absolutePath)) {\n      ret.add(getFileStatus(p.makeQualified(this)));\n    }\n    return ret.toArray(new FileStatus[0]);\n  }"
        }
    },
    {
        "filename": "HADOOP-13132.json",
        "creation_time": "2016-05-11T11:42:30.000+0000",
        "bug_report": {
            "Title": "Handle ClassCastException on AuthenticationException in LoadBalancingKMSClientProvider",
            "Description": "An Oozie job with a single shell action fails due to a ClassCastException occurring in the LoadBalancingKMSClientProvider class. The error arises when an AuthenticationException is incorrectly cast to a GeneralSecurityException during the decryption of an encrypted key. This issue prevents the Oozie job from completing successfully and results in the YARN logs not being reported or saved. The problematic cast occurs in the decryptEncryptedKey method, specifically when handling exceptions thrown by the doOp method.",
            "StackTrace": [
                "java.lang.ClassCastException: org.apache.hadoop.security.authentication.client.AuthenticationException cannot be cast to java.security.GeneralSecurityException",
                "at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.decryptEncryptedKey(LoadBalancingKMSClientProvider.java:189)",
                "at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:388)",
                "at org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1419)",
                "at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1521)",
                "at org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:108)",
                "at org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:59)",
                "at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:577)",
                "at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:683)",
                "at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:679)",
                "at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)",
                "at org.apache.hadoop.fs.FileContext.create(FileContext.java:679)",
                "at org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter$1.run(AggregatedLogFormat.java:382)",
                "at org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter$1.run(AggregatedLogFormat.java:377)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter.<init>(AggregatedLogFormat.java:376)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainers(AppLogAggregatorImpl.java:246)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregation(AppLogAggregatorImpl.java:456)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run(AppLogAggregatorImpl.java:421)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$2.run(LogAggregationService.java:384)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the ClassCastException is the incorrect casting of an AuthenticationException to a GeneralSecurityException in the LoadBalancingKMSClientProvider.decryptEncryptedKey method. This occurs when the cause of a WrapperException is not an instance of GeneralSecurityException, leading to the failure of the cast.",
            "StepsToReproduce": [
                "1. Set up an Oozie job with a single shell action that requires decryption of an encrypted key.",
                "2. Execute the Oozie job.",
                "3. Observe the logs for a ClassCastException related to AuthenticationException."
            ],
            "ExpectedBehavior": "The Oozie job should complete successfully, and the YARN logs should be reported and saved without any exceptions.",
            "ObservedBehavior": "The Oozie job fails with a ClassCastException, preventing the job from completing and causing YARN logs not to be reported or saved.",
            "Suggestions": "Modify the decryptEncryptedKey method to check the type of the exception before attempting to cast it to GeneralSecurityException. Implement proper handling for AuthenticationException to avoid the ClassCastException.",
            "problem_location": {
                "files": [
                    "LoadBalancingKMSClientProvider.java"
                ],
                "classes": [
                    "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider"
                ],
                "methods": [
                    "decryptEncryptedKey",
                    "doOp"
                ]
            },
            "possible_fix": "In the decryptEncryptedKey method, replace the line that throws the ClassCastException with a type check: \n\n```java\ncatch (WrapperException we) {\n    Throwable cause = we.getCause();\n    if (cause instanceof GeneralSecurityException) {\n        throw (GeneralSecurityException) cause;\n    } else {\n        // Handle AuthenticationException or other exceptions appropriately\n    }\n}\n```"
        },
        "possible_fix_code": {
            "decryptEncryptedKey": "  public KeyVersion\n      decryptEncryptedKey(final EncryptedKeyVersion encryptedKeyVersion)\n          throws IOException, GeneralSecurityException {\n    try {\n      return doOp(new ProviderCallable<KeyVersion>() {\n        @Override\n        public KeyVersion call(KMSClientProvider provider)\n            throws IOException, GeneralSecurityException {\n          return provider.decryptEncryptedKey(encryptedKeyVersion);\n        }\n      }, nextIdx());\n    } catch (WrapperException we) {\n      Throwable cause = we.getCause();\n      if (cause instanceof GeneralSecurityException) {\n          throw (GeneralSecurityException) cause;\n      } else if (cause instanceof AuthenticationException) {\n          // Handle AuthenticationException appropriately\n          throw new IOException(\"Authentication failed: \" + cause.getMessage(), cause);\n      } else {\n          throw new IOException(\"Unexpected exception: \" + cause.getMessage(), cause);\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HADOOP-15121.json",
        "creation_time": "2017-12-15T07:41:39.000+0000",
        "bug_report": {
            "Title": "Encounter NullPointerException when using DecayRpcScheduler",
            "Description": "A NullPointerException is encountered when using the DecayRpcScheduler in the Hadoop framework. The issue arises when the configuration for the scheduler is set to 'org.apache.hadoop.ipc.DecayRpcScheduler'. The stack trace indicates that the exception occurs in the 'getMetrics' method of the MetricsProxy inner class of DecayRpcScheduler, suggesting that an uninitialized object is being accessed. The metricsProxy's delegate field appears to not be properly initialized during the scheduler's initialization process.",
            "StackTrace": [
                "2017-12-15 15:26:34,662 ERROR impl.MetricsSourceAdapter (MetricsSourceAdapter.java:getMetrics(202)) - Error getting metrics from source DecayRpcSchedulerMetrics2.ipc.8020",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getMetrics(DecayRpcScheduler.java:781)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:199)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:182)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:155)",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)",
                "at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)",
                "at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:66)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:100)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)",
                "at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.registerMetrics2Source(DecayRpcScheduler.java:709)",
                "at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.<init>(DecayRpcScheduler.java:685)",
                "at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getInstance(DecayRpcScheduler.java:693)",
                "at org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:236)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:423)",
                "at org.apache.hadoop.ipc.CallQueueManager.createScheduler(CallQueueManager.java:102)",
                "at org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:76)",
                "at org.apache.hadoop.ipc.Server.<init>(Server.java:2612)",
                "at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:958)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:374)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:349)",
                "at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:800)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:415)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:755)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:697)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:905)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:884)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1610)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1678)"
            ],
            "RootCause": "The NullPointerException is caused by the 'metricsProxy' in the DecayRpcScheduler not properly initializing its 'delegate' field during the initialization process. This leads to an attempt to access a null reference when metrics are being retrieved.",
            "StepsToReproduce": [
                "Set the configuration 'ipc.8020.scheduler.impl' to 'org.apache.hadoop.ipc.DecayRpcScheduler'.",
                "Start the Hadoop Namenode.",
                "Observe the logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The DecayRpcScheduler should initialize correctly and provide metrics without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to retrieve metrics from the DecayRpcScheduler, indicating that an object reference is null.",
            "Suggestions": "Ensure that the 'metricsProxy' is properly initialized in the DecayRpcScheduler's constructor. Verify that all necessary dependencies are correctly set up before invoking methods that rely on them.",
            "problem_location": {
                "files": [
                    "DecayRpcScheduler.java",
                    "MetricsSourceAdapter.java"
                ],
                "classes": [
                    "org.apache.hadoop.ipc.DecayRpcScheduler",
                    "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter"
                ],
                "methods": [
                    "DecayRpcScheduler.DecayRpcScheduler",
                    "DecayRpcScheduler$MetricsProxy.getMetrics"
                ]
            },
            "possible_fix": "In the DecayRpcScheduler class, ensure that the 'metricsProxy' is initialized correctly in the constructor. For example, add a check to initialize the 'delegate' field before it is accessed in the 'getMetrics' method."
        },
        "possible_fix_code": {
            "org.apache.hadoop.ipc.DecayRpcScheduler": "package org.apache.hadoop.ipc;\n\nimport org.apache.hadoop.metrics2.MetricsSource;\nimport org.apache.hadoop.metrics2.impl.MetricsSourceAdapter;\n\npublic class DecayRpcScheduler {\n    private MetricsProxy metricsProxy;\n\n    public DecayRpcScheduler() {\n        // Initialize metricsProxy with a proper MetricsSource implementation\n        MetricsSource source = getMetricsSource(); // Assume this method retrieves a valid MetricsSource\n        this.metricsProxy = new MetricsProxy(source);\n    }\n\n    private MetricsSource getMetricsSource() {\n        // Logic to retrieve or create a valid MetricsSource instance\n        return new SomeMetricsSourceImpl(); // Replace with actual implementation\n    }\n\n    class MetricsProxy {\n        private final MetricsSource source;\n\n        public MetricsProxy(MetricsSource source) {\n            this.source = source;\n        }\n\n        public void getMetrics() {\n            // Ensure source is not null before accessing it\n            if (source == null) {\n                throw new IllegalStateException(\"MetricsSource is not initialized\");\n            }\n            // Logic to retrieve metrics from the source\n        }\n    }\n}"
        }
    },
    {
        "filename": "HADOOP-8110.json",
        "creation_time": "2012-02-24T18:49:27.000+0000",
        "bug_report": {
            "Title": "TestViewFsTrash occasionally fails",
            "Description": "The JUnit test `TestViewFsTrash.testTrash` is failing intermittently, indicating an assertion failure where the expected value was `0` but the actual value was `1`. This suggests that the test is checking for a condition related to the trash management functionality in Hadoop's file system, specifically that no items should be present in the trash after a certain operation. Recent builds have shown multiple failures, indicating a potential underlying issue with the trash management implementation or the test setup.",
            "StackTrace": [
                "junit.framework.AssertionFailedError: -expunge failed expected:<0> but was:<1>",
                "at junit.framework.Assert.fail(Assert.java:47)",
                "at junit.framework.Assert.failNotEquals(Assert.java:283)",
                "at junit.framework.Assert.assertEquals(Assert.java:64)",
                "at junit.framework.Assert.assertEquals(Assert.java:195)",
                "at org.apache.hadoop.fs.TestTrash.trashShell(TestTrash.java:322)",
                "at org.apache.hadoop.fs.viewfs.TestViewFsTrash.testTrash(TestViewFsTrash.java:73)"
            ],
            "RootCause": "The assertion failure indicates that the trash management functionality is not behaving as expected, leading to an unexpected number of items in the trash. This could be due to a bug in the implementation or incorrect test setup.",
            "StepsToReproduce": [
                "Run the JUnit test suite for `TestViewFsTrash`.",
                "Observe the failure in the `testTrash` method where the assertion fails due to an unexpected value."
            ],
            "ExpectedBehavior": "The trash management functionality should result in zero items being present in the trash after the appropriate operations are performed.",
            "ObservedBehavior": "The test fails with an assertion that indicates one item is present in the trash when none was expected.",
            "Suggestions": "Review the implementation of the trash management functionality in Hadoop to identify any bugs. Additionally, ensure that the test setup and teardown are correctly managing the state before and after the test runs.",
            "problem_location": {
                "files": [
                    "TestTrash.java",
                    "TestViewFsTrash.java"
                ],
                "classes": [
                    "org.apache.hadoop.fs.TestTrash",
                    "org.apache.hadoop.fs.viewfs.TestViewFsTrash"
                ],
                "methods": [
                    "TestTrash.trashShell",
                    "TestViewFsTrash.testTrash"
                ]
            },
            "possible_fix": "Investigate the `trashShell` method in `TestTrash` to ensure it correctly sets up the test environment. If necessary, modify the implementation of the trash management functionality to ensure it correctly handles the expected state."
        },
        "possible_fix_code": {
            "TestViewFsTrash.testTrash": "public void testTrash() throws Exception {\n    // Setup the test environment\n    FileSystem fs = FileSystem.get(new Configuration());\n    Path trashPath = new Path(fs.getTrashRoot(), \"testTrash\");\n    fs.mkdirs(trashPath);\n\n    // Perform the operation that should result in no items in the trash\n    // For example, delete a file and ensure it goes to trash\n    Path testFile = new Path(trashPath, \"testFile.txt\");\n    fs.create(testFile).close();\n    fs.delete(testFile, false);\n\n    // Wait for the trash to process the deletion\n    Thread.sleep(1000); // Adjust the sleep time as necessary\n\n    // Check the number of items in the trash\n    FileStatus[] trashContents = fs.listStatus(trashPath);\n    int itemCount = trashContents.length;\n\n    // Assert that the trash is empty\n    assertEquals(0, itemCount);\n\n    // Cleanup the test environment\n    fs.delete(trashPath, true);\n}"
        }
    },
    {
        "filename": "HADOOP-11400.json",
        "creation_time": "2014-12-12T10:05:52.000+0000",
        "bug_report": {
            "Title": "GraphiteSink does not reconnect to Graphite after 'broken pipe'",
            "Description": "The issue arises when the GraphiteSink encounters a network error, specifically a 'broken pipe' error, which prevents it from reconnecting to the Graphite server. This results in metrics not being sent. The stack trace indicates that the error occurs during the flushing of metrics in the GraphiteSinkFixed class, where a SocketException is thrown due to an unexpected closure of the socket connection. The GraphiteSink is initialized only once per application runtime, and there is no implemented reconnection logic to handle such network failures.",
            "StackTrace": [
                "2014-12-11 16:39:21,655 ERROR org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: Got sink exception, retry in 4806ms",
                "org.apache.hadoop.metrics2.MetricsException: Error flushing metrics",
                "at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:120)",
                "at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:184)",
                "at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)",
                "at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)",
                "at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:129)",
                "at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:88)",
                "Caused by: java.net.SocketException: Broken pipe",
                "at java.net.SocketOutputStream.socketWrite0(Native Method)",
                "at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)",
                "at java.net.SocketOutputStream.write(SocketOutputStream.java:159)",
                "at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221)",
                "at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291)",
                "at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:295)",
                "at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:141)",
                "at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:229)",
                "at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:118)"
            ],
            "RootCause": "The root cause of the issue is a SocketException with the message 'Broken pipe', which occurs when the GraphiteSink attempts to flush metrics to a closed socket connection. The lack of reconnection logic in the GraphiteSinkFixed class exacerbates the problem, as it does not attempt to re-establish the connection after a failure.",
            "StepsToReproduce": [
                "1. Start the application with GraphiteSink configured.",
                "2. Simulate a network failure that causes a 'broken pipe' error.",
                "3. Observe that the GraphiteSink does not attempt to reconnect to the Graphite server."
            ],
            "ExpectedBehavior": "The GraphiteSink should automatically attempt to reconnect to the Graphite server after a network failure, ensuring that metrics are sent continuously.",
            "ObservedBehavior": "After a network error, the GraphiteSink fails to reconnect to the Graphite server, resulting in metrics not being sent.",
            "Suggestions": "Implement reconnection logic in the GraphiteSinkFixed class to handle SocketExceptions and attempt to re-establish the connection to the Graphite server after a failure.",
            "problem_location": {
                "files": [
                    "GraphiteSinkFixed.java"
                ],
                "classes": [
                    "org.apache.hadoop.metrics2.sink.GraphiteSinkFixed"
                ],
                "methods": [
                    "GraphiteSinkFixed.flush"
                ]
            },
            "possible_fix": "Add a reconnection mechanism in the flush method of GraphiteSinkFixed to handle SocketExceptions and retry the connection to the Graphite server."
        },
        "possible_fix_code": {
            "GraphiteSinkFixed.flush": "public void flush() {\n    try {\n        // Attempt to flush metrics to the Graphite server\n        // Assuming 'outputStream' is the OutputStream connected to the Graphite server\n        outputStream.flush();\n    } catch (SocketException e) {\n        // Handle broken pipe error\n        LOG.error(\"SocketException occurred: \" + e.getMessage() + \", attempting to reconnect...\");\n        reconnectToGraphite(); // Method to handle reconnection logic\n    } catch (IOException e) {\n        LOG.error(\"IOException occurred while flushing metrics: \" + e.getMessage());\n    }\n}\n\nprivate void reconnectToGraphite() {\n    // Logic to re-establish connection to the Graphite server\n    try {\n        // Close existing connection if necessary\n        if (outputStream != null) {\n            outputStream.close();\n        }\n        // Reinitialize the connection to the Graphite server\n        // Assuming 'createConnection()' is a method that sets up the connection\n        createConnection();\n        LOG.info(\"Reconnected to Graphite server successfully.\");\n    } catch (IOException e) {\n        LOG.error(\"Failed to reconnect to Graphite server: \" + e.getMessage());\n    }\n}"
        }
    },
    {
        "filename": "HADOOP-9865.json",
        "creation_time": "2013-08-12T23:22:58.000+0000",
        "bug_report": {
            "Title": "FileContext.globStatus() has a regression with respect to relative path",
            "Description": "The issue was identified during the execution of the unit test TestMRJobClient on Windows, where a job failed due to an inability to create a Jar with the correct classpath. This failure is linked to the use of a relative path in the FileContext.globStatus() method, which is not supported in this context. The problem appears to be a regression introduced by HADOOP-9817, as the test passes on Linux, indicating a platform-specific issue. The stack trace reveals that the error originates from the Path.checkNotRelative method, which enforces the requirement for absolute paths in Hadoop operations.",
            "StackTrace": [
                "2013-08-12 16:12:05,937 WARN  [ContainersLauncher #0] launcher.ContainerLaunch (ContainerLaunch.java:call(270)) - Failed to launch container.",
                "org.apache.hadoop.HadoopIllegalArgumentException: Path is relative",
                "\tat org.apache.hadoop.fs.Path.checkNotRelative(Path.java:74)",
                "\tat org.apache.hadoop.fs.FileContext.getFSofPath(FileContext.java:304)",
                "\tat org.apache.hadoop.fs.Globber.schemeFromPath(Globber.java:107)",
                "\tat org.apache.hadoop.fs.Globber.glob(Globber.java:128)",
                "\tat org.apache.hadoop.fs.FileContext$Util.globStatus(FileContext.java:1908)",
                "\tat org.apache.hadoop.fs.FileUtil.createJarWithClassPath(FileUtil.java:1247)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.sanitizeEnv(ContainerLaunch.java:679)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:232)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:1)",
                "\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "\tat java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "\tat java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is the use of a relative path in the FileContext.globStatus() method, which leads to a HadoopIllegalArgumentException. This is a regression from HADOOP-9817, where the handling of paths was altered, resulting in this failure on Windows systems.",
            "StepsToReproduce": [
                "Run the unit test TestMRJobClient on a Windows environment.",
                "Ensure that the job attempts to create a Jar with a classpath that includes a relative path.",
                "Observe the failure in the job launch due to the relative path error."
            ],
            "ExpectedBehavior": "The job should launch successfully, and the status should be retrievable without errors, regardless of the operating system.",
            "ObservedBehavior": "The job fails to launch due to a relative path being passed to FileContext.globStatus(), resulting in a HadoopIllegalArgumentException.",
            "Suggestions": "To resolve this issue, ensure that absolute paths are used when specifying paths in Hadoop operations. Modify the code in FileUtil.createJarWithClassPath to convert relative paths to absolute paths before passing them to FileContext.globStatus().",
            "problem_location": {
                "files": [
                    "ContainerLaunch.java",
                    "FileUtil.java",
                    "FileContext.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch",
                    "org.apache.hadoop.fs.FileUtil",
                    "org.apache.hadoop.fs.FileContext"
                ],
                "methods": [
                    "ContainerLaunch.sanitizeEnv",
                    "FileUtil.createJarWithClassPath",
                    "FileContext.globStatus"
                ]
            },
            "possible_fix": "Modify the createJarWithClassPath method in FileUtil to check if the provided path is relative and convert it to an absolute path using the appropriate Hadoop FileSystem methods before calling globStatus."
        },
        "possible_fix_code": {
            "FileUtil.createJarWithClassPath": "  public static String createJarWithClassPath(String inputClassPath, Path pwd,\n      Map<String, String> callerEnv) throws IOException {\n    // Replace environment variables, case-insensitive on Windows\n    @SuppressWarnings(\"unchecked\")\n    Map<String, String> env = Shell.WINDOWS ? new CaseInsensitiveMap(callerEnv) :\n      callerEnv;\n    String[] classPathEntries = inputClassPath.split(File.pathSeparator);\n    for (int i = 0; i < classPathEntries.length; ++i) {\n      classPathEntries[i] = StringUtils.replaceTokens(classPathEntries[i],\n        StringUtils.ENV_VAR_PATTERN, env);\n    }\n    File workingDir = new File(pwd.toString());\n    if (!workingDir.mkdirs()) {\n      // If mkdirs returns false because the working directory already exists,\n      // then this is acceptable.  If it returns false due to some other I/O\n      // error, then this method will fail later with an IOException while saving\n      // the jar.\n      LOG.debug(\"mkdirs false for \" + workingDir + \", execution will continue\");\n    }\n\n    // Append all entries\n    List<String> classPathEntryList = new ArrayList<String>(\n      classPathEntries.length);\n    for (String classPathEntry: classPathEntries) {\n      // Convert to absolute path if it is relative\n      Path entryPath = new Path(classPathEntry);\n      if (!entryPath.isAbsolute()) {\n        entryPath = entryPath.makeQualified(FileContext.getFileContext().getDefaultFileSystem().getUri(), entryPath);\n      }\n      if (classPathEntry.endsWith(\"*\")) {\n        // Append all jars that match the wildcard\n        Path globPath = entryPath.suffix(\"{.jar,.JAR}\");\n        FileStatus[] wildcardJars = FileContext.getLocalFSFileContext().util()\n          .globStatus(globPath);\n        if (wildcardJars != null) {\n          for (FileStatus wildcardJar: wildcardJars) {\n            classPathEntryList.add(wildcardJar.getPath().toUri().toURL()\n              .toExternalForm());\n          }\n        }\n      } else {\n        // Append just this entry\n        String classPathEntryUrl = entryPath.toUri().toURL()\n          .toExternalForm();\n\n        // File.toURI only appends trailing '/' if it can determine that it is a\n        // directory that already exists.  (See JavaDocs.)  If this entry had a\n        // trailing '/' specified by the caller, then guarantee that the\n        // classpath entry in the manifest has a trailing '/', and thus refers to\n        // a directory instead of a file.  This can happen if the caller is\n        // creating a classpath jar referencing a directory that hasn't been\n        // created yet, but will definitely be created before running.\n        if (classPathEntry.endsWith(Path.SEPARATOR) &&\n            !classPathEntryUrl.endsWith(Path.SEPARATOR)) {\n          classPathEntryUrl = classPathEntryUrl + Path.SEPARATOR;\n        }\n        classPathEntryList.add(classPathEntryUrl);\n      }\n    }\n    String jarClassPath = StringUtils.join(\" \", classPathEntryList);\n\n    // Create the manifest\n    Manifest jarManifest = new Manifest();\n    jarManifest.getMainAttributes().putValue(\n        Attributes.Name.MANIFEST_VERSION.toString(), \"1.0\");\n    jarManifest.getMainAttributes().putValue(\n        Attributes.Name.CLASS_PATH.toString(), jarClassPath);\n\n    // Write the manifest to output JAR file\n    File classPathJar = File.createTempFile(\"classpath-\", \".jar\", workingDir);\n    FileOutputStream fos = null;\n    BufferedOutputStream bos = null;\n    JarOutputStream jos = null;\n    try {\n      fos = new FileOutputStream(classPathJar);\n      bos = new BufferedOutputStream(fos);\n      jos = new JarOutputStream(bos, jarManifest);\n    } finally {\n      IOUtils.cleanup(LOG, jos, bos, fos);\n    }\n\n    return classPathJar.getCanonicalPath();\n  }"
        }
    },
    {
        "filename": "HADOOP-9977.json",
        "creation_time": "2013-09-17T22:11:54.000+0000",
        "bug_report": {
            "Title": "Hadoop services won't start with different keypass and keystorepass when https is enabled",
            "Description": "When enabling SSL in Hadoop, if the keystore is created with different keypass and keystore passwords, the services such as Namenode, ResourceManager, Datanode, Nodemanager, and SecondaryNamenode fail to start. The issue arises specifically when the key cannot be recovered due to a mismatch in the passwords or other keystore-related issues. The error message indicates a `java.security.UnrecoverableKeyException`, which typically occurs when the provided password does not match the key's password or if the key is not present in the keystore.",
            "StackTrace": [
                "2013-09-17 21:39:00,794 FATAL namenode.NameNode (NameNode.java:main(1325)) - Exception in namenode join",
                "java.io.IOException: java.security.UnrecoverableKeyException: Cannot recover key",
                "at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:222)",
                "at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:174)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer$1.<init>(NameNodeHttpServer.java:76)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:74)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:626)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:488)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:684)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:669)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1254)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1320)",
                "Caused by: java.security.UnrecoverableKeyException: Cannot recover key",
                "at sun.security.provider.KeyProtector.recover(KeyProtector.java:328)",
                "at sun.security.provider.JavaKeyStore.engineGetKey(JavaKeyStore.java:138)",
                "at sun.security.provider.JavaKeyStore$JKS.engineGetKey(JavaKeyStore.java:55)",
                "at java.security.KeyStore.getKey(KeyStore.java:792)",
                "at sun.security.ssl.SunX509KeyManagerImpl.<init>(SunX509KeyManagerImpl.java:131)",
                "at sun.security.ssl.KeyManagerFactoryImpl$SunX509.engineInit(KeyManagerFactoryImpl.java:68)",
                "at javax.net.ssl.KeyManagerFactory.init(KeyManagerFactoryImpl.java:259)",
                "at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:170)",
                "at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:121)",
                "at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:220)",
                "... 9 more"
            ],
            "RootCause": "The root cause of the issue is the `java.security.UnrecoverableKeyException`, which indicates that the key cannot be recovered from the keystore due to incorrect passwords or the key not being present in the keystore.",
            "StepsToReproduce": [
                "Enable SSL in the Hadoop configuration.",
                "Create a keystore with different keypass and keystore passwords using the command: keytool -genkey -alias host1 -keyalg RSA -keysize 1024 -dname 'CN=host1,OU=cm,O=cm,L=san jose,ST=ca,C=us' -keypass hadoop -keystore keystore.jks -storepass hadoopKey.",
                "Set the following properties in ssl-server.xml: <property><name>ssl.server.keystore.keypassword</name><value>hadoop</value></property> and <property><name>ssl.server.keystore.password</name><value>hadoopKey</value></property>.",
                "Attempt to start the Namenode, ResourceManager, Datanode, Nodemanager, and SecondaryNamenode."
            ],
            "ExpectedBehavior": "The Hadoop services should start successfully without any errors related to key recovery.",
            "ObservedBehavior": "The Hadoop services fail to start, and the following error is logged: 'java.security.UnrecoverableKeyException: Cannot recover key'.",
            "Suggestions": "Verify that the keypass and keystore password are correct and match the ones used during the keystore creation. Ensure that the key alias exists in the keystore and that the keystore file is not corrupted. If necessary, recreate the keystore with matching passwords.",
            "problem_location": {
                "files": [
                    "ssl-server.xml",
                    "NameNode.java",
                    "HttpServer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.NameNode",
                    "org.apache.hadoop.http.HttpServer",
                    "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory"
                ],
                "methods": [
                    "NameNode.startHttpServer",
                    "HttpServer.init",
                    "FileBasedKeyStoresFactory.init"
                ]
            },
            "possible_fix": "Ensure that the passwords used for the keystore and key are consistent. If the keystore is corrupted, restore it from a backup or recreate it with the correct parameters."
        },
        "possible_fix_code": {
            "FileBasedKeyStoresFactory.init": "  public void init(SSLFactory.Mode mode)\n    throws IOException, GeneralSecurityException {\n\n    boolean requireClientCert =\n      conf.getBoolean(SSLFactory.SSL_REQUIRE_CLIENT_CERT_KEY, true);\n\n    // certificate store\n    String keystoreType =\n      conf.get(resolvePropertyName(mode, SSL_KEYSTORE_TYPE_TPL_KEY),\n               DEFAULT_KEYSTORE_TYPE);\n    KeyStore keystore = KeyStore.getInstance(keystoreType);\n    String keystorePassword = null;\n    if (requireClientCert || mode == SSLFactory.Mode.SERVER) {\n      String locationProperty =\n        resolvePropertyName(mode, SSL_KEYSTORE_LOCATION_TPL_KEY);\n      String keystoreLocation = conf.get(locationProperty, \"\");\n      if (keystoreLocation.isEmpty()) {\n        throw new GeneralSecurityException(\"The property '\" + locationProperty +\n          \"' has not been set in the ssl configuration file.\");\n      }\n      String passwordProperty =\n        resolvePropertyName(mode, SSL_KEYSTORE_PASSWORD_TPL_KEY);\n      keystorePassword = conf.get(passwordProperty, \"\");\n      if (keystorePassword.isEmpty()) {\n        throw new GeneralSecurityException(\"The property '\" + passwordProperty +\n          \"' has not been set in the ssl configuration file.\");\n      }\n      LOG.debug(mode.toString() + \" KeyStore: \" + keystoreLocation);\n\n      InputStream is = new FileInputStream(keystoreLocation);\n      try {\n        keystore.load(is, keystorePassword.toCharArray());\n      } finally {\n        is.close();\n      }\n      LOG.debug(mode.toString() + \" Loaded KeyStore: \" + keystoreLocation);\n    } else {\n      keystore.load(null, null);\n    }\n    KeyManagerFactory keyMgrFactory = KeyManagerFactory\n        .getInstance(SSLFactory.SSLCERTIFICATE);\n      \n    keyMgrFactory.init(keystore, (keystorePassword != null) ?\n                                 keystorePassword.toCharArray() : null);\n    keyManagers = keyMgrFactory.getKeyManagers();\n\n    //trust store\n    String truststoreType =\n      conf.get(resolvePropertyName(mode, SSL_TRUSTSTORE_TYPE_TPL_KEY),\n               DEFAULT_KEYSTORE_TYPE);\n\n    String locationProperty =\n      resolvePropertyName(mode, SSL_TRUSTSTORE_LOCATION_TPL_KEY);\n    String truststoreLocation = conf.get(locationProperty, \"\");\n    if (truststoreLocation.isEmpty()) {\n      throw new GeneralSecurityException(\"The property '\" + locationProperty +\n        \"' has not been set in the ssl configuration file.\");\n    }\n\n    String passwordProperty = resolvePropertyName(mode,\n                                                  SSL_TRUSTSTORE_PASSWORD_TPL_KEY);\n    String truststorePassword = conf.get(passwordProperty, \"\");\n    if (truststorePassword.isEmpty()) {\n      throw new GeneralSecurityException(\"The property '\" + passwordProperty +\n        \"' has not been set in the ssl configuration file.\");\n    }\n    long truststoreReloadInterval =\n      conf.getLong(\n        resolvePropertyName(mode, SSL_TRUSTSTORE_RELOAD_INTERVAL_TPL_KEY),\n        DEFAULT_SSL_TRUSTSTORE_RELOAD_INTERVAL);\n\n    LOG.debug(mode.toString() + \" TrustStore: \" + truststoreLocation);\n\n    trustManager = new ReloadingX509TrustManager(truststoreType,\n                                                 truststoreLocation,\n                                                 truststorePassword,\n                                                 truststoreReloadInterval);\n    trustManager.init();\n    LOG.debug(mode.toString() + \" Loaded TrustStore: \" + truststoreLocation);\n\n    trustManagers = new TrustManager[]{trustManager};\n  }"
        }
    },
    {
        "filename": "HADOOP-12611.json",
        "creation_time": "2015-12-01T17:14:38.000+0000",
        "bug_report": {
            "Title": "TestZKSignerSecretProvider#testMultipleInit occasionally fail",
            "Description": "The test case `testMultipleInit` in the `TestZKSignerSecretProvider` class is failing intermittently, resulting in an `AssertionError` that indicates an expected null value is not null. This issue appears to be related to the improper initialization of the `ZKSignerSecretProvider` class, particularly in its interaction with ZooKeeper. The failure may have been introduced after the changes made in HADOOP-12181, which altered the initialization logic of the `ZKSignerSecretProvider`.",
            "StackTrace": [
                "java.lang.AssertionError: expected null, but was:<[B@142bad79>",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.failNotNull(Assert.java:664)",
                "at org.junit.Assert.assertNull(Assert.java:646)",
                "at org.junit.Assert.assertNull(Assert.java:656)",
                "at org.apache.hadoop.security.authentication.util.TestZKSignerSecretProvider.testMultipleInit(TestZKSignerSecretProvider.java:149)",
                "2015-11-29 00:24:33,325 ERROR ZKSignerSecretProvider - An unexpected exception occurred while pulling data from ZooKeeper",
                "java.lang.IllegalStateException: instance must be started before calling this method",
                "at com.google.common.base.Preconditions.checkState(Preconditions.java:145)",
                "at org.apache.curator.framework.imps.CuratorFrameworkImpl.getData(CuratorFrameworkImpl.java:363)",
                "at org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.pullFromZK(ZKSignerSecretProvider.java:341)",
                "at org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.rollSecret(ZKSignerSecretProvider.java:264)"
            ],
            "RootCause": "The root cause of the issue is the failure to properly initialize the `ZKSignerSecretProvider` before invoking methods that interact with ZooKeeper. Specifically, the `pullFromZK` method throws an `IllegalStateException` if it is called before the instance is fully initialized, leading to the `AssertionError` in the test case when the expected value is not null.",
            "StepsToReproduce": [
                "Run the test case `testMultipleInit` in the `TestZKSignerSecretProvider` class.",
                "Ensure that the `ZKSignerSecretProvider` is initialized multiple times within the test.",
                "Observe the failure of the test due to the `AssertionError`."
            ],
            "ExpectedBehavior": "The test should pass without any assertion errors, indicating that the expected value is null as intended.",
            "ObservedBehavior": "The test fails with an `AssertionError`, indicating that a value expected to be null is not null, along with an `IllegalStateException` indicating that the instance must be started before calling the method.",
            "Suggestions": "Ensure that the `init` method of the `ZKSignerSecretProvider` is called before any other methods that interact with ZooKeeper. Review the test case to ensure proper initialization and consider adding logging to verify the state of the `ZKSignerSecretProvider` before and after initialization.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/ZKSignerSecretProvider.java",
                    "hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/util/TestZKSignerSecretProvider.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider",
                    "org.apache.hadoop.security.authentication.util.TestZKSignerSecretProvider"
                ],
                "methods": [
                    "ZKSignerSecretProvider.pullFromZK",
                    "ZKSignerSecretProvider.rollSecret",
                    "TestZKSignerSecretProvider.testMultipleInit"
                ]
            },
            "possible_fix": "Review the initialization sequence in the `testMultipleInit` method to ensure that the `init` method of `ZKSignerSecretProvider` is called before any other method that interacts with ZooKeeper. Additionally, consider adding checks to prevent multiple initializations that could lead to race conditions."
        },
        "possible_fix_code": {
            "ZKSignerSecretProvider.init": "  @Override\n  public void init(Properties config, ServletContext servletContext,\n          long tokenValidity) throws Exception {\n    Object curatorClientObj = servletContext.getAttribute(\n            ZOOKEEPER_SIGNER_SECRET_PROVIDER_CURATOR_CLIENT_ATTRIBUTE);\n    if (curatorClientObj != null\n            && curatorClientObj instanceof CuratorFramework) {\n      client = (CuratorFramework) curatorClientObj;\n    } else {\n      client = createCuratorClient(config);\n      servletContext.setAttribute(\n          ZOOKEEPER_SIGNER_SECRET_PROVIDER_CURATOR_CLIENT_ATTRIBUTE, client);\n    }\n    this.tokenValidity = tokenValidity;\n    shouldDisconnect = Boolean.parseBoolean(\n            config.getProperty(DISCONNECT_FROM_ZOOKEEPER_ON_SHUTDOWN, \"true\"));\n    path = config.getProperty(ZOOKEEPER_PATH);\n    if (path == null) {\n      throw new IllegalArgumentException(ZOOKEEPER_PATH\n              + \" must be specified\");\n    }\n    try {\n      nextRolloverDate = System.currentTimeMillis() + tokenValidity;\n      // everyone tries to do this, only one will succeed and only when the\n      // znode doesn't already exist.  Everyone else will synchronize on the\n      // data from the znode\n      client.create().creatingParentsIfNeeded()\n              .forPath(path, generateZKData(generateRandomSecret(),\n              generateRandomSecret(), null));\n      zkVersion = 0;\n      LOG.info(\"Creating secret znode\");\n    } catch (KeeperException.NodeExistsException nee) {\n      LOG.info(\"The secret znode already exists, retrieving data\");\n    }\n    // Synchronize on the data from the znode\n    // passing true tells it to parse out all the data for initing\n    pullFromZK(true);\n    long initialDelay = nextRolloverDate - System.currentTimeMillis();\n    // If it's in the past, try to find the next interval that we should\n    // be using\n    if (initialDelay < 1l) {\n      int i = 1;\n      while (initialDelay < 1l) {\n        initialDelay = nextRolloverDate + tokenValidity * i\n                - System.currentTimeMillis();\n        i++;\n      }\n    }\n    super.startScheduler(initialDelay, tokenValidity);\n  }"
        }
    },
    {
        "filename": "HADOOP-10142.json",
        "creation_time": "2013-12-03T06:33:32.000+0000",
        "bug_report": {
            "Title": "Avoid groups lookup for unprivileged users such as \"dr.who\"",
            "Description": "The system generates excessive logs when attempting to retrieve group information for non-existent users, such as 'dr.who'. The log entries indicate that the user does not exist, leading to repeated warnings and potential performance issues. This behavior is triggered by the `ShellBasedUnixGroupsMapping.getUnixGroups` method, which calls the `Shell.execCommand` method to execute a command that checks for user groups. If the user is not found, an `ExitCodeException` is thrown, resulting in a warning log entry.",
            "StackTrace": [
                "2013-12-03 11:34:56,589 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user dr.who",
                "org.apache.hadoop.util.Shell$ExitCodeException: id: dr.who: No such user",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:504)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:417)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:636)",
                "at org.apache.hadoop.util.Shell.execCommand(Shell.java:725)",
                "at org.apache.hadoop.util.Shell.execCommand(Shell.java:708)",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52)",
                "at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)",
                "at org.apache.hadoop.security.Groups.getGroups(Groups.java:95)",
                "at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1376)",
                "at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:63)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker(FSNamesystem.java:3228)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListingInt(FSNamesystem.java:4063)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListing(FSNamesystem.java:4052)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getListing(NameNodeRpcServer.java:748)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getDirectoryListing(NamenodeWebHdfsMethods.java:715)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getListingStream(NamenodeWebHdfsMethods.java:727)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get(NamenodeWebHdfsMethods.java:675)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.access$400(NamenodeWebHdfsMethods.java:114)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3.run(NamenodeWebHdfsMethods.java:623)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3.run(NamenodeWebHdfsMethods.java:618)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1515)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get(NamenodeWebHdfsMethods.java:618)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getRoot(NamenodeWebHdfsMethods.java:586)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(HttpMethodRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(HttpMethodRule.java:84)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:384)",
                "at org.apache.hadoop.hdfs.web.AuthFilter.doFilter(AuthFilter.java:85)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1310)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)"
            ],
            "RootCause": "The root cause of the issue is that the user 'dr.who' does not exist on the system, leading to an `ExitCodeException` when attempting to retrieve group information for this user.",
            "StepsToReproduce": [
                "Attempt to access group information for a non-existent user, such as 'dr.who', using WebHDFS.",
                "Observe the logs generated by the system for warnings related to user group retrieval."
            ],
            "ExpectedBehavior": "The system should not attempt to look up groups for users that do not exist, thereby avoiding unnecessary log entries and performance overhead.",
            "ObservedBehavior": "The system generates multiple warning log entries indicating that the user 'dr.who' does not exist, leading to excessive logging and potential performance issues.",
            "Suggestions": "To mitigate this issue, verify the existence of users before attempting to retrieve their group information. If a user does not exist, either create the user or update the configuration to use a valid user.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Shell.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.ShellBasedUnixGroupsMapping",
                    "org.apache.hadoop.util.Shell"
                ],
                "methods": [
                    "ShellBasedUnixGroupsMapping.getUnixGroups",
                    "Shell.execCommand",
                    "Shell.runCommand"
                ]
            },
            "possible_fix": "To prevent the logging of errors for non-existent users, modify the `getUnixGroups` method to check if the user exists before calling `execCommand`. If the user does not exist, return an empty list instead of executing the command."
        },
        "possible_fix_code": {
            "getUnixGroups": "  private static List<String> getUnixGroups(final String user) throws IOException {\n    // Check if the user exists before attempting to retrieve groups\n    if (!userExists(user)) {\n      LOG.warn(\"User does not exist: \" + user);\n      return Collections.emptyList(); // Return an empty list if user does not exist\n    }\n    String result = \"\";\n    try {\n      result = Shell.execCommand(Shell.getGroupsForUserCommand(user));\n    } catch (ExitCodeException e) {\n      // if we didn't get the group - just return empty list;\n      LOG.warn(\"got exception trying to get groups for user \" + user, e);\n    }\n    \n    StringTokenizer tokenizer =\n        new StringTokenizer(result, Shell.TOKEN_SEPARATOR_REGEX);\n    List<String> groups = new LinkedList<String>();\n    while (tokenizer.hasMoreTokens()) {\n      groups.add(tokenizer.nextToken());\n    }\n    return groups;\n  }\n\n  // Helper method to check if a user exists\n  private static boolean userExists(String user) {\n    // Implement logic to check if the user exists in the system\n    // This could involve executing a command like 'id' or 'getent'\n    // For simplicity, we will return false here, but this should be replaced\n    // with actual user existence check logic.\n    return false; // Placeholder implementation\n  }"
        }
    },
    {
        "filename": "HADOOP-14727.json",
        "creation_time": "2017-08-02T21:56:38.000+0000",
        "bug_report": {
            "Title": "Socket not closed properly when reading Configurations with BlockReaderRemote",
            "Description": "This issue was identified during Cloudera's internal testing of the alpha4 release, where multiple hosts experienced a shortage of file descriptors (FDs). Investigation revealed that both the Oozie server and the Yarn JobHistoryServer had numerous sockets in the CLOSE_WAIT state. The problem was consistently reproducible by accessing the JobHistoryServer (JHS) web UI and navigating through job logs. Despite reviewing the BlockReaderRemote implementation, no leaks were initially detected. However, adding debug logs for Peer creation and closure indicated that the CLOSE_WAIT sockets originated from the BlockReaderFactory's getRemoteBlockReaderFromTcp method. Further analysis showed that reverting recent commits to the Configuration class eliminated the CLOSE_WAIT sockets, suggesting a link between these changes and the socket management issue.",
            "StackTrace": [
                "2017-08-02 13:58:59,901 INFO org.apache.hadoop.hdfs.client.impl.BlockReaderFactory: ____ associated peer NioInetPeer(Socket[addr=/10.17.196.28,port=20002,localport=42512]) with blockreader org.apache.hadoop.hdfs.client.impl.BlockReaderRemote@717ce109",
                "java.lang.Exception: test",
                "at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:745)",
                "at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)",
                "at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:636)",
                "at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)",
                "at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:749)",
                "at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:807)",
                "at java.io.DataInputStream.read(DataInputStream.java:149)",
                "at com.ctc.wstx.io.StreamBootstrapper.ensureLoaded(StreamBootstrapper.java:482)",
                "at com.ctc.wstx.io.StreamBootstrapper.resolveStreamEncoding(StreamBootstrapper.java:306)",
                "at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:167)",
                "at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2649)",
                "at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2697)",
                "at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2662)",
                "at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2545)",
                "at org.apache.hadoop.conf.Configuration.get(Configuration.java:1076)",
                "at org.apache.hadoop.mapreduce.counters.Limits.init(Limits.java:45)",
                "at org.apache.hadoop.mapreduce.counters.Limits.reset(Limits.java:130)",
                "at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.loadFullHistoryData(CompletedJob.java:363)",
                "at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.<init>(CompletedJob.java:105)",
                "at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.loadJob(HistoryFileManager.java:473)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.loadJob(CachedHistoryStorage.java:180)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.access$000(CachedHistoryStorage.java:52)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage$1.load(CachedHistoryStorage.java:103)",
                "at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568)",
                "at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350)",
                "at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313)",
                "at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228)",
                "at com.google.common.cache.LocalCache.get(LocalCache.java:3965)",
                "at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3969)",
                "at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4829)",
                "at com.google.common.cache.LocalCache$LocalManualCache.getUnchecked(LocalCache.java:4834)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getFullJob(CachedHistoryStorage.java:193)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistory.getJob(JobHistory.java:220)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.requireJob(AppController.java:416)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts(AppController.java:277)",
                "at org.apache.hadoop.mapreduce.v2.hs.webapp.HsController.attempts(HsController.java:152)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:162)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)",
                "at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287)",
                "at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)",
                "at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)",
                "at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:941)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:875)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)",
                "at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)",
                "at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter$1.java:133)",
                "at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter$1.java:130)",
                "at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter$1.java:203)",
                "at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1552)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)",
                "at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)",
                "at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)",
                "at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
                "at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)",
                "at org.eclipse.jetty.server.Server.handle(Server.java:534)",
                "at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)",
                "at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)",
                "at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)",
                "at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)",
                "at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The root cause of the issue appears to be related to the improper management of InputStream resources in the BlockReaderRemote implementation, leading to sockets remaining in the CLOSE_WAIT state due to not being closed correctly after use.",
            "StepsToReproduce": [
                "Access the JobHistoryServer (JHS) web UI.",
                "Navigate through a job and its logs."
            ],
            "ExpectedBehavior": "All sockets should be properly closed after their use, preventing any sockets from remaining in the CLOSE_WAIT state.",
            "ObservedBehavior": "Numerous sockets remain in the CLOSE_WAIT state, leading to exhaustion of file descriptors on the affected hosts.",
            "Suggestions": "Review the implementation of the BlockReaderRemote and ensure that InputStreams are closed properly after their use. Consider adding explicit close calls or using try-with-resources statements to manage resource closure.",
            "problem_location": {
                "files": [
                    "BlockReaderFactory.java",
                    "DFSInputStream.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.client.impl.BlockReaderFactory",
                    "org.apache.hadoop.hdfs.DFSInputStream"
                ],
                "methods": [
                    "getRemoteBlockReaderFromTcp",
                    "blockSeekTo"
                ]
            },
            "possible_fix": "Ensure that InputStreams are closed properly in the BlockReaderRemote implementation. This may involve modifying the getRemoteBlockReaderFromTcp method to include proper resource management."
        },
        "possible_fix_code": {
            "getRemoteBlockReaderFromTcp": "private BlockReader getRemoteBlockReaderFromTcp(String blockPoolId, Block block, long timeout) throws IOException {\n    Socket socket = null;\n    InputStream in = null;\n    try {\n        socket = createSocket();\n        in = socket.getInputStream();\n        // Additional logic to read from the input stream\n        return new BlockReaderRemote(blockPoolId, block, in, socket);\n    } catch (IOException e) {\n        LOG.error(\"Error getting remote block reader: \" + e.getMessage(), e);\n        throw e;\n    } finally {\n        // Ensure the InputStream and Socket are closed properly\n        if (in != null) {\n            try {\n                in.close();\n            } catch (IOException e) {\n                LOG.warn(\"Failed to close InputStream: \" + e.getMessage(), e);\n            }\n        }\n        if (socket != null) {\n            try {\n                socket.close();\n            } catch (IOException e) {\n                LOG.warn(\"Failed to close socket: \" + e.getMessage(), e);\n            }\n        }\n    }\n}"
        }
    }
]