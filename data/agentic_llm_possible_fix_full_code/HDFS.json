[
    {
        "filename": "HDFS-4558.json",
        "creation_time": "2013-03-06T11:10:08.000+0000",
        "bug_report": {
            "Title": "start balancer failed with NPE",
            "Description": "The balancer fails to start due to a NullPointerException (NPE) in the BlockPlacementPolicy class. The error occurs when the balancer attempts to check replication policy compatibility, leading to a failure in the initialization process.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicy.getInstance(BlockPlacementPolicy.java:165)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.checkReplicationPolicyCompatibility(Balancer.java:799)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.<init>(Balancer.java:808)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:831)"
            ],
            "RootCause": "The NullPointerException is likely caused by null parameters being passed to the getInstance method of BlockPlacementPolicy or the initialize method in the BlockPlacementPolicy subclass not handling null values properly.",
            "StepsToReproduce": [
                "Start the balancer with a configuration that may have null parameters.",
                "Ensure that the BlockPlacementPolicy is invoked during the balancer's initialization."
            ],
            "ExpectedBehavior": "The balancer should start successfully without throwing a NullPointerException.",
            "ObservedBehavior": "The balancer fails to start and throws a NullPointerException.",
            "Suggestions": "Ensure that the parameters passed to BlockPlacementPolicy.getInstance are not null. Additionally, verify that the initialize method in any subclass of BlockPlacementPolicy properly handles null values.",
            "problem_location": {
                "files": [
                    "BlockPlacementPolicy.java",
                    "Balancer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicy",
                    "org.apache.hadoop.hdfs.server.balancer.Balancer"
                ],
                "methods": [
                    "BlockPlacementPolicy.getInstance",
                    "Balancer.checkReplicationPolicyCompatibility"
                ]
            },
            "possible_fix": "Add null checks for the parameters in the getInstance method of BlockPlacementPolicy and ensure that the initialize method in BlockPlacementPolicyDefault handles null parameters appropriately."
        },
        "possible_fix_code": {
            "BlockPlacementPolicy.getInstance": "  public static BlockPlacementPolicy getInstance(Configuration conf, \n                                                 FSClusterStats stats,\n                                                 NetworkTopology clusterMap) {\n    if (conf == null || stats == null || clusterMap == null) {\n        throw new IllegalArgumentException(\"Configuration, FSClusterStats, and NetworkTopology must not be null\");\n    }\n    Class<? extends BlockPlacementPolicy> replicatorClass =\n                      conf.getClass(\"dfs.block.replicator.classname\",\n                                    BlockPlacementPolicyDefault.class,\n                                    BlockPlacementPolicy.class);\n    BlockPlacementPolicy replicator = (BlockPlacementPolicy) ReflectionUtils.newInstance(\n                                                             replicatorClass, conf);\n    replicator.initialize(conf, stats, clusterMap);\n    return replicator;\n  }"
        }
    },
    {
        "filename": "HDFS-13039.json",
        "creation_time": "2018-01-19T18:52:12.000+0000",
        "bug_report": {
            "Title": "StripedBlockReader#createBlockReader leaks socket on IOException",
            "Description": "The DataNode in the cluster is experiencing a severe resource leak, resulting in millions of {{CLOSE_WAIT}} connections. This issue is primarily caused by the application exceeding the limit of open file descriptors, leading to a `java.io.IOException` with the message 'Too many open files'. The log indicates that the DataNode cannot open any new files or sockets, which is critical for its operation. The stack trace shows that the error occurs during the acceptance of new connections, indicating a failure in handling socket connections properly.",
            "StackTrace": [
                "java.io.IOException: Too many open files",
                "at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)",
                "at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422)",
                "at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250)",
                "at io.netty.channel.socket.nio.NioServerSocketChannel.doReadMessages(NioServerSocketChannel.java:135)",
                "at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:75)",
                "at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:563)",
                "at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:504)",
                "at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:418)",
                "at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:390)",
                "at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:742)",
                "at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:145)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The root cause of the issue is the exhaustion of file descriptors due to improper management of socket connections, leading to a 'Too many open files' error.",
            "StepsToReproduce": [
                "Run the EC on the cluster with a high number of simultaneous connections.",
                "Monitor the number of open file descriptors using a command like 'lsof'.",
                "Observe the log for 'Too many open files' errors."
            ],
            "ExpectedBehavior": "The DataNode should be able to handle multiple connections without exceeding the file descriptor limit, allowing it to open new files and sockets as needed.",
            "ObservedBehavior": "The DataNode is unable to open new files or sockets, resulting in a 'Too many open files' error and millions of {{CLOSE_WAIT}} connections.",
            "Suggestions": "1. Increase the file descriptor limits on the operating system using 'ulimit'. 2. Ensure that all file streams, sockets, and channels are properly closed after use. 3. Implement connection pooling to manage resources more efficiently. 4. Monitor open file descriptors to identify and address leaks.",
            "problem_location": {
                "files": [
                    "StripedBlockReader.java"
                ],
                "classes": [
                    "com.example.StripedBlockReader"
                ],
                "methods": [
                    "createBlockReader"
                ]
            },
            "possible_fix": "Ensure that all socket connections are closed properly in a 'finally' block or use try-with-resources to manage resources effectively. Additionally, consider implementing connection pooling to reduce the number of simultaneous open connections."
        },
        "possible_fix_code": {
            "createBlockReader": "public void createBlockReader() {\n    Socket socket = null;\n    try {\n        // Code to create and use the socket\n        socket = new Socket(host, port);\n        // Additional logic for reading from the socket\n    } catch (IOException e) {\n        // Handle the IOException\n        System.err.println(\"IOException occurred: \" + e.getMessage());\n    } finally {\n        // Ensure the socket is closed to prevent resource leaks\n        if (socket != null) {\n            try {\n                socket.close();\n            } catch (IOException e) {\n                System.err.println(\"Failed to close socket: \" + e.getMessage());\n            }\n        }\n    }\n}"
        }
    },
    {
        "filename": "HDFS-13023.json",
        "creation_time": "2018-01-16T20:27:39.000+0000",
        "bug_report": {
            "Title": "Journal Sync does not work on a secure cluster",
            "Description": "The JournalNodeSyncer fails to sync with the journal node due to an authorization issue. The error log indicates that the user `nn/xxx` is not authorized to access the `QJournalProtocol`, which is restricted to the principal `nn/xxx@EXAMPLE.COM`. This suggests a misconfiguration in user permissions or Kerberos authentication settings.",
            "StackTrace": [
                "2018-01-10 01:15:40,517 INFO server.JournalNodeSyncer (JournalNodeSyncer.java:syncWithJournalAtIndex(235)) - Syncing Journal /0.0.0.0:8485 with xxx, journal id: mycluster",
                "2018-01-10 01:15:40,583 ERROR server.JournalNodeSyncer (JournalNodeSyncer.java:syncWithJournalAtIndex(259)) - Could not sync with Journal at xxx/xxx:8485",
                "com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User nn/xxx (auth:PROXY) via jn/xxx (auth:KERBEROS) is not authorized for protocol interface org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol: this service is only accessible by nn/xxx@EXAMPLE.COM",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:242)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)",
                "at com.sun.proxy.$Proxy16.getEditLogManifest(Unknown Source)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncWithJournalAtIndex(JournalNodeSyncer.java:254)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncJournals(JournalNodeSyncer.java:230)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.lambda$startSyncJournalsDaemon$0(JournalNodeSyncer.java:190)",
                "at java.lang.Thread.run(Thread.java:748)",
                "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User nn/xxx (auth:PROXY) via jn/xxx (auth:KERBEROS) is not authorized for protocol interface org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol: this service is only accessible by nn/xxx",
                "at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1437)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1347)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)",
                "... 6 more"
            ],
            "RootCause": "The root cause of the issue is an authorization failure where the user `nn/xxx` does not have the necessary permissions to access the `QJournalProtocol`. This is likely due to incorrect Kerberos principal configuration or user permissions in the Hadoop security settings.",
            "StepsToReproduce": [
                "1. Set up a secure Hadoop cluster with Kerberos authentication.",
                "2. Attempt to sync the journal using the JournalNodeSyncer.",
                "3. Observe the error message indicating authorization failure."
            ],
            "ExpectedBehavior": "The JournalNodeSyncer should successfully sync with the journal node without any authorization errors.",
            "ObservedBehavior": "The JournalNodeSyncer fails to sync with the journal node, throwing an authorization exception indicating that the user is not authorized for the `QJournalProtocol`.",
            "Suggestions": "1. Verify that the user `nn/xxx` is configured with the correct Kerberos principal `nn/xxx@EXAMPLE.COM`.\n2. Review the Hadoop configuration files (core-site.xml, hdfs-site.xml) to ensure proper security settings for Kerberos authentication.\n3. Check and modify the access control lists (ACLs) to grant the necessary permissions to the user for accessing the `QJournalProtocol`.\n4. Consult Hadoop logs for additional context on the authorization failure.",
            "problem_location": {
                "files": [
                    "JournalNodeSyncer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer"
                ],
                "methods": [
                    "syncWithJournalAtIndex"
                ]
            },
            "possible_fix": "Ensure that the user `nn/xxx` has the correct Kerberos principal and permissions set up. This may involve updating the Hadoop configuration files and modifying user permissions in the security settings."
        },
        "possible_fix_code": {
            "syncWithJournalAtIndex": "private void syncWithJournalAtIndex(int index) {\n    LOG.info(\"Syncing Journal \" + jn.getBoundIpcAddress().getAddress() + \":\" + jn.getBoundIpcAddress().getPort() + \" with \" + otherJNProxies.get(index) + \", journal id: \" + jid);\n    final QJournalProtocolPB jnProxy = otherJNProxies.get(index).jnProxy;\n    if (jnProxy == null) {\n      LOG.error(\"JournalNode Proxy not found.\");\n      return;\n    }\n\n    List<RemoteEditLog> thisJournalEditLogs;\n    try {\n      thisJournalEditLogs = journal.getEditLogManifest(0, false).getLogs();\n    } catch (IOException e) {\n      LOG.error(\"Exception in getting local edit log manifest\", e);\n      return;\n    }\n\n    GetEditLogManifestResponseProto editLogManifest;\n    try {\n      editLogManifest = jnProxy.getEditLogManifest(null,\n          GetEditLogManifestRequestProto.newBuilder().setJid(jidProto)\n              .setSinceTxId(0)\n              .setInProgressOk(false).build());\n    } catch (ServiceException e) {\n      LOG.error(\"Could not sync with Journal at \" + otherJNProxies.get(journalNodeIndexForSync), e);\n      return;\n    }\n\n    // Check user authorization before proceeding\n    if (!isUserAuthorized()) {\n      LOG.error(\"User is not authorized to access QJournalProtocol.\");\n      return;\n    }\n\n    getMissingLogSegments(thisJournalEditLogs, editLogManifest, otherJNProxies.get(index));\n  }\n\n  private boolean isUserAuthorized() {\n    // Implement the logic to check if the user has the correct Kerberos principal\n    // and permissions to access the QJournalProtocol\n    String currentUser = UserGroupInformation.getCurrentUser().getUserName();\n    return currentUser.equals(\"nn/xxx@EXAMPLE.COM\"); // Example check\n  }"
        }
    },
    {
        "filename": "HDFS-3157.json",
        "creation_time": "2012-03-28T14:35:53.000+0000",
        "bug_report": {
            "Title": "Error in deleting block is keep on coming from DN even after the block report and directory scanning has happened",
            "Description": "The issue arises in a Hadoop Distributed File System (HDFS) setup with one NameNode (NN) and three DataNodes (DN1, DN2, DN3) with a replication factor of 2. The problem occurs when a file, 'a.txt', is written and then blocks are deleted from DN1 while the file remains open. After closing the file, the system attempts to delete the blocks, leading to errors in the DataNode logs indicating that the block cannot be found in the volume map. The stack trace shows that the error originates from the `FSDataset.invalidate` method, which is responsible for invalidating and deleting blocks.",
            "StackTrace": [
                "2012-03-19 13:41:36,905 INFO org.apache.hadoop.hdfs.StateChange: BLOCK NameSystem.addToCorruptReplicasMap: duplicate requested for blk_2903555284838653156 to add as corrupt on XX.XX.XX.XX by /XX.XX.XX.XX because reported RBW replica with genstamp 1002 does not match COMPLETE block's genstamp in block map 1003",
                "2012-03-19 13:41:39,588 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* Removing block blk_2903555284838653156_1003 from neededReplications as it has enough replicas.",
                "2012-02-29 13:54:13,126 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block blk_2903555284838653156_1003. BlockInfo not found in volumeMap.",
                "2012-02-29 13:54:13,126 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Error processing datanode Command",
                "java.io.IOException: Error in deleting blocks.",
                "at org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate(FSDataset.java:2061)",
                "at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:581)",
                "at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:545)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand(BPServiceActor.java:690)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:522)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:662)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The root cause of the issue is that the `FSDataset.invalidate` method is unable to find the block in the volume map due to a mismatch in the generation stamp or the block not being present in the expected state. This leads to an IOException being thrown when attempting to delete the block.",
            "StepsToReproduce": [
                "1. Set up a cluster with 1 NameNode and 3 DataNodes (DN1, DN2, DN3) with a replication factor of 2.",
                "2. Write a file 'a.txt' and keep it open without closing it.",
                "3. Delete the blocks from DN1 while the file is still open.",
                "4. Close the file.",
                "5. Observe the logs for errors related to block deletion."
            ],
            "ExpectedBehavior": "The blocks should be successfully deleted from the DataNode without any errors, and the system should update the block map accordingly.",
            "ObservedBehavior": "The system logs show warnings and errors indicating that the block cannot be found in the volume map, leading to an IOException when attempting to delete the blocks.",
            "Suggestions": "Investigate the state of the blocks in the volume map and ensure that the generation stamps match. Consider adding checks in the `invalidate` method to handle cases where blocks are not found more gracefully.",
            "problem_location": {
                "files": [
                    "FSDataset.java",
                    "DataNode.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.FSDataset",
                    "org.apache.hadoop.hdfs.server.datanode.DataNode"
                ],
                "methods": [
                    "FSDataset.invalidate",
                    "BPOfferService.processCommandFromActive",
                    "BPOfferService.processCommandFromActor"
                ]
            },
            "possible_fix": "In the `FSDataset.invalidate` method, add additional logging to capture the state of the block and its generation stamp before attempting deletion. Ensure that the method handles cases where the block is not found in the volume map without throwing an IOException."
        },
        "possible_fix_code": {
            "FSDataset.invalidate": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error = false;\n    for (int i = 0; i < invalidBlks.length; i++) {\n      File f = null;\n      final FSVolume v;\n      synchronized (this) {\n        f = getFile(bpid, invalidBlks[i].getBlockId());\n        ReplicaInfo dinfo = volumeMap.get(bpid, invalidBlks[i]);\n        if (dinfo == null || \n            dinfo.getGenerationStamp() != invalidBlks[i].getGenerationStamp()) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                           + invalidBlks[i] + \n                           \". BlockInfo not found in volumeMap.\");\n          error = true;\n          continue;\n        }\n        v = (FSVolume)dinfo.getVolume();\n        if (f == null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Block not found in blockMap.\" +\n                            ((v == null) ? \" \" : \" Block found in volumeMap.\"));\n          error = true;\n          continue;\n        }\n        if (v == null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". No volume for this block.\" +\n                            \" Block found in blockMap. \" + f + \".\");\n          error = true;\n          continue;\n        }\n        File parent = f.getParentFile();\n        if (parent == null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Parent not found for file \" + f + \".\");\n          error = true;\n          continue;\n        }\n        ReplicaState replicaState = dinfo.getState();\n        if (replicaState == ReplicaState.FINALIZED || \n            (replicaState == ReplicaState.RUR && \n                ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() == \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      File metaFile = DatanodeUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp());\n\n      // Additional logging to capture the state of the block before deletion\n      DataNode.LOG.info(\"Attempting to delete block: \" + invalidBlks[i] + \", Generation Stamp: \" + invalidBlks[i].getGenerationStamp());\n\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f, metaFile,\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-4850.json",
        "creation_time": "2013-05-25T00:18:15.000+0000",
        "bug_report": {
            "Title": "fix OfflineImageViewer to work on fsimages with empty files or snapshots",
            "Description": "The issue arises when using the OfflineImageViewer on a filesystem image (fsimage) that includes an empty file. After creating an empty file in HDFS and forcing a checkpoint, the OfflineImageViewer fails with a NegativeArraySizeException. This exception indicates that the method attempting to read the file's data is trying to create an array with a negative size, which is not permissible in Java. The stack trace points to the readString method in the Text class, which relies on the WritableUtils.readVIntInRange method to determine the length of the string being read. If the data being processed does not conform to the expected format, it can lead to a negative length being calculated, resulting in the exception.",
            "StackTrace": [
                "Input ended unexpectedly.",
                "2013-05-24 17:01:13,622 ERROR [main] offlineImageViewer.OfflineImageViewer (OfflineImageViewer.java:go(140)) - image loading failed at offset 402",
                "Exception in thread \"main\" java.lang.NegativeArraySizeException",
                "\tat org.apache.hadoop.io.Text.readString(Text.java:458)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processPermission(ImageLoaderCurrent.java:370)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINode(ImageLoaderCurrent.java:671)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processChildren(ImageLoaderCurrent.java:557)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:464)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:470)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processLocalNameINodesWithSnapshot(ImageLoaderCurrent.java:444)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINodes(ImageLoaderCurrent.java:398)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.loadImage(ImageLoaderCurrent.java:199)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.go(OfflineImageViewer.java:136)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.main(OfflineImageViewer.java:260)"
            ],
            "RootCause": "The NegativeArraySizeException is caused by the readString method in the Text class attempting to create a byte array with a negative size. This occurs when WritableUtils.readVIntInRange returns a negative value due to improperly formatted data being read from the DataInput stream.",
            "StepsToReproduce": [
                "Deploy hadoop-trunk HDFS.",
                "Create a directory /user/schu/.",
                "Force a checkpoint and fetch the fsimage.",
                "Run the OfflineImageViewer on the fsimage using the command: hdfs oiv -i fsimage_0000000000000000004 -o oiv_out_1.",
                "Create an empty file /user/schu/testFile1.",
                "Force another checkpoint and fetch the new fsimage.",
                "Run the OfflineImageViewer again using the command: hdfs oiv -i fsimage_0000000000000000008 -o oiv_out_2."
            ],
            "ExpectedBehavior": "The OfflineImageViewer should successfully process the fsimage and include all files, including empty files, without throwing exceptions.",
            "ObservedBehavior": "The OfflineImageViewer fails with a NegativeArraySizeException when processing the fsimage that includes an empty file.",
            "Suggestions": "Investigate the data being read by the readString method to ensure it conforms to the expected format. Consider adding validation to handle cases where the length read is negative.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/Text.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/WritableUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.io.Text",
                    "org.apache.hadoop.io.WritableUtils"
                ],
                "methods": [
                    "Text.readString",
                    "WritableUtils.readVIntInRange"
                ]
            },
            "possible_fix": "Review the implementation of WritableUtils.readVIntInRange to ensure it correctly handles unexpected data formats. Consider adding error handling in the readString method to manage cases where a negative length is returned."
        },
        "possible_fix_code": {
            "org.apache.hadoop.io.Text.readString": "  public static String readString(DataInput in, int maxLength)\n      throws IOException {\n    int length = WritableUtils.readVIntInRange(in, 0, maxLength);\n    if (length < 0) {\n        throw new IOException(\"Invalid string length: \" + length);\n    }\n    byte [] bytes = new byte[length];\n    in.readFully(bytes, 0, length);\n    return decode(bytes);\n  }"
        }
    },
    {
        "filename": "HDFS-3415.json",
        "creation_time": "2012-05-13T13:44:43.000+0000",
        "bug_report": {
            "Title": "During NameNode starting up, it may pick wrong storage directory inspector when the layout versions of the storage directories are different",
            "Description": "The issue occurs when starting the NameNode with multiple storage directories configured. If one of the storage directories has a different layout version than the others, specifically if it is set to 123 while the default is -40, a NullPointerException is thrown during the recovery process. This happens because the system attempts to access a storage file that does not exist due to the mismatch in layout versions.",
            "StackTrace": [
                "2012-05-13 19:01:41,483 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.hdfs.server.namenode.NNStorage.getStorageFile(NNStorage.java:686)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditsInStorageDir(FSImagePreTransactionalStorageInspector.java:243)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getLatestEditsFiles(FSImagePreTransactionalStorageInspector.java:261)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditLogStreams(FSImagePreTransactionalStorageInspector.java:276)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:596)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:247)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:498)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:390)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:354)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:368)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:402)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:564)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:545)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1093)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1151)"
            ],
            "RootCause": "The NullPointerException is caused by the `getStorageFile` method in the `NNStorage` class, which is called with a null reference for the `StorageDirectory` or an improperly initialized `NameNodeFile` type. This occurs during the recovery process when the layout versions of the storage directories are inconsistent.",
            "StepsToReproduce": [
                "1. Configure three storage directories for the NameNode.",
                "2. Write 10 files to the NameNode.",
                "3. Modify the version file of one storage directory to have a layout version of 123, differing from the default layout version of -40.",
                "4. Stop the NameNode.",
                "5. Start the NameNode again."
            ],
            "ExpectedBehavior": "The NameNode should start up without errors, correctly identifying and using the appropriate storage directories based on their layout versions.",
            "ObservedBehavior": "A NullPointerException is thrown during the startup of the NameNode, indicating that a storage file could not be retrieved due to a null reference.",
            "Suggestions": "Ensure that all storage directories have consistent layout versions before starting the NameNode. Implement checks to validate the layout versions of storage directories during startup.",
            "problem_location": {
                "files": [
                    "NNStorage.java",
                    "FSImagePreTransactionalStorageInspector.java",
                    "FSImage.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.NNStorage",
                    "org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector",
                    "org.apache.hadoop.hdfs.server.namenode.FSImage"
                ],
                "methods": [
                    "NNStorage.getStorageFile",
                    "FSImagePreTransactionalStorageInspector.getEditsInStorageDir",
                    "FSImage.loadFSImage"
                ]
            },
            "possible_fix": "Add validation logic in the `NNStorage` class to check for null references in `StorageDirectory` and `NameNodeFile` before attempting to access storage files. Ensure that layout versions are consistent across all configured storage directories."
        },
        "possible_fix_code": {
            "NNStorage.getStorageFile": "  static File getStorageFile(StorageDirectory sd, NameNodeFile type) {\n    if (sd == null) {\n      throw new IllegalArgumentException(\"StorageDirectory cannot be null\");\n    }\n    if (type == null) {\n      throw new IllegalArgumentException(\"NameNodeFile type cannot be null\");\n    }\n    return new File(sd.getCurrentDir(), type.getName());\n  }"
        }
    },
    {
        "filename": "HDFS-2245.json",
        "creation_time": "2011-08-10T22:55:33.000+0000",
        "bug_report": {
            "Title": "BlockManager.chooseTarget(..) throws NPE",
            "Description": "A NullPointerException (NPE) is thrown in the `chooseTarget` method of the `BlockManager` class when attempting to allocate a new block. The stack trace indicates that the error occurs when the method tries to access the `blockplacement` variable, which is likely uninitialized or null. This method is responsible for selecting appropriate DataNodes for block replication based on various parameters. The issue may stem from the initialization of the `blockplacement` variable or null parameters being passed to the method.",
            "StackTrace": [
                "java.io.IOException: java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1225)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1455)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:786)"
            ],
            "RootCause": "The root cause of the NullPointerException is likely due to the `blockplacement` variable in the `BlockManager` class being uninitialized or null when the `chooseTarget` method is called. This can occur if the `BlockPlacementPolicy.getInstance(...)` method fails to initialize `blockplacement` properly, possibly due to incorrect configuration settings.",
            "StepsToReproduce": [
                "1. Attempt to add a block to the HDFS using the `addBlock` method.",
                "2. Ensure that the configuration for block placement is set up incorrectly or the `blockplacement` variable is not initialized.",
                "3. Observe the NullPointerException thrown in the logs."
            ],
            "ExpectedBehavior": "The `chooseTarget` method should successfully select DataNodes for block replication without throwing a NullPointerException, returning an array of valid DataNode descriptors.",
            "ObservedBehavior": "A NullPointerException is thrown when the `chooseTarget` method is invoked, indicating that an object reference is null.",
            "Suggestions": "Check the initialization of the `blockplacement` variable in the `BlockManager` constructor. Ensure that the configuration key `dfs.block.replicator.classname` is set correctly in the `hdfs-site.xml` file and that the corresponding class can be instantiated without errors. If the configuration is correct, further debugging may be required to trace the initialization process.",
            "problem_location": {
                "files": [
                    "BlockManager.java",
                    "FSNamesystem.java",
                    "NameNode.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager",
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem",
                    "org.apache.hadoop.hdfs.server.namenode.NameNode"
                ],
                "methods": [
                    "BlockManager.chooseTarget",
                    "FSNamesystem.getAdditionalBlock",
                    "NameNode.addBlock"
                ]
            },
            "possible_fix": "Ensure that the `blockplacement` variable is properly initialized in the `BlockManager` constructor. Verify that the `BlockPlacementPolicy.getInstance(...)` method is called with valid parameters and that the configuration settings in `hdfs-site.xml` are correct."
        },
        "possible_fix_code": {
            "BlockManager.chooseTarget": "  public DatanodeDescriptor[] chooseTarget(final String src,\n      final int numOfReplicas, final DatanodeDescriptor client,\n      final HashMap<Node, Node> excludedNodes,\n      final long blocksize) throws IOException {\n    // Ensure blockplacement is initialized before use\n    if (blockplacement == null) {\n      throw new IOException(\"BlockPlacementPolicy is not initialized. Please check the configuration.\");\n    }\n    // choose targets for the new block to be allocated.\n    final DatanodeDescriptor targets[] = blockplacement.chooseTarget(\n        src, numOfReplicas, client, excludedNodes, blocksize);\n    if (targets.length < minReplication) {\n      throw new IOException(\"File \" + src + \" could only be replicated to \" +\n                            targets.length + \" nodes, instead of \" +\n                            minReplication + \". There are \"\n                            + getDatanodeManager().getNetworkTopology().getNumOfLeaves()\n                            + \" datanode(s) running but \" + excludedNodes.size() +\n                            \" node(s) are excluded in this operation.\");\n    }\n    return targets;\n  }"
        }
    },
    {
        "filename": "HDFS-10320.json",
        "creation_time": "2016-04-20T23:02:09.000+0000",
        "bug_report": {
            "Title": "Rack failures may result in NN terminate",
            "Description": "The issue arises when there are rack failures in the Hadoop cluster, leading to a situation where only one rack remains available. This condition triggers the `chooseRandom` method in the `BlockPlacementPolicyDefault` class, which subsequently calls the `chooseRandom` method in the `NetworkTopology` class. When the latter fails to find a suitable datanode within the specified scope, it throws an `InvalidTopologyException`. This exception propagates up to the `ReplicationMonitor` thread in the `BlockManager`, causing the NameNode (NN) to terminate unexpectedly.",
            "StackTrace": [
                "2016-02-24 09:22:01,514  WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true)",
                "2016-02-24 09:22:01,958  ERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor thread received Runtime exception.",
                "org.apache.hadoop.net.NetworkTopology$InvalidTopologyException: Failed to find datanode (scope=\"\" excludedScope=\"/rack_a5\").",
                "at org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:729)",
                "at org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:694)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:635)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:580)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:348)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:214)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:111)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.chooseTargets(BlockManager.java:3746)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.access$200(BlockManager.java:3711)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1400)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1306)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3682)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3634)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is a misconfiguration in the Hadoop network topology or a lack of available datanodes in the specified rack, which prevents the `chooseRandom` method from finding a suitable datanode, leading to an `InvalidTopologyException`.",
            "StepsToReproduce": [
                "Simulate rack failures in the Hadoop cluster to leave only one rack available.",
                "Attempt to place a block that requires replication across multiple racks.",
                "Monitor the logs for the `InvalidTopologyException` and subsequent termination of the NameNode."
            ],
            "ExpectedBehavior": "The system should be able to handle rack failures gracefully, ensuring that the NameNode remains operational and can still place blocks across available datanodes.",
            "ObservedBehavior": "The NameNode terminates unexpectedly due to an unhandled `InvalidTopologyException` when it fails to find a datanode in the specified scope.",
            "Suggestions": "Review and correct the network topology configuration to ensure that datanodes are properly assigned to racks. Additionally, ensure that there are sufficient datanodes available in the cluster to meet replication requirements.",
            "problem_location": {
                "files": [
                    "NetworkTopology.java",
                    "BlockPlacementPolicyDefault.java"
                ],
                "classes": [
                    "org.apache.hadoop.net.NetworkTopology",
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault"
                ],
                "methods": [
                    "NetworkTopology.chooseRandom",
                    "BlockPlacementPolicyDefault.chooseRandom"
                ]
            },
            "possible_fix": "Check the configuration files for the Hadoop cluster to ensure that datanodes are correctly assigned to racks. If necessary, modify the `chooseRandom` method in `NetworkTopology` to handle cases where no datanodes are available more gracefully, potentially by implementing fallback logic."
        },
        "possible_fix_code": {
            "NetworkTopology.chooseRandom": "  private Node chooseRandom(String scope, String excludedScope){\n    if (excludedScope != null) {\n      if (scope.startsWith(excludedScope)) {\n        return null;\n      }\n      if (!excludedScope.startsWith(scope)) {\n        excludedScope = null;\n      }\n    }\n    Node node = getNode(scope);\n    if (!(node instanceof InnerNode)) {\n      return node;\n    }\n    InnerNode innerNode = (InnerNode)node;\n    int numOfDatanodes = innerNode.getNumOfLeaves();\n    if (excludedScope == null) {\n      node = null;\n    } else {\n      node = getNode(excludedScope);\n      if (!(node instanceof InnerNode)) {\n        numOfDatanodes -= 1;\n      } else {\n        numOfDatanodes -= ((InnerNode)node).getNumOfLeaves();\n      }\n    }\n    if (numOfDatanodes == 0) {\n      // Handle the case where no datanodes are available more gracefully\n      return null; // or throw a custom exception if needed\n    }\n    int leaveIndex = r.nextInt(numOfDatanodes);\n    return innerNode.getLeaf(leaveIndex, node);\n  }"
        }
    },
    {
        "filename": "HDFS-4201.json",
        "creation_time": "2012-11-16T21:28:57.000+0000",
        "bug_report": {
            "Title": "NPE in BPServiceActor#sendHeartBeat",
            "Description": "A NullPointerException (NPE) was observed in the logs during the execution of the `sendHeartBeat` method in the `BPServiceActor` class. The stack trace indicates that the exception is thrown when attempting to access properties of potentially uninitialized objects. The likely culprits are the `bpRegistration`, `dn`, or `bpNamenode` objects, which may not have been properly initialized due to a failure in the registration process or other configuration issues.",
            "StackTrace": [
                "2012-09-25 04:33:20,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode svsrs00127/11.164.162.226:8020 using DELETEREPORT_INTERVAL of 300000 msec BLOCKREPORT_INTERVAL of 21600000msec Initial delay: 0msec; heartBeatInterval=3000",
                "2012-09-25 04:33:20,782 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in BPOfferService for Block pool BP-1678908700-11.164.162.226-1342785481826 (storage id DS-1031100678-11.164.162.251-5010-1341933415989) service to svsrs00127/11.164.162.226:8020",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:434)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:520)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:673)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "RootCause": "The NPE occurs in the `sendHeartBeat` method due to uninitialized objects: `bpRegistration`, `dn`, or `bpNamenode`. Specifically, if the `register` method is not successfully executed before `offerService` is called, it can lead to these objects being null when accessed.",
            "StepsToReproduce": [
                "1. Start the DataNode service.",
                "2. Ensure that the connection to the NameNode is established.",
                "3. Simulate a failure in the registration process (e.g., by causing the NameNode to be unreachable).",
                "4. Observe the logs for the NPE when the `sendHeartBeat` method is invoked."
            ],
            "ExpectedBehavior": "The `sendHeartBeat` method should successfully send a heartbeat to the NameNode without throwing any exceptions, provided that all necessary objects (`bpRegistration`, `dn`, and `bpNamenode`) are properly initialized.",
            "ObservedBehavior": "A NullPointerException is thrown in the `sendHeartBeat` method, indicating that one or more of the required objects are null.",
            "Suggestions": "Ensure that the `register` method is called successfully before entering the `offerService` loop. Implement null checks for `bpRegistration`, `dn`, and `bpNamenode` in the `sendHeartBeat` method to prevent NPEs.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.BPServiceActor"
                ],
                "methods": [
                    "BPServiceActor.sendHeartBeat",
                    "BPServiceActor.offerService",
                    "BPServiceActor.register"
                ]
            },
            "possible_fix": "Add null checks in the `sendHeartBeat` method to handle cases where `bpRegistration`, `dn`, or `bpNamenode` may be null. For example:\n\n```java\nif (bpRegistration == null || dn == null || bpNamenode == null) {\n    throw new IllegalStateException(\"Required objects are not initialized.\");\n}\n```"
        },
        "possible_fix_code": {
            "sendHeartBeat": "  HeartbeatResponse sendHeartBeat() throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat from service actor: \" + this);\n    }\n    // Check for null objects to prevent NPE\n    if (bpRegistration == null || dn == null || bpNamenode == null) {\n        throw new IllegalStateException(\"Required objects are not initialized.\");\n    }\n    // reports number of failed volumes\n    StorageReport[] report = { new StorageReport(bpRegistration.getStorageID(),\n        false,\n        dn.getFSDataset().getCapacity(),\n        dn.getFSDataset().getDfsUsed(),\n        dn.getFSDataset().getRemaining(),\n        dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId())) };\n    return bpNamenode.sendHeartbeat(bpRegistration, report,\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        dn.getFSDataset().getNumFailedVolumes());\n  }"
        }
    },
    {
        "filename": "HDFS-6904.json",
        "creation_time": "2014-08-21T10:17:21.000+0000",
        "bug_report": {
            "Title": "YARN unable to renew delegation token fetched via webhdfs due to incorrect service port",
            "Description": "YARN is unable to renew delegation tokens obtained via the WebHDFS REST API. The issue arises when a user creates a delegation token using the WebHDFS REST API and subsequently passes this token to YARN during application submission via the YARN REST API. When YARN attempts to renew the delegation token, it fails because the token service is incorrectly pointing to the RPC port instead of the WebHDFS port. The stack trace indicates an IOException due to an unexpected HTTP response with a code of -1, suggesting that the request to renew the token did not succeed, possibly due to network issues or incorrect URL formation.",
            "StackTrace": [
                "2014-08-19 03:12:54,733 WARN  security.DelegationTokenRenewer (DelegationTokenRenewer.java:handleDTRenewerAppSubmitEvent(661)) - Unable to add the application to the delegation token renewer.",
                "java.io.IOException: Failed to renew token: Kind: WEBHDFS delegation, Service: NameNodeIP:8020, Ident: (WEBHDFS delegation token 2222 for hrt_qa)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:394)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.renewToken(DelegationTokenRenewer.java:473)",
                "Caused by: java.io.IOException: Unexpected HTTP response: code=-1 != 200, op=RENEWDELEGATIONTOKEN, message=null"
            ],
            "RootCause": "The root cause of the issue is that the Namenode generates a delegation token of kind WebHDFS but does not change the service port to the correct WebHDFS port. As a result, when YARN tries to renew the delegation token, it attempts to contact WebHDFS on the RPC port, leading to an unexpected HTTP response.",
            "StepsToReproduce": [
                "1. Create a delegation token using the WebHDFS REST API.",
                "2. Pass the created token to YARN as part of the application submission via the YARN REST API.",
                "3. Observe the failure when YARN attempts to renew the delegation token."
            ],
            "ExpectedBehavior": "YARN should successfully renew the delegation token using the correct WebHDFS service port, allowing continued access to the resources secured by the token.",
            "ObservedBehavior": "YARN fails to renew the delegation token, resulting in an IOException with an unexpected HTTP response code of -1.",
            "Suggestions": "Verify that the Namenode is configured to generate delegation tokens with the correct service port for WebHDFS. Ensure that the WebHDFS service is reachable and that the correct URL is being used for token renewal.",
            "problem_location": {
                "files": [
                    "DelegationTokenRenewer.java",
                    "WebHdfsFileSystem.java",
                    "NamenodeWebHdfsMethods.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer",
                    "org.apache.hadoop.hdfs.web.WebHdfsFileSystem"
                ],
                "methods": [
                    "DelegationTokenRenewer.handleAppSubmitEvent",
                    "WebHdfsFileSystem.renewDelegationToken",
                    "WebHdfsFileSystem.validateResponse"
                ]
            },
            "possible_fix": "Ensure that the service port for the WebHDFS delegation token is set correctly in the Namenode configuration. Modify the token generation logic in the NamenodeWebHdfsMethods to ensure the correct port is used when generating the token."
        },
        "possible_fix_code": {
            "WebHdfsFileSystem.renewDelegationToken": "  public synchronized long renewDelegationToken(final Token<?> token) throws IOException {\n    final HttpOpParam.Op op = HttpOpParam.Op.RENEWDELEGATIONTOKEN;\n    // Ensure the correct WebHDFS port is used for the request\n    URL url = new URL(getWebHdfsUrl(token)); // Assuming getWebHdfsUrl constructs the correct URL with the WebHDFS port\n    return new FsPathResponseRunner<Long>(op, null,\n        new TokenArgumentParam(token.encodeToUrlString())) {\n      @Override\n      Long decodeResponse(Map<?,?> json) throws IOException {\n        return (Long) json.get(\"long\");\n      }\n    }.run();\n  }\n\n  private String getWebHdfsUrl(Token<?> token) {\n    // Logic to construct the correct WebHDFS URL using the token's service information\n    // This should ensure that the URL points to the WebHDFS service port instead of the RPC port\n    return \"http://\" + token.getService() + \":<WebHDFS_PORT>/webhdfs/v1/\"; // Replace <WebHDFS_PORT> with the actual port\n  }"
        }
    },
    {
        "filename": "HDFS-13721.json",
        "creation_time": "2018-07-05T20:11:57.000+0000",
        "bug_report": {
            "Title": "NPE in DataNode due to uninitialized DiskBalancer",
            "Description": "A NullPointerException (NPE) is thrown in the DataNode class when attempting to retrieve the disk balancer status via the `getDiskBalancerStatus` method. This occurs during JMX operations, specifically when the `diskBalancer` instance variable is accessed without being properly initialized. The issue arises during the startup of the DataNode, where the `initDiskBalancer` method is not called, leading to the `diskBalancer` remaining null. This results in the NPE when `getDiskBalancerStatus` tries to invoke `queryWorkStatus()` on a null reference.",
            "StackTrace": [
                "2018-06-28 05:11:47,650 ERROR org.apache.hadoop.jmx.JMXJsonServlet: getting attribute DiskBalancerStatus of Hadoop:service=DataNode,name=DataNodeInfo threw an exception",
                "javax.management.RuntimeMBeanException: java.lang.NullPointerException",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:839)",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:651)",
                "at org.apache.hadoop.jmx.JMXJsonServlet.writeAttribute(JMXJsonServlet.java:338)",
                "at org.apache.hadoop.jmx.JMXJsonServlet.doGet(JMXJsonServlet.java:210)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1537)",
                "at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)",
                "at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)",
                "at org.eclipse.jetty.server.Server.handle(Server.java:534)",
                "at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)",
                "at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)",
                "at java.lang.Thread.run(Thread.java:748)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.getDiskBalancerStatus(DataNode.java:3146)"
            ],
            "RootCause": "The root cause of the NPE is that the `diskBalancer` instance variable in the DataNode class is not initialized before it is accessed in the `getDiskBalancerStatus` method. This happens because the `initDiskBalancer` method, which is responsible for initializing `diskBalancer`, is not called during the DataNode's startup process.",
            "StepsToReproduce": [
                "Start the DataNode service.",
                "Attempt to access the DiskBalancer status via JMX.",
                "Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The DataNode should successfully return the disk balancer status without throwing an exception, indicating that the disk balancer is properly initialized.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to access the disk balancer status, indicating that the `diskBalancer` is null.",
            "Suggestions": "Ensure that the `initDiskBalancer` method is called during the DataNode's startup process, specifically in the `startDataNode` method, to initialize the `diskBalancer` before it is accessed.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.DataNode"
                ],
                "methods": [
                    "DataNode.getDiskBalancerStatus",
                    "DataNode.initDiskBalancer",
                    "DataNode.startDataNode"
                ]
            },
            "possible_fix": "Add a call to `initDiskBalancer` in the `startDataNode` method to ensure that the `diskBalancer` is initialized before any attempts to access it in `getDiskBalancerStatus`."
        },
        "possible_fix_code": {
            "DataNode.startDataNode": "  void startDataNode(List<StorageLocation> dataDirectories,\n                     SecureResources resources\n                     ) throws IOException {\n\n    // settings global for all BPs in the Data Node\n    this.secureResources = resources;\n    synchronized (this) {\n      this.dataDirs = dataDirectories;\n    }\n    this.dnConf = new DNConf(this);\n    checkSecureConfig(dnConf, getConf(), resources);\n\n    if (dnConf.maxLockedMemory > 0) {\n      if (!NativeIO.POSIX.getCacheManipulator().verifyCanMlock()) {\n        throw new RuntimeException(String.format(\n            \"Cannot start datanode because the configured max locked memory\" +\n            \" size (%s) is greater than zero and native code is not available.\",\n            DFS_DATANODE_MAX_LOCKED_MEMORY_KEY));\n      }\n      if (Path.WINDOWS) {\n        NativeIO.Windows.extendWorkingSetSize(dnConf.maxLockedMemory);\n      } else {\n        long ulimit = NativeIO.POSIX.getCacheManipulator().getMemlockLimit();\n        if (dnConf.maxLockedMemory > ulimit) {\n          throw new RuntimeException(String.format(\n            \"Cannot start datanode because the configured max locked memory\" +\n            \" size (%s) of %d bytes is more than the datanode's available\" +\n            \" RLIMIT_MEMLOCK ulimit of %d bytes.\",\n            DFS_DATANODE_MAX_LOCKED_MEMORY_KEY,\n            dnConf.maxLockedMemory,\n            ulimit));\n        }\n      }\n    }\n    LOG.info(\"Starting DataNode with maxLockedMemory = {}\",\n        dnConf.maxLockedMemory);\n\n    int volFailuresTolerated = dnConf.getVolFailuresTolerated();\n    int volsConfigured = dnConf.getVolsConfigured();\n    if (volFailuresTolerated < 0 || volFailuresTolerated >= volsConfigured) {\n      throw new DiskErrorException(\"Invalid value configured for \"\n          + \"dfs.datanode.failed.volumes.tolerated - \" + volFailuresTolerated\n          + \". Value configured is either less than 0 or >= \"\n          + \"to the number of configured volumes (\" + volsConfigured + \").\");\n    }\n\n    storage = new DataStorage();\n    \n    // global DN settings\n    registerMXBean();\n    initDataXceiver();\n    startInfoServer();\n    pauseMonitor = new JvmPauseMonitor();\n    pauseMonitor.init(getConf());\n    pauseMonitor.start();\n\n    // Initialize the disk balancer\n    initDiskBalancer(data, getConf());\n\n    // BlockPoolTokenSecretManager is required to create ipc server.\n    this.blockPoolTokenSecretManager = new BlockPoolTokenSecretManager();\n\n    // Login is done by now. Set the DN user name.\n    dnUserName = UserGroupInformation.getCurrentUser().getUserName();\n    LOG.info(\"dnUserName = {}\", dnUserName);\n    LOG.info(\"supergroup = {}\", supergroup);\n    initIpcServer();\n\n    metrics = DataNodeMetrics.create(getConf(), getDisplayName());\n    peerMetrics = dnConf.peerStatsEnabled ?\n        DataNodePeerMetrics.create(getDisplayName()) : null;\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n\n    ecWorker = new ErasureCodingWorker(getConf(), this);\n    blockRecoveryWorker = new BlockRecoveryWorker(this);\n\n    blockPoolManager = new BlockPoolManager(this);\n    blockPoolManager.refreshNamenodes(getConf());\n\n    // Create the ReadaheadPool from the DataNode context so we can\n    // exit without having to explicitly shutdown its thread pool.\n    readaheadPool = ReadaheadPool.getInstance();\n    saslClient = new SaslDataTransferClient(dnConf.getConf(),\n        dnConf.saslPropsResolver, dnConf.trustedChannelResolver);\n    saslServer = new SaslDataTransferServer(dnConf, blockPoolTokenSecretManager);\n    startMetricsLogger();\n\n    if (dnConf.diskStatsEnabled) {\n      diskMetrics = new DataNodeDiskMetrics(this,\n          dnConf.outliersReportIntervalMs);\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-7180.json",
        "creation_time": "2014-10-02T03:07:04.000+0000",
        "bug_report": {
            "Title": "NFSv3 gateway frequently gets stuck due to GC",
            "Description": "The NFSv3 gateway, running on a single node in a Hadoop 2.5.0 (HDFS only) cluster, frequently becomes unresponsive after prolonged operation, particularly following significant data uploads via rsync. While HDFS operations (e.g., 'hdfs dfs -ls') continue to function normally, the NFSv3 daemon experiences repeated failures, as indicated by persistent 'not responding' messages in the system logs. The issue manifests after approximately one day of operation, with several hundred gigabytes of data uploaded. The logs also reveal a potential problem with a DataNode (10.0.3.176) that is reported as 'bad' despite the HDFS cluster reporting all nodes as operational.",
            "StackTrace": [
                "[1859245.368108] nfs: server localhost not responding, still trying",
                "[1859245.368111] nfs: server localhost not responding, still trying",
                "[1859245.368115] nfs: server localhost not responding, still trying",
                "[1859245.368119] nfs: server localhost not responding, still trying",
                "[1859245.368123] nfs: server localhost not responding, still trying",
                "[1859245.368127] nfs: server localhost not responding, still trying",
                "[1859245.368131] nfs: server localhost not responding, still trying",
                "[1859245.368135] nfs: server localhost not responding, still trying",
                "[1859245.368138] nfs: server localhost not responding, still trying",
                "[1859245.368142] nfs: server localhost not responding, still trying",
                "[1859245.368146] nfs: server localhost not responding, still trying",
                "[1859245.368150] nfs: server localhost not responding, still trying",
                "[1859245.368153] nfs: server localhost not responding, still trying",
                "java.io.IOException: Bad response ERROR for block BP-1960069741-10.0.3.170-1410430543652:blk_1074363564_623643 from datanode 10.0.3.176:50010"
            ],
            "RootCause": "The NFSv3 daemon becomes unresponsive due to repeated 'not responding' errors from the NFS server, likely caused by a failure in the DataNode (10.0.3.176) that is unable to provide valid responses for block requests. This is compounded by potential garbage collection (GC) issues that may be affecting the performance of the NFS service.",
            "StepsToReproduce": [
                "1. Set up a Hadoop 2.5.0 cluster with HDFS only.",
                "2. Start and mount the NFSv3 gateway on a single node.",
                "3. Upload several hundred gigabytes of data using rsync.",
                "4. Monitor the NFSv3 daemon and system logs for 'not responding' messages."
            ],
            "ExpectedBehavior": "The NFSv3 gateway should remain responsive and allow users to upload data without interruption, with the ability to list and access mounted directories.",
            "ObservedBehavior": "The NFSv3 daemon frequently becomes unresponsive, displaying repeated 'not responding' messages in the logs, and operations such as 'ls' and 'df -hT' on the mounted directory hang indefinitely.",
            "Suggestions": "Investigate the health and configuration of the DataNode (10.0.3.176) to ensure it is functioning correctly. Consider increasing the timeout settings for NFS operations or adjusting garbage collection parameters to improve responsiveness.",
            "problem_location": {
                "files": [
                    "nfs3.log",
                    "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.DFSOutputStream",
                    "org.apache.hadoop.hdfs.DFSClient"
                ],
                "methods": [
                    "DFSOutputStream.handleIOException",
                    "DFSClient"
                ]
            },
            "possible_fix": "Review and potentially modify the configuration of the DataNode (10.0.3.176) to address the 'bad response' issue. Additionally, consider implementing error handling in the NFSv3 gateway to manage unresponsive states more gracefully."
        },
        "possible_fix_code": {
            "DFSOutputStream.handleIOException": "public void handleIOException(IOException e) throws IOException {\n    // Log the IOException for debugging purposes\n    LOG.error(\"IOException occurred: \" + e.getMessage(), e);\n\n    // Check if the exception is due to a bad response from a DataNode\n    if (e.getMessage().contains(\"Bad response ERROR\")) {\n        // Implement a retry mechanism or handle the error gracefully\n        // For example, we could log a warning and attempt to reconnect to the DataNode\n        LOG.warn(\"Bad response from DataNode, attempting to reconnect...\");\n        reconnectToDataNode();\n    } else {\n        // For other IOExceptions, we can rethrow the exception to be handled by the caller\n        throw e;\n    }\n}\n\nprivate void reconnectToDataNode() {\n    // Logic to reconnect to the DataNode\n    // This could involve resetting the connection or refreshing the DataNode list\n    // For example:\n    this.datanodeInfo = fetchDatanodeInfo();\n    LOG.info(\"Reconnected to DataNode: \" + this.datanodeInfo);\n}"
        }
    },
    {
        "filename": "HDFS-6102.json",
        "creation_time": "2014-03-13T18:27:36.000+0000",
        "bug_report": {
            "Title": "Lower the default maximum items per directory to fix PB fsimage loading",
            "Description": "During testing, an issue was identified where creating a large number of directories within a single directory caused the fsimage size to exceed the maximum limit for Protocol Buffer messages. This resulted in an error when attempting to load the fsimage, specifically an `InvalidProtocolBufferException` indicating that the protocol message was too large. The error suggests that the maximum size limit for Protocol Buffer messages is 64MB, which is being exceeded in this scenario.",
            "StackTrace": [
                "2014-03-13 13:57:03,901 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 24523605 INodes.",
                "2014-03-13 13:57:59,038 ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: Failed to load image from FSImageFile(file=/dfs/nn/current/fsimage_0000000000024532742, cpktTxId=0000000000024532742)",
                "com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large. May be malicious. Use CodedInputStream.setSizeLimit() to increase the size limit.",
                "at com.google.protobuf.InvalidProtocolBufferException.sizeLimitExceeded(InvalidProtocolBufferException.java:110)",
                "at com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:755)",
                "at com.google.protobuf.CodedInputStream.readRawByte(CodedInputStream.java:769)",
                "at com.google.protobuf.CodedInputStream.readRawVarint64(CodedInputStream.java:462)",
                "at com.google.protobuf.CodedInputStream.readUInt64(CodedInputStream.java:188)",
                "at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.<init>(FsImageProto.java:9839)",
                "at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.<init>(FsImageProto.java:9770)",
                "at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$1.parsePartialFrom(FsImageProto.java:9901)",
                "at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$1.parsePartialFrom(FsImageProto.java:9896)"
            ],
            "RootCause": "The root cause of the issue is that the fsimage size exceeds the maximum allowed size for Protocol Buffer messages (64MB), leading to an `InvalidProtocolBufferException`. This occurs when attempting to load a large number of INodes into memory.",
            "StepsToReproduce": [
                "Create a large number of directories within a single parent directory.",
                "Attempt to load the fsimage that contains these directories."
            ],
            "ExpectedBehavior": "The system should successfully load the fsimage without exceeding the Protocol Buffer message size limit.",
            "ObservedBehavior": "The system fails to load the fsimage, throwing an `InvalidProtocolBufferException` due to the message size exceeding the limit.",
            "Suggestions": "To resolve this issue, consider the following steps: 1. Increase the size limit for Protocol Buffer messages using `CodedInputStream.setSizeLimit()`, if available. 2. Review the application configuration for any settings related to message size limits. 3. Consider breaking down large messages into smaller ones to avoid exceeding the limit. 4. Ensure that the data being processed is from a trusted source to mitigate security concerns.",
            "problem_location": {
                "files": [
                    "FSImage.java",
                    "CodedInputStream.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSImage",
                    "com.google.protobuf.CodedInputStream"
                ],
                "methods": [
                    "FSImage.load",
                    "CodedInputStream.setSizeLimit"
                ]
            },
            "possible_fix": "If the method `CodedInputStream.setSizeLimit()` is found in the documentation or codebase, implement it to increase the size limit. If not, consult the Protocol Buffers documentation for alternative methods or configuration settings to handle larger messages."
        },
        "possible_fix_code": {
            "FSImage.load": "public void load() throws IOException {\n    // Set a higher size limit for Protocol Buffer messages to avoid InvalidProtocolBufferException\n    CodedInputStream codedInputStream = CodedInputStream.newInstance(...);\n    codedInputStream.setSizeLimit(100 * 1024 * 1024); // Set limit to 100MB, adjust as necessary\n\n    // Existing loading logic\n    // ...\n}"
        }
    },
    {
        "filename": "HDFS-11377.json",
        "creation_time": "2017-01-26T23:40:53.000+0000",
        "bug_report": {
            "Title": "Balancer hung due to no available mover threads",
            "Description": "The HDFS balancer is experiencing a hang condition when operating on a large cluster with over 3000 Datanodes. The issue arises when there are no available mover threads, leading to the balancer waiting indefinitely. The stack trace indicates that the main thread is in a TIMED_WAITING state, specifically in the `waitForMoveCompletion` method of the `Dispatcher` class, which is designed to wait for all pending block moves to complete. The logs show multiple warnings about the unavailability of mover threads, indicating that the balancer is unable to proceed with block movements.",
            "StackTrace": [
                "\"main\" #1 prio=5 os_prio=0 tid=0x00007ff6cc014800 nid=0x6b2c waiting on condition [0x00007ff6d1bad000]",
                "java.lang.Thread.State: TIMED_WAITING (sleeping)",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher.waitForMoveCompletion(Dispatcher.java:1043)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchBlockMoves(Dispatcher.java:1017)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchAndCheckContinue(Dispatcher.java:981)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.runOneIteration(Balancer.java:611)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:663)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer$Cli.run(Balancer.java:776)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:905)"
            ],
            "RootCause": "The root cause of the issue is that the `DDatanode.isPendingQEmpty()` method returns false when there are no mover threads available, causing the balancer to hang indefinitely while waiting for block moves to complete.",
            "StepsToReproduce": [
                "1. Set up a large HDFS cluster with more than 3000 Datanodes.",
                "2. Initiate the balancer process.",
                "3. Monitor the logs for warnings about 'No mover threads available'.",
                "4. Observe the stack trace indicating the balancer is in a TIMED_WAITING state."
            ],
            "ExpectedBehavior": "The balancer should efficiently manage block movements without hanging, even when there are no mover threads available, and should complete its operation in a timely manner.",
            "ObservedBehavior": "The balancer hangs indefinitely in a TIMED_WAITING state, unable to proceed with block movements due to the lack of available mover threads.",
            "Suggestions": "Consider increasing the number of mover threads available to the balancer or implementing a timeout mechanism to prevent indefinite waiting. Additionally, review the logic in the `waitForMoveCompletion` method to ensure it can handle scenarios where no mover threads are available.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.balancer.Dispatcher",
                    "org.apache.hadoop.hdfs.server.balancer.Balancer"
                ],
                "methods": [
                    "Dispatcher.waitForMoveCompletion",
                    "Dispatcher.dispatchBlockMoves",
                    "Balancer.runOneIteration"
                ]
            },
            "possible_fix": "To address the issue, consider modifying the `waitForMoveCompletion` method to include a timeout for waiting on pending block moves. Additionally, ensure that the balancer can gracefully handle scenarios where no mover threads are available, possibly by implementing a retry mechanism or logging more detailed information about the state of the mover threads."
        },
        "possible_fix_code": {
            "Dispatcher.waitForMoveCompletion": "  public static boolean waitForMoveCompletion(\n      Iterable<? extends StorageGroup> targets) {\n    boolean hasFailure = false;\n    long timeout = 60000; // 60 seconds timeout\n    long startTime = System.currentTimeMillis();\n    for(;;) {\n      boolean empty = true;\n      for (StorageGroup t : targets) {\n        if (!t.getDDatanode().isPendingQEmpty()) {\n          empty = false;\n          break;\n        } else {\n          hasFailure |= t.getDDatanode().hasFailure;\n        }\n      }\n      if (empty) {\n        return hasFailure; // all pending queues are empty\n      }\n      if (System.currentTimeMillis() - startTime > timeout) {\n        System.err.println(\"Timeout waiting for move completion\");\n        return hasFailure; // return failure status after timeout\n      }\n      try {\n        Thread.sleep(1000);\n      } catch (InterruptedException ignored) {\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-6753.json",
        "creation_time": "2014-07-25T11:36:43.000+0000",
        "bug_report": {
            "Title": "Initialize checkDisk when DirectoryScanner not able to get files list for scanning",
            "Description": "The bug report outlines an issue in a Hadoop cluster where the DataNode fails to shut down despite all configured volumes being marked as failed due to permission issues and disk space limitations. The cluster consists of three DataNodes, and the error arises when the DataNode attempts to perform write operations on a full disk, leading to a DiskOutOfSpaceException. The DirectoryScanner also fails to compile a report due to invalid directory access, which further complicates the situation.",
            "StackTrace": [
                "2014-07-21 14:10:52,814 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: XX1.XX1.XX1.XX1:50010:DataXceiver error processing WRITE_BLOCK operation src: /XX2.XX2.XX2.XX2:10106 dst: /XX1.XX1.XX1.XX1:50010",
                "org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=4096 B) is less than the block size (=134217728 B).",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:60)",
                "2014-07-21 14:13:00,180 WARN org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Exception occurred while compiling report:",
                "java.io.IOException: Invalid directory or I/O error occurred for dir: /mnt/tmp_Datanode/current/BP-1384489961-XX2.XX2.XX2.XX2-845784615183/current/finalized",
                "at org.apache.hadoop.fs.FileUtil.listFiles(FileUtil.java:1164)",
                "at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.compileReport(DirectoryScanner.java:596)"
            ],
            "RootCause": "The root cause of the issue is insufficient disk space on the DataNode, leading to a DiskOutOfSpaceException. The available space is less than the required block size, which prevents the DataNode from performing write operations and causes it to fail to shut down as expected.",
            "StepsToReproduce": [
                "Step 1: Change the permissions of /mnt/tmp_Datanode to root.",
                "Step 2: Fill the /mnt/tmp_Datanode disk to capacity.",
                "Step 3: Attempt to perform client write operations."
            ],
            "ExpectedBehavior": "The DataNode should shut down when all configured volumes are marked as failed due to insufficient disk space or permission issues.",
            "ObservedBehavior": "The DataNode does not shut down even when all configured volumes are marked as failed, and a DiskOutOfSpaceException is thrown during write operations.",
            "Suggestions": "Check the disk space on the relevant volumes and ensure that there is sufficient space (at least equal to the block size of 134217728 bytes) for Hadoop to function properly. Additionally, review the permission settings for the directories used by the DataNode.",
            "problem_location": {
                "files": [
                    "/mnt/tmp_Datanode",
                    "/home/REX/data/dfs1/data",
                    "/home/REX/data/dfs2/data",
                    "/opt/REX/dfs/data"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.DataNode",
                    "org.apache.hadoop.hdfs.server.datanode.DirectoryScanner",
                    "org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy"
                ],
                "methods": [
                    "DataNode.writeBlock",
                    "DirectoryScanner.compileReport",
                    "RoundRobinVolumeChoosingPolicy.chooseVolume"
                ]
            },
            "possible_fix": "Ensure that the DataNode has sufficient disk space available and correct any permission issues on the directories. Consider implementing a check in the DataNode's write operation method to handle cases where all volumes are marked as failed."
        },
        "possible_fix_code": {
            "DataNode.writeBlock": "public void writeBlock(...) throws IOException {\n    // Check available disk space before proceeding with write operations\n    if (!hasSufficientDiskSpace()) {\n        throw new DiskOutOfSpaceException(\"Insufficient disk space available for write operations.\");\n    }\n    // Existing logic for writing a block\n}\n\nprivate boolean hasSufficientDiskSpace() {\n    // Logic to check if there is sufficient disk space available\n    long requiredSpace = 134217728; // Block size\n    for (V volume : volumes) {\n        if (volume.getAvailable() >= requiredSpace) {\n            return true;\n        }\n    }\n    return false;\n}"
        }
    },
    {
        "filename": "HDFS-3443.json",
        "creation_time": "2012-05-18T11:30:50.000+0000",
        "bug_report": {
            "Title": "Fix NPE when namenode transition to active during startup by adding checkNNStartup() in NameNodeRpcServer",
            "Description": "During the startup of the NameNode (NN), when transitioning from standby to active state, a NullPointerException (NPE) occurs in the `startActiveServices` method of the `FSNamesystem` class. This happens if certain objects, such as `dir`, `blockManager`, or `leaseManager`, are not properly initialized before they are accessed. The issue arises specifically when the `editLogTailer.catchupDuringFailover()` method is called, which expects these objects to be in a valid state. To prevent this, a check for the initialization of these components should be added before they are used.",
            "StackTrace": [
                "2012-05-18 16:51:27,586 WARN org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call org.apache.hadoop.ha.HAServiceProtocol.transitionToActive from XX.XX.XX.55:58004: error: java.lang.NullPointerException",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:602)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1287)",
                "\tat org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)",
                "\tat org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:63)",
                "\tat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1219)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:978)"
            ],
            "RootCause": "The root cause of the NullPointerException is the access of uninitialized objects in the `startActiveServices` method of the `FSNamesystem` class. Specifically, the `dir`, `blockManager`, or `leaseManager` objects may not be properly initialized before they are accessed, leading to the NPE when `editLogTailer.catchupDuringFailover()` is called.",
            "StepsToReproduce": [
                "Start the NameNode in standby mode.",
                "Trigger the transition to active state.",
                "Ensure that the `editLogTailer` is invoked during the transition."
            ],
            "ExpectedBehavior": "The NameNode should transition to the active state without throwing any exceptions, and all necessary services should be initialized correctly.",
            "ObservedBehavior": "A NullPointerException is thrown during the transition to active state, specifically when attempting to catch up with the edit log.",
            "Suggestions": "Add a check to ensure that all necessary components (`dir`, `blockManager`, `leaseManager`) are initialized before they are accessed in the `startActiveServices` method. Consider implementing a method like `checkNNStartup()` to validate the state of these components.",
            "problem_location": {
                "files": [
                    "FSNamesystem.java",
                    "NameNodeRpcServer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem",
                    "org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer"
                ],
                "methods": [
                    "FSNamesystem.startActiveServices",
                    "NameNodeRpcServer.transitionToActive"
                ]
            },
            "possible_fix": "In the `startActiveServices` method, add checks for the initialization of `dir`, `blockManager`, and `leaseManager` before they are accessed. For example:\n\nif (dir == null || blockManager == null || leaseManager == null) {\n    throw new IllegalStateException(\"NameNode components are not initialized.\");\n}"
        },
        "possible_fix_code": {
            "FSNamesystem.startActiveServices": "  void startActiveServices() throws IOException {\n    LOG.info(\"Starting services required for active state\");\n    writeLock();\n    try {\n      // Check for initialization of critical components\n      if (dir == null || blockManager == null || leaseManager == null) {\n        throw new IllegalStateException(\"NameNode components are not initialized.\");\n      }\n\n      FSEditLog editLog = dir.fsImage.getEditLog();\n      \n      if (!editLog.isOpenForWrite()) {\n        // During startup, we're already open for write during initialization.\n        editLog.initJournalsForWrite();\n        // May need to recover\n        editLog.recoverUnclosedStreams();\n        \n        LOG.info(\"Catching up to latest edits from old active before \" +\n            \"taking over writer role in edits logs.\");\n        editLogTailer.catchupDuringFailover();\n        \n        LOG.info(\"Reprocessing replication and invalidation queues...\");\n        blockManager.getDatanodeManager().markAllDatanodesStale();\n        blockManager.clearQueues();\n        blockManager.processAllPendingDNMessages();\n        blockManager.processMisReplicatedBlocks();\n        \n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"NameNode metadata after re-processing \" +\n              \"replication and invalidation queues during failover:\\n\" +\n              metaSaveAsString());\n        }\n        \n        long nextTxId = dir.fsImage.getLastAppliedTxId() + 1;\n        LOG.info(\"Will take over writing edit logs at txnid \" + \n            nextTxId);\n        editLog.setNextTxId(nextTxId);\n\n        dir.fsImage.editLog.openForWrite();\n      }\n      if (haEnabled) {\n        // Renew all of the leases before becoming active.\n        // This is because, while we were in standby mode,\n        // the leases weren't getting renewed on this NN.\n        // Give them all a fresh start here.\n        leaseManager.renewAllLeases();\n      }\n      leaseManager.startMonitor();\n      startSecretManagerIfNecessary();\n    } finally {\n      writeUnlock();\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-11479.json",
        "creation_time": "2017-03-01T11:08:07.000+0000",
        "bug_report": {
            "Title": "Socket re-use address option should be used in SimpleUdpServer",
            "Description": "The NFS gateway restart can fail due to a bind error in the SimpleUdpServer. The error occurs when attempting to bind a socket to the address 0.0.0.0:4242, which is already in use, leading to a ChannelException. To prevent this issue, the re-use address option should be enabled in the SimpleUdpServer to allow binding even when the socket is in the TIME_WAIT state.",
            "StackTrace": [
                "2017-02-28 04:19:53,495 FATAL mount.MountdBase (MountdBase.java:startUDPServer(66)) - Failed to start the UDP server.",
                "org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:4242",
                "at org.jboss.netty.bootstrap.ConnectionlessBootstrap.bind(ConnectionlessBootstrap.java:204)",
                "at org.apache.hadoop.oncrpc.SimpleUdpServer.run(SimpleUdpServer.java:68)",
                "at org.apache.hadoop.mount.MountdBase.startUDPServer(MountdBase.java:64)",
                "at org.apache.hadoop.mount.MountdBase.start(MountdBase.java:97)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:71)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)",
                "Caused by: java.net.BindException: Address already in use",
                "at sun.nio.ch.Net.bind0(Native Method)",
                "at sun.nio.ch.Net.bind(Net.java:433)",
                "at sun.nio.ch.DatagramChannelImpl.bind(DatagramChannelImpl.java:691)",
                "at sun.nio.ch.DatagramSocketAdaptor.bind(DatagramSocketAdaptor.java:91)",
                "at org.jboss.netty.channel.socket.nio.NioDatagramPipelineSink.bind(NioDatagramPipelineSink.java:129)",
                "at org.jboss.netty.channel.socket.nio.NioDatagramPipelineSink.eventSunk(NioDatagramPipelineSink.java:77)",
                "at org.jboss.netty.channel.Channels.bind(Channels.java:561)",
                "at org.jboss.netty.channel.AbstractChannel.bind(AbstractChannel.java:189)",
                "at org.jboss.netty.bootstrap.ConnectionlessBootstrap.bind(ConnectionlessBootstrap.java:198)"
            ],
            "RootCause": "The root cause of the issue is a BindException indicating that the address 0.0.0.0:4242 is already in use, preventing the SimpleUdpServer from binding to the socket. This typically occurs when another process is using the same port.",
            "StepsToReproduce": [
                "Attempt to restart the NFS gateway while the SimpleUdpServer is already running.",
                "Observe the logs for the bind error indicating the address is already in use."
            ],
            "ExpectedBehavior": "The SimpleUdpServer should successfully bind to the specified address and port, allowing the NFS gateway to restart without errors.",
            "ObservedBehavior": "The SimpleUdpServer fails to bind to the address 0.0.0.0:4242, resulting in a ChannelException and preventing the NFS gateway from restarting.",
            "Suggestions": "Enable the SO_REUSEADDR socket option in the SimpleUdpServer to allow binding to the port even if it is in the TIME_WAIT state. Additionally, check for any other processes that may be using the port 4242.",
            "problem_location": {
                "files": [
                    "MountdBase.java",
                    "SimpleUdpServer.java"
                ],
                "classes": [
                    "org.apache.hadoop.mount.MountdBase",
                    "org.apache.hadoop.oncrpc.SimpleUdpServer"
                ],
                "methods": [
                    "MountdBase.startUDPServer",
                    "SimpleUdpServer.run"
                ]
            },
            "possible_fix": "In the SimpleUdpServer class, modify the socket binding code to include the SO_REUSEADDR option. Example code snippet:\n\n```java\nDatagramChannel channel = DatagramChannel.open();\nchannel.socket().setReuseAddress(true);\nchannel.bind(new InetSocketAddress(4242));\n```"
        },
        "possible_fix_code": {
            "SimpleUdpServer.run": "  public void run() {\n    // Configure the client.\n    DatagramChannelFactory f = new NioDatagramChannelFactory(\n        Executors.newCachedThreadPool(), workerCount);\n\n    server = new ConnectionlessBootstrap(f);\n    server.setPipeline(Channels.pipeline(RpcUtil.STAGE_RPC_MESSAGE_PARSER,\n        rpcProgram, RpcUtil.STAGE_RPC_UDP_RESPONSE));\n\n    server.setOption(\"broadcast\", \"false\");\n    server.setOption(\"sendBufferSize\", SEND_BUFFER_SIZE);\n    server.setOption(\"receiveBufferSize\", RECEIVE_BUFFER_SIZE);\n\n    // Enable SO_REUSEADDR to allow binding even if the socket is in TIME_WAIT state\n    DatagramChannel channel = DatagramChannel.open();\n    channel.socket().setReuseAddress(true);\n    channel.bind(new InetSocketAddress(port));\n\n    // Listen to the UDP port\n    ch = server.bind(new InetSocketAddress(port));\n    InetSocketAddress socketAddr = (InetSocketAddress) ch.getLocalAddress();\n    boundPort = socketAddr.getPort();\n\n    LOG.info(\"Started listening to UDP requests at port \" + boundPort + \" for \"\n        + rpcProgram + \" with workerCount \" + workerCount);\n  }"
        }
    },
    {
        "filename": "HDFS-8055.json",
        "creation_time": "2015-04-02T23:21:55.000+0000",
        "bug_report": {
            "Title": "NullPointerException when topology script is missing.",
            "Description": "A NullPointerException is being thrown in the Hadoop framework when the topology script is missing, specifically during the execution of the `sortLocatedBlocks` method in the `DatanodeManager` class. This issue arises when the method attempts to process located blocks without proper validation of the input parameters, leading to uninitialized objects being accessed. The stack trace indicates that the exception occurs while trying to retrieve block locations, which suggests that the underlying data structure may not be properly initialized or populated when the method is called.",
            "StackTrace": [
                "org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedBlocks(DatanodeManager.java:359)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1789)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)"
            ],
            "RootCause": "The root cause of the NullPointerException is the lack of null checks in the `sortLocatedBlocks` method of the `DatanodeManager` class. Specifically, if the `getDatanodeByHost` method returns null (indicating that the hostname is not mapped to any datanode), and if the DNS resolution fails, the `client` variable remains null. Additionally, if the `locatedblocks` list is null or contains null elements, this will also lead to a NullPointerException when the method attempts to iterate over it.",
            "StepsToReproduce": [
                "1. Ensure that the topology script is missing from the configuration.",
                "2. Attempt to access block locations in HDFS that rely on the `sortLocatedBlocks` method.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The system should validate the presence of the topology script and handle cases where the datanode cannot be resolved without throwing a NullPointerException. Proper error messages should be displayed to inform the user of the missing configuration.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the application to fail when attempting to access block locations due to missing or uninitialized objects.",
            "Suggestions": "Implement null checks in the `sortLocatedBlocks` method to ensure that the `client`, `locatedblocks`, and the results of `b.getLocations()` are not null before proceeding with operations that assume they are non-null. Additionally, improve the validation logic to check for the presence of the topology script and provide informative error messages.",
            "problem_location": {
                "files": [
                    "DatanodeManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager"
                ],
                "methods": [
                    "DatanodeManager.sortLocatedBlocks",
                    "DatanodeManager.getDatanodeByHost"
                ]
            },
            "possible_fix": "Add null checks in the `sortLocatedBlocks` method as follows:\n\n```java\nif (client == null) {\n    // Handle the case where the client is null\n    return;\n}\nif (locatedblocks == null) {\n    // Handle the case where locatedblocks is null\n    return;\n}\nfor (LocatedBlock b : locatedblocks) {\n    if (b.getLocations() == null) {\n        // Handle the case where block locations are null\n        continue;\n    }\n    // Proceed with sorting logic\n}\n```"
        },
        "possible_fix_code": {
            "DatanodeManager.sortLocatedBlocks": "  public void sortLocatedBlocks(final String targethost,\n      final List<LocatedBlock> locatedblocks) {\n    //sort the blocks\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client = getDatanodeByHost(targethost);\n    if (client == null) {\n      List<String> hosts = new ArrayList<String> (1);\n      hosts.add(targethost);\n      String rName = dnsToSwitchMapping.resolve(hosts).get(0);\n      if (rName != null)\n        client = new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n    }\n    \n    if (locatedblocks == null) {\n      // Handle the case where locatedblocks is null\n      return;\n    }\n    \n    Comparator<DatanodeInfo> comparator = avoidStaleDataNodesForRead ?\n        new DFSUtil.DecomStaleComparator(staleInterval) : \n        DFSUtil.DECOM_COMPARATOR;\n        \n    for (LocatedBlock b : locatedblocks) {\n      if (b == null) {\n        // Handle the case where LocatedBlock is null\n        continue;\n      }\n      DatanodeInfo[] di = b.getLocations();\n      if (di == null) {\n        // Handle the case where block locations are null\n        continue;\n      }\n      // Move decommissioned/stale datanodes to the bottom\n      Arrays.sort(di, comparator);\n      \n      int lastActiveIndex = di.length - 1;\n      while (lastActiveIndex > 0 && isInactive(di[lastActiveIndex])) {\n          --lastActiveIndex;\n      }\n      int activeLen = lastActiveIndex + 1;      \n      networktopology.sortByDistance(client, b.getLocations(), activeLen);\n      // must update cache since we modified locations array\n      b.updateCachedStorageInfo();\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-6533.json",
        "creation_time": "2014-06-14T16:15:02.000+0000",
        "bug_report": {
            "Title": "TestBPOfferService#testBasicFunctionality fails intermittently",
            "Description": "The test `testBasicFunctionality` in the `TestBPOfferService` class fails intermittently on the CI server, as reported in the build logs. The failure occurs due to the expected invocation of the `registerDatanode` method on the `datanodeProtocolClientSideTranslatorPB` mock not happening during the test execution. This issue is not reproducible locally, indicating potential environmental differences or timing issues in the test setup.",
            "StackTrace": [
                "org.mockito.exceptions.verification.WantedButNotInvoked: Wanted but not invoked: datanodeProtocolClientSideTranslatorPB.registerDatanode(<any>);",
                "-> at org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBasicFunctionality(TestBPOfferService.java:175)",
                "Actually, there were zero interactions with this mock."
            ],
            "RootCause": "The root cause of the failure is that the `registerDatanode` method on the mock `datanodeProtocolClientSideTranslatorPB` was expected to be called during the execution of the `testBasicFunctionality` method, but it was never invoked. This suggests that the test setup may not be triggering the expected interactions with the mock.",
            "StepsToReproduce": [
                "Run the test `testBasicFunctionality` in the `TestBPOfferService` class on the CI server.",
                "Observe the failure related to the mock interaction with `registerDatanode`."
            ],
            "ExpectedBehavior": "The `registerDatanode` method should be invoked on the mock `datanodeProtocolClientSideTranslatorPB` during the execution of the `testBasicFunctionality` test, confirming that the test conditions are met.",
            "ObservedBehavior": "The test fails with an error indicating that the `registerDatanode` method was not invoked, leading to zero interactions with the mock.",
            "Suggestions": "Review the mock setup in the `testBasicFunctionality` method to ensure that the conditions for invoking `registerDatanode` are correctly established. Consider adding logging to trace the execution flow and verify that all necessary conditions are met for the method to be called.",
            "problem_location": {
                "files": [
                    "TestBPOfferService.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.TestBPOfferService"
                ],
                "methods": [
                    "testBasicFunctionality"
                ]
            },
            "possible_fix": "Ensure that the mock for `DatanodeProtocolClientSideTranslatorPB` is properly instantiated and that the test logic in `testBasicFunctionality` leads to the invocation of `registerDatanode`. This may involve adjusting the test setup or adding necessary conditions to trigger the method call."
        },
        "possible_fix_code": {
            "testBasicFunctionality": "import org.junit.Test;\nimport org.mockito.Mockito;\nimport static org.mockito.Mockito.*;\n\npublic class TestBPOfferService {\n    private DatanodeProtocolClientSideTranslatorPB datanodeProtocolClientSideTranslatorPB;\n\n    @Before\n    public void setUp() {\n        datanodeProtocolClientSideTranslatorPB = Mockito.mock(DatanodeProtocolClientSideTranslatorPB.class);\n    }\n\n    @Test\n    public void testBasicFunctionality() {\n        // Arrange: Set up any necessary conditions for the test\n        // For example, if there are any prerequisites for registerDatanode to be called\n\n        // Act: Call the method that should trigger registerDatanode\n        // This could be a method that interacts with the datanodeProtocolClientSideTranslatorPB mock\n        // Example: someService.methodThatShouldCallRegisterDatanode();\n\n        // Assert: Verify that registerDatanode was called\n        verify(datanodeProtocolClientSideTranslatorPB, times(1)).registerDatanode(any());\n    }\n}"
        }
    },
    {
        "filename": "HDFS-10609.json",
        "creation_time": "2016-07-11T17:14:39.000+0000",
        "bug_report": {
            "Title": "Uncaught InvalidEncryptionKeyException during pipeline recovery may abort downstream applications",
            "Description": "During normal operations, the system is designed to handle the InvalidEncryptionKeyException gracefully, allowing clients to retry after fetching a new encryption key. However, when this exception occurs during pipeline recovery, it is not caught properly, leading to downstream applications like SOLR being aborted. The stack trace indicates that the exception is thrown in the DataTransferSaslUtil class during the SASL handshake process, specifically in the readSaslMessageAndNegotiatedCipherOption method, which fails to handle the ERROR_UNKNOWN_KEY status appropriately.",
            "StackTrace": [
                "org.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=557709482) doesn't exist. Current key: 1350592619",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:1308)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1272)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1433)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1147)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:632)"
            ],
            "RootCause": "The root cause of the issue is that the InvalidEncryptionKeyException is not being handled during the pipeline recovery process. Specifically, the readSaslMessageAndNegotiatedCipherOption method in DataTransferSaslUtil throws this exception when it encounters an ERROR_UNKNOWN_KEY status, which indicates that the required block key is missing. This leads to the failure of the SASL handshake and subsequently aborts downstream applications.",
            "StepsToReproduce": [
                "1. Start a pipeline recovery process in HDFS.",
                "2. Ensure that the encryption key required for the operation is missing or invalid.",
                "3. Observe the behavior of downstream applications like SOLR during this process."
            ],
            "ExpectedBehavior": "The system should catch the InvalidEncryptionKeyException during pipeline recovery and allow the client to fetch a new encryption key, retry the operation, and continue without aborting downstream applications.",
            "ObservedBehavior": "The InvalidEncryptionKeyException is thrown during pipeline recovery, causing downstream applications such as SOLR to abort their operations.",
            "Suggestions": "To resolve this issue, the code in the DataTransferSaslUtil class should be modified to catch the InvalidEncryptionKeyException during the pipeline recovery process and implement a retry mechanism similar to that in the createBlockOutputStream method. Additionally, configuration settings related to encryption keys and cipher suites should be reviewed to ensure they are correctly set up.",
            "problem_location": {
                "files": [
                    "DataTransferSaslUtil.java",
                    "SaslDataTransferClient.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil",
                    "org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient"
                ],
                "methods": [
                    "DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption",
                    "SaslDataTransferClient.doSaslHandshake"
                ]
            },
            "possible_fix": "Modify the readSaslMessageAndNegotiatedCipherOption method to catch InvalidEncryptionKeyException and implement a retry mechanism. Ensure that the configuration for encryption keys is valid and that the key management system is functioning correctly."
        },
        "possible_fix_code": {
            "DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption": "  public static SaslResponseWithNegotiatedCipherOption\n      readSaslMessageAndNegotiatedCipherOption(InputStream in)\n      throws IOException {\n    DataTransferEncryptorMessageProto proto =\n        DataTransferEncryptorMessageProto.parseFrom(vintPrefixed(in));\n    if (proto.getStatus() == DataTransferEncryptorStatus.ERROR_UNKNOWN_KEY) {\n      // Handle the InvalidEncryptionKeyException gracefully\n      LOG.warn(\"Invalid encryption key encountered: {}. Retrying to fetch a new key.\", proto.getMessage());\n      // Implement retry logic or key fetching mechanism here\n      throw new InvalidEncryptionKeyException(proto.getMessage());\n    } else if (proto.getStatus() == DataTransferEncryptorStatus.ERROR) {\n      throw new IOException(proto.getMessage());\n    } else {\n      byte[] response = proto.getPayload().toByteArray();\n      List<CipherOption> options = PBHelperClient.convertCipherOptionProtos(\n          proto.getCipherOptionList());\n      CipherOption option = null;\n      if (options != null && !options.isEmpty()) {\n        option = options.get(0);\n      }\n      return new SaslResponseWithNegotiatedCipherOption(response, option);\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-2310.json",
        "creation_time": "2011-09-05T15:31:00.000+0000",
        "bug_report": {
            "Title": "TestBackupNode fails since HADOOP-7524 went in.",
            "Description": "The issue arises when the TestBackupNode fails due to an 'Unknown protocol' error related to the JournalProtocol in Hadoop. The logs indicate that the JournalProtocol is not registered with the server, leading to an IOException. This suggests potential misconfigurations, missing dependencies, or version mismatches in the Hadoop setup.",
            "StackTrace": [
                "2011-09-05 10:44:36,811 INFO  ipc.Server (Server.java:run(1497)) - IPC Server handler 0 on 60758, call: startLogSegment(NamenodeRegistration(localhost:60754, role=NameNode), 3), rpc version=2, client version=1, methodsFingerPrint=-852377201 from 127.0.0.1:60760, error: ",
                "java.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.hdfs.server.protocol.JournalProtocol",
                "at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)",
                "2011-09-05 10:44:36,813 ERROR namenode.FSEditLog (FSEditLog.java:mapJournalsAndReportErrors(1048)) - Error starting log segment 3 (journal JournalAndStream(mgr=org.apache.hadoop.hdfs.server.namenode.BackupJournalManager@2876b359, stream=null))",
                "java.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.hdfs.server.protocol.JournalProtocol",
                "at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)"
            ],
            "RootCause": "The root cause of the issue is an 'Unknown protocol' error for the JournalProtocol, which indicates that the Hadoop system is unable to recognize or find the specified protocol. This can be attributed to misconfigurations, missing dependencies, or version incompatibilities.",
            "StepsToReproduce": [
                "1. Set up a Hadoop environment with the relevant configurations.",
                "2. Attempt to start the TestBackupNode.",
                "3. Observe the logs for any 'Unknown protocol' errors related to JournalProtocol."
            ],
            "ExpectedBehavior": "The TestBackupNode should start successfully without any errors related to protocol recognition, and the JournalProtocol should be properly registered with the server.",
            "ObservedBehavior": "The TestBackupNode fails to start, and the logs show an 'Unknown protocol' error for the JournalProtocol, indicating that it is not recognized by the server.",
            "Suggestions": "Check the Hadoop configuration files (e.g., hdfs-site.xml, core-site.xml) for any misconfigurations. Ensure that all necessary dependencies are included in the classpath and that the versions of Hadoop on the client and server are compatible.",
            "problem_location": {
                "files": [
                    "Server.java",
                    "FSEditLog.java"
                ],
                "classes": [
                    "org.apache.hadoop.ipc.Server",
                    "org.apache.hadoop.hdfs.server.namenode.FSEditLog"
                ],
                "methods": [
                    "Server.run",
                    "FSEditLog.mapJournalsAndReportErrors"
                ]
            },
            "possible_fix": "Verify and update the Hadoop configuration files to ensure that the JournalProtocol is correctly defined and registered. Additionally, check for any missing JAR files or dependencies that may be required for the protocol."
        },
        "possible_fix_code": {
            "Server.run": "public Writable run() throws Exception {\n    // Ensure the protocol is registered before making the call\n    if (!isProtocolRegistered(call.connection.protocolName)) {\n        throw new IOException(\"Unknown protocol: \" + call.connection.protocolName);\n    }\n    // make the call\n    return call(call.connection.protocolName, \n                call.param, call.timestamp);\n}"
        }
    },
    {
        "filename": "HDFS-3385.json",
        "creation_time": "2012-05-08T12:45:31.000+0000",
        "bug_report": {
            "Title": "ClassCastException when trying to append a file",
            "Description": "A ClassCastException occurs in a Hadoop application when attempting to append to a file. The error arises specifically during the lease recovery process in the FSNamesystem class, where the system attempts to cast a BlockInfo object to a BlockInfoUnderConstruction. This indicates a mismatch in the expected state of the block, suggesting potential issues with state management during file operations.",
            "StackTrace": [
                "2012-05-08 18:13:40,506 WARN  util.KerberosName (KerberosName.java:<clinit>(87)) - Kerberos krb5 configuration not found, setting default realm to empty",
                "Exception in thread \"main\" java.lang.ClassCastException: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo cannot be cast to org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:1787)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1584)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:1824)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:425)",
                "at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1150)",
                "at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1189)",
                "at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1177)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:221)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:1)",
                "at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:981)",
                "at org.apache.hadoop.hdfs.server.datanode.DeleteMe.main(DeleteMe.java:26)"
            ],
            "RootCause": "The ClassCastException occurs because the system attempts to cast a BlockInfo object to a BlockInfoUnderConstruction during the lease recovery process. This indicates a mismatch in expected block states, likely due to improper state management during file operations.",
            "StepsToReproduce": [
                "1. Attempt to append to a file in the Hadoop HDFS.",
                "2. Ensure that the file is in a state that requires lease recovery.",
                "3. Observe the ClassCastException in the logs."
            ],
            "ExpectedBehavior": "The system should successfully append to the file without throwing a ClassCastException, properly managing the state of the blocks involved.",
            "ObservedBehavior": "A ClassCastException is thrown, indicating that a BlockInfo object cannot be cast to a BlockInfoUnderConstruction, disrupting the append operation.",
            "Suggestions": "Review the state management logic in the FSNamesystem class, particularly in the recoverLeaseInternal, appendFile, and startFileInternal methods. Ensure that the system correctly identifies the state of blocks during lease recovery.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem"
                ],
                "methods": [
                    "recoverLeaseInternal",
                    "appendFile",
                    "startFileInternal"
                ]
            },
            "possible_fix": "Investigate the logic in recoverLeaseInternal to ensure that it correctly handles the state of blocks. Consider adding checks to prevent casting errors when the block state is not as expected."
        },
        "possible_fix_code": {
            "recoverLeaseInternal": "  private void recoverLeaseInternal(INode fileInode, \n      String src, String holder, String clientMachine, boolean force)\n      throws IOException {\n    assert hasWriteLock();\n    if (fileInode != null && fileInode.isUnderConstruction()) {\n      INodeFileUnderConstruction pendingFile = (INodeFileUnderConstruction) fileInode;\n      Lease lease = leaseManager.getLease(holder);\n      if (!force && lease != null) {\n        Lease leaseFile = leaseManager.getLeaseByPath(src);\n        if ((leaseFile != null && leaseFile.equals(lease)) ||\n            lease.getHolder().equals(holder)) { \n          throw new AlreadyBeingCreatedException(\n            \"failed to create file \" + src + \" for \" + holder +\n            \" on client \" + clientMachine + \n            \" because current leaseholder is trying to recreate file.\");\n        }\n      }\n      lease = leaseManager.getLease(pendingFile.getClientName());\n      if (lease == null) {\n        throw new AlreadyBeingCreatedException(\n          \"failed to create file \" + src + \" for \" + holder +\n          \" on client \" + clientMachine + \n          \" because pendingCreates is non-null but no leases found.\");\n      }\n      if (force) {\n        LOG.info(\"recoverLease: recover lease \" + lease + \", src=\" + src +\n          \" from client \" + pendingFile.getClientName());\n        internalReleaseLease(lease, src, holder);\n      } else {\n        assert lease.getHolder().equals(pendingFile.getClientName()) :\n          \"Current lease holder \" + lease.getHolder() +\n          \" does not match file creator \" + pendingFile.getClientName();\n        if (lease.expiredSoftLimit()) {\n          LOG.info(\"startFile: recover lease \" + lease + \", src=\" + src +\n              \" from client \" + pendingFile.getClientName());\n          boolean isClosed = internalReleaseLease(lease, src, null);\n          if(!isClosed)\n            throw new RecoveryInProgressException(\n                \"Failed to close file \" + src +\n                \". Lease recovery is in progress. Try again later.\");\n        } else {\n          BlockInfo lastBlock = pendingFile.getLastBlock();\n          if (lastBlock != null) {\n            if (lastBlock instanceof BlockInfoUnderConstruction && \n                ((BlockInfoUnderConstruction) lastBlock).getBlockUCState() ==\n                BlockUCState.UNDER_RECOVERY) {\n              throw new RecoveryInProgressException(\n                \"Recovery in progress, file [\" + src + \"], \" +\n                \"lease owner [\" + lease.getHolder() + \"]\");\n            }\n          }\n          throw new AlreadyBeingCreatedException(\n            \"Failed to create file [\" + src + \"] for [\" + holder +\n            \"] on client [\" + clientMachine +\n            \"], because this file is already being created by [\" +\n            pendingFile.getClientName() + \"] on [\" +\n            pendingFile.getClientMachine() + \"]\");\n        }\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-4006.json",
        "creation_time": "2012-10-04T22:05:14.000+0000",
        "bug_report": {
            "Title": "TestCheckpoint#testSecondaryHasVeryOutOfDateImage occasionally fails due to unexpected exit",
            "Description": "The test `TestCheckpoint#testSecondaryHasVeryOutOfDateImage` intermittently fails due to an unexpected exit caused by a `NullPointerException` (NPE) during the checkpointing process. This issue arises when the background checkpoint operation conflicts with explicit checkpoints initiated by the tests. The stack trace indicates that the NPE occurs in the `doCheckpoint` method of the `SecondaryNameNode` class, suggesting that one or more objects required for the checkpointing process are not properly initialized.",
            "StackTrace": [
                "2012-09-16 01:55:05,901 FATAL hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1355)) - Test resulted in an unexpected exit",
                "org.apache.hadoop.util.ExitUtil$ExitException: Fatal exception with message null",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:480)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:331)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2.run(SecondaryNameNode.java:298)",
                "at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:452)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:294)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The `NullPointerException` in the `doCheckpoint` method is likely caused by uninitialized objects, specifically `checkpointImage` or `namenode`. This can occur if the configuration does not include the necessary parameters, particularly the `DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY`, which is crucial for the proper initialization of the `namenode`.",
            "StepsToReproduce": [
                "Run the test `TestCheckpoint#testSecondaryHasVeryOutOfDateImage` multiple times.",
                "Observe the test execution for intermittent failures due to unexpected exits."
            ],
            "ExpectedBehavior": "The test should complete successfully without any unexpected exits or exceptions.",
            "ObservedBehavior": "The test occasionally fails with a `NullPointerException`, leading to an unexpected exit.",
            "Suggestions": "Ensure that the Hadoop configuration files (like `core-site.xml` and `hdfs-site.xml`) are correctly set up with all required parameters, especially those related to the NameNode's service address. Verify the initialization of `checkpointImage` and `namenode` in the `initialize` method of the `SecondaryNameNode` class.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode",
                    "org.apache.hadoop.conf.Configuration"
                ],
                "methods": [
                    "SecondaryNameNode.doCheckpoint",
                    "SecondaryNameNode.initialize",
                    "Configuration.get"
                ]
            },
            "possible_fix": "Ensure that the configuration passed to the SecondaryNameNode's initialize method is correct, including valid checkpoint directories and a reachable NameNode address. Check for any missing or incorrect properties in the configuration."
        },
        "possible_fix_code": {
            "SecondaryNameNode.doCheckpoint": "  boolean doCheckpoint() throws IOException {\n    checkpointImage.ensureCurrentDirExists();\n    NNStorage dstStorage = checkpointImage.getStorage();\n    \n    // Ensure that namenode is initialized before proceeding\n    if (namenode == null) {\n        throw new IOException(\"Namenode is not initialized. Check configuration for DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY.\");\n    }\n\n    // Tell the namenode to start logging transactions in a new edit file\n    // Returns a token that would be used to upload the merged image.\n    CheckpointSignature sig = namenode.rollEditLog();\n    \n    if ((checkpointImage.getNamespaceID() == 0) ||\n        (sig.isSameCluster(checkpointImage) &&\n         !sig.storageVersionMatches(checkpointImage.getStorage()))) {\n      // if we\\'re a fresh 2NN, or if we\\'re on the same cluster and our storage\n      // needs an upgrade, just take the storage info from the server.\n      dstStorage.setStorageInfo(sig);\n      dstStorage.setClusterID(sig.getClusterID());\n      dstStorage.setBlockPoolID(sig.getBlockpoolID());\n    }\n    sig.validateStorageInfo(checkpointImage);\n\n    // error simulation code for junit test\n    CheckpointFaultInjector.getInstance().afterSecondaryCallsRollEditLog();\n\n    RemoteEditLogManifest manifest =\n      namenode.getEditLogManifest(sig.mostRecentCheckpointTxId + 1);\n\n    boolean loadImage = downloadCheckpointFiles(\n        fsName, checkpointImage, sig, manifest);   // Fetch fsimage and edits\n    doMerge(sig, manifest, loadImage, checkpointImage, namesystem);\n    \n    //\n    // Upload the new image into the NameNode. Then tell the Namenode\n    // to make this new uploaded image as the most current image.\n    //\n    long txid = checkpointImage.getLastAppliedTxId();\n    TransferFsImage.uploadImageFromStorage(fsName, getImageListenAddress(),\n        dstStorage, txid);\n\n    // error simulation code for junit test\n    CheckpointFaultInjector.getInstance().afterSecondaryUploadsNewImage();\n\n    LOG.warn(\"Checkpoint done. New Image Size: \" \n             + dstStorage.getFsImageName(txid).length());\n    \n    return loadImage;\n  }"
        }
    },
    {
        "filename": "HDFS-6715.json",
        "creation_time": "2014-07-21T21:26:25.000+0000",
        "bug_report": {
            "Title": "webhdfs won't fail over when it gets java.io.IOException: Namenode is in startup mode",
            "Description": "During high availability (HA) testing of a MapReduce job using the webhdfs file system, we encountered an issue where the system fails to handle requests properly when the Namenode is still in the startup phase. The logs indicate that the job fails with an IOException stating that the Namenode is in startup mode, which prevents the job from committing successfully.",
            "StackTrace": [
                "2014-04-17 05:08:06,346 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1397710493213_0001_r_000008_0: Container killed by the ApplicationMaster.",
                "Container killed on request. Exit code is 143",
                "Container exited with a non-zero exit code 143",
                "2014-04-17 05:08:10,205 ERROR [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Could not commit job",
                "java.io.IOException: Namenode is in startup mode"
            ],
            "RootCause": "The root cause of the issue is that the Namenode is not fully initialized and is still in the startup mode, which prevents it from processing requests. This can occur due to various reasons such as ongoing initialization, configuration issues, or resource constraints.",
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with high availability enabled.",
                "2. Start a MapReduce job that interacts with the webhdfs file system.",
                "3. Monitor the Namenode's startup process and observe the logs for any indications of it being in startup mode.",
                "4. Check if the job fails with the IOException related to the Namenode."
            ],
            "ExpectedBehavior": "The system should allow the MapReduce job to complete successfully without encountering an IOException related to the Namenode's startup status.",
            "ObservedBehavior": "The MapReduce job fails with an IOException stating that the Namenode is in startup mode, leading to the job not being committed and the container being killed.",
            "Suggestions": "1. Wait for the Namenode to complete its initialization before submitting jobs. 2. Review the Namenode logs for any errors or warnings that may indicate why it is taking longer to start. 3. Check the Hadoop configuration files for correctness. 4. Ensure that the server has adequate resources allocated for the Namenode.",
            "problem_location": {
                "files": [
                    "NameNode.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.NameNode"
                ],
                "methods": [
                    "getStartupStatus",
                    "getServiceStatus",
                    "getState",
                    "getStartupErrors",
                    "getCommonStartupIssues",
                    "getCommonCauses"
                ]
            },
            "possible_fix": "Ensure that the Namenode is fully initialized before allowing jobs to be submitted. This may involve adding checks in the job submission process to verify the Namenode's status or implementing a retry mechanism for job submissions until the Namenode is ready."
        },
        "possible_fix_code": {
            "submitJob": "public void submitJob(Job job) throws IOException {\n    // Check if the Namenode is fully initialized before submitting the job\n    while (isNamenodeInStartupMode()) {\n        try {\n            Thread.sleep(1000); // Wait for 1 second before checking again\n        } catch (InterruptedException e) {\n            Thread.currentThread().interrupt(); // Restore interrupted status\n            throw new IOException(\"Job submission interrupted\", e);\n        }\n    }\n    // Proceed with job submission\n    // ... (existing job submission logic here)\n}\n\nprivate boolean isNamenodeInStartupMode() {\n    // Logic to check if the Namenode is in startup mode\n    // This could involve checking the state of the Namenode\n    return namenode.getServiceStatus() == NamenodeServiceStatus.STARTUP;\n}"
        }
    },
    {
        "filename": "HDFS-2392.json",
        "creation_time": "2011-09-30T20:29:09.000+0000",
        "bug_report": {
            "Title": "Dist with hftp is failing again",
            "Description": "The Hadoop DistCp operation is failing when attempting to copy files from a specified HFTP source path to a destination in HDFS. The logs indicate that the source path is being accessed, but the operation results in a failure with no files copied, skipped, or failed, despite one reported failure. This suggests potential issues with the source or destination paths, permissions, or configuration of the DistCp job.",
            "StackTrace": [
                "java.io.IOException: Copied: 0 Skipped: 0 Failed: 1",
                "at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "RootCause": "The root cause of the failure appears to be related to issues with the source or destination paths, permissions, or configuration errors in the DistCp job, as indicated by the IOException stating that no files were copied.",
            "StepsToReproduce": [
                "Run the command: hadoop distcp hftp://<NN Hostname>:50070/user/hadoopqa/input1/part-00000 /user/hadoopqa/out3",
                "Check the logs for any errors or warnings related to the DistCp operation."
            ],
            "ExpectedBehavior": "The DistCp operation should successfully copy the specified file from the HFTP source path to the HDFS destination without any errors.",
            "ObservedBehavior": "The DistCp operation fails with an IOException, indicating that no files were copied, skipped, or failed, despite one failure being reported.",
            "Suggestions": "Verify the source and destination paths for correctness, check user permissions for accessing the source and writing to the destination, and review the configuration of the DistCp job for any errors.",
            "problem_location": {
                "files": [
                    "DistCp.java"
                ],
                "classes": [
                    "org.apache.hadoop.tools.DistCp"
                ],
                "methods": [
                    "DistCp.copy"
                ]
            },
            "possible_fix": "Ensure that the source path exists and is accessible, the destination path is valid, and that the user has the necessary permissions. Additionally, review the configuration settings for the DistCp job."
        },
        "possible_fix_code": {
            "DistCp.copy": "public void copy() throws IOException {\n    // Ensure the source path exists and is accessible\n    if (!sourcePathExists()) {\n        throw new IOException(\"Source path does not exist: \" + sourcePath);\n    }\n\n    // Ensure the destination path is valid\n    if (!destinationPathIsValid()) {\n        throw new IOException(\"Invalid destination path: \" + destinationPath);\n    }\n\n    // Check user permissions for source and destination\n    if (!userHasPermission()) {\n        throw new IOException(\"User does not have permission to access source or write to destination.\");\n    }\n\n    // Proceed with the copy operation\n    try {\n        // Your existing copy logic here\n    } catch (Exception e) {\n        throw new IOException(\"Failed to copy files: \" + e.getMessage(), e);\n    }\n}"
        }
    },
    {
        "filename": "HDFS-11472.json",
        "creation_time": "2017-02-28T18:39:13.000+0000",
        "bug_report": {
            "Title": "Fix inconsistent replica size after a data pipeline failure",
            "Description": "A critical issue has been identified in the Hadoop Distributed File System (HDFS) where a replica's on-disk length is less than the acknowledged length, violating assumptions in the recovery code. This inconsistency can lead to data corruption and recovery failures. The problem arises when exceptions are thrown within the `BlockReceiver#receivePacket` method, causing the in-memory replica size to remain outdated while the data is still written to disk. This report details the observed errors and suggests improvements to the recovery process.",
            "StackTrace": [
                "2017-01-08 01:41:03,532 WARN org.apache.hadoop.hdfs.server.protocol.InterDatanodeProtocol: Failed to obtain replica info for block (=BP-947993742-10.204.0.136-1362248978912:blk_2526438952_1101394519586) from datanode (=DatanodeInfoWithStorage[10.204.138.17:1004,null,null])",
                "java.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: getBytesOnDisk() < getVisibleLength(), rip=ReplicaBeingWritten, blk_2526438952_1101394519586, RBW",
                "  getNumBytes()     = 27530",
                "  getBytesOnDisk()  = 27006",
                "  getVisibleLength()= 27268",
                "  getVolume()       = /data/6/hdfs/datanode/current",
                "  getBlockFile()    = /data/6/hdfs/datanode/current/BP-947993742-10.204.0.136-1362248978912/current/rbw/blk_2526438952",
                "  bytesAcked=27268",
                "  bytesOnDisk=27006",
                "        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2284)",
                "        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2260)",
                "        at org.apache.hadoop.hdfs.server.datanode.DataNode.initReplicaRecovery(DataNode.java:2566)",
                "        at org.apache.hadoop.hdfs.server.datanode.DataNode.callInitReplicaRecovery(DataNode.java:2577)",
                "        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2645)",
                "        at org.apache.hadoop.hdfs.server.datanode.DataNode.access$400(DataNode.java:245)",
                "        at org.apache.hadoop.hdfs.server.datanode.DataNode$5.run(DataNode.java:2551)",
                "        at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is the failure to update the in-memory replica size when exceptions occur during the execution of `BlockReceiver#receivePacket`. This leads to a mismatch between the on-disk size and the acknowledged size, resulting in inconsistencies that the recovery code cannot handle properly.",
            "StepsToReproduce": [
                "1. Initiate a data transfer to a datanode in HDFS.",
                "2. Simulate an interruption during the data transfer (e.g., by closing the channel).",
                "3. Observe the logs for exceptions thrown during the transfer.",
                "4. Check the replica's on-disk length and acknowledged length to confirm the inconsistency."
            ],
            "ExpectedBehavior": "The system should ensure that the on-disk length of a replica matches the acknowledged length after a data transfer, even in the event of exceptions. The recovery process should be able to handle any discrepancies without leading to data corruption.",
            "ObservedBehavior": "In the event of an exception during data transfer, the on-disk length of the replica is less than the acknowledged length, leading to a failure in the recovery process and potential data corruption.",
            "Suggestions": "Improve the replica recovery code to handle cases where the on-disk size is less than the acknowledged size. Ensure that the in-memory checksum is updated accordingly during recovery processes.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl"
                ],
                "methods": [
                    "initReplicaRecovery",
                    "checkReplicaFiles"
                ]
            },
            "possible_fix": "Modify the `initReplicaRecovery` method to ensure that the in-memory replica size is updated correctly even when exceptions occur. Additionally, enhance the `checkReplicaFiles` method to include checks for discrepancies between on-disk and acknowledged sizes."
        },
        "possible_fix_code": {
            "initReplicaRecovery": "  static ReplicaRecoveryInfo initReplicaRecovery(String bpid, ReplicaMap map,\n      Block block, long recoveryId, long xceiverStopTimeout) throws IOException {\n    while (true) {\n      try {\n        try (AutoCloseableLock lock = map.getLock().acquire()) {\n          ReplicaRecoveryInfo recoveryInfo = initReplicaRecoveryImpl(bpid, map, block, recoveryId);\n          // Update the in-memory replica size to match the acknowledged size\n          if (recoveryInfo.getBytesOnDisk() < recoveryInfo.getVisibleLength()) {\n            recoveryInfo.setBytesOnDisk(recoveryInfo.getVisibleLength());\n          }\n          return recoveryInfo;\n        }\n      } catch (MustStopExistingWriter e) {\n        e.getReplicaInPipeline().stopWriter(xceiverStopTimeout);\n      }\n    }\n  }",
            "checkReplicaFiles": "  static void checkReplicaFiles(final ReplicaInfo r) throws IOException {\n    //check replica's data exists\n    if (!r.blockDataExists()) {\n      throw new FileNotFoundException(\"Block data not found, r=\" + r);\n    }\n    if (r.getBytesOnDisk() != r.getBlockDataLength()) {\n      throw new IOException(\"Block length mismatch, len=\"\n          + r.getBlockDataLength() + \" but r=\" + r);\n    }\n\n    //check replica's meta file\n    if (!r.metadataExists()) {\n      throw new IOException(r.getMetadataURI() + \" does not exist, r=\" + r);\n    }\n    if (r.getMetadataLength() == 0) {\n      throw new IOException(\"Metafile is empty, r=\" + r);\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-10760.json",
        "creation_time": "2016-08-12T08:29:24.000+0000",
        "bug_report": {
            "Title": "DataXceiver#run() should not log InvalidToken exception as an error",
            "Description": "The DataXceiver#run() method currently logs the InvalidToken exception as an error when a client attempts to access a block with an expired token. This behavior is misleading since the token expiration is a normal scenario that should not be treated as a server error. The log entry generated includes details about the expired block token, which can clutter the logs and obscure actual errors. The DataXceiver#checkAccess() method already logs the InvalidToken as a warning, making the error log in run() redundant.",
            "StackTrace": [
                "2016-08-11 02:41:09,817 ERROR datanode.DataNode (DataXceiver.java:run(269)) - XXXXXXX:50010:DataXceiver error processing READ_BLOCK operation  src: /10.17.1.5:38844 dst: /10.17.1.5:50010",
                "org.apache.hadoop.security.token.SecretManager$InvalidToken: Block token with block_token_identifier (expiryDate=1470850746803, keyId=-2093956963, userId=hbase, blockPoolId=BP-641703426-10.17.1.2-1468517918886, blockId=1077120201, access modes=[READ]) is expired.",
                "at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:280)",
                "at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:301)",
                "at org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager.checkAccess(BlockPoolTokenSecretManager.java:97)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess(DataXceiver.java:1236)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:481)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:242)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the DataXceiver#run() method is incorrectly logging the InvalidToken exception as an error instead of treating it as a normal condition when a block token has expired. This leads to unnecessary error logs that can confuse operators and developers.",
            "StepsToReproduce": [
                "1. Start the Hadoop DataNode service.",
                "2. Use a client to request a block read operation with an expired block token.",
                "3. Observe the logs generated by the DataXceiver."
            ],
            "ExpectedBehavior": "When a block token is expired, the system should log a warning for the InvalidToken exception in DataXceiver#checkAccess() and not log it as an error in DataXceiver#run().",
            "ObservedBehavior": "The system logs the InvalidToken exception as an error in DataXceiver#run(), which is misleading and clutters the error logs.",
            "Suggestions": "Modify the DataXceiver#run() method to catch the InvalidToken exception and avoid logging it as an error. Instead, allow the warning logged by DataXceiver#checkAccess() to suffice.",
            "problem_location": {
                "files": [
                    "DataXceiver.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.DataXceiver"
                ],
                "methods": [
                    "DataXceiver.run",
                    "DataXceiver.checkAccess"
                ]
            },
            "possible_fix": "In the DataXceiver#run() method, catch the InvalidToken exception and replace the error logging with a no-op or a debug log. This will prevent the misleading error log from being generated."
        },
        "possible_fix_code": {
            "DataXceiver.run": "public void run() {\n    try {\n        // Existing code for processing READ_BLOCK operation\n    } catch (SecretManager.InvalidToken e) {\n        // Log as debug instead of error to avoid cluttering logs\n        LOG.debug(\"InvalidToken exception caught: {}\", e.getMessage());\n        // Optionally, you can handle any additional logic here if needed\n    } catch (Exception e) {\n        // Handle other exceptions as errors\n        LOG.error(\"DataXceiver error processing READ_BLOCK operation\", e);\n    }\n}"
        }
    },
    {
        "filename": "HDFS-13635.json",
        "creation_time": "2018-05-29T19:54:50.000+0000",
        "bug_report": {
            "Title": "Incorrect message when block is not found",
            "Description": "When a client attempts to open a file, it requests the DataNode to check the visible length of the blocks. If the block is not found on the DataNode, the system erroneously throws a 'Cannot append to a non-existent replica' message. This is misleading because the method `getReplicaVisibleLength()` is invoked for purposes other than appending to a block. The expected behavior should simply indicate that the block is not found. This issue has been observed in CDH5.13 and appears to persist in the Apache Hadoop trunk.",
            "StackTrace": [
                "2018-05-29 09:23:41,966 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getReplicaVisibleLength from 10.0.0.14:53217 Call#38334117 Retry#0",
                "org.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Cannot append to a non-existent replica BP-725378529-10.236.236.8-1410027444173:13276792346",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo(FsDatasetImpl.java:792)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaVisibleLength(FsDatasetImpl.java:2588)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.getReplicaVisibleLength(DataNode.java:2756)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolServerSideTranslatorPB.getReplicaVisibleLength(ClientDatanodeProtocolServerSideTranslatorPB.java:107)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ClientDatanodeProtocolService$2.callBlockingMethod(ClientDatanodeProtocolProtos.java:17873)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2217)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2213)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2211)"
            ],
            "RootCause": "The root cause of the issue is that the method `getReplicaInfo(String bpid, long blkid)` in the `FsDatasetImpl` class throws a `ReplicaNotFoundException` when it cannot find the requested replica. This occurs when the replica has either been deleted or was never created, leading to the misleading error message about appending to a non-existent replica.",
            "StepsToReproduce": [
                "1. Open a file in the system that is expected to have blocks.",
                "2. Ensure that the block is not present on the DataNode.",
                "3. Observe the error message returned by the system."
            ],
            "ExpectedBehavior": "The system should return a message indicating that the block is not found, rather than an error about appending to a non-existent replica.",
            "ObservedBehavior": "The system incorrectly throws a 'Cannot append to a non-existent replica' message when the block is not found.",
            "Suggestions": "Review the implementation of the `getReplicaVisibleLength()` method to ensure it handles the case of missing blocks correctly. Consider modifying the error message to reflect the actual issue of the block not being found.",
            "problem_location": {
                "files": [
                    "FsDatasetImpl.java",
                    "DataNode.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl",
                    "org.apache.hadoop.hdfs.server.datanode.DataNode"
                ],
                "methods": [
                    "FsDatasetImpl.getReplicaInfo",
                    "DataNode.getReplicaVisibleLength"
                ]
            },
            "possible_fix": "Modify the error handling in the `getReplicaVisibleLength()` method to return a more appropriate message when a block is not found, rather than indicating an append operation failure."
        },
        "possible_fix_code": {
            "DataNode.getReplicaVisibleLength": "public long getReplicaVisibleLength(String bpid, long blkid) throws IOException {\n    try {\n        ReplicaInfo replicaInfo = fsDataset.getReplicaInfo(bpid, blkid);\n        return replicaInfo.getVisibleLength();\n    } catch (ReplicaNotFoundException e) {\n        // Modify the error handling to provide a clearer message\n        throw new IOException(\"Block not found: \" + bpid + \":\" + blkid);\n    }\n}"
        }
    },
    {
        "filename": "HDFS-11608.json",
        "creation_time": "2017-03-31T17:37:20.000+0000",
        "bug_report": {
            "Title": "HDFS write crashed with block size greater than 2 GB",
            "Description": "The HDFS client crashes when attempting to write files larger than 2 GB using a block size greater than 2 GB. Specifically, writing a 3 GB file with a block size of 3 GB results in an OutOfMemoryError on the client side and an IOException on the DataNode. The stack trace indicates that the DataNode encounters an IOException due to an incorrect packet payload size of 2147483128, which exceeds the maximum allowed size for a packet in Java. This issue is likely related to the handling of packet sizes in the Hadoop HDFS codebase.",
            "StackTrace": [
                "2017-03-30 16:34:33,828 ERROR datanode.DataNode (DataXceiver.java:run(278)) - c6401.ambari.apache.org:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.64.101:47167 dst: /192.168.64.101:50010",
                "java.io.IOException: Incorrect value for packet payload size: 2147483128",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:159)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:137)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:502)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:806)"
            ],
            "RootCause": "The root cause of the issue is an attempt to process a packet with a payload size of 2147483128 bytes, which exceeds the maximum allowed packet size of 16 MB (defined as MAX_PACKET_SIZE in the PacketReceiver class). This indicates a misconfiguration or a bug in the code that generates the packet size.",
            "StepsToReproduce": [
                "1. Configure HDFS to use a block size greater than 2 GB.",
                "2. Attempt to write a file larger than 2 GB (e.g., 3 GB) to HDFS.",
                "3. Observe the OutOfMemoryError on the client and IOException on the DataNode."
            ],
            "ExpectedBehavior": "The HDFS client should successfully write files larger than 2 GB without crashing, and the DataNode should process the write operation without throwing an IOException.",
            "ObservedBehavior": "The HDFS client crashes with an OutOfMemoryError, and the DataNode throws an IOException indicating an incorrect packet payload size.",
            "Suggestions": "Review the configuration settings for block size and ensure that the packet size is correctly calculated and does not exceed the maximum allowed size. Consider implementing validation checks to prevent packet sizes from exceeding the defined limits.",
            "problem_location": {
                "files": [
                    "PacketReceiver.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver"
                ],
                "methods": [
                    "PacketReceiver.doRead"
                ]
            },
            "possible_fix": "Ensure that the packet size calculations in the doRead method of the PacketReceiver class correctly validate against the MAX_PACKET_SIZE constant. Implement checks to prevent the generation of packet sizes that exceed this limit."
        },
        "possible_fix_code": {
            "doRead": "  private void doRead(ReadableByteChannel ch, InputStream in)\n      throws IOException {\n    // Each packet looks like:\n    //   PLEN    HLEN      HEADER     CHECKSUMS  DATA\n    //   32-bit  16-bit   <protobuf>  <variable length>\n    //\n    // PLEN:      Payload length\n    //            = length(PLEN) + length(CHECKSUMS) + length(DATA)\n    //            This length includes its own encoded length in\n    //            the sum for historical reasons.\n    //\n    // HLEN:      Header length\n    //            = length(HEADER)\n    //\n    // HEADER:    the actual packet header fields, encoded in protobuf\n    // CHECKSUMS: the crcs for the data chunk. May be missing if\n    //            checksums were not requested\n    // DATA       the actual block data\n    Preconditions.checkState(curHeader == null || !curHeader.isLastPacketInBlock());\n\n    curPacketBuf.clear();\n    curPacketBuf.limit(PacketHeader.PKT_LENGTHS_LEN);\n    doReadFully(ch, in, curPacketBuf);\n    curPacketBuf.flip();\n    int payloadLen = curPacketBuf.getInt();\n\n    if (payloadLen < Ints.BYTES) {\n      // The \"payload length\" includes its own length. Therefore it\n      // should never be less than 4 bytes\n      throw new IOException(\"Invalid payload length \" +\n          payloadLen);\n    }\n    int dataPlusChecksumLen = payloadLen - Ints.BYTES;\n    int headerLen = curPacketBuf.getShort();\n    if (headerLen < 0) {\n      throw new IOException(\"Invalid header length \" + headerLen);\n    }\n\n    LOG.trace(\"readNextPacket: dataPlusChecksumLen={}, headerLen={}\",\n        dataPlusChecksumLen, headerLen);\n\n    // Sanity check the buffer size so we don\\'t allocate too much memory\n    // and OOME.\n    int totalLen = payloadLen + headerLen;\n    if (totalLen < 0 || totalLen > MAX_PACKET_SIZE) {\n      throw new IOException(\"Incorrect value for packet payload size: \" +\n                            payloadLen);\n    }\n\n    // Ensure that the payload length does not exceed the maximum allowed size\n    if (payloadLen > MAX_PACKET_SIZE) {\n      throw new IOException(\"Payload length exceeds maximum allowed size: \" + payloadLen);\n    }\n\n    // Make sure we have space for the whole packet, and\n    // read it.\n    reallocPacketBuf(PacketHeader.PKT_LENGTHS_LEN +\n        dataPlusChecksumLen + headerLen);\n    curPacketBuf.clear();\n    curPacketBuf.position(PacketHeader.PKT_LENGTHS_LEN);\n    curPacketBuf.limit(PacketHeader.PKT_LENGTHS_LEN +\n        dataPlusChecksumLen + headerLen);\n    doReadFully(ch, in, curPacketBuf);\n    curPacketBuf.flip();\n    curPacketBuf.position(PacketHeader.PKT_LENGTHS_LEN);\n\n    // Extract the header from the front of the buffer (after the length prefixes)\n    byte[] headerBuf = new byte[headerLen];\n    curPacketBuf.get(headerBuf);\n    if (curHeader == null) {\n      curHeader = new PacketHeader();\n    }\n    curHeader.setFieldsFromData(payloadLen, headerBuf);\n\n    // Compute the sub-slices of the packet\n    int checksumLen = dataPlusChecksumLen - curHeader.getDataLen();\n    if (checksumLen < 0) {\n      throw new IOException(\"Invalid packet: data length in packet header \" +\n          \"exceeds data length received. dataPlusChecksumLen=\" +\n          dataPlusChecksumLen + \" header: \" + curHeader);\n    }\n\n    reslicePacket(headerLen, checksumLen, curHeader.getDataLen());\n  }"
        }
    },
    {
        "filename": "HDFS-12638.json",
        "creation_time": "2017-10-11T11:29:04.000+0000",
        "bug_report": {
            "Title": "Delete copy-on-truncate block along with the original block, when deleting a file being truncated",
            "Description": "The NameNode crashes due to a NullPointerException (NPE) when attempting to delete a file that is being truncated. The stack trace indicates that the NPE occurs in the `chooseTargets` method of the `ReplicationWork` class, specifically when it tries to access a `BlockCollection` that is null. This issue seems to be related to changes made in HDFS-9754, which removed checks for null `BlockCollection` instances. The logs show that the `ReplicationMonitor` thread encounters this exception, leading to the NameNode's exit.",
            "StackTrace": [
                "2017-10-11 16:29:06,161 ERROR [ReplicationMonitor] org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor thread received Runtime exception.",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:55)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1532)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1491)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3792)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3744)",
                "at java.lang.Thread.run(Thread.java:834)"
            ],
            "RootCause": "The root cause of the NullPointerException is that the `BlockCollection` passed to the `ReplicationWork` constructor is null. This can occur if the block being processed does not have an associated `BlockCollection`, which is critical for the replication process.",
            "StepsToReproduce": [
                "1. Create a file in HDFS.",
                "2. Start truncating the file.",
                "3. Attempt to delete the file while it is being truncated."
            ],
            "ExpectedBehavior": "The system should successfully delete the file and its associated blocks without crashing the NameNode.",
            "ObservedBehavior": "The NameNode crashes with a NullPointerException when trying to delete a file that is being truncated.",
            "Suggestions": "Ensure that the `BlockCollection` is properly initialized before passing it to the `ReplicationWork` constructor. Consider adding null checks for `BlockCollection` in the `chooseTargets` method to prevent the NPE.",
            "problem_location": {
                "files": [
                    "ReplicationWork.java",
                    "BlockManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork",
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager"
                ],
                "methods": [
                    "ReplicationWork.chooseTargets",
                    "BlockManager.computeReplicationWorkForBlocks"
                ]
            },
            "possible_fix": "In the `ReplicationWork` constructor, ensure that the `BlockCollection` is not null before proceeding. Additionally, add a null check in the `chooseTargets` method to handle cases where `getBc()` might return null."
        },
        "possible_fix_code": {
            "ReplicationWork.chooseTargets": "  void chooseTargets(BlockPlacementPolicy blockplacement,\n      BlockStoragePolicySuite storagePolicySuite,\n      Set<Node> excludedNodes) {\n    // Check if the BlockCollection is null before proceeding\n    if (getBc() == null) {\n        throw new IllegalStateException(\"BlockCollection cannot be null\");\n    }\n    assert getSrcNodes().length > 0\n        : \"At least 1 source node should have been selected\";\n    try {\n      DatanodeStorageInfo[] chosenTargets = blockplacement.chooseTarget(\n          getBc().getName(), getAdditionalReplRequired(), getSrcNodes()[0],\n          getLiveReplicaStorages(), false, excludedNodes,\n          getBlock().getNumBytes(),\n          storagePolicySuite.getPolicy(getBc().getStoragePolicyID()),\n          null);\n      setTargets(chosenTargets);\n    } finally {\n      getSrcNodes()[0].decrementPendingReplicationWithoutTargets();\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-12383.json",
        "creation_time": "2017-08-31T20:43:50.000+0000",
        "bug_report": {
            "Title": "Re-encryption updater should handle canceled tasks better",
            "Description": "An issue was observed where the re-encryption updater exited unexpectedly due to a `CancellationException`. This occurred during the processing of re-encryption tasks, leading to subsequent tasks not executing as expected. The logs indicate that a cancellation was initiated, but the updater did not handle this scenario gracefully, resulting in an abrupt exit.",
            "StackTrace": [
                "java.util.concurrent.CancellationException",
                "at java.util.concurrent.FutureTask.report(FutureTask.java:121)",
                "at java.util.concurrent.FutureTask.get(FutureTask.java:192)",
                "at org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.takeAndProcessTasks(ReencryptionUpdater.java:404)",
                "at org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.run(ReencryptionUpdater.java:250)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is a `CancellationException` thrown when the re-encryption updater attempts to retrieve the result of a task that was cancelled. This indicates that the updater does not properly handle the cancellation of tasks, leading to an unexpected exit.",
            "StepsToReproduce": [
                "1. Initiate the re-encryption process for a zone.",
                "2. Cancel the re-encryption tasks while they are in progress.",
                "3. Observe the behavior of the re-encryption updater."
            ],
            "ExpectedBehavior": "The re-encryption updater should handle cancelled tasks gracefully, allowing other tasks to continue executing without exiting unexpectedly.",
            "ObservedBehavior": "The re-encryption updater exits with a `CancellationException`, and no further tasks are executed after the cancellation.",
            "Suggestions": "Implement proper handling of `CancellationException` in the `ReencryptionUpdater` class to ensure that the updater can continue processing other tasks even if some are cancelled.",
            "problem_location": {
                "files": [
                    "ReencryptionUpdater.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater"
                ],
                "methods": [
                    "takeAndProcessTasks",
                    "run"
                ]
            },
            "possible_fix": "Modify the `takeAndProcessTasks` method in the `ReencryptionUpdater` class to catch the `CancellationException` and log the cancellation without terminating the updater thread."
        },
        "possible_fix_code": {
            "takeAndProcessTasks": "  private void takeAndProcessTasks() throws Exception {\n    final Future<ReencryptionTask> completed = batchService.take();\n    throttle();\n    checkPauseForTesting();\n    ReencryptionTask task = null;\n    try {\n      task = completed.get();\n    } catch (CancellationException e) {\n      LOG.debug(\"Skipped canceled re-encryption task, cancellation occurred.\");\n      return; // Exit the method gracefully if the task was cancelled\n    }\n\n    boolean shouldRetry;\n    do {\n      dir.getFSNamesystem().writeLock();\n      try {\n        throttleTimerLocked.start();\n        processTask(task);\n        shouldRetry = false;\n      } catch (RetriableException | SafeModeException re) {\n        // Keep retrying until succeed.\n        LOG.info(\"Exception when processing re-encryption task for zone {}, retrying...\", task.zoneId, re);\n        shouldRetry = true;\n        Thread.sleep(faultRetryInterval);\n      } catch (IOException ioe) {\n        LOG.warn(\"Failure processing re-encryption task for zone {}\", task.zoneId, ioe);\n        ++task.numFailures;\n        task.processed = true;\n        shouldRetry = false;\n      } finally {\n        dir.getFSNamesystem().writeUnlock(\"reencryptUpdater\");\n        throttleTimerLocked.stop();\n      }\n      // logSync regardless, to prevent edit log buffer overflow triggering\n      // logSync inside FSN writelock.\n      dir.getEditLog().logSync();\n    } while (shouldRetry);\n  }"
        }
    },
    {
        "filename": "HDFS-5322.json",
        "creation_time": "2013-10-07T22:52:54.000+0000",
        "bug_report": {
            "Title": "HDFS delegation token not found in cache errors seen on secure HA clusters",
            "Description": "During high availability (HA) testing, we encountered errors indicating that HDFS delegation tokens could not be found in the cache. This issue leads to job failures, as the system is unable to authenticate users properly due to missing tokens. The stack trace reveals that the error originates from the `UserGroupInformation.doAs` method, which attempts to execute a job but fails due to an invalid or missing delegation token.",
            "StackTrace": [
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)",
                "Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 11 for hrt_qa) can't be found in cache",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1347)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1300)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)",
                "at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source)"
            ],
            "RootCause": "The root cause of the issue is that the HDFS delegation token is either expired or not properly cached, leading to the failure of job execution. This is likely due to improper management of the token lifecycle, which includes creation, renewal, and expiration.",
            "StepsToReproduce": [
                "1. Set up a secure HA cluster with HDFS.",
                "2. Run a job that requires delegation tokens.",
                "3. Monitor the logs for errors related to missing delegation tokens."
            ],
            "ExpectedBehavior": "The system should successfully cache the HDFS delegation tokens, allowing jobs to authenticate and execute without encountering token-related errors.",
            "ObservedBehavior": "Jobs fail with errors indicating that the HDFS delegation token cannot be found in the cache, resulting in authentication failures.",
            "Suggestions": "Investigate the token caching mechanism in the Hadoop security framework. Ensure that delegation tokens are being properly created, cached, and renewed. Consider implementing logging around token management to identify where the failure occurs.",
            "problem_location": {
                "files": [
                    "UserGroupInformation.java",
                    "YarnChild.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.UserGroupInformation",
                    "org.apache.hadoop.mapred.YarnChild"
                ],
                "methods": [
                    "UserGroupInformation.doAs",
                    "YarnChild.main"
                ]
            },
            "possible_fix": "Review the implementation of the delegation token caching mechanism in the Hadoop security classes. Ensure that tokens are being stored correctly and that their lifecycle is managed properly. If necessary, add checks to handle expired tokens gracefully."
        },
        "possible_fix_code": {
            "UserGroupInformation.doAs": "  public <T> T doAs(PrivilegedExceptionAction<T> action\n                    ) throws IOException, InterruptedException {\n    try {\n      logPrivilegedAction(subject, action);\n      return Subject.doAs(subject, action);\n    } catch (PrivilegedActionException pae) {\n      Throwable cause = pae.getCause();\n      LOG.error(\"PriviledgedActionException as:\" + this + \" cause:\" + cause);\n      if (cause instanceof IOException) {\n        throw (IOException) cause;\n      } else if (cause instanceof Error) {\n        throw (Error) cause;\n      } else if (cause instanceof RuntimeException) {\n        throw (RuntimeException) cause;\n      } else if (cause instanceof InterruptedException) {\n        throw (InterruptedException) cause;\n      } else {\n        throw new UndeclaredThrowableException(cause);\n      }\n    } finally {\n      // Check if the delegation token is valid and refresh if necessary\n      if (isTokenExpired()) {\n        refreshDelegationToken();\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-11741.json",
        "creation_time": "2017-05-02T17:57:25.000+0000",
        "bug_report": {
            "Title": "Long running balancer may fail due to expired DataEncryptionKey",
            "Description": "A long-running balancer in Hadoop HDFS may fail to operate correctly due to the KeyManager returning an expired DataEncryptionKey. This issue manifests as an InvalidEncryptionKeyException, indicating that the required block key (keyID=1005215027) does not exist, while the current key is 1005215030. The balancer's KeyManager synchronizes with the NameNode regarding block keys but fails to update the DataEncryptionKey accordingly. This problem is exacerbated in clusters with a Kerberos ticket lifetime of 10 hours and a default block token expiration of 10 hours, leading to failures after 20-30 hours of operation.",
            "StackTrace": [
                "2017-04-30 05:03:58,661 WARN  [pool-1464-thread-10] balancer.Dispatcher (Dispatcher.java:dispatch(325)) - Failed to move blk_1067352712_3913241 with size=546650 from 10.0.0.134:50010:DISK to 10.0.0.98:50010:DISK through 10.0.0.134:50010",
                "org.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=1005215027) doesn't exist. Current key: 1005215030",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.dispatch(Dispatcher.java:311)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.access$2300(Dispatcher.java:182)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$1.run(Dispatcher.java:899)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is the failure of the KeyManager to update the DataEncryptionKey in synchronization with the block keys from the NameNode, leading to the use of an expired key during data transfer operations.",
            "StepsToReproduce": [
                "Set up a Hadoop HDFS cluster with a Kerberos ticket lifetime of 10 hours.",
                "Run a long-running balancer operation for more than 20 hours.",
                "Monitor the logs for any InvalidEncryptionKeyException related to missing block keys."
            ],
            "ExpectedBehavior": "The balancer should successfully move blocks between nodes without encountering any encryption key errors, ensuring that the correct DataEncryptionKey is used throughout the operation.",
            "ObservedBehavior": "The balancer fails to move blocks after a prolonged operation, resulting in an InvalidEncryptionKeyException due to the absence of the required block key.",
            "Suggestions": "Review the KeyManager's implementation to ensure it updates the DataEncryptionKey in accordance with the block keys. Consider implementing a mechanism to refresh or validate the DataEncryptionKey periodically during long-running operations.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/DataTransferSaslUtil.java",
                    "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java",
                    "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/DataEncryptionKeyFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil",
                    "org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient",
                    "org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory"
                ],
                "methods": [
                    "DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption",
                    "SaslDataTransferClient.doSaslHandshake",
                    "DataEncryptionKeyFactory.newDataEncryptionKey"
                ]
            },
            "possible_fix": "Ensure that the KeyManager updates the DataEncryptionKey whenever block keys are synchronized with the NameNode. This may involve modifying the synchronization logic to include DataEncryptionKey updates."
        },
        "possible_fix_code": {
            "DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption": "  public static SaslResponseWithNegotiatedCipherOption\n      readSaslMessageAndNegotiatedCipherOption(InputStream in)\n      throws IOException {\n    DataTransferEncryptorMessageProto proto =\n        DataTransferEncryptorMessageProto.parseFrom(vintPrefixed(in));\n    if (proto.getStatus() == DataTransferEncryptorStatus.ERROR_UNKNOWN_KEY) {\n      throw new InvalidEncryptionKeyException(proto.getMessage());\n    } else if (proto.getStatus() == DataTransferEncryptorStatus.ERROR) {\n      throw new IOException(proto.getMessage());\n    } else {\n      byte[] response = proto.getPayload().toByteArray();\n      List<CipherOption> options = PBHelperClient.convertCipherOptionProtos(\n          proto.getCipherOptionList());\n      CipherOption option = null;\n      if (options != null && !options.isEmpty()) {\n        option = options.get(0);\n      }\n      // Ensure DataEncryptionKey is updated when block keys are synchronized\n      updateDataEncryptionKey();\n      return new SaslResponseWithNegotiatedCipherOption(response, option);\n    }\n  }",
            "SaslDataTransferClient.doSaslHandshake": "  private IOStreamPair doSaslHandshake(InetAddress addr,\n      OutputStream underlyingOut, InputStream underlyingIn, String userName,\n      Map<String, String> saslProps,\n      CallbackHandler callbackHandler) throws IOException {\n\n    DataOutputStream out = new DataOutputStream(underlyingOut);\n    DataInputStream in = new DataInputStream(underlyingIn);\n\n    SaslParticipant sasl= SaslParticipant.createClientSaslParticipant(userName,\n        saslProps, callbackHandler);\n\n    out.writeInt(SASL_TRANSFER_MAGIC_NUMBER);\n    out.flush();\n\n    try {\n      // Start of handshake - \"initial response\" in SASL terminology.\n      sendSaslMessage(out, new byte[0]);\n\n      // step 1\n      byte[] remoteResponse = readSaslMessage(in);\n      byte[] localResponse = sasl.evaluateChallengeOrResponse(remoteResponse);\n      List<CipherOption> cipherOptions = null;\n      String cipherSuites = conf.get(\n          DFS_ENCRYPT_DATA_TRANSFER_CIPHER_SUITES_KEY);\n      if (requestedQopContainsPrivacy(saslProps)) {\n        // Negotiate cipher suites if configured.  Currently, the only supported\n        // cipher suite is AES/CTR/NoPadding, but the protocol allows multiple\n        // values for future expansion.\n        if (cipherSuites != null && !cipherSuites.isEmpty()) {\n          if (!cipherSuites.equals(CipherSuite.AES_CTR_NOPADDING.getName())) {\n            throw new IOException(String.format(\"Invalid cipher suite, %s=%s\",\n                DFS_ENCRYPT_DATA_TRANSFER_CIPHER_SUITES_KEY, cipherSuites));\n          }\n          CipherOption option = new CipherOption(CipherSuite.AES_CTR_NOPADDING);\n          cipherOptions = Lists.newArrayListWithCapacity(1);\n          cipherOptions.add(option);\n        }\n      }\n      sendSaslMessageAndNegotiationCipherOptions(out, localResponse,\n          cipherOptions);\n\n      // step 2 (client-side only)\n      SaslResponseWithNegotiatedCipherOption response =\n          readSaslMessageAndNegotiatedCipherOption(in);\n      localResponse = sasl.evaluateChallengeOrResponse(response.payload);\n      assert localResponse == null;\n\n      // SASL handshake is complete\n      checkSaslComplete(sasl, saslProps);\n\n      CipherOption cipherOption = null;\n      if (sasl.isNegotiatedQopPrivacy()) {\n        // Unwrap the negotiated cipher option\n        cipherOption = unwrap(response.cipherOption, sasl);\n        if (LOG.isDebugEnabled()) {\n          if (cipherOption == null) {\n            // No cipher suite is negotiated\n            if (cipherSuites != null && !cipherSuites.isEmpty()) {\n              // the client accepts some cipher suites, but the server does not.\n              LOG.debug(\"Client accepts cipher suites {}, \"\n                      + \"but server {} does not accept any of them\",\n                  cipherSuites, addr.toString());\n            }\n          } else {\n            LOG.debug(\"Client using cipher suite {} with server {}\",\n                cipherOption.getCipherSuite().getName(), addr.toString());\n          }\n        }\n      }\n\n      // If negotiated cipher option is not null, we will use it to create\n      // stream pair.\n      return cipherOption != null ? createStreamPair(\n          conf, cipherOption, underlyingOut, underlyingIn, false) :\n          sasl.createStreamPair(out, in);\n    } catch (IOException ioe) {\n      sendGenericSaslErrorMessage(out, ioe.getMessage());\n      throw ioe;\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-3936.json",
        "creation_time": "2012-09-14T05:35:12.000+0000",
        "bug_report": {
            "Title": "MiniDFSCluster shutdown races with BlocksMap usage",
            "Description": "The issue arises during the shutdown of the MiniDFSCluster, where a race condition occurs between the BlockManager's operations and the shutdown process. Specifically, the thread responsible for closing the BlockManager (FSN#stopCommonServices) holds the FSN lock while attempting to close the BlockManager. Meanwhile, the BlockManager is trying to acquire the FSN lock to access the BlocksMap, leading to a deadlock situation. This results in a NullPointerException when the BlockManager attempts to retrieve a BlockCollection for a block that is no longer valid or has not been properly initialized.",
            "StackTrace": [
                "2012-09-13 18:54:12,526 FATAL hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1355)) - Test resulted in an unexpected exit",
                "org.apache.hadoop.util.ExitUtil$ExitException: Fatal exception with message null",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap.getBlockCollection(BlocksMap.java:101)",
                "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1132)",
                "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1107)",
                "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3061)",
                "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3023)",
                "\tat java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the NullPointerException is that the BlockManager is trying to access a BlockCollection for a block that does not exist in the BlocksMap. This can occur due to race conditions where blocks are removed or not properly initialized while the BlockManager is still processing replication work.",
            "StepsToReproduce": [
                "1. Start a MiniDFSCluster instance.",
                "2. Trigger a shutdown of the MiniDFSCluster while there are ongoing block management operations.",
                "3. Observe the logs for a NullPointerException related to BlocksMap."
            ],
            "ExpectedBehavior": "During the shutdown of the MiniDFSCluster, all block management operations should complete successfully without any exceptions, ensuring that all resources are released properly.",
            "ObservedBehavior": "The MiniDFSCluster experiences an unexpected exit due to a NullPointerException, indicating that the BlockManager attempted to access a BlockCollection that was not available.",
            "Suggestions": "To mitigate this issue, ensure that the BlockManager's operations are properly synchronized with the shutdown process. Consider implementing additional checks to verify the existence of blocks in the BlocksMap before attempting to access them.",
            "problem_location": {
                "files": [
                    "MiniDFSCluster.java",
                    "BlocksMap.java",
                    "BlockManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap",
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager"
                ],
                "methods": [
                    "BlocksMap.getBlockCollection",
                    "BlockManager.computeReplicationWorkForBlocks",
                    "BlockManager.computeReplicationWork"
                ]
            },
            "possible_fix": "Implement a check in the BlockManager's computeReplicationWorkForBlocks method to ensure that the BlockCollection is valid before proceeding with replication work. Additionally, review the shutdown sequence to ensure that it does not interfere with ongoing block management operations."
        },
        "possible_fix_code": {
            "BlockManager.computeReplicationWorkForBlocks": "  int computeReplicationWorkForBlocks(List<List<Block>> blocksToReplicate) {\n    int requiredReplication, numEffectiveReplicas;\n    List<DatanodeDescriptor> containingNodes, liveReplicaNodes;\n    DatanodeDescriptor srcNode;\n    BlockCollection bc = null;\n    int additionalReplRequired;\n\n    int scheduledWork = 0;\n    List<ReplicationWork> work = new LinkedList<ReplicationWork>();\n\n    namesystem.writeLock();\n    try {\n      synchronized (neededReplications) {\n        for (int priority = 0; priority < blocksToReplicate.size(); priority++) {\n          for (Block block : blocksToReplicate.get(priority)) {\n            // block should belong to a file\n            bc = blocksMap.getBlockCollection(block);\n            // abandoned block or block reopened for append\n            if(bc == null || bc instanceof MutableBlockCollection) {\n              neededReplications.remove(block, priority); // remove from neededReplications\n              neededReplications.decrementReplicationIndex(priority);\n              continue;\n            }\n\n            requiredReplication = bc.getReplication();\n\n            // get a source data-node\n            containingNodes = new ArrayList<DatanodeDescriptor>();\n            liveReplicaNodes = new ArrayList<DatanodeDescriptor>();\n            NumberReplicas numReplicas = new NumberReplicas();\n            srcNode = chooseSourceDatanode(\n                block, containingNodes, liveReplicaNodes, numReplicas);\n            if(srcNode == null) { // block can not be replicated from any node\n              LOG.debug(\"Block \" + block + \" cannot be repl from any node\");\n              continue;\n            }\n\n            assert liveReplicaNodes.size() == numReplicas.liveReplicas();\n            // do not schedule more if enough replicas is already pending\n            numEffectiveReplicas = numReplicas.liveReplicas() +\n                                    pendingReplications.getNumReplicas(block);\n\n            if (numEffectiveReplicas >= requiredReplication) {\n              if ( (pendingReplications.getNumReplicas(block) > 0) ||\n                   (blockHasEnoughRacks(block)) ) {\n                neededReplications.remove(block, priority); // remove from neededReplications\n                neededReplications.decrementReplicationIndex(priority);\n                NameNode.stateChangeLog.info(\"BLOCK* \"\n                    + \"Removing block \" + block\n                    + \" from neededReplications as it has enough replicas.\");\n                continue;\n              }\n            }\n\n            if (numReplicas.liveReplicas() < requiredReplication) {\n              additionalReplRequired = requiredReplication\n                  - numEffectiveReplicas;\n            } else {\n              additionalReplRequired = 1; // Needed on a new rack\n            }\n            work.add(new ReplicationWork(block, bc, srcNode,\n                containingNodes, liveReplicaNodes, additionalReplRequired,\n                priority));\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    HashMap<Node, Node> excludedNodes\n        = new HashMap<Node, Node>();\n    for(ReplicationWork rw : work){\n      // Exclude all of the containing nodes from being targets.\n      // This list includes decommissioning or corrupt nodes.\n      excludedNodes.clear();\n      for (DatanodeDescriptor dn : rw.containingNodes) {\n        excludedNodes.put(dn, dn);\n      }\n\n      // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n      // It is costly to extract the filename for which chooseTargets is called,\n      // so for now we pass in the block collection itself.\n      rw.targets = blockplacement.chooseTarget(rw.bc,\n          rw.additionalReplRequired, rw.srcNode, rw.liveReplicaNodes,\n          excludedNodes, rw.block.getNumBytes());\n    }\n\n    namesystem.writeLock();\n    try {\n      for(ReplicationWork rw : work){\n        DatanodeDescriptor[] targets = rw.targets;\n        if(targets == null || targets.length == 0){\n          rw.targets = null;\n          continue;\n        }\n\n        synchronized (neededReplications) {\n          Block block = rw.block;\n          int priority = rw.priority;\n          // Recheck since global lock was released\n          // block should belong to a file\n          bc = blocksMap.getBlockCollection(block);\n          // abandoned block or block reopened for append\n          if(bc == null || bc instanceof MutableBlockCollection) {\n            neededReplications.remove(block, priority); // remove from neededReplications\n            rw.targets = null;\n            neededReplications.decrementReplicationIndex(priority);\n            continue;\n          }\n          requiredReplication = bc.getReplication();\n\n          // do not schedule more if enough replicas is already pending\n          NumberReplicas numReplicas = countNodes(block);\n          numEffectiveReplicas = numReplicas.liveReplicas() +\n            pendingReplications.getNumReplicas(block);\n\n          if (numEffectiveReplicas >= requiredReplication) {\n            if ( (pendingReplications.getNumReplicas(block) > 0) ||\n                 (blockHasEnoughRacks(block)) ) {\n              neededReplications.remove(block, priority); // remove from neededReplications\n              neededReplications.decrementReplicationIndex(priority);\n              rw.targets = null;\n              NameNode.stateChangeLog.info(\"BLOCK* \"\n                  + \"Removing block \" + block\n                  + \" from neededReplications as it has enough replicas.\");\n              continue;\n            }\n          }\n\n          if ( (numReplicas.liveReplicas() >= requiredReplication) &&\n               (!blockHasEnoughRacks(block)) ) {\n            if (rw.srcNode.getNetworkLocation().equals(targets[0].getNetworkLocation())) {\n              //No use continuing, unless a new rack in this case\n              continue;\n            }\n          }\n\n          // Add block to the to be replicated list\n          rw.srcNode.addBlockToBeReplicated(block, targets);\n          scheduledWork++;\n\n          for (DatanodeDescriptor dn : targets) {\n            dn.incBlocksScheduled();\n          }\n\n          // Move the block-replication into a \"pending\" state.\n          // The reason we use 'pending' is so we can retry\n          // replications that fail after an appropriate amount of time.\n          pendingReplications.add(block, targets.length);\n          if(NameNode.stateChangeLog.isDebugEnabled()) {\n            NameNode.stateChangeLog.debug(\n                \"BLOCK* block \" + block\n                + \" is moved from neededReplications to pendingReplications\");\n          }\n\n          // remove from neededReplications\n          if(numEffectiveReplicas + targets.length >= requiredReplication) {\n            neededReplications.remove(block, priority); // remove from neededReplications\n            neededReplications.decrementReplicationIndex(priority);\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    if (NameNode.stateChangeLog.isInfoEnabled()) {\n      // log which blocks have been scheduled for replication\n      for(ReplicationWork rw : work){\n        DatanodeDescriptor[] targets = rw.targets;\n        if (targets != null && targets.length != 0) {\n          StringBuilder targetList = new StringBuilder(\"datanode(s)\");\n          for (int k = 0; k < targets.length; k++) {\n            targetList.append(' ');\n            targetList.append(targets[k]);\n          }\n          NameNode.stateChangeLog.info(\n                  \"BLOCK* ask \"\n                  + rw.srcNode + \" to replicate \"\n                  + rw.block + \" to \" + targetList);\n        }\n      }\n    }\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n        NameNode.stateChangeLog.debug(\n          \"BLOCK* neededReplications = \" + neededReplications.size()\n          + \" pendingReplications = \" + pendingReplications.size());\n    }\n\n    return scheduledWork;\n  }"
        }
    },
    {
        "filename": "HDFS-6348.json",
        "creation_time": "2014-05-07T12:34:54.000+0000",
        "bug_report": {
            "Title": "SecondaryNameNode not terminating properly on runtime exceptions",
            "Description": "The Secondary NameNode fails to terminate when a RuntimeException occurs during startup due to a missing class in the classpath. Specifically, the class `com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy` is not found, leading to a `ClassNotFoundException`. This prevents the JVM from exiting as the RMI thread remains alive, which is not a daemon thread. The attached thread dump provides further insights into the state of the application during this failure.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found",
                "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1900)",
                "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.getInstance(BlockPlacementPolicy.java:199)",
                "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:256)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:635)",
                "\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:260)",
                "\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init>(SecondaryNameNode.java:205)",
                "\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:695)",
                "Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found",
                "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1868)",
                "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1892)",
                "\t... 6 more",
                "Caused by: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found",
                "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1774)",
                "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1866)",
                "\t... 7 more"
            ],
            "RootCause": "The root cause of the issue is a ClassNotFoundException for the class `com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy`, which indicates that this class is not available in the classpath during the initialization of the Secondary NameNode.",
            "StepsToReproduce": [
                "Configure the Secondary NameNode with an incorrect configuration that leads to a RuntimeException.",
                "Start the Secondary NameNode.",
                "Observe that the process does not terminate despite the RuntimeException being thrown."
            ],
            "ExpectedBehavior": "The Secondary NameNode should terminate gracefully upon encountering a RuntimeException during startup.",
            "ObservedBehavior": "The Secondary NameNode process remains alive, and the JVM does not exit due to the RMI thread being active.",
            "Suggestions": "To resolve the ClassNotFoundException, check the project's build configuration (e.g., pom.xml for Maven or build.gradle for Gradle) to ensure that the necessary dependencies for `com.huawei.hadoop.hdfs` are included. Verify the classpath setup and rebuild the project after making any changes.",
            "problem_location": {
                "files": [
                    "SecondaryNameNode.java",
                    "Configuration.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode",
                    "org.apache.hadoop.conf.Configuration"
                ],
                "methods": [
                    "SecondaryNameNode.initialize",
                    "Configuration.getClass"
                ]
            },
            "possible_fix": "Add the missing dependency for `com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy` in the project's build configuration. Ensure that the classpath is correctly set and rebuild the project."
        },
        "possible_fix_code": {
            "SecondaryNameNode.initialize": "  private void initialize(final Configuration conf,\n      CommandLineOpts commandLineOpts) throws IOException {\n    final InetSocketAddress infoSocAddr = getHttpAddress(conf);\n    final String infoBindAddress = infoSocAddr.getHostName();\n    UserGroupInformation.setConfiguration(conf);\n    if (UserGroupInformation.isSecurityEnabled()) {\n      SecurityUtil.login(conf,\n          DFSConfigKeys.DFS_SECONDARY_NAMENODE_KEYTAB_FILE_KEY,\n          DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY, infoBindAddress);\n    }\n    // initiate Java VM metrics\n    DefaultMetricsSystem.initialize(\"SecondaryNameNode\");\n    JvmMetrics.create(\"SecondaryNameNode\",\n        conf.get(DFSConfigKeys.DFS_METRICS_SESSION_ID_KEY),\n        DefaultMetricsSystem.instance());\n\n    // Create connection to the namenode.\n    shouldRun = true;\n    nameNodeAddr = NameNode.getServiceAddress(conf, true);\n\n    this.conf = conf;\n    this.namenode = NameNodeProxies.createNonHAProxy(conf, nameNodeAddr, \n        NamenodeProtocol.class, UserGroupInformation.getCurrentUser(),\n        true).getProxy();\n\n    // initialize checkpoint directories\n    fsName = getInfoServer();\n    checkpointDirs = FSImage.getCheckpointDirs(conf,\n                                  \"/tmp/hadoop/dfs/namesecondary\");\n    checkpointEditsDirs = FSImage.getCheckpointEditsDirs(conf,\n                                  \"/tmp/hadoop/dfs/namesecondary\");    \n    checkpointImage = new CheckpointStorage(conf, checkpointDirs, checkpointEditsDirs);\n    checkpointImage.recoverCreate(commandLineOpts.shouldFormat());\n    checkpointImage.deleteTempEdits();\n    \n    namesystem = new FSNamesystem(conf, checkpointImage, true);\n\n    // Disable quota checks\n    namesystem.dir.disableQuotaChecks();\n\n    // Initialize other scheduling parameters from the configuration\n    checkpointConf = new CheckpointConf(conf);\n\n    final InetSocketAddress httpAddr = infoSocAddr;\n\n    final String httpsAddrString = conf.get(\n        DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTPS_ADDRESS_KEY,\n        DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTPS_ADDRESS_DEFAULT);\n    InetSocketAddress httpsAddr = NetUtils.createSocketAddr(httpsAddrString);\n\n    HttpServer2.Builder builder = DFSUtil.httpServerTemplateForNNAndJN(conf,\n        httpAddr, httpsAddr, \"secondary\",\n        DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_INTERNAL_SPNEGO_PRINCIPAL_KEY,\n        DFSConfigKeys.DFS_SECONDARY_NAMENODE_KEYTAB_FILE_KEY);\n\n    nameNodeStatusBeanName = MBeans.register(\"SecondaryNameNode\",\n            \"SecondaryNameNodeInfo\", this);\n\n    infoServer = builder.build();\n\n    infoServer.setAttribute(\"secondary.name.node\", this);\n    infoServer.setAttribute(\"name.system.image\", checkpointImage);\n    infoServer.setAttribute(JspHelper.CURRENT_CONF, conf);\n    infoServer.addInternalServlet(\"imagetransfer\", ImageServlet.PATH_SPEC,\n        ImageServlet.class, true);\n    infoServer.start();\n\n    LOG.info(\"Web server init done\");\n\n    HttpConfig.Policy policy = DFSUtil.getHttpPolicy(conf);\n    int connIdx = 0;\n    if (policy.isHttpEnabled()) {\n      InetSocketAddress httpAddress = infoServer.getConnectorAddress(connIdx++);\n      conf.set(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,\n          NetUtils.getHostPortString(httpAddress));\n    }\n\n    if (policy.isHttpsEnabled()) {\n      InetSocketAddress httpsAddress = infoServer.getConnectorAddress(connIdx);\n      conf.set(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTPS_ADDRESS_KEY,\n          NetUtils.getHostPortString(httpsAddress));\n    }\n\n    LOG.info(\"Checkpoint Period   :\" + checkpointConf.getPeriod() + \" secs \"\n        + \"(\" + checkpointConf.getPeriod() / 60 + \" min)\");\n    LOG.info(\"Log Size Trigger    :\" + checkpointConf.getTxnCount() + \" txns\");\n\n    // Check for the required class and terminate if not found\n    try {\n        Class.forName(\"com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy\");\n    } catch (ClassNotFoundException e) {\n        LOG.fatal(\"Required class not found, terminating: \" + e.getMessage());\n        terminate(1);\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-7884.json",
        "creation_time": "2015-03-04T15:47:50.000+0000",
        "bug_report": {
            "Title": "NullPointerException in BlockSender",
            "Description": "A `NullPointerException` is occurring in the `BlockSender` class of the Hadoop HDFS implementation. The exception is thrown at line 264 of the `BlockSender` constructor, specifically when attempting to obtain a volume reference from the `datanode.data` object. This suggests that either the `datanode` or the `data` field is null, leading to the failure when trying to call `getVolume(block)`.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:264)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:506)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:249)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The `NullPointerException` is likely caused by dereferencing a null object in the `BlockSender` constructor, specifically when trying to access the `datanode.data` object to call `getVolume(block)`. This indicates that either `datanode` or `data` is not properly initialized.",
            "StepsToReproduce": [
                "Attempt to read a block using the `DataXceiver` class.",
                "Ensure that the `datanode` object is not initialized or is in an invalid state."
            ],
            "ExpectedBehavior": "The system should successfully read the block without throwing a `NullPointerException`, and the `BlockSender` should be initialized correctly with a valid volume reference.",
            "ObservedBehavior": "A `NullPointerException` is thrown during the initialization of the `BlockSender`, preventing the block from being read.",
            "Suggestions": "Check the initialization of the `datanode` and its `data` field to ensure they are not null before invoking methods on them. Implement null checks to prevent dereferencing null objects.",
            "problem_location": {
                "files": [
                    "BlockSender.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.BlockSender"
                ],
                "methods": [
                    "BlockSender.init"
                ]
            },
            "possible_fix": "Add null checks in the `BlockSender` constructor to ensure that `datanode` and `datanode.data` are not null before calling `getVolume(block)`. For example:\n\nif (datanode == null || datanode.data == null) {\n    throw new IllegalStateException(\"Datanode or data is not initialized.\");\n}\nthis.volumeRef = datanode.data.getVolume(block).obtainReference();"
        },
        "possible_fix_code": {
            "org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>": "public BlockSender(ExtendedBlock block, long offset, long length, boolean sendChecksum, boolean dropCacheBehindAllReads, boolean dropCacheBehindLargeReads, DataNode datanode, String clientTraceFmt, CachingStrategy cachingStrategy) throws IOException {\n    this.block = block;\n    this.offset = offset;\n    this.length = length;\n    this.sendChecksum = sendChecksum;\n    this.dropCacheBehindAllReads = dropCacheBehindAllReads;\n    this.dropCacheBehindLargeReads = dropCacheBehindLargeReads;\n    this.datanode = datanode;\n    this.clientTraceFmt = clientTraceFmt;\n    this.cachingStrategy = cachingStrategy;\n\n    // Add null checks to prevent NullPointerException\n    if (datanode == null || datanode.data == null) {\n        throw new IllegalStateException(\"Datanode or data is not initialized.\");\n    }\n\n    this.volumeRef = datanode.data.getVolume(block).obtainReference();\n}"
        }
    },
    {
        "filename": "HDFS-7996.json",
        "creation_time": "2015-03-26T21:19:55.000+0000",
        "bug_report": {
            "Title": "After swapping a volume, BlockReceiver reports ReplicaNotFoundException",
            "Description": "When a disk is removed from an actively writing DataNode in HDFS, the BlockReceiver encounters a ReplicaNotFoundException. This occurs because the replicas are removed from memory while the BlockReceiver is still attempting to finalize a block. The removal of the volume leads to the BlockReceiver trying to append data to a non-existent replica, resulting in the exception being thrown. The logs indicate that the volume was removed and the block-level storage was also deleted, which directly correlates with the exception thrown by the BlockReceiver.",
            "StackTrace": [
                "2015-03-26 08:02:43,154 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Removed volume: /data/2/dfs/dn/current",
                "2015-03-26 08:02:43,163 INFO org.apache.hadoop.hdfs.server.common.Storage: Removing block level storage: /data/2/dfs/dn/current/BP-51301509-10.20.202.114-1427296597742",
                "2015-03-26 08:02:43,163 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in BlockReceiver.run():",
                "org.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Cannot append to a non-existent replica BP-51301509-10.20.202.114-1427296597742:blk_1073742979_2160",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo(FsDatasetImpl.java:615)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.finalizeBlock(FsDatasetImpl.java:1362)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.finalizeBlock(BlockReceiver.java:1281)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1241)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the ReplicaNotFoundException is the premature removal of the volume from the DataNode while the BlockReceiver is still processing an append operation. This leads to the BlockReceiver attempting to finalize a block for a replica that no longer exists in memory.",
            "StepsToReproduce": [
                "1. Start a DataNode and begin writing data to a disk.",
                "2. While the DataNode is actively writing, remove the disk from the DataNode.",
                "3. Observe the logs for the ReplicaNotFoundException being thrown by the BlockReceiver."
            ],
            "ExpectedBehavior": "The system should handle the removal of a volume gracefully, ensuring that no append operations are attempted on non-existent replicas, and should not throw a ReplicaNotFoundException.",
            "ObservedBehavior": "The BlockReceiver throws a ReplicaNotFoundException when it attempts to append data to a replica that has been removed from memory due to the volume being deleted.",
            "Suggestions": "To mitigate this issue, ensure that all append operations are completed before removing a volume from the DataNode. Implement checks to prevent volume removal while active write operations are ongoing.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl",
                    "org.apache.hadoop.hdfs.server.datanode.BlockReceiver"
                ],
                "methods": [
                    "FsDatasetImpl.getReplicaInfo",
                    "FsDatasetImpl.finalizeBlock",
                    "BlockReceiver.PacketResponder.finalizeBlock"
                ]
            },
            "possible_fix": "Ensure that the volume removal process waits for all active operations to complete. This can be achieved by implementing a locking mechanism or a state check before allowing volume removal."
        },
        "possible_fix_code": {
            "FsDatasetImpl.finalizeBlock": "  public synchronized void finalizeBlock(ExtendedBlock b) throws IOException {\n    if (Thread.interrupted()) {\n      // Don't allow data modifications from interrupted threads\n      throw new IOException(\"Cannot finalize block from Interrupted Thread\");\n    }\n    // Check if there are any active write operations before finalizing the block\n    if (isVolumeActive(b.getBlockPoolId())) {\n      throw new IOException(\"Cannot finalize block while volume is active for writes\");\n    }\n    ReplicaInfo replicaInfo = getReplicaInfo(b);\n    if (replicaInfo.getState() == ReplicaState.FINALIZED) {\n      // this is legal, when recovery happens on a file that has\n      // been opened for append but never modified\n      return;\n    }\n    finalizeReplica(b.getBlockPoolId(), replicaInfo);\n  }"
        }
    },
    {
        "filename": "HDFS-4302.json",
        "creation_time": "2012-12-11T19:44:59.000+0000",
        "bug_report": {
            "Title": "Precondition in EditLogFileInputStream's length() method is checked too early in NameNode startup, causing fatal exception",
            "Description": "During the startup of a NameNode in standby mode with DEBUG logging enabled, an exception is triggered due to the premature access of the length of an EditLogFileInputStream. The issue arises when the debug log attempts to log the length of the input stream before the stream's advertised size is initialized. This occurs in the FSEditLogLoader class, specifically when the length() method of EditLogFileInputStream is called, leading to an IllegalStateException. The root cause is that the input stream is not yet ready for length access, resulting in a fatal shutdown of the NameNode.",
            "StackTrace": [
                "2012-12-11 10:45:33,464 FATAL namenode.NameNode (NameNode.java:main(1224)) - Exception in namenode join",
                "java.lang.IllegalStateException: must get input stream before length is available",
                "at com.google.common.base.Preconditions.checkState(Preconditions.java:145)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$URLLog.length(EditLogFileInputStream.java:405)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.length(EditLogFileInputStream.java:258)",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:125)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:88)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:697)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:604)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:447)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:409)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:400)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:434)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:606)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:591)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1153)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1219)"
            ],
            "RootCause": "The root cause of the issue is that the EditLogFileInputStream's advertisedSize is not initialized before its length() method is called during the NameNode startup process. This leads to an IllegalStateException being thrown when the precondition check fails.",
            "StepsToReproduce": [
                "1. Start the NameNode in standby mode.",
                "2. Enable DEBUG logging for the NameNode.",
                "3. Observe the logs for the IllegalStateException related to EditLogFileInputStream's length() method."
            ],
            "ExpectedBehavior": "The NameNode should start successfully without throwing any exceptions, and the length of the EditLogFileInputStream should be accessible after it has been properly initialized.",
            "ObservedBehavior": "The NameNode fails to start and throws an IllegalStateException indicating that the input stream must be obtained before its length can be accessed.",
            "Suggestions": "Ensure that the EditLogFileInputStream is fully initialized before any calls to its length() method are made. This may involve restructuring the initialization sequence in the FSEditLogLoader or related classes.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileInputStream.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader",
                    "org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream",
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem",
                    "org.apache.hadoop.hdfs.server.namenode.NameNode"
                ],
                "methods": [
                    "FSEditLogLoader.loadEditRecords",
                    "EditLogFileInputStream.length",
                    "FSNamesystem.loadFromDisk",
                    "NameNode.initialize"
                ]
            },
            "possible_fix": "Modify the FSEditLogLoader to ensure that the EditLogFileInputStream is fully initialized before any calls to its length() method. This may involve adding checks or restructuring the order of operations during the NameNode startup process."
        },
        "possible_fix_code": {
            "NameNode.initialize": "  protected void initialize(Configuration conf) throws IOException {\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    loadNamesystem(conf);\n\n    rpcServer = createRpcServer(conf);\n    \n    try {\n      validateConfigurationSettings(conf);\n    } catch (IOException e) {\n      LOG.fatal(e.toString());\n      throw e;\n    }\n\n    startCommonServices(conf);\n  }",
            "FSNamesystem.loadFromDisk": "  public static FSNamesystem loadFromDisk(Configuration conf,\n      Collection<URI> namespaceDirs, List<URI> namespaceEditsDirs)\n      throws IOException {\n\n    if (namespaceDirs.size() == 1) {\n      LOG.warn(\"Only one image storage directory (\"\n          + DFS_NAMENODE_NAME_DIR_KEY + \") configured. Beware of dataloss\"\n          + \" due to lack of redundant storage directories!\");\n    }\n    if (namespaceEditsDirs.size() == 1) {\n      LOG.warn(\"Only one namespace edits storage directory (\"\n          + DFS_NAMENODE_EDITS_DIR_KEY + \") configured. Beware of dataloss\"\n          + \" due to lack of redundant storage directories!\");\n    }\n\n    FSImage fsImage = new FSImage(conf, namespaceDirs, namespaceEditsDirs);\n    FSNamesystem namesystem = new FSNamesystem(conf, fsImage);\n    StartupOption startOpt = NameNode.getStartupOption(conf);\n    if (startOpt == StartupOption.RECOVER) {\n      namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n    }\n\n    long loadStart = now();\n    String nameserviceId = DFSUtil.getNamenodeNameServiceId(conf);\n    namesystem.loadFSImage(startOpt, fsImage,\n      HAUtil.isHAEnabled(conf, nameserviceId));\n    long timeTakenToLoadFSImage = now() - loadStart;\n    LOG.info(\"Finished loading FSImage in \" + timeTakenToLoadFSImage + \" msecs\");\n    NameNodeMetrics nnMetrics = NameNode.getNameNodeMetrics();\n    if (nnMetrics != null) {\n      nnMetrics.setFsImageLoadTime((int) timeTakenToLoadFSImage);\n    }\n    return namesystem;\n  }",
            "FSImage.loadFSImage": "  private void loadFSImage(File curFile, MD5Hash expectedMd5,\n      FSNamesystem target, MetaRecoveryContext recovery) throws IOException {\n    FSImageFormat.Loader loader = new FSImageFormat.Loader(\n        conf, target);\n    loader.load(curFile);\n    target.setBlockPoolId(this.getBlockPoolID());\n\n    // Check that the image digest we loaded matches up with what\n    // we expected\n    MD5Hash readImageMd5 = loader.getLoadedImageMd5();\n    if (expectedMd5 != null &&\n        !expectedMd5.equals(readImageMd5)) {\n      throw new IOException(\"Image file \" + curFile +\n          \" is corrupt with MD5 checksum of \" + readImageMd5 +\n          \" but expecting \" + expectedMd5);\n    }\n\n    long txId = loader.getLoadedImageTxId();\n    LOG.info(\"Loaded image for txid \" + txId + \" from \" + curFile);\n    lastAppliedTxId = txId;\n    storage.setMostRecentCheckpointInfo(txId, curFile.lastModified());\n  }"
        }
    },
    {
        "filename": "HDFS-11849.json",
        "creation_time": "2017-05-18T11:31:48.000+0000",
        "bug_report": {
            "Title": "JournalNode startup failure exception should be logged in log file",
            "Description": "The JournalNode fails to start due to a Kerberos login failure when attempting to authenticate using a keytab file. The exception indicates that the login attempt for the principal 'xxx/yyyy@ZZZZ.COM' from the keytab 'dummy.keytab' has failed, which is critical for the secure operation of the Hadoop environment. The stack trace shows that the failure occurs in the 'UserGroupInformation.loginUserFromKeytab' method, which is responsible for handling the login process. Notably, this exception is not being logged in the log file, which is a significant oversight.",
            "StackTrace": [
                "Exception in thread \"main\" java.io.IOException: Login failure for xxx/yyyy@ZZZZ.COM from keytab dummy.keytab: javax.security.auth.login.LoginException: host1",
                "at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:994)",
                "at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:281)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNode.start(JournalNode.java:153)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNode.run(JournalNode.java:132)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNode.main(JournalNode.java:318)"
            ],
            "RootCause": "The root cause of the issue is a login failure due to potential problems with the keytab file, such as incorrect permissions, an invalid principal, or misconfigurations in the Kerberos setup.",
            "StepsToReproduce": [
                "Attempt to start the JournalNode with the specified keytab file and principal.",
                "Ensure that the keytab file is accessible and has the correct permissions.",
                "Verify that the principal is valid and correctly configured in the Kerberos settings."
            ],
            "ExpectedBehavior": "The JournalNode should start successfully without any exceptions, and any login failures should be logged appropriately in the log file.",
            "ObservedBehavior": "The JournalNode fails to start due to a login exception, and this exception is not recorded in the log file.",
            "Suggestions": "Check the permissions of the keytab file, ensure the principal is valid, and verify the Kerberos configuration. Additionally, implement logging for exceptions in the JournalNode startup process to capture such failures in the future.",
            "problem_location": {
                "files": [
                    "UserGroupInformation.java",
                    "SecurityUtil.java",
                    "JournalNode.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.UserGroupInformation",
                    "org.apache.hadoop.security.SecurityUtil",
                    "org.apache.hadoop.hdfs.qjournal.server.JournalNode"
                ],
                "methods": [
                    "UserGroupInformation.loginUserFromKeytab",
                    "SecurityUtil.login",
                    "JournalNode.start"
                ]
            },
            "possible_fix": "Ensure that the keytab file has the correct permissions and that the principal is valid. Additionally, modify the JournalNode class to log exceptions during the startup process to ensure that any future login failures are captured in the log file."
        },
        "possible_fix_code": {
            "UserGroupInformation.loginUserFromKeytab": "  static void loginUserFromKeytab(String user,\n                                  String path\n                                  ) throws IOException {\n    if (!isSecurityEnabled())\n      return;\n\n    keytabFile = path;\n    keytabPrincipal = user;\n    Subject subject = new Subject();\n    LoginContext login; \n    long start = 0;\n    try {\n      login = newLoginContext(HadoopConfiguration.KEYTAB_KERBEROS_CONFIG_NAME,\n            subject, new HadoopConfiguration());\n      start = Time.now();\n      login.login();\n      metrics.loginSuccess.add(Time.now() - start);\n      loginUser = new UserGroupInformation(subject, false);\n      loginUser.setLogin(login);\n      loginUser.setAuthenticationMethod(AuthenticationMethod.KERBEROS);\n    } catch (LoginException le) {\n      if (start > 0) {\n        metrics.loginFailure.add(Time.now() - start);\n      }\n      KerberosAuthException kae = new KerberosAuthException(LOGIN_FAILURE, le);\n      kae.setUser(user);\n      kae.setKeytabFile(path);\n      LOG.error(\"Login failed for user \" + user + \" using keytab file \" + path + \": \" + le.getMessage());\n      throw kae;\n    }\n    LOG.info(\"Login successful for user \" + keytabPrincipal\n        + \" using keytab file \" + keytabFile);\n  }"
        }
    },
    {
        "filename": "HDFS-4841.json",
        "creation_time": "2013-05-22T17:01:36.000+0000",
        "bug_report": {
            "Title": "FsShell commands using secure webhdfs fail ClientFinalizer shutdown hook",
            "Description": "In Hadoop version 3.0.0-SNAPSHOT, an issue arises when executing FsShell commands with the webhdfs:// URI while security is enabled. The command completes successfully but triggers a warning indicating that the ShutdownHook 'ClientFinalizer' has failed. This is accompanied by an IllegalStateException stating that a shutdown is already in progress, preventing the addition of new shutdown hooks. The problem does not occur when security is disabled, suggesting a conflict during the shutdown process when security features are active.",
            "StackTrace": [
                "2013-05-22 09:46:58,660 WARN  [Thread-3] util.ShutdownHookManager (ShutdownHookManager.java:run(56)) - ShutdownHook 'ClientFinalizer' failed, java.lang.IllegalStateException: Shutdown in progress, cannot add a shutdownHook",
                "java.lang.IllegalStateException: Shutdown in progress, cannot add a shutdownHook",
                "at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:152)",
                "at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2400)",
                "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2372)",
                "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:352)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$DtRenewer.getWebHdfs(WebHdfsFileSystem.java:1001)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$DtRenewer.cancel(WebHdfsFileSystem.java:1013)",
                "at org.apache.hadoop.security.token.Token.cancel(Token.java:382)",
                "at org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction.cancel(DelegationTokenRenewer.java:152)",
                "at org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction.access$200(DelegationTokenRenewer.java:58)",
                "at org.apache.hadoop.fs.DelegationTokenRenewer.removeRenewAction(DelegationTokenRenewer.java:241)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.close(WebHdfsFileSystem.java:822)",
                "at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2446)",
                "at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2463)",
                "at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)"
            ],
            "RootCause": "The root cause of the issue is an IllegalStateException thrown by the ShutdownHookManager when an attempt is made to add a new shutdown hook while the system is already in the process of shutting down. This is indicated by the shutdownInProgress flag being set to true.",
            "StepsToReproduce": [
                "Enable security in Hadoop configuration.",
                "Run the command: hadoop fs -ls webhdfs://hdfs-upgrade-pseudo.ent.cloudera.com:50070/",
                "Observe the warning about the ShutdownHook 'ClientFinalizer' failure."
            ],
            "ExpectedBehavior": "The command should execute without warnings, and the ShutdownHook 'ClientFinalizer' should complete successfully.",
            "ObservedBehavior": "The command completes but triggers a warning about the ShutdownHook 'ClientFinalizer' failing due to an IllegalStateException.",
            "Suggestions": "Investigate the shutdown process in the Hadoop framework to ensure that shutdown hooks are managed correctly during security-enabled operations. Consider reviewing the timing of shutdown hook registrations and the conditions under which they are added.",
            "problem_location": {
                "files": [
                    "ShutdownHookManager.java",
                    "FileSystem.java",
                    "WebHdfsFileSystem.java",
                    "Token.java",
                    "DelegationTokenRenewer.java"
                ],
                "classes": [
                    "org.apache.hadoop.util.ShutdownHookManager",
                    "org.apache.hadoop.fs.FileSystem",
                    "org.apache.hadoop.hdfs.web.WebHdfsFileSystem",
                    "org.apache.hadoop.security.token.Token",
                    "org.apache.hadoop.fs.DelegationTokenRenewer"
                ],
                "methods": [
                    "ShutdownHookManager.addShutdownHook",
                    "FileSystem$Cache.getInternal",
                    "FileSystem.get",
                    "WebHdfsFileSystem$DtRenewer.getWebHdfs",
                    "Token.cancel",
                    "DelegationTokenRenewer.removeRenewAction"
                ]
            },
            "possible_fix": "Review the implementation of the shutdown process in the ShutdownHookManager to ensure that it correctly handles the addition of shutdown hooks during shutdown. Consider adding checks or modifying the order of operations to prevent this exception from occurring."
        },
        "possible_fix_code": {
            "org.apache.hadoop.util.ShutdownHookManager.addShutdownHook": "  public void addShutdownHook(Runnable shutdownHook, int priority) {\n    if (shutdownHook == null) {\n      throw new IllegalArgumentException(\"shutdownHook cannot be NULL\");\n    }\n    // Check if shutdown is in progress before adding a new hook\n    if (shutdownInProgress.get()) {\n      // Log a warning instead of throwing an exception\n      LOG.warn(\"Attempted to add a shutdown hook while shutdown is in progress.\");\n      return; // Prevent adding the hook\n    }\n    hooks.add(new HookEntry(shutdownHook, priority));\n  }"
        }
    },
    {
        "filename": "HDFS-3384.json",
        "creation_time": "2012-05-08T09:01:10.000+0000",
        "bug_report": {
            "Title": "DataStreamer thread should be closed immediately when failed to setup a PipelineForAppendOrRecovery",
            "Description": "The issue arises when attempting to write a file to HDFS after manually corrupting a block. The stack trace indicates that the DataStreamer encounters an EOFException, followed by a NullPointerException and an IOException, suggesting that the DataStreamer is not handling the failure to set up a pipeline correctly. This leads to the DataStreamer thread not being closed as expected, which can cause resource leaks and further errors in the system.",
            "StackTrace": [
                "2012-04-19 09:33:10,776 INFO  hdfs.DFSClient (DFSOutputStream.java:createBlockOutputStream(1059)) - Exception in createBlockOutputStream",
                "java.io.EOFException: Premature EOF: no length prefix available",
                "\tat org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)",
                "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1039)",
                "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:939)",
                "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)",
                "2012-04-19 09:33:10,807 WARN  hdfs.DFSClient (DFSOutputStream.java:run(549)) - DataStreamer Exception",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:510)",
                "2012-04-19 09:33:10,807 WARN  hdfs.DFSClient (DFSOutputStream.java:hflush(1511)) - Error while syncing",
                "java.io.IOException: All datanodes 10.18.40.20:50010 are bad. Aborting...",
                "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:908)",
                "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)",
                "java.io.IOException: All datanodes 10.18.40.20:50010 are bad. Aborting...",
                "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:908)",
                "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)"
            ],
            "RootCause": "The root cause of the issue is that the DataStreamer does not properly handle the EOFException and subsequent NullPointerException when attempting to set up a pipeline for appending or recovering data. This results in the DataStreamer thread remaining active despite the failure, leading to resource management issues.",
            "StepsToReproduce": [
                "1. Write a file to HDFS.",
                "2. Manually corrupt a block in the HDFS.",
                "3. Call the append method on the file."
            ],
            "ExpectedBehavior": "The DataStreamer should close immediately upon failing to set up a pipeline for append or recovery, releasing any resources and preventing further errors.",
            "ObservedBehavior": "The DataStreamer thread remains open and does not close, leading to additional exceptions and potential resource leaks.",
            "Suggestions": "Implement proper exception handling in the DataStreamer class to ensure that the thread is closed immediately when an error occurs during pipeline setup. This may involve checking for null references and ensuring that resources are released appropriately.",
            "problem_location": {
                "files": [
                    "DFSOutputStream.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.DFSOutputStream",
                    "org.apache.hadoop.hdfs.DFSOutputStream.DataStreamer"
                ],
                "methods": [
                    "DFSOutputStream.DataStreamer.setupPipelineForAppendOrRecovery",
                    "DFSOutputStream.DataStreamer.run"
                ]
            },
            "possible_fix": "In the setupPipelineForAppendOrRecovery method, add checks to handle EOFException and NullPointerException gracefully, ensuring that the DataStreamer thread is terminated properly in case of failure."
        },
        "possible_fix_code": {
            "DFSOutputStream.DataStreamer.setupPipelineForAppendOrRecovery": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes == null || nodes.length == 0) {\n        String msg = \"Could not get block locations. \" + \"Source file \\\"\" + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed = true;\n        return false;\n      }\n      \n      boolean success = false;\n      long newGS = 0L;\n      while (!success && !streamerClosed && dfsClient.clientRunning) {\n        boolean isRecovery = hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex >= 0) {\n          StringBuilder pipelineMsg = new StringBuilder();\n          for (int j = 0; j < nodes.length; j++) {\n            pipelineMsg.append(nodes[j]);\n            if (j < nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length <= 1) {\n            lastException = new IOException(\"All datanodes \" + pipelineMsg + \" are bad. Aborting...\");\n            streamerClosed = true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block + \" in pipeline \" + pipelineMsg + \": bad datanode \" + nodes[errorIndex]);\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes = new DatanodeInfo[nodes.length-1];\n          System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n          System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex, newnodes.length-errorIndex);\n          nodes = newnodes;\n          hasError = false;\n          lastException = null;\n          errorIndex = -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication, nodes, isAppend, isHflushed)) {\n          addDatanode2ExistingPipeline();\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb = dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS = lb.getBlock().getGenerationStamp();\n        accessToken = lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        success = createBlockOutputStream(nodes, newGS, isRecovery);\n      }\n\n      if (!success) {\n        // Handle failure to set up pipeline\n        closeStream();\n        streamerClosed = true;\n      }\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock = new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock, nodes);\n        // update client side generation stamp\n        block = newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }"
        }
    },
    {
        "filename": "HDFS-5657.json",
        "creation_time": "2013-12-11T21:59:46.000+0000",
        "bug_report": {
            "Title": "Race condition causes writeback state error in NFS gateway",
            "Description": "A race condition between the NFS gateway writeback executor thread and the new write handler thread can lead to a failure in the writeback state check. This issue manifests as an `IllegalStateException` when the `openFileCtx` is found to have a false async status during the execution of the `executeWriteBack` method in the `OpenFileCtx` class. The stack trace indicates that the error occurs when the `AsyncDataService` attempts to run the write-back operation, but the context is not properly set up for asynchronous operations.",
            "StackTrace": [
                "2013-11-26 10:34:07,859 DEBUG nfs3.RpcProgramNfs3 (Nfs3Utils.java:writeChannel(113)) - WRITE_RPC_CALL_END______957880843",
                "2013-11-26 10:34:07,863 DEBUG nfs3.OpenFileCtx (OpenFileCtx.java:offerNextToWrite(832)) - The asyn write task has no pending writes, fileId: 30938",
                "2013-11-26 10:34:07,871 ERROR nfs3.AsyncDataService (AsyncDataService.java:run(136)) - Asyn data service got error:java.lang.IllegalStateException: The openFileCtx has false async status",
                "at com.google.common.base.Preconditions.checkState(Preconditions.java:145)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.executeWriteBack(OpenFileCtx.java:890)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService$WriteBackTask.run(AsyncDataService.java:134)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2013-11-26 10:34:07,901 DEBUG nfs3.RpcProgramNfs3 (RpcProgramNfs3.java:write(707)) - requested offset=917504 and current filesize=917504",
                "2013-11-26 10:34:07,902 DEBUG nfs3.WriteManager (WriteManager.java:handleWrite(131)) - handleWrite fileId: 30938 offset: 917504 length:65536 stableHow:0"
            ],
            "RootCause": "The root cause of the issue is that the `asyncStatus` in the `OpenFileCtx` is not being set correctly before the `executeWriteBack` method is called. This can occur due to race conditions or mismanagement of the write context, particularly in the `checkAndStartWrite` method, which is responsible for initiating the asynchronous write operation.",
            "StepsToReproduce": [
                "1. Start the NFS gateway.",
                "2. Initiate multiple write operations concurrently to the same file.",
                "3. Monitor the logs for the `IllegalStateException` related to the `openFileCtx` async status."
            ],
            "ExpectedBehavior": "The system should handle concurrent write operations without throwing an `IllegalStateException`, ensuring that the `openFileCtx` has a valid async status when executing write-back operations.",
            "ObservedBehavior": "The system throws an `IllegalStateException` indicating that the `openFileCtx` has a false async status when attempting to execute a write-back operation.",
            "Suggestions": "Review the logic in the `checkAndStartWrite` and `receivedNewWriteInternal` methods to ensure that the `asyncStatus` is set correctly before any write-back operations are initiated. Consider implementing synchronization mechanisms to prevent race conditions.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.java",
                    "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx",
                    "org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService"
                ],
                "methods": [
                    "OpenFileCtx.executeWriteBack",
                    "OpenFileCtx.checkAndStartWrite",
                    "OpenFileCtx.receivedNewWriteInternal"
                ]
            },
            "possible_fix": "Ensure that the `asyncStatus` is set to true in the `checkAndStartWrite` method when a write operation is initiated. Additionally, review the conditions under which `asyncStatus` is reset to false in the `executeWriteBack` method to prevent premature resets."
        },
        "possible_fix_code": {
            "OpenFileCtx.checkAndStartWrite": "  private synchronized boolean checkAndStartWrite(\n      AsyncDataService asyncDataService, WriteCtx writeCtx) {\n    \n    if (writeCtx.getOffset() == nextOffset.get()) {\n      if (!asyncStatus) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trigger the write back task. Current nextOffset: \"\n              + nextOffset.get());\n        }\n        asyncStatus = true;\n        asyncDataService.execute(new AsyncDataService.WriteBackTask(this));\n      } else {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"The write back thread is working.\");\n        }\n      }\n      return true;\n    } else {\n      return false;\n    }\n  }",
            "OpenFileCtx.executeWriteBack": "  void executeWriteBack() {\n    Preconditions.checkState(asyncStatus,\n        \"The openFileCtx has false async status\");\n    try {\n      while (activeState) {\n        WriteCtx toWrite = offerNextToWrite();\n        if (toWrite != null) {\n          // Do the write\n          doSingleWrite(toWrite);\n          updateLastAccessTime();\n        } else {\n          break;\n        }\n      }\n      \n      if (!activeState && LOG.isDebugEnabled()) {\n        LOG.debug(\"The openFileCtx is not active anymore, fileId: \"\n            + latestAttr.getFileId());\n      }\n    } finally {\n      // make sure we reset asyncStatus to false\n      asyncStatus = false;\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-11827.json",
        "creation_time": "2017-05-16T06:28:49.000+0000",
        "bug_report": {
            "Title": "NPE is thrown when log level changed in BlockPlacementPolicyDefault#chooseRandom() method",
            "Description": "A NullPointerException (NPE) occurs in the `chooseRandom()` method of the `BlockPlacementPolicyDefault` class when changing the log level of BlockPlacementPolicy using the 'hadoop daemonlog' command. The stack trace indicates that the exception is triggered during the execution of the replication monitor thread, specifically when attempting to choose a target for block placement. The absence of necessary null checks in the `chooseRandom()` method is suspected to be the root cause of this issue.",
            "StackTrace": [
                "2017-05-16 13:36:51,053 ERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor thread received Runtime exception.",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:666)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:607)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:375)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:446)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:446)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:240)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:119)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithMultiDC.chooseTargetInternal(BlockPlacementPolicyWithMultiDC.java:263)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithMultiDC.chooseTarget(BlockPlacementPolicyWithMultiDC.java:214)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.chooseTargets(BlockManager.java:3836)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.access$200(BlockManager.java:3801)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1394)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1300)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3764)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3711)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The NullPointerException is likely caused by uninitialized variables in the `chooseRandom()` method of the `BlockPlacementPolicyDefault` class, which lacks necessary null checks.",
            "StepsToReproduce": [
                "1. Change the log level of BlockPlacementPolicy using the 'hadoop daemonlog' command.",
                "2. Monitor the logs for the replication monitor thread.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The system should successfully change the log level without throwing any exceptions, and the replication monitor should function normally.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the replication monitor thread to fail.",
            "Suggestions": "Implement null checks in the `chooseRandom()` method to ensure that all object references are properly initialized before use. Additionally, review the configuration settings for data nodes and racks to ensure they are correctly set up.",
            "problem_location": {
                "files": [
                    "BlockPlacementPolicyDefault.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault"
                ],
                "methods": [
                    "chooseRandom",
                    "chooseRemoteRack",
                    "chooseTarget"
                ]
            },
            "possible_fix": "Add null checks in the `chooseRandom()` method to handle cases where object references may be null. For example:\n\n```java\nif (someObject == null) {\n    // Handle the null case appropriately\n}\n```"
        },
        "possible_fix_code": {
            "chooseRandom": "  protected DatanodeStorageInfo chooseRandom(int numOfReplicas,\n                            String scope,\n                            Set<Node> excludedNodes,\n                            long blocksize,\n                            int maxNodesPerRack,\n                            List<DatanodeStorageInfo> results,\n                            boolean avoidStaleNodes,\n                            EnumMap<StorageType, Integer> storageTypes)\n                            throws NotEnoughReplicasException {\n    StringBuilder builder = null;\n    if (LOG.isDebugEnabled()) {\n      builder = debugLoggingBuilder.get();\n      builder.setLength(0);\n      builder.append(\"[\");\n    }\n    boolean badTarget = false;\n    DatanodeStorageInfo firstChosen = null;\n    while (numOfReplicas > 0) {\n      // the storage type that current node has\n      StorageType includeType = null;\n      DatanodeDescriptor chosenNode = null;\n      if (clusterMap instanceof DFSNetworkTopology) {\n        for (StorageType type : storageTypes.keySet()) {\n          chosenNode = chooseDataNode(scope, excludedNodes, type);\n\n          if (chosenNode != null) {\n            includeType = type;\n            break;\n          }\n        }\n      } else {\n        chosenNode = chooseDataNode(scope, excludedNodes);\n      }\n\n      if (chosenNode == null) {\n        break;\n      }\n      Preconditions.checkState(excludedNodes.add(chosenNode), \"chosenNode \"\n          + chosenNode + \" is already in excludedNodes \" + excludedNodes);\n      if (LOG.isDebugEnabled()) {\n        builder.append(\"\\nNode \").append(NodeBase.getPath(chosenNode))\n            .append(\" [\");\n      }\n      DatanodeStorageInfo storage = null;\n      if (isGoodDatanode(chosenNode, maxNodesPerRack, considerLoad,\n          results, avoidStaleNodes)) {\n        for (Iterator<Map.Entry<StorageType, Integer>> iter = storageTypes\n            .entrySet().iterator(); iter.hasNext();) {\n          Map.Entry<StorageType, Integer> entry = iter.next();\n\n          // If there is one storage type the node has already contained,\n          // then no need to loop through other storage type.\n          if (includeType != null && entry.getKey() != includeType) {\n            continue;\n          }\n\n          storage = chooseStorage4Block(\n              chosenNode, blocksize, results, entry.getKey());\n          if (storage != null) {\n            numOfReplicas--;\n            if (firstChosen == null) {\n              firstChosen = storage;\n            }\n            // add node (subclasses may also add related nodes) to excludedNode\n            addToExcludedNodes(chosenNode, excludedNodes);\n            int num = entry.getValue();\n            if (num == 1) {\n              iter.remove();\n            } else {\n              entry.setValue(num - 1);\n            }\n            break;\n          }\n        }\n\n        if (LOG.isDebugEnabled()) {\n          builder.append(\"\\n]\");\n        }\n\n        // If no candidate storage was found on this DN then set badTarget.\n        badTarget = (storage == null);\n      }\n    }\n    if (numOfReplicas > 0) {\n      String detail = enableDebugLogging;\n      if (LOG.isDebugEnabled()) {\n        if (badTarget && builder != null) {\n          detail = builder.toString();\n          builder.setLength(0);\n        } else {\n          detail = \"\";\n        }\n      }\n      throw new NotEnoughReplicasException(detail);\n    }\n    \n    return firstChosen;\n  }"
        }
    },
    {
        "filename": "HDFS-6804.json",
        "creation_time": "2014-08-01T05:37:23.000+0000",
        "bug_report": {
            "Title": "Add test for race condition between transferring block and appending block causes 'Unexpected checksum mismatch exception'",
            "Description": "A checksum mismatch error occurs during the transfer of a block in the Hadoop Distributed File System (HDFS). The error log indicates that while the source DataNode successfully transmits the block, the destination DataNode reports a checksum mismatch, leading to an IOException. This results in the source DataNode's replica being incorrectly marked as corrupt by the NameNode, despite passing checksum verification. The issue may stem from network instability, data corruption, or misconfiguration in HDFS settings.",
            "StackTrace": [
                "java.io.IOException: Terminating due to a checksum error.",
                "java.io.IOException: Unexpected checksum mismatch while writing BP-2072804351-192.168.2.104-1406008383435:blk_1073741997_9248 from /192.168.2.101:39495",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:536)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:703)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:575)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:115)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:68)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The root cause of the issue appears to be a race condition between the block transfer and the checksum verification process, potentially exacerbated by network issues or misconfiguration in HDFS settings.",
            "StepsToReproduce": [
                "Initiate a block transfer from one DataNode to another.",
                "Monitor the logs for checksum verification messages during the transfer.",
                "Observe if the destination DataNode reports a checksum mismatch."
            ],
            "ExpectedBehavior": "The block should be successfully transferred and verified without any checksum mismatch errors, and the replica on the source DataNode should remain valid.",
            "ObservedBehavior": "The destination DataNode reports a checksum mismatch, leading to the source DataNode's replica being incorrectly marked as corrupt by the NameNode.",
            "Suggestions": "1. Check network stability to ensure no packet loss occurs during data transmission. 2. Verify the integrity of the data being written to ensure it is not corrupted. 3. Review HDFS configuration settings related to checksums for consistency across nodes. 4. Examine logs for additional error messages that may provide context. 5. Test with smaller data blocks to isolate the issue. 6. Consider upgrading Hadoop to a more recent version to address potential bugs.",
            "problem_location": {
                "files": [
                    "BlockReceiver.java",
                    "DataXceiver.java",
                    "Receiver.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.BlockReceiver",
                    "org.apache.hadoop.hdfs.server.datanode.DataXceiver",
                    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver"
                ],
                "methods": [
                    "BlockReceiver.receivePacket",
                    "DataXceiver.writeBlock",
                    "Receiver.processOp"
                ]
            },
            "possible_fix": "Investigate and implement error handling improvements in the block transfer process, ensuring that checksum verification is robust against race conditions. Additionally, consider adding logging to capture more detailed information during the transfer process."
        },
        "possible_fix_code": {
            "BlockReceiver.receivePacket": "  private int receivePacket() throws IOException {\n    // read the next packet\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader header = packetReceiver.getHeader();\n    if (LOG.isDebugEnabled()){\n      LOG.debug(\"Receiving one packet for block \" + block +\n                \": \" + header);\n    }\n\n    // Sanity check the header\n    if (header.getOffsetInBlock() > replicaInfo.getNumBytes()) {\n      throw new IOException(\"Received an out-of-sequence packet for \" + block + \n          \"from \" + inAddr + \" at offset \" + header.getOffsetInBlock() +\n          \". Expecting packet starting at \" + replicaInfo.getNumBytes());\n    }\n    if (header.getDataLen() < 0) {\n      throw new IOException(\"Got wrong length during writeBlock(\" + block + \n                            \") from \" + inAddr + \" at offset \" + \n                            header.getOffsetInBlock() + \": \" +\n                            header.getDataLen()); \n    }\n\n    long offsetInBlock = header.getOffsetInBlock();\n    long seqno = header.getSeqno();\n    boolean lastPacketInBlock = header.isLastPacketInBlock();\n    int len = header.getDataLen();\n    boolean syncBlock = header.getSyncBlock();\n\n    // avoid double sync'ing on close\n    if (syncBlock && lastPacketInBlock) {\n      this.syncOnClose = false;\n    }\n\n    // update received bytes\n    long firstByteInBlock = offsetInBlock;\n    offsetInBlock += len;\n    if (replicaInfo.getNumBytes() < offsetInBlock) {\n      replicaInfo.setNumBytes(offsetInBlock);\n    }\n    \n    // put in queue for pending acks, unless sync was requested\n    if (responder != null && !syncBlock && !shouldVerifyChecksum()) {\n      ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n          lastPacketInBlock, offsetInBlock, Status.SUCCESS);\n    }\n\n    //First write the packet to the mirror:\n    if (mirrorOut != null && !mirrorError) {\n      try {\n        long begin = Time.monotonicNow();\n        packetReceiver.mirrorPacketTo(mirrorOut);\n        mirrorOut.flush();\n        long duration = Time.monotonicNow() - begin;\n        if (duration > datanodeSlowLogThresholdMs) {\n          LOG.warn(\"Slow BlockReceiver write packet to mirror took \" + duration\n              + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms)\");\n        }\n      } catch (IOException e) {\n        handleMirrorOutError(e);\n      }\n    }\n    \n    ByteBuffer dataBuf = packetReceiver.getDataSlice();\n    ByteBuffer checksumBuf = packetReceiver.getChecksumSlice();\n    \n    if (lastPacketInBlock || len == 0) {\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"Receiving an empty packet or the end of the block \" + block);\n      }\n      // sync block if requested\n      if (syncBlock) {\n        flushOrSync(true);\n      }\n    } else {\n      int checksumLen = ((len + bytesPerChecksum - 1)/bytesPerChecksum)*\n                                                            checksumSize;\n\n      if ( checksumBuf.capacity() != checksumLen) {\n        throw new IOException(\"Length of checksums in packet \" +\n            checksumBuf.capacity() + \" does not match calculated checksum \" +\n            \"length \" + checksumLen);\n      }\n\n      if (shouldVerifyChecksum()) {\n        try {\n          verifyChunks(dataBuf, checksumBuf);\n        } catch (IOException ioe) {\n          // checksum error detected locally. there is no reason to continue.\n          if (responder != null) {\n            try {\n              ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n                  lastPacketInBlock, offsetInBlock,\n                  Status.ERROR_CHECKSUM);\n              // Wait until the responder sends back the response\n              // and interrupt this thread.\n              Thread.sleep(3000);\n            } catch (InterruptedException e) { }\n          }\n          throw new IOException(\"Terminating due to a checksum error.\" + ioe);\n        }\n \n        if (needsChecksumTranslation) {\n          // overwrite the checksums in the packet buffer with the\n          // appropriate polynomial for the disk storage.\n          translateChunks(dataBuf, checksumBuf);\n        }\n      }\n      \n      // by this point, the data in the buffer uses the disk checksum\n\n      byte[] lastChunkChecksum;\n      \n      try {\n        long onDiskLen = replicaInfo.getBytesOnDisk();\n        if (onDiskLen<offsetInBlock) {\n          //finally write to the disk :\n          \n          if (onDiskLen % bytesPerChecksum != 0) { \n            // prepare to overwrite last checksum\n            adjustCrcFilePosition();\n          }\n          \n          // If this is a partial chunk, then read in pre-existing checksum\n          if (firstByteInBlock % bytesPerChecksum != 0) {\n            LOG.info(\"Packet starts at \" + firstByteInBlock +\n                     \" for \" + block +\n                     \" which is not a multiple of bytesPerChecksum \" +\n                     bytesPerChecksum);\n            long offsetInChecksum = BlockMetadataHeader.getHeaderSize() +\n                onDiskLen / bytesPerChecksum * checksumSize;\n            computePartialChunkCrc(onDiskLen, offsetInChecksum, bytesPerChecksum);\n          }\n\n          int startByteToDisk = (int)(onDiskLen-firstByteInBlock) \n              + dataBuf.arrayOffset() + dataBuf.position();\n\n          int numBytesToDisk = (int)(offsetInBlock-onDiskLen);\n          \n          // Write data to disk.\n          long begin = Time.monotonicNow();\n          out.write(dataBuf.array(), startByteToDisk, numBytesToDisk);\n          long duration = Time.monotonicNow() - begin;\n          if (duration > datanodeSlowLogThresholdMs) {\n            LOG.warn(\"Slow BlockReceiver write data to disk cost:\" + duration\n                + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms)\");\n          }\n\n          // If this is a partial chunk, then verify that this is the only\n          // chunk in the packet. Calculate new crc for this chunk.\n          if (partialCrc != null) {\n            if (len > bytesPerChecksum) {\n              throw new IOException(\"Got wrong length during writeBlock(\" + \n                                    block + \") from \" + inAddr + \" \" +\n                                    \"A packet can have only one partial chunk.\"+\n                                    \" len = \" + len + \n                                    \" bytesPerChecksum \" + bytesPerChecksum);\n            }\n            partialCrc.update(dataBuf.array(), startByteToDisk, numBytesToDisk);\n            byte[] buf = FSOutputSummer.convertToByteStream(partialCrc, checksumSize);\n            lastChunkChecksum = Arrays.copyOfRange(\n              buf, buf.length - checksumSize, buf.length\n            );\n            checksumOut.write(buf);\n            if(LOG.isDebugEnabled()) {\n              LOG.debug(\"Writing out partial crc for data len \" + len);\n            }\n            partialCrc = null;\n          } else {\n            lastChunkChecksum = Arrays.copyOfRange(\n                checksumBuf.array(),\n                checksumBuf.arrayOffset() + checksumBuf.position() + checksumLen - checksumSize,\n                checksumBuf.arrayOffset() + checksumBuf.position() + checksumLen);\n            checksumOut.write(checksumBuf.array(),\n                checksumBuf.arrayOffset() + checksumBuf.position(),\n                checksumLen);\n          }\n          /// flush entire packet, sync if requested\n          flushOrSync(syncBlock);\n          \n          replicaInfo.setLastChecksumAndDataLen(\n            offsetInBlock, lastChunkChecksum\n          );\n\n          datanode.metrics.incrBytesWritten(len);\n\n          manageWriterOsCache(offsetInBlock);\n        }\n      } catch (IOException iex) {\n        datanode.checkDiskError();\n        throw iex;\n      }\n    }\n\n    // if sync was requested, put in queue for pending acks here\n    // (after the fsync finished)\n    if (responder != null && (syncBlock || shouldVerifyChecksum())) {\n      ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n          lastPacketInBlock, offsetInBlock, Status.SUCCESS);\n    }\n\n    if (throttler != null) { // throttle I/O\n      throttler.throttle(len);\n    }\n    \n    return lastPacketInBlock?-1:len;\n  }"
        }
    },
    {
        "filename": "HDFS-5843.json",
        "creation_time": "2014-01-28T06:05:16.000+0000",
        "bug_report": {
            "Title": "DFSClient.getFileChecksum() throws IOException if checksum is disabled",
            "Description": "When a file is created with checksum disabled (e.g., using ChecksumOpt.disabled()), invoking FileSystem.getFileChecksum() results in an IOException. The stack trace indicates that the DFSClient's getFileChecksum method fails to retrieve the block MD5 checksum due to issues with block metadata or availability. Additionally, the logs reveal an ArithmeticException in the DataXceiver class, suggesting a division by zero error during checksum calculation, likely caused by invalid or empty metadata.",
            "StackTrace": [
                "java.io.IOException: Fail to get block MD5 for BP-341493254-192.168.1.10-1390888724459:blk_1073741825_1001",
                "at org.apache.hadoop.hdfs.DFSClient.getFileChecksum(DFSClient.java:1965)",
                "at org.apache.hadoop.hdfs.DFSClient.getFileChecksum(DFSClient.java:1771)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1186)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1194)",
                "2014-01-27 21:58:46,329 ERROR datanode.DataNode (DataXceiver.java:run(225)) - 127.0.0.1:52398:DataXceiver error processing BLOCK_CHECKSUM operation src: /127.0.0.1:52407 dest: /127.0.0.1:52398",
                "java.lang.ArithmeticException: / by zero",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockChecksum(DataXceiver.java:658)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opBlockChecksum(Receiver.java:169)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:77)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)",
                "at java.lang.Thread.run(Thread.java:695)"
            ],
            "RootCause": "The IOException in the getFileChecksum method indicates issues with retrieving the block's MD5 checksum, likely due to problems with block metadata or availability. The ArithmeticException in the blockChecksum method suggests a potential bug where the calculation of crcPerBlock could lead to division by zero if the metadata length is not valid.",
            "StepsToReproduce": [
                "Create a file with checksum disabled using ChecksumOpt.disabled()",
                "Call FileSystem.getFileChecksum() on the created file",
                "Observe the IOException thrown"
            ],
            "ExpectedBehavior": "The system should return the MD5 checksum of the file without throwing an IOException, even if checksums are disabled.",
            "ObservedBehavior": "An IOException is thrown indicating failure to retrieve the block MD5 checksum, along with an ArithmeticException due to division by zero in the DataXceiver class.",
            "Suggestions": "Implement additional error handling in the blockChecksum method to check for valid metadata before performing calculations. Ensure that the getFileChecksum method can gracefully handle scenarios where blocks are unavailable or corrupted.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.DFSClient",
                    "org.apache.hadoop.hdfs.server.datanode.DataXceiver"
                ],
                "methods": [
                    "DFSClient.getFileChecksum",
                    "DataXceiver.blockChecksum"
                ]
            },
            "possible_fix": "In the blockChecksum method, add checks to validate the metadata length before performing the division for crcPerBlock calculation. Additionally, enhance error handling in the getFileChecksum method to manage cases where block metadata is invalid or unavailable."
        },
        "possible_fix_code": {
            "DFSClient.getFileChecksum": "  private static MD5MD5CRC32FileChecksum getFileChecksum(String src,\n      String clientName,\n      ClientProtocol namenode, SocketFactory socketFactory, int socketTimeout,\n      DataEncryptionKey encryptionKey, boolean connectToDnViaHostname)\n      throws IOException {\n    //get all block locations\n    LocatedBlocks blockLocations = callGetBlockLocations(namenode, src, 0, Long.MAX_VALUE);\n    if (null == blockLocations) {\n      throw new FileNotFoundException(\"File does not exist: \" + src);\n    }\n    List<LocatedBlock> locatedblocks = blockLocations.getLocatedBlocks();\n    final DataOutputBuffer md5out = new DataOutputBuffer();\n    int bytesPerCRC = -1;\n    DataChecksum.Type crcType = DataChecksum.Type.DEFAULT;\n    long crcPerBlock = 0;\n    boolean refetchBlocks = false;\n    int lastRetriedIndex = -1;\n\n    //get block checksum for each block\n    for(int i = 0; i < locatedblocks.size(); i++) {\n      if (refetchBlocks) {  // refetch to get fresh tokens\n        blockLocations = callGetBlockLocations(namenode, src, 0, Long.MAX_VALUE);\n        if (null == blockLocations) {\n          throw new FileNotFoundException(\"File does not exist: \" + src);\n        }\n        locatedblocks = blockLocations.getLocatedBlocks();\n        refetchBlocks = false;\n      }\n      LocatedBlock lb = locatedblocks.get(i);\n      final ExtendedBlock block = lb.getBlock();\n      final DatanodeInfo[] datanodes = lb.getLocations();\n      \n      //try each datanode location of the block\n      final int timeout = 3000 * datanodes.length + socketTimeout;\n      boolean done = false;\n      for(int j = 0; !done && j < datanodes.length; j++) {\n        DataOutputStream out = null;\n        DataInputStream in = null;\n        \n        try {\n          //connect to a datanode\n          IOStreamPair pair = connectToDN(socketFactory, connectToDnViaHostname,\n              encryptionKey, datanodes[j], timeout);\n          out = new DataOutputStream(new BufferedOutputStream(pair.out,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          in = new DataInputStream(pair.in);\n\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"write to \" + datanodes[j] + \": \"\n                + Op.BLOCK_CHECKSUM + \", block=\" + block);\n          }\n          // get block MD5\n          new Sender(out).blockChecksum(block, lb.getBlockToken());\n\n          final BlockOpResponseProto reply =\n            BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n\n          if (reply.getStatus() != Status.SUCCESS) {\n            if (reply.getStatus() == Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException();\n            } else {\n              throw new IOException(\"Bad response \" + reply + \" for block \"\n                  + block + \" from datanode \" + datanodes[j]);\n            }\n          }\n          \n          OpBlockChecksumResponseProto checksumData =\n            reply.getChecksumResponse();\n\n          //read byte-per-checksum\n          final int bpc = checksumData.getBytesPerCrc();\n          if (i == 0) { //first block\n            bytesPerCRC = bpc;\n          }\n          else if (bpc != bytesPerCRC) {\n            throw new IOException(\"Byte-per-checksum not matched: bpc=\" + bpc\n                + \" but bytesPerCRC=\" + bytesPerCRC);\n          }\n          \n          //read crc-per-block\n          final long cpb = checksumData.getCrcPerBlock();\n          if (locatedblocks.size() > 1 && i == 0) {\n            crcPerBlock = cpb;\n          }\n\n          //read md5\n          final MD5Hash md5 = new MD5Hash(\n              checksumData.getMd5().toByteArray());\n          md5.write(md5out);\n          \n          // read crc-type\n          final DataChecksum.Type ct;\n          if (checksumData.hasCrcType()) {\n            ct = PBHelper.convert(checksumData\n                .getCrcType());\n          } else {\n            LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                      \"inferring checksum by reading first byte\");\n            ct = inferChecksumTypeByReading(\n                clientName, socketFactory, socketTimeout, lb, datanodes[j],\n                encryptionKey, connectToDnViaHostname);\n          }\n\n          if (i == 0) { // first block\n            crcType = ct;\n          } else if (crcType != DataChecksum.Type.MIXED\n              && crcType != ct) {\n            // if crc types are mixed in a file\n            crcType = DataChecksum.Type.MIXED;\n          }\n\n          done = true;\n\n          if (LOG.isDebugEnabled()) {\n            if (i == 0) {\n              LOG.debug(\"set bytesPerCRC=\" + bytesPerCRC\n                  + \", crcPerBlock=\" + crcPerBlock);\n            }\n            LOG.debug(\"got reply from \" + datanodes[j] + \": md5=\" + md5);\n          }\n        } catch (InvalidBlockTokenException ibte) {\n          if (i > lastRetriedIndex) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                  + \"for file \" + src + \" for block \" + block\n                  + \" from datanode \" + datanodes[j]\n                  + \". Will retry the block once.\");\n            }\n            lastRetriedIndex = i;\n            done = true; // actually it's not done; but we\\'ll retry\n            i--; // repeat at i-th block\n            refetchBlocks = true;\n            break;\n          }\n        } catch (IOException ie) {\n          LOG.warn(\"src=\" + src + \", datanodes[\"+j+\"]=\" + datanodes[j], ie);\n        } finally {\n          IOUtils.closeStream(in);\n          IOUtils.closeStream(out);\n        }\n      }\n\n      if (!done) {\n        throw new IOException(\"Fail to get block MD5 for \" + block);\n      }\n    }\n\n    //compute file MD5\n    final MD5Hash fileMD5 = MD5Hash.digest(md5out.getData()); \n    switch (crcType) {\n      case CRC32:\n        return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      case CRC32C:\n        return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      default:\n        // If there is no block allocated for the file,\n        // return one with the magic entry that matches what previous\n        // hdfs versions return.\n        if (locatedblocks.size() == 0) {\n          return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n        }\n\n        // we should never get here since the validity was checked\n        // when getCrcType() was called above.\n        return null;\n    }\n  }",
            "DataXceiver.blockChecksum": "  public void blockChecksum(final ExtendedBlock block,\n      final Token<BlockTokenIdentifier> blockToken) throws IOException {\n    final DataOutputStream out = new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final LengthInputStream metadataIn = \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn = new DataInputStream(new BufferedInputStream(\n        metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header = BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum = header.getChecksum(); \n      final int bytesPerCRC = checksum.getBytesPerChecksum();\n      final long metadataLength = metadataIn.getLength();\n      if (metadataLength <= BlockMetadataHeader.getHeaderSize()) {\n        throw new IOException(\"Invalid metadata length for block: \" + block);\n      }\n      final long crcPerBlock = (metadataLength - BlockMetadataHeader.getHeaderSize()) / checksum.getChecksumSize();\n      \n      //compute block checksum\n      final MD5Hash md5 = MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block=\" + block + \", bytesPerCRC=\" + bytesPerCRC\n            + \", crcPerBlock=\" + crcPerBlock + \", md5=\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          .setCrcType(PBHelper.convert(checksum.getChecksumType()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }"
        }
    },
    {
        "filename": "HDFS-8070.json",
        "creation_time": "2015-04-06T19:57:58.000+0000",
        "bug_report": {
            "Title": "Pre-HDFS-7915 DFSClient cannot use short circuit on post-HDFS-7915 DataNode",
            "Description": "The issue arises from the HDFS ShortCircuitShm layer, which appears to be locking tasks during multi-threaded split-generation. This problem was encountered immediately after upgrading the DataNode, raising concerns about compatibility between the 2.8.0 DataNode and the 2.7.0 Client. The stack traces indicate failures in releasing shared memory slots, leading to exceptions such as `ERROR_INVALID` and `ClosedChannelException`. These errors suggest that the system is attempting to access unregistered shared memory segments, which may be a result of improper initialization or cleanup processes.",
            "StackTrace": [
                "2015-04-06 00:04:30,781 ERROR [ShortCircuitCache_SlotReleaser] shortcircuit.ShortCircuitCache: ShortCircuitCache(0x29e82045): failed to release short-circuit shared memory slot Slot(slotIdx=2, shm=DfsClientShm(a86ee34576d93c4964005d90b0d97c38)) by sending ReleaseShortCircuitAccessRequestProto to /grid/0/cluster/hdfs/dn_socket.  Closing shared memory segment.",
                "java.io.IOException: ERROR_INVALID: there is no shared memory segment registered with shmId a86ee34576d93c4964005d90b0d97c38",
                "2015-04-02 18:58:47,653 [DataXceiver for client unix:/grid/0/cluster/hdfs/dn_socket [Passing file descriptors for block BP-942051088-172.18.0.41-1370508013893:blk_1076973408_1099515627985]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: cn060-10.l42scl.hortonworks.com:50010:DataXceiver error processing REQUEST_SHORT_CIRCUIT_FDS operation src: unix:/grid/0/cluster/hdfs/dn_socket dst: <local>",
                "java.io.EOFException"
            ],
            "RootCause": "The root cause of the issue is the failure to manage shared memory segments properly, leading to attempts to access unregistered segments. This is compounded by the closed channels and EOF exceptions that occur during short-circuit read operations.",
            "StepsToReproduce": [
                "Upgrade the DataNode to version 2.8.0.",
                "Attempt to perform multi-threaded split-generation using a 2.7.0 Client.",
                "Monitor the logs for errors related to short-circuit shared memory access."
            ],
            "ExpectedBehavior": "The system should successfully manage shared memory segments, allowing for efficient short-circuit reads without encountering errors related to unregistered segments or closed channels.",
            "ObservedBehavior": "The system fails to release shared memory slots, resulting in `ERROR_INVALID` messages and exceptions such as `ClosedChannelException` and `EOFException` during short-circuit read operations.",
            "Suggestions": "1. Verify the configuration of shared memory segments in the Hadoop environment to ensure they are properly registered. 2. Review the code for proper resource management, particularly in the `ShortCircuitCache` and `DataXceiver` classes. 3. Examine Hadoop logs for additional context on the failures. 4. Consult Hadoop documentation for guidance on short-circuit read configurations.",
            "problem_location": {
                "files": [
                    "ShortCircuitCache.java",
                    "DataXceiver.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache",
                    "org.apache.hadoop.hdfs.server.datanode.DataXceiver"
                ],
                "methods": [
                    "ShortCircuitCache.releaseShortCircuitAccess",
                    "DataXceiver.requestShortCircuitFds"
                ]
            },
            "possible_fix": "Ensure that shared memory segments are correctly initialized and cleaned up in the `ShortCircuitCache` class. Additionally, review the `requestShortCircuitFds` method in the `DataXceiver` class to handle shared memory access more robustly."
        },
        "possible_fix_code": {
            "opRequestShortCircuitFds": "private void opRequestShortCircuitFds(DataInputStream in) throws IOException {\n    final OpRequestShortCircuitAccessProto proto =\n      OpRequestShortCircuitAccessProto.parseFrom(vintPrefixed(in));\n    SlotId slotId = (proto.hasSlotId()) ? \n        PBHelper.convert(proto.getSlotId()) : null;\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      // Ensure shared memory segment is registered before requesting\n      if (!isSharedMemorySegmentRegistered(slotId)) {\n          throw new IOException(\"Shared memory segment not registered: \" + slotId);\n      }\n      requestShortCircuitFds(PBHelper.convert(proto.getHeader().getBlock()),\n          PBHelper.convert(proto.getHeader().getToken()),\n          slotId, proto.getMaxVersion(), true);\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }\n\n  private boolean isSharedMemorySegmentRegistered(SlotId slotId) {\n      // Logic to check if the shared memory segment is registered\n      // This is a placeholder for the actual implementation\n      return true; // Assume it is registered for now\n  }"
        }
    },
    {
        "filename": "HDFS-1085.json",
        "creation_time": "2010-04-06T22:49:40.000+0000",
        "bug_report": {
            "Title": "hftp read failing silently",
            "Description": "During a large-scale distcp operation using hftp, multiple tasks encountered failures due to a file size mismatch. The expected file size was approximately 1.6 GB, but only about 184.9 MB was successfully copied to a temporary file. This indicates that while the read operation did not fail outright, the resulting file was significantly smaller than anticipated, suggesting potential issues in the file transfer process.",
            "StackTrace": [
                "2010-04-06 17:56:43,005 INFO org.apache.hadoop.tools.DistCp: FAIL 2010/0/part-00032 : java.io.IOException: File size not matched: copied 193855488 bytes (184.9m) to tmpfile (=hdfs://omehost.com:8020/somepath/part-00032) but expected 1710327403 bytes (1.6g) from hftp://someotherhost/somepath/part-00032",
                "at org.apache.hadoop.tools.DistCp$CopyFilesMapper.copy(DistCp.java:435)",
                "at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:543)",
                "at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:310)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:159)"
            ],
            "RootCause": "The root cause of the issue appears to be a mismatch in the expected and actual file sizes during the copy operation, which could stem from network issues, file corruption, or incorrect file paths.",
            "StepsToReproduce": [
                "Initiate a distcp operation using hftp to copy a large file from a remote host.",
                "Monitor the logs for any file size mismatch errors during the copy process."
            ],
            "ExpectedBehavior": "The distcp operation should successfully copy the entire file, resulting in a temporary file that matches the expected size.",
            "ObservedBehavior": "The operation fails with an IOException indicating that the copied file size does not match the expected size, leading to incomplete data transfer.",
            "Suggestions": "Investigate potential network issues, verify the integrity of the source file, and ensure that the correct file paths are being used in the distcp command.",
            "problem_location": {
                "files": [
                    "DistCp.java"
                ],
                "classes": [
                    "org.apache.hadoop.tools.DistCp"
                ],
                "methods": [
                    "DistCp.copy",
                    "DistCp.run"
                ]
            },
            "possible_fix": "Review the implementation of the DistCp.copy method to ensure proper error handling and file size verification. Consider adding logging for file size checks before and after the copy operation."
        },
        "possible_fix_code": {
            "DistCp.copy": "public void copy(Path src, Path dest) throws IOException {\n    // Initialize the file system and check the source file size\n    FileSystem fs = FileSystem.get(src.toUri(), getConf());\n    long expectedSize = fs.getFileStatus(src).getLen();\n\n    // Perform the copy operation\n    try (FSDataInputStream in = fs.open(src); FSDataOutputStream out = fs.create(dest)) {\n        byte[] buffer = new byte[4096];\n        int bytesRead;\n        long totalBytesCopied = 0;\n\n        while ((bytesRead = in.read(buffer)) > 0) {\n            out.write(buffer, 0, bytesRead);\n            totalBytesCopied += bytesRead;\n        }\n\n        // Verify the copied file size\n        if (totalBytesCopied != expectedSize) {\n            throw new IOException(\"File size not matched: copied \" + totalBytesCopied + \" bytes to \" + dest + \" but expected \" + expectedSize + \" bytes\");\n        }\n    } catch (IOException e) {\n        // Log the error and rethrow\n        LOG.error(\"Error during file copy from \" + src + \" to \" + dest, e);\n        throw e;\n    }\n}"
        }
    },
    {
        "filename": "HDFS-12339.json",
        "creation_time": "2017-08-23T03:16:49.000+0000",
        "bug_report": {
            "Title": "NFS Gateway on Shutdown Gives Unregistration Failure. Does Not Unregister with rpcbind Portmapper",
            "Description": "When stopping the NFS Gateway, an error is logged indicating an unregistration failure with the rpcbind portmapper. The logs show a `RuntimeException` caused by a `SocketException`, specifically stating that the 'Socket is closed'. This issue arises when the system attempts to send data over a socket that has already been closed, leading to failure in unregistering the NFS service.",
            "StackTrace": [
                "2017-08-17 18:09:16,529 ERROR org.apache.hadoop.oncrpc.RpcProgram: Unregistration failure with localhost:2049, portmap entry: (PortmapMapping-100003:3:6:2049)",
                "2017-08-17 18:09:16,531 WARN org.apache.hadoop.util.ShutdownHookManager: ShutdownHook 'NfsShutdownHook' failed, java.lang.RuntimeException: Unregistration failure",
                "java.lang.RuntimeException: Unregistration failure",
                "Caused by: java.net.SocketException: Socket is closed",
                "at java.net.DatagramSocket.send(DatagramSocket.java:641)",
                "at org.apache.hadoop.oncrpc.SimpleUdpClient.run(SimpleUdpClient.java:62)"
            ],
            "RootCause": "The root cause of the issue is a `SocketException` indicating that the socket is closed when an attempt is made to send data. This suggests improper socket management, where the socket may be closed prematurely or not managed correctly during the shutdown process.",
            "StepsToReproduce": [
                "1. Start the NFS Gateway service.",
                "2. Attempt to stop the NFS Gateway service.",
                "3. Observe the logs for any errors related to unregistration."
            ],
            "ExpectedBehavior": "The NFS Gateway should unregister successfully with the rpcbind portmapper without any errors.",
            "ObservedBehavior": "An error is logged indicating a failure to unregister due to a closed socket, resulting in a `RuntimeException`.",
            "Suggestions": "Ensure that the socket is properly managed and not closed before all operations are complete. Implement error handling to manage socket states effectively and prevent premature closure.",
            "problem_location": {
                "files": [
                    "SimpleUdpClient.java"
                ],
                "classes": [
                    "org.apache.hadoop.oncrpc.SimpleUdpClient"
                ],
                "methods": [
                    "SimpleUdpClient.run"
                ]
            },
            "possible_fix": "Review the socket management logic in the `SimpleUdpClient` class to ensure that the socket remains open until all necessary operations are completed. Consider adding checks to prevent sending data over a closed socket."
        },
        "possible_fix_code": {
            "SimpleUdpClient.run": "  public void run() throws IOException {\n    InetAddress IPAddress = InetAddress.getByName(host);\n    byte[] sendData = request.getBytes();\n    byte[] receiveData = new byte[65535];\n    // Use the provided socket if there is one, else just make a new one.\n    DatagramSocket socket = this.clientSocket == null ?\n        new DatagramSocket() : this.clientSocket;\n\n    try {\n      DatagramPacket sendPacket = new DatagramPacket(sendData, sendData.length,\n          IPAddress, port);\n      socket.send(sendPacket);\n      socket.setSoTimeout(udpTimeoutMillis);\n      DatagramPacket receivePacket = new DatagramPacket(receiveData,\n          receiveData.length);\n      socket.receive(receivePacket);\n  \n      // Check reply status\n      XDR xdr = new XDR(Arrays.copyOfRange(receiveData, 0,\n          receivePacket.getLength()));\n      RpcReply reply = RpcReply.read(xdr);\n      if (reply.getState() != RpcReply.ReplyState.MSG_ACCEPTED) {\n        throw new IOException(\"Request failed: \" + reply.getState());\n      }\n    } catch (SocketException e) {\n      // Handle the case where the socket is closed\n      throw new IOException(\"Socket operation failed: \" + e.getMessage(), e);\n    } finally {\n      // If the client socket was passed in to this UDP client, it\\'s on the\n      // caller of this UDP client to close that socket.\n      if (this.clientSocket == null) {\n        socket.close();\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-6520.json",
        "creation_time": "2014-06-12T02:32:29.000+0000",
        "bug_report": {
            "Title": "hdfs fsck -move passes invalid length value when creating BlockReader",
            "Description": "An error occurs when executing the command 'fsck -move' in Hadoop's HDFS. The process involves setting up a pseudo cluster, copying a file to HDFS, corrupting a block of that file, and then running fsck to check the integrity of the filesystem. The fsck command identifies a corrupt block and attempts to move the corrupted file to '/lost+found'. However, an IOException is thrown, indicating an issue with reading the block data, specifically an expected empty end-of-read packet. This suggests potential data corruption or misconfiguration in the Hadoop setup.",
            "StackTrace": [
                "java.io.IOException: Expected empty end-of-read packet! Header: PacketHeader with packetLen=66048 header data: offsetInBlock: 65536 seqno: 1 lastPacketInBlock: false dataLen: 65536",
                "at org.apache.hadoop.hdfs.RemoteBlockReader2.readTrailingEmptyPacket(RemoteBlockReader2.java:259)",
                "at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:220)",
                "at org.apache.hadoop.hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:138)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlock(NamenodeFsck.java:649)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlocksToLostFound(NamenodeFsck.java:543)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:460)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:324)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.fsck(NamenodeFsck.java:233)"
            ],
            "RootCause": "The root cause of the issue is an IOException thrown by the RemoteBlockReader2 class, indicating that the expected empty end-of-read packet was not received. This suggests that the block data may be corrupted or not formatted correctly, potentially due to network issues or misconfiguration in the Hadoop setup.",
            "StepsToReproduce": [
                "Set up a pseudo cluster.",
                "Copy a file to HDFS.",
                "Corrupt a block of the file.",
                "Run fsck to check the filesystem integrity.",
                "Execute 'fsck -move' to attempt moving the corrupted file to '/lost+found'."
            ],
            "ExpectedBehavior": "The fsck -move command should successfully move the corrupted file to the '/lost+found' directory without throwing any exceptions.",
            "ObservedBehavior": "An IOException is thrown indicating an expected empty end-of-read packet was not received, preventing the corrupted file from being moved.",
            "Suggestions": "Check for data corruption in the block being read. Ensure that the Hadoop configuration is correct and that there are no network issues affecting the connection to DataNodes. Consider implementing additional logging to capture more details about the block reading process.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.RemoteBlockReader2",
                    "org.apache.hadoop.hdfs.server.namenode.NamenodeFsck"
                ],
                "methods": [
                    "RemoteBlockReader2.readTrailingEmptyPacket",
                    "NamenodeFsck.copyBlock"
                ]
            },
            "possible_fix": "Review the implementation of the readTrailingEmptyPacket method to ensure it correctly handles the end-of-read packet. Additionally, verify the copyBlock method in NamenodeFsck to ensure it properly checks for data integrity and handles connection issues with DataNodes."
        },
        "possible_fix_code": {
            "RemoteBlockReader2.readTrailingEmptyPacket": "  private void readTrailingEmptyPacket() throws IOException {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Reading empty packet at end of read\");\n    }\n    \n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader trailer = packetReceiver.getHeader();\n    if (!trailer.isLastPacketInBlock() ||\n       trailer.getDataLen() != 0) {\n      throw new IOException(\"Expected empty end-of-read packet! Header: \" +\n                            trailer);\n    }\n  }",
            "NamenodeFsck.copyBlock": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures = 0;\n    InetSocketAddress targetAddr = null;\n    TreeSet<DatanodeInfo> deadNodes = new TreeSet<DatanodeInfo>();\n    BlockReader blockReader = null; \n    ExtendedBlock block = lblock.getBlock(); \n\n    while (blockReader == null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode = bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr = NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures >= DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        String file = BlockReaderFactory.getFileName(targetAddr,\n            block.getBlockPoolId(), block.getBlockId());\n        blockReader = new BlockReaderFactory(dfs.getConf()).\n            setFileName(file).\n            setBlock(block).\n            setBlockToken(lblock.getBlockToken()).\n            setStartOffset(0).\n            setLength(-1).\n            setVerifyChecksum(true).\n            setClientName(\"fsck\").\n            setDatanodeInfo(chosenNode).\n            setInetSocketAddress(targetAddr).\n            setCachingStrategy(CachingStrategy.newDropBehind()).\n            setClientCacheContext(dfs.getClientContext()).\n            setConfiguration(namenode.conf).\n            setRemotePeerFactory(new RemotePeerFactory() {\n              @Override\n              public Peer newConnectedPeer(InetSocketAddress addr)\n                  throws IOException {\n                Peer peer = null;\n                Socket s = NetUtils.getDefaultSocketFactory(conf).createSocket();\n                try {\n                  s.connect(addr, HdfsServerConstants.READ_TIMEOUT);\n                  s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n                  peer = TcpPeerServer.peerFromSocketAndKey(s, namenode.getRpcServer().\n                        getDataEncryptionKey());\n                } finally {\n                  if (peer == null) {\n                    IOUtils.closeQuietly(s);\n                  }\n                }\n                return peer;\n              }\n            }).\n            build();\n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n      }\n    }\n    byte[] buf = new byte[1024];\n    int cnt = 0;\n    boolean success = true;\n    long bytesRead = 0;\n    try {\n      while ((cnt = blockReader.read(buf, 0, buf.length)) > 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead += cnt;\n      }\n      if ( bytesRead != block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success = false;\n    } finally {\n      if (blockReader != null) {\n        blockReader.close();\n      }\n    }\n    if (!success) {\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-10715.json",
        "creation_time": "2016-08-02T08:10:23.000+0000",
        "bug_report": {
            "Title": "NPE when applying AvailableSpaceBlockPlacementPolicy",
            "Description": "The introduction of the AvailableSpaceBlockPlacementPolicy in HDFS-8131 has led to a NullPointerException (NPE) under certain conditions. The NPE occurs in the `compareDataNode` method of the `AvailableSpaceBlockPlacementPolicy` class when it attempts to compare two `DatanodeDescriptor` objects that may be null. The stack trace indicates that the issue arises during the execution of the `chooseDataNode` method, which retrieves these descriptors using the `clusterMap.chooseRandom` method. If either of these calls returns null, it results in an NPE when `compareDataNode` is invoked.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.compareDataNode(AvailableSpaceBlockPlacementPolicy.java:95)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.chooseDataNode(AvailableSpaceBlockPlacementPolicy.java:80)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:691)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:665)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseLocalRack(BlockPlacementPolicyDefault.java:572)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:457)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:367)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:242)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:114)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:130)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1606)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3315)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:679)",
                "at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:214)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:489)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)"
            ],
            "RootCause": "The root cause of the NPE is that the `chooseDataNode` method retrieves two `DatanodeDescriptor` objects using `clusterMap.chooseRandom`, which can return null. When either of these descriptors is null, the subsequent call to `compareDataNode` results in a NullPointerException.",
            "StepsToReproduce": [
                "1. Deploy the AvailableSpaceBlockPlacementPolicy in an HDFS environment.",
                "2. Trigger a block addition that invokes the `chooseDataNode` method.",
                "3. Ensure that the `clusterMap.chooseRandom` method returns null for one or both `DatanodeDescriptor` objects."
            ],
            "ExpectedBehavior": "The system should handle cases where `chooseRandom` returns null, either by retrying the selection or by implementing a fallback mechanism to avoid NPE.",
            "ObservedBehavior": "The system throws a NullPointerException when attempting to compare two null `DatanodeDescriptor` objects in the `compareDataNode` method.",
            "Suggestions": "Implement null checks in the `chooseDataNode` method before calling `compareDataNode`. If either descriptor is null, handle the situation gracefully, possibly by retrying the selection or returning a default value.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy",
                    "org.apache.hadoop.net.NetworkTopology"
                ],
                "methods": [
                    "AvailableSpaceBlockPlacementPolicy.chooseDataNode",
                    "AvailableSpaceBlockPlacementPolicy.compareDataNode",
                    "NetworkTopology.chooseRandom"
                ]
            },
            "possible_fix": "Add null checks in the `chooseDataNode` method:\n\nprotected DatanodeDescriptor chooseDataNode(final String scope, final Collection<Node> excludedNode) {\n    DatanodeDescriptor a = (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNode);\n    DatanodeDescriptor b = (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNode);\n    if (a == null || b == null) {\n        // Handle null case, e.g., log a warning and return a default value or retry\n    }\n    int ret = compareDataNode(a, b);\n    // Existing logic follows...\n}"
        },
        "possible_fix_code": {
            "AvailableSpaceBlockPlacementPolicy.chooseDataNode": "  protected DatanodeDescriptor chooseDataNode(final String scope,\n      final Collection<Node> excludedNode) {\n    DatanodeDescriptor a =\n        (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNode);\n    DatanodeDescriptor b =\n        (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNode);\n    if (a == null || b == null) {\n        // Handle null case, e.g., log a warning and return a default value or retry\n        return null; // or handle as appropriate\n    }\n    int ret = compareDataNode(a, b);\n    if (ret == 0) {\n      return a;\n    } else if (ret < 0) {\n      return (RAND.nextInt(100) < balancedPreference) ? a : b;\n    } else {\n      return (RAND.nextInt(100) < balancedPreference) ? b : a;\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-3332.json",
        "creation_time": "2012-04-27T04:42:38.000+0000",
        "bug_report": {
            "Title": "NullPointerException in DN when directoryscanner is trying to report bad blocks",
            "Description": "A NullPointerException (NPE) occurs in the DataNode (DN) when the DirectoryScanner attempts to report bad blocks. This issue arises specifically during the execution of the `reportBadBlocks` method in the `BPServiceActor` class, where a `DatanodeInfo` object is created using a potentially uninitialized `bpRegistration` field. The stack trace indicates that the NPE is thrown when the `DatanodeID` constructor is called, which suggests that `bpRegistration` is null at that point.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.protocol.DatanodeID.<init>(DatanodeID.java:66)",
                "at org.apache.hadoop.hdfs.protocol.DatanodeInfo.<init>(DatanodeInfo.java:87)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reportBadBlocks(BPServiceActor.java:238)",
                "at org.apache.hadoop.hdfs.server.datanode.BPOfferService.reportBadBlocks(BPOfferService.java:187)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:559)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.checkAndUpdate(FsDatasetImpl.java:1377)",
                "at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.reconcile(DirectoryScanner.java:318)",
                "at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.run(DirectoryScanner.java:284)"
            ],
            "RootCause": "The root cause of the NullPointerException is that the `bpRegistration` field in the `BPServiceActor` class is null when the `reportBadBlocks` method is invoked. This indicates that the `bpRegistration` is not being initialized properly, likely during the registration process with the NameNode.",
            "StepsToReproduce": [
                "1. Start a NameNode (NN) and a DataNode (DN) with HA configuration.",
                "2. Corrupt a block in the DataNode's storage.",
                "3. Trigger the DirectoryScanner to run, which attempts to report the bad block.",
                "4. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The DataNode should successfully report bad blocks to the NameNode without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown when the DirectoryScanner attempts to report bad blocks, causing the reporting process to fail.",
            "Suggestions": "Ensure that the `bpRegistration` field in the `BPServiceActor` class is properly initialized before it is used in the `reportBadBlocks` method. This may involve checking the registration process with the NameNode to confirm that it completes successfully.",
            "problem_location": {
                "files": [
                    "BPServiceActor.java",
                    "DirectoryScanner.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.BPServiceActor",
                    "org.apache.hadoop.hdfs.server.datanode.DirectoryScanner"
                ],
                "methods": [
                    "BPServiceActor.reportBadBlocks",
                    "DirectoryScanner.run"
                ]
            },
            "possible_fix": "Check the initialization of `bpRegistration` in the `BPServiceActor` class. Ensure that it is set correctly during the registration process with the NameNode. If necessary, add null checks before its usage in the `reportBadBlocks` method to prevent the NullPointerException."
        },
        "possible_fix_code": {
            "org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reportBadBlocks": "  void reportBadBlocks(ExtendedBlock block) {\n    if (bpRegistration == null) {\n      LOG.error(\"bpRegistration is null. Cannot report bad blocks.\");\n      return;\n    }\n    DatanodeInfo[] dnArr = { new DatanodeInfo(bpRegistration) };\n    LocatedBlock[] blocks = { new LocatedBlock(block, dnArr) }; \n    \n    try {\n      bpNamenode.reportBadBlocks(blocks);  \n    } catch (IOException e){\n      /* One common reason is that NameNode could be in safe mode.\n       * Should we keep on retrying in that case?\n       */\n      LOG.warn(\"Failed to report bad block \" + block + \" to namenode : \"\n          + \" Exception\", e);\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-6130.json",
        "creation_time": "2014-03-20T07:15:47.000+0000",
        "bug_report": {
            "Title": "NPE when upgrading namenode from fsimages older than -32",
            "Description": "During the upgrade of an old Hadoop cluster (version 0.20.2-cdh3u1) to a trunk instance, a NullPointerException (NPE) occurs when the High Availability (HA) feature is enabled. The error arises specifically when executing the command 'hdfs namenode -initializeSharedEdits'. The stack trace indicates that the NPE is triggered in the FSDirectory.isReservedName method, suggesting that a null argument is being passed during the loading of the filesystem image.",
            "StackTrace": [
                "14/03/20 15:06:42 FATAL namenode.NameNode: Exception in namenode join",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.isReservedName(FSDirectory.java:2984)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.addToParent(FSImageFormatPBINode.java:205)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeDirectorySection(FSImageFormatPBINode.java:162)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.loadInternal(FSImageFormatProtobuf.java:243)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:168)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.load(FSImageFormat.java:120)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:895)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:881)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:704)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:271)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:894)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:653)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits(NameNode.java:912)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1276)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1360)"
            ],
            "RootCause": "The NullPointerException in the FSDirectory.isReservedName method is likely caused by a null argument being passed during the loading of the filesystem image. This occurs when the FSImage is being processed, particularly in the context of the FSImageFormat and FSNamesystem methods.",
            "StepsToReproduce": [
                "1. Set up an old Hadoop cluster (version 0.20.2-cdh3u1).",
                "2. Enable High Availability (HA) in the configuration.",
                "3. Run the command 'hdfs namenode -initializeSharedEdits'."
            ],
            "ExpectedBehavior": "The namenode should initialize shared edits without throwing any exceptions, allowing the upgrade process to complete successfully.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the namenode initialization to fail and the upgrade process to be interrupted.",
            "Suggestions": "Investigate the FSImage.loadFSImage and FSNamesystem.loadFromDisk methods to identify where the null reference is being introduced. Implement necessary checks or validations to ensure that valid arguments are passed to the isReservedName method.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSDirectory",
                    "org.apache.hadoop.hdfs.server.namenode.FSImage",
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem"
                ],
                "methods": [
                    "FSDirectory.isReservedName",
                    "FSImage.loadFSImage",
                    "FSNamesystem.loadFromDisk"
                ]
            },
            "possible_fix": "Add null checks in the FSImage.loadFSImage and FSNamesystem.loadFromDisk methods to ensure that valid data is passed to the FSDirectory.isReservedName method. For example, before calling isReservedName, check if the input string is null and handle it appropriately."
        },
        "possible_fix_code": {
            "FSImage.loadFSImage": "  private void loadFSImage(File curFile, MD5Hash expectedMd5,\n      FSNamesystem target, MetaRecoveryContext recovery) throws IOException {\n    // BlockPoolId is required when the FsImageLoader loads the rolling upgrade\n    // information. Make sure the ID is properly set.\n    target.setBlockPoolId(this.getBlockPoolID());\n\n    FSImageFormat.LoaderDelegator loader = FSImageFormat.newLoader(conf, target);\n    loader.load(curFile);\n\n    // Check that the image digest we loaded matches up with what\n    // we expected\n    MD5Hash readImageMd5 = loader.getLoadedImageMd5();\n    if (expectedMd5 != null &&\n        !expectedMd5.equals(readImageMd5)) {\n      throw new IOException(\"Image file \" + curFile +\n          \" is corrupt with MD5 checksum of \" + readImageMd5 +\n          \" but expecting \" + expectedMd5);\n    }\n\n    long txId = loader.getLoadedImageTxId();\n    LOG.info(\"Loaded image for txid \" + txId + \" from \" + curFile);\n    lastAppliedTxId = txId;\n    storage.setMostRecentCheckpointInfo(txId, curFile.lastModified());\n\n    // Add null check for reserved names\n    if (curFile == null || !curFile.exists()) {\n        throw new IOException(\"File does not exist or is null: \" + curFile);\n    }\n  }",
            "FSNamesystem.loadFromDisk": "  static FSNamesystem loadFromDisk(Configuration conf) throws IOException {\n\n    checkConfiguration(conf);\n    FSImage fsImage = new FSImage(conf,\n        FSNamesystem.getNamespaceDirs(conf),\n        FSNamesystem.getNamespaceEditsDirs(conf));\n    FSNamesystem namesystem = new FSNamesystem(conf, fsImage, false);\n    StartupOption startOpt = NameNode.getStartupOption(conf);\n    if (startOpt == StartupOption.RECOVER) {\n      namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n    }\n\n    long loadStart = now();\n    try {\n      namesystem.loadFSImage(startOpt);\n    } catch (IOException ioe) {\n      LOG.warn(\"Encountered exception loading fsimage\", ioe);\n      fsImage.close();\n      throw ioe;\n    }\n    long timeTakenToLoadFSImage = now() - loadStart;\n    LOG.info(\"Finished loading FSImage in \" + timeTakenToLoadFSImage + \" msecs\");\n    NameNodeMetrics nnMetrics = NameNode.getNameNodeMetrics();\n    if (nnMetrics != null) {\n      nnMetrics.setFsImageLoadTime((int) timeTakenToLoadFSImage);\n    }\n    return namesystem;\n  }"
        }
    },
    {
        "filename": "HDFS-2827.json",
        "creation_time": "2012-01-23T09:34:51.000+0000",
        "bug_report": {
            "Title": "Cannot save namespace after renaming a directory above a file with an open lease",
            "Description": "The issue arises when attempting to save the namespace after performing a series of file system operations in Hadoop's HDFS. Specifically, the operations involve creating a directory, creating a file without closing the output stream, and renaming the parent directory of the file. Upon checkpointing, an IOException is thrown indicating that the system found a path for the file but could not locate a corresponding entry in the namespace. This suggests a potential inconsistency in the HDFS namespace management, particularly related to files that are under construction.",
            "StackTrace": [
                "2012-01-23 15:03:14,204 ERROR namenode.FSImage (FSImage.java:run(795)) - Unable to save image for E:\\HDFS-1623\\hadoop-hdfs-project\\hadoop-hdfs\\build\\test\\data\\dfs\\name3",
                "java.io.IOException: saveLeases found path /test1/est/abc.txt but no matching entry in namespace.[/test1/est/abc.txt]",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveFilesUnderConstruction(FSNamesystem.java:4336)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver.save(FSImageFormat.java:588)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImage(FSImage.java:761)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.run(FSImage.java:789)",
                "at java.lang.Thread.run(Unknown Source)"
            ],
            "RootCause": "The root cause of the issue is that the `saveFilesUnderConstruction` method in the `FSNamesystem` class is unable to find the file `/test1/est/abc.txt` in the HDFS namespace during the save operation. This occurs because the file is either not present or was not properly created in the namespace, leading to an IOException.",
            "StepsToReproduce": [
                "1. Execute `fs.mkdirs(new Path('/test1'))` to create a new directory.",
                "2. Create a file with `FSDataOutputStream create = fs.create(new Path('/test/abc.txt'))` without closing the output stream.",
                "3. Rename the directory with `fs.rename(new Path('/test/'), new Path('/test1/'))`.",
                "4. Wait for the checkpoint to complete and observe the error."
            ],
            "ExpectedBehavior": "The system should successfully save the namespace without any errors, indicating that the file `/test1/est/abc.txt` is correctly recognized in the HDFS namespace.",
            "ObservedBehavior": "An IOException is thrown during the checkpointing process, stating that the path `/test1/est/abc.txt` was found but has no matching entry in the namespace.",
            "Suggestions": "1. Ensure that the file `/test1/est/abc.txt` is created successfully and exists in the HDFS namespace before renaming the directory. 2. Verify that there are no issues with the HDFS namespace that could prevent the file from being recognized. 3. Check for any ongoing operations that might affect the file's state.",
            "problem_location": {
                "files": [
                    "FSNamesystem.java",
                    "FSImage.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem",
                    "org.apache.hadoop.hdfs.server.namenode.FSImage"
                ],
                "methods": [
                    "FSNamesystem.saveFilesUnderConstruction",
                    "FSImage.saveFSImage"
                ]
            },
            "possible_fix": "To resolve the issue, ensure that the file creation process is completed and the file is properly registered in the HDFS namespace before performing the rename operation. Additionally, consider adding checks to confirm the existence of files under construction before attempting to save the namespace."
        },
        "possible_fix_code": {
            "saveFilesUnderConstruction": "  void saveFilesUnderConstruction(DataOutputStream out) throws IOException {\n    // This is run by an inferior thread of saveNamespace, which holds a read\n    // lock on our behalf. If we took the read lock here, we could block\n    // for fairness if a writer is waiting on the lock.\n    synchronized (leaseManager) {\n      out.writeInt(leaseManager.countPath()); // write the size\n\n      for (Lease lease : leaseManager.getSortedLeases()) {\n        for(String path : lease.getPaths()) {\n          // verify that path exists in namespace\n          INode node;\n          try {\n            node = dir.getFileINode(path);\n          } catch (UnresolvedLinkException e) {\n            throw new AssertionError(\"Lease files should reside on this FS\");\n          }\n          if (node == null) {\n            throw new IOException(\"saveLeases found path \" + path +\n                                  \" but no matching entry in namespace.\");\n          }\n          if (!node.isUnderConstruction()) {\n            throw new IOException(\"saveLeases found path \" + path +\n                                  \" but is not under construction.\");\n          }\n          INodeFileUnderConstruction cons = (INodeFileUnderConstruction) node;\n          FSImageSerialization.writeINodeUnderConstruction(out, cons, path);\n        }\n      }\n    }\n    // Additional check to ensure that files under construction are properly registered\n    for (Lease lease : leaseManager.getSortedLeases()) {\n      for (String path : lease.getPaths()) {\n        INode node = dir.getFileINode(path);\n        if (node == null) {\n          throw new IOException(\"File under construction not found in namespace: \" + path);\n        }\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-11056.json",
        "creation_time": "2016-10-25T23:00:33.000+0000",
        "bug_report": {
            "Title": "Concurrent append and read operations lead to checksum error",
            "Description": "This issue arises when two clients interact with the same file concurrently: one client continuously appends data while the other reads from it. The reader eventually encounters a checksum error, indicating data corruption. The problem manifests after a few minutes of operation on a local Mac environment, affecting httpfs clients and potentially other append clients. The logs reveal multiple exceptions related to missing data blocks and checksum mismatches, suggesting underlying issues with data integrity and replication in HDFS.",
            "StackTrace": [
                "java.io.IOException: No data exists for block BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182",
                "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getBlockInputStream(FsDatasetImpl.java:773)",
                "org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:400)",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:581)",
                "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:150)",
                "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:102)",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:289)",
                "java.lang.RuntimeException: org.apache.hadoop.fs.ChecksumException: Checksum CRC32C not matched for file /tmp/bar.txt at position 0: expected=C893FEDE but computed=69322F90"
            ],
            "RootCause": "The root cause of the issue is a combination of missing data blocks and checksum mismatches during concurrent read and append operations. The `getBlockInputStream` method fails to retrieve the block due to its absence, leading to a checksum error when the reader attempts to validate the data.",
            "StepsToReproduce": [
                "1. Start two clients: one for appending data and another for reading from the same file.",
                "2. Continuously append data to the file using the first client.",
                "3. Simultaneously read from the file using the second client.",
                "4. Monitor for checksum errors in the reading client after a few minutes."
            ],
            "ExpectedBehavior": "The reading client should be able to read the data without encountering any checksum errors, ensuring data integrity during concurrent operations.",
            "ObservedBehavior": "The reading client encounters a checksum error, indicating that the data read does not match the expected checksum, suggesting data corruption.",
            "Suggestions": "To resolve these issues, ensure that data blocks are properly replicated and not deleted. Use HDFS tools to verify data integrity and repair any corrupted blocks. Additionally, check the replication factor settings to ensure sufficient replication for durability.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl"
                ],
                "methods": [
                    "getBlockInputStream",
                    "getStoredBlock",
                    "fetchReplicaInfo",
                    "getBlockReplica"
                ]
            },
            "possible_fix": "Review the implementation of the `getBlockInputStream` method to ensure it handles missing blocks gracefully. Implement checks to verify block availability before read operations and consider enhancing the replication strategy to prevent data loss during concurrent access."
        },
        "possible_fix_code": {
            "getBlockInputStream": "  public InputStream getBlockInputStream(ExtendedBlock b,\n      long seekOffset) throws IOException {\n\n    ReplicaInfo info;\n    synchronized(this) {\n      info = volumeMap.get(b.getBlockPoolId(), b.getLocalBlock());\n    }\n\n    if (info != null && info.getVolume().isTransientStorage()) {\n      ramDiskReplicaTracker.touch(b.getBlockPoolId(), b.getBlockId());\n      datanode.getMetrics().incrRamDiskBlocksReadHits();\n    }\n\n    // Check if the block data exists before attempting to read\n    if (info != null) {\n      if (info.blockDataExists()) {\n        return info.getDataInputStream(seekOffset);\n      } else {\n        throw new IOException(\"Block data does not exist for block \" + b);\n      }\n    } else {\n      throw new IOException(\"No data exists for block \" + b);\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-6825.json",
        "creation_time": "2014-08-06T01:31:34.000+0000",
        "bug_report": {
            "Title": "Edit log corruption due to delayed block removal",
            "Description": "The issue arises when a client attempts to append to a file (`/solr/hierarchy/core_node1/data/tlog/tlog.xyz`) that has expired its lease. This leads to a series of events where the file is deleted, but pending blocks remain, causing the `commitBlockSynchronization()` method to create an `InodeFile` unaware of the deletion. Consequently, a `FileNotFoundException` is thrown during the update of disk space, which is not handled properly, leading to corruption in the edit log.",
            "StackTrace": [
                "2014-08-04 23:49:44,133 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(lastblock=BP-.., newgenerationstamp=..., newlength=..., newtargets=..., closeFile=true, deleteBlock=false)",
                "2014-08-04 23:49:44,133 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Unexpected exception while updating disk space.",
                "java.io.FileNotFoundException: Path not found: /solr/hierarchy/core_node1/data/tlog/tlog.xyz",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateSpaceConsumed(FSDirectory.java:1807)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitOrCompleteLastBlock(FSNamesystem.java:3975)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.closeFileCommitBlocks(FSNamesystem.java:4178)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:4146)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization(NameNodeRpcServer.java:662)",
                "at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolServerSideTranslatorPB.java:270)",
                "at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:28073)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)"
            ],
            "RootCause": "The root cause of the issue is the failure to handle the deletion of a file while there are still pending blocks associated with it. This leads to an attempt to access a non-existent file, resulting in a `FileNotFoundException` during the update of disk space.",
            "StepsToReproduce": [
                "1. Create a file at the path `/solr/hierarchy/core_node1/data/tlog/tlog.xyz`.",
                "2. Attempt to append to the file and let the lease expire.",
                "3. Observe that the file is deleted while pending blocks remain.",
                "4. Trigger the `commitBlockSynchronization()` method."
            ],
            "ExpectedBehavior": "The system should properly handle the deletion of files and ensure that no pending blocks are left unprocessed, preventing any attempts to access non-existent files.",
            "ObservedBehavior": "The system throws a `FileNotFoundException` when trying to update disk space for a deleted file, leading to corruption in the edit log.",
            "Suggestions": "Review the lease management and block handling logic to ensure that pending blocks are cleared when a file is deleted. Implement proper error handling for `FileNotFoundException` to prevent corruption in the edit log.",
            "problem_location": {
                "files": [
                    "FSDirectory.java",
                    "FSNamesystem.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSDirectory",
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem"
                ],
                "methods": [
                    "FSDirectory.updateSpaceConsumed",
                    "FSNamesystem.commitOrCompleteLastBlock",
                    "FSNamesystem.commitBlockSynchronization"
                ]
            },
            "possible_fix": "Consider adding checks in the `commitBlockSynchronization()` method to verify the existence of the file before processing pending blocks. Additionally, ensure that the `FileNotFoundException` is properly handled to avoid writing invalid operations to the edit log."
        },
        "possible_fix_code": {
            "commitBlockSynchronization": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock=\" + lastblock\n             + \", newgenerationstamp=\" + newgenerationstamp\n             + \", newlength=\" + newlength\n             + \", newtargets=\" + Arrays.asList(newtargets)\n             + \", closeFile=\" + closeFile\n             + \", deleteBlock=\" + deleteblock + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src = \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock = getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock == null) {\n        if (deleteblock) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (=\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (=\" + lastblock + \") not found\");\n        }\n      }\n      INodeFile iFile = ((INode)storedBlock.getBlockCollection()).asFile();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (=\" + lastblock\n                    + \") since the file (=\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      // Check if the file still exists before proceeding\n      String path = iFile.getFullPathName();\n      if (path == null || !fileExists(path)) {\n        LOG.warn(\"File does not exist: \" + path);\n        return;\n      }\n\n      long recoveryId =\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId != newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel = ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove = iFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        ArrayList<DatanodeDescriptor> trimmedTargets =\n            new ArrayList<DatanodeDescriptor>(newtargets.length);\n        ArrayList<String> trimmedStorages =\n            new ArrayList<String>(newtargets.length);\n        if (newtargets.length > 0) {\n          for (int i = 0; i < newtargets.length; ++i) {\n            DatanodeDescriptor targetNode =\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode != null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (=\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) && !trimmedTargets.isEmpty()) {\n          for (int i = 0; i < trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo =\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo != null) {\n              storageInfo.addBlock(storedBlock);\n            }\n          }\n        }\n\n        DatanodeStorageInfo[] trimmedStorageInfos =\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n      }\n\n      if (closeFile) {\n        src = closeFileCommitBlocks(iFile, storedBlock);\n      } else {\n        src = iFile.getFullPathName();\n        persistBlocks(src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock=\" + lastblock\n          + \", file=\" + src\n          + \", newgenerationstamp=\" + newgenerationstamp\n          + \", newlength=\" + newlength\n          + \", newtargets=\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }\n\n  private boolean fileExists(String path) {\n    // Implement logic to check if the file exists in the filesystem\n    return true; // Placeholder for actual implementation\n  }"
        }
    },
    {
        "filename": "HDFS-5710.json",
        "creation_time": "2014-01-01T04:06:03.000+0000",
        "bug_report": {
            "Title": "FSDirectory#getFullPathName should check inodes against null",
            "Description": "The bug report highlights a critical issue in the Hadoop HDFS implementation where the method `FSDirectory.getFullPathName` does not adequately check for null `INode` references. This oversight leads to a `NullPointerException` when the method attempts to access properties of a null `INode`. The stack trace indicates that the exception occurs during the execution of the `getFullPathName` method, which calls `getFullPathINodes`, which in turn calls `getRelativePathINodes`. The latter method does not handle null `INode` references properly, resulting in the failure of the replication monitor thread.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFullPathName(FSDirectory.java:1871)",
                "at org.apache.hadoop.hdfs.server.namenode.INode.getFullPathName(INode.java:482)",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.getName(INodeFile.java:316)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.chooseTarget(BlockPlacementPolicy.java:118)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1259)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1167)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3158)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3112)",
                "at java.lang.Thread.run(Thread.java:724)"
            ],
            "RootCause": "The root cause of the `NullPointerException` is that the `getFullPathName` method in `FSDirectory` does not check if the `INode` returned by `getFullPathINodes` is null before proceeding to access its properties. This leads to an attempt to dereference a null object.",
            "StepsToReproduce": [
                "Trigger the replication monitor in the Hadoop HDFS environment.",
                "Ensure that the file corresponding to the `INode` has been deleted or is otherwise unavailable.",
                "Observe the logs for the `NullPointerException` in the `FSDirectory.getFullPathName` method."
            ],
            "ExpectedBehavior": "The system should handle null `INode` references gracefully, either by returning a meaningful error message or by skipping the operation without throwing an exception.",
            "ObservedBehavior": "The system throws a `NullPointerException` when attempting to access properties of a null `INode`, causing the replication monitor thread to fail.",
            "Suggestions": "Implement a null check for the `INode` in the `getFullPathName` method before proceeding with its operations. Additionally, ensure that the `getRelativePathINodes` method handles null `INode` references appropriately.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSDirectory"
                ],
                "methods": [
                    "FSDirectory.getFullPathName",
                    "FSDirectory.getFullPathINodes",
                    "FSDirectory.getRelativePathINodes"
                ]
            },
            "possible_fix": "Add a null check in the `getFullPathName` method as follows:\n\n```java\nstatic String getFullPathName(INode inode) {\n    if (inode == null) {\n        NameNode.stateChangeLog.warn(\"INode is null, cannot get full path.\");\n        return null;\n    }\n    INode[] inodes = getFullPathINodes(inode);\n    return getFullPathName(inodes, inodes.length - 1);\n}\n```"
        },
        "possible_fix_code": {
            "FSDirectory.getFullPathName": "  static String getFullPathName(INode inode) {\n    if (inode == null) {\n        NameNode.stateChangeLog.warn(\"INode is null, cannot get full path.\");\n        return null;\n    }\n    INode[] inodes = getFullPathINodes(inode);\n    return getFullPathName(inodes, inodes.length - 1);\n  }"
        }
    },
    {
        "filename": "HDFS-3555.json",
        "creation_time": "2012-06-20T17:36:48.000+0000",
        "bug_report": {
            "Title": "Idle client socket triggers DN ERROR log (should be INFO or DEBUG)",
            "Description": "The Datanode service is logging a `java.net.SocketTimeoutException` at the ERROR level when it fails to send data to a client that has stopped reading. This exception indicates that the datanode was unable to write to the socket channel within the specified timeout of 480 seconds. Given that this situation is not critical and does not indicate a failure in the datanode's operation, it should be logged at the INFO or DEBUG level instead of ERROR.",
            "StackTrace": [
                "2012-06-18 17:47:13 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode DatanodeRegistration(x.x.x.x:50010, storageID=DS-196671195-10.10.120.67-50010-1334328338972, infoPort=50075, ipcPort=50020):DataXceiver",
                "java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.10.120.67:50010 remote=/10.10.120.67:59282]",
                "at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:246)",
                "at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:159)",
                "at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:198)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendChunks(BlockSender.java:397)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:493)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:267)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:163)"
            ],
            "RootCause": "The root cause of the issue is a `SocketTimeoutException` that occurs when the datanode attempts to write data to a socket channel that is not ready for writing within the specified timeout period. This can be attributed to various factors such as network issues, high server load, or misconfiguration in the Hadoop environment.",
            "StepsToReproduce": [
                "1. Start the Datanode service.",
                "2. Connect a client to the Datanode.",
                "3. Stop the client from reading data.",
                "4. Monitor the Datanode logs for ERROR level messages."
            ],
            "ExpectedBehavior": "The Datanode should log the socket timeout situation at the INFO or DEBUG level, indicating that the client has stopped reading data, rather than logging it as an ERROR.",
            "ObservedBehavior": "The Datanode logs a `SocketTimeoutException` at the ERROR level, indicating a timeout while waiting for the socket channel to be ready for writing.",
            "Suggestions": "Change the logging level for `SocketTimeoutException` in the Datanode service from ERROR to INFO or DEBUG to reflect that this situation is not critical.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.java",
                    "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.SocketIOWithTimeout.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.BlockSender",
                    "org.apache.hadoop.net.SocketIOWithTimeout"
                ],
                "methods": [
                    "BlockSender.sendChunks",
                    "BlockSender.sendBlock",
                    "SocketIOWithTimeout.waitForIO"
                ]
            },
            "possible_fix": "Modify the logging level in the `BlockSender` class where the `SocketTimeoutException` is caught to log at INFO or DEBUG level instead of ERROR. This can be done by changing the log statement in the `sendChunks` or `sendBlock` methods."
        },
        "possible_fix_code": {
            "BlockSender.sendBlock": "long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    if (out == null) {\n      throw new IOException( \"out stream is null\" );\n    }\n    initialOffset = offset;\n    long totalRead = 0;\n    OutputStream streamForSendChunks = out;\n    \n    lastCacheDropOffset = initialOffset;\n\n    if (isLongRead() && blockInFd != null) {\n      // Advise that this file descriptor will be accessed sequentially.\n      NativeIO.posixFadviseIfPossible(blockInFd, 0, 0, NativeIO.POSIX_FADV_SEQUENTIAL);\n    }\n    \n    // Trigger readahead of beginning of file if configured.\n    manageOsCache();\n\n    final long startTime = ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n    try {\n      int maxChunksPerPacket;\n      int pktSize = PacketHeader.PKT_HEADER_LEN;\n      boolean transferTo = transferToAllowed && !verifyChecksum\n          && baseStream instanceof SocketOutputStream\n          && blockIn instanceof FileInputStream;\n      if (transferTo) {\n        FileChannel fileChannel = ((FileInputStream)blockIn).getChannel();\n        blockInPosition = fileChannel.position();\n        streamForSendChunks = baseStream;\n        maxChunksPerPacket = numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n        \n        // Smaller packet size to only hold checksum when doing transferTo\n        pktSize += checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket = Math.max(1,\n            numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n        // Packet size includes both checksum and data\n        pktSize += (chunkSize + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf = ByteBuffer.allocate(pktSize);\n\n      while (endOffset > offset) {\n        manageOsCache();\n        long len = sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n            transferTo, throttler);\n        offset += len;\n        totalRead += len + (numberOfChunks(len) * checksumSize);\n        seqno++;\n      }\n      try {\n        // send an empty packet to mark the end of the block\n        sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n            throttler);\n        out.flush();\n      } catch (IOException e) { //socket error\n        throw ioeToSocketException(e);\n      }\n\n      sentEntireByteRange = true;\n    } finally {\n      if (clientTraceFmt != null) {\n        final long endTime = System.nanoTime();\n        ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n            initialOffset, endTime - startTime));\n      }\n      close();\n    }\n    return totalRead;\n  }"
        }
    },
    {
        "filename": "HDFS-12363.json",
        "creation_time": "2017-08-28T05:34:21.000+0000",
        "bug_report": {
            "Title": "Possible NPE in BlockManager$StorageInfoDefragmenter#scanAndCompactStorages",
            "Description": "A NullPointerException (NPE) was observed in the BlockManager's StorageInfoDefragmenter during the execution of the scanAndCompactStorages method. The error indicates that the code is attempting to access an uninitialized object or variable, which is a common issue in Java applications. The stack trace shows that the exception occurs at line 3897 in BlockManager.java, specifically when trying to retrieve storage information for a datanode.",
            "StackTrace": [
                "ERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Thread received Runtime exception.",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.scanAndCompactStorages(BlockManager.java:3897)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.run(BlockManager.java:3852)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2017-08-21 22:14:05,303 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1",
                "2017-08-21 22:14:05,313 INFO org.apache.hadoop.hdfs.server.namenode.NameNode:"
            ],
            "RootCause": "The NullPointerException is likely caused by an attempt to access a storage object that has not been initialized. This could be due to missing initialization of storage components or incorrect assumptions about the state of the datanode or its storage information.",
            "StepsToReproduce": [
                "1. Start the Hadoop HDFS server.",
                "2. Trigger the scanAndCompactStorages method in the BlockManager.",
                "3. Monitor the logs for any NullPointerException errors."
            ],
            "ExpectedBehavior": "The scanAndCompactStorages method should successfully retrieve and process storage information for all datanodes without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the BlockManager to fail during the execution of the scanAndCompactStorages method.",
            "Suggestions": "Check the initialization of storage components in the BlockManager. Ensure that all necessary configurations are set correctly. Review any recent changes to the code or environment that might have affected the state of the BlockManager.",
            "problem_location": {
                "files": [
                    "BlockManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager",
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter"
                ],
                "methods": [
                    "BlockManager$StorageInfoDefragmenter.scanAndCompactStorages"
                ]
            },
            "possible_fix": "Ensure that the datanodeManager and its associated storage information are properly initialized before accessing them in the scanAndCompactStorages method. This may involve adding null checks or initializing storage objects appropriately."
        },
        "possible_fix_code": {
            "scanAndCompactStorages": "    private void scanAndCompactStorages() throws InterruptedException {\n      ArrayList<String> datanodesAndStorages = new ArrayList<>();\n      for (DatanodeDescriptor node : datanodeManager.getDatanodeListForReport(DatanodeReportType.ALL)) {\n        if (node == null) continue; // Check for null node\n        for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n          if (storage == null) continue; // Check for null storage\n          try {\n            namesystem.readLock();\n            double ratio = storage.treeSetFillRatio();\n            if (ratio < storageInfoDefragmentRatio) {\n              datanodesAndStorages.add(node.getDatanodeUuid());\n              datanodesAndStorages.add(storage.getStorageID());\n            }\n            LOG.info(\"StorageInfo TreeSet fill ratio {} : {}{}\", storage.getStorageID(), ratio,\n                     (ratio < storageInfoDefragmentRatio) ? \" (queued for defragmentation)\" : \"\");\n          } finally {\n            namesystem.readUnlock();\n          }\n        }\n      }\n      if (!datanodesAndStorages.isEmpty()) {\n        for (int i = 0; i < datanodesAndStorages.size(); i += 2) {\n          namesystem.writeLock();\n          try {\n            DatanodeStorageInfo storage = datanodeManager.getDatanode(datanodesAndStorages.get(i)).getStorageInfo(datanodesAndStorages.get(i + 1));\n            if (storage != null) {\n              boolean aborted = !storage.treeSetCompact(storageInfoDefragmentTimeout);\n              if (aborted) {\n                // Compaction timed out, reset iterator to continue with the same storage next iteration.\n                i -= 2;\n              }\n              LOG.info(\"StorageInfo TreeSet defragmented {} : {}{}\", storage.getStorageID(), storage.treeSetFillRatio(), aborted ? \" (aborted)\" : \"\");\n            }\n          } finally {\n            namesystem.writeUnlock();\n          }\n          // Wait between each iteration\n          Thread.sleep(1000);\n        }\n      }\n    }"
        }
    },
    {
        "filename": "HDFS-7916.json",
        "creation_time": "2015-03-11T10:40:00.000+0000",
        "bug_report": {
            "Title": "'reportBadBlocks' from datanodes to standby Node BPServiceActor goes for infinite loop",
            "Description": "The issue arises when the BPServiceActor for the StandbyNode attempts to report a bad block to the namenode. If any bad block is detected, the reporting process enters an infinite loop due to repeated failures in the reporting mechanism. The stack trace indicates that the failure occurs in the `ReportBadBlockAction.reportTo` method, which is responsible for sending bad block reports to the namenode. The failure is likely caused by an `IOException` during the call to `bpNamenode.reportBadBlocks(locatedBlock)`, which can occur if the namenode is in a state that does not allow write operations, such as being in safe mode.",
            "StackTrace": [
                "2015-03-11 19:43:41,528 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to report bad block BP-1384821822-10.224.54.68-1422634566395:blk_1079544278_5812006 to namenode: stobdtserver3/10.224.54.70:18010",
                "org.apache.hadoop.hdfs.server.datanode.BPServiceActorActionException: Failed to report bad block BP-1384821822-10.224.54.68-1422634566395:blk_1079544278_5812006 to namenode:",
                "at org.apache.hadoop.hdfs.server.datanode.ReportBadBlockAction.reportTo(ReportBadBlockAction.java:63)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processQueueMessages(BPServiceActor.java:1020)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:762)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:856)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is an `IOException` thrown during the execution of the `reportTo` method in the `ReportBadBlockAction` class, specifically when attempting to call `bpNamenode.reportBadBlocks(locatedBlock)`. This can happen if the namenode is in safe mode or if there are issues with the block manager when trying to mark the block as corrupt.",
            "StepsToReproduce": [
                "1. Introduce a bad block in the HDFS environment.",
                "2. Monitor the BPServiceActor for the StandbyNode.",
                "3. Observe the logs for repeated attempts to report the bad block."
            ],
            "ExpectedBehavior": "The system should successfully report the bad block to the namenode without entering an infinite loop, and the block should be marked as corrupt in the block manager.",
            "ObservedBehavior": "The system enters an infinite loop attempting to report the bad block, resulting in repeated warnings in the logs and failure to mark the block as corrupt.",
            "Suggestions": "Check the state of the namenode to ensure it is not in safe mode. Additionally, review the implementation of the `reportBadBlocks` method in the `FSNamesystem` class to ensure it handles exceptions properly and does not lead to infinite retries.",
            "problem_location": {
                "files": [
                    "ReportBadBlockAction.java",
                    "BPServiceActor.java",
                    "FSNamesystem.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.ReportBadBlockAction",
                    "org.apache.hadoop.hdfs.server.datanode.BPServiceActor",
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem"
                ],
                "methods": [
                    "ReportBadBlockAction.reportTo",
                    "BPServiceActor.processQueueMessages",
                    "FSNamesystem.reportBadBlocks"
                ]
            },
            "possible_fix": "In the `reportTo` method, add checks to handle the case when the namenode is in safe mode or when there are issues with the block manager. Implement a backoff strategy for retries to prevent infinite loops."
        },
        "possible_fix_code": {
            "ReportBadBlockAction.reportTo": "  public void reportTo(DatanodeProtocolClientSideTranslatorPB bpNamenode, \n    DatanodeRegistration bpRegistration) throws BPServiceActorActionException {\n    if (bpRegistration == null) {\n      return;\n    }\n    DatanodeInfo[] dnArr = { new DatanodeInfo(bpRegistration) };\n    String[] uuids = { storageUuid };\n    StorageType[] types = { storageType };\n    LocatedBlock[] locatedBlock = { new LocatedBlock(block,\n        dnArr, uuids, types) };\n\n    int retryCount = 0;\n    final int maxRetries = 5;\n    while (retryCount < maxRetries) {\n      try {\n        bpNamenode.reportBadBlocks(locatedBlock);\n        return; // Successfully reported\n      } catch (IOException e) {\n        retryCount++;\n        if (retryCount >= maxRetries) {\n          throw new BPServiceActorActionException(\"Failed to report bad block \"\n              + block + \" to namenode after \" + maxRetries + \" attempts: \", e);\n        }\n        // Optional: Implement a backoff strategy here if needed\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-9549.json",
        "creation_time": "2015-12-11T16:25:08.000+0000",
        "bug_report": {
            "Title": "TestCacheDirectives#testExceedsCapacity is flaky",
            "Description": "The test case `TestCacheDirectives.testExceedsCapacity` is failing intermittently in Jenkins for the trunk and trunk-Java8 branches. The failure occurs due to an assertion error indicating that the pending cached list for the DataNode at `127.0.0.1:54134` is not empty when it was expected to be. This suggests that the caching mechanism is not properly clearing or processing cache directives, particularly when the cache exceeds its capacity.",
            "StackTrace": [
                "java.lang.AssertionError: Pending cached list of 127.0.0.1:54134 is not empty, [{blockId=1073741841, replication=1, mark=true}]",
                "\tat org.junit.Assert.fail(Assert.java:88)",
                "\tat org.junit.Assert.assertTrue(Assert.java:41)",
                "\tat org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.checkPendingCachedEmpty(TestCacheDirectives.java:1479)",
                "\tat org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testExceedsCapacity(TestCacheDirectives.java:1502)"
            ],
            "RootCause": "The root cause of the issue appears to be related to the `processCacheReportImpl` method in the `CacheManager` class, which is responsible for managing the pending cached blocks. If this method does not execute correctly or is not called, the pending cached list may not be cleared, leading to the assertion failure in the test case.",
            "StepsToReproduce": [
                "Run the test case `TestCacheDirectives.testExceedsCapacity` in a Jenkins environment.",
                "Ensure that the caching mechanism is under load to exceed its capacity.",
                "Observe the test results for intermittent failures."
            ],
            "ExpectedBehavior": "The pending cached list should be empty after processing cache directives, indicating that all cached blocks have been successfully managed and cleared.",
            "ObservedBehavior": "The pending cached list is not empty, resulting in an assertion failure during the test execution.",
            "Suggestions": "Investigate the `processCacheReportImpl` method in the `CacheManager` class to ensure that it is correctly processing and clearing the pending cached blocks. Additionally, review the test setup to confirm that it accurately simulates the conditions under which the cache report is processed.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.CacheManager",
                    "org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives"
                ],
                "methods": [
                    "CacheManager.processCacheReportImpl",
                    "TestCacheDirectives.checkPendingCachedEmpty"
                ]
            },
            "possible_fix": "Review and potentially modify the logic in the `processCacheReportImpl` method to ensure that it correctly handles the addition and removal of cached blocks from the pending list. Ensure that the method is invoked appropriately during the test execution."
        },
        "possible_fix_code": {
            "CacheManager.processCacheReportImpl": "  private void processCacheReportImpl(final DatanodeDescriptor datanode,\n      final List<Long> blockIds) {\n    CachedBlocksList cached = datanode.getCached();\n    cached.clear();\n    CachedBlocksList cachedList = datanode.getCached();\n    CachedBlocksList pendingCachedList = datanode.getPendingCached();\n    for (Iterator<Long> iter = blockIds.iterator(); iter.hasNext(); ) {\n      long blockId = iter.next();\n      LOG.trace(\"Cache report from datanode {} has block {}\", datanode,\n          blockId);\n      CachedBlock cachedBlock =\n          new CachedBlock(blockId, (short)0, false);\n      CachedBlock prevCachedBlock = cachedBlocks.get(cachedBlock);\n      // Add the block ID from the cache report to the cachedBlocks map\n      // if it's not already there.\n      if (prevCachedBlock != null) {\n        cachedBlock = prevCachedBlock;\n      } else {\n        cachedBlocks.put(cachedBlock);\n        LOG.trace(\"Added block {} to cachedBlocks\", cachedBlock);\n      }\n      // Add the block to the datanode's implicit cached block list\n      // if it's not already there.  Similarly, remove it from the pending\n      // cached block list if it exists there.\n      if (!cachedBlock.isPresent(cachedList)) {\n        cachedList.add(cachedBlock);\n        LOG.trace(\"Added block {} to CACHED list.\", cachedBlock);\n      }\n      if (cachedBlock.isPresent(pendingCachedList)) {\n        pendingCachedList.remove(cachedBlock);\n        LOG.trace(\"Removed block {} from PENDING_CACHED list.\", cachedBlock);\n      }\n    }\n    // Ensure that the pending cached list is cleared after processing\n    if (pendingCachedList.isEmpty()) {\n      LOG.info(\"Pending cached list for datanode {} is now empty.\", datanode);\n    } else {\n      LOG.warn(\"Pending cached list for datanode {} is not empty after processing: {}\", datanode, pendingCachedList);\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-11164.json",
        "creation_time": "2016-11-22T05:58:03.000+0000",
        "bug_report": {
            "Title": "Mover should avoid unnecessary retries if the block is pinned",
            "Description": "The Mover component in the Hadoop Distributed File System (HDFS) is currently configured to retry moving blocks that are pinned, leading to unnecessary failure attempts. When a pinned block is targeted for movement, an IOException is thrown, indicating that the block cannot be copied due to its pinned status. This behavior is exacerbated by the configuration setting {{dfs.mover.retry.max.attempts}}, which causes the Mover to continue retrying until the maximum number of attempts is reached, despite the fact that pinned blocks cannot be moved. This report aims to address the need for the Mover to recognize pinned blocks and avoid retrying their movement, thereby improving efficiency and reducing log clutter.",
            "StackTrace": [
                "2016-11-22 10:56:10,537 WARN org.apache.hadoop.hdfs.server.balancer.Dispatcher: Failed to move blk_1073741825_1001 with size=52 from 127.0.0.1:19501:DISK to 127.0.0.1:19758:ARCHIVE through 127.0.0.1:19501",
                "java.io.IOException: Got error, status=ERROR, status message opReplaceBlock BP-1772076264-10.252.146.200-1479792322960:blk_1073741825_1001 received exception java.io.IOException: Got error, status=ERROR, status message Not able to copy block 1073741825 to /127.0.0.1:19826 because it's pinned , copy block BP-1772076264-10.252.146.200-1479792322960:blk_1073741825_1001 from /127.0.0.1:19501, reportedBlock move is failed",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:118)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.receiveResponse(Dispatcher.java:417)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.dispatch(Dispatcher.java:358)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.access$5(Dispatcher.java:322)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$1.run(Dispatcher.java:1075)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the Mover does not currently check if a block is pinned before attempting to move it. This results in repeated IOException errors and unnecessary retries, as pinned blocks cannot be moved.",
            "StepsToReproduce": [
                "1. Configure the Mover with a retry limit using dfs.mover.retry.max.attempts.",
                "2. Pin a block in the HDFS.",
                "3. Attempt to move the pinned block to another datanode.",
                "4. Observe the repeated IOException errors in the logs."
            ],
            "ExpectedBehavior": "The Mover should recognize that a block is pinned and avoid attempting to move it, thus preventing any IOException related to block movement failures.",
            "ObservedBehavior": "The Mover attempts to move a pinned block, resulting in repeated IOException errors and unnecessary retries, cluttering the logs.",
            "Suggestions": "Implement a check in the Mover's logic to determine if a block is pinned before attempting to move it. If the block is pinned, the Mover should skip the movement attempt and log a warning instead of retrying.",
            "problem_location": {
                "files": [
                    "org/apache/hadoop/hdfs/server/balancer/Dispatcher.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.balancer.Dispatcher"
                ],
                "methods": [
                    "Dispatcher$PendingMove.receiveResponse",
                    "Dispatcher$PendingMove.dispatch"
                ]
            },
            "possible_fix": "Modify the Mover's logic in the Dispatcher class to include a check for the pinned status of blocks before attempting to move them. This could involve adding a method call to check the block's status and conditionally skip the move operation if the block is pinned."
        },
        "possible_fix_code": {
            "Dispatcher$PendingMove.receiveResponse": "    private void receiveResponse(DataInputStream in) throws IOException {\n      long startTime = Time.monotonicNow();\n      BlockOpResponseProto response =\n          BlockOpResponseProto.parseFrom(vintPrefixed(in));\n      while (response.getStatus() == Status.IN_PROGRESS) {\n        // read intermediate responses\n        response = BlockOpResponseProto.parseFrom(vintPrefixed(in));\n        // Stop waiting for slow block moves. Even if it stops waiting,\n        // the actual move may continue.\n        if (stopWaitingForResponse(startTime)) {\n          throw new IOException(\"Block move timed out\");\n        }\n      }\n      // Check if the block is pinned before proceeding\n      if (isBlockPinned(reportedBlock)) {\n        LOG.warn(\"Skipping move for pinned block: \" + reportedBlock);\n        return; // Skip the move if the block is pinned\n      }\n      String logInfo = \"reportedBlock move is failed\";\n      DataTransferProtoUtil.checkBlockOpStatus(response, logInfo);\n    }\n\n    private boolean isBlockPinned(Block block) {\n      // Implement logic to check if the block is pinned\n      // This is a placeholder; actual implementation may vary\n      return block.isPinned(); // Assuming Block class has isPinned method\n    }"
        }
    },
    {
        "filename": "HDFS-5291.json",
        "creation_time": "2013-10-02T23:52:02.000+0000",
        "bug_report": {
            "Title": "Clients need to retry when Active NN is in SafeMode",
            "Description": "During testing, it was observed that the NameNode (NN) transitions to SafeMode immediately after becoming active. This behavior can lead to HBase region servers timing out and subsequently shutting down. To enhance system resilience, clients should be allowed to retry operations when High Availability (HA) is enabled and the Active NameNode is in SafeMode. The issue is exacerbated by an InterruptedException occurring in the EditLogTailer, which is responsible for processing edit logs in a multi-threaded environment.",
            "StackTrace": [
                "java.lang.InterruptedException: sleep interrupted",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:334)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:279)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:296)"
            ],
            "RootCause": "The root cause of the issue is the InterruptedException thrown in the EditLogTailer class, which indicates that a thread was interrupted while it was sleeping. This interruption prevents the EditLogTailer from processing edit logs correctly, leading to the NameNode entering SafeMode prematurely.",
            "StepsToReproduce": [
                "1. Start the NameNode in active state.",
                "2. Monitor the transition to SafeMode.",
                "3. Observe the logs for any InterruptedException in the EditLogTailer."
            ],
            "ExpectedBehavior": "When the NameNode transitions to active state, it should not enter SafeMode immediately unless there is a legitimate reason. Clients should be able to retry their operations without encountering timeouts.",
            "ObservedBehavior": "The NameNode enters SafeMode immediately after transitioning to active state, causing HBase region servers to timeout and shut down.",
            "Suggestions": "Implement a retry mechanism for clients when the Active NameNode is in SafeMode. Additionally, review the handling of InterruptedException in the EditLogTailer to ensure it can recover gracefully from interruptions.",
            "problem_location": {
                "files": [
                    "EditLogTailer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer"
                ],
                "methods": [
                    "EditLogTailer.doWork"
                ]
            },
            "possible_fix": "Review the EditLogTailer class to ensure that InterruptedException is handled properly. Consider adding logic to allow the EditLogTailer to resume its work after an interruption, or to log the interruption and clean up resources appropriately."
        },
        "possible_fix_code": {
            "doWork": "    private void doWork() {\n      while (shouldRun) {\n        try {\n          // There's no point in triggering a log roll if the Standby hasn't\n          // read any more transactions since the last time a roll was\n          // triggered. \n          if (tooLongSinceLastLoad() &&\n              lastRollTriggerTxId < lastLoadedTxnId) {\n            triggerActiveLogRoll();\n          }\n          /**\n           * Check again in case someone calls {@link EditLogTailer#stop} while\n           * we're triggering an edit log roll, since ipc.Client catches and\n           * ignores {@link InterruptedException} in a few places. This fixes\n           * the bug described in HDFS-2823.\n           */\n          if (!shouldRun) {\n            break;\n          }\n          doTailEdits();\n        } catch (EditLogInputException elie) {\n          LOG.warn(\"Error while reading edits from disk. Will try again.\", elie);\n        } catch (InterruptedException ie) {\n          // Log the interruption and clean up resources\n          LOG.warn(\"Edit log tailer interrupted, cleaning up and retrying...\");\n          // Optionally, you can set shouldRun to false if you want to stop\n          // the thread on interruption, or handle it as needed.\n          continue;\n        } catch (Throwable t) {\n          LOG.fatal(\"Unknown error encountered while tailing edits. \" +\n              \"Shutting down standby NN.\", t);\n          terminate(1, t);\n        }\n\n        try {\n          Thread.sleep(sleepTimeMs);\n        } catch (InterruptedException e) {\n          LOG.warn(\"Edit log tailer interrupted during sleep\", e);\n          // Restore the interrupt status\n          Thread.currentThread().interrupt();\n        }\n      }\n    }"
        }
    },
    {
        "filename": "HDFS-12836.json",
        "creation_time": "2017-11-17T20:26:32.000+0000",
        "bug_report": {
            "Title": "startTxId could be greater than endTxId when tailing in-progress edit log",
            "Description": "When the configuration {{dfs.ha.tail-edits.in-progress}} is set to true, the edit log tailer attempts to process in-progress edit log segments. However, there is a potential flaw in the code where the start transaction ID retrieved from the remote log can exceed the end transaction ID. This situation can lead to exceptions during the replay of the edit log, as indicated by the stack trace errors. Specifically, the method responsible for reading the edit logs may encounter a premature end-of-file condition, resulting in transaction ID mismatches.",
            "StackTrace": [
                "2017-11-17 19:55:41,165 ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: Error replaying edit log at offset 1048576.  Expected transaction ID was 87",
                "org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$PrematureEOFException: got premature end-of-file at txid 86; expected file to go up to 85",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:197)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:205)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:293)"
            ],
            "RootCause": "The root cause of the issue is that the start transaction ID from the remote log can be greater than the end transaction ID, leading to a mismatch during the edit log replay process. This results in exceptions such as PrematureEOFException and EditLogInputException, indicating that the edit log file ended unexpectedly before reaching the expected transaction ID.",
            "StepsToReproduce": [
                "Set the configuration dfs.ha.tail-edits.in-progress to true.",
                "Initiate the edit log tailer.",
                "Ensure that the remote log has in-progress segments with transaction IDs that can lead to startTxId being greater than endTxId.",
                "Observe the logs for errors related to transaction ID mismatches."
            ],
            "ExpectedBehavior": "The edit log tailer should successfully process all edit log segments without encountering transaction ID mismatches or premature end-of-file errors.",
            "ObservedBehavior": "The edit log tailer fails with errors indicating that the expected transaction ID does not match the actual transaction ID found, leading to exceptions during the log replay process.",
            "Suggestions": "To resolve this issue, consider implementing checks to ensure that the start transaction ID is always less than or equal to the end transaction ID before processing the edit logs. Additionally, verify the integrity of the edit log files and ensure that the NameNode is in a consistent state.",
            "problem_location": {
                "files": [
                    "EditLogTailer.java",
                    "FSImage.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer",
                    "org.apache.hadoop.hdfs.server.namenode.FSImage"
                ],
                "methods": [
                    "EditLogTailer.doTailEdits",
                    "FSImage.loadEdits"
                ]
            },
            "possible_fix": "Add validation logic in the EditLogTailer class to check that remoteLog.getStartTxId() is less than or equal to endTxId before proceeding with the edit log processing. This can prevent the occurrence of transaction ID mismatches."
        },
        "possible_fix_code": {
            "EditLogTailer.doTailEdits": "  void doTailEdits() throws IOException, InterruptedException {\n    namesystem.writeLockInterruptibly();\n    try {\n      FSImage image = namesystem.getFSImage();\n\n      long lastTxnId = image.getLastAppliedTxId();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"lastTxnId: \" + lastTxnId);\n      }\n      Collection<EditLogInputStream> streams;\n      try {\n        streams = editLog.selectInputStreams(lastTxnId + 1, 0,\n            null, inProgressOk, true);\n      } catch (IOException ioe) {\n        LOG.warn(\"Edits tailer failed to find any streams. Will try again \" +\n            \"later.\", ioe);\n        return;\n      }\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"edit streams to load from: \" + streams.size());\n      }\n\n      long editsLoaded = 0;\n      for (EditLogInputStream stream : streams) {\n        long startTxId = stream.getLastTxId();\n        if (startTxId > lastTxnId) {\n          LOG.error(\"Start transaction ID \" + startTxId + \" is greater than last applied transaction ID \" + lastTxnId + \". Skipping this stream.\");\n          continue;\n        }\n        try {\n          editsLoaded += image.loadEdits(stream, lastTxnId + 1, namesystem);\n        } catch (EditLogInputException elie) {\n          editsLoaded += elie.getNumEditsLoaded();\n          throw elie;\n        }\n      }\n\n      if (editsLoaded > 0) {\n        lastLoadTimeMs = monotonicNow();\n      }\n      lastLoadedTxnId = image.getLastAppliedTxId();\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-8113.json",
        "creation_time": "2015-04-09T11:29:31.000+0000",
        "bug_report": {
            "Title": "Add check for null BlockCollection pointers in BlockInfoContiguous structures",
            "Description": "The copy constructor for the BlockInfoContiguous class can throw a NullPointerException if the BlockCollection (bc) is null. This issue has been observed in DataNodes that fail to report blocks to the NameNode. The stack trace indicates that the NullPointerException occurs in the BlockInfo constructor, which is called during block reporting. The problem persists even in versions later than the one currently in use.",
            "StackTrace": [
                "2015-03-08 19:28:13,442 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: RemoteException in offerService",
                "org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo.(BlockInfo.java:80)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt.(BlockManager.java:1696)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.checkReplicaCorrupt(BlockManager.java:2185)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReportedBlock(BlockManager.java:2047)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.reportDiff(BlockManager.java:1950)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:1823)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:1750)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.blockReport(NameNodeRpcServer.java:1069)",
                "at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.blockReport(DatanodeProtocolServerSideTranslatorPB.java:152)",
                "at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:26382)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1623)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)"
            ],
            "RootCause": "The root cause of the NullPointerException is the attempt to access the BlockCollection (bc) in the BlockInfoContiguous constructor when it is null. This can occur if the BlockInfoContiguous instance is created without a valid BlockCollection reference.",
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with DataNodes.",
                "2. Ensure that a DataNode has a BlockCollection that is null.",
                "3. Trigger a block report from the DataNode to the NameNode.",
                "4. Observe the logs for a NullPointerException in the BlockInfo constructor."
            ],
            "ExpectedBehavior": "The system should handle null BlockCollection references gracefully, either by initializing them or by preventing the creation of BlockInfoContiguous instances with null references.",
            "ObservedBehavior": "A NullPointerException is thrown when the BlockInfo constructor is invoked with a null BlockCollection, causing block reports to fail.",
            "Suggestions": "Implement a null check for the BlockCollection in the BlockInfoContiguous constructor to prevent the NullPointerException. Additionally, ensure that all BlockCollection references are properly initialized before use.",
            "problem_location": {
                "files": [
                    "BlockInfo.java",
                    "BlockManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo",
                    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager"
                ],
                "methods": [
                    "BlockInfo.BlockInfoContiguous",
                    "BlockManager.checkReplicaCorrupt",
                    "BlockManager.processReportedBlock"
                ]
            },
            "possible_fix": "In the BlockInfoContiguous constructor, add a check for null BlockCollection: \n\nprotected BlockInfoContiguous(BlockInfoContiguous from) {\n    if (from.bc == null) {\n        throw new IllegalArgumentException(\"BlockCollection cannot be null\");\n    }\n    this.bc = from.bc;\n    this.bc.getBlockReplication();\n}"
        },
        "possible_fix_code": {
            "BlockInfo.BlockInfoContiguous": "protected BlockInfoContiguous(BlockInfoContiguous from) {\n    if (from.bc == null) {\n        throw new IllegalArgumentException(\"BlockCollection cannot be null\");\n    }\n    this.bc = from.bc;\n    this.bc.getBlockReplication();\n}"
        }
    },
    {
        "filename": "HDFS-10512.json",
        "creation_time": "2016-06-09T14:34:04.000+0000",
        "bug_report": {
            "Title": "VolumeScanner may terminate due to NPE in DataNode.reportBadBlocks",
            "Description": "The VolumeScanner component in the Hadoop HDFS may terminate unexpectedly due to a NullPointerException (NPE) thrown in the DataNode's reportBadBlocks method. This issue has been observed in a production environment running CDH 5.5.1 and persists in the upstream trunk. The stack trace indicates that the NPE occurs when the method attempts to report bad blocks, specifically when it tries to access the volume associated with a block that cannot be found.",
            "StackTrace": [
                "2016-04-07 20:30:53,830 WARN org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Reporting bad BP-1800173197-10.204.68.5-1444425156296:blk_1170134484_96468685 on /dfs/dn",
                "2016-04-07 20:30:53,831 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/dfs/dn, DS-89b72832-2a8c-48f3-8235-48e6c5eb5ab3) exiting because of exception",
                "java.lang.NullPointerException",
                "        at org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:1018)",
                "        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler.handle(VolumeScanner.java:287)",
                "        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.scanBlock(VolumeScanner.java:443)",
                "        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:547)",
                "        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:621)",
                "2016-04-07 20:30:53,832 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/dfs/dn, DS-89b72832-2a8c-48f3-8235-48e6c5eb5ab3) exiting."
            ],
            "RootCause": "The NullPointerException in the reportBadBlocks method is likely caused by either the getBPOSForBlock method returning null (indicating no associated block pool offer service) or the getFSDataset().getVolume(block) method returning null (indicating that the volume for the block cannot be found). Additionally, if the getFSDataset() method itself returns null due to an uninitialized data field in the DataNode class, this would also lead to the NPE.",
            "StepsToReproduce": [
                "1. Start the VolumeScanner on a DataNode.",
                "2. Ensure that there are blocks that are marked as bad.",
                "3. Trigger the reporting of bad blocks to observe the NullPointerException."
            ],
            "ExpectedBehavior": "The VolumeScanner should successfully report bad blocks without terminating unexpectedly.",
            "ObservedBehavior": "The VolumeScanner terminates with a NullPointerException when attempting to report bad blocks.",
            "Suggestions": "Check the initialization of the data field in the DataNode class to ensure it is not null. Additionally, validate the implementation of getBPOSForBlock and getFSDataset().getVolume to ensure they handle cases where the block or volume cannot be found appropriately.",
            "problem_location": {
                "files": [
                    "DataNode.java",
                    "VolumeScanner.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.DataNode",
                    "org.apache.hadoop.hdfs.server.datanode.VolumeScanner"
                ],
                "methods": [
                    "DataNode.reportBadBlocks",
                    "DataNode.getBPOSForBlock",
                    "DataNode.getFSDataset"
                ]
            },
            "possible_fix": "Ensure that the getFSDataset() method in DataNode is properly initialized. Add null checks for the return values of getBPOSForBlock and getFSDataset().getVolume before using them in reportBadBlocks. Example code modification:\n\npublic void reportBadBlocks(ExtendedBlock block) throws IOException {\n    BPOfferService bpos = getBPOSForBlock(block);\n    FsVolumeSpi volume = getFSDataset() != null ? getFSDataset().getVolume(block) : null;\n    if (bpos != null && volume != null) {\n        bpos.reportBadBlocks(block, volume.getStorageID(), volume.getStorageType());\n    } else {\n        // Handle the case where bpos or volume is null\n    }\n}"
        },
        "possible_fix_code": {
            "DataNode.reportBadBlocks": "  public void reportBadBlocks(ExtendedBlock block) throws IOException {\n    BPOfferService bpos = getBPOSForBlock(block);\n    FsVolumeSpi volume = getFSDataset() != null ? getFSDataset().getVolume(block) : null;\n    if (bpos != null && volume != null) {\n        bpos.reportBadBlocks(\n            block, volume.getStorageID(), volume.getStorageType());\n    } else {\n        // Handle the case where bpos or volume is null\n        // This could involve logging the error or taking alternative action\n        if (bpos == null) {\n            System.err.println(\"No BPOfferService found for block: \" + block);\n        }\n        if (volume == null) {\n            System.err.println(\"No volume found for block: \" + block);\n        }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-10729.json",
        "creation_time": "2016-08-08T02:36:05.000+0000",
        "bug_report": {
            "Title": "Improve log message for edit loading failures caused by FS limit checks.",
            "Description": "A critical issue has been identified where the Standby NameNode crashes due to a NullPointerException (NPE) when attempting to load edits. The stack trace indicates that the crash occurs in the FSDirectory class, specifically in the getFileEncryptionInfo method, which is called during the processing of edit log operations. The root cause appears to be related to exceeding the maximum directory item limit, leading to a MaxDirectoryItemsExceededException. This situation results in the NameNode being unable to restart after the crash, causing significant disruption in the HDFS environment.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFileEncryptionInfo(FSDirectory.java:2914)",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.createFileStatus(FSDirectory.java:2469)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:375)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:230)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:139)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:829)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:232)"
            ],
            "RootCause": "The root cause of the issue is the MaxDirectoryItemsExceededException, which indicates that the directory item limit of 1,048,576 has been exceeded. This leads to a failure in the FSDirectory's unprotectedAddFile method, which subsequently returns null, causing a NullPointerException when the result is accessed.",
            "StepsToReproduce": [
                "1. Set up a Hadoop HDFS environment with a directory containing more than 1,048,576 items.",
                "2. Attempt to load edits into the NameNode.",
                "3. Observe the crash and the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The NameNode should successfully load edits without crashing, even when the directory item limit is approached, and should provide a clear log message indicating any issues with directory limits.",
            "ObservedBehavior": "The NameNode crashes with a NullPointerException when the directory item limit is exceeded, preventing it from restarting and causing service disruption.",
            "Suggestions": "Consider increasing the value of dfs.namenode.fs-limits.max-directory-items to a higher threshold (e.g., 6,400,000) to accommodate more items in the directory. However, this should be done with caution to avoid potential performance issues.",
            "problem_location": {
                "files": [
                    "FSDirectory.java",
                    "FSEditLogLoader.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSDirectory",
                    "org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader"
                ],
                "methods": [
                    "FSDirectory.getFileEncryptionInfo",
                    "FSDirectory.verifyMaxDirItems",
                    "FSEditLogLoader.applyEditLogOp"
                ]
            },
            "possible_fix": "In FSDirectory.java, modify the verifyMaxDirItems method to handle cases where the directory item limit is exceeded more gracefully, potentially by logging a warning instead of throwing an exception that leads to a crash."
        },
        "possible_fix_code": {
            "FSDirectory.verifyMaxDirItems": "void verifyMaxDirItems(INodeDirectory parent, String parentPath) throws MaxDirectoryItemsExceededException {\n    final int count = parent.getChildrenList(CURRENT_STATE_ID).size();\n    if (count >= maxDirItems) {\n        final MaxDirectoryItemsExceededException e = new MaxDirectoryItemsExceededException(maxDirItems, count);\n        if (namesystem.isImageLoaded()) {\n            e.setPathName(parentPath);\n            NameNode.LOG.error(\"FSDirectory.verifyMaxDirItems: \" + e.getLocalizedMessage());\n            // Log a warning instead of throwing an exception\n            NameNode.LOG.warn(\"Directory item limit exceeded for path: \" + parentPath + \". Current count: \" + count + \".\");\n        } else {\n            // Do not throw if edits log is still being processed\n            NameNode.LOG.error(\"FSDirectory.verifyMaxDirItems: \" + e.getLocalizedMessage());\n        }\n    }\n}"
        }
    },
    {
        "filename": "HDFS-13040.json",
        "creation_time": "2018-01-19T19:47:02.000+0000",
        "bug_report": {
            "Title": "Kerberized inotify client fails despite kinit properly",
            "Description": "This issue is related to the failure of the Kerberized inotify client to read edit logs from the NameNode after the Kerberos ticket has expired. The client, using the principal 'hdfs@EXAMPLE.COM', encounters an error when attempting to access edit logs from the NameNodes (hdfs/nn1.example.com and hdfs/nn2.example.com) after they have been running longer than the Kerberos ticket lifetime. The error indicates that the edit log streams are shorter than expected, which can lead to potential metadata loss. This situation arises because the inotify client does not have the same principal as the NameNode, preventing the NameNode from re-authenticating on behalf of the client during the failover process.",
            "StackTrace": [
                "18/01/19 11:23:02 WARN security.UserGroupInformation: PriviledgedActionException as:hdfs@GCE.CLOUDERA.COM (auth:KERBEROS) cause:org.apache.hadoop.ipc.RemoteException(java.io.IOException): We encountered an error reading https://nn2.example.com:8481/getJournal?jid=ns1&segmentTxId=8662&storageInfo=-60%3A353531113%3A0%3Acluster3, https://nn1.example.com:8481/getJournal?jid=ns1&segmentTxId=8662&storageInfo=-60%3A353531113%3A0%3Acluster3.  During automatic edit log failover, we noticed that all of the remaining edit log streams are shorter than the current one!  The best remaining edit log ends at transaction 8683, but we thought we could read up to transaction 8684.  If you continue, metadata will be lost forever!",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:213)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.readOp(NameNodeRpcServer.java:1701)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEditsFromTxid(NameNodeRpcServer.java:1763)",
                "at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getEditsFromTxid(AuthorizationProviderProxyClientProtocol.java:1011)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getEditsFromTxid(ClientNamenodeProtocolServerSideTranslatorPB.java:1490)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2212)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2210)"
            ],
            "RootCause": "The root cause of the issue is that the inotify client is unable to read the edit logs from the NameNode due to the expiration of the Kerberos ticket. The NameNode cannot re-authenticate on behalf of the client because the client's principal does not match the NameNode's principal, leading to a failure in the automatic failover process.",
            "StepsToReproduce": [
                "1. Start the Namenodes with valid Kerberos credentials.",
                "2. Allow the Namenodes to run longer than the Kerberos ticket lifetime.",
                "3. Attempt to access edit logs using the inotify client with a different principal than the NameNode."
            ],
            "ExpectedBehavior": "The inotify client should be able to read the edit logs from the NameNode without encountering errors, even after the Kerberos ticket has expired, by allowing the NameNode to re-authenticate on behalf of the client.",
            "ObservedBehavior": "The inotify client fails to read the edit logs, resulting in an error indicating that the edit log streams are shorter than expected, which could lead to metadata loss.",
            "Suggestions": "Implement a proxy user mechanism that allows the NameNode to retrieve edits on behalf of the inotify client, ensuring that the correct principal is used during the authentication process.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogInputStream.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream",
                    "org.apache.hadoop.hdfs.server.namenode.EditLogInputStream",
                    "org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer"
                ],
                "methods": [
                    "RedundantEditLogInputStream.nextOp",
                    "EditLogInputStream.readOp",
                    "NameNodeRpcServer.getEditsFromTxid"
                ]
            },
            "possible_fix": "Modify the inotify client to use a proxy user that matches the active NameNode's principal, allowing the NameNode to authenticate on behalf of the client. This may involve changes in the authentication logic within the Hadoop framework."
        },
        "possible_fix_code": {
            "RedundantEditLogInputStream.nextOp": "  protected FSEditLogOp nextOp() throws IOException {\n    while (true) {\n      switch (state) {\n      case SKIP_UNTIL:\n       try {\n          if (prevTxId != HdfsServerConstants.INVALID_TXID) {\n            LOG.info(\"Fast-forwarding stream '\" + streams[curIdx].getName() +\n                \"' to transaction ID \" + (prevTxId + 1));\n            streams[curIdx].skipUntil(prevTxId + 1);\n          }\n        } catch (IOException e) {\n          prevException = e;\n          state = State.STREAM_FAILED;\n        }\n        state = State.OK;\n        break;\n      case OK:\n        try {\n          FSEditLogOp op = streams[curIdx].readOp();\n          if (op == null) {\n            state = State.EOF;\n            if (streams[curIdx].getLastTxId() == prevTxId) {\n              return null;\n            } else {\n              throw new PrematureEOFException(\"got premature end-of-file \" +\n                  \"at txid \" + prevTxId + \"; expected file to go up to \" +\n                  streams[curIdx].getLastTxId());\n            }\n          }\n          prevTxId = op.getTransactionId();\n          return op;\n        } catch (IOException e) {\n          prevException = e;\n          state = State.STREAM_FAILED;\n        }\n        break;\n      case STREAM_FAILED:\n        if (curIdx + 1 == streams.length) {\n          throw prevException;\n        }\n        long oldLast = streams[curIdx].getLastTxId();\n        long newLast = streams[curIdx + 1].getLastTxId();\n        if (newLast < oldLast) {\n          throw new IOException(\"We encountered an error reading \" +\n              streams[curIdx].getName() + \".  During automatic edit log \" +\n              \"failover, we noticed that all of the remaining edit log \" +\n              \"streams are shorter than the current one!  The best \" +\n              \"remaining edit log ends at transaction \" +\n              newLast + \", but we thought we could read up to transaction \" +\n              oldLast + \".  If you continue, metadata will be lost forever!\");\n        }\n        LOG.error(\"Got error reading edit log input stream \" +\n          streams[curIdx].getName() + \"; failing over to edit log \" +\n          streams[curIdx + 1].getName(), prevException);\n        curIdx++;\n        state = State.SKIP_UNTIL;\n        break;\n      case STREAM_FAILED_RESYNC:\n        if (curIdx + 1 == streams.length) {\n          if (prevException instanceof PrematureEOFException) {\n            // bypass early EOF check\n            state = State.EOF;\n          } else {\n            streams[curIdx].resync();\n            state = State.SKIP_UNTIL;\n          }\n        } else {\n          LOG.error(\"failing over to edit log \" +\n              streams[curIdx + 1].getName());\n          curIdx++;\n          state = State.SKIP_UNTIL;\n        }\n        break;\n      case EOF:\n        return null;\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-3374.json",
        "creation_time": "2012-05-04T22:08:19.000+0000",
        "bug_report": {
            "Title": "hdfs' TestDelegationToken fails intermittently with a race condition",
            "Description": "The test case for TestDelegationToken is failing intermittently due to a race condition where the MiniDFSCluster is shut down before the DelegationTokenSecretManager can update the master key. This leads to a situation where the NameNode attempts to access edit streams that are no longer available, resulting in a fatal error. The stack trace indicates that the FSEditLog is unable to log updates because no edit streams are accessible, which is critical for maintaining the filesystem state.",
            "StackTrace": [
                "2012-05-04 15:03:51,521 WARN  common.Storage (FSImage.java:updateRemovedDirs(224)) - Removing storage dir /home/horton/src/hadoop/build/test/data/dfs/name1",
                "2012-05-04 15:03:51,522 FATAL namenode.FSNamesystem (FSEditLog.java:fatalExit(388)) - No edit streams are accessible",
                "java.lang.Exception: No edit streams are accessible",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.fatalExit(FSEditLog.java:388)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.exitIfNoStreams(FSEditLog.java:407)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsAndStorageDir(FSEditLog.java:432)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsStreamsAndStorageDirs(FSEditLog.java:468)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync(FSEditLog.java:1028)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logUpdateMasterKey(FSNamesystem.java:5641)",
                "at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager.logUpdateMasterKey(DelegationTokenSecretManager.java:286)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.updateCurrentKey(AbstractDelegationTokenSecretManager.java:150)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.rollMasterKey(AbstractDelegationTokenSecretManager.java:174)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:385)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is that the MiniDFSCluster is being shut down before the DelegationTokenSecretManager can update the master key, leading to the NameNode being unable to access edit streams, which are essential for logging changes to the filesystem.",
            "StepsToReproduce": [
                "Run the TestDelegationToken test case in the Hadoop HDFS environment.",
                "Ensure that the MiniDFSCluster is configured to shut down before the DelegationTokenSecretManager completes its key update.",
                "Observe the failure in the test case due to the fatal error regarding edit streams."
            ],
            "ExpectedBehavior": "The test case should complete successfully without any fatal errors, with the NameNode able to access edit streams and log updates correctly.",
            "ObservedBehavior": "The test case fails intermittently with a fatal error indicating that no edit streams are accessible, leading to a crash.",
            "Suggestions": "To mitigate this issue, ensure that the MiniDFSCluster does not shut down until after the DelegationTokenSecretManager has successfully updated the master key. Additionally, review the configuration settings in hdfs-site.xml to ensure proper paths and permissions for edit logs.",
            "problem_location": {
                "files": [
                    "FSEditLog.java",
                    "FSNamesystem.java",
                    "DelegationTokenSecretManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSEditLog",
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem",
                    "org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager"
                ],
                "methods": [
                    "FSEditLog.fatalExit",
                    "FSEditLog.exitIfNoStreams",
                    "FSNamesystem.logUpdateMasterKey",
                    "DelegationTokenSecretManager.logUpdateMasterKey"
                ]
            },
            "possible_fix": "Consider adding synchronization mechanisms to ensure that the MiniDFSCluster does not shut down until all necessary updates to the edit logs have been completed. Additionally, verify and adjust the configuration settings in hdfs-site.xml to ensure that the edit log directories are correctly specified and accessible."
        },
        "possible_fix_code": {
            "FSEditLog.logSync": "  public void logSync() {\n    long syncStart = 0;\n\n    // Fetch the transactionId of this thread. \n    long mytxid = myTransactionId.get().txid;\n    \n    boolean sync = false;\n    try {\n      EditLogOutputStream logStream = null;\n      synchronized (this) {\n        try {\n          printStatistics(false);\n\n          // if somebody is already syncing, then wait\n          while (mytxid > synctxid && isSyncRunning) {\n            try {\n              wait(1000);\n            } catch (InterruptedException ie) {\n            }\n          }\n  \n          //\n          // If this transaction was already flushed, then nothing to do\n          //\n          if (mytxid <= synctxid) {\n            numTransactionsBatchedInSync++;\n            if (metrics != null) {\n              // Metrics is non-null only when used inside name node\n              metrics.incrTransactionsBatchedInSync();\n            }\n            return;\n          }\n     \n          // now, this thread will do the sync\n          syncStart = txid;\n          isSyncRunning = true;\n          sync = true;\n  \n          // swap buffers\n          try {\n            if (journalSet.isEmpty()) {\n              throw new IOException(\"No journals available to flush\");\n            }\n            editLogStream.setReadyToFlush();\n          } catch (IOException e) {\n            LOG.fatal(\"Could not sync enough journals to persistent storage. \"\n                + \"Unsynced transactions: \" + (txid - synctxid),\n                new Exception());\n            runtime.exit(1);\n          }\n        } finally {\n          // Prevent RuntimeException from blocking other log edit write \n          doneWithAutoSyncScheduling();\n        }\n        //editLogStream may become null,\n        //so store a local variable for flush.\n        logStream = editLogStream;\n      }\n      \n      // do the sync\n      long start = now();\n      try {\n        if (logStream != null) {\n          logStream.flush();\n        }\n      } catch (IOException ex) {\n        synchronized (this) {\n          LOG.fatal(\"Could not sync enough journals to persistent storage. \"\n              + \"Unsynced transactions: \" + (txid - synctxid), new Exception());\n          runtime.exit(1);\n        }\n      }\n      long elapsed = now() - start;\n  \n      if (metrics != null) { // Metrics non-null only when used inside name node\n        metrics.addSync(elapsed);\n      }\n      \n    } finally {\n      // Prevent RuntimeException from blocking other log edit sync \n      synchronized (this) {\n        if (sync) {\n          synctxid = syncStart;\n          isSyncRunning = false;\n        }\n        this.notifyAll();\n     }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-2359.json",
        "creation_time": "2011-09-23T00:20:53.000+0000",
        "bug_report": {
            "Title": "NPE found in Datanode log while Disk failed during different HDFS operation",
            "Description": "During a distributed copy operation (distcp) in an HDFS cluster with 4 DataNodes, each having 12 disks, a scenario was created where 3 disks on one DataNode were intentionally failed by setting their data directory permissions to 000. Although the distcp job completed successfully, several NullPointerExceptions (NPEs) were logged in the DataNode logs. The logs indicate issues with block deletion and scanning, particularly related to the `DataBlockScanner` and `FSDataset` classes.",
            "StackTrace": [
                "java.io.IOException: Error in deleting blocks.",
                "at org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate(FSDataset.java:1820)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.processCommand(DataNode.java:1074)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.processCommand(DataNode.java:1036)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.offerService(DataNode.java:891)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1419)",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$LogFileHandler.appendLine(DataBlockScanner.java:788)",
                "at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.updateScanStatusInternal(DataBlockScanner.java:365)",
                "at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.verifiedByClient(DataBlockScanner.java:308)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:205)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:99)"
            ],
            "RootCause": "The root cause of the issue appears to be twofold: an IOException during block deletion in the `FSDataset.invalidate` method, likely due to invalid paths or permissions, and a NullPointerException in the `DataBlockScanner`, which may occur if the `BlockPoolSliceScanner` is not properly initialized or is missing.",
            "StepsToReproduce": [
                "1. Set up a cluster with 4 DataNodes, each having 12 disks.",
                "2. Configure hdfs-site.xml with 'dfs.datanode.failed.volumes.tolerated=3'.",
                "3. Execute a distcp command to copy data between HDFS locations.",
                "4. In a separate terminal, set the permissions of 3 disks on one DataNode to 000.",
                "5. Monitor the DataNode logs for NullPointerExceptions."
            ],
            "ExpectedBehavior": "The distcp operation should complete successfully without any exceptions in the DataNode logs, and the DataNode should handle disk failures gracefully without throwing NullPointerExceptions.",
            "ObservedBehavior": "The distcp operation completed successfully, but NullPointerExceptions were logged in the DataNode logs, indicating issues with block deletion and scanning.",
            "Suggestions": "Investigate the initialization of the `BlockPoolSliceScanner` in the `DataBlockScanner` class to ensure it is properly set up before being accessed. Additionally, review the permissions and paths used in the `FSDataset.invalidate` method to prevent IOException during block deletion.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataBlockScanner.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.FSDataset",
                    "org.apache.hadoop.hdfs.server.datanode.DataBlockScanner"
                ],
                "methods": [
                    "FSDataset.invalidate",
                    "DataBlockScanner$LogFileHandler.appendLine",
                    "DataBlockScanner.updateScanStatusInternal"
                ]
            },
            "possible_fix": "Ensure that the `BlockPoolSliceScanner` is initialized correctly in the `DataBlockScanner` class. Additionally, add error handling in the `FSDataset.invalidate` method to manage cases where block paths are invalid or inaccessible."
        },
        "possible_fix_code": {
            "FSDataset.invalidate": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error = false;\n    for (int i = 0; i < invalidBlks.length; i++) {\n      File f = null;\n      FSVolume v;\n      synchronized (this) {\n        f = getFile(bpid, invalidBlks[i]);\n        ReplicaInfo dinfo = volumeMap.get(bpid, invalidBlks[i]);\n        if (dinfo == null || \n            dinfo.getGenerationStamp() != invalidBlks[i].getGenerationStamp()) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                           + invalidBlks[i] + \n                           \". BlockInfo not found in volumeMap.\");\n          error = true;\n          continue;\n        }\n        v = dinfo.getVolume();\n        if (f == null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Block not found in blockMap.\" +\n                            ((v == null) ? \" \" : \" Block found in volumeMap.\"));\n          error = true;\n          continue;\n        }\n        if (v == null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". No volume for this block.\" +\n                            \" Block found in blockMap. \" + f + \".\");\n          error = true;\n          continue;\n        }\n        File parent = f.getParentFile();\n        if (parent == null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Parent not found for file \" + f + \".\");\n          error = true;\n          continue;\n        }\n        ReplicaState replicaState = dinfo.getState();\n        if (replicaState == ReplicaState.FINALIZED || \n            (replicaState == ReplicaState.RUR && \n                ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() == \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      File metaFile = getMetaFile(f, invalidBlks[i].getGenerationStamp());\n      long dfsBytes = f.length() + metaFile.length();\n\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f, metaFile, dfsBytes,\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-10986.json",
        "creation_time": "2016-10-08T00:50:36.000+0000",
        "bug_report": {
            "Title": "DFSAdmin should log detailed error message if any",
            "Description": "The DFSAdmin tool in Hadoop is currently not providing sufficient error messages when encountering IOExceptions, particularly when attempting to connect to DataNodes or NameNodes. For instance, commands like `hdfs dfsadmin -getBalancerBandwidth` and `hdfs dfsadmin -getDatanodeInfo` return a vague 'Datanode unreachable' message without any stack trace or detailed error information. This lack of clarity makes it difficult for users to diagnose issues. In contrast, when a connection is attempted on an incorrect port, more informative logs are generated, indicating the nature of the connection failure. The `DFSAdmin#run` method is responsible for executing commands and should be enhanced to log detailed error messages when exceptions occur.",
            "StackTrace": [
                "java.net.ConnectException: Connection refused",
                "at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)",
                "at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)",
                "at org.apache.hadoop.hdfs.tools.DFSAdmin.run(DFSAdmin.java:2073)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)",
                "at org.apache.hadoop.hdfs.tools.DFSAdmin.main(DFSAdmin.java:2225)"
            ],
            "RootCause": "The root cause of the issue is that the DFSAdmin tool does not log detailed error messages when it encounters a `java.net.ConnectException: Connection refused`. This typically indicates that the server is not accepting connections on the specified port, which can occur if the NameNode or DataNode is not running or if the configuration is incorrect.",
            "StepsToReproduce": [
                "Run the command `hdfs dfsadmin -getBalancerBandwidth 127.0.0.1:9866`.",
                "Observe the output which states 'Datanode unreachable'.",
                "Run the command with an incorrect port, e.g., `hdfs dfsadmin -getBalancerBandwidth 127.0.0.1:9999`.",
                "Check the logs for detailed error messages."
            ],
            "ExpectedBehavior": "When a connection attempt fails, the DFSAdmin tool should log a detailed error message that includes the exception stack trace, allowing users to understand the nature of the failure.",
            "ObservedBehavior": "The DFSAdmin tool returns a vague 'Datanode unreachable' message without providing any stack trace or detailed error information, making it difficult for users to diagnose the issue.",
            "Suggestions": "Modify the `DFSAdmin#run` method to ensure that exceptions are logged with detailed messages, including stack traces, when connection attempts fail. This will improve user experience and facilitate easier debugging.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.tools.DFSAdmin"
                ],
                "methods": [
                    "DFSAdmin.run"
                ]
            },
            "possible_fix": "In the `DFSAdmin#run` method, ensure that all caught exceptions are logged with their stack traces. For example, modify the catch block for IOException to include a log statement that outputs the exception details: `LOG.error(\"Connection failed: \", e);`."
        },
        "possible_fix_code": {
            "DFSAdmin.run": "  public int run(String[] argv) throws Exception {\n\n    if (argv.length < 1) {\n      printUsage(\"\");\n      return -1;\n    }\n\n    int exitCode = -1;\n    int i = 0;\n    String cmd = argv[i++];\n\n    //\n    // verify that we have enough command line parameters\n    //\n    if (\"-safemode\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-allowSnapshot\".equalsIgnoreCase(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-disallowSnapshot\".equalsIgnoreCase(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-report\".equals(cmd)) {\n      if (argv.length < 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-saveNamespace\".equals(cmd)) {\n      if (argv.length != 1 && argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-rollEdits\".equals(cmd)) {\n      if (argv.length != 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-restoreFailedStorage\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-refreshNodes\".equals(cmd)) {\n      if (argv.length != 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-finalizeUpgrade\".equals(cmd)) {\n      if (argv.length != 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (RollingUpgradeCommand.matches(cmd)) {\n      if (argv.length < 1 || argv.length > 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-metasave\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-refreshServiceAcl\".equals(cmd)) {\n      if (argv.length != 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-refresh\".equals(cmd)) {\n      if (argv.length < 3) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-refreshUserToGroupsMappings\".equals(cmd)) {\n      if (argv.length != 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-printTopology\".equals(cmd)) {\n      if(argv.length != 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-refreshNamenodes\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-reconfig\".equals(cmd)) {\n      if (argv.length != 4) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-deleteBlockPool\".equals(cmd)) {\n      if ((argv.length != 3) && (argv.length != 4)) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-setBalancerBandwidth\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-getBalancerBandwidth\".equalsIgnoreCase(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-fetchImage\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-shutdownDatanode\".equals(cmd)) {\n      if ((argv.length != 2) && (argv.length != 3)) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-getDatanodeInfo\".equals(cmd)) {\n      if (argv.length != 2) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    } else if (\"-triggerBlockReport\".equals(cmd)) {\n      if (argv.length < 1) {\n        printUsage(cmd);\n        return exitCode;\n      }\n    }\n    \n    // initialize DFSAdmin\n    try {\n      init();\n    } catch (RPC.VersionMismatch v) {\n      System.err.println(\"Version Mismatch between client and server\" +\n                         \"... command aborted.\");\n      return exitCode;\n    } catch (IOException e) {\n      System.err.println(\"Bad connection to DFS... command aborted.\");\n      return exitCode;\n    }\n\n    Exception debugException = null;\n    exitCode = 0;\n    try {\n      if (\"-report\".equals(cmd)) {\n        report(argv, i);\n      } else if (\"-safemode\".equals(cmd)) {\n        setSafeMode(argv, i);\n      } else if (\"-allowSnapshot\".equalsIgnoreCase(cmd)) {\n        allowSnapshot(argv);\n      } else if (\"-disallowSnapshot\".equalsIgnoreCase(cmd)) {\n        disallowSnapshot(argv);\n      } else if (\"-saveNamespace\".equals(cmd)) {\n        exitCode = saveNamespace(argv);\n      } else if (\"-rollEdits\".equals(cmd)) {\n        exitCode = rollEdits();\n      } else if (\"-restoreFailedStorage\".equals(cmd)) {\n        exitCode = restoreFailedStorage(argv[i]);\n      } else if (\"-refreshNodes\".equals(cmd)) {\n        exitCode = refreshNodes();\n      } else if (\"-finalizeUpgrade\".equals(cmd)) {\n        exitCode = finalizeUpgrade();\n      } else if (RollingUpgradeCommand.matches(cmd)) {\n        exitCode = RollingUpgradeCommand.run(getDFS(), argv, i);\n      } else if (\"-metasave\".equals(cmd)) {\n        exitCode = metaSave(argv, i);\n      } else if (ClearQuotaCommand.matches(cmd)) {\n        exitCode = new ClearQuotaCommand(argv, i, getDFS()).runAll();\n      } else if (SetQuotaCommand.matches(cmd)) {\n        exitCode = new SetQuotaCommand(argv, i, getDFS()).runAll();\n      } else if (ClearSpaceQuotaCommand.matches(cmd)) {\n        exitCode = new ClearSpaceQuotaCommand(argv, i, getDFS()).runAll();\n      } else if (SetSpaceQuotaCommand.matches(cmd)) {\n        exitCode = new SetSpaceQuotaCommand(argv, i, getDFS()).runAll();\n      } else if (\"-refreshServiceAcl\".equals(cmd)) {\n        exitCode = refreshServiceAcl();\n      } else if (\"-refreshUserToGroupsMappings\".equals(cmd)) {\n        exitCode = refreshUserToGroupsMappings();\n      } else if (\"-refreshSuperUserGroupsConfiguration\".equals(cmd)) {\n        exitCode = refreshSuperUserGroupsConfiguration();\n      } else if (\"-refreshCallQueue\".equals(cmd)) {\n        exitCode = refreshCallQueue();\n      } else if (\"-refresh\".equals(cmd)) {\n        exitCode = genericRefresh(argv, i);\n      } else if (\"-printTopology\".equals(cmd)) {\n        exitCode = printTopology();\n      } else if (\"-refreshNamenodes\".equals(cmd)) {\n        exitCode = refreshNamenodes(argv, i);\n      } else if (\"-deleteBlockPool\".equals(cmd)) {\n        exitCode = deleteBlockPool(argv, i);\n      } else if (\"-setBalancerBandwidth\".equals(cmd)) {\n        exitCode = setBalancerBandwidth(argv, i);\n      } else if (\"-getBalancerBandwidth\".equals(cmd)) {\n        exitCode = getBalancerBandwidth(argv, i);\n      } else if (\"-fetchImage\".equals(cmd)) {\n        exitCode = fetchImage(argv, i);\n      } else if (\"-shutdownDatanode\".equals(cmd)) {\n        exitCode = shutdownDatanode(argv, i);\n      } else if (\"-evictWriters\".equals(cmd)) {\n        exitCode = evictWriters(argv, i);\n      } else if (\"-getDatanodeInfo\".equals(cmd)) {\n        exitCode = getDatanodeInfo(argv, i);\n      } else if (\"-reconfig\".equals(cmd)) {\n        exitCode = reconfig(argv, i);\n      } else if (\"-triggerBlockReport\".equals(cmd)) {\n        exitCode = triggerBlockReport(argv);\n      } else if (\"-help\".equals(cmd)) {\n        if (i < argv.length) {\n          printHelp(argv[i]);\n        } else {\n          printHelp(\"\");\n        }\n      } else {\n        exitCode = -1;\n        System.err.println(cmd.substring(1) + \": Unknown command\");\n        printUsage(\"\");\n      }\n    } catch (IllegalArgumentException arge) {\n      debugException = arge;\n      exitCode = -1;\n      System.err.println(cmd.substring(1) + \": \" + arge.getLocalizedMessage());\n      printUsage(cmd);\n    } catch (RemoteException e) {\n      //\n      // This is a error returned by hadoop server. Print\n      // out the first line of the error message, ignore the stack trace.\n      exitCode = -1;\n      debugException = e;\n      try {\n        String[] content;\n        content = e.getLocalizedMessage().split(\"\\\\n\");\n        System.err.println(cmd.substring(1) + \": \" + content[0]);\n      } catch (Exception ex) {\n        System.err.println(cmd.substring(1) + \": \" + ex.getLocalizedMessage());\n        debugException = ex;\n      }\n    } catch (Exception e) {\n      exitCode = -1;\n      debugException = e;\n      System.err.println(cmd.substring(1) + \": \" + e.getLocalizedMessage());\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Exception encountered:\", debugException);\n    }\n    return exitCode;\n  }"
        }
    },
    {
        "filename": "HDFS-6455.json",
        "creation_time": "2014-05-28T18:01:15.000+0000",
        "bug_report": {
            "Title": "NFS: Exception should be added in NFS log for invalid separator in nfs.exports.allowed.hosts",
            "Description": "The NFS server fails to start when an invalid separator is used in the 'dfs.nfs.exports.allowed.hosts' property. The error message for this issue should be logged in the NFS log file instead of the nfs.out file. The current implementation does not provide adequate logging for this error, leading to confusion during troubleshooting.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.IllegalArgumentException: Incorrectly formatted line 'host1 ro:host2 rw'",
                "at org.apache.hadoop.nfs.NfsExports.getMatch(NfsExports.java:356)",
                "at org.apache.hadoop.nfs.NfsExports.<init>(NfsExports.java:151)",
                "at org.apache.hadoop.nfs.NfsExports.getInstance(NfsExports.java:54)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.<init>(RpcProgramNfs3.java:176)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.<init>(Nfs3.java:43)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.main(Nfs3.java:59)"
            ],
            "RootCause": "The root cause of the issue is the incorrect formatting of the NFS exports configuration line. The line 'host1 ro:host2 rw' contains a colon (':'), which is not a valid separator according to the expected formats defined in the 'getMatch' method of the NfsExports class. The method expects either a single host or a host followed by a privilege, separated by whitespace.",
            "StepsToReproduce": [
                "1. Set the property 'dfs.nfs.exports.allowed.hosts' with an invalid separator.",
                "   Example: <property><name>dfs.nfs.exports.allowed.hosts</name><value>host1 ro:host2 rw</value></property>",
                "2. Restart the NFS server.",
                "3. Observe the console output and log files."
            ],
            "ExpectedBehavior": "The NFS server should start successfully and log any configuration errors in the NFS log file, providing clear feedback on the issue.",
            "ObservedBehavior": "The NFS server fails to start, and the error message regarding the incorrectly formatted line is printed to the nfs.out file instead of the NFS log file. The server shuts down without logging the error in the expected location.",
            "Suggestions": "Modify the logging mechanism to ensure that errors related to invalid configurations are logged in the NFS log file. Additionally, enforce validation of the configuration format before attempting to start the NFS server.",
            "problem_location": {
                "files": [
                    "NfsExports.java"
                ],
                "classes": [
                    "org.apache.hadoop.nfs.NfsExports"
                ],
                "methods": [
                    "NfsExports.getMatch"
                ]
            },
            "possible_fix": "Update the NfsExports class to include a logging statement in the getMatch method that logs an error message when an IllegalArgumentException is thrown due to incorrectly formatted lines. Ensure that the logging is directed to the NFS log file."
        },
        "possible_fix_code": {
            "getMatch": "  private static Match getMatch(String line) {\n    String[] parts = line.split(\"\\\\s+\");\n    final String host;\n    AccessPrivilege privilege = AccessPrivilege.READ_ONLY;\n    switch (parts.length) {\n    case 1:\n      host = parts[0].toLowerCase().trim();\n      break;\n    case 2:\n      host = parts[0].toLowerCase().trim();\n      String option = parts[1].trim();\n      if (\"rw\".equalsIgnoreCase(option)) {\n        privilege = AccessPrivilege.READ_WRITE;\n      }\n      break;\n    default:\n      String errorMessage = \"Incorrectly formatted line '\" + line + \"'\";\n      LOG.error(errorMessage); // Log the error message to the NFS log file\n      throw new IllegalArgumentException(errorMessage);\n    }\n    if (host.equals(\"*\")) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using match all for '\" + host + \"' and \" + privilege);\n      }\n      return new AnonymousMatch(privilege);\n    } else if (CIDR_FORMAT_SHORT.matcher(host).matches()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using CIDR match for '\" + host + \"' and \" + privilege);\n      }\n      return new CIDRMatch(privilege, new SubnetUtils(host).getInfo());\n    } else if (CIDR_FORMAT_LONG.matcher(host).matches()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using CIDR match for '\" + host + \"' and \" + privilege);\n      }\n      String[] pair = host.split(\"/\");\n      return new CIDRMatch(privilege,\n          new SubnetUtils(pair[0], pair[1]).getInfo());\n    } else if (host.contains(\"*\") || host.contains(\"?\") || host.contains(\"[\")\n        || host.contains(\"]\")) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using Regex match for '\" + host + \"' and \" + privilege);\n      }\n      return new RegexMatch(privilege, host);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Using exact match for '\" + host + \"' and \" + privilege);\n    }\n    return new ExactMatch(privilege, host);\n  }"
        }
    },
    {
        "filename": "HDFS-2882.json",
        "creation_time": "2012-02-02T18:11:20.000+0000",
        "bug_report": {
            "Title": "DN continues to start up, even if block pool fails to initialize",
            "Description": "The DataNode (DN) was started on a machine that had insufficient disk space on one of its drives. During initialization, an IOException occurred while attempting to create a directory for the block pool, which resulted in a failure to initialize the block pool. Despite this failure, the DataNode continued to run, leading to NullPointerExceptions (NPEs) when it attempted to perform block reports. This issue was observed on the HDFS-1623 branch and may also affect the trunk version.",
            "StackTrace": [
                "2012-02-02 09:56:50,499 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool BP-448349972-172.29.5.192-1323816762969 (storage id DS-507718931-172.29.5.194-11072-1297842002148) service to styx01.sf.cloudera.com/172.29.5.192:8021",
                "java.io.IOException: Mkdirs failed to create /data/1/scratch/todd/styx-datadir/current/BP-448349972-172.29.5.192-1323816762969/tmp",
                "at org.apache.hadoop.hdfs.server.datanode.FSDataset$BlockPoolSlice.<init>(FSDataset.java:335)"
            ],
            "RootCause": "The root cause of the issue is an IOException that occurs when the DataNode attempts to create a directory for the block pool. This is likely due to insufficient disk space, which prevents the directory from being created.",
            "StepsToReproduce": [
                "1. Start a DataNode on a machine with insufficient disk space on one of its drives.",
                "2. Monitor the logs for any fatal errors related to block pool initialization."
            ],
            "ExpectedBehavior": "The DataNode should fail to start if it cannot initialize the block pool due to an IOException, and it should not continue running.",
            "ObservedBehavior": "The DataNode starts running despite the failure to initialize the block pool, leading to subsequent NullPointerExceptions when attempting to perform block reports.",
            "Suggestions": "Ensure that there is sufficient disk space available on the DataNode before starting it. Additionally, implement a check to prevent the DataNode from starting if the block pool cannot be initialized.",
            "problem_location": {
                "files": [
                    "FSDataset.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.DataNode",
                    "org.apache.hadoop.hdfs.server.datanode.FSDataset"
                ],
                "methods": [
                    "FSDataset$BlockPoolSlice.init"
                ]
            },
            "possible_fix": "Modify the DataNode initialization logic to check for sufficient disk space and prevent startup if the block pool cannot be initialized. This could involve adding a validation step before the directory creation attempt."
        },
        "possible_fix_code": {
            "DataNode.initialize": "public void initialize() throws IOException {\n    // Check for sufficient disk space before proceeding\n    if (!hasSufficientDiskSpace()) {\n        throw new IOException(\"Insufficient disk space to initialize DataNode.\");\n    }\n    // Proceed with block pool initialization\n    try {\n        // Attempt to create the block pool directory\n        createBlockPoolDirectory();\n    } catch (IOException e) {\n        // Log the error and prevent DataNode from starting\n        LOG.fatal(\"Initialization failed for block pool: \" + e.getMessage(), e);\n        throw e;\n    }\n}\n\nprivate boolean hasSufficientDiskSpace() {\n    // Implement logic to check available disk space\n    // Return true if sufficient, false otherwise\n    return true; // Placeholder for actual disk space check\n}\n\nprivate void createBlockPoolDirectory() throws IOException {\n    // Logic to create the block pool directory\n    // This is where the IOException might occur\n}"
        }
    },
    {
        "filename": "HDFS-5185.json",
        "creation_time": "2013-09-11T14:38:23.000+0000",
        "bug_report": {
            "Title": "DN fails to startup if one of the data dir is full",
            "Description": "The DataNode fails to start up when one of the configured data directories is out of space, resulting in an IOException. The error occurs during the initialization of a block pool, specifically when attempting to create a temporary directory for the block pool. The failure is indicated by the following exception in the logs: \n\n`java.io.IOException: Mkdirs failed to create /opt/nish/data/current/BP-123456-1234567/tmp`.\n\nThis issue can arise due to various reasons including insufficient permissions, lack of disk space, non-existent parent directories, or filesystem issues.",
            "StackTrace": [
                "2013-09-11 17:48:43,680 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool <registering> (storage id DS-308316523-xx.xx.xx.xx-64015-1378896293604) service to /nn1:65110",
                "java.io.IOException: Mkdirs failed to create /opt/nish/data/current/BP-123456-1234567/tmp",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.<init>(BlockPoolSlice.java:105)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.addBlockPool(FsVolumeImpl.java:216)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addBlockPool(FsVolumeList.java:155)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addBlockPool(FsDatasetImpl.java:1593)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:834)",
                "at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:311)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:217)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:660)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is an IOException thrown by the `addBlockPool` method in the `FsVolumeImpl` class, which fails to create a directory for the block pool due to one or more of the following reasons: insufficient permissions, lack of disk space, non-existent parent directories, or filesystem issues.",
            "StepsToReproduce": [
                "Configure a DataNode with multiple data directories.",
                "Fill one of the data directories to its capacity.",
                "Attempt to start the DataNode service."
            ],
            "ExpectedBehavior": "The DataNode should start successfully, utilizing the available data directories, even if one of them is full.",
            "ObservedBehavior": "The DataNode fails to start, throwing an IOException indicating that it cannot create a required directory due to the full data directory.",
            "Suggestions": "To resolve this issue, check the following: \n- Ensure that the user running the Hadoop process has the necessary permissions to create directories in the specified path. \n- Verify that there is sufficient disk space available on the filesystem. \n- Ensure that the parent directory exists; if not, create it manually or adjust the configuration. \n- Check the filesystem status to ensure it is writable and not experiencing issues.",
            "problem_location": {
                "files": [
                    "FsVolumeImpl.java",
                    "DataNode.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl",
                    "org.apache.hadoop.hdfs.server.datanode.DataNode"
                ],
                "methods": [
                    "FsVolumeImpl.addBlockPool",
                    "DataNode.initBlockPool"
                ]
            },
            "possible_fix": "Check and modify the permissions of the `/opt/nish/data/current/BP-123456-1234567` directory to ensure the Hadoop process can write to it. Additionally, ensure that there is enough disk space and that the parent directory exists."
        },
        "possible_fix_code": {
            "FsVolumeImpl.addBlockPool": "  void addBlockPool(String bpid, Configuration conf) throws IOException {\n    File bpdir = new File(currentDir, bpid);\n    // Check if the parent directory exists, if not, create it\n    if (!bpdir.getParentFile().exists()) {\n        if (!bpdir.getParentFile().mkdirs()) {\n            throw new IOException(\"Failed to create parent directory: \" + bpdir.getParentFile().getAbsolutePath());\n        }\n    }\n    BlockPoolSlice bp = new BlockPoolSlice(bpid, this, bpdir, conf);\n    bpSlices.put(bpid, bp);\n  }"
        }
    },
    {
        "filename": "HDFS-13164.json",
        "creation_time": "2018-02-17T00:40:21.000+0000",
        "bug_report": {
            "Title": "File not closed if streamer fails with DSQuotaExceededException",
            "Description": "This issue arises during yarn log aggregation but can theoretically occur for any client when the disk space quota for a directory in HDFS is exceeded. When a file is created, the client initiates a request to the NameNode (NN) to obtain a DFSOutputStream. If the disk space quota is exceeded during the writing process, the streamer attempts to get an additional block, resulting in a DSQuotaExceededException. The client then attempts to close the stream, but due to the implementation of the close method, it may not properly close the stream if the streamer has already encountered the quota exception. This leads to a file being left in an open-for-write state, potentially causing lease renewer leaks.",
            "StackTrace": [
                "org.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /DIR is exceeded: quota = 2000000 B = 1.91 MB but diskspace consumed = 404139552 B = 385.42 MB",
                "at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyDiskspaceQuota(DirectoryWithQuotaFeature.java:149)",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:2124)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3484)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:788)"
            ],
            "RootCause": "The root cause of the issue is that the close method of the DFSOutputStream does not properly handle the case where the streamer has encountered a DSQuotaExceededException, leading to the stream remaining open and the file being left in an open-for-write state.",
            "StepsToReproduce": [
                "Set a disk space quota for a directory in HDFS.",
                "Attempt to write a file that exceeds the quota.",
                "Observe the DSQuotaExceededException being thrown.",
                "Call the close method on the DFSOutputStream."
            ],
            "ExpectedBehavior": "The file should be properly closed, and no open-for-write state should remain if the quota is exceeded.",
            "ObservedBehavior": "The file remains in an open-for-write state, and the stream is not closed properly, potentially leading to resource leaks.",
            "Suggestions": "To resolve this issue, consider implementing a check in the close method to ensure that if a DSQuotaExceededException has occurred, the stream is closed properly. Additionally, increasing the disk space quota or cleaning up the directory may prevent the exception from occurring.",
            "problem_location": {
                "files": [
                    "DFSOutputStream.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.DFSOutputStream"
                ],
                "methods": [
                    "DFSOutputStream.closeImpl"
                ]
            },
            "possible_fix": "Modify the closeImpl method in DFSOutputStream to handle the case where a DSQuotaExceededException has occurred, ensuring that the stream is closed properly even if the quota is exceeded."
        },
        "possible_fix_code": {
            "DFSOutputStream.closeImpl": "  void closeImpl() throws IOException {\n    try {\n        // Existing close logic\n        if (streamer != null) {\n            streamer.close();\n        }\n    } catch (DSQuotaExceededException e) {\n        // Log the exception\n        LOG.warn(\"DSQuotaExceededException encountered while closing: \" + e.getMessage());\n        // Ensure the stream is closed properly even if quota exceeded\n        if (streamer != null) {\n            streamer.close();\n        }\n    } finally {\n        // Additional cleanup if necessary\n        if (outputStream != null) {\n            outputStream.close();\n        }\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-11508.json",
        "creation_time": "2017-03-07T04:55:50.000+0000",
        "bug_report": {
            "Title": "Fix bind failure in SimpleTCPServer & Portmap where bind fails because socket is in TIME_WAIT state",
            "Description": "The application encounters a bind failure in the SimpleTCPServer and Portmap components due to the socket being in a TIME_WAIT state. This issue arises when the server attempts to bind to port 4242, which is already in use, leading to a ChannelException caused by a BindException. The socket options need to be modified to include the setReuseAddress option to allow the socket to bind even if it is in the TIME_WAIT state.",
            "StackTrace": [
                "2017-03-05 08:37:13,283 INFO  oncrpc.SimpleUdpServer (SimpleUdpServer.java:run(73)) - Started listening to UDP requests at port 4242 for Rpc program: mountd at localhost:4242 with workerCount 1",
                "2017-03-05 08:37:13,298 FATAL mount.MountdBase (MountdBase.java:startTCPServer(85)) - Failed to start the TCP server.",
                "org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:4242",
                "at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)",
                "at org.apache.hadoop.oncrpc.SimpleTcpServer.run(SimpleTcpServer.java:86)",
                "at org.apache.hadoop.mount.MountdBase.startTCPServer(MountdBase.java:83)",
                "at org.apache.hadoop.mount.MountdBase.start(MountdBase.java:98)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:71)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)",
                "Caused by: java.net.BindException: Address already in use",
                "at sun.nio.ch.Net.bind0(Native Method)",
                "at sun.nio.ch.Net.bind(Net.java:433)",
                "at sun.nio.ch.Net.bind(Net.java:425)",
                "at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)",
                "at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)",
                "at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)",
                "at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)",
                "at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)",
                "at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2017-03-05 08:37:13,304 INFO  util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 1",
                "2017-03-05 08:37:13,308 INFO  nfs3.Nfs3Base (LogAdapter.java:info(45)) - SHUTDOWN_MSG: \n/************************************************************"
            ],
            "RootCause": "The root cause of the bind failure is that the application is attempting to bind to port 4242, which is already in use by another process or is in a TIME_WAIT state due to a previous connection not being fully released.",
            "StepsToReproduce": [
                "Start the application that uses SimpleTCPServer and Portmap.",
                "Attempt to bind to port 4242 while another instance of the application or a different application is already using that port."
            ],
            "ExpectedBehavior": "The application should successfully bind to port 4242 and start the TCP server without any exceptions.",
            "ObservedBehavior": "The application fails to bind to port 4242, resulting in a ChannelException and a BindException indicating that the address is already in use.",
            "Suggestions": "To resolve this issue, consider the following solutions: Check for any running processes using port 4242 and terminate them, modify the application to use a different port, or implement the setReuseAddress option in the socket configuration.",
            "problem_location": {
                "files": [
                    "SimpleTcpServer.java",
                    "MountdBase.java"
                ],
                "classes": [
                    "org.apache.hadoop.oncrpc.SimpleTcpServer",
                    "org.apache.hadoop.mount.MountdBase"
                ],
                "methods": [
                    "SimpleTcpServer.run",
                    "MountdBase.startTCPServer"
                ]
            },
            "possible_fix": "Modify the socket binding code in SimpleTcpServer to include the setReuseAddress option, allowing the socket to bind even if it is in a TIME_WAIT state."
        },
        "possible_fix_code": {
            "SimpleTcpServer.run": "  public void run() {\n    // Configure the Server.\n    ChannelFactory factory;\n    if (workerCount == 0) {\n      // Use default workers: 2 * the number of available processors\n      factory = new NioServerSocketChannelFactory(\n          Executors.newCachedThreadPool(), Executors.newCachedThreadPool());\n    } else {\n      factory = new NioServerSocketChannelFactory(\n          Executors.newCachedThreadPool(), Executors.newCachedThreadPool(),\n          workerCount);\n    }\n\n    server = new ServerBootstrap(factory);\n    server.setPipelineFactory(new ChannelPipelineFactory() {\n\n      @Override\n      public ChannelPipeline getPipeline() throws Exception {\n        return Channels.pipeline(RpcUtil.constructRpcFrameDecoder(),\n            RpcUtil.STAGE_RPC_MESSAGE_PARSER, rpcProgram,\n            RpcUtil.STAGE_RPC_TCP_RESPONSE);\n      }\n    });\n    server.setOption(\"child.tcpNoDelay\", true);\n    server.setOption(\"child.keepAlive\", true);\n    // Set the SO_REUSEADDR option to allow binding to the port even if it is in TIME_WAIT state\n    server.setOption(\"child.reuseAddress\", true);\n\n    // Listen to TCP port\n    ch = server.bind(new InetSocketAddress(port));\n    InetSocketAddress socketAddr = (InetSocketAddress) ch.getLocalAddress();\n    boundPort = socketAddr.getPort();\n\n    LOG.info(\"Started listening to TCP requests at port \" + boundPort + \" for \"\n        + rpcProgram + \" with workerCount \" + workerCount);\n  }"
        }
    },
    {
        "filename": "HDFS-2991.json",
        "creation_time": "2012-02-23T02:06:51.000+0000",
        "bug_report": {
            "Title": "Failure to Load Edits: ClassCastException",
            "Description": "During scale testing of trunk at revision r1291606, an IOException occurred while attempting to replay the edit log at a specific offset. The error indicates a ClassCastException when the system tries to cast an INodeFile to an INodeFileUnderConstruction. This suggests a potential inconsistency in the state of the file system or the edit log, possibly due to a corrupted edit log or an unexpected state of the file being processed.",
            "StackTrace": [
                "java.io.IOException: Error replaying edit log at offset 1354251",
                "Recent opcode offsets: 1350014 1350176 1350312 1354251",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:418)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:93)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:79)",
                "Caused by: java.lang.ClassCastException: org.apache.hadoop.hdfs.server.namenode.INodeFile cannot be cast to org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:213)"
            ],
            "RootCause": "The root cause of the issue is a ClassCastException occurring in the applyEditLogOp method of the FSEditLogLoader class. This happens when the method attempts to cast an INodeFile to an INodeFileUnderConstruction, indicating that the edit log is trying to close a file that is not in the expected state.",
            "StepsToReproduce": [
                "1. Perform scale testing on the Hadoop HDFS trunk at revision r1291606.",
                "2. Trigger the loading of the edit log that includes the problematic offset (1354251).",
                "3. Observe the resulting IOException and ClassCastException in the logs."
            ],
            "ExpectedBehavior": "The system should successfully replay the edit log without any exceptions, correctly handling the state of files being closed or modified.",
            "ObservedBehavior": "An IOException is thrown indicating an error while replaying the edit log, specifically a ClassCastException when attempting to cast an INodeFile to an INodeFileUnderConstruction.",
            "Suggestions": "To resolve this issue, check the integrity of the edit log and ensure that the file system state is consistent with the operations being applied. If the edit log is found to be corrupted, consider repairing it or restoring from a backup.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.java",
                    "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFile.java",
                    "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader",
                    "org.apache.hadoop.hdfs.server.namenode.INodeFile",
                    "org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction"
                ],
                "methods": [
                    "FSEditLogLoader.loadEditRecords",
                    "FSEditLogLoader.applyEditLogOp"
                ]
            },
            "possible_fix": "Review the applyEditLogOp method to ensure that it correctly handles the state of files being closed. Implement checks to prevent casting errors by verifying the state of the INode before attempting to cast it."
        },
        "possible_fix_code": {
            "FSEditLogLoader.applyEditLogOp": "  private void applyEditLogOp(FSEditLogOp op, FSDirectory fsDir,\n      int logVersion) throws IOException {\n    switch (op.opCode) {\n    case OP_ADD: {\n      AddCloseOp addCloseOp = (AddCloseOp)op;\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + addCloseOp.path +\n            \" numblocks : \" + addCloseOp.blocks.length +\n            \" clientHolder \" + addCloseOp.clientName +\n            \" clientMachine \" + addCloseOp.clientMachine);\n      }\n      INodeFile oldFile = getINodeFile(fsDir, addCloseOp.path);\n      INodeFile newFile = oldFile;\n      if (oldFile == null) { // this is OP_ADD on a new file (case 1)\n        final short replication  = fsNamesys.getBlockManager().adjustReplication(addCloseOp.replication);\n        PermissionStatus permissions = fsNamesys.getUpgradePermission();\n        if (addCloseOp.permissions != null) {\n          permissions = addCloseOp.permissions;\n        }\n        long blockSize = addCloseOp.blockSize;\n        if (-8 <= logVersion && blockSize == 0) {\n          if (addCloseOp.blocks.length > 1) {\n            blockSize = addCloseOp.blocks[0].getNumBytes();\n          } else {\n            long first = ((addCloseOp.blocks.length == 1)?\n                addCloseOp.blocks[0].getNumBytes(): 0);\n            blockSize = Math.max(fsNamesys.getDefaultBlockSize(), first);\n          }\n        }\n        newFile = (INodeFile)fsDir.unprotectedAddFile(\n            addCloseOp.path, permissions,\n            replication, addCloseOp.mtime,\n            addCloseOp.atime, blockSize,\n            true, addCloseOp.clientName, addCloseOp.clientMachine);\n        fsNamesys.leaseManager.addLease(addCloseOp.clientName, addCloseOp.path);\n      } else { // This is OP_ADD on an existing file\n        if (!oldFile.isUnderConstruction()) {\n          if (FSNamesystem.LOG.isDebugEnabled()) {\n            FSNamesystem.LOG.debug(\"Reopening an already-closed file \" +\n                \"for append\");\n          }\n          fsNamesys.prepareFileForWrite(addCloseOp.path, oldFile,\n              addCloseOp.clientName, addCloseOp.clientMachine, null,\n              false);\n          newFile = getINodeFile(fsDir, addCloseOp.path);\n        }\n      }\n      updateBlocks(fsDir, addCloseOp, newFile);\n      break;\n    }\n    case OP_CLOSE: {\n      AddCloseOp addCloseOp = (AddCloseOp)op;\n      if (FSNamesystem.LOG.isDebugEnabled()) {\n        FSNamesystem.LOG.debug(op.opCode + \": \" + addCloseOp.path +\n            \" numblocks : \" + addCloseOp.blocks.length +\n            \" clientHolder \" + addCloseOp.clientName +\n            \" clientMachine \" + addCloseOp.clientMachine);\n      }\n      INodeFile oldFile = getINodeFile(fsDir, addCloseOp.path);\n      if (oldFile == null) {\n        throw new IOException(\"Operation trying to close non-existent file \" +\n            addCloseOp.path);\n      }\n      updateBlocks(fsDir, addCloseOp, oldFile);\n      if (!(oldFile instanceof INodeFileUnderConstruction)) {\n        throw new IOException(\"Cannot close a file that is not under construction: \" + addCloseOp.path);\n      }\n      INodeFileUnderConstruction ucFile = (INodeFileUnderConstruction) oldFile;\n      fsNamesys.leaseManager.removeLeaseWithPrefixPath(addCloseOp.path);\n      INodeFile newFile = ucFile.convertToInodeFile();\n      fsDir.replaceNode(addCloseOp.path, ucFile, newFile);\n      break;\n    }\n    // Other cases remain unchanged...\n    default:\n      throw new IOException(\"Invalid operation read \" + op.opCode);\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-4404.json",
        "creation_time": "2013-01-15T01:43:17.000+0000",
        "bug_report": {
            "Title": "Create file failure when the machine of first attempted NameNode is down",
            "Description": "In a Hadoop environment with multiple NameNodes and DataNodes, a failure occurs when attempting to create a file if the first NameNode (NN1) is down. The client attempts to connect to the IP address 160.161.0.155 on port 8020, which corresponds to NN1. The connection fails due to a SocketTimeoutException, indicating that the client cannot establish a connection within the specified timeout period of 10 seconds. This issue is exacerbated by the unavailability of the first NameNode, leading to a cascading failure in file creation operations.",
            "StackTrace": [
                "java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]",
                "at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)",
                "at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:524)",
                "at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:474)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1286)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1156)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:187)",
                "at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1128)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:232)",
                "at test.TestLease.main(TestLease.java:45)"
            ],
            "RootCause": "The root cause of the issue is the unavailability of the first NameNode (NN1) at IP address 160.161.0.155, which leads to a SocketTimeoutException when the client attempts to connect to it. The connection cannot be established within the timeout period due to the server being down.",
            "StepsToReproduce": [
                "1. Set up a Hadoop environment with two NameNodes (NN1 and NN2) and multiple DataNodes.",
                "2. Ensure that NN1 is down or unreachable.",
                "3. Attempt to create a file using the Hadoop Distributed File System (HDFS) API, targeting NN1.",
                "4. Observe the resulting SocketTimeoutException in the logs."
            ],
            "ExpectedBehavior": "The system should successfully create a file by connecting to an available NameNode (NN2) if the first NameNode (NN1) is down.",
            "ObservedBehavior": "The system fails to create a file and throws a SocketTimeoutException due to the inability to connect to the downed NameNode (NN1).",
            "Suggestions": "To resolve this issue, consider the following steps: 1. Ensure that the first NameNode (NN1) is operational and reachable. 2. Implement a failover mechanism to automatically switch to the second NameNode (NN2) when NN1 is down. 3. Review and adjust the Hadoop configuration to improve connection handling and timeout settings.",
            "problem_location": {
                "files": [
                    "Client.java",
                    "NetUtils.java",
                    "SocketIOWithTimeout.java",
                    "DFSClient.java",
                    "DistributedFileSystem.java"
                ],
                "classes": [
                    "org.apache.hadoop.ipc.Client",
                    "org.apache.hadoop.net.NetUtils",
                    "org.apache.hadoop.net.SocketIOWithTimeout",
                    "org.apache.hadoop.hdfs.DFSClient",
                    "org.apache.hadoop.hdfs.DistributedFileSystem"
                ],
                "methods": [
                    "Client.getConnection",
                    "NetUtils.connect",
                    "SocketIOWithTimeout.connect",
                    "DFSClient.create",
                    "DistributedFileSystem.create"
                ]
            },
            "possible_fix": "Ensure that the first NameNode (NN1) is operational. If NN1 is expected to be down, modify the client configuration to prioritize connections to NN2 or implement a failover strategy. Additionally, consider increasing the socket timeout value in the configuration to accommodate slower network conditions."
        },
        "possible_fix_code": {
            "org.apache.hadoop.ipc.Client.getConnection": "public Connection getConnection() throws IOException {\n    // Attempt to connect to the first NameNode (NN1)\n    try {\n        return NetUtils.connect(socket, nn1Address, timeout);\n    } catch (SocketTimeoutException e) {\n        // Log the timeout exception\n        LOG.warn(\"Connection to NN1 timed out, attempting to connect to NN2...\");\n        // Attempt to connect to the second NameNode (NN2)\n        try {\n            return NetUtils.connect(socket, nn2Address, timeout);\n        } catch (SocketTimeoutException e2) {\n            // Log the timeout exception for NN2\n            LOG.error(\"Connection to NN2 also timed out.\", e2);\n            throw e2; // Rethrow the exception if both connections fail\n        }\n    }\n}"
        }
    },
    {
        "filename": "HDFS-8276.json",
        "creation_time": "2015-04-28T10:11:35.000+0000",
        "bug_report": {
            "Title": "LazyPersistFileScrubber should be disabled if scrubber interval configured zero",
            "Description": "The issue arises when the Hadoop NameNode is configured with a scrub interval of zero for the property `dfs.namenode.lazypersist.file.scrub.interval.sec`. According to the error message, this configuration leads to an `IllegalArgumentException` during the initialization of the FSNamesystem, causing the NameNode startup to fail. The expected behavior is that a zero value should disable the scrubber instead of causing a failure.",
            "StackTrace": [
                "2015-04-27 23:47:31,744 ERROR org.apache.hadoop.hdfs.server.namenode.FSNamesystem: FSNamesystem initialization failed.",
                "java.lang.IllegalArgumentException: dfs.namenode.lazypersist.file.scrub.interval.sec must be non-zero.",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:828)"
            ],
            "RootCause": "The root cause of the issue is that the configuration property `dfs.namenode.lazypersist.file.scrub.interval.sec` is set to zero, which is not allowed and leads to an `IllegalArgumentException` during the initialization of the FSNamesystem.",
            "StepsToReproduce": [
                "Set the property `dfs.namenode.lazypersist.file.scrub.interval.sec` to zero in the Hadoop configuration.",
                "Attempt to start the Hadoop NameNode."
            ],
            "ExpectedBehavior": "The NameNode should start successfully without errors, and the scrubber should be disabled when the interval is set to zero.",
            "ObservedBehavior": "The NameNode fails to start, throwing an `IllegalArgumentException` indicating that the scrub interval must be non-zero.",
            "Suggestions": "Modify the `dfs.namenode.lazypersist.file.scrub.interval.sec` property in the Hadoop configuration to a non-zero value (e.g., 60 seconds) and restart the Hadoop services.",
            "problem_location": {
                "files": [
                    "hdfs-site.xml"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem"
                ],
                "methods": []
            },
            "possible_fix": "To resolve the issue, update the `hdfs-site.xml` file to include the following property:\n\n```xml\n<property>\n    <name>dfs.namenode.lazypersist.file.scrub.interval.sec</name>\n    <value>60</value> <!-- Set this to a non-zero value as required -->\n</property>\n```\n\nAfter making this change, save the file and restart the Hadoop services."
        },
        "possible_fix_code": {
            "FSNamesystem": "public FSNamesystem(Configuration conf) throws IOException {\n    // Check the scrub interval configuration\n    int scrubInterval = conf.getInt(\"dfs.namenode.lazypersist.file.scrub.interval.sec\", 0);\n    if (scrubInterval <= 0) {\n        // Log a warning and disable the scrubber\n        LOG.warn(\"Scrubber interval is set to zero or negative. Disabling the scrubber.\");\n        this.scrubberEnabled = false;\n    } else {\n        this.scrubberEnabled = true;\n        // Initialize scrubber with the valid interval\n        this.scrubber = new Scrubber(scrubInterval);\n    }\n    // Other initialization code...\n}"
        }
    },
    {
        "filename": "HDFS-12369.json",
        "creation_time": "2017-08-28T22:56:02.000+0000",
        "bug_report": {
            "Title": "Edit log corruption due to hard lease recovery of not-closed file which has snapshots",
            "Description": "The issue arises from a `FileNotFoundException` encountered during the initialization of the NameNode in HDFS. This exception indicates that a specific file, which is expected to exist, is missing. The stack trace reveals that the error occurs while loading the filesystem image and edit logs, specifically when the `FSEditLogLoader` attempts to apply edit log operations. The logs show a sequence of operations on the file, including an unexpected deletion without a prior close operation, which is critical for maintaining file integrity. The lease manager's hard limit recovery process triggered a close operation on the file, leading to this inconsistency.",
            "StackTrace": [
                "2017-08-17 14:32:18,418 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.",
                "java.io.FileNotFoundException: File does not exist: /home/Events/CancellationSurvey_MySQL/2015/12/31/.part-00000.9nlJ3M",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:66)",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:429)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:232)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:141)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:897)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:750)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:318)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1125)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:789)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:614)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:676)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:844)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:823)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1547)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1615)"
            ],
            "RootCause": "The root cause of the issue is the failure to properly close a file before it was deleted, leading to a `FileNotFoundException` when the NameNode attempts to load the filesystem image and edit logs. The lease manager's recovery process triggered a close operation due to a hard lease limit, but this was not logged correctly, resulting in the file being marked as deleted without a proper close.",
            "StepsToReproduce": [
                "1. Create a file in HDFS and perform operations such as OPEN, ADD_BLOCK, and CLOSE.",
                "2. Simulate an append operation by adding another block to the file.",
                "3. Delete the file without closing it properly.",
                "4. Wait for the lease manager to trigger a hard lease recovery.",
                "5. Attempt to restart the NameNode."
            ],
            "ExpectedBehavior": "The NameNode should successfully start and load the filesystem image and edit logs without encountering a `FileNotFoundException`. All files should be properly closed before deletion, ensuring data integrity.",
            "ObservedBehavior": "The NameNode fails to start due to a `FileNotFoundException`, indicating that the expected file does not exist at the specified path. This occurs during the loading of the filesystem image and edit logs.",
            "Suggestions": "To resolve the issue, verify the existence of the missing file and ensure that all files are properly closed before deletion. Additionally, review the lease management process to prevent premature file deletions.",
            "problem_location": {
                "files": [
                    "FSNamesystem.java",
                    "FSEditLogLoader.java",
                    "NameNode.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem",
                    "org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader",
                    "org.apache.hadoop.hdfs.server.namenode.NameNode"
                ],
                "methods": [
                    "FSNamesystem.loadFromDisk",
                    "FSEditLogLoader.applyEditLogOp",
                    "NameNode.initialize"
                ]
            },
            "possible_fix": "Ensure that the file close operation is logged correctly before any delete operation is performed. Review the lease management logic to prevent files from being deleted without proper closure. Consider adding checks to verify file existence before attempting to load the filesystem image."
        },
        "possible_fix_code": {
            "FSNamesystem.loadFromDisk": "  static FSNamesystem loadFromDisk(Configuration conf) throws IOException {\n\n    checkConfiguration(conf);\n    FSImage fsImage = new FSImage(conf,\n        FSNamesystem.getNamespaceDirs(conf),\n        FSNamesystem.getNamespaceEditsDirs(conf));\n    FSNamesystem namesystem = new FSNamesystem(conf, fsImage, false);\n    StartupOption startOpt = NameNode.getStartupOption(conf);\n    if (startOpt == StartupOption.RECOVER) {\n      namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n    }\n\n    long loadStart = monotonicNow();\n    try {\n      namesystem.loadFSImage(startOpt);\n    } catch (FileNotFoundException fnfe) {\n      LOG.error(\"File not found during loading FSImage: \" + fnfe.getMessage());\n      // Additional logging or handling can be added here\n      fsImage.close();\n      throw fnfe;\n    } catch (IOException ioe) {\n      LOG.warn(\"Encountered exception loading fsimage\", ioe);\n      fsImage.close();\n      throw ioe;\n    }\n    long timeTakenToLoadFSImage = monotonicNow() - loadStart;\n    LOG.info(\"Finished loading FSImage in \" + timeTakenToLoadFSImage + \" msecs\");\n    NameNodeMetrics nnMetrics = NameNode.getNameNodeMetrics();\n    if (nnMetrics != null) {\n      nnMetrics.setFsImageLoadTime((int) timeTakenToLoadFSImage);\n    }\n    namesystem.getFSDirectory().createReservedStatuses(namesystem.getCTime());\n    return namesystem;\n  }"
        }
    },
    {
        "filename": "HDFS-6462.json",
        "creation_time": "2014-05-29T00:19:03.000+0000",
        "bug_report": {
            "Title": "NFS: fsstat request fails with the secure hdfs",
            "Description": "The fsstat request fails in a secure Hadoop environment due to Kerberos authentication issues. The error indicates that no valid credentials were provided, which is likely caused by the inability to find a valid Kerberos ticket-granting ticket (TGT). This issue arises when the NFS server is started as UserA, who is configured with a keytab file and principal, but the authentication fails when attempting to mount the NFS as the root user.",
            "StackTrace": [
                "2014-05-29 00:09:13,698 DEBUG nfs3.RpcProgramNfs3 (RpcProgramNfs3.java:fsstat(1654)) - NFS FSSTAT fileId: 16385",
                "2014-05-29 00:09:13,706 WARN  ipc.Client (Client.java:run(672)) - Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "2014-05-29 00:09:13,710 WARN  nfs3.RpcProgramNfs3 (RpcProgramNfs3.java:fsstat(1681)) - Exception",
                "java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: \"host1/0.0.0.0\"; destination host is: \"host1\":8020;"
            ],
            "RootCause": "The root cause of the issue is the failure to authenticate due to missing or invalid Kerberos credentials. The error message indicates that the application cannot find a valid Kerberos ticket-granting ticket (TGT), which is essential for establishing a secure connection.",
            "StepsToReproduce": [
                "1) Create user named UserB and UserA",
                "2) Create group named GroupB",
                "3) Add root and UserB users to GroupB, ensuring UserA is not in GroupB",
                "4) Set the following properties in hdfs-site.xml and core-site.xml:",
                "   - hdfs-site.xml: Set dfs.nfs.keytab.file to /tmp/keytab/UserA.keytab and dfs.nfs.kerberos.principal to UserA@EXAMPLE.COM",
                "   - core-site.xml: Set hadoop.proxyuser.UserA.groups to GroupB and hadoop.proxyuser.UserA.hosts to *",
                "5) Start the NFS server as UserA",
                "6) Mount NFS as the root user",
                "7) Run the command: df /tmp/tmp_mnt/"
            ],
            "ExpectedBehavior": "The command 'df /tmp/tmp_mnt/' should return the disk usage statistics without any errors, indicating successful NFS mount and access.",
            "ObservedBehavior": "The command fails with an Input/output error, and the NFS logs indicate a failure in Kerberos authentication due to missing valid credentials.",
            "Suggestions": "To resolve the issue, verify the following: 1) Ensure the keytab file exists and is accessible at the specified path. 2) Confirm that the principal in the keytab matches the one used in the configuration. 3) Renew the Kerberos ticket using the 'kinit' command if it has expired. 4) Review the Kerberos configuration file (krb5.conf) for correctness. 5) Test network connectivity to the Kerberos Key Distribution Center (KDC).",
            "problem_location": {
                "files": [
                    "hdfs-site.xml",
                    "core-site.xml"
                ],
                "classes": [
                    "org.apache.hadoop.security.UserGroupInformation",
                    "org.apache.hadoop.ipc.Client",
                    "org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3"
                ],
                "methods": [
                    "UserGroupInformation.loginUserFromKeytabAndReturnUGI",
                    "UserGroupInformation.loginUserFromKeytab",
                    "UserGroupInformation.getCredentials"
                ]
            },
            "possible_fix": "Ensure the keytab file is correctly specified and accessible. Validate the principal name in the keytab file. If necessary, update the configuration files and restart the NFS server."
        },
        "possible_fix_code": {
            "org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.fsstat": "  public FSSTAT3Response fsstat(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    FSSTAT3Response response = new FSSTAT3Response(Nfs3Status.NFS3_OK);\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    FSSTAT3Request request = null;\n    try {\n      request = new FSSTAT3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid FSSTAT request\");\n      return new FSSTAT3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    FileHandle handle = request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS FSSTAT fileId: \" + handle.getFileId());\n    }\n\n    try {\n      // Ensure the user has valid Kerberos credentials before proceeding\n      UserGroupInformation ugi = UserGroupInformation.getLoginUser();\n      if (ugi == null || !ugi.hasKerberosCredentials()) {\n        response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n        return response;\n      }\n\n      // Use superUserClient to get file system status\n      FsStatus fsStatus = superUserClient.getDiskStatus();\n      long totalBytes = fsStatus.getCapacity();\n      long freeBytes = fsStatus.getRemaining();\n      \n      Nfs3FileAttributes attrs = writeManager.getFileAttr(dfsClient, handle,\n          iug);\n      if (attrs == null) {\n        LOG.info(\"Can't get path for fileId:\" + handle.getFileId());\n        return new FSSTAT3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n      \n      long maxFsObjects = config.getLong(\"dfs.max.objects\", 0);\n      if (maxFsObjects == 0) {\n        maxFsObjects = Integer.MAX_VALUE;\n      }\n      \n      return new FSSTAT3Response(Nfs3Status.NFS3_OK, attrs, totalBytes,\n          freeBytes, freeBytes, maxFsObjects, maxFsObjects, maxFsObjects, 0);\n    } catch (RemoteException r) {\n      LOG.warn(\"Exception \", r);\n      IOException io = r.unwrapRemoteException();\n      if (io instanceof AuthorizationException) {\n        return new FSSTAT3Response(Nfs3Status.NFS3ERR_ACCES);\n      } else {\n        return new FSSTAT3Response(Nfs3Status.NFS3ERR_IO);\n      }\n    } catch (IOException e) {\n      LOG.warn(\"Exception \", e);\n      return new FSSTAT3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }"
        }
    },
    {
        "filename": "HDFS-5425.json",
        "creation_time": "2013-10-25T06:53:28.000+0000",
        "bug_report": {
            "Title": "Renaming underconstruction file with snapshots can make NN failure on restart",
            "Description": "The issue arises during snapshot operations such as createSnapshot and renameSnapshot. Upon restarting the NameNode (NN), an exception is thrown, leading to a shutdown. The stack trace indicates a `java.lang.IllegalStateException` triggered by the `replaceChild` method in the `INodeDirectoryWithSnapshot` class, specifically when attempting to replace a child node in the directory structure. This suggests that the state of the `ChildrenDiff` object is invalid for the replacement operation, likely due to the absence of the expected child node.",
            "StackTrace": [
                "2013-10-24 21:07:03,040 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join",
                "java.lang.IllegalStateException",
                "\tat com.google.common.base.Preconditions.checkState(Preconditions.java:133)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$ChildrenDiff.replace(INodeDirectoryWithSnapshot.java:82)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$ChildrenDiff.access$700(INodeDirectoryWithSnapshot.java:62)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$DirectoryDiffList.replaceChild(INodeDirectoryWithSnapshot.java:397)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$DirectoryDiffList.access$900(INodeDirectoryWithSnapshot.java:376)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot.replaceChild(INodeDirectoryWithSnapshot.java:598)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedReplaceINodeFile(FSDirectory.java:1548)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.replaceINodeFile(FSDirectory.java:1537)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.loadFilesUnderConstruction(FSImageFormat.java:855)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.load(FSImageFormat.java:350)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:910)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:899)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:751)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:720)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:266)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:784)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:563)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:422)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:472)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:670)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:655)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1245)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1311)",
                "2013-10-24 21:07:03,050 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1",
                "2013-10-24 21:07:03,052 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:"
            ],
            "RootCause": "The `IllegalStateException` is thrown because the `replaceChild` method in the `ChildrenDiff` class checks that the child being replaced is indeed the expected `oldChild`. If the `oldChild` is not found in the list or if the node at the found index is not the same as `oldChild`, the method throws this exception, indicating an invalid state for the replacement operation.",
            "StepsToReproduce": [
                "1. Perform snapshot operations such as createSnapshot or renameSnapshot.",
                "2. Restart the NameNode.",
                "3. Observe the exception thrown during the startup process."
            ],
            "ExpectedBehavior": "The NameNode should start up without exceptions, successfully loading the filesystem image and handling any under-construction files appropriately.",
            "ObservedBehavior": "The NameNode fails to start, throwing a `java.lang.IllegalStateException` during the loading of the filesystem image, leading to a shutdown.",
            "Suggestions": "Investigate the state of the `ChildrenDiff` object before the replacement operation to ensure that the expected child node exists. Consider adding checks to handle cases where the child node is not found or is in an unexpected state.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeDirectoryWithSnapshot.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot"
                ],
                "methods": [
                    "INodeDirectoryWithSnapshot.replaceChild",
                    "INodeDirectoryWithSnapshot.ChildrenDiff.replace"
                ]
            },
            "possible_fix": "In the `replaceChild` method, add validation to check if the `oldChild` exists in the list before attempting to replace it. If it does not exist, handle the situation gracefully, possibly by logging a warning or throwing a more descriptive exception."
        },
        "possible_fix_code": {
            "INodeDirectoryWithSnapshot.replaceChild": "public void replaceChild(final INode oldChild, final INode newChild,\n      final INodeMap inodeMap) {\n    super.replaceChild(oldChild, newChild, inodeMap);\n    if (oldChild == null || newChild == null) {\n        throw new IllegalArgumentException(\"oldChild and newChild must not be null\");\n    }\n    if (!diffs.contains(oldChild)) {\n        // Log a warning or handle the situation gracefully\n        System.err.println(\"Warning: oldChild not found in diffs, cannot replace.\");\n        return;\n    }\n    diffs.replaceChild(ListType.CREATED, oldChild, newChild);\n  }"
        }
    },
    {
        "filename": "HDFS-13145.json",
        "creation_time": "2018-02-13T19:22:56.000+0000",
        "bug_report": {
            "Title": "SBN crash when transition to ANN with in-progress edit tailing enabled",
            "Description": "The issue arises when the system is configured with in-progress edit log tailing enabled. During the transition from Standby NameNode (SBN) to Active NameNode (ANN), the system attempts to send two batches of edits to Journal Nodes (JNs): a normal edit batch followed by a dummy batch to update the commit ID. If the ANN crashes during this process, the SBN encounters an `IllegalStateException` when trying to write to the edit log, as it detects active read streams that conflict with the write operation. This situation leads to a failure in state management within the NameNode, particularly in a High Availability (HA) setup.",
            "StackTrace": [
                "java.lang.IllegalStateException: Cannot start writing at txid 24312595802 when there is a stream available for read: ......",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.openForWrite(FSEditLog.java:329)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1196)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1839)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1707)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1622)",
                "at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)",
                "at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:4460)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:851)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:794)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:249)"
            ],
            "RootCause": "The root cause of the crash is an `IllegalStateException` triggered when the NameNode attempts to write to the edit log while there are active read streams. This indicates improper state management during the transition from standby to active state, particularly in a High Availability (HA) setup.",
            "StepsToReproduce": [
                "1. Enable in-progress edit log tailing in the configuration.",
                "2. Start the SBN and allow it to process edits.",
                "3. Trigger a transition to the ANN while the SBN is still processing edits.",
                "4. Observe the system behavior during the transition."
            ],
            "ExpectedBehavior": "The system should successfully transition from SBN to ANN without encountering any exceptions, allowing the NameNode to write to the edit log without conflicts from active read streams.",
            "ObservedBehavior": "The system crashes with an `IllegalStateException`, indicating that it cannot start writing to the edit log due to active read streams, resulting in no running NameNodes in the cluster.",
            "Suggestions": "Implement additional synchronization mechanisms to ensure that all read operations are completed before allowing the NameNode to transition to the active state. This may involve modifying the state management logic in the `startActiveServices` and `openForWrite` methods.",
            "problem_location": {
                "files": [
                    "FSEditLog.java",
                    "FSNamesystem.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSEditLog",
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem"
                ],
                "methods": [
                    "FSEditLog.openForWrite",
                    "FSNamesystem.startActiveServices"
                ]
            },
            "possible_fix": "In the `openForWrite` method, ensure that the state checks for active read streams are robust and consider implementing a mechanism to wait for all read operations to complete before allowing write operations to proceed. Additionally, review the state transition logic in `startActiveServices` to ensure proper synchronization."
        },
        "possible_fix_code": {
            "FSEditLog.openForWrite": "  synchronized void openForWrite(int layoutVersion) throws IOException {\n    Preconditions.checkState(state == State.BETWEEN_LOG_SEGMENTS,\n        \"Bad state: %s\", state);\n\n    long segmentTxId = getLastWrittenTxId() + 1;\n    // Safety check: we should never start a segment if there are\n    // newer txids readable.\n    List<EditLogInputStream> streams = new ArrayList<EditLogInputStream>();\n    journalSet.selectInputStreams(streams, segmentTxId, true, false);\n    if (!streams.isEmpty()) {\n      String error = String.format(\"Cannot start writing at txid %s \" +\n        \"when there is a stream available for read: %s\",\n        segmentTxId, streams.get(0));\n      IOUtils.cleanupWithLogger(LOG,\n          streams.toArray(new EditLogInputStream[0]));\n      throw new IllegalStateException(error);\n    }\n\n    // Wait for all read operations to complete before proceeding\n    waitForReadOperationsToComplete();\n\n    startLogSegmentAndWriteHeaderTxn(segmentTxId, layoutVersion);\n    assert state == State.IN_SEGMENT : \"Bad state: \" + state;\n  }"
        }
    },
    {
        "filename": "HDFS-8807.json",
        "creation_time": "2015-07-22T20:45:55.000+0000",
        "bug_report": {
            "Title": "dfs.datanode.data.dir does not handle spaces between storageType and URI correctly",
            "Description": "The issue arises when a space is inadvertently added between the storage type and the file URI in the configuration for `dfs.datanode.data.dir`. This misconfiguration leads to a failure during the DataNode startup process. For example, the configuration:\n\n```xml\n<property>\n    <name>dfs.datanode.data.dir</name>\n    <value>\n      [DISK] file://tmp/hadoop-aengineer/disk1/dfs/data\n    </value>\n  </property>\n```\n\nresults in a parsing error due to an illegal character in the URI scheme. The correct configuration should not have a space:\n\n```xml\n<property>\n    <name>dfs.datanode.data.dir</name>\n    <value>\n      [DISK]file://tmp/hadoop-aengineer/disk1/dfs/data\n    </value>\n  </property>\n```\n\nThe DataNode logs indicate a `java.lang.IllegalArgumentException` caused by a `java.net.URISyntaxException`, specifically stating that there is an illegal character in the scheme name at index 0.",
            "StackTrace": [
                "2015-07-22 13:34:36,812 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain",
                "java.lang.IllegalArgumentException: java.net.URISyntaxException: Illegal character in scheme name at index 0:  file://tmp/hadoop-aengineer/disk1/dfs/data",
                "at org.apache.hadoop.fs.Path.initialize(Path.java:204)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:170)",
                "at org.apache.hadoop.hdfs.server.datanode.StorageLocation.parse(StorageLocation.java:97)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.getStorageLocations(DataNode.java:2314)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2298)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2349)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2529)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2553)",
                "Caused by: java.net.URISyntaxException: Illegal character in scheme name at index 0:  file://tmp/hadoop-aengineer/disk1/dfs/data",
                "at java.net.URI$Parser.fail(URI.java:2829)",
                "at java.net.URI$Parser.checkChars(URI.java:3002)",
                "at java.net.URI$Parser.checkChar(URI.java:3012)",
                "at java.net.URI.parse(URI.java:3028)",
                "at java.net.URI.<init>(URI.java:753)",
                "at org.apache.hadoop.fs.Path.initialize(Path.java:201)"
            ],
            "RootCause": "The root cause of the issue is an improperly formatted URI in the configuration for `dfs.datanode.data.dir`. Specifically, the URI is missing a valid scheme due to the presence of a space between the storage type and the URI, leading to a `URISyntaxException` when the `Path` class attempts to initialize the URI.",
            "StepsToReproduce": [
                "1. Modify the `dfs.datanode.data.dir` property in the configuration file to include a space between the storage type and the URI, e.g., `[DISK] file://tmp/hadoop-aengineer/disk1/dfs/data`.",
                "2. Start the DataNode service.",
                "3. Observe the startup logs for errors."
            ],
            "ExpectedBehavior": "The DataNode should start successfully without any parsing errors, and the storage locations should be correctly initialized based on the provided configuration.",
            "ObservedBehavior": "The DataNode fails to start, logging a fatal error due to an `IllegalArgumentException` caused by a `URISyntaxException`, indicating an illegal character in the scheme name.",
            "Suggestions": "Ensure that the configuration for `dfs.datanode.data.dir` does not contain any spaces between the storage type and the URI. The correct format should be `[DISK]file://tmp/hadoop-aengineer/disk1/dfs/data`.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.StorageLocation.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.datanode.StorageLocation",
                    "org.apache.hadoop.fs.Path"
                ],
                "methods": [
                    "StorageLocation.parse",
                    "Path.initialize"
                ]
            },
            "possible_fix": "Update the configuration to remove any spaces between the storage type and the URI. Ensure that the URI is formatted correctly as `file:///tmp/hadoop-aengineer/disk1/dfs/data`."
        },
        "possible_fix_code": {
            "StorageLocation.parse": "  public static StorageLocation parse(String rawLocation)\n      throws IOException, SecurityException {\n    Matcher matcher = regex.matcher(rawLocation);\n    StorageType storageType = StorageType.DEFAULT;\n    String location = rawLocation;\n\n    if (matcher.matches()) {\n      String classString = matcher.group(1);\n      location = matcher.group(2);\n      if (!classString.isEmpty()) {\n        storageType =\n            StorageType.valueOf(StringUtils.toUpperCase(classString));\n      }\n    }\n\n    // Fix the location format to ensure it has the correct URI scheme\n    if (location.startsWith(\"file://\")) {\n      location = location.replaceFirst(\"file://\", \"file:///\" );\n    }\n\n    return new StorageLocation(storageType, new Path(location).toUri());\n  }"
        }
    },
    {
        "filename": "HDFS-3436.json",
        "creation_time": "2012-05-17T10:36:31.000+0000",
        "bug_report": {
            "Title": "Adding new DataNode to existing pipeline fails in case of Append/Recovery",
            "Description": "The issue arises when attempting to append data to a file in an HDFS cluster with 4 DataNodes, where one DataNode (DN3) has been stopped. The failure occurs during the execution of the `addDatanode2ExistingPipeline` method, which is responsible for adding a new DataNode to the existing data pipeline. The stack trace indicates multiple exceptions, including `java.io.IOException` and `java.io.EOFException`, suggesting problems with DataNode connectivity and data streaming.",
            "StackTrace": [
                "2012-04-24 22:06:09,947 INFO  hdfs.DFSClient (DFSOutputStream.java:createBlockOutputStream(1063)) - Exception in createBlockOutputStream",
                "java.io.IOException: Bad connect ack with firstBadLink as *******:50010",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1053)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:943)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)",
                "2012-04-24 22:06:09,947 WARN  hdfs.DFSClient (DFSOutputStream.java:setupPipelineForAppendOrRecovery(916)) - Error Recovery for block BP-1023239-10.18.40.233-1335275282109:blk_296651611851855249_1253 in pipeline *****:50010, ******:50010, *****:50010: bad datanode ******:50010",
                "2012-04-24 22:06:10,072 WARN  hdfs.DFSClient (DFSOutputStream.java:run(549)) - DataStreamer Exception",
                "java.io.EOFException: Premature EOF: no length prefix available",
                "at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)"
            ],
            "RootCause": "The root cause of the issue is related to the failure of the `createBlockOutputStream` method in the `DFSOutputStream` class, which is unable to establish a connection to the DataNode due to a bad acknowledgment. This is compounded by EOFExceptions indicating unexpected end-of-file conditions, likely due to communication issues or data corruption.",
            "StepsToReproduce": [
                "1. Set up a cluster with 4 DataNodes.",
                "2. Write a file to 3 DataNodes (DN1, DN2, DN3).",
                "3. Stop DN3.",
                "4. Attempt to append data to the file."
            ],
            "ExpectedBehavior": "The system should successfully append data to the file by adding a new DataNode to the existing pipeline without any errors.",
            "ObservedBehavior": "The append operation fails with exceptions indicating bad connection acknowledgments and premature EOFs, preventing the addition of the new DataNode to the pipeline.",
            "Suggestions": "Investigate network connectivity issues between the client and DataNodes. Ensure that the DataNode being added is operational and correctly configured. Review the error handling in the `createBlockOutputStream` and `processDatanodeError` methods to improve resilience against transient errors.",
            "problem_location": {
                "files": [
                    "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.DFSOutputStream",
                    "org.apache.hadoop.hdfs.DataStreamer"
                ],
                "methods": [
                    "DFSOutputStream.createBlockOutputStream",
                    "DFSOutputStream.processDatanodeError",
                    "DFSOutputStream.addDatanode2ExistingPipeline"
                ]
            },
            "possible_fix": "Review and enhance the error handling logic in the `createBlockOutputStream` method to better manage connection failures. Consider implementing retries or fallback mechanisms when a DataNode is temporarily unavailable."
        },
        "possible_fix_code": {
            "createBlockOutputStream": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      Status pipelineStatus = SUCCESS;\n      String firstBadLink = \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i = 0; i < nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline = \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      boolean result = false;\n      DataOutputStream out = null;\n      try {\n        assert null == s : \"Previous socket unclosed\";\n        s = createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout = dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        //\n        // Xmit header info to datanode\n        //\n        out = new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(s, writeTimeout),\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        \n        assert null == blockReplyStream : \"Previous blockReplyStream unclosed\";\n        blockReplyStream = new DataInputStream(NetUtils.getInputStream(s));\n\n        // send the request\n        new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n            nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n            nodes.length, block.getNumBytes(), bytesSent, newGS, checksum);\n\n        // receive ack for connect\n        BlockOpResponseProto resp = BlockOpResponseProto.parseFrom(\n            HdfsProtoUtil.vintPrefixed(blockReplyStream));\n        pipelineStatus = resp.getStatus();\n        firstBadLink = resp.getFirstBadLink();\n        \n        if (pipelineStatus != SUCCESS) {\n          if (pipelineStatus == Status.ERROR_ACCESS_TOKEN) {\n            throw new InvalidBlockTokenException(\n                \"Got access token error for connect ack with firstBadLink as \"\n                    + firstBadLink);\n          } else {\n            throw new IOException(\"Bad connect ack with firstBadLink as \"\n                + firstBadLink);\n          }\n        }\n        assert null == blockStream : \"Previous blockStream unclosed\";\n        blockStream = out;\n        result =  true; // success\n\n      } catch (IOException ie) {\n\n        DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n\n        // find the datanode that matches\n        if (firstBadLink.length() != 0) {\n          for (int i = 0; i < nodes.length; i++) {\n            if (nodes[i].getXferAddr().equals(firstBadLink)) {\n              errorIndex = i;\n              break;\n            }\n          }\n        } else {\n          errorIndex = 0;\n        }\n        hasError = true;\n        setLastException(ie);\n        result =  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s = null;\n          IOUtils.closeStream(out);\n          out = null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream = null;\n        }\n      }\n\n      // Retry logic for transient errors\n      if (hasError) {\n        for (int retry = 0; retry < MAX_RETRIES; retry++) {\n          try {\n            // Attempt to re-establish connection and resend request\n            return createBlockOutputStream(nodes, newGS, recoveryFlag);\n          } catch (IOException e) {\n            DFSClient.LOG.info(\"Retrying createBlockOutputStream due to error: \", e);\n          }\n        }\n      }\n\n      return result;\n    }"
        }
    }
]