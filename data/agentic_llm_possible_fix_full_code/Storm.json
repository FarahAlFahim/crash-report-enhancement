[
    {
        "filename": "STORM-2443.json",
        "creation_time": "2017-03-31T08:09:04.000+0000",
        "bug_report": {
            "Title": "Nimbus throws error when changing log level on UI topology page",
            "Description": "A NullPointerException is thrown in the Nimbus class when attempting to change the logging configuration on the UI topology page. The stack trace indicates that the error occurs in the setLogConfig method, which is responsible for setting the logging configuration for a given topology. The parameters passed to this method may not be properly initialized, leading to the exception.",
            "StackTrace": [
                "2017-03-30 16:53:26.954 o.a.s.d.n.Nimbus pool-14-thread-56 [WARN] set log config topology exception. (topology id='rolling-1-1490860365')",
                "java.lang.NullPointerException: null",
                "at org.apache.storm.daemon.nimbus.Nimbus.setLogConfig(Nimbus.java:2688) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.generated.Nimbus$Processor$setLogConfig.getResult(Nimbus.java:3295) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.generated.Nimbus$Processor$setLogConfig.getResult(Nimbus.java:3280) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:160) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_66]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_66]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_66]"
            ],
            "RootCause": "The NullPointerException is likely caused by a null reference in one of the parameters passed to the setLogConfig method, particularly the LogConfig config parameter, which needs to be instantiated before being passed.",
            "StepsToReproduce": [
                "Navigate to the UI topology page.",
                "Attempt to change the logging level for a topology.",
                "Observe the error in the Nimbus log."
            ],
            "ExpectedBehavior": "The logging configuration should be updated without any errors, and the new log level should be reflected in the system.",
            "ObservedBehavior": "A NullPointerException is thrown, preventing the logging configuration from being updated.",
            "Suggestions": "Review the code that calls setLogConfig to ensure that all parameters, especially the LogConfig object, are properly initialized before the method is invoked.",
            "problem_location": {
                "files": [
                    "Nimbus.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.nimbus.Nimbus"
                ],
                "methods": [
                    "setLogConfig"
                ]
            },
            "possible_fix": "Ensure that the LogConfig object is instantiated and not null before calling setLogConfig. For example, check the calling code to confirm that a valid LogConfig instance is being passed."
        },
        "possible_fix_code": {
            "setLogConfig": "    public void setLogConfig(String name, LogConfig config, org.apache.thrift.async.AsyncMethodCallback resultHandler) throws org.apache.thrift.TException {\n      if (config == null) {\n          throw new IllegalArgumentException(\"LogConfig cannot be null\");\n      }\n      checkReady();\n      setLogConfig_call method_call = new setLogConfig_call(name, config, resultHandler, this, ___protocolFactory, ___transport);\n      this.___currentMethod = method_call;\n      ___manager.call(method_call);\n    }"
        }
    },
    {
        "filename": "STORM-3213.json",
        "creation_time": "2018-09-05T16:16:45.000+0000",
        "bug_report": {
            "Title": "500 Server Error on __acker component page on Storm UI",
            "Description": "A 500 Server Error occurs when attempting to access the __acker component page in the Storm UI. The error is traced back to a `NullPointerException` in the `getComponentPageInfo` method of the `Nimbus` class, which is invoked during the processing of the request. The stack trace indicates that the error propagates through multiple layers, ultimately leading to an internal error in the application. The root cause appears to be related to the handling of the `topology` object, which may not be properly initialized.",
            "StackTrace": [
                "org.apache.storm.thrift.TApplicationException: Internal error processing getComponentPageInfo",
                "at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:79)",
                "at org.apache.storm.generated.Nimbus$Client.recv_getComponentPageInfo(Nimbus.java:1359)",
                "at org.apache.storm.generated.Nimbus$Client.getComponentPageInfo(Nimbus.java:1343)",
                "at org.apache.storm.daemon.ui.UIHelpers.getComponentPage(UIHelpers.java:1559)",
                "2018-09-05 16:15:24.927 o.a.s.t.ProcessFunction pool-21-thread-55 [ERROR] Internal error processing getComponentPageInfo",
                "java.lang.RuntimeException: java.lang.NullPointerException",
                "at org.apache.storm.daemon.nimbus.Nimbus.getComponentPageInfo(Nimbus.java:4238) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:4577) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:4556) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:169) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) [shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) [shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]",
                "at java.lang.Thread.run(Thread.java:748)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.storm.scheduler.resource.ResourceUtils.getBoltResources(ResourceUtils.java:37) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.nimbus.Nimbus.getComponentPageInfo(Nimbus.java:4192) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]"
            ],
            "RootCause": "The `NullPointerException` in the `getComponentPageInfo` method of the `Nimbus` class is likely due to an uninitialized or null `topology` object, which is accessed in the `getBoltResources` method. This occurs when the `topology` object is not properly initialized before the method call.",
            "StepsToReproduce": [
                "Access the __acker component page in the Storm UI.",
                "Observe the server response."
            ],
            "ExpectedBehavior": "The component page should load successfully, displaying the relevant information about the __acker component without any errors.",
            "ObservedBehavior": "A 500 Server Error is returned, indicating an internal error while processing the request.",
            "Suggestions": "Ensure that the `topology` object is properly initialized and passed to the `getComponentPageInfo` method. Review the code paths leading to this method to confirm that the `topology` is always valid.",
            "problem_location": {
                "files": [
                    "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                    "storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.nimbus.Nimbus",
                    "org.apache.storm.scheduler.resource.ResourceUtils"
                ],
                "methods": [
                    "Nimbus.getComponentPageInfo",
                    "ResourceUtils.getBoltResources"
                ]
            },
            "possible_fix": "Check the initialization of the `topology` object in the `Nimbus` class. Ensure that it is not null before calling `getComponentPageInfo`. If necessary, add null checks or initialize the `topology` appropriately."
        },
        "possible_fix_code": {
            "org.apache.storm.daemon.nimbus.Nimbus.getComponentPageInfo": "    public void getComponentPageInfo(java.lang.String topology_id, java.lang.String component_id, java.lang.String window, boolean is_include_sys, org.apache.storm.thrift.async.AsyncMethodCallback<ComponentPageInfo> resultHandler) throws org.apache.storm.thrift.TException {\n        checkReady();\n        // Ensure topology is initialized before proceeding\n        StormTopology topology = getTopologyById(topology_id); // Hypothetical method to retrieve topology\n        if (topology == null) {\n            throw new IllegalArgumentException(\"Topology not found for ID: \" + topology_id);\n        }\n        getComponentPageInfo_call method_call = new getComponentPageInfo_call(topology_id, component_id, window, is_include_sys, resultHandler, this, ___protocolFactory, ___transport);\n        this.___currentMethod = method_call;\n        ___manager.call(method_call);\n    }"
        }
    },
    {
        "filename": "STORM-2496.json",
        "creation_time": "2017-04-28T08:17:47.000+0000",
        "bug_report": {
            "Title": "Dependency artifacts should be uploaded to blobstore with READ permission for all",
            "Description": "When submitting a topology with dependency artifacts, the artifacts are uploaded to the blobstore under the user account that performs the submission. Since these artifacts are shared globally, other users may need access to them. In non-secured clusters, this is acceptable; however, in secured environments, the Supervisor fails to retrieve the artifact due to insufficient permissions, leading to a crash. The error logs indicate an `AuthorizationException`, specifically stating that the user lacks READ access to the required JAR file.",
            "StackTrace": [
                "2017-04-28 04:56:46.594 o.a.s.l.AsyncLocalizer Async Localizer [WARN] Caught Exception While Downloading (rethrowing)...",
                "org.apache.storm.generated.AuthorizationException: null",
                "at org.apache.storm.localizer.Localizer.downloadBlob(Localizer.java:535) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.localizer.Localizer.access$000(Localizer.java:65) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:505) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:481) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]",
                "2017-04-28 04:56:46.597 o.a.s.d.s.Slot SLOT_6701 [ERROR] Error when processing event",
                "java.util.concurrent.ExecutionException: AuthorizationException(msg:<user> does not have READ access to dep-org.apache.curator-curator-framework-jar-2.10.0.jar)",
                "at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_112]",
                "at java.util.concurrent.FutureTask.get(FutureTask.java:206) ~[?:1.8.0_112]",
                "at org.apache.storm.localizer.LocalDownloadedResource$NoCancelFuture.get(LocalDownloadedResource.java:63) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:380) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:275) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:740) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "Caused by: org.apache.storm.generated.AuthorizationException",
                "at org.apache.storm.localizer.Localizer.downloadBlob(Localizer.java:535) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.localizer.Localizer.access$000(Localizer.java:65) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:505) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:481) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]",
                "2017-04-28 04:56:46.597 o.a.s.u.Utils SLOT_6701 [ERROR] Halting process: Error when processing an event",
                "java.lang.RuntimeException: Halting process: Error when processing an event",
                "at org.apache.storm.utils.Utils.exitProcess(Utils.java:1774) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:774) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "2017-04-28 04:56:46.599 o.a.s.d.s.Supervisor Thread-7 [INFO] Shutting down supervisor 775c158b-0a2d-40be-9e02-a9662d8bc5c4"
            ],
            "RootCause": "The root cause of the issue is that the user attempting to access the uploaded dependency artifact does not have the necessary READ permissions, resulting in an `AuthorizationException` when the Supervisor tries to download the blob.",
            "StepsToReproduce": [
                "1. Submit a topology with dependency artifacts using a specific user account.",
                "2. Attempt to access the uploaded artifact from a different user account that lacks READ permissions.",
                "3. Observe the Supervisor crashing due to the authorization failure."
            ],
            "ExpectedBehavior": "The Supervisor should be able to access and download the dependency artifacts without encountering permission issues, allowing the topology to function correctly.",
            "ObservedBehavior": "The Supervisor fails to download the required artifact due to an `AuthorizationException`, leading to a crash.",
            "Suggestions": "To resolve this issue, ensure that all uploaded artifacts have READ permissions for all users or at least for the Supervisor. Review the authorization settings for blob storage in Apache Storm.",
            "problem_location": {
                "files": [
                    "Localizer.java",
                    "Slot.java",
                    "Utils.java"
                ],
                "classes": [
                    "org.apache.storm.localizer.Localizer",
                    "org.apache.storm.daemon.supervisor.Slot",
                    "org.apache.storm.utils.Utils"
                ],
                "methods": [
                    "Localizer.downloadBlob",
                    "Slot.handleWaitingForBlobLocalization",
                    "Utils.exitProcess"
                ]
            },
            "possible_fix": "Modify the blob upload process to set READ permissions for all users. This may involve updating the `manageBlobPermissions` method in the Localizer class to ensure proper access control."
        },
        "possible_fix_code": {
            "Localizer.downloadBlob": "  private LocalizedResource downloadBlob(Map conf, String key, File localFile,\n      String user, boolean uncompress, boolean isUpdate)\n      throws AuthorizationException, KeyNotFoundException, IOException {\n    ClientBlobStore blobstore = null;\n    try {\n      blobstore = getClientBlobStore();\n      long nimbusBlobVersion = ServerUtils.nimbusVersionOfBlob(key, blobstore);\n      long oldVersion = ServerUtils.localVersionOfBlob(localFile.toString());\n      FileOutputStream out = null;\n      PrintWriter writer = null;\n      int numTries = 0;\n      String localizedPath = localFile.toString();\n      String localFileWithVersion = ServerUtils.constructBlobWithVersionFileName(localFile.toString(),\n              nimbusBlobVersion);\n      String localVersionFile = ServerUtils.constructVersionFileName(localFile.toString());\n      String downloadFile = localFileWithVersion;\n      if (uncompress) {\n        // we need to download to temp file and then unpack into the one requested\n        downloadFile = new File(localFile.getParent(), TO_UNCOMPRESS + localFile.getName()).toString();\n      }\n      while (numTries < _blobDownloadRetries) {\n        out = new FileOutputStream(downloadFile);\n        numTries++;\n        try {\n          // Ensure that the blob has READ permissions for all users\n          setBlobPermissions(conf, \"*\", localFileWithVersion);\n          setBlobPermissions(conf, \"*\", localVersionFile);\n          if (!ServerUtils.canUserReadBlob(blobstore.getBlobMeta(key), user)) {\n            throw new AuthorizationException(user + \" does not have READ access to \" + key);\n          }\n          InputStreamWithMeta in = blobstore.getBlob(key);\n          byte[] buffer = new byte[1024];\n          int len;\n          while ((len = in.read(buffer)) >= 0) {\n            out.write(buffer, 0, len);\n          }\n          out.close();\n          in.close();\n          if (uncompress) {\n            ServerUtils.unpack(new File(downloadFile), new File(localFileWithVersion));\n            LOG.debug(\"uncompressed \" + downloadFile + \" to: \" + localFileWithVersion);\n          }\n\n          // Next write the version.\n          LOG.info(\"Blob: \" + key + \" updated with new Nimbus-provided version: \" +\n              nimbusBlobVersion + \" local version was: \" + oldVersion);\n          // The false parameter ensures overwriting the version file, not appending\n          writer = new PrintWriter(\n              new BufferedWriter(new FileWriter(localVersionFile, false)));\n          writer.println(nimbusBlobVersion);\n          writer.close();\n\n          // Update permissions for the new blob version\n          setBlobPermissions(conf, \"*\", localFileWithVersion);\n          setBlobPermissions(conf, \"*\", localVersionFile);\n\n          // Update the key.current symlink. First create tmp symlink and do\n          // move of tmp to current so that the operation is atomic.\n          String tmp_uuid_local = java.util.UUID.randomUUID().toString();\n          LOG.debug(\"Creating a symlink @\" + localFile + \".\" + tmp_uuid_local + \" , \" +\n              \"linking to: \" + localFile + \".\" + nimbusBlobVersion);\n          File uuid_symlink = new File(localFile + \".\" + tmp_uuid_local);\n\n          Files.createSymbolicLink(uuid_symlink.toPath(),\n              Paths.get(ServerUtils.constructBlobWithVersionFileName(localFile.toString(),\n                      nimbusBlobVersion)));\n          File current_symlink = new File(ServerUtils.constructBlobCurrentSymlinkName(\n                  localFile.toString()));\n          Files.move(uuid_symlink.toPath(), current_symlink.toPath(), ATOMIC_MOVE);\n        } catch (AuthorizationException ae) {\n          // we consider this non-retriable exceptions\n          if (out != null) {\n            out.close();\n          }\n          new File(downloadFile).delete();\n          throw ae;\n        } catch (IOException | KeyNotFoundException e) {\n          if (out != null) {\n            out.close();\n          }\n          if (writer != null) {\n            writer.close();\n          }\n          new File(downloadFile).delete();\n          if (uncompress) {\n            try {\n              FileUtils.deleteDirectory(new File(localFileWithVersion));\n            } catch (IOException ignore) {}\n          }\n          if (!isUpdate) {\n            // don\\'t want to remove existing version file if its an update\n            new File(localVersionFile).delete();\n          }\n\n          if (numTries < _blobDownloadRetries) {\n            LOG.error(\"Failed to download blob, retrying\", e);\n          } else {\n            throw e;\n          }\n        }\n      }\n      return new LocalizedResource(key, localizedPath, uncompress);\n    } finally {\n      if(blobstore != null) {\n        blobstore.shutdown();\n      }\n    }\n  }"
        }
    },
    {
        "filename": "STORM-2879.json",
        "creation_time": "2018-01-03T07:07:49.000+0000",
        "bug_report": {
            "Title": "Supervisor collapse continuously when there is an expired assignment for overdue storm",
            "Description": "The issue arises when a topology is reassigned or killed in a cluster, leading the supervisor to delete four critical files associated with an overdue storm: storm-code, storm-ser, storm-jar, and LocalAssignment. The cleanup process does not utilize transactions, which means that if an exception occurs during the deletion of these files, a local assignment may remain on disk. Upon supervisor restart, the slots are initialized, and the container attempts to recover from LocalAssignments. However, it encounters a KeyNotFoundException when trying to fetch the files from the Nimbus/Master, causing the supervisor to collapse repeatedly until all local assignments are manually cleaned up.",
            "StackTrace": [
                "2017-12-27 14:15:04.434 o.a.s.l.AsyncLocalizer [INFO] Cleaning up unused topologies in /opt/meituan/storm/data/supervisor/stormdist",
                "2017-12-27 14:15:04.434 o.a.s.d.s.AdvancedFSOps [INFO] Deleting path /opt/meituan/storm/data/supervisor/stormdist/app_dpsr_realtime_shop_vane_allcates-14-1513685785",
                "2017-12-27 14:15:04.502 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormjar.jar from blob store.",
                "org.apache.storm.generated.KeyNotFoundException: null",
                "2017-12-27 14:15:05.140 o.a.s.u.Utils [INFO] Could not extract resources from /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormjar.jar"
            ],
            "RootCause": "The root cause of the issue is the failure to delete necessary files during the cleanup process, which leads to a KeyNotFoundException when the supervisor attempts to recover from LocalAssignments. The missing configuration file (stormconf.ser) is critical for the blob download process, and its absence results in the supervisor's inability to locate required blobs.",
            "StepsToReproduce": [
                "1. Assign or kill a topology in the cluster.",
                "2. Ensure that the cleanup process is triggered for overdue storms.",
                "3. Observe the logs for any exceptions during the deletion of storm-code, storm-ser, storm-jar, and LocalAssignment.",
                "4. Restart the supervisor and check for KeyNotFoundException in the logs."
            ],
            "ExpectedBehavior": "The supervisor should successfully delete all associated files and recover from LocalAssignments without encountering any exceptions, allowing it to function normally.",
            "ObservedBehavior": "The supervisor continuously collapses due to repeated KeyNotFoundExceptions when attempting to recover from LocalAssignments, which is caused by the absence of the stormconf.ser file.",
            "Suggestions": "Ensure that the cleanup process is transactional to prevent leftover local assignments. Additionally, verify that the stormconf.ser file is present and accessible at the expected path to avoid KeyNotFoundExceptions during recovery.",
            "problem_location": {
                "files": [
                    "Slot.java",
                    "ConfigUtils.java"
                ],
                "classes": [
                    "org.apache.storm.drpc.Slot",
                    "org.apache.storm.utils.ConfigUtils"
                ],
                "methods": [
                    "Slot.cleanupCurrentContainer",
                    "ConfigUtils.readSupervisorStormConfGivenPath"
                ]
            },
            "possible_fix": "Implement a transactional cleanup process in the Slot.cleanupCurrentContainer method to ensure that all necessary files are deleted successfully. Additionally, check the path for the stormconf.ser file and ensure it is created and accessible during the topology assignment process."
        },
        "possible_fix_code": {
            "Slot.cleanupCurrentContainer": "public void cleanupCurrentContainer() {\n    // Begin a transaction for cleanup\n    Transaction transaction = beginTransaction();\n    try {\n        // Attempt to delete necessary files\n        deleteFile(stormCodePath);\n        deleteFile(stormSerPath);\n        deleteFile(stormJarPath);\n        deleteFile(localAssignmentPath);\n\n        // Commit the transaction if all deletions are successful\n        transaction.commit();\n    } catch (Exception e) {\n        // Rollback the transaction in case of any failure\n        transaction.rollback();\n        // Log the error for debugging\n        log.error(\"Cleanup failed: \" + e.getMessage(), e);\n    } finally {\n        // Ensure the transaction is closed\n        transaction.close();\n    }\n\n    // Check for the presence of the stormconf.ser file\n    if (!new File(stormConfPath).exists()) {\n        log.error(\"Configuration file stormconf.ser is missing at: \" + stormConfPath);\n        throw new FileNotFoundException(\"Configuration file not found\");\n    }\n}"
        }
    },
    {
        "filename": "STORM-3012.json",
        "creation_time": "2018-03-27T15:30:32.000+0000",
        "bug_report": {
            "Title": "Nimbus will crash if pacemaker is restarted",
            "Description": "The Nimbus service crashes with a NullPointerException (NPE) when the Pacemaker is restarted. This issue arises due to a failure in establishing a connection with the Pacemaker service, leading to a timeout and subsequent NPE when Nimbus attempts to access heartbeat data that was not retrieved. The stack trace indicates that the crash occurs during the cleanup process in Nimbus, specifically when it tries to handle heartbeat messages from the Pacemaker service.",
            "StackTrace": [
                "org.apache.storm.pacemaker.PacemakerClient.waitUntilReady(PacemakerClient.java:213)",
                "org.apache.storm.pacemaker.PacemakerClient.send(PacemakerClient.java:182)",
                "org.apache.storm.pacemaker.PacemakerClientPool.sendAll(PacemakerClientPool.java:65)",
                "org.apache.storm.cluster.PaceMakerStateStorage.get_worker_hb_children(PaceMakerStateStorage.java:193)",
                "org.apache.storm.cluster.StormClusterStateImpl.heartbeatStorms(StormClusterStateImpl.java:408)",
                "org.apache.storm.daemon.nimbus.Nimbus.topoIdsToClean(Nimbus.java:765)",
                "org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2148)"
            ],
            "RootCause": "The root cause of the crash is a NullPointerException triggered by the failure to retrieve heartbeat data from the Pacemaker service. This failure occurs because the connection to the Pacemaker is not established, leading to a timeout and a null response being processed in Nimbus.",
            "StepsToReproduce": [
                "Restart the Pacemaker service.",
                "Monitor the Nimbus logs for connection attempts to the Pacemaker.",
                "Observe the crash occurring due to a NullPointerException in the Nimbus service."
            ],
            "ExpectedBehavior": "Nimbus should handle the Pacemaker restart gracefully without crashing, maintaining its ability to process heartbeat messages and perform cleanup operations.",
            "ObservedBehavior": "Nimbus crashes with a NullPointerException when attempting to access heartbeat data after the Pacemaker service is restarted.",
            "Suggestions": "Investigate the connection handling logic in the PacemakerClient and ensure that Nimbus can handle cases where the Pacemaker is temporarily unavailable. Implement error handling to prevent NPEs when heartbeat data cannot be retrieved.",
            "problem_location": {
                "files": [
                    "PacemakerClient.java",
                    "PacemakerClientPool.java",
                    "PaceMakerStateStorage.java",
                    "StormClusterStateImpl.java",
                    "Nimbus.java"
                ],
                "classes": [
                    "org.apache.storm.pacemaker.PacemakerClient",
                    "org.apache.storm.pacemaker.PacemakerClientPool",
                    "org.apache.storm.cluster.PaceMakerStateStorage",
                    "org.apache.storm.cluster.StormClusterStateImpl",
                    "org.apache.storm.daemon.nimbus.Nimbus"
                ],
                "methods": [
                    "PacemakerClient.waitUntilReady",
                    "PacemakerClient.send",
                    "PacemakerClientPool.sendAll",
                    "PaceMakerStateStorage.get_worker_hb_children",
                    "StormClusterStateImpl.heartbeatStorms",
                    "Nimbus.topoIdsToClean",
                    "Nimbus.doCleanup"
                ]
            },
            "possible_fix": "Add null checks and error handling in the Nimbus cleanup logic to prevent NPEs when heartbeat data is unavailable. Consider implementing a retry mechanism or fallback logic when the Pacemaker service is temporarily unreachable."
        },
        "possible_fix_code": {
            "Nimbus.doCleanup": "    public void doCleanup() {\n        try {\n            List<String> topoIds = topoIdsToClean();\n            if (topoIds == null || topoIds.isEmpty()) {\n                LOG.warn(\"No topology IDs to clean.\");\n                return;\n            }\n            for (String topoId : topoIds) {\n                // Perform cleanup for each topology ID\n                // ... (cleanup logic here)\n            }\n        } catch (NullPointerException e) {\n            LOG.error(\"Failed to clean up topologies due to null pointer exception: {}\", e.getMessage());\n            // Handle the case where heartbeat data is unavailable\n            // Optionally implement a retry mechanism or fallback logic\n        } catch (Exception e) {\n            LOG.error(\"An unexpected error occurred during cleanup: {}\", e.getMessage());\n        }\n    }"
        }
    },
    {
        "filename": "STORM-3073.json",
        "creation_time": "2018-05-15T11:12:21.000+0000",
        "bug_report": {
            "Title": "In some cases workers may crash because pendingEmits is full",
            "Description": "The issue arises when the `pendingEmits` queue in the Apache Storm framework becomes full, leading to a crash of the worker threads. This was observed while running the `ThroughputVsLatency` topology. The stack trace indicates that a `java.lang.RuntimeException` is thrown due to a `java.lang.IllegalStateException` with the message 'Queue full'. The error occurs when the executor attempts to add a new tuple to the queue, which has reached its capacity. The topology reemits failed tuples directly from the `fail` method, which can be triggered by tick tuples, potentially exacerbating the issue if the queue is already near full capacity.",
            "StackTrace": [
                "2018-05-15 11:35:28.365 o.a.s.u.Utils Thread-16-spout-executor[8, 8] [ERROR] Async loop died!",
                "java.lang.RuntimeException: java.lang.IllegalStateException: Queue full",
                "at org.apache.storm.executor.Executor.accept(Executor.java:282) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:133) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.utils.JCQueue.consume(JCQueue.java:110) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:168) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:157) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.utils.Utils$2.run(Utils.java:349) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]",
                "Caused by: java.lang.IllegalStateException: Queue full",
                "at java.util.AbstractQueue.add(AbstractQueue.java:98) ~[?:1.8.0_144]",
                "at org.apache.storm.daemon.worker.WorkerTransfer.tryTransferRemote(WorkerTransfer.java:113) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.worker.WorkerState.tryTransferRemote(WorkerState.java:516) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.ExecutorTransfer.tryTransfer(ExecutorTransfer.java:66) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutOutputCollectorImpl.sendSpoutMsg(SpoutOutputCollectorImpl.java:140) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutOutputCollectorImpl.emit(SpoutOutputCollectorImpl.java:70) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.spout.SpoutOutputCollector.emit(SpoutOutputCollector.java:42) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.loadgen.LoadSpout.fail(LoadSpout.java:135) ~[stormjar.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutExecutor.failSpoutMsg(SpoutExecutor.java:360) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutExecutor$1.expire(SpoutExecutor.java:120) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutExecutor$1.expire(SpoutExecutor.java:113) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.utils.RotatingMap.rotate(RotatingMap.java:63) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutExecutor.tupleActionFn(SpoutExecutor.java:295) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.Executor.accept(Executor.java:278) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]"
            ],
            "RootCause": "The root cause of the issue is that the `pendingEmits` queue in the executor is full, preventing the addition of new tuples. This is likely due to a high rate of incoming events or slow processing of existing events, leading to a backlog.",
            "StepsToReproduce": [
                "Run the `ThroughputVsLatency` topology from the Apache Storm examples.",
                "Ensure that the topology is configured to emit a high volume of tuples.",
                "Monitor the `pendingEmits` queue and observe the conditions under which it becomes full."
            ],
            "ExpectedBehavior": "The system should process incoming tuples without crashing, even under high load, by managing the queue effectively and preventing it from reaching full capacity.",
            "ObservedBehavior": "The worker crashes with a 'Queue full' error when attempting to add a new tuple to the `pendingEmits` queue, indicating that the queue has reached its capacity.",
            "Suggestions": "Consider increasing the queue size if configurable, optimizing spout processing to handle events more efficiently, implementing backpressure to control the rate of incoming events, and enhancing error handling to manage situations when the queue is full.",
            "problem_location": {
                "files": [
                    "storm-client/src/jvm/org/apache/storm/executor/Executor.java",
                    "storm-client/src/jvm/org/apache/storm/utils/JCQueue.java",
                    "storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java"
                ],
                "classes": [
                    "org.apache.storm.executor.Executor",
                    "org.apache.storm.utils.JCQueue",
                    "org.apache.storm.executor.spout.SpoutExecutor"
                ],
                "methods": [
                    "Executor.accept",
                    "JCQueue.consume",
                    "JCQueue.consumeImpl",
                    "SpoutExecutor.failSpoutMsg"
                ]
            },
            "possible_fix": "To mitigate the issue, consider implementing the following changes: 1) Increase the capacity of the `pendingEmits` queue if possible. 2) Optimize the processing logic in the spout to ensure that tuples are processed and removed from the queue more quickly. 3) Implement a backpressure mechanism to slow down the rate of incoming tuples when the queue is nearing capacity."
        },
        "possible_fix_code": {
            "Executor.accept": "    public void accept(Object event) {\n        if (event == JCQueue.INTERRUPT) {\n            throw new RuntimeException(new InterruptedException(\"JCQ processing interrupted\"));\n        }\n        AddressedTuple addressedTuple = (AddressedTuple) event;\n        int taskId = addressedTuple.getDest();\n\n        TupleImpl tuple = (TupleImpl) addressedTuple.getTuple();\n        if (isDebug) {\n            LOG.info(\"Processing received message FOR {} TUPLE: {}\", taskId, tuple);\n        }\n\n        try {\n            if (taskId != AddressedTuple.BROADCAST_DEST) {\n                tupleActionFn(taskId, tuple);\n            } else {\n                for (Integer t : taskIds) {\n                    tupleActionFn(t, tuple);\n                }\n            }\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n        // Check if the pendingEmits queue is full and handle it\n        if (pendingEmits.isFull()) {\n            // Implement backpressure or error handling logic here\n            LOG.warn(\"Pending emits queue is full. Implementing backpressure.\");\n            // Optionally, you could pause or slow down the processing\n        }\n    }"
        }
    },
    {
        "filename": "STORM-1672.json",
        "creation_time": "2016-03-31T19:24:18.000+0000",
        "bug_report": {
            "Title": "ClassCastException in StatsUtil when processing component statistics",
            "Description": "The application encounters a ClassCastException when attempting to process component statistics in the StatsUtil class. The error occurs specifically in the filterSysStreams method, which expects a Map but receives a Long instead. This issue arises during the aggregation of statistics for components, particularly when the sid2emitted or sid2transferred maps are populated. The root cause is likely due to incorrect data types being stored in the beat map, particularly under the STATS key.",
            "StackTrace": [
                "2016-03-31 14:21:44.576 o.a.s.t.s.AbstractNonblockingServer$FrameBuffer [ERROR] Unexpected throwable while invoking!",
                "java.lang.ClassCastException: java.lang.Long cannot be cast to java.util.Map",
                "at org.apache.storm.stats.StatsUtil.filterSysStreams(StatsUtil.java:1696)",
                "at org.apache.storm.stats.StatsUtil.aggPreMergeCompPageBolt(StatsUtil.java:240)",
                "at org.apache.storm.stats.StatsUtil.aggCompExecStats(StatsUtil.java:1130)",
                "at org.apache.storm.stats.StatsUtil.aggregateCompStats(StatsUtil.java:1108)",
                "at org.apache.storm.stats.StatsUtil.aggCompExecsStats(StatsUtil.java:1236)",
                "at org.apache.storm.daemon.nimbus$fn__3490$exec_fn__789__auto__$reify__3519.getComponentPageInfo(nimbus.clj:2130)",
                "at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:3826)",
                "at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:3810)",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:158)",
                "at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)",
                "at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The ClassCastException occurs because a Long value is being passed where a Map is expected, likely due to incorrect data being populated in the beat map, particularly under the STATS key. This can happen if the data structure is not consistent or if the statistics are not being aggregated correctly.",
            "StepsToReproduce": [
                "Trigger the component statistics aggregation process in the application.",
                "Ensure that the data being fed into the beat map includes a Long value for a key that is expected to be a Map.",
                "Observe the logs for the ClassCastException."
            ],
            "ExpectedBehavior": "The system should successfully aggregate and process component statistics without throwing a ClassCastException.",
            "ObservedBehavior": "The system throws a ClassCastException when attempting to cast a Long to a Map during the processing of component statistics.",
            "Suggestions": "Review the data being populated in the beat map, particularly under the STATS key, to ensure that all values are of the expected Map type. Implement type checks or validations when retrieving values from the map to prevent ClassCastExceptions.",
            "problem_location": {
                "files": [
                    "storm-core/src/jvm/org/apache/storm/stats/StatsUtil.java"
                ],
                "classes": [
                    "org.apache.storm.stats.StatsUtil"
                ],
                "methods": [
                    "filterSysStreams",
                    "aggPreMergeCompPageBolt",
                    "getMapByKey",
                    "aggCompExecStats"
                ]
            },
            "possible_fix": "Ensure that the methods populating the beat map correctly handle data types. For example, in the aggPreMergeCompPageBolt method, add checks to confirm that the values retrieved from the stat2win2sid2num map are indeed Maps before passing them to filterSysStreams."
        },
        "possible_fix_code": {
            "aggPreMergeCompPageBolt": "    public static Map<String, Object> aggPreMergeCompPageBolt(Map<String, Object> beat, String window, boolean includeSys) {\n        Map<String, Object> ret = new HashMap<>();\n\n        putKV(ret, EXECUTOR_ID, getByKey(beat, \"exec-id\"));\n        putKV(ret, HOST, getByKey(beat, HOST));\n        putKV(ret, PORT, getByKey(beat, PORT));\n        putKV(ret, UPTIME, getByKey(beat, UPTIME));\n        putKV(ret, NUM_EXECUTORS, 1);\n        putKV(ret, NUM_TASKS, getByKey(beat, NUM_TASKS));\n\n        Map stat2win2sid2num = getMapByKey(beat, STATS);\n        putKV(ret, CAPACITY, computeAggCapacity(stat2win2sid2num, getByKeyOr0(beat, UPTIME).intValue()));\n\n        // calc cid+sid->input_stats\n        Map inputStats = new HashMap();\n        Map sid2acked = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, ACKED), TO_STRING).get(window);\n        Map sid2failed = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, FAILED), TO_STRING).get(window);\n        putKV(inputStats, ACKED, sid2acked != null ? sid2acked : new HashMap());\n        putKV(inputStats, FAILED, sid2failed != null ? sid2failed : new HashMap());\n\n        inputStats = swapMapOrder(inputStats);\n\n        Map sid2execLat = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, EXEC_LATENCIES), TO_STRING).get(window);\n        Map sid2procLat = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, PROC_LATENCIES), TO_STRING).get(window);\n        Map sid2exec = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, EXECUTED), TO_STRING).get(window);\n        mergeMaps(inputStats, aggBoltStreamsLatAndCount(sid2execLat, sid2procLat, sid2exec));\n        putKV(ret, CID_SID_TO_IN_STATS, inputStats);\n\n        // calc sid->output_stats\n        Map outputStats = new HashMap();\n        Map sid2emitted = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, EMITTED), TO_STRING).get(window);\n        Map sid2transferred = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, TRANSFERRED), TO_STRING).get(window);\n        if (sid2emitted != null) {\n            putKV(outputStats, EMITTED, filterSysStreams(sid2emitted, includeSys));\n        } else {\n            putKV(outputStats, EMITTED, new HashMap());\n        }\n        if (sid2transferred != null) {\n            putKV(outputStats, TRANSFERRED, filterSysStreams(sid2transferred, includeSys));\n        } else {\n            putKV(outputStats, TRANSFERRED, new HashMap());\n        }\n        outputStats = swapMapOrder(outputStats);\n        putKV(ret, SID_TO_OUT_STATS, outputStats);\n\n        // Validate types before passing to filterSysStreams\n        if (sid2emitted instanceof Map && sid2transferred instanceof Map) {\n            putKV(ret, SID_TO_OUT_STATS, outputStats);\n        } else {\n            throw new IllegalArgumentException(\"Expected Map for sid2emitted or sid2transferred\");\n        }\n\n        return ret;\n    }"
        }
    },
    {
        "filename": "STORM-1520.json",
        "creation_time": "2016-02-03T02:48:58.000+0000",
        "bug_report": {
            "Title": "Nimbus Clojure/Zookeeper issue ('stateChanged' method not found)",
            "Description": "The issue arises when deploying or undeploying topologies in Nimbus, leading to an error logged in nimbus.log. The error indicates that the method 'stateChanged' cannot be found in the dynamically generated class 'org.apache.storm.cluster$mk_storm_cluster_state$reify$reify__6413'. This suggests a potential misconfiguration or an assumption about the method's existence that is not met in the current codebase. The basic functionality of Nimbus appears unaffected, but it becomes unresponsive and requires a manual restart.",
            "StackTrace": [
                "2016-02-02 21:34:04.308 o.a.s.s.o.a.c.f.l.ListenerContainer [ERROR] Listener (org.apache.storm.cluster_state.zookeeper_state_factory$_mkState$reify$reify__12660@22587507) threw an exception",
                "java.lang.IllegalArgumentException: No matching method found: stateChanged for class org.apache.storm.cluster$mk_storm_cluster_state$reify$reify__6413",
                "at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:53)",
                "at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)",
                "at org.apache.storm.cluster_state.zookeeper_state_factory$_mkState$reify$reify__12660.stateChanged(zookeeper_state_factory.clj:145)",
                "at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:259)",
                "at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:255)",
                "at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)",
                "at org.apache.storm.shade.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)",
                "at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:84)",
                "at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:253)",
                "at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:43)",
                "at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:111)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is the absence of the 'stateChanged' method in the dynamically generated class 'org.apache.storm.cluster$mk_storm_cluster_state$reify$reify__6413'. This indicates a potential misconfiguration or an incorrect assumption about the method's existence in the class hierarchy.",
            "StepsToReproduce": [
                "Deploy a topology using Nimbus.",
                "Undeploy the same topology.",
                "Check the nimbus.log for errors related to 'stateChanged'."
            ],
            "ExpectedBehavior": "The Nimbus service should handle topology deployments and undeployments without throwing exceptions, and it should remain responsive throughout the process.",
            "ObservedBehavior": "Upon deploying or undeploying topologies, Nimbus throws an IllegalArgumentException due to the missing 'stateChanged' method, leading to unresponsiveness and requiring a manual restart.",
            "Suggestions": "Investigate the configuration of the zookeeper_state_factory and ensure that all necessary methods are defined and accessible. Consider checking for updates or patches in the Apache Storm project that may address this issue.",
            "problem_location": {
                "files": [
                    "zookeeper_state_factory.clj"
                ],
                "classes": [
                    "org.apache.storm.cluster_state.zookeeper_state_factory",
                    "org.apache.storm.cluster$mk_storm_cluster_state$reify$reify__6413"
                ],
                "methods": [
                    "stateChanged"
                ]
            },
            "possible_fix": "Ensure that the 'stateChanged' method is properly defined in the expected context or class. If it is part of an interface or protocol, verify that the class implements it correctly."
        },
        "possible_fix_code": {
            "stateChanged": "public void stateChanged(ConnectionState newState) {\n    // Implement the logic to handle state changes here.\n    // This could include actions based on the new state, such as updating internal state,\n    // notifying other components, or logging the state change.\n    switch (newState) {\n        case CONNECTED:\n            // Handle connected state\n            break;\n        case DISCONNECTED:\n            // Handle disconnected state\n            break;\n        case RECONNECTED:\n            // Handle reconnected state\n            break;\n        default:\n            // Handle unknown state\n            break;\n    }\n}"
        }
    },
    {
        "filename": "STORM-1977.json",
        "creation_time": "2016-07-17T09:07:06.000+0000",
        "bug_report": {
            "Title": "Leader Nimbus crashes with getClusterInfo when it doesn't have one or more replicated topology codes",
            "Description": "The issue arises when a Nimbus instance gains leadership without having all the necessary replicated topology codes. This situation occurs due to a change in logic with the introduction of BlobStore, which allows Nimbus to assume leadership even if it lacks certain topology codes. When a request for getClusterInfo is made, the Nimbus instance crashes due to a KeyNotFoundException, indicating that it is trying to access a blob that does not exist in the local file system blob store.",
            "StackTrace": [
                "KeyNotFoundException(msg:production-topology-2-1468745167-stormcode.ser)",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:149)",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication(LocalFsBlobStore.java:268)",
                "at org.apache.storm.daemon.nimbus$get_blob_replication_count.invoke(nimbus.clj:498)",
                "at org.apache.storm.daemon.nimbus$get_cluster_info.invoke(nimbus.clj:1401)",
                "at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9612.getClusterInfo(nimbus.clj:1838)",
                "at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3724)",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)"
            ],
            "RootCause": "The root cause of the crash is the KeyNotFoundException thrown by the LocalFsBlobStore when attempting to retrieve metadata for a blob that does not exist. This occurs because Nimbus 2, which has assumed leadership, does not have access to the required topology blobs due to the absence of replicated topology codes.",
            "StepsToReproduce": [
                "Comment out cleanup-corrupt-topologies! from nimbus.clj.",
                "Patch the Storm cluster.",
                "Launch Nimbus 1 (leader).",
                "Run a topology.",
                "Kill Nimbus 1.",
                "Launch Nimbus 2 from a different node.",
                "Request getClusterInfo from Nimbus 2."
            ],
            "ExpectedBehavior": "Nimbus 2 should successfully return the cluster information without crashing, even if it does not have all replicated topology codes.",
            "ObservedBehavior": "Nimbus 2 crashes with a KeyNotFoundException when getClusterInfo is requested, indicating that it cannot find the required blob.",
            "Suggestions": "Investigate the blob management process to ensure that all necessary blobs are uploaded and accessible. Consider implementing checks to prevent Nimbus from gaining leadership without all required topology codes.",
            "problem_location": {
                "files": [
                    "nimbus.clj",
                    "LocalFsBlobStore.java"
                ],
                "classes": [
                    "org.apache.storm.blobstore.LocalFsBlobStore",
                    "org.apache.storm.daemon.nimbus"
                ],
                "methods": [
                    "LocalFsBlobStore.getStoredBlobMeta",
                    "LocalFsBlobStore.getBlob",
                    "LocalFsBlobStore.checkForBlobOrDownload"
                ]
            },
            "possible_fix": "Ensure that Nimbus instances do not gain leadership unless they have all required topology blobs. This may involve adding validation logic in the leadership election process or ensuring that blobs are correctly uploaded and available in the blob store."
        },
        "possible_fix_code": {
            "LocalFsBlobStore.getStoredBlobMeta": "    private SettableBlobMeta getStoredBlobMeta(String key) throws KeyNotFoundException {\n        InputStream in = null;\n        try {\n            LocalFsBlobStoreFile pf = fbs.read(META_PREFIX + key);\n            try {\n                in = pf.getInputStream();\n            } catch (FileNotFoundException fnf) {\n                throw new KeyNotFoundException(key);\n            }\n            ByteArrayOutputStream out = new ByteArrayOutputStream();\n            byte[] buffer = new byte[2048];\n            int len;\n            while ((len = in.read(buffer)) > 0) {\n                out.write(buffer, 0, len);\n            }\n            in.close();\n            in = null;\n            return Utils.thriftDeserialize(SettableBlobMeta.class, out.toByteArray());\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        } finally {\n            if (in != null) {\n                try {\n                    in.close();\n                } catch (IOException e) {\n                    // Ignored\n                }\n            }\n        }\n    }"
        }
    },
    {
        "filename": "STORM-2988.json",
        "creation_time": "2018-03-07T14:55:22.000+0000",
        "bug_report": {
            "Title": "\"Error on initialization of server mk-worker\" when using org.apache.storm.metrics2.reporters.JmxStormReporter on worker",
            "Description": "The issue arises when attempting to initialize a worker in Apache Storm with the JmxStormReporter configured in storm.yaml. The configuration appears to be correct, as metrics are reported successfully for nimbus and supervisors. However, upon submitting a topology, the worker fails to initialize, throwing an IllegalArgumentException related to the conversion of a configuration map to a String. This suggests a potential misconfiguration or an unsupported data format being passed to the JmxStormReporter.",
            "StackTrace": [
                "2018-03-07 15:39:19.182 o.a.s.d.worker main [ERROR] Error on initialization of server mk-worker",
                "java.lang.IllegalArgumentException: Don't know how to convert {\"class\" \"org.apache.storm.metrics2.reporters.JmxStormReporter\", \"daemons\" [\"supervisor\" \"nimbus\" \"worker\"], \"report.period\" 10, \"report.period.units\" \"SECONDS\"} + to String",
                "at org.apache.storm.utils.Utils.getString(Utils.java:848) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.metrics2.reporters.JmxStormReporter.getMetricsJMXDomain(JmxStormReporter.java:70) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.metrics2.reporters.JmxStormReporter.prepare(JmxStormReporter.java:51) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.metrics2.StormMetricRegistry.startReporter(StormMetricRegistry.java:119) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.metrics2.StormMetricRegistry.start(StormMetricRegistry.java:102) ~[storm-core-1.2.1.jar:1.2.1]"
            ],
            "RootCause": "The root cause of the issue is an IllegalArgumentException thrown by the JmxStormReporter when it attempts to convert a configuration map to a String. This indicates that the configuration format may be incorrect or incompatible with the expected input for the Utils.getString method.",
            "StepsToReproduce": [
                "Configure storm.yaml with the JmxStormReporter settings as specified.",
                "Start the nimbus and supervisor processes.",
                "Submit a topology to the Storm cluster.",
                "Observe the worker initialization logs for errors."
            ],
            "ExpectedBehavior": "The worker should initialize successfully and report metrics to JMX without any errors.",
            "ObservedBehavior": "The worker fails to initialize, resulting in an IllegalArgumentException related to the configuration map passed to the JmxStormReporter.",
            "Suggestions": "Check the configuration settings for the JmxStormReporter in storm.yaml. Ensure that the configuration is correctly formatted and that all required fields are present. Verify that the data structure being passed to the getString method is compatible with what the method expects.",
            "problem_location": {
                "files": [
                    "storm.yaml",
                    "JmxStormReporter.java",
                    "Utils.java"
                ],
                "classes": [
                    "org.apache.storm.metrics2.reporters.JmxStormReporter",
                    "org.apache.storm.utils.Utils"
                ],
                "methods": [
                    "JmxStormReporter.getMetricsJMXDomain",
                    "Utils.getString"
                ]
            },
            "possible_fix": "Modify the configuration in storm.yaml to ensure that the values are correctly formatted and compatible with the expected input types for the JmxStormReporter. For example, ensure that the 'report.period' is specified as a string if required."
        },
        "possible_fix_code": {
            "JmxStormReporter.getMetricsJMXDomain": "public String getMetricsJMXDomain() {\n    // Assuming the configuration is stored in a Map<String, Object>\n    Map<String, Object> config = getConfig();\n    // Convert the configuration map to a String in a compatible format\n    StringBuilder sb = new StringBuilder();\n    for (Map.Entry<String, Object> entry : config.entrySet()) {\n        sb.append(entry.getKey()).append(\"=\").append(entry.getValue().toString()).append(\", \");\n    }\n    // Remove the last comma and space\n    if (sb.length() > 0) {\n        sb.setLength(sb.length() - 2);\n    }\n    return sb.toString();\n}"
        }
    },
    {
        "filename": "STORM-2321.json",
        "creation_time": "2017-01-24T04:18:07.000+0000",
        "bug_report": {
            "Title": "Nimbus did not come up after restart",
            "Description": "During high availability (HA) testing, the Nimbus service was restarted. Post-restart, the Nimbus failed to initialize properly, leading to a series of errors primarily related to Zookeeper interactions. The logs indicate a `NoNodeException` for the blob key `/blobstore/KillLeaderThenSubmitNewTopology1-1-1484715309-stormjar.jar`, suggesting that the expected node does not exist in Zookeeper. This absence triggers a cascade of exceptions, including `NoSuchElementException` and `TTransportException`, as the system attempts to access or manipulate non-existent data.",
            "StackTrace": [
                "2017-01-18 04:57:58.247 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyKillLeaderThenSubmitNewTopology1-1-1484715309-stormjar.jar",
                "org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /blobstore/KillLeaderThenSubmitNewTopology1-1-1484715309-stormjar.jar",
                "java.util.NoSuchElementException",
                "java.lang.RuntimeException: org.apache.storm.thrift.transport.TTransportException",
                "org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:92)",
                "org.apache.storm.blobstore.NimbusBlobStore.createStateInZookeeper(NimbusBlobStore.java:349)"
            ],
            "RootCause": "The root cause of the issue is the `NoNodeException` indicating that the specified blob node does not exist in Zookeeper. This is likely due to a failure in the blob upload process or a misconfiguration in the Nimbus client, preventing the expected state from being created in Zookeeper.",
            "StepsToReproduce": [
                "1. Restart the Nimbus service during HA testing.",
                "2. Monitor the logs for errors related to blob storage and Zookeeper interactions.",
                "3. Observe the failure to initialize due to missing blob nodes."
            ],
            "ExpectedBehavior": "The Nimbus service should successfully restart and initialize, creating or accessing the necessary blob nodes in Zookeeper without errors.",
            "ObservedBehavior": "After the restart, the Nimbus service fails to come up, logging multiple errors related to missing blob nodes and Zookeeper exceptions.",
            "Suggestions": "Ensure that the blob associated with the topology is correctly uploaded to Zookeeper before restarting the Nimbus service. Verify the configuration of the Nimbus client and the blob store to ensure proper initialization.",
            "problem_location": {
                "files": [
                    "Nimbus.java",
                    "BlobStoreUtils.java",
                    "BlobSynchronizer.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.nimbus.Nimbus",
                    "org.apache.storm.blobstore.BlobStoreUtils",
                    "org.apache.storm.blobstore.BlobSynchronizer"
                ],
                "methods": [
                    "Nimbus.submitTopology",
                    "BlobStoreUtils.createStateInZookeeper",
                    "BlobSynchronizer.syncBlobs"
                ]
            },
            "possible_fix": "Investigate the blob upload process to ensure that blobs are correctly uploaded to Zookeeper. Consider adding checks to verify the existence of required nodes before attempting to access them. Review the `createStateInZookeeper` method in `NimbusBlobStore` and ensure it handles cases where the node does not exist."
        },
        "possible_fix_code": {
            "Nimbus.createStateInZookeeper": "    public void createStateInZookeeper(String key) {\n        try {\n            synchronized(client) {\n                if (client.getClient().exists(key) == null) {\n                    throw new RuntimeException(\"Node does not exist in Zookeeper: \" + key);\n                }\n                client.getClient().createStateInZookeeper(key);\n            }\n        } catch (TException e) {\n            throw new RuntimeException(e);\n        }\n    }"
        }
    },
    {
        "filename": "STORM-3013.json",
        "creation_time": "2018-03-28T04:47:28.000+0000",
        "bug_report": {
            "Title": "Deactivated topology restarts if data flows into Kafka",
            "Description": "The issue arises when a Storm topology is deactivated, and subsequent data is produced into Kafka. This action triggers an exception in the Storm application, indicating that the Kafka consumer has been closed prematurely. The stack trace reveals a `RuntimeException` caused by an `IllegalStateException`, specifically stating that 'This consumer has already been closed.' This suggests a problem with the lifecycle management of the Kafka consumer within the Storm framework.",
            "StackTrace": [
                "2018-03-28 09:50:23.804 o.a.s.d.executor Thread-83-kafkaLogs-executor[130 130] [INFO] Deactivating spout kafkaLogs:(130)",
                "2018-03-28 09:51:01.289 o.a.s.util Thread-17-kafkaLogs-executor[139 139] [ERROR] Async loop died!",
                "java.lang.RuntimeException: java.lang.IllegalStateException: This consumer has already been closed.",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:522) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:487) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatch(DisruptorQueue.java:477) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.disruptor$consume_batch.invoke(disruptor.clj:70) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.daemon.executor$fn__4975$fn__4990$fn__5021.invoke(executor.clj:634) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.2.1.jar:1.2.1]",
                "at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]",
                "Caused by: java.lang.IllegalStateException: This consumer has already been closed.",
                "at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:1787) ~[stormjar.jar:?]",
                "at org.apache.kafka.clients.consumer.KafkaConsumer.beginningOffsets(KafkaConsumer.java:1622) ~[stormjar.jar:?]",
                "at org.apache.storm.kafka.spout.metrics.KafkaOffsetMetric.getValueAndReset(KafkaOffsetMetric.java:79) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.daemon.executor$metrics_tick$fn__4899.invoke(executor.clj:345) ~[storm-core-1.2.1.jar:1.2.1]",
                "at clojure.core$map$fn__4553.invoke(core.clj:2622) ~[clojure-1.7.0.jar:?]",
                "at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?]",
                "at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?]",
                "at clojure.lang.RT.seq(RT.java:507) ~[clojure-1.7.0.jar:?]",
                "at clojure.core$seq__4128.invoke(core.clj:137) ~[clojure-1.7.0.jar:?]",
                "at clojure.core$next__4112.invoke(core.clj:64) ~[clojure-1.7.0.jar:?]",
                "at clojure.core.protocols$fn__6523.invoke(protocols.clj:170) ~[clojure-1.7.0.jar:?]",
                "at clojure.core.protocols$fn__6478$G__6473__6487.invoke(protocols.clj:19) ~[clojure-1.7.0.jar:?]",
                "at clojure.core.protocols$seq_reduce.invoke(protocols.clj:31) ~[clojure-1.7.0.jar:?]",
                "at clojure.core$reduce.invoke(core.clj:6519) ~[clojure-1.7.0.jar:?]",
                "at clojure.core$into.invoke(core.clj:6600) ~[clojure-1.7.0.jar:?]",
                "at org.apache.storm.daemon.executor$metrics_tick.invoke(executor.clj:349) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.daemon.executor$fn__4975$tuple_action_fn__4981.invoke(executor.clj:522) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.daemon.executor$mk_task_receiver$fn__4964.invoke(executor.clj:471) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.disruptor$clojure_handler$reify__4475.onEvent(disruptor.clj:41) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:509) ~[storm-core-1.2.1.jar:1.2.1]"
            ],
            "RootCause": "The root cause of the issue is that the Kafka consumer is being closed before it can be reused, leading to an IllegalStateException when attempting to access it after deactivation of the topology.",
            "StepsToReproduce": [
                "Deactivate the Storm topology.",
                "Produce records into the Kafka topic associated with the topology.",
                "Observe the exception thrown by the Storm application."
            ],
            "ExpectedBehavior": "The Storm topology should handle deactivation gracefully without throwing exceptions when data is produced into Kafka.",
            "ObservedBehavior": "An exception is thrown indicating that the Kafka consumer has already been closed, leading to a failure in processing incoming data.",
            "Suggestions": "Review the lifecycle management of the Kafka consumer in the Storm topology to ensure that it is not closed prematurely. Implement checks to prevent operations on a closed consumer.",
            "problem_location": {
                "files": [
                    "KafkaSpout.java",
                    "Executor.java"
                ],
                "classes": [
                    "org.apache.storm.kafka.spout.KafkaSpout",
                    "org.apache.storm.daemon.executor"
                ],
                "methods": [
                    "KafkaSpout.close",
                    "KafkaSpout.nextTuple",
                    "Executor.metrics_tick"
                ]
            },
            "possible_fix": "Ensure that the Kafka consumer is only closed when it is no longer needed and add error handling to manage attempts to access a closed consumer."
        },
        "possible_fix_code": {
            "KafkaSpout.close": "    public void close() {\n        if (kafkaConsumer != null) {\n            try {\n                kafkaConsumer.close();\n            } catch (Exception e) {\n                LOG.error(\"Error closing Kafka consumer: \", e);\n            }\n        }\n        // Additional cleanup logic if necessary\n    }\n",
            "KafkaSpout.nextTuple": "    public void nextTuple() {\n        if (kafkaConsumer == null) {\n            LOG.warn(\"Kafka consumer is closed or not initialized.\");\n            return;\n        }\n        // Existing logic for processing tuples\n    }\n",
            "Executor.metrics_tick": "    public void metrics_tick() {\n        if (kafkaConsumer == null) {\n            LOG.warn(\"Kafka consumer is closed or not initialized. Skipping metrics tick.\");\n            return;\n        }\n        // Existing logic for metrics tick\n    }\n"
        }
    },
    {
        "filename": "STORM-3117.json",
        "creation_time": "2018-06-20T21:37:56.000+0000",
        "bug_report": {
            "Title": "Deleting blobs for running topologies hoses Nimbus",
            "Description": "The issue arises when executing a sequence of operations involving the submission and deletion of topology blobs in Apache Storm. The following pseudo-code is used:\n\n```java\ncluster.submitTopology(cluster.getTopologiesJarFile(), topoName, config, topology);\ncluster.waitTopologyUp(topoName);\ncluster.deleteAllBlobs();\n```\n\nThis sequence leads to Nimbus getting stuck and subsequently restarting due to missing blob files, specifically the topology jar and serialized code files. The stack trace indicates that Nimbus cannot find the required blobs, resulting in a `WrappedKeyNotFoundException` and an `IllegalStateException` related to missing credentials in ZooKeeper.",
            "StackTrace": [
                "2018-06-20 15:48:37.947 o.a.s.d.n.Nimbus timer [ERROR] Error while processing event",
                "java.lang.RuntimeException: KeyNotFoundException(msg:wc-topology-test-1-1529509694-stormcode.ser)",
                "Caused by: org.apache.storm.utils.WrappedKeyNotFoundException: wc-topology-test-1-1529509694-stormcode.ser",
                "2018-06-20 15:48:54.635 o.a.s.u.Utils main [ERROR] Received error in main thread.. terminating server...",
                "java.lang.Error: java.lang.IllegalStateException: Could not find credentials for topology wc-topology-test-1-1529509694 at path /storms."
            ],
            "RootCause": "The root cause of the issue is the failure of Nimbus to locate the necessary blobs (jar files and serialized objects) after they have been deleted. This is compounded by the absence of required credentials in ZooKeeper, leading to an `IllegalStateException`.",
            "StepsToReproduce": [
                "1. Submit a topology using the provided pseudo-code.",
                "2. Wait for the topology to be fully operational.",
                "3. Execute the command to delete all blobs associated with the topology."
            ],
            "ExpectedBehavior": "Nimbus should successfully manage the topology lifecycle without encountering errors related to missing blobs or credentials, allowing for proper cleanup and resource management.",
            "ObservedBehavior": "Nimbus gets stuck and restarts due to exceptions thrown when it cannot find the required blobs and credentials, leading to continuous error messages and server termination.",
            "Suggestions": "Ensure that the required blobs are uploaded correctly to the blob store before attempting to delete them. Additionally, verify and configure the ACLs and credentials for the topology in ZooKeeper to prevent access issues.",
            "problem_location": {
                "files": [
                    "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                    "storm-server/src/main/java/org/apache/storm/blobstore/LocalFsBlobStore.java",
                    "storm-server/src/main/java/org/apache/storm/zookeeper/AclEnforcement.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.nimbus.Nimbus",
                    "org.apache.storm.blobstore.LocalFsBlobStore",
                    "org.apache.storm.zookeeper.AclEnforcement"
                ],
                "methods": [
                    "Nimbus.getBlobMeta",
                    "LocalFsBlobStore.getStoredBlobMeta",
                    "AclEnforcement.verifyAcls"
                ]
            },
            "possible_fix": "To resolve the issue, ensure that the blobs are correctly uploaded to the blob store before deletion. Additionally, review the ACLs and credentials in ZooKeeper, particularly ensuring that the topology's credentials are correctly configured and accessible."
        },
        "possible_fix_code": {
            "org.apache.storm.daemon.nimbus.Nimbus.getBlobMeta": "    public void getBlobMeta(java.lang.String key, org.apache.storm.thrift.async.AsyncMethodCallback<ReadableBlobMeta> resultHandler) throws org.apache.storm.thrift.TException {\n      checkReady();\n      // Ensure that the blob exists before attempting to retrieve its metadata\n      if (!blobExists(key)) {\n          throw new WrappedKeyNotFoundException(\"Blob with key \" + key + \" does not exist.\");\n      }\n      getBlobMeta_call method_call = new getBlobMeta_call(key, resultHandler, this, ___protocolFactory, ___transport);\n      this.___currentMethod = method_call;\n      ___manager.call(method_call);\n    }"
        }
    },
    {
        "filename": "STORM-2993.json",
        "creation_time": "2018-03-12T19:04:16.000+0000",
        "bug_report": {
            "Title": "Storm HDFS bolt throws ClosedChannelException when Time rotation policy is used",
            "Description": "The Storm connector is encountering a `ClosedChannelException` during file writing operations when the Time rotation policy is applied. This issue arises in the worker logs, indicating that the HDFS bolt attempts to write to a closed output stream, likely due to improper synchronization in the timed rotation logic.",
            "StackTrace": [
                "java.nio.channels.ClosedChannelException: null",
                "at org.apache.hadoop.hdfs.ExceptionLastSeen.throwException4Close(ExceptionLastSeen.java:73) ~[stormjar.jar:?]",
                "at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:153) ~[stormjar.jar:?]",
                "at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:105)",
                "at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)",
                "at java.io.DataOutputStream.write(DataOutputStream.java:107)",
                "at java.io.FilterOutputStream.write(FilterOutputStream.java:97)",
                "at org.apache.storm.hdfs.common.HDFSWriter.doWrite(HDFSWriter.java:48)",
                "at org.apache.storm.hdfs.common.AbstractHDFSWriter.write(AbstractHDFSWriter.java:40)",
                "at org.apache.storm.hdfs.bolt.AbstractHdfsBolt.execute(AbstractHdfsBolt.java:158)"
            ],
            "RootCause": "The `ClosedChannelException` is thrown because the HDFS bolt attempts to write to an output stream that has already been closed. This is likely due to the Time rotation policy not synchronizing properly, leading to premature closure of the output stream.",
            "StepsToReproduce": [
                "1. Configure the Storm HDFS bolt with a Time rotation policy.",
                "2. Start the Storm topology and let it run for a period that triggers file rotation.",
                "3. Monitor the worker logs for any `ClosedChannelException` errors."
            ],
            "ExpectedBehavior": "The HDFS bolt should successfully write data to HDFS without encountering a `ClosedChannelException`, even when the Time rotation policy is in effect.",
            "ObservedBehavior": "The HDFS bolt throws a `ClosedChannelException`, indicating that it is trying to write to a closed output stream, which disrupts the data writing process.",
            "Suggestions": "Review the implementation of the Time rotation policy to ensure proper synchronization. Consider adding checks to prevent writing to a closed output stream. Additionally, investigate the lifecycle management of the output stream in the `AbstractHDFSWriter` class.",
            "problem_location": {
                "files": [
                    "external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/HDFSWriter.java",
                    "external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/AbstractHDFSWriter.java"
                ],
                "classes": [
                    "org.apache.storm.hdfs.common.HDFSWriter",
                    "org.apache.storm.hdfs.common.AbstractHDFSWriter"
                ],
                "methods": [
                    "HDFSWriter.doWrite",
                    "AbstractHDFSWriter.write"
                ]
            },
            "possible_fix": "Ensure that the output stream is not closed before the `doWrite` method is called. Implement synchronization mechanisms in the Time rotation policy to prevent concurrent access issues that may lead to closing the stream prematurely."
        },
        "possible_fix_code": {
            "HDFSWriter.doWrite": "    protected void doWrite(Tuple tuple) throws IOException {\n        if (out == null || out.isClosed()) {\n            throw new IOException(\"Output stream is closed. Cannot write data.\");\n        }\n        byte[] bytes = this.format.format(tuple);\n        out.write(bytes);\n        this.offset += bytes.length;\n    }"
        }
    },
    {
        "filename": "STORM-1540.json",
        "creation_time": "2016-02-11T22:55:05.000+0000",
        "bug_report": {
            "Title": "Topology Debug/Sampling Breaks Trident Topologies",
            "Description": "When deploying a Trident topology with debug/sampling enabled, workers crash due to a serialization issue. The error indicates that an instance of `org.apache.storm.trident.tuple.ConsList` is not serializable, leading to a `NotSerializableException`. This occurs during the serialization process handled by Kryo, which is unable to serialize the `ConsList` object as it does not implement the `Serializable` interface.",
            "StackTrace": [
                "2016-02-11 14:13:23.617 o.a.s.util [ERROR] Async loop died!",
                "java.lang.RuntimeException: java.lang.RuntimeException: java.io.NotSerializableException: org.apache.storm.trident.tuple.ConsList",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:448) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:414) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.disruptor$consume_loop_STAR_$fn__7651.invoke(disruptor.clj:83) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.util$async_loop$fn__554.invoke(util.clj:484) [storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_72]",
                "Caused by: java.lang.RuntimeException: java.io.NotSerializableException: org.apache.storm.trident.tuple.ConsList",
                "at org.apache.storm.serialization.SerializableSerializer.write(SerializableSerializer.java:41) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:568) ~[kryo-2.21.jar:?]",
                "at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:75) ~[kryo-2.21.jar:?]",
                "at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18) ~[kryo-2.21.jar:?]",
                "at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:486) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:44) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.serialization.KryoTupleSerializer.serialize(KryoTupleSerializer.java:44) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.worker$mk_transfer_fn$transfer_fn__8346.invoke(worker.clj:186) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.executor$start_batch_transfer__GT_worker_handler_BANG_$fn__8037.invoke(executor.clj:309) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.disruptor$clojure_handler$reify__7634.onEvent(disruptor.clj:40) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:435) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]"
            ],
            "RootCause": "The root cause of the issue is that the `ConsList` class does not implement the `Serializable` interface, which is required for serialization in Java. This leads to a `NotSerializableException` when an instance of `ConsList` is attempted to be serialized by Kryo.",
            "StepsToReproduce": [
                "Deploy a Trident topology.",
                "Enable debug/sampling."
            ],
            "ExpectedBehavior": "The Trident topology should run without crashing, and workers should process data without serialization errors.",
            "ObservedBehavior": "Workers crash with a `NotSerializableException` for the `ConsList` class when debug/sampling is enabled.",
            "Suggestions": "To resolve the serialization issue, consider the following strategies: 1) Implement the `Serializable` interface in the `ConsList` class, 2) Create a custom serializer for `ConsList` using Kryo, 3) Use a wrapper class that implements `Serializable` and contains a `ConsList` instance, or 4) Replace `ConsList` with a different data structure that is already serializable.",
            "problem_location": {
                "files": [
                    "storm-core-1.0.0-SNAPSHOT.jar"
                ],
                "classes": [
                    "org.apache.storm.trident.tuple.ConsList",
                    "org.apache.storm.serialization.KryoValuesSerializer"
                ],
                "methods": []
            },
            "possible_fix": "Implement the `Serializable` interface in the `ConsList` class or create a custom serializer for it in Kryo. Alternatively, consider using a different data structure that is serializable."
        },
        "possible_fix_code": {
            "org.apache.storm.trident.tuple.ConsList": "package org.apache.storm.trident.tuple;\n\nimport java.io.Serializable;\n\npublic class ConsList<T> implements Serializable {\n    private static final long serialVersionUID = 1L;\n    private final T head;\n    private final ConsList<T> tail;\n\n    public ConsList(T head, ConsList<T> tail) {\n        this.head = head;\n        this.tail = tail;\n    }\n\n    public T getHead() {\n        return head;\n    }\n\n    public ConsList<T> getTail() {\n        return tail;\n    }\n\n    // Additional methods for ConsList can be added here\n}"
        }
    },
    {
        "filename": "STORM-2275.json",
        "creation_time": "2017-01-04T23:21:06.000+0000",
        "bug_report": {
            "Title": "Nimbus crashed during state transition of topology",
            "Description": "The Nimbus service in Apache Storm encountered a crash during the state transition of a topology, specifically when processing an event. The logs indicate a `NullPointerException` occurring in the `Nimbus` class, which leads to a `RuntimeException` that halts the process. The issue arises from an assumption that certain variables are non-null, which is incorrect.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.NullPointerException",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$delayEvent$16(Nimbus.java:1174)",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:83)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.storm.daemon.nimbus.Nimbus.transition(Nimbus.java:1215)",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$delayEvent$16(Nimbus.java:1172)",
                "... 1 more",
                "java.lang.RuntimeException: Halting process: Error while processing event",
                "at org.apache.storm.utils.Utils.exitProcess(Utils.java:1792)",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$15(Nimbus.java:1107)",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:104)"
            ],
            "RootCause": "The `NullPointerException` is caused by the assumption that certain variables in the `Nimbus` class are non-null during the event processing. This leads to an attempt to access a null reference, resulting in a crash.",
            "StepsToReproduce": [
                "Deploy a topology using the Nimbus service.",
                "Trigger a state transition for the topology.",
                "Monitor the Nimbus logs for errors during the transition."
            ],
            "ExpectedBehavior": "The Nimbus service should successfully process the state transition of the topology without encountering any exceptions.",
            "ObservedBehavior": "The Nimbus service crashes with a `NullPointerException`, halting the process and preventing the state transition from completing.",
            "Suggestions": "Review the initialization of variables in the `Nimbus` class to ensure that all necessary objects are properly instantiated before use. Check for any missing configurations that might lead to null references.",
            "problem_location": {
                "files": [
                    "Nimbus.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.nimbus.Nimbus"
                ],
                "methods": [
                    "Nimbus.transition",
                    "Nimbus.delayEvent"
                ]
            },
            "possible_fix": "Ensure that all variables used in the `transition` method are properly initialized. Add null checks where necessary to prevent `NullPointerException`. Consider implementing better error handling for event processing."
        },
        "possible_fix_code": {
            "Nimbus.transition": "public void transition() {\n    // Ensure all necessary variables are initialized before use\n    if (someVariable == null) {\n        throw new IllegalStateException(\"someVariable must be initialized before transition.\");\n    }\n    // Additional null checks for other variables\n    if (anotherVariable == null) {\n        throw new IllegalStateException(\"anotherVariable must be initialized before transition.\");\n    }\n    // Proceed with the transition logic\n    // ... (existing transition logic here)\n}"
        }
    },
    {
        "filename": "STORM-2873.json",
        "creation_time": "2017-12-29T18:44:56.000+0000",
        "bug_report": {
            "Title": "Backpressure implementation deletes ephemeral nodes too frequently",
            "Description": "The backpressure implementation in the Storm application is causing the deletion of ephemeral znodes in ZooKeeper too frequently. This behavior leads to a `NoAuthException` when the application attempts to delete a znode without the necessary authentication. The issue arises from the misconfiguration of ZooKeeper authentication credentials or the access control lists (ACLs) for the znodes. The following stack trace illustrates the exception encountered during this process.",
            "StackTrace": [
                "2017-09-18 15:00:34.980 b.s.util WorkerBackpressureThread [WARN] Expecting exception of class: class org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException, but exception chain only contains: (#<NoAuthException org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /backpressure/TestTopology--170916-005358-117-1505523256/d11af335-6e03-48b6-801e-7369acfe9ffd-127.0.0.1-6721>)",
                "2017-09-18 15:00:34.980 b.s.d.worker WorkerBackpressureThread [ERROR] workerBackpressure update failed when connecting to ZK ... will retry",
                "java.lang.RuntimeException: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /backpressure/TestTopology--170916-005358-117-1505523256/d11af335-6e03-48b6-801e-7369acfe9ffd-127.0.0.1-6721",
                "at backtype.storm.util$wrap_in_runtime.invoke(util.clj:52) ~[storm-core-0.10.2.y.jar:0.10.2.y]",
                "at backtype.storm.zookeeper$delete_node.doInvoke(zookeeper.clj:110) ~[storm-core-0.10.2.y.jar:0.10.2.y]",
                "at clojure.lang.RestFn.invoke(RestFn.java:464) ~[clojure-1.6.0.jar:?]",
                "at backtype.storm.zookeeper$delete_recursive.invoke(zookeeper.clj:189) ~[storm-core-0.10.2.y.jar:0.10.2.y]",
                "at backtype.storm.cluster_state.zookeeper_state_factory$_mkState$reify__4207.delete_node(zookeeper_state_factory.clj:117) ~[storm-core-0.10.2.y.jar:0.10.2.y]",
                "Caused by: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /backpressure/TestTopology--170916-005358-117-1505523256/d11af335-6e03-48b6-801e-7369acfe9ffd-127.0.0.1-6721"
            ],
            "RootCause": "The root cause of the issue is a `NoAuthException` from ZooKeeper, indicating that the application is attempting to delete a znode without the necessary authentication credentials or permissions.",
            "StepsToReproduce": [
                "Configure the Storm application to use ZooKeeper for backpressure management.",
                "Attempt to delete an ephemeral znode in ZooKeeper without providing valid authentication credentials.",
                "Observe the resulting `NoAuthException` in the logs."
            ],
            "ExpectedBehavior": "The application should successfully delete the ephemeral znode in ZooKeeper without encountering authentication errors.",
            "ObservedBehavior": "The application fails to delete the znode and logs a `NoAuthException`, indicating insufficient permissions.",
            "Suggestions": "Ensure that the Storm application is configured with the correct ZooKeeper authentication credentials. Review the ACLs for the znodes to confirm that the application has the necessary permissions to perform delete operations. Additionally, check the configuration settings for both the Storm application and ZooKeeper for any discrepancies.",
            "problem_location": {
                "files": [
                    "zookeeper.clj"
                ],
                "classes": [
                    "backtype.storm.zookeeper"
                ],
                "methods": [
                    "delete_node",
                    "delete_recursive"
                ]
            },
            "possible_fix": "Verify and update the ZooKeeper authentication credentials in the Storm configuration. Ensure that the ACLs for the znodes allow the necessary operations for the application."
        },
        "possible_fix_code": {
            "run": "    public void run() {\n        while (running) {\n            try {\n                synchronized (trigger) {\n                    trigger.wait(100);\n                }\n                // Ensure that the application has the correct ZooKeeper authentication credentials before attempting to delete znodes\n                if (hasValidZooKeeperCredentials()) {\n                    callback.onEvent(workerData); // check all executors and update zk backpressure throttle for the worker if needed\n                } else {\n                    // Log a warning or handle the case where credentials are invalid\n                    System.err.println(\"Invalid ZooKeeper credentials. Cannot perform delete operations.\");\n                }\n            } catch (InterruptedException interEx) {\n                // ignored, we are shutting down.\n            }\n        }\n    }"
        }
    },
    {
        "filename": "STORM-2279.json",
        "creation_time": "2017-01-05T20:59:11.000+0000",
        "bug_report": {
            "Title": "Unable to open bolt page of storm ui",
            "Description": "The latest version of the Apache Storm codebase is causing an inability to access the UI for viewing bolt information. When attempting to open the UI, an Internal Server Error is encountered, accompanied by a TTransportException in the stack trace. Additionally, an ArrayIndexOutOfBoundsException is logged in the nimbus.log, indicating an issue with index handling in the Nimbus class's getComponentPageInfo method. The error occurs when the index calculated from the hashcode of the component ID results in a negative value, leading to an invalid access in the tasks list.",
            "StackTrace": [
                "Internal Server Error",
                "org.apache.storm.thrift.transport.TTransportException",
                "at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)",
                "at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)",
                "at org.apache.storm.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)",
                "at org.apache.storm.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:101)",
                "at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)",
                "at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)",
                "at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)",
                "at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)",
                "at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:77)",
                "at org.apache.storm.generated.Nimbus$Client.recv_getComponentPageInfo(Nimbus.java:1369)",
                "at org.apache.storm.generated.Nimbus$Client.getComponentPageInfo(Nimbus.java:1353)",
                "at org.apache.storm.ui.core$component_page.invoke(core.clj:1026)",
                "at org.apache.storm.ui.core$fn__4308.invoke(core.clj:1214)",
                "at org.apache.storm.shade.compojure.core$make_route$fn__789.invoke(core.clj:100)",
                "at org.apache.storm.shade.compojure.core$if_route$fn__777.invoke(core.clj:46)",
                "at org.apache.storm.shade.compojure.core$if_method$fn__770.invoke(core.clj:31)",
                "at org.apache.storm.shade.compojure.core$routing$fn__795.invoke(core.clj:113)",
                "at clojure.core$some.invoke(core.clj:2570)",
                "at org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:113)",
                "at clojure.lang.RestFn.applyTo(RestFn.java:139)",
                "at clojure.core$apply.invoke(core.clj:632)",
                "at org.apache.storm.shade.compojure.core$routes$fn__799.invoke(core.clj:118)",
                "at org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__3573.invoke(json.clj:56)",
                "at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__1924.invoke(multipart_params.clj:118)",
                "at org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__3102.invoke(reload.clj:22)",
                "at org.apache.storm.ui.helpers$requests_middleware$fn__2152.invoke(helpers.clj:54)",
                "at org.apache.storm.ui.core$catch_errors$fn__4474.invoke(core.clj:1460)",
                "at org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__1844.invoke(keyword_params.clj:35)",
                "at org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__1887.invoke(nested_params.clj:84)",
                "at org.apache.storm.shade.ring.middleware.params$wrap_params$fn__1816.invoke(params.clj:64)",
                "at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__1924.invoke(multipart_params.clj:118)",
                "at org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__2139.invoke(flash.clj:35)",
                "at org.apache.storm.shade.ring.middleware.session$wrap_session$fn__2125.invoke(session.clj:98)",
                "at org.apache.storm.shade.ring.util.servlet$make_service_method$fn__1674.invoke(servlet.clj:127)",
                "at org.apache.storm.shade.ring.util.servlet$servlet$fn__1678.invoke(servlet.clj:136)",
                "at org.apache.storm.shade.ring.util.servlet.proxy$javax.servlet.http.HttpServlet$ff19274a.service(Unknown Source)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:654)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1320)",
                "at org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)",
                "at org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)",
                "at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)",
                "at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)",
                "at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)",
                "at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)",
                "at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)",
                "at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is an ArrayIndexOutOfBoundsException occurring in the getComponentPageInfo method of the Nimbus class. This happens when the task index calculated from the component ID's hashcode results in a negative value, leading to an invalid access in the tasks list.",
            "StepsToReproduce": [
                "Set up the latest version of Apache Storm using Vagrant.",
                "Navigate to the bolt page URL: http://node1:8080/component.html?id=SlidingTimeCorrectness-winSec1slideSec1VerificationBolt&topology_id=SlidingWindowTestw1s1-2-1483646178.",
                "Observe the Internal Server Error and the stack trace in the UI."
            ],
            "ExpectedBehavior": "The UI should display the bolt information without any errors, allowing users to view the details of the specified component.",
            "ObservedBehavior": "An Internal Server Error is displayed, and the UI fails to load the bolt information due to exceptions in the backend.",
            "Suggestions": "1. Validate the input parameters (topology_id and component_id) to ensure they correspond to existing components in the topology. 2. Implement error handling in the getComponentPageInfo method to manage cases where the calculated index is negative. 3. Ensure that the Thrift server is properly configured and accessible.",
            "problem_location": {
                "files": [
                    "Nimbus.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.nimbus.Nimbus"
                ],
                "methods": [
                    "getComponentPageInfo"
                ]
            },
            "possible_fix": "Modify the getComponentPageInfo method to include input validation for the task index calculation. Ensure that the index is always non-negative before accessing the tasks list. Example code change:\n\nint taskIndex = TupleUtils.listHashCode(Arrays.asList(componentId)) % tasks.size();\nif (taskIndex < 0) {\n    taskIndex = 0; // or handle the error appropriately\n}\nint taskId = tasks.get(taskIndex);"
        },
        "possible_fix_code": {
            "getComponentPageInfo": "    public void getComponentPageInfo(String topology_id, String component_id, String window, boolean is_include_sys, org.apache.thrift.async.AsyncMethodCallback resultHandler) throws org.apache.thrift.TException {\n        checkReady();\n        // Validate input parameters\n        if (topology_id == null || component_id == null) {\n            throw new IllegalArgumentException(\"Topology ID and Component ID cannot be null.\");\n        }\n        // Calculate task index\n        int taskIndex = TupleUtils.listHashCode(Arrays.asList(component_id)) % tasks.size();\n        if (taskIndex < 0) {\n            taskIndex = 0; // Default to the first task if index is negative\n        }\n        getComponentPageInfo_call method_call = new getComponentPageInfo_call(topology_id, component_id, window, is_include_sys, resultHandler, this, ___protocolFactory, ___transport);\n        this.___currentMethod = method_call;\n        ___manager.call(method_call);\n    }"
        }
    },
    {
        "filename": "STORM-3079.json",
        "creation_time": "2018-05-17T19:29:10.000+0000",
        "bug_report": {
            "Title": "Improve getMessage support for ThriftExceptions",
            "Description": "The current implementation of the Thrift-generated code does not support the `getMessage()` method, leading to confusion when encountering error call stacks with null messages. This issue is particularly evident in the context of the `KeyNotFoundException` thrown by the `LocalFsBlobStore` class in Apache Storm, indicating that a requested blob could not be found in the local file system. The stack trace shows that the error occurs during blob retrieval operations, which suggests a need for better logging and error handling to provide clearer messages.",
            "StackTrace": [
                "2018-05-16 21:15:04.596 o.a.s.d.n.Nimbus timer [INFO] Exception {}",
                "org.apache.storm.generated.KeyNotFoundException: null",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:258) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getBlob(LocalFsBlobStore.java:393) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.blobstore.BlobStore.readBlobTo(BlobStore.java:310) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.blobstore.BlobStore.readBlob(BlobStore.java:339) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.TopoCache.readTopology(TopoCache.java:67) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.readStormTopologyAsNimbus(Nimbus.java:670) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.rmDependencyJarsInTopology(Nimbus.java:2333) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2387) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2674) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.StormTimer$1.run(StormTimer.java:111) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227) ~[storm-client-2.0.0.y.jar:2.0.0.y]"
            ],
            "RootCause": "The root cause of the issue is the `KeyNotFoundException` being thrown when the system attempts to retrieve a blob that does not exist in the local file system blob store. This indicates that either the blob was never stored or the key used for retrieval is incorrect.",
            "StepsToReproduce": [
                "Attempt to retrieve a blob using a key that does not exist in the local file system blob store.",
                "Observe the resulting `KeyNotFoundException` in the logs."
            ],
            "ExpectedBehavior": "When a blob is successfully retrieved, the system should return the blob data along with appropriate metadata. If the blob does not exist, a clear and informative error message should be logged instead of a null message.",
            "ObservedBehavior": "The system throws a `KeyNotFoundException` with a null message, leading to confusion in the logs and making it difficult to diagnose the issue.",
            "Suggestions": "Implement better error handling in the `getBlob` method of the `LocalFsBlobStore` class to provide a meaningful message when a blob is not found. Additionally, consider enhancing the logging mechanism to include more context about the error.",
            "problem_location": {
                "files": [
                    "LocalFsBlobStore.java"
                ],
                "classes": [
                    "org.apache.storm.blobstore.LocalFsBlobStore"
                ],
                "methods": [
                    "getBlob"
                ]
            },
            "possible_fix": "Modify the `getBlob` method in the `LocalFsBlobStore` class to include a more descriptive error message when a `KeyNotFoundException` is thrown. For example, include the key that was attempted to be accessed in the exception message."
        },
        "possible_fix_code": {
            "getBlob": "    public InputStreamWithMeta getBlob(String key, Subject who) throws AuthorizationException, KeyNotFoundException {\n        validateKey(key);\n        if (!checkForBlobOrDownload(key)) {\n            checkForBlobUpdate(key);\n        }\n        SettableBlobMeta meta = getStoredBlobMeta(key);\n        if (meta == null) {\n            throw new KeyNotFoundException(\"Blob not found for key: \" + key);\n        }\n        _aclHandler.hasPermissions(meta.get_acl(), READ, who, key);\n        try {\n            return new BlobStoreFileInputStream(fbs.read(DATA_PREFIX + key));\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n    }"
        }
    },
    {
        "filename": "STORM-3096.json",
        "creation_time": "2018-06-05T18:39:44.000+0000",
        "bug_report": {
            "Title": "Blobstores deleted before topologies can be submitted",
            "Description": "The issue arises from a race condition during topology submission in Apache Storm, where the Nimbus timer triggers the cleanup process (doCleanup()) prematurely, leading to the deletion of necessary blobs. Despite previous attempts to address this in STORM-3053, the error persists. The specific method identified as problematic is idsOfTopologiesWithPrivateWorkerKeys(), which is responsible for managing topology submissions. The stack trace indicates that a WrappedKeyNotFoundException occurs when the system attempts to access blobs that have already been deleted.",
            "StackTrace": [
                "org.apache.storm.utils.WrappedKeyNotFoundException: topology-testHardCoreFaultTolerance-4-18-1528026822-stormcode.ser",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:259)",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getBlob(LocalFsBlobStore.java:394)",
                "at org.apache.storm.blobstore.BlobStore.readBlobTo(BlobStore.java:310)",
                "at org.apache.storm.blobstore.BlobStore.readBlob(BlobStore.java:339)",
                "at org.apache.storm.daemon.nimbus.TopoCache.readTopology(TopoCache.java:67)",
                "at org.apache.storm.daemon.nimbus.Nimbus.readStormTopologyAsNimbus(Nimbus.java:680)",
                "at org.apache.storm.daemon.nimbus.Nimbus.rmDependencyJarsInTopology(Nimbus.java:2389)",
                "at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2443)"
            ],
            "RootCause": "The root cause of the issue is the premature deletion of blobs required for topology execution, triggered by the Nimbus cleanup process. The WrappedKeyNotFoundException indicates that the system cannot find the necessary blobs, which may have been deleted before the topology could be fully submitted.",
            "StepsToReproduce": [
                "Submit a topology with the name 'topology-testHardCoreFaultTolerance-4-18-1528026822'.",
                "Monitor the Nimbus logs for cleanup actions.",
                "Observe the occurrence of WrappedKeyNotFoundException related to the topology's blobs."
            ],
            "ExpectedBehavior": "The topology should be submitted successfully without any exceptions, and all required blobs should be available for execution.",
            "ObservedBehavior": "The topology submission fails with a WrappedKeyNotFoundException, indicating that the required blobs are missing due to premature deletion.",
            "Suggestions": "Investigate the timing of the doCleanup() method in relation to topology submissions. Consider implementing a mechanism to delay cleanup until all topologies are fully processed. Additionally, review the blob storage configuration to ensure blobs are not deleted prematurely.",
            "problem_location": {
                "files": [
                    "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                    "storm-server/src/main/java/org/apache/storm/blobstore/LocalFsBlobStore.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.nimbus.Nimbus",
                    "org.apache.storm.blobstore.LocalFsBlobStore"
                ],
                "methods": [
                    "Nimbus.doCleanup",
                    "LocalFsBlobStore.getStoredBlobMeta",
                    "LocalFsBlobStore.getBlob"
                ]
            },
            "possible_fix": "Modify the doCleanup() method in Nimbus to ensure it only executes after all topologies have been fully submitted and processed. Additionally, review the logic in getStoredBlobMeta and getBlob to handle cases where blobs may not be present."
        },
        "possible_fix_code": {
            "Nimbus.doCleanup": "    public void doCleanup() {\n        // Ensure that cleanup only occurs after all topologies have been fully submitted and processed.\n        if (!areAllTopologiesProcessed()) {\n            return; // Exit if topologies are still being processed\n        }\n        // Proceed with cleanup logic\n        // Existing cleanup code...\n    }\n\n    private boolean areAllTopologiesProcessed() {\n        // Logic to check if all topologies are processed\n        // This could involve checking a status flag or a list of active topologies\n        return true; // Placeholder for actual implementation\n    }"
        }
    },
    {
        "filename": "STORM-1642.json",
        "creation_time": "2016-03-21T07:34:06.000+0000",
        "bug_report": {
            "Title": "NullPointerException when deserializing in KryoTupleDeserializer",
            "Description": "A NullPointerException (NPE) occurs during the deserialization process in an Apache Storm application. The error arises when the system attempts to deserialize a byte array using the KryoTupleDeserializer. The stack trace indicates that the NPE is triggered when the method setBuffer is called with a null byte array. This suggests that the data being passed for deserialization is either null or improperly initialized. The issue may stem from the DisruptorQueue, which retrieves objects from a buffer and does not adequately handle null values before passing them to the deserializer.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.NullPointerException",
                "at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:135)",
                "at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:106)",
                "at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80)",
                "at backtype.storm.daemon.executor$fn__5694$fn__5707$fn__5758.invoke(executor.clj:819)",
                "at backtype.storm.util$async_loop$fn__545.invoke(util.clj:479)",
                "at clojure.lang.AFn.run(AFn.java:22)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.NullPointerException",
                "at com.esotericsoftware.kryo.io.Input.setBuffer(Input.java:57)",
                "at backtype.storm.serialization.KryoTupleDeserializer.deserialize(KryoTupleDeserializer.java:47)",
                "at backtype.storm.daemon.executor$mk_task_receiver$fn__5615.invoke(executor.clj:433)",
                "at backtype.storm.disruptor$clojure_handler$reify__5189.onEvent(disruptor.clj:58)",
                "at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:132)"
            ],
            "RootCause": "The NullPointerException is caused by a null byte array being passed to the deserialize method in KryoTupleDeserializer. This is likely due to null objects being retrieved from the buffer in DisruptorQueue without proper validation.",
            "StepsToReproduce": [
                "Run the application with a configuration that triggers deserialization in KryoTupleDeserializer.",
                "Ensure that the buffer in DisruptorQueue contains null values.",
                "Observe the logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The deserialization process should successfully convert the byte array into a Tuple object without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown during the deserialization process, causing the application to halt.",
            "Suggestions": "1. Investigate the code responsible for populating the buffer in DisruptorQueue to ensure that null values are not added. 2. Implement null checks in the consumeBatchToCursor method to prevent null objects from being passed to the handler. 3. Add logging to trace the flow of data into the buffer.",
            "problem_location": {
                "files": [
                    "KryoTupleDeserializer.java",
                    "DisruptorQueue.java"
                ],
                "classes": [
                    "backtype.storm.serialization.KryoTupleDeserializer",
                    "backtype.storm.utils.DisruptorQueue"
                ],
                "methods": [
                    "KryoTupleDeserializer.deserialize",
                    "DisruptorQueue.consumeBatchToCursor"
                ]
            },
            "possible_fix": "Add null checks in the consumeBatchToCursor method to prevent null objects from being passed to the handler. Additionally, ensure that the buffer is populated with valid data before deserialization."
        },
        "possible_fix_code": {
            "DisruptorQueue.consumeBatchToCursor": "    private void consumeBatchToCursor(long cursor, EventHandler<Object> handler) {\n        for (long curr = _consumer.get() + 1; curr <= cursor; curr++) {\n            try {\n                AtomicReference<Object> mo = _buffer.get(curr);\n                Object o = mo.getAndSet(null);\n                if (o == INTERRUPT) {\n                    throw new InterruptedException(\"Disruptor processing interrupted\");\n                } else if (o == null) {\n                    LOG.error(\"NULL found in {}:{}\", this.getName(), cursor);\n                    continue; // Skip null objects to prevent passing them to the handler\n                } else {\n                    handler.onEvent(o, curr, curr == cursor);\n                    if (_enableBackpressure && _cb != null && (_metrics.writePos() - curr + _overflowCount.get()) <= _lowWaterMark) {\n                        try {\n                            if (_throttleOn) {\n                                _throttleOn = false;\n                                _cb.lowWaterMark();\n                            }\n                        } catch (Exception e) {\n                            throw new RuntimeException(\"Exception during calling lowWaterMark callback!\");\n                        }\n                    }\n                }\n            } catch (Exception e) {\n                throw new RuntimeException(e);\n            }\n        }\n        _consumer.set(cursor);\n    }"
        }
    },
    {
        "filename": "STORM-2700.json",
        "creation_time": "2017-08-21T14:09:50.000+0000",
        "bug_report": {
            "Title": "Blobstore shouldn't check ACL when Blobstore Acl validation disabled",
            "Description": "The issue arises when the configuration setting `storm.blobstore.acl.validation.enabled` is set to false, yet the blobstore continues to enforce ACL checks. This leads to an `AuthorizationException` indicating that the user 'ethan' lacks READ access to the resource 'key1', causing failures in the blob localization process essential for the Apache Storm application. The subsequent `RuntimeException` indicates that the process is halted due to this authorization issue.",
            "StackTrace": [
                "2017-08-21 13:56:19.800 o.a.s.d.s.Slot SLOT_6702 [ERROR] Error when processing event",
                "java.util.concurrent.ExecutionException: AuthorizationException(msg:ethan does not have READ access to key1)",
                "at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_131]",
                "at java.util.concurrent.FutureTask.get(FutureTask.java:206) ~[?:1.8.0_131]",
                "at org.apache.storm.localizer.LocalDownloadedResource$NoCancelFuture.get(LocalDownloadedResource.java:63) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:410) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:305) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:789) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "Caused by: org.apache.storm.generated.AuthorizationException",
                "at org.apache.storm.localizer.Localizer.downloadBlob(Localizer.java:527) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.localizer.Localizer.access$000(Localizer.java:68) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:497) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:473) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_131]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]",
                "at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_131]",
                "2017-08-21 13:56:19.800 o.a.s.u.Utils SLOT_6702 [ERROR] Halting process: Error when processing an event",
                "java.lang.RuntimeException: Halting process: Error when processing an event",
                "at org.apache.storm.utils.Utils.exitProcess(Utils.java:437) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:823) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "2017-08-21 13:56:19.802 o.a.s.d.s.Supervisor Thread-6 [INFO] Shutting down supervisor b350cfb4-b333-4ea5-965e-b0698aaea80f-10.88.214.182",
                "2017-08-21 13:56:19.803 o.a.s.e.EventManagerImp Thread-5 [INFO] Event manager interrupted"
            ],
            "RootCause": "The root cause of the issue is that the blobstore is incorrectly enforcing ACL checks even when the ACL validation is disabled, leading to an `AuthorizationException` for the user 'ethan' who does not have the required READ access to the resource 'key1'.",
            "StepsToReproduce": [
                "1. Create a blobstore with permission set to one user (e.g., mapredqa).",
                "   sudo -u mapredqa storm blobstore create --file test-blobstore.txt --acl u:mapredqa:rwa key1",
                "2. Submit a topology with topology.blobstore.map config as someone else (e.g., ethan).",
                "   sudo -u ethan storm jar /tmp/storm-starter-2.0.0-SNAPSHOT.jar org.apache.storm.starter.WordCountTopology wc -c topology.blobstore.map='{\"key1\":{\"localname\":\"test-blobstore.txt\", \"uncompress\":false}}'"
            ],
            "ExpectedBehavior": "When `storm.blobstore.acl.validation.enabled` is set to false, the blobstore should not enforce any ACL checks, allowing any user to access the blob without authorization errors.",
            "ObservedBehavior": "Despite the ACL validation being disabled, the system throws an `AuthorizationException` indicating that user 'ethan' does not have READ access to 'key1', halting the process.",
            "Suggestions": "To resolve this issue, verify the configuration settings related to ACL validation in the Storm application. Ensure that the blobstore correctly respects the `storm.blobstore.acl.validation.enabled` setting. If necessary, grant READ access to the user 'ethan' for the resource 'key1'.",
            "problem_location": {
                "files": [
                    "Localizer.java",
                    "Slot.java",
                    "Utils.java"
                ],
                "classes": [
                    "org.apache.storm.localizer.Localizer",
                    "org.apache.storm.daemon.supervisor.Slot",
                    "org.apache.storm.utils.Utils"
                ],
                "methods": [
                    "Localizer.downloadBlob",
                    "Slot.handleWaitingForBlobLocalization",
                    "Utils.exitProcess"
                ]
            },
            "possible_fix": "Check the implementation of the blobstore ACL validation logic to ensure it correctly bypasses checks when `storm.blobstore.acl.validation.enabled` is set to false. If the logic is flawed, modify it to respect this configuration setting."
        },
        "possible_fix_code": {
            "Localizer.downloadBlob": "  private LocalizedResource downloadBlob(Map<String, Object> conf, String key, File localFile,\n      String user, boolean uncompress, boolean isUpdate)\n      throws AuthorizationException, KeyNotFoundException, IOException {\n    ClientBlobStore blobstore = null;\n    try {\n      blobstore = getClientBlobStore();\n      long nimbusBlobVersion = ServerUtils.nimbusVersionOfBlob(key, blobstore);\n      long oldVersion = ServerUtils.localVersionOfBlob(localFile.toString());\n      FileOutputStream out = null;\n      PrintWriter writer = null;\n      int numTries = 0;\n      String localizedPath = localFile.toString();\n      String localFileWithVersion = ServerUtils.constructBlobWithVersionFileName(localFile.toString(),\n              nimbusBlobVersion);\n      String localVersionFile = ServerUtils.constructVersionFileName(localFile.toString());\n      String downloadFile = localFileWithVersion;\n      if (uncompress) {\n        // we need to download to temp file and then unpack into the one requested\n        downloadFile = new File(localFile.getParent(), TO_UNCOMPRESS + localFile.getName()).toString();\n      }\n      while (numTries < _blobDownloadRetries) {\n        out = new FileOutputStream(downloadFile);\n        numTries++;\n        try {\n          // Check if ACL validation is enabled before enforcing access control\n          if (ObjectReader.getBoolean(conf.get(\"storm.blobstore.acl.validation.enabled\"), true) && !ServerUtils.canUserReadBlob(blobstore.getBlobMeta(key), user)) {\n            throw new AuthorizationException(user + \" does not have READ access to \" + key);\n          }\n          InputStreamWithMeta in = blobstore.getBlob(key);\n          byte[] buffer = new byte[1024];\n          int len;\n          while ((len = in.read(buffer)) >= 0) {\n            out.write(buffer, 0, len);\n          }\n          out.close();\n          in.close();\n          if (uncompress) {\n            ServerUtils.unpack(new File(downloadFile), new File(localFileWithVersion));\n            LOG.debug(\"uncompressed \" + downloadFile + \" to: \" + localFileWithVersion);\n          }\n\n          // Next write the version.\n          LOG.info(\"Blob: \" + key + \" updated with new Nimbus-provided version: \" +\n              nimbusBlobVersion + \" local version was: \" + oldVersion);\n          // The false parameter ensures overwriting the version file, not appending\n          writer = new PrintWriter(\n              new BufferedWriter(new FileWriter(localVersionFile, false)));\n          writer.println(nimbusBlobVersion);\n          writer.close();\n\n          try {\n            setBlobPermissions(conf, user, localFileWithVersion);\n            setBlobPermissions(conf, user, localVersionFile);\n\n            // Update the key.current symlink. First create tmp symlink and do\n            // move of tmp to current so that the operation is atomic.\n            String tmp_uuid_local = java.util.UUID.randomUUID().toString();\n            LOG.debug(\"Creating a symlink @\" + localFile + \".\" + tmp_uuid_local + \" , \" +\n                \"linking to: \" + localFile + \".\" + nimbusBlobVersion);\n            File uuid_symlink = new File(localFile + \".\" + tmp_uuid_local);\n\n            Files.createSymbolicLink(uuid_symlink.toPath(),\n                Paths.get(ServerUtils.constructBlobWithVersionFileName(localFile.toString(),\n                        nimbusBlobVersion)));\n            File current_symlink = new File(ServerUtils.constructBlobCurrentSymlinkName(\n                    localFile.toString()));\n            Files.move(uuid_symlink.toPath(), current_symlink.toPath(), ATOMIC_MOVE);\n          } catch (IOException e) {\n            // if we fail after writing the version file but before we move current link we need to\n            // restore the old version to the file\n            try {\n              PrintWriter restoreWriter = new PrintWriter(\n                  new BufferedWriter(new FileWriter(localVersionFile, false)));\n              restoreWriter.println(oldVersion);\n              restoreWriter.close();\n            } catch (IOException ignore) {}\n            throw e;\n          }\n\n          String oldBlobFile = localFile + \".\" + oldVersion;\n          try {\n            // Remove the old version. Note that if a number of processes have that file open,\n            // the OS will keep the old blob file around until they all close the handle and only\n            // then deletes it. No new process will open the old blob, since the users will open the\n            // blob through the \"blob.current\" symlink, which always points to the latest version of\n            // a blob. Remove the old version after the current symlink is updated as to not affect\n            // anyone trying to read it.\n            if ((oldVersion != -1) && (oldVersion != nimbusBlobVersion)) {\n              LOG.info(\"Removing an old blob file:\" + oldBlobFile);\n              Files.delete(Paths.get(oldBlobFile));\n            }\n          } catch (IOException e) {\n            // At this point we have downloaded everything and moved symlinks.  If the remove of\n            // old fails just log an error\n            LOG.error(\"Exception removing old blob version: \" + oldBlobFile);\n          }\n\n          break;\n        } catch (AuthorizationException ae) {\n          // we consider this non-retriable exceptions\n          if (out != null) {\n            out.close();\n          }\n          new File(downloadFile).delete();\n          throw ae;\n        } catch (IOException | KeyNotFoundException e) {\n          if (out != null) {\n            out.close();\n          }\n          if (writer != null) {\n            writer.close();\n          }\n          new File(downloadFile).delete();\n          if (uncompress) {\n            try {\n              FileUtils.deleteDirectory(new File(localFileWithVersion));\n            } catch (IOException ignore) {}\n          }\n          if (!isUpdate) {\n            // don't want to remove existing version file if its an update\n            new File(localVersionFile).delete();\n          }\n\n          if (numTries < _blobDownloadRetries) {\n            LOG.error(\"Failed to download blob, retrying\", e);\n          } else {\n            throw e;\n          }\n        }\n      }\n      return new LocalizedResource(key, localizedPath, uncompress);\n    } finally {\n      if(blobstore != null) {\n        blobstore.shutdown();\n      }\n    }\n  }"
        }
    },
    {
        "filename": "STORM-1663.json",
        "creation_time": "2016-03-29T06:07:27.000+0000",
        "bug_report": {
            "Title": "Clicking on an active topology from storm ui home page and then refreshing the page throws exception",
            "Description": "When a user clicks on an active topology from the Storm UI home page and subsequently refreshes the page, an exception is thrown. The exception is of type `org.apache.storm.thrift.transport.TTransportException`, which indicates a failure in the transport layer of Apache Storm's Thrift communication. This issue may arise due to various factors such as network connectivity problems, server unavailability, misconfiguration, or timeouts.",
            "StackTrace": [
                "org.apache.storm.thrift.transport.TTransportException",
                "at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)",
                "at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)",
                "at org.apache.storm.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)",
                "at org.apache.storm.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)",
                "at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)",
                "at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)",
                "at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)",
                "at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)",
                "at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:77)",
                "at org.apache.storm.generated.Nimbus$Client.recv_getTopologyPageInfo(Nimbus.java:1243)",
                "at org.apache.storm.generated.Nimbus$Client.getTopologyPageInfo(Nimbus.java:1228)",
                "at org.apache.storm.ui.core$topology_page.invoke(core.clj:638)",
                "at org.apache.storm.ui.core$fn__3662.invoke(core.clj:987)",
                "at org.apache.storm.shade.compojure.core$make_route$fn__302.invoke(core.clj:93)",
                "at org.apache.storm.shade.compojure.core$if_route$fn__290.invoke(core.clj:39)",
                "at org.apache.storm.shade.compojure.core$if_method$fn__283.invoke(core.clj:24)",
                "at org.apache.storm.shade.compojure.core$routing$fn__308.invoke(core.clj:106)",
                "at clojure.core$some.invoke(core.clj:2570)",
                "at org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:106)",
                "at clojure.lang.RestFn.applyTo(RestFn.java:139)",
                "at clojure.core$apply.invoke(core.clj:632)",
                "at org.apache.storm.shade.compojure.core$routes$fn__312.invoke(core.clj:111)",
                "at org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__1204.invoke(json.clj:56)",
                "at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__765.invoke(multipart_params.clj:103)",
                "at org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__724.invoke(reload.clj:22)",
                "at org.apache.storm.ui.helpers$requests_middleware$fn__3091.invoke(helpers.clj:50)",
                "at org.apache.storm.ui.core$catch_errors$fn__3837.invoke(core.clj:1250)",
                "at org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__2852.invoke(keyword_params.clj:27)",
                "at org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__2892.invoke(nested_params.clj:65)",
                "at org.apache.storm.shade.ring.middleware.params$wrap_params$fn__2823.invoke(params.clj:55)",
                "at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__765.invoke(multipart_params.clj:103)",
                "at org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__3075.invoke(flash.clj:14)",
                "at org.apache.storm.shade.ring.middleware.session$wrap_session$fn__3063.invoke(session.clj:43)",
                "at org.apache.storm.shade.ring.middleware.cookies$wrap_cookies$fn__2991.invoke(cookies.clj:160)",
                "at org.apache.storm.shade.ring.util.servlet$make_service_method$fn__2729.invoke(servlet.clj:127)",
                "at org.apache.storm.shade.ring.util.servlet$servlet$fn__2733.invoke(servlet.clj:136)",
                "at org.apache.storm.shade.ring.util.servlet.proxy$javax.servlet.http.HttpServlet$ff19274a.service(Unknown Source)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:654)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1320)",
                "at org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)",
                "at org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)",
                "at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)",
                "at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)",
                "at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)",
                "at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is identified as a `TTransportException`, which typically indicates problems with the transport layer in Apache Storm's Thrift communication. This can be due to network issues, server unavailability, misconfiguration, or timeouts.",
            "StepsToReproduce": [
                "Navigate to the Storm UI home page.",
                "Click on an active topology.",
                "Refresh the page."
            ],
            "ExpectedBehavior": "The topology page should refresh without throwing any exceptions, displaying the current state of the topology.",
            "ObservedBehavior": "An exception of type `TTransportException` is thrown, indicating a failure in the transport layer.",
            "Suggestions": "To troubleshoot this issue, check the following: 1. Network connectivity between the client and server. 2. Ensure the server is running and accessible. 3. Review configuration settings for the Storm topology and Thrift client. 4. Check logs for additional error messages.",
            "problem_location": {
                "files": [
                    "core.clj",
                    "helpers.clj"
                ],
                "classes": [
                    "org.apache.storm.ui.core",
                    "org.apache.storm.ui.helpers"
                ],
                "methods": [
                    "core$topology_page.invoke",
                    "helpers$requests_middleware$fn__3091.invoke"
                ]
            },
            "possible_fix": "Investigate and resolve any network connectivity issues, ensure the server is operational, and verify the configuration settings for the Storm topology and Thrift client."
        },
        "possible_fix_code": {
            "org.apache.storm.generated.Nimbus.recv_getTopologyPageInfo": "    public TopologyPageInfo recv_getTopologyPageInfo() throws NotAliveException, AuthorizationException, org.apache.thrift.TException\n    {\n      getTopologyPageInfo_result result = new getTopologyPageInfo_result();\n      try {\n          receiveBase(result, \"getTopologyPageInfo\");\n      } catch (org.apache.thrift.TTransportException e) {\n          // Log the transport exception and rethrow it\n          LOG.error(\"Transport exception occurred while receiving topology page info: \", e);\n          throw e;\n      }\n      if (result.is_set_success()) {\n        return result.success;\n      }\n      if (result.e != null) {\n        throw result.e;\n      }\n      if (result.aze != null) {\n        throw result.aze;\n      }\n      throw new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.MISSING_RESULT, \"getTopologyPageInfo failed: unknown result\");\n    }"
        }
    },
    {
        "filename": "STORM-2518.json",
        "creation_time": "2017-05-17T06:26:37.000+0000",
        "bug_report": {
            "Title": "NPE during uploading dependency artifacts with secured cluster",
            "Description": "A NullPointerException (NPE) occurs when attempting to upload dependency artifacts in a secured cluster environment. The issue arises during the process of adding Access Control Lists (ACLs) to a user, specifically when the 'name' field is not provided, which is optional in the Thrift specification. The Nimbus service fails to check for null values before processing, leading to the NPE in the `fixACLsForUser` method of the `BlobStoreAclHandler` class.",
            "StackTrace": [
                "java.lang.NullPointerException: null",
                "at org.apache.storm.blobstore.BlobStoreAclHandler.fixACLsForUser(BlobStoreAclHandler.java:382)",
                "at org.apache.storm.blobstore.BlobStoreAclHandler.normalizeSettableACLs(BlobStoreAclHandler.java:357)",
                "at org.apache.storm.blobstore.BlobStoreAclHandler.normalizeSettableBlobMeta(BlobStoreAclHandler.java:306)",
                "at org.apache.storm.blobstore.LocalFsBlobStore.createBlob(LocalFsBlobStore.java:103)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)",
                "at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)",
                "at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9064.beginCreateBlob(nimbus.clj:2047)",
                "at org.apache.storm.generated.Nimbus$Processor$beginCreateBlob.getResult(Nimbus.java:3430)",
                "at org.apache.storm.generated.Nimbus$Processor$beginCreateBlob.getResult(Nimbus.java:3414)",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.storm.security.auth.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:144)",
                "at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The NullPointerException is likely caused by null parameters being passed to the methods involved in the ACL processing. Specifically, the 'acls' parameter in the `fixACLsForUser` and `normalizeSettableACLs` methods, or the 'who' parameter in `normalizeSettableACLs`, or the 'meta' parameter in `normalizeSettableBlobMeta` could be null.",
            "StepsToReproduce": [
                "Attempt to upload dependency artifacts to a secured cluster.",
                "Ensure that the 'name' field for the user is not provided (left null).",
                "Monitor the logs for any NullPointerException errors."
            ],
            "ExpectedBehavior": "The system should successfully upload the dependency artifacts and apply the appropriate ACLs without throwing any exceptions.",
            "ObservedBehavior": "The system throws a NullPointerException, causing the upload of artifacts to fail and preventing topology submission.",
            "Suggestions": "Implement null checks for the parameters in the `fixACLsForUser`, `normalizeSettableACLs`, and `normalizeSettableBlobMeta` methods to prevent the NullPointerException. Ensure that the 'name' field is handled correctly when it is optional.",
            "problem_location": {
                "files": [
                    "BlobStoreAclHandler.java"
                ],
                "classes": [
                    "org.apache.storm.blobstore.BlobStoreAclHandler"
                ],
                "methods": [
                    "fixACLsForUser",
                    "normalizeSettableACLs",
                    "normalizeSettableBlobMeta"
                ]
            },
            "possible_fix": "Add null checks in the methods to handle cases where 'acls', 'who', or 'meta' may be null. For example, in `fixACLsForUser`, check if 'acls' is null before iterating over it. In `normalizeSettableACLs`, check if 'acls' or 'who' is null before proceeding with ACL normalization."
        },
        "possible_fix_code": {
            "fixACLsForUser": "    private void fixACLsForUser(List<AccessControl> acls, String user, int mask) {\n        if (acls == null) {\n            throw new IllegalArgumentException(\"Access control list cannot be null\");\n        }\n        if (user == null) {\n            throw new IllegalArgumentException(\"User cannot be null\");\n        }\n        boolean foundUserACL = false;\n        for (AccessControl control : acls) {\n            if (control.get_type() == AccessControlType.USER && control.get_name().equals(user)) {\n                int currentAccess = control.get_access();\n                if ((currentAccess & mask) != mask) {\n                    control.set_access(currentAccess | mask);\n                }\n                foundUserACL = true;\n                break;\n            }\n        }\n        if (!foundUserACL) {\n            AccessControl userACL = new AccessControl();\n            userACL.set_type(AccessControlType.USER);\n            userACL.set_name(user);\n            userACL.set_access(mask);\n            acls.add(userACL);\n        }\n    }",
            "normalizeSettableACLs": "    private final List<AccessControl> normalizeSettableACLs(String key, List<AccessControl> acls, Subject who,\n                                                            int opMask) {\n        if (acls == null) {\n            throw new IllegalArgumentException(\"Access control list cannot be null\");\n        }\n        if (who == null) {\n            throw new IllegalArgumentException(\"Subject cannot be null\");\n        }\n        List<AccessControl> cleanAcls = removeBadACLs(acls);\n        Set<String> userNames = getUserNamesFromSubject(who);\n        for (String user : userNames) {\n            fixACLsForUser(cleanAcls, user, opMask);\n        }\n        if ((who == null || userNames.isEmpty()) && !worldEverything(acls)) {\n            cleanAcls.addAll(BlobStoreAclHandler.WORLD_EVERYTHING);\n            LOG.debug(\"Access Control for key {} is normalized to world everything {}\", key, cleanAcls);\n            if (!acls.isEmpty())\n                LOG.warn(\"Access control for blob with key {} is normalized to WORLD_EVERYTHING\", key);\n        }\n        return cleanAcls;\n    }",
            "normalizeSettableBlobMeta": "    public void normalizeSettableBlobMeta(String key, SettableBlobMeta meta, Subject who, int opMask) {\n        if (meta == null) {\n            throw new IllegalArgumentException(\"SettableBlobMeta cannot be null\");\n        }\n        meta.set_acl(normalizeSettableACLs(key, meta.get_acl(), who, opMask));\n    }"
        }
    },
    {
        "filename": "STORM-3124.json",
        "creation_time": "2018-06-27T13:28:01.000+0000",
        "bug_report": {
            "Title": "Failures talking to Pacemaker",
            "Description": "The application is experiencing sporadic failures when attempting to communicate with the Pacemaker service, which is critical for managing the state of the Storm cluster. The logs indicate repeated errors related to timeouts and null responses from the PacemakerClient, leading to cascading failures in the Nimbus service. The stack trace reveals that the Nimbus service is unable to launch topologies due to these connection issues, resulting in a RuntimeException and an IllegalStateException.",
            "StackTrace": [
                "2018-06-25 20:21:05.220 o.a.s.p.PacemakerClient timer [ERROR] Not getting response or getting null response. Making 7 more attempts.",
                "2018-06-25 20:21:06.220 o.a.s.p.PacemakerClient timer [ERROR] error attempting to write to a channel Timed out waiting for channel ready..",
                "2018-06-25 20:21:13.224 o.a.s.p.PacemakerClientPool timer [WARN] Failed to connect to the pacemaker server openqe74blue-n2.blue.ygrid.yahoo.com",
                "java.lang.RuntimeException: java.lang.RuntimeException: org.apache.storm.pacemaker.PacemakerConnectionException: Failed to connect to any Pacemaker.",
                "Caused by: org.apache.storm.pacemaker.PacemakerConnectionException: Failed to connect to any Pacemaker.",
                "java.lang.IllegalStateException: instance must be started before calling this method"
            ],
            "RootCause": "The primary issue is a failure to connect to the Pacemaker service, which is essential for managing the state of the Storm cluster. This leads to a PacemakerConnectionException and subsequent cascading failures in the Nimbus service, which attempts to perform operations on an uninitialized instance.",
            "StepsToReproduce": [
                "Attempt to launch a topology in the Storm application.",
                "Monitor the logs for connection attempts to the Pacemaker service.",
                "Observe the repeated timeout and null response errors."
            ],
            "ExpectedBehavior": "The application should successfully connect to the Pacemaker service, allowing the Nimbus service to manage the state of the Storm cluster and launch topologies without errors.",
            "ObservedBehavior": "The application fails to connect to the Pacemaker service, resulting in repeated timeout errors and cascading failures in the Nimbus service, preventing the launch of topologies.",
            "Suggestions": "Investigate network connectivity to the Pacemaker service, check the configuration settings for the Pacemaker client, and ensure that the Pacemaker service is running properly. Additionally, review the initialization sequence of the Nimbus service to ensure it is started correctly before making calls.",
            "problem_location": {
                "files": [
                    "PacemakerClient.java",
                    "PaceMakerStateStorage.java",
                    "Nimbus.java"
                ],
                "classes": [
                    "org.apache.storm.pacemaker.PacemakerClient",
                    "org.apache.storm.cluster.PaceMakerStateStorage",
                    "org.apache.storm.daemon.nimbus.Nimbus"
                ],
                "methods": [
                    "PacemakerClient.sendAll",
                    "PaceMakerStateStorage.get_worker_hb_children",
                    "Nimbus.submitTopologyWithOpts"
                ]
            },
            "possible_fix": "Ensure that the Pacemaker service is reachable and properly configured. Modify the connection handling in the PacemakerClient to include more robust error handling and retries. Additionally, verify that the Nimbus service is fully initialized before attempting to connect to the Pacemaker."
        },
        "possible_fix_code": {
            "get_worker_hb_children": "    public List<String> get_worker_hb_children(String path, boolean watch) {\n        int retry = maxRetries;\n        while (true) {\n            try {\n                HashSet<String> retSet = new HashSet<>();\n\n                HBMessage message = new HBMessage(HBServerMessageType.GET_ALL_NODES_FOR_PATH, HBMessageData.path(path));\n                List<HBMessage> responses = pacemakerClientPool.sendAll(message);\n                for (HBMessage response : responses) {\n                    if (response.get_type() != HBServerMessageType.GET_ALL_NODES_FOR_PATH_RESPONSE) {\n                        LOG.error(\"get_worker_hb_children: Invalid Response Type\");\n                        continue;\n                    }\n                    if (response.get_data().get_nodes().get_pulseIds() != null) {\n                        retSet.addAll(response.get_data().get_nodes().get_pulseIds());\n                    }\n                }\n\n                LOG.debug(\"Successful get_worker_hb_children\");\n                return new ArrayList<>(retSet);\n            } catch (PacemakerConnectionException e) {\n                if (retry <= 0) {\n                    throw new RuntimeException(e);\n                }\n                retry--;\n                LOG.error(\"{} Failed to get_worker_hb_children. Will make {} more attempts.\", e.getMessage(), retry);\n                // Adding a small sleep before retrying to avoid hammering the server\n                try {\n                    Thread.sleep(1000);\n                } catch (InterruptedException ie) {\n                    Thread.currentThread().interrupt();\n                    throw new RuntimeException(ie);\n                }\n            } catch (InterruptedException e) {\n                LOG.debug(\"get_worker_hb_children got interrupted: {}\", e);\n                throw new RuntimeException(e);\n            }\n        }\n    }"
        }
    },
    {
        "filename": "STORM-2095.json",
        "creation_time": "2016-09-14T16:00:30.000+0000",
        "bug_report": {
            "Title": "Nimbus dies and never recovers due to java.nio.file.DirectoryNotEmptyException",
            "Description": "The issue arises when the Apache Storm Nimbus service attempts to delete a blob during its initialization process after being restarted while a blob is being created. The deletion fails due to a DirectoryNotEmptyException, which prevents Nimbus from starting successfully. This occurs specifically in the LocalFsBlobStore.deleteBlob method, which calls the FileBlobStoreImpl.delete method that fails when the target directory is not empty.",
            "StackTrace": [
                "2016-09-14 15:07:48.518 o.a.s.zookeeper [INFO] Queued up for leader lock.",
                "2016-09-14 15:07:48.576 o.a.s.zookeeper [INFO] xxx gained leadership",
                "2016-09-14 15:07:48.581 o.a.s.d.nimbus [ERROR] Error on initialization of server service-handler",
                "java.lang.RuntimeException: java.nio.file.DirectoryNotEmptyException: /opt/storm/storm-local/blobs/955/some_big_file",
                "\tat org.apache.storm.blobstore.LocalFsBlobStore.deleteBlob(LocalFsBlobStore.java:229)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "\tat java.lang.reflect.Method.invoke(Method.java:497)",
                "\tat clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)",
                "\tat clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)",
                "\tat org.apache.storm.daemon.nimbus$setup_blobstore.invoke(nimbus.clj:1196)",
                "\tat org.apache.storm.daemon.nimbus$fn__7064$exec_fn__2461__auto____7065.invoke(nimbus.clj:1416)",
                "\tat clojure.lang.AFn.applyToHelper(AFn.java:156)",
                "\tat clojure.lang.AFn.applyTo(AFn.java:144)",
                "\tat clojure.core$apply.invoke(core.clj:630)",
                "\tat org.apache.storm.daemon.nimbus$fn__7064$service_handler__7308.doInvoke(nimbus.clj:1358)",
                "\tat clojure.lang.RestFn.invoke(RestFn.java:421)",
                "\tat org.apache.storm.daemon.nimbus$launch_server_BANG_.invoke(nimbus.clj:2206)",
                "\tat org.apache.storm.daemon.nimbus$_launch.invoke(nimbus.clj:2239)",
                "\tat org.apache.storm.daemon.nimbus$_main.invoke(nimbus.clj:2262)",
                "\tat clojure.lang.AFn.applyToHelper(AFn.java:152)",
                "\tat clojure.lang.AFn.applyTo(AFn.java:144)",
                "\tat org.apache.storm.daemon.nimbus.main(Unknown Source)",
                "Caused by: java.nio.file.DirectoryNotEmptyException: /opt/storm/storm-local/blobs/955/some_big_file",
                "\tat sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)",
                "\tat sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)",
                "\tat java.nio.file.Files.deleteIfExists(Files.java:1165)",
                "\tat org.apache.storm.blobstore.FileBlobStoreImpl.delete(FileBlobStoreImpl.java:239)",
                "\tat org.apache.storm.blobstore.FileBlobStoreImpl.deleteKey(FileBlobStoreImpl.java:178)",
                "\tat org.apache.storm.blobstore.LocalFsBlobStore.deleteBlob(LocalFsBlobStore.java:226)",
                "\t... 19 more"
            ],
            "RootCause": "The root cause of the issue is that the Nimbus service is attempting to delete a blob from a directory that is not empty, leading to a DirectoryNotEmptyException. This occurs in the deleteBlob method of LocalFsBlobStore, which calls the delete method in FileBlobStoreImpl that fails when it encounters a non-empty directory.",
            "StepsToReproduce": [
                "1) Create a blobstore key for a large file (1 or 2 GB). Size of the file does not matter if Nimbus can be killed while the blob is being created.",
                "2) While the blob is being created, restart Nimbus.",
                "3) Observe that Nimbus fails to start due to DirectoryNotEmptyException."
            ],
            "ExpectedBehavior": "The partial blobstore key should be deleted cleanly, allowing Nimbus to start without issues.",
            "ObservedBehavior": "Nimbus fails to start and continuously dies due to a DirectoryNotEmptyException when trying to delete the blob.",
            "Suggestions": "To resolve this issue, ensure that the directory is empty before attempting to delete it, or implement a recursive deletion method that can handle non-empty directories.",
            "problem_location": {
                "files": [
                    "LocalFsBlobStore.java",
                    "FileBlobStoreImpl.java"
                ],
                "classes": [
                    "org.apache.storm.blobstore.LocalFsBlobStore",
                    "org.apache.storm.blobstore.FileBlobStoreImpl"
                ],
                "methods": [
                    "LocalFsBlobStore.deleteBlob",
                    "FileBlobStoreImpl.delete"
                ]
            },
            "possible_fix": "Implement a check in the deleteBlob method to ensure the directory is empty before attempting deletion, or modify the delete method to handle non-empty directories recursively."
        },
        "possible_fix_code": {
            "LocalFsBlobStore.deleteBlob": "    public void deleteBlob(String key, Subject who) throws AuthorizationException, KeyNotFoundException {\n        validateKey(key);\n        checkForBlobOrDownload(key);\n        SettableBlobMeta meta = getStoredBlobMeta(key);\n        _aclHandler.hasPermissions(meta.get_acl(), WRITE, who, key);\n        try {\n            // Ensure the directory is empty before attempting to delete\n            File blobDir = new File(DATA_PREFIX + key);\n            if (blobDir.isDirectory() && blobDir.list().length > 0) {\n                // Optionally, implement a recursive delete method here if needed\n                throw new IOException(\"Directory not empty: \" + blobDir.getPath());\n            }\n            fbs.deleteKey(DATA_PREFIX + key);\n            fbs.deleteKey(META_PREFIX + key);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n    }"
        }
    },
    {
        "filename": "STORM-2847.json",
        "creation_time": "2017-12-07T16:51:01.000+0000",
        "bug_report": {
            "Title": "Exception thrown after rebalance IllegalArgumentException",
            "Description": "After a rebalance, the storm-kafka-client spout attempts to check the current position of partitions that are no longer assigned to the current spout instance. This issue arises in a topology with multiple spout instances, leading to an IllegalArgumentException when the Kafka consumer tries to access the position of a partition it is not assigned to.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: You can only check the position for partitions assigned to this consumer.",
                "at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1262)",
                "at org.apache.storm.kafka.spout.KafkaSpout.commitOffsetsForAckedTuples(KafkaSpout.java:473)"
            ],
            "RootCause": "The root cause of the issue is that the Kafka consumer is attempting to call the position method for a TopicPartition that it is not assigned to. This typically occurs due to misconfiguration or timing issues in partition assignment, especially after a rebalance.",
            "StepsToReproduce": [
                "1. Set up a Kafka topology with multiple spout instances.",
                "2. Trigger a rebalance in the topology.",
                "3. Ensure that the spout instances attempt to commit offsets for acknowledged tuples."
            ],
            "ExpectedBehavior": "The Kafka consumer should only check the position for partitions that are currently assigned to it, and no exceptions should be thrown during the offset commit process.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating that the consumer is trying to check the position for partitions that it is not assigned to.",
            "Suggestions": "Review the partition assignment logic to ensure that the Kafka consumer is properly assigned partitions before attempting to check their positions. Consider adding checks to verify the assignment before calling the position method.",
            "problem_location": {
                "files": [
                    "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java"
                ],
                "classes": [
                    "org.apache.storm.kafka.spout.KafkaSpout"
                ],
                "methods": [
                    "KafkaSpout.commitOffsetsForAckedTuples"
                ]
            },
            "possible_fix": "Ensure that the Kafka consumer's partition assignment is correctly handled before calling kafkaConsumer.position(tp). This may involve adding a check to confirm that the partition is assigned to the consumer."
        },
        "possible_fix_code": {
            "KafkaSpout.commitOffsetsForAckedTuples": "    private void commitOffsetsForAckedTuples() {\n        // Find offsets that are ready to be committed for every topic partition\n        final Map<TopicPartition, OffsetAndMetadata> nextCommitOffsets = new HashMap<>();\n        for (Map.Entry<TopicPartition, OffsetManager> tpOffset : offsetManagers.entrySet()) {\n            final OffsetAndMetadata nextCommitOffset = tpOffset.getValue().findNextCommitOffset();\n            if (nextCommitOffset != null) {\n                nextCommitOffsets.put(tpOffset.getKey(), nextCommitOffset);\n            }\n        }\n\n        // Commit offsets that are ready to be committed for every topic partition\n        if (!nextCommitOffsets.isEmpty()) {\n            kafkaConsumer.commitSync(nextCommitOffsets);\n            LOG.debug(\"Offsets successfully committed to Kafka [{}]\", nextCommitOffsets);\n            for (Map.Entry<TopicPartition, OffsetAndMetadata> tpOffset : nextCommitOffsets.entrySet()) {\n                final TopicPartition tp = tpOffset.getKey();\n                // Check if the partition is assigned to this consumer before calling position\n                if (kafkaConsumer.assignment().contains(tp)) {\n                    long position = kafkaConsumer.position(tp);\n                    long committedOffset = tpOffset.getValue().offset();\n                    if (position < committedOffset) {\n                        LOG.debug(\"Consumer fell behind committed offset. Catching up. Position was [{}], skipping to [{}]\", position, committedOffset);\n                        kafkaConsumer.seek(tp, committedOffset);\n                        waitingToEmit = null;\n                    }\n                } else {\n                    LOG.warn(\"Skipping position check for unassigned partition [{}]\", tp);\n                }\n                final OffsetManager offsetManager = offsetManagers.get(tp);\n                offsetManager.commit(tpOffset.getValue());\n                LOG.debug(\"[{}] uncommitted offsets for partition [{}] after commit\", offsetManager.getNumUncommittedOffsets(), tp);\n            }\n        } else {\n            LOG.trace(\"No offsets to commit. {}\", this);\n        }\n    }"
        }
    },
    {
        "filename": "STORM-1114.json",
        "creation_time": "2015-10-15T15:41:36.000+0000",
        "bug_report": {
            "Title": "Race Condition in Trident Zookeeper zk-node Create/Delete",
            "Description": "In a production environment for certain Trident topologies, a race condition has been identified where multiple workers attempt to create a ZooKeeper node that already exists or delete a node that has already been removed. This leads to worker processes crashing due to unhandled exceptions. The issue arises from the `createNode` and `delete` methods in the `TransactionalState` class, which do not adequately check for the existence of nodes before performing operations.",
            "StackTrace": [
                "Caused by: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /ignoreStoredMetadata",
                "at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:119) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at storm.trident.topology.state.TransactionalState.createNode(TransactionalState.java:100) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "Caused by: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /rainbowHdfsPath",
                "at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at storm.trident.topology.state.TransactionalState.delete(TransactionalState.java:126) ~[storm-core-0.10.1.y.jar:0.10.1.y]"
            ],
            "RootCause": "The root cause of the issue is a race condition in the `TransactionalState` class where the `createNode` method does not check if a node already exists before attempting to create it, leading to a `NodeExistsException`. Similarly, the `delete` method does not verify if a node exists before trying to delete it, resulting in a `NoNodeException`.",
            "StepsToReproduce": [
                "Deploy a Trident topology that utilizes the `TransactionalState` class.",
                "Simultaneously trigger multiple workers to create a ZooKeeper node at the same path.",
                "Attempt to delete the same node from different workers."
            ],
            "ExpectedBehavior": "The system should check for the existence of a ZooKeeper node before attempting to create or delete it, preventing exceptions and ensuring that worker processes do not crash.",
            "ObservedBehavior": "Workers crash with exceptions indicating that they are trying to create a node that already exists or delete a node that does not exist.",
            "Suggestions": "Implement checks in the `createNode` method to verify if a node exists before creating it. Similarly, modify the `delete` method to check for the existence of a node before attempting to delete it. This will prevent the race condition and associated exceptions.",
            "problem_location": {
                "files": [
                    "TransactionalState.java"
                ],
                "classes": [
                    "storm.trident.topology.state.TransactionalState"
                ],
                "methods": [
                    "createNode",
                    "delete"
                ]
            },
            "possible_fix": "In the `createNode` method, add a check for the existence of the node using `curator.checkExists().forPath(path)` before attempting to create it. In the `delete` method, check if the node exists before calling `_curator.delete().forPath(path)`."
        },
        "possible_fix_code": {
            "createNode": "    protected static void createNode(CuratorFramework curator, String path,\n            byte[] data, List<ACL> acls, CreateMode mode) throws Exception {\n        ProtectACLCreateModePathAndBytesable<String> builder =\n            curator.create().creatingParentsIfNeeded();\n\n        // Check if the node already exists before creating it\n        if (curator.checkExists().forPath(path) != null) {\n            throw new KeeperException.NodeExistsException(\"Node already exists: \" + path);\n        }\n\n        if (acls == null) {\n            if (mode == null ) {\n                TransactionalState.forPath(builder, path, data);\n            } else {\n                TransactionalState.forPath(builder.withMode(mode), path, data);\n            }\n            return;\n        }\n\n        TransactionalState.forPath(builder.withACL(acls), path, data);\n    }",
            "delete": "    public void delete(String path) {\n        path = \"/\" + path;\n        // Check if the node exists before attempting to delete it\n        try {\n            if (_curator.checkExists().forPath(path) == null) {\n                throw new KeeperException.NoNodeException(\"Node does not exist: \" + path);\n            }\n            _curator.delete().forPath(path);\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }"
        }
    },
    {
        "filename": "STORM-2811.json",
        "creation_time": "2017-11-12T08:37:10.000+0000",
        "bug_report": {
            "Title": "Nimbus may throw NPE if the same topology is killed multiple times, and the integration test kills the same topology multiple times",
            "Description": "The issue arises when the Nimbus component of Apache Storm attempts to kill a topology that has already been killed multiple times. This leads to a `NullPointerException` (NPE) in the `IStormClusterState.getTopoId` method. The stack trace indicates that the NPE occurs when trying to access the name of a `StormBase` object that is expected to be returned by the `stormBase` method but is null. This situation can occur if the `stormBase` method does not handle certain inputs correctly, resulting in a null return value.",
            "StackTrace": [
                "2017-11-12 08:45:50.353 o.a.s.d.n.Nimbus pool-14-thread-47 [WARN] Kill topology exception. (topology name='SlidingWindowTest-window20-slide10')",
                "java.lang.NullPointerException: null",
                "at org.apache.storm.cluster.IStormClusterState.getTopoId(IStormClusterState.java:171) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.nimbus.Nimbus.tryReadTopoConfFromName(Nimbus.java:1970) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.nimbus.Nimbus.killTopologyWithOpts(Nimbus.java:2760) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.generated.Nimbus$Processor$killTopologyWithOpts.getResult(Nimbus.java:3226) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.generated.Nimbus$Processor$killTopologyWithOpts.getResult(Nimbus.java:3210) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.10.0.jar:0.10.0]",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.10.0.jar:0.10.0]",
                "at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:167) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) ~[libthrift-0.10.0.jar:0.10.0]",
                "at org.apache.thrift.server.Invocation.run(Invocation.java:18) ~[libthrift-0.10.0.jar:0.10.0]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_144]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_144]",
                "at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]"
            ],
            "RootCause": "The `NullPointerException` occurs in the `IStormClusterState.getTopoId` method when it attempts to call `get_name()` on a null object returned by the `stormBase` method. This indicates that `stormBase` may not be handling its inputs correctly, leading to a null return value.",
            "StepsToReproduce": [
                "1. Deploy a topology using Apache Storm.",
                "2. Kill the topology using the Nimbus component.",
                "3. Attempt to kill the same topology multiple times in succession."
            ],
            "ExpectedBehavior": "The system should handle multiple kill requests for the same topology gracefully without throwing exceptions.",
            "ObservedBehavior": "The system throws a `NullPointerException` when attempting to kill a topology that has already been killed.",
            "Suggestions": "Review the implementation of the `stormBase` method to ensure it correctly handles inputs and does not return null when it should return a valid `StormBase` object. Implement null checks before accessing methods on potentially null objects.",
            "problem_location": {
                "files": [
                    "IStormClusterState.java",
                    "Nimbus.java"
                ],
                "classes": [
                    "org.apache.storm.cluster.IStormClusterState",
                    "org.apache.storm.daemon.nimbus.Nimbus"
                ],
                "methods": [
                    "IStormClusterState.getTopoId",
                    "IStormClusterState.stormBase",
                    "Nimbus.killTopologyWithOpts"
                ]
            },
            "possible_fix": "Modify the `stormBase` method to ensure it returns a valid `StormBase` object or handles cases where it cannot find a corresponding topology. Additionally, add null checks in the `getTopoId` method to prevent NPEs."
        },
        "possible_fix_code": {
            "IStormClusterState.getTopoId": "    default Optional<String> getTopoId(final String topologyName) {\n        String ret = null;\n        for (String topoId: activeStorms()) {\n            StormBase stormBase = stormBase(topoId, null);\n            if (stormBase != null) {\n                String name = stormBase.get_name();\n                if (topologyName.equals(name)) {\n                    ret = topoId;\n                    break;\n                }\n            } else {\n                // Log or handle the case where stormBase is null if necessary\n            }\n        }\n        return Optional.ofNullable(ret);\n    }"
        }
    },
    {
        "filename": "STORM-2903.json",
        "creation_time": "2018-01-19T17:10:01.000+0000",
        "bug_report": {
            "Title": "Fix possible NullPointerException in AbstractAutoCreds",
            "Description": "A NullPointerException was observed while testing the Hive token mechanism, specifically in the `AbstractAutoCreds` class of the Apache Storm library. The exception occurs when the method `addTokensToUGI` attempts to access or modify an object that has not been initialized. The stack trace indicates that the error originates from line 219 of `AbstractAutoCreds`, which is called by `populateSubject` on line 118. This suggests that there may be uninitialized variables or objects being accessed in these methods.",
            "StackTrace": [
                "Caused by: java.lang.NullPointerException",
                "at org.apache.storm.common.AbstractAutoCreds.addTokensToUGI(AbstractAutoCreds.java:219)",
                "at org.apache.storm.common.AbstractAutoCreds.populateSubject(AbstractAutoCreds.java:118)",
                "at org.apache.storm.security.auth.AuthUtils.populateSubject(AuthUtils.java:228)",
                "... 10 more"
            ],
            "RootCause": "The root cause of the NullPointerException is likely due to an attempt to access or modify an uninitialized object within the `addTokensToUGI` method of the `AbstractAutoCreds` class.",
            "StepsToReproduce": [
                "1. Initialize the Hive token mechanism.",
                "2. Trigger the method that calls `populateSubject` in the `AuthUtils` class.",
                "3. Observe the logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The system should successfully add tokens to the UserGroupInformation (UGI) without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown, halting the process and indicating an error during initialization.",
            "Suggestions": "Review the `addTokensToUGI` and `populateSubject` methods to ensure all objects are properly initialized before being accessed. Consider adding null checks to prevent the exception.",
            "problem_location": {
                "files": [
                    "AbstractAutoCreds.java"
                ],
                "classes": [
                    "org.apache.storm.common.AbstractAutoCreds"
                ],
                "methods": [
                    "AbstractAutoCreds.addTokensToUGI",
                    "AbstractAutoCreds.populateSubject"
                ]
            },
            "possible_fix": "Add null checks for all objects being accessed in the `addTokensToUGI` method. For example:\n\n```java\nif (token != null) {\n    // proceed with adding token\n} else {\n    // handle the null case appropriately\n}\n```"
        },
        "possible_fix_code": {
            "AbstractAutoCreds.addTokensToUGI": "public void addTokensToUGI(UserGroupInformation ugi, Token<?> token) {\n    if (ugi == null) {\n        throw new IllegalArgumentException(\"UserGroupInformation cannot be null\");\n    }\n    if (token != null) {\n        ugi.addToken(token);\n    } else {\n        // Handle the null case appropriately, e.g., log a warning or throw an exception\n        System.err.println(\"Warning: Token is null, cannot add to UGI.\");\n    }\n}"
        }
    },
    {
        "filename": "STORM-3168.json",
        "creation_time": "2018-08-01T19:31:42.000+0000",
        "bug_report": {
            "Title": "AsyncLocalizer cleanup appears to crash",
            "Description": "The application is experiencing repeated errors related to blobstore downloads, specifically within the `AsyncLocalizer` class. Debug logging was enabled, but cleanup messages expected every 30 seconds were absent, indicating a potential crash. Upon restarting the supervisor, logging resumed, suggesting that the cleanup process is not functioning as intended. The error logs indicate a `java.util.concurrent.ExecutionException` caused by a `java.lang.RuntimeException`, which is ultimately due to a `KeyNotFoundException` when attempting to download a blob from the Apache Storm blob store.",
            "StackTrace": [
                "2018-07-30 23:25:35.691 o.a.s.l.AsyncLocalizer AsyncLocalizer Executor - 2 [ERROR] Could not update blob, will retry again later",
                "java.util.concurrent.ExecutionException: java.lang.RuntimeException: Could not download...",
                "at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_131]",
                "at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895) ~[?:1.8.0_131]",
                "at org.apache.storm.localizer.AsyncLocalizer.updateBlobs(AsyncLocalizer.java:303) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_131]",
                "at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [?:1.8.0_131]",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_131]",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [?:1.8.0_131]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]",
                "at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]",
                "Caused by: java.lang.RuntimeException: Could not download...",
                "at org.apache.storm.localizer.AsyncLocalizer.lambda$downloadOrUpdate$69(AsyncLocalizer.java:268) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "Caused by: org.apache.storm.generated.KeyNotFoundException",
                "at org.apache.storm.generated.Nimbus$getBlobMeta_result$getBlobMeta_resultStandardScheme.read(Nimbus.java:25853) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.generated.Nimbus$getBlobMeta_result$getBlobMeta_resultStandardScheme.read(Nimbus.java:25821) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.generated.Nimbus$getBlobMeta_result.read(Nimbus.java:25752) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:88) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.generated.Nimbus$Client.recv_getBlobMeta(Nimbus.java:798) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.generated.Nimbus$Client.getBlobMeta(Nimbus.java:785) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.blobstore.NimbusBlobStore.getBlobMeta(NimbusBlobStore.java:85) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.localizer.LocallyCachedTopologyBlob.getRemoteVersion(LocallyCachedTopologyBlob.java:122) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.localizer.AsyncLocalizer.lambda$downloadOrUpdate$69(AsyncLocalizer.java:252) ~[storm-server-2.0.0.y.jar:2.0.0.y]"
            ],
            "RootCause": "The root cause of the issue is a `KeyNotFoundException` indicating that the application is attempting to download a blob from the blob store, but the specified key does not exist. This leads to a failure in the blob update process.",
            "StepsToReproduce": [
                "Enable debug logging for the AsyncLocalizer.",
                "Observe the logs for cleanup messages every 30 seconds.",
                "Wait for the cleanup process to fail and check for error messages related to blob updates."
            ],
            "ExpectedBehavior": "The cleanup process should run continuously, logging any failures or errors encountered during blob updates every 30 seconds.",
            "ObservedBehavior": "The cleanup process fails to log messages and appears to crash, leading to repeated error messages in the logs regarding blob updates.",
            "Suggestions": "1. Verify that the blob key being used for downloads is correct and that the blob has been uploaded to the blob store. 2. Review the blob store configuration to ensure it is properly set up and accessible. 3. Implement error handling to manage situations where a blob cannot be found, including logging the error and providing fallback mechanisms. 4. Consult Apache Storm documentation for insights on common blob management issues.",
            "problem_location": {
                "files": [
                    "AsyncLocalizer.java",
                    "NimbusBlobStore.java"
                ],
                "classes": [
                    "org.apache.storm.localizer.AsyncLocalizer",
                    "org.apache.storm.blobstore.NimbusBlobStore"
                ],
                "methods": [
                    "AsyncLocalizer.updateBlobs",
                    "NimbusBlobStore.getBlobMeta"
                ]
            },
            "possible_fix": "Ensure that the blob key used in the `AsyncLocalizer` is valid and exists in the blob store. Additionally, consider adding error handling in the `updateBlobs` method to manage `KeyNotFoundException` gracefully."
        },
        "possible_fix_code": {
            "AsyncLocalizer.updateBlobs": "    void updateBlobs() {\n        List<CompletableFuture<?>> futures = new ArrayList<>();\n        futures.add(downloadOrUpdate(topologyBlobs.values()));\n        if (symlinksDisabled) {\n            LOG.warn(\"symlinks are disabled so blobs cannot be downloaded.\");\n        } else {\n            for (ConcurrentMap<String, LocalizedResource> map : userArchives.values()) {\n                futures.add(downloadOrUpdate(map.values()));\n            }\n\n            for (ConcurrentMap<String, LocalizedResource> map : userFiles.values()) {\n                futures.add(downloadOrUpdate(map.values()));\n            }\n        }\n        for (CompletableFuture<?> f : futures) {\n            try {\n                f.get();\n            } catch (Exception e) {\n                if (Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {\n                    LOG.error(\"Network error while updating blobs, will retry again later\", e);\n                } else if (Utils.exceptionCauseIsInstanceOf(NimbusLeaderNotFoundException.class, e)) {\n                    LOG.error(\"Nimbus unavailable to update blobs, will retry again later\", e);\n                } else if (Utils.exceptionCauseIsInstanceOf(KeyNotFoundException.class, e)) {\n                    LOG.error(\"Blob key not found, please verify the blob key and ensure it exists in the blob store.\", e);\n                } else {\n                    LOG.error(\"Could not update blob, will retry again later\", e);\n                }\n            }\n        }\n    }"
        }
    },
    {
        "filename": "STORM-2986.json",
        "creation_time": "2018-03-05T21:41:24.000+0000",
        "bug_report": {
            "Title": "NPE from LogCleaner",
            "Description": "A NullPointerException (NPE) occurs in the LogCleaner class of the Apache Storm framework when attempting to clean up old logs. This issue arises when the LogCleaner tries to access directories for cleanup, but the required 'workers-artifacts' directory does not exist prior to submitting any topologies. The log output indicates that the exception is thrown during the execution of the selectDirsForCleanup method, which is likely attempting to operate on a null object or an uninitialized variable.",
            "StackTrace": [
                "java.lang.NullPointerException: null",
                "at java.util.Arrays.stream(Arrays.java:5004) ~[?:1.8.0_131]",
                "at org.apache.storm.daemon.logviewer.utils.LogCleaner.selectDirsForCleanup(LogCleaner.java:217) ~[storm-webapp-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.logviewer.utils.LogCleaner.run(LogCleaner.java:135) ~[storm-webapp-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.StormTimer$1.run(StormTimer.java:207) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:81) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]"
            ],
            "RootCause": "The root cause of the NullPointerException is that the LogCleaner is attempting to access directories for cleanup without the necessary 'workers-artifacts' directory being created beforehand. This results in the selectDirsForCleanup method trying to operate on a null reference.",
            "StepsToReproduce": [
                "Set the configuration property logviewer.cleanup.interval.secs to 10.",
                "Start the LogCleaner thread.",
                "Observe the log output for the NullPointerException."
            ],
            "ExpectedBehavior": "The LogCleaner should successfully clean up old logs without throwing any exceptions, even if the 'workers-artifacts' directory does not exist.",
            "ObservedBehavior": "A NullPointerException is thrown when the LogCleaner attempts to clean up old logs due to the absence of the 'workers-artifacts' directory.",
            "Suggestions": "To prevent this issue, ensure that the 'workers-artifacts' directory is created before the LogCleaner attempts to access it. Alternatively, implement a check in the LogCleaner to handle the absence of this directory gracefully.",
            "problem_location": {
                "files": [
                    "LogCleaner.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.logviewer.utils.LogCleaner"
                ],
                "methods": [
                    "selectDirsForCleanup",
                    "run"
                ]
            },
            "possible_fix": "Add a check in the selectDirsForCleanup method to verify the existence of the 'workers-artifacts' directory before attempting to access it. If it does not exist, log a warning and skip the cleanup process."
        },
        "possible_fix_code": {
            "selectDirsForCleanup": "public List<String> selectDirsForCleanup() {\n    List<String> directoriesToClean = new ArrayList<>();\n    String workersArtifactsDir = getWorkersArtifactsDir();\n    // Check if the 'workers-artifacts' directory exists before proceeding\n    if (workersArtifactsDir == null || !checkDirExists(workersArtifactsDir)) {\n        LOG.warn(\"'workers-artifacts' directory does not exist. Skipping cleanup.\");\n        return directoriesToClean; // Return an empty list if the directory does not exist\n    }\n    // Proceed with the cleanup logic if the directory exists\n    // ... (existing logic to populate directoriesToClean) ...\n    return directoriesToClean;\n}"
        }
    },
    {
        "filename": "STORM-2197.json",
        "creation_time": "2016-11-10T03:57:30.000+0000",
        "bug_report": {
            "Title": "NimbusClient connections leak due to leakage in ThriftClient.",
            "Description": "The Nimbus client is experiencing connection leaks when errors occur during the connection process to Nimbus. Specifically, the TSocket instance created in the ThriftClient is not being closed properly in the event of an error. The error log indicates a failure related to GSS (Generic Security Services) during the initiation of a connection using Thrift with Kerberos authentication, which may stem from misconfigurations or network issues.",
            "StackTrace": [
                "org.apache.thrift7.transport.TTransportException: Peer indicated failure: GSS initiate failed",
                "at org.apache.thrift7.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:199)",
                "at org.apache.thrift7.transport.TSaslTransport.open(TSaslTransport.java:277)",
                "at org.apache.thrift7.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)",
                "at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:145)",
                "at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:141)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:140)",
                "at backtype.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:48)",
                "at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:103)",
                "at backtype.storm.security.auth.ThriftClient.<init>(ThriftClient.java:72)",
                "at backtype.storm.utils.NimbusClient.<init>(NimbusClient.java:106)",
                "at backtype.storm.utils.NimbusClient.getConfiguredClientAs(NimbusClient.java:82)",
                "at backtype.storm.ui.core$nimbus_summary.invoke(core.clj:584)",
                "at backtype.storm.ui.core$fn__10334.invoke(core.clj:1009)",
                "at compojure.core$make_route$fn__7476.invoke(core.clj:93)",
                "at compojure.core$if_route$fn__7464.invoke(core.clj:39)",
                "at compojure.core$if_method$fn__7457.invoke(core.clj:24)",
                "at compojure.core$routing$fn__7482.invoke(core.clj:106)",
                "at clojure.core$some.invoke(core.clj:2515)"
            ],
            "RootCause": "The root cause of the issue is a failure in the SASL negotiation process during the connection attempt, specifically indicated by the error 'GSS initiate failed'. This suggests potential misconfigurations in the Kerberos setup, such as incorrect keytab files or principal names, which prevent successful authentication and lead to unclosed connections.",
            "StepsToReproduce": [
                "Attempt to connect to Nimbus using the NimbusClient.",
                "Ensure that the Kerberos configuration is incorrect (e.g., wrong keytab or principal).",
                "Observe the logs for the error message indicating GSS initiation failure."
            ],
            "ExpectedBehavior": "The Nimbus client should successfully establish a connection to Nimbus and close any associated resources, including TSocket instances, even in the event of an error.",
            "ObservedBehavior": "Connections to Nimbus remain open and unclosed when errors occur during the connection process, leading to resource leaks.",
            "Suggestions": "Review and correct the Kerberos configuration, including keytab files and principal names. Additionally, ensure that the ThriftClient properly handles exceptions by closing the TSocket in case of connection errors.",
            "problem_location": {
                "files": [
                    "ThriftClient.java",
                    "KerberosSaslTransportPlugin.java"
                ],
                "classes": [
                    "backtype.storm.security.auth.ThriftClient",
                    "backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin"
                ],
                "methods": [
                    "ThriftClient.reconnect",
                    "KerberosSaslTransportPlugin.connect"
                ]
            },
            "possible_fix": "In the connect method of KerberosSaslTransportPlugin, ensure that the TSocket is closed in the catch block when an exception occurs during the SASL transport opening process."
        },
        "possible_fix_code": {
            "connect": "public TTransport connect(TTransport transport, String serverHost, String asUser) throws TTransportException, IOException {\n        //create an authentication callback handler\n        ClientCallbackHandler client_callback_handler = new ClientCallbackHandler(login_conf);\n        \n        //login our user\n        Login login = null;\n        TTransport saslTransport = null;\n        try { \n            //specify a configuration object to be used\n            Configuration.setConfiguration(login_conf); \n            //now login\n            login  = new Login(AuthUtils.LOGIN_CONTEXT_CLIENT, client_callback_handler);\n        } catch (LoginException ex) {\n            LOG.error(\"Server failed to login in principal:\" + ex, ex);\n            throw new RuntimeException(ex);\n        }\n\n        final Subject subject = login.getSubject();\n        if (subject.getPrivateCredentials(KerberosTicket.class).isEmpty()) { //error\n            throw new RuntimeException(\"Fail to verify user principal with section \\\"\"\n                        +AuthUtils.LOGIN_CONTEXT_CLIENT+\"\\\" in login configuration file \" + login_conf);\n        }\n\n        final String principal = StringUtils.isBlank(asUser) ? getPrincipal(subject) : asUser;\n        String serviceName = AuthUtils.get(login_conf, AuthUtils.LOGIN_CONTEXT_CLIENT, \"serviceName\");\n        if (serviceName == null) {\n            serviceName = AuthUtils.SERVICE; \n        }\n        Map<String, String> props = new TreeMap<String,String>();\n        props.put(Sasl.QOP, \"auth\");\n        props.put(Sasl.SERVER_AUTH, \"false\");\n\n        LOG.debug(\"SASL GSSAPI client transport is being established\");\n        saslTransport = new TSaslClientTransport(KERBEROS, \n                principal, \n                serviceName, \n                serverHost,\n                props,\n                null, \n                transport);\n\n        //open Sasl transport with the login credential\n        try {\n            Subject.doAs(subject,\n                    new PrivilegedExceptionAction<Void>() {\n                public Void run() {\n                    try {\n                        LOG.debug(\"do as:\" + principal);\n                        saslTransport.open();\n                    }\n                    catch (Exception e) {\n                        LOG.error(\"Client failed to open SaslClientTransport to interact with a server during session initiation: \" + e, e);\n                    }\n                    return null;\n                }\n            });\n        } catch (PrivilegedActionException e) {\n            throw new RuntimeException(e);\n        } finally {\n            if (saslTransport != null) {\n                try {\n                    saslTransport.close();\n                } catch (IOException e) {\n                    LOG.error(\"Failed to close SaslClientTransport: \" + e.getMessage(), e);\n                }\n            }\n        }\n\n        return saslTransport;\n    }"
        }
    },
    {
        "filename": "STORM-1596.json",
        "creation_time": "2016-03-02T23:42:56.000+0000",
        "bug_report": {
            "Title": "Multiple Subject sharing Kerberos TGT - causes services to fail",
            "Description": "The issue arises when multiple threads access the same Kerberos Subject, leading to a situation where a Service Ticket in use by one thread can be invalidated by another thread. This concurrency issue manifests during high parallelism scenarios, such as when running BasicDRPCTopology in a secure cluster. The error logs indicate a failure in SASL negotiation due to invalid Kerberos tickets, specifically highlighting a 'BAD TGS SERVER NAME' error, which suggests a mismatch between the Service Principal Name (SPN) and the expected service.",
            "StackTrace": [
                "2016-01-20 15:52:26.904 o.a.t.t.TSaslTransport [ERROR] SASL negotiation failure",
                "javax.security.sasl.SaslException: GSS initiate failed",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211) ~[?:1.8.0_40]",
                "at org.apache.thrift7.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.thrift7.transport.TSaslTransport.open(TSaslTransport.java:271) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.thrift7.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:195) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:191) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_40]",
                "at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_40]",
                "at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:190) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:54) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:109) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.drpc.DRPCInvocationsClient.reconnectClient(DRPCInvocationsClient.java:57) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.drpc.ReturnResults.reconnectClient(ReturnResults.java:113) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.drpc.ReturnResults.execute(ReturnResults.java:103) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.daemon.executor$fn__6377$tuple_action_fn__6379.invoke(executor.clj:689) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.daemon.executor$mk_task_receiver$fn__6301.invoke(executor.clj:448) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.disruptor$clojure_handler$reify__6018.onEvent(disruptor.clj:40) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:437) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:416) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.daemon.executor$fn__6377$fn__6390$fn__6441.invoke(executor.clj:801) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.util$async_loop$fn__742.invoke(util.clj:482) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_40]"
            ],
            "RootCause": "The root cause of the issue is a concurrency problem where multiple threads are accessing the same Kerberos Subject, leading to invalidation of Service Tickets. The specific error 'BAD TGS SERVER NAME' indicates that the Service Principal Name (SPN) used in the Kerberos ticket does not match the expected value for the service being accessed.",
            "StepsToReproduce": [
                "Run BasicDRPCTopology with high parallelism in a secure cluster.",
                "Ensure multiple threads are accessing the same Kerberos Subject."
            ],
            "ExpectedBehavior": "The system should allow multiple threads to access the same Kerberos Subject without invalidating Service Tickets, enabling successful SASL negotiation and Kerberos authentication.",
            "ObservedBehavior": "SASL negotiation fails with a 'BAD TGS SERVER NAME' error, causing services to fail due to invalid Kerberos tickets.",
            "Suggestions": "To resolve this issue, ensure that the Service Principal Name (SPN) is correctly configured and matches the expected value for the service. Additionally, verify the Kerberos configuration in the krb5.conf file, check DNS resolution, and ensure that the Key Distribution Center (KDC) is functioning properly.",
            "problem_location": {
                "files": [
                    "KerberosSaslTransportPlugin.java",
                    "TSaslTransport.java"
                ],
                "classes": [
                    "backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin",
                    "org.apache.thrift7.transport.TSaslTransport"
                ],
                "methods": [
                    "KerberosSaslTransportPlugin.connect",
                    "TSaslTransport.open"
                ]
            },
            "possible_fix": "Verify and correct the Service Principal Name (SPN) configuration in the Kerberos settings. Ensure that the krb5.conf file is properly set up with the correct realm and KDC settings. Check for DNS issues that may affect SPN resolution."
        },
        "possible_fix_code": {
            "KerberosSaslTransportPlugin.connect": "    public TTransport connect(TTransport transport, String serverHost, String asUser) throws TTransportException, IOException {\n        //create an authentication callback handler\n        ClientCallbackHandler client_callback_handler = new ClientCallbackHandler(login_conf);\n        \n        //login our user\n        Login login = null;\n        try { \n            //specify a configuration object to be used\n            Configuration.setConfiguration(login_conf); \n            //now login\n            login  = new Login(AuthUtils.LOGIN_CONTEXT_CLIENT, client_callback_handler);\n        } catch (LoginException ex) {\n            LOG.error(\"Server failed to login in principal:\" + ex, ex);\n            throw new RuntimeException(ex);\n        }\n\n        final Subject subject = login.getSubject();\n        if (subject.getPrivateCredentials(KerberosTicket.class).isEmpty()) { //error\n            throw new RuntimeException(\"Fail to verify user principal with section \\\"\" + AuthUtils.LOGIN_CONTEXT_CLIENT + \"\\\" in login configuration file \" + login_conf);\n        }\n\n        final String principal = StringUtils.isBlank(asUser) ? getPrincipal(subject) : asUser;\n        String serviceName = AuthUtils.get(login_conf, AuthUtils.LOGIN_CONTEXT_CLIENT, \"serviceName\");\n        if (serviceName == null) {\n            serviceName = AuthUtils.SERVICE; \n        }\n        Map<String, String> props = new TreeMap<String,String>();\n        props.put(Sasl.QOP, \"auth\");\n        props.put(Sasl.SERVER_AUTH, \"false\");\n\n        LOG.debug(\"SASL GSSAPI client transport is being established\");\n        final TTransport sasalTransport = new TSaslClientTransport(KERBEROS, \n                principal, \n                serviceName, \n                serverHost,\n                props,\n                null, \n                transport);\n\n        //open Sasl transport with the login credential\n        try {\n            Subject.doAs(subject,\n                    new PrivilegedExceptionAction<Void>() {\n                public Void run() {\n                    try {\n                        LOG.debug(\"do as:\" + principal);\n                        sasalTransport.open();\n                    }\n                    catch (Exception e) {\n                        LOG.error(\"Client failed to open SaslClientTransport to interact with a server during session initiation: \" + e, e);\n                        throw new RuntimeException(e);\n                    }\n                    return null;\n                }\n            });\n        } catch (PrivilegedActionException e) {\n            throw new RuntimeException(e);\n        }\n\n        return sasalTransport;\n    }"
        }
    },
    {
        "filename": "STORM-2142.json",
        "creation_time": "2016-10-10T04:42:01.000+0000",
        "bug_report": {
            "Title": "ReportErrorAndDie runs suicide function only when InterruptedException or InterruptedIOException is thrown",
            "Description": "The issue arises when the EvaluationFilter or EvaluationFunction throws an exception, causing the async loop for the executor to terminate while other processes continue. The stack trace indicates a RuntimeException caused by an InvocationTargetException, specifically due to an attempt to convert a null value to an integer. This behavior is inconsistent with the expected functionality of the ReportErrorAndDie method, which should only trigger a suicide function for exceptions other than InterruptedException or InterruptedIOException.",
            "StackTrace": [
                "2016-10-08 14:12:29.597 o.a.s.u.Utils Thread-23-b-0-LOGICALFILTER_6-LOGICALPROJECT_7-executor[5 5] [ERROR] Async loop died!",
                "java.lang.RuntimeException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:468) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_66]",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_66]",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_66]",
                "at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_66]",
                "at org.codehaus.janino.ScriptEvaluator.evaluate(ScriptEvaluator.java:982) ~[dep-janino-2.7.6-dcb5bd18-a5dd-4976-a967-0108dcf46df0.jar.1475903522000:2.7.6]",
                "Caused by: java.lang.RuntimeException: Cannot convert null to int",
                "at org.apache.calcite.runtime.SqlFunctions.cannotConvert(SqlFunctions.java:1023) ~[dep-calcite-core-1.9.0-e7846de7-7024-4041-89e4-67dd5edf31e8.jar.1475903521000:1.9.0]",
                "at org.apache.calcite.runtime.SqlFunctions.toInt(SqlFunctions.java:1134) ~[dep-calcite-core-1.9.0-e7846de7-7024-4041-89e4-67dd5edf31e8.jar.1475903521000:1.9.0]",
                "at SC.eval0(Unknown Source) ~[?:?]"
            ],
            "RootCause": "The root cause of the issue is a failure to handle null values during SQL function evaluation, specifically when attempting to convert a null value to an integer. This results in a RuntimeException being thrown, which leads to the unexpected termination of the async loop.",
            "StepsToReproduce": [
                "Trigger an EvaluationFilter or EvaluationFunction that throws an exception.",
                "Observe the behavior of the async loop for the executor."
            ],
            "ExpectedBehavior": "The system should log the exception and continue executing other processes without terminating the async loop, except when InterruptedException or InterruptedIOException is thrown.",
            "ObservedBehavior": "The async loop dies when a RuntimeException occurs, leading to unexpected termination of the executor.",
            "Suggestions": "Implement null checks before attempting to convert values to integers in the SQL function evaluation process. Ensure that default values are provided when null is encountered to prevent RuntimeExceptions.",
            "problem_location": {
                "files": [
                    "SqlFunctions.java",
                    "ScriptEvaluator.java"
                ],
                "classes": [
                    "org.apache.calcite.runtime.SqlFunctions",
                    "org.codehaus.janino.ScriptEvaluator"
                ],
                "methods": [
                    "SqlFunctions.toInt",
                    "SqlFunctions.cannotConvert",
                    "ScriptEvaluator.evaluate"
                ]
            },
            "possible_fix": "Add null checks in the SqlFunctions class methods to handle null values appropriately before conversion attempts. For example, modify the toInt method to return a default value or throw a more descriptive exception if the input is null."
        },
        "possible_fix_code": {
            "org.apache.calcite.runtime.SqlFunctions.toInt": "    public static int toInt(Object value) {\n        if (value == null) {\n            // Return a default value or throw a descriptive exception\n            return 0; // Default value for null\n        }\n        if (value instanceof Number) {\n            return ((Number) value).intValue();\n        } else if (value instanceof String) {\n            try {\n                return Integer.parseInt((String) value);\n            } catch (NumberFormatException e) {\n                throw new RuntimeException(\"Cannot convert String to int: \" + value, e);\n            }\n        }\n        throw new RuntimeException(\"Cannot convert value to int: \" + value);\n    }"
        }
    },
    {
        "filename": "STORM-2400.json",
        "creation_time": "2017-03-08T04:32:34.000+0000",
        "bug_report": {
            "Title": "Intermittent failure in nimbus because of errors from LeaderLatch#getLeader()",
            "Description": "This issue is reported to Curator with CURATOR-358. The method org.apache.curator.framework.recipes.leader.LeaderLatch#getLeader() intermittently throws a KeeperException with code NONODE, indicating that the expected ZooKeeper node does not exist. This can occur if a participant's ephemeral node is removed due to a closed connection or session timeout. The stack trace shows that the error arises when attempting to retrieve the leader node, which may have been deleted before the call completes. The current implementation does not retry on KeeperException with NoNode code, which could lead to this intermittent failure. Enhancements to the retry logic or configuration options for handling specific KeeperException codes may be necessary.",
            "StackTrace": [
                "2016-11-15 06:09:33.954 o.a.s.d.nimbus [ERROR] Error when processing event",
                "org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /storm/leader-lock/_c_97c09eed-5bba-4ac8-a05f-abdc4e8e95cf-latch-0000000002",
                "at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:111)",
                "at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51)",
                "at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:304)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:293)",
                "at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:108)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:290)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:281)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:42)",
                "at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderSelector.participantForPath(LeaderSelector.java:375)",
                "at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderSelector.getLeader(LeaderSelector.java:346)",
                "at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch.getLeader(LeaderLatch.java:454)"
            ],
            "RootCause": "The root cause of the issue is that the LeaderLatch#getLeader() method is attempting to access a ZooKeeper node that has been deleted, resulting in a NoNodeException. This can happen if the ephemeral node for a participant is removed due to session timeout or connection issues.",
            "StepsToReproduce": [
                "1. Start the application that uses LeaderLatch.",
                "2. Ensure that the ZooKeeper session for a participant is unstable or experiences a timeout.",
                "3. Trigger the LeaderLatch#getLeader() method.",
                "4. Observe the intermittent NoNodeException in the logs."
            ],
            "ExpectedBehavior": "The system should be able to handle the absence of the leader node gracefully, either by retrying the operation or by providing a fallback mechanism.",
            "ObservedBehavior": "The system throws a KeeperException with NoNode code when attempting to retrieve the leader node, leading to intermittent failures.",
            "Suggestions": "Implement a retry mechanism for KeeperException with NoNode code in the LeaderLatch#getLeader() method. Additionally, consider adding configuration options to specify which KeeperException codes should trigger a retry.",
            "problem_location": {
                "files": [
                    "LeaderLatch.java"
                ],
                "classes": [
                    "org.apache.curator.framework.recipes.leader.LeaderLatch"
                ],
                "methods": [
                    "getLeader"
                ]
            },
            "possible_fix": "Modify the LeaderLatch#getLeader() method to include a retry loop for KeeperException with NoNode code. Example code snippet:\n\nif (e instanceof KeeperException.NoNodeException) {\n    // Implement retry logic here\n}"
        },
        "possible_fix_code": {
            "getLeader": "public String getLeader() throws Exception {\n    int retryCount = 0;\n    int maxRetries = 3;\n    long retryDelay = 1000; // 1 second delay between retries\n    while (true) {\n        try {\n            // Attempt to get the leader node\n            return leaderSelector.getLeader();\n        } catch (KeeperException.NoNodeException e) {\n            if (retryCount < maxRetries) {\n                retryCount++;\n                Thread.sleep(retryDelay);\n                continue; // Retry getting the leader\n            } else {\n                throw e; // Rethrow after max retries\n            }\n        }\n    }\n}"
        }
    },
    {
        "filename": "STORM-3084.json",
        "creation_time": "2018-05-24T20:45:32.000+0000",
        "bug_report": {
            "Title": "2.x NPE on Nimbus startup",
            "Description": "The Nimbus server in Apache Storm version 2.0.0.y encounters a NullPointerException during startup, specifically while processing events related to supervisor details. The error occurs in the `readAllSupervisorDetails` method, which is invoked during the Nimbus server's initialization process. This leads to a RuntimeException that halts the server, as indicated by the stack trace.",
            "StackTrace": [
                "2018-05-24 09:27:06.012 o.a.s.d.n.Nimbus timer [ERROR] Error while processing event java.lang.RuntimeException: java.lang.NullPointerException at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2685) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "Caused by: java.lang.NullPointerException at org.apache.storm.daemon.nimbus.Nimbus.readAllSupervisorDetails(Nimbus.java:1814) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:1906) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2057) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2003) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2681) ~[storm-server-2.0.0.y.jar:2.0.0.y]"
            ],
            "RootCause": "The root cause of the issue is a NullPointerException occurring in the `readAllSupervisorDetails` method of the Nimbus class. This suggests that there may be an attempt to access or process supervisor details that are not properly initialized or are null.",
            "StepsToReproduce": [
                "Start the Nimbus server using Apache Storm version 2.0.0.y.",
                "Monitor the logs for any errors during the startup process."
            ],
            "ExpectedBehavior": "The Nimbus server should start successfully without encountering any exceptions, and it should be able to read and process supervisor details correctly.",
            "ObservedBehavior": "The Nimbus server fails to start and logs a NullPointerException, leading to a RuntimeException that halts the process.",
            "Suggestions": "Investigate the initialization of supervisor details to ensure they are properly set before being accessed in the `readAllSupervisorDetails` method. Consider adding null checks or initializing necessary components to prevent the NullPointerException.",
            "problem_location": {
                "files": [
                    "Nimbus.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.nimbus.Nimbus"
                ],
                "methods": [
                    "readAllSupervisorDetails",
                    "computeNewSchedulerAssignments",
                    "mkAssignments"
                ]
            },
            "possible_fix": "Add null checks in the `readAllSupervisorDetails` method to handle cases where supervisor details may not be initialized. Ensure that all necessary components are properly set up before invoking this method."
        },
        "possible_fix_code": {
            "readAllSupervisorDetails": "public void readAllSupervisorDetails() {\n    if (supervisorDetails == null) {\n        LOG.error(\"Supervisor details are not initialized.\");\n        return;\n    }\n    // Existing logic to read supervisor details\n    for (SupervisorDetail detail : supervisorDetails) {\n        // Process each supervisor detail\n    }\n}"
        }
    },
    {
        "filename": "STORM-3118.json",
        "creation_time": "2018-06-21T13:46:08.000+0000",
        "bug_report": {
            "Title": "Netty incompatibilities with Pacemaker",
            "Description": "The Nimbus service encounters critical issues when interacting with the Pacemaker component, leading to exceptions that prevent topology submission. The primary error is an EncoderException caused by an IndexOutOfBoundsException, indicating that the system attempts to write more data to a buffer than it can accommodate. Additionally, an IllegalStateException arises when the CuratorFramework instance is not properly initialized before method calls, further complicating the topology submission process.",
            "StackTrace": [
                "2018-06-21 08:55:17.762 o.a.s.p.PacemakerClientHandler client-worker-2 [ERROR] Exception occurred in Pacemaker.",
                "org.apache.storm.shade.io.netty.handler.codec.EncoderException: java.lang.IndexOutOfBoundsException: writerIndex(713) + minWritableBytes(2) exceeds maxCapacity(713): UnpooledHeapByteBuf(ridx: 0, widx: 713, cap: 713/713)",
                "at org.apache.storm.shade.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:106) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:801) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:814) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:794) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:1066) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:305) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.messaging.netty.KerberosSaslClientHandler.channelActive(KerberosSaslClientHandler.java:65) [storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.ChannelInboundHandlerAdapter.channelActive(ChannelInboundHandlerAdapter.java:64) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline$HeadContext.channelActive(DefaultChannelPipeline.java:1422) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.fireChannelActive(DefaultChannelPipeline.java:941) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:311) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:341) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:635) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:582) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:499) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:461) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]",
                "Caused by: java.lang.IndexOutOfBoundsException: writerIndex(713) + minWritableBytes(2) exceeds maxCapacity(713): UnpooledHeapByteBuf(ridx: 0, widx: 713, cap: 713/713)",
                "at org.apache.storm.shade.io.netty.buffer.AbstractByteBuf.ensureWritable0(AbstractByteBuf.java:276) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.buffer.AbstractByteBuf.writeShort(AbstractByteBuf.java:966) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.messaging.netty.SaslMessageToken.write(SaslMessageToken.java:104) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.pacemaker.codec.ThriftEncoder.encodeNettySerializable(ThriftEncoder.java:44) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.pacemaker.codec.ThriftEncoder.encode(ThriftEncoder.java:77) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:88) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "... 26 more",
                "2018-06-21 09:10:46.343 o.a.s.d.n.Nimbus pool-37-thread-250 [WARN] Topology submission exception. (topology name='testStormKafkaNewApi')",
                "java.lang.IllegalStateException: instance must be started before calling this method",
                "at org.apache.storm.shade.org.apache.curator.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:444) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.checkExists(CuratorFrameworkImpl.java:432) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.zookeeper.ClientZookeeper.existsNode(ClientZookeeper.java:144) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.zookeeper.ClientZookeeper.mkdirsImpl(ClientZookeeper.java:288) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.zookeeper.ClientZookeeper.mkdirs(ClientZookeeper.java:70) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.cluster.ZKStateStorage.mkdirs(ZKStateStorage.java:114) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.cluster.PaceMakerStateStorage.mkdirs(PaceMakerStateStorage.java:69) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.cluster.StormClusterStateImpl.setupHeatbeats(StormClusterStateImpl.java:435) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:3009) [storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3508) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3487) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) [storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]",
                "at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]"
            ],
            "RootCause": "The root cause of the issue is twofold: an IndexOutOfBoundsException occurs when the SaslMessageToken's write method attempts to write data to a ByteBuf that does not have sufficient capacity, and an IllegalStateException arises when the CuratorFramework instance is not started before invoking methods that require it to be in a started state.",
            "StepsToReproduce": [
                "Attempt to submit a topology using the Nimbus service with the Pacemaker component enabled.",
                "Monitor the logs for any exceptions related to buffer management or state management."
            ],
            "ExpectedBehavior": "The topology should be submitted successfully without any exceptions, and the Pacemaker component should handle data correctly without exceeding buffer limits.",
            "ObservedBehavior": "The topology submission fails with an EncoderException and an IllegalStateException, preventing successful operation.",
            "Suggestions": "1. Review the logic in the SaslMessageToken's write method to ensure that the buffer is allocated with sufficient capacity for the data being written. 2. Ensure that the CuratorFramework instance is properly initialized and started before any method calls that require it to be in a started state.",
            "problem_location": {
                "files": [
                    "storm-client/src/jvm/org/apache/storm/messaging/netty/SaslMessageToken.java",
                    "storm-client/src/jvm/org/apache/storm/zookeeper/ClientZookeeper.java"
                ],
                "classes": [
                    "org.apache.storm.messaging.netty.SaslMessageToken",
                    "org.apache.storm.zookeeper.ClientZookeeper"
                ],
                "methods": [
                    "SaslMessageToken.write"
                ]
            },
            "possible_fix": "In the SaslMessageToken class, modify the write method to check the buffer's capacity before writing data. Additionally, ensure that the CuratorFramework instance is started by calling the start() method before any operations that require it to be in a started state."
        },
        "possible_fix_code": {
            "SaslMessageToken.write": "    public void write(ByteBuf dest) {\n        int payload_len = 0;\n        if (token != null) {\n            payload_len = token.length;\n        }\n\n        dest.writeShort(IDENTIFIER);\n        dest.writeInt(payload_len);\n\n        // Check if the buffer has enough capacity before writing the token\n        if (payload_len > 0 && dest.writableBytes() >= payload_len) {\n            dest.writeBytes(token);\n        } else if (payload_len > 0) {\n            throw new IndexOutOfBoundsException(\"Not enough capacity in the buffer to write the token.\");\n        }\n    }"
        }
    },
    {
        "filename": "STORM-2158.json",
        "creation_time": "2016-10-20T12:56:58.000+0000",
        "bug_report": {
            "Title": "OutOfMemoryError in Nimbus' SimpleTransportPlugin",
            "Description": "The application encounters a {{java.lang.OutOfMemoryError}} when a malformed Thrift request is sent to Nimbus' {{SimpleTransportPlugin}}. This issue arises due to the server's inability to handle the incoming data, leading to excessive memory consumption. The error is logged in nimbus.log, indicating that the server is exiting due to an uncaught error related to memory allocation. The root cause is linked to the lack of specification for the {{maxReadBufferBytes}} parameter in the {{THsHaServer}} configuration.",
            "StackTrace": [
                "2016-10-20 12:54:42.926 o.a.t.s.THsHaServer [ERROR] run() exiting due to uncaught error",
                "java.lang.OutOfMemoryError: Java heap space",
                "at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) ~[?:1.8.0_92-internal]",
                "at java.nio.ByteBuffer.allocate(ByteBuffer.java:335) ~[?:1.8.0_92-internal]",
                "at org.apache.thrift7.server.AbstractNonblockingServer$FrameBuffer.read(AbstractNonblockingServer.java:371) ~[storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]",
                "at org.apache.thrift7.server.AbstractNonblockingServer$AbstractSelectThread.handleRead(AbstractNonblockingServer.java:203) ~[storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]",
                "at org.apache.thrift7.server.TNonblockingServer$SelectAcceptThread.select(TNonblockingServer.java:207) ~[storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]",
                "at org.apache.thrift7.server.TNonblockingServer$SelectAcceptThread.run(TNonblockingServer.java:158) [storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]"
            ],
            "RootCause": "The OutOfMemoryError is caused by the server attempting to allocate more memory than is available in the JVM heap, exacerbated by the absence of a defined maximum read buffer size in the THsHaServer configuration.",
            "StepsToReproduce": [
                "Start the Nimbus server.",
                "Send a malformed Thrift request using the command: echo 'Hello' | nc localhost 6627.",
                "Observe the server logs for the OutOfMemoryError."
            ],
            "ExpectedBehavior": "The Nimbus server should handle incoming requests without running out of memory, processing valid requests efficiently and returning appropriate responses.",
            "ObservedBehavior": "The Nimbus server crashes with a java.lang.OutOfMemoryError when a malformed Thrift request is sent, indicating that it cannot allocate the necessary memory.",
            "Suggestions": "To resolve this issue, consider increasing the JVM heap size using the -Xmx option, and specify the maxReadBufferBytes parameter in the THsHaServer configuration to limit the amount of data read into memory.",
            "problem_location": {
                "files": [
                    "SimpleTransportPlugin.java",
                    "THsHaServer.java"
                ],
                "classes": [
                    "org.apache.thrift7.server.THsHaServer",
                    "org.apache.thrift7.server.AbstractNonblockingServer"
                ],
                "methods": [
                    "THsHaServer.run",
                    "AbstractNonblockingServer$FrameBuffer.read"
                ]
            },
            "possible_fix": "Increase the JVM heap size by adding the option -Xmx2g (or appropriate size) to the server startup command. Additionally, modify the THsHaServer configuration to include a maxReadBufferBytes parameter to prevent excessive memory allocation."
        },
        "possible_fix_code": {
            "THsHaServer.run": "public void run() {\n    // Set a maximum read buffer size to prevent excessive memory allocation\n    int maxReadBufferBytes = 1024 * 1024; // Example: 1 MB\n    this.maxReadBufferBytes = maxReadBufferBytes;\n\n    // Existing run logic...\n    try {\n        // Server logic to handle incoming requests\n    } catch (Exception e) {\n        // Log the exception and handle it appropriately\n        log.error(\"Error in THsHaServer.run: \" + e.getMessage(), e);\n    }\n}"
        }
    },
    {
        "filename": "STORM-2682.json",
        "creation_time": "2017-08-07T15:20:27.000+0000",
        "bug_report": {
            "Title": "Supervisor crashes with NullPointerException",
            "Description": "The Apache Storm supervisor crashes approximately 30 seconds after startup due to a NullPointerException. The stack trace indicates that the error occurs during the processing of an event, specifically when the `updateBlobs` method in the `Localizer` class is called. This method attempts to retrieve and update resources based on the user, but it encounters a null reference, leading to the crash.",
            "StackTrace": [
                "2017-08-07 17:12:34.620 o.a.s.e.EventManagerImp Thread-4 [ERROR] {} Error when processing event",
                "java.lang.NullPointerException: null",
                "at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:936) ~[?:1.8.0_121]",
                "at org.apache.storm.localizer.Localizer.updateBlobs(Localizer.java:332) ~[storm-core-1.0.4.jar:1.0.4]",
                "at org.apache.storm.daemon.supervisor.timer.UpdateBlobs.updateBlobsForTopology(UpdateBlobs.java:99) ~[storm-core-1.0.4.jar:1.0.4]",
                "at org.apache.storm.daemon.supervisor.timer.UpdateBlobs.run(UpdateBlobs.java:72) ~[storm-core-1.0.4.jar:1.0.4]",
                "at org.apache.storm.event.EventManagerImp$1.run(EventManagerImp.java:54) ~[storm-core-1.0.4.jar:1.0.4]",
                "2017-08-07 17:12:34.620 o.a.s.u.Utils Thread-4 [ERROR] Halting process: Error when processing an event",
                "java.lang.RuntimeException: Halting process: Error when processing an event",
                "at org.apache.storm.utils.Utils.exitProcess(Utils.java:1750) ~[storm-core-1.0.4.jar:1.0.4]",
                "at org.apache.storm.event.EventManagerImp$1.run(EventManagerImp.java:63) ~[storm-core-1.0.4.jar:1.0.4]",
                "2017-08-07 17:12:34.631 o.a.s.d.s.Supervisor Thread-5 [INFO] Shutting down supervisor 65a0f977-474c-4938-a4f5-bc99939e96ff"
            ],
            "RootCause": "The NullPointerException is likely caused by missing or improperly initialized resources in the `updateBlobs` method of the `Localizer` class. Specifically, the `_userRsrc` map may not contain the user, leading to a null `LocalizedResourceSet`, or the configuration maps may not be properly populated.",
            "StepsToReproduce": [
                "Start the Apache Storm supervisor.",
                "Wait for approximately 30 seconds.",
                "Observe the logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The supervisor should start successfully and manage resources without crashing.",
            "ObservedBehavior": "The supervisor crashes with a NullPointerException after about 30 seconds of operation.",
            "Suggestions": "Ensure that the user resources and configuration maps are properly initialized and populated before calling the `updateBlobs` and `updateBlobsForTopology` methods. Validate that the `stormId` and user are correctly set in the configuration.",
            "problem_location": {
                "files": [
                    "storm-server/src/main/java/org/apache/storm/localizer/Localizer.java",
                    "storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/UpdateBlobs.java"
                ],
                "classes": [
                    "org.apache.storm.localizer.Localizer",
                    "org.apache.storm.daemon.supervisor.timer.UpdateBlobs"
                ],
                "methods": [
                    "Localizer.updateBlobs",
                    "UpdateBlobs.updateBlobsForTopology"
                ]
            },
            "possible_fix": "Add null checks for the user resources in the `_userRsrc` map and ensure that the configuration maps are populated correctly before invoking the `updateBlobs` method. For example, in `updateBlobs`, check if `lrsrcSet` is null and handle it appropriately."
        },
        "possible_fix_code": {
            "Localizer.updateBlobs": "  public List<LocalizedResource> updateBlobs(List<LocalResource> localResources,\n       String user) throws AuthorizationException, KeyNotFoundException, IOException {\n    LocalizedResourceSet lrsrcSet = _userRsrc.get(user);\n    ArrayList<LocalizedResource> results = new ArrayList<>();\n    ArrayList<Callable<LocalizedResource>> updates = new ArrayList<>();\n\n    if (lrsrcSet == null) {\n      // resource set must have been removed\n      LOG.warn(\"No resources found for user: {}\", user);\n      return results;\n    }\n    ClientBlobStore blobstore = null;\n    try {\n      blobstore = getClientBlobStore();\n      for (LocalResource localResource: localResources) {\n        String key = localResource.getBlobName();\n        if (key == null) {\n          LOG.warn(\"Blob name is null for local resource: {}\", localResource);\n          continue;\n        }\n        LocalizedResource lrsrc = lrsrcSet.get(key, localResource.shouldUncompress());\n        if (lrsrc == null) {\n          LOG.warn(\"Blob requested for update doesn't exist: {}\", key);\n          continue;\n        } else if ((boolean)_conf.getOrDefault(Config.DISABLE_SYMLINKS, false)) {\n          LOG.warn(\"Symlinks are disabled so blobs cannot be downloaded.\");\n          continue;\n        } else {\n          // update it if either the version isn't the latest or if any local blob files are missing\n          if (!isLocalizedResourceUpToDate(lrsrc, blobstore) ||\n              !isLocalizedResourceDownloaded(lrsrc)) {\n            LOG.debug(\"Updating blob: {}\", key);\n            updates.add(new DownloadBlob(this, _conf, key, new File(lrsrc.getFilePath()), user,\n                lrsrc.isUncompressed(), true));\n          }\n        }\n      }\n    } finally {\n      if(blobstore != null) {\n        blobstore.shutdown();\n      }\n    }\n    try {\n      List<Future<LocalizedResource>> futures = _updateExecService.invokeAll(updates);\n      for (Future<LocalizedResource> futureRsrc : futures) {\n        try {\n          LocalizedResource lrsrc = futureRsrc.get();\n          // put the resource just in case it was removed at same time by the cleaner\n          LocalizedResourceSet newSet = new LocalizedResourceSet(user);\n          LocalizedResourceSet newlrsrcSet = _userRsrc.putIfAbsent(user, newSet);\n          if (newlrsrcSet == null) {\n            newlrsrcSet = newSet;\n          }\n          newlrsrcSet.putIfAbsent(lrsrc.getKey(), lrsrc, lrsrc.isUncompressed());\n          results.add(lrsrc);\n        }\n        catch (ExecutionException e) {\n          LOG.error(\"Error updating blob: \", e);\n          if (e.getCause() instanceof AuthorizationException) {\n            throw (AuthorizationException)e.getCause();\n          }\n          if (e.getCause() instanceof KeyNotFoundException) {\n            throw (KeyNotFoundException)e.getCause();\n          }\n        }\n      }\n    } catch (RejectedExecutionException re) {\n      LOG.error(\"Error updating blobs : \", re);\n    } catch (InterruptedException ie) {\n      throw new IOException(\"Interrupted Exception\", ie);\n    }\n    return results;\n  }"
        }
    },
    {
        "filename": "STORM-3103.json",
        "creation_time": "2018-06-13T18:23:11.000+0000",
        "bug_report": {
            "Title": "Nimbus Stuck Shutting Down Causing Leadership Issues on Startup",
            "Description": "The Nimbus component of Apache Storm is encountering multiple RuntimeExceptions during startup, leading to a forced halt and subsequent leadership confusion. The primary issue appears to be a NullPointerException occurring in the Nimbus class, specifically during the execution of methods related to supervisor details and scheduler assignments. This situation is exacerbated by the Nimbus instance failing to assert its leadership status, which is critical in a distributed system where only the leader can perform certain operations.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.NullPointerException",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2685)",
                "at org.apache.storm.StormTimer$1.run(StormTimer.java:111)",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.storm.daemon.nimbus.Nimbus.readAllSupervisorDetails(Nimbus.java:1814)",
                "at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:1906)",
                "at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2057)",
                "at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2003)",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2681)",
                "java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}",
                "at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1311)",
                "at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2807)"
            ],
            "RootCause": "The root cause of the issue is a NullPointerException in the Nimbus class, specifically in the readAllSupervisorDetails method, which indicates that an object required for processing has not been initialized. Additionally, the Nimbus instance is unable to assert its leadership status, leading to further complications in processing events.",
            "StepsToReproduce": [
                "Start the Nimbus server.",
                "Attempt to submit a topology while the server is initializing.",
                "Observe the logs for any RuntimeExceptions or leadership errors."
            ],
            "ExpectedBehavior": "The Nimbus server should start without errors, successfully establish its leadership, and be able to process topology submissions without encountering NullPointerExceptions.",
            "ObservedBehavior": "The Nimbus server encounters a NullPointerException during startup, leading to a forced halt and confusion regarding leadership status, preventing it from processing topology submissions.",
            "Suggestions": "Check the initialization of objects related to supervisor details in the Nimbus class. Ensure that the Nimbus instance is correctly configured to assert its leadership status. Review the cluster configuration to prevent multiple instances from attempting to act as leaders.",
            "problem_location": {
                "files": [
                    "Nimbus.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.nimbus.Nimbus"
                ],
                "methods": [
                    "readAllSupervisorDetails",
                    "computeNewSchedulerAssignments",
                    "mkAssignments",
                    "assertIsLeader",
                    "submitTopologyWithOpts"
                ]
            },
            "possible_fix": "Ensure that all necessary objects are initialized before they are accessed in the readAllSupervisorDetails method. Additionally, verify the leadership election process to ensure that the Nimbus instance can correctly assert its leadership status."
        },
        "possible_fix_code": {
            "submitTopologyWithOpts": "    public void submitTopologyWithOpts(java.lang.String name, java.lang.String uploadedJarLocation, java.lang.String jsonConf, StormTopology topology, SubmitOptions options, org.apache.storm.thrift.async.AsyncMethodCallback<Void> resultHandler) throws org.apache.storm.thrift.TException {\n        // Ensure that the Nimbus instance is ready and is the leader before proceeding\n        assertIsLeader();\n        checkReady();\n        submitTopologyWithOpts_call method_call = new submitTopologyWithOpts_call(name, uploadedJarLocation, jsonConf, topology, options, resultHandler, this, ___protocolFactory, ___transport);\n        this.___currentMethod = method_call;\n        ___manager.call(method_call);\n    }"
        }
    }
]